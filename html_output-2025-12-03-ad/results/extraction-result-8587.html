<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8587 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8587</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8587</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b160bd68351b3dea29556a4128ccfe8c083213ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b160bd68351b3dea29556a4128ccfe8c083213ad" target="_blank">MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> MME-Reasoning is introduced, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions and conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities.</p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8587.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8587.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Pro-T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.5-Pro-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal 'thinking' MLLM variant evaluated in this paper; configured to generate extended chain-of-thought style outputs at test time to improve reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5-Pro-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large language model in 'thinking' configuration that produces long-form intermediate reasoning (chain-of-thought style) at inference to improve multimodal logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A curated multimodal benchmark introduced in this paper that tests strict logical reasoning across three types — deductive, inductive, and abductive — with questions designed to minimize reliance on perception and domain knowledge; includes multiple-choice, free-form, and rule-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Thinking-mode inference (long-form chain-of-thought / CoT style outputs) evaluated directly on MME-Reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 60.2%; Deductive 64.0%, Inductive 51.7%, Abductive 62.8% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Top-performing model in the benchmark; outperforms many chat/open-source baselines. Thinking-mode variants generally outperform their chat counterparts in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite leading average score, still limited (60.2%) overall; exhibits imbalance across capability types (weaker on pattern analysis and spatial/temporal than on calculation or causal chaining in some splits). Abductive reasoning remains challenging for many models, though Gemini-Pro performed relatively well on the abductive subset.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates that 'thinking' configuration (longer CoT-style outputs) improves multimodal logical reasoning, but gains are imperfect and do not close the gap to human-level performance; increasing output length helps up to diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8587.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seed1.5-VL-T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seed1.5-VL-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source thinking MLLM evaluated as a top performer on MME-Reasoning, shown to benefit substantially from a thinking-mode configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seed1.5-VL-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal model in a 'thinking' variant designed to produce longer reasoning traces (CoT) at test time to improve problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive multimodal logical reasoning benchmark (deduction, induction, abduction) introduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Thinking-mode inference (extended chain-of-thought outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 59.9%; Deductive 64.5%, Inductive 52.3%, Abductive 60.8% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Seed1.5-VL-Thinking outperformed its chat/base Seed1.5-VL by +12.4 points on average, indicating a large test-time gain from thinking-mode.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still far from human performance (human mini-set 83.4%); shows the common gap where abductive reasoning lags behind deductive; planning & exploration tasks are weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large test-time chain-of-thought outputs substantially improve scores for some closed-source models, but do not eliminate deficiencies in abductive reasoning or planning/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8587.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o4-mini (Thinking variant evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source 'thinking' MLLM evaluated on MME-Reasoning that produces extended reasoning outputs and shows strong performance relative to many baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o4-mini (thinking eval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal model (o4-mini) evaluated in a thinking configuration that encourages longer reasoning traces. Referenced in the paper among top-thinking models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>The MME-Reasoning benchmark tests strict multimodal logical reasoning across deductive, inductive, and abductive tasks, minimizing perceptual and heavy-knowledge requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Thinking-mode inference (long chain-of-thought outputs) and comparison versus chat-mode versions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 57.5%; Deductive 60.6%, Inductive 51.4%, Abductive 59.0% (Table 3). One of the top-performing thinking models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>o4-mini exceeded GPT-4o (chat) by approximately 15.5 points in comparisons reported by the paper, showing a strong thinking-mode advantage over the chat baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large output lengths observed (case study showed up to 24.6k tokens produced), with repetitive reflections and diminishing returns; struggles on planning & exploration and some pattern-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Thinking-mode yields substantial gains vs chat-mode for this model, but increasing chain length exhibits diminishing returns and can introduce redundancy and compute overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8587.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source chat-oriented multimodal model used as a chat baseline in evaluations; typically produces shorter outputs and shows lower token efficiency than thinking models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal 'chat' model from OpenAI evaluated as a chat baseline on MME-Reasoning; typically configured for conversational responses rather than long chain-of-thought outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MME-Reasoning: multimodal logical reasoning benchmark focusing on deductive, inductive, abductive problems with minimized perception/knowledge confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chat-style inference baseline; compared to thinking-mode variants and other test-time compute scaling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 30.2%; Deductive 29.0%, Inductive 34.7%, Abductive 27.9% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperformed relative to thinking-mode variants (e.g., o4-mini and Seed1.5-VL-T); thinking-mode models outperformed chat models by large margins in many comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower performance on this strict logical reasoning benchmark; shorter outputs and lower token efficiency limit reasoning depth; weak on planning/exploration style open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Chat-configured models demonstrate lower reasoning performance on MME-Reasoning compared to thinking-mode models, supporting the paper's conclusion that test-time reasoning-mode scaling (longer CoT) aids logical reasoning but is compute-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8587.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal chat model evaluated at multiple scales (7B, 32B, 72B); used to study scaling, thinking-mode variants (QVQ preview), and test-time compute interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal chat model (7B parameter variant) from the Qwen family evaluated across MME-Reasoning; the family also includes larger variants used as reward models or in RL training in related baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Curated multimodal logical reasoning benchmark of deductive, inductive, abductive tasks with multiple evaluation formats (multiple-choice, free-form, rule-based).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chat baseline; also used in comparisons involving 'thinking' style QVQ-72B-preview and Test-Time Compute Scaling (MCTS) where Qwen2.5-VL-32B was used as a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 26.8%; Deductive 31.4%, Inductive 27.5%, Abductive 20.9% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QVQ-72B-Preview (thinking variant) slightly improved over Qwen2.5-VL at larger size; using larger Qwen2.5-VL-32B as reward model in MCTS reduced performance (Table A.5), indicating TTS did not help here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Poorer performance on abductive reasoning (20.9%) relative to deductive; test-time compute scaling (MCTS) led to performance drops for sampled experiments, suggesting search-based TTS can harm generalization on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Open-source chat models lag behind closed-source thinking models on strict logical reasoning; increasing model size within this family helps somewhat, but test-time compute scaling via MCTS can degrade performance on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8587.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VL-Rethinker-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VL-Rethinker (72B, thinking variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 'thinking' MLLM applying RL-based incentives for self-reflection; evaluated as a thinking-model that aims to increase self-reflection and chain length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VL-Rethinker-72B (thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model trained with reinforcement learning to incentivize self-reflection and extended reasoning (Rethinker-style methods), evaluated in a thinking configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive multimodal logical reasoning benchmark emphasizing deductive/inductive/abductive tasks with minimized perception & domain knowledge confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Reinforcement-incentivized self-reflection (Rethinker-style), thinking-mode inference producing longer rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 35.8%; Deductive 39.0%, Inductive 36.0%, Abductive 31.9% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>VL-Rethinker shows modest improvements over some base open-source models (e.g., improves over Qwen2.5-VL in thinking variants by ~1.7 points as reported), but remains well below top closed-source thinking models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although designed to encourage reflection, performance gains are modest; still shows a notable gap on abductive reasoning and in planning/exploration tasks; some rule-based RL methods failed to generalize at 7B scale.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>RL-style incentives for self-reflection yield modest benefits; the paper recommends further innovation in training paradigms to transfer LLM gains into multimodal domains rather than directly copying R1-style recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8587.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MM-Eureka-Qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MM-Eureka (Qwen-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model trained with rule-based RL (MM-Eureka) evaluated in the paper as a representative rule-based RL approach applied to MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MM-Eureka-Qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model family using Generalized Reinforcement Preference Optimization / rule-based RL to encourage reasoning behaviors; instantiated with Qwen-32B base in the MM-Eureka pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A strict multimodal logical reasoning benchmark covering deductive, inductive, abductive reasoning with multiple question formats and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Rule-based RL (GRPO-style / R1-inspired training that rewards adherence to rules, longer chains, and reflective behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 30.6%; Deductive 32.9%, Inductive 30.5%, Abductive 28.1% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MM-Eureka variants (7B and 32B) did not consistently outperform their base models — in many 7B-scale instances, rule-based RL decreased performance relative to base models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Rule-based RL did not always bring improvements and often degraded performance at smaller scales (7B); suggests poor generalization of some R1-like training recipes to multimodal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Paper concludes that while rule-based RL can activate longer outputs and reflective behavior, it is not a panacea; direct replication of R1-style approaches can fail in multimodal settings and may reduce generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8587.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8587.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1-VL (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based RL (R1-style) multimodal model evaluated to probe whether R1-style RL improves strict logical reasoning in MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R1-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal model trained with R1-style rule-based reinforcement learning targeted at improving reasoning behavior; evaluated at 7B scale in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MME-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>The MME-Reasoning benchmark of multimodal logical tasks (deduction, induction, abduction) designed to require reasoning rather than perception or heavy domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Rule-based reinforcement learning (R1-style) intended to incentivize longer reasoning chains and reflective steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 21.1%; Deductive 25.3%, Inductive 21.8%, Abductive 15.8% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>At 7B scale, R1-VL and similar R1-style models frequently underperformed compared to non-R1 or larger variants; paper reports performance degradation for many 7B rule-based RL models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large drop in performance at 7B; struggles particularly on abductive reasoning and planning/exploration; indicates R1-style training did not reliably transfer benefits to multimodal strict logical reasoning at small scale.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>R1-style rule-based RL can encourage longer outputs but does not consistently improve strict logical reasoning in MLLMs; training-paradigm innovation is necessary to realize LLM-like R1 gains in multimodal models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Multimodal chain-of-thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Vision-r1: Incentivizing reasoning capability in multimodal large language models <em>(Rating: 2)</em></li>
                <li>MM-EUREKA: Exploring visual aha moment with rule-based large-scale reinforcement learning <em>(Rating: 2)</em></li>
                <li>MathVista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 1)</em></li>
                <li>PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns <em>(Rating: 1)</em></li>
                <li>Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8587",
    "paper_id": "paper-b160bd68351b3dea29556a4128ccfe8c083213ad",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Gemini-2.5-Pro-T",
            "name_full": "Gemini-2.5-Pro-Thinking",
            "brief_description": "A closed-source multimodal 'thinking' MLLM variant evaluated in this paper; configured to generate extended chain-of-thought style outputs at test time to improve reasoning depth.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5-Pro-Thinking",
            "model_description": "Closed-source multimodal large language model in 'thinking' configuration that produces long-form intermediate reasoning (chain-of-thought style) at inference to improve multimodal logical reasoning.",
            "model_size": null,
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "A curated multimodal benchmark introduced in this paper that tests strict logical reasoning across three types — deductive, inductive, and abductive — with questions designed to minimize reliance on perception and domain knowledge; includes multiple-choice, free-form, and rule-based tasks.",
            "method_or_approach": "Thinking-mode inference (long-form chain-of-thought / CoT style outputs) evaluated directly on MME-Reasoning.",
            "performance": "Average accuracy 60.2%; Deductive 64.0%, Inductive 51.7%, Abductive 62.8% (Table 3).",
            "baseline_comparison": "Top-performing model in the benchmark; outperforms many chat/open-source baselines. Thinking-mode variants generally outperform their chat counterparts in the paper's comparisons.",
            "limitations_or_failures": "Despite leading average score, still limited (60.2%) overall; exhibits imbalance across capability types (weaker on pattern analysis and spatial/temporal than on calculation or causal chaining in some splits). Abductive reasoning remains challenging for many models, though Gemini-Pro performed relatively well on the abductive subset.",
            "insights_or_conclusions": "Demonstrates that 'thinking' configuration (longer CoT-style outputs) improves multimodal logical reasoning, but gains are imperfect and do not close the gap to human-level performance; increasing output length helps up to diminishing returns.",
            "uuid": "e8587.0",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Seed1.5-VL-T",
            "name_full": "Seed1.5-VL-Thinking",
            "brief_description": "A closed-source thinking MLLM evaluated as a top performer on MME-Reasoning, shown to benefit substantially from a thinking-mode configuration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Seed1.5-VL-Thinking",
            "model_description": "Closed-source multimodal model in a 'thinking' variant designed to produce longer reasoning traces (CoT) at test time to improve problem solving.",
            "model_size": null,
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "Comprehensive multimodal logical reasoning benchmark (deduction, induction, abduction) introduced in this paper.",
            "method_or_approach": "Thinking-mode inference (extended chain-of-thought outputs).",
            "performance": "Average accuracy 59.9%; Deductive 64.5%, Inductive 52.3%, Abductive 60.8% (Table 3).",
            "baseline_comparison": "Seed1.5-VL-Thinking outperformed its chat/base Seed1.5-VL by +12.4 points on average, indicating a large test-time gain from thinking-mode.",
            "limitations_or_failures": "Still far from human performance (human mini-set 83.4%); shows the common gap where abductive reasoning lags behind deductive; planning & exploration tasks are weaker.",
            "insights_or_conclusions": "Large test-time chain-of-thought outputs substantially improve scores for some closed-source models, but do not eliminate deficiencies in abductive reasoning or planning/exploration.",
            "uuid": "e8587.1",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "o4-mini",
            "name_full": "o4-mini (Thinking variant evaluated)",
            "brief_description": "A closed-source 'thinking' MLLM evaluated on MME-Reasoning that produces extended reasoning outputs and shows strong performance relative to many baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o4-mini (thinking eval)",
            "model_description": "Closed-source multimodal model (o4-mini) evaluated in a thinking configuration that encourages longer reasoning traces. Referenced in the paper among top-thinking models.",
            "model_size": null,
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "The MME-Reasoning benchmark tests strict multimodal logical reasoning across deductive, inductive, and abductive tasks, minimizing perceptual and heavy-knowledge requirements.",
            "method_or_approach": "Thinking-mode inference (long chain-of-thought outputs) and comparison versus chat-mode versions.",
            "performance": "Average accuracy 57.5%; Deductive 60.6%, Inductive 51.4%, Abductive 59.0% (Table 3). One of the top-performing thinking models.",
            "baseline_comparison": "o4-mini exceeded GPT-4o (chat) by approximately 15.5 points in comparisons reported by the paper, showing a strong thinking-mode advantage over the chat baseline.",
            "limitations_or_failures": "Large output lengths observed (case study showed up to 24.6k tokens produced), with repetitive reflections and diminishing returns; struggles on planning & exploration and some pattern-analysis tasks.",
            "insights_or_conclusions": "Thinking-mode yields substantial gains vs chat-mode for this model, but increasing chain length exhibits diminishing returns and can introduce redundancy and compute overhead.",
            "uuid": "e8587.2",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed-source chat-oriented multimodal model used as a chat baseline in evaluations; typically produces shorter outputs and shows lower token efficiency than thinking models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal 'chat' model from OpenAI evaluated as a chat baseline on MME-Reasoning; typically configured for conversational responses rather than long chain-of-thought outputs.",
            "model_size": null,
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "MME-Reasoning: multimodal logical reasoning benchmark focusing on deductive, inductive, abductive problems with minimized perception/knowledge confounds.",
            "method_or_approach": "Chat-style inference baseline; compared to thinking-mode variants and other test-time compute scaling methods.",
            "performance": "Average accuracy 30.2%; Deductive 29.0%, Inductive 34.7%, Abductive 27.9% (Table 3).",
            "baseline_comparison": "Underperformed relative to thinking-mode variants (e.g., o4-mini and Seed1.5-VL-T); thinking-mode models outperformed chat models by large margins in many comparisons.",
            "limitations_or_failures": "Lower performance on this strict logical reasoning benchmark; shorter outputs and lower token efficiency limit reasoning depth; weak on planning/exploration style open-ended tasks.",
            "insights_or_conclusions": "Chat-configured models demonstrate lower reasoning performance on MME-Reasoning compared to thinking-mode models, supporting the paper's conclusion that test-time reasoning-mode scaling (longer CoT) aids logical reasoning but is compute-intensive.",
            "uuid": "e8587.3",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen2.5-VL-7B",
            "name_full": "Qwen2.5-VL (7B)",
            "brief_description": "An open-source multimodal chat model evaluated at multiple scales (7B, 32B, 72B); used to study scaling, thinking-mode variants (QVQ preview), and test-time compute interventions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL-7B",
            "model_description": "Open-source multimodal chat model (7B parameter variant) from the Qwen family evaluated across MME-Reasoning; the family also includes larger variants used as reward models or in RL training in related baselines.",
            "model_size": "7B",
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "Curated multimodal logical reasoning benchmark of deductive, inductive, abductive tasks with multiple evaluation formats (multiple-choice, free-form, rule-based).",
            "method_or_approach": "Chat baseline; also used in comparisons involving 'thinking' style QVQ-72B-preview and Test-Time Compute Scaling (MCTS) where Qwen2.5-VL-32B was used as a reward model.",
            "performance": "Average accuracy 26.8%; Deductive 31.4%, Inductive 27.5%, Abductive 20.9% (Table 3).",
            "baseline_comparison": "QVQ-72B-Preview (thinking variant) slightly improved over Qwen2.5-VL at larger size; using larger Qwen2.5-VL-32B as reward model in MCTS reduced performance (Table A.5), indicating TTS did not help here.",
            "limitations_or_failures": "Poorer performance on abductive reasoning (20.9%) relative to deductive; test-time compute scaling (MCTS) led to performance drops for sampled experiments, suggesting search-based TTS can harm generalization on these tasks.",
            "insights_or_conclusions": "Open-source chat models lag behind closed-source thinking models on strict logical reasoning; increasing model size within this family helps somewhat, but test-time compute scaling via MCTS can degrade performance on the benchmark.",
            "uuid": "e8587.4",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "VL-Rethinker-72B",
            "name_full": "VL-Rethinker (72B, thinking variant)",
            "brief_description": "An open-source 'thinking' MLLM applying RL-based incentives for self-reflection; evaluated as a thinking-model that aims to increase self-reflection and chain length.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VL-Rethinker-72B (thinking)",
            "model_description": "Open-source multimodal model trained with reinforcement learning to incentivize self-reflection and extended reasoning (Rethinker-style methods), evaluated in a thinking configuration.",
            "model_size": "72B",
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "Comprehensive multimodal logical reasoning benchmark emphasizing deductive/inductive/abductive tasks with minimized perception & domain knowledge confounds.",
            "method_or_approach": "Reinforcement-incentivized self-reflection (Rethinker-style), thinking-mode inference producing longer rationales.",
            "performance": "Average accuracy 35.8%; Deductive 39.0%, Inductive 36.0%, Abductive 31.9% (Table 4).",
            "baseline_comparison": "VL-Rethinker shows modest improvements over some base open-source models (e.g., improves over Qwen2.5-VL in thinking variants by ~1.7 points as reported), but remains well below top closed-source thinking models.",
            "limitations_or_failures": "Although designed to encourage reflection, performance gains are modest; still shows a notable gap on abductive reasoning and in planning/exploration tasks; some rule-based RL methods failed to generalize at 7B scale.",
            "insights_or_conclusions": "RL-style incentives for self-reflection yield modest benefits; the paper recommends further innovation in training paradigms to transfer LLM gains into multimodal domains rather than directly copying R1-style recipes.",
            "uuid": "e8587.5",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MM-Eureka-Qwen-32B",
            "name_full": "MM-Eureka (Qwen-32B)",
            "brief_description": "A multimodal model trained with rule-based RL (MM-Eureka) evaluated in the paper as a representative rule-based RL approach applied to MLLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MM-Eureka-Qwen-32B",
            "model_description": "Model family using Generalized Reinforcement Preference Optimization / rule-based RL to encourage reasoning behaviors; instantiated with Qwen-32B base in the MM-Eureka pipeline.",
            "model_size": "32B",
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "A strict multimodal logical reasoning benchmark covering deductive, inductive, abductive reasoning with multiple question formats and difficulty levels.",
            "method_or_approach": "Rule-based RL (GRPO-style / R1-inspired training that rewards adherence to rules, longer chains, and reflective behavior).",
            "performance": "Average accuracy 30.6%; Deductive 32.9%, Inductive 30.5%, Abductive 28.1% (Table 4).",
            "baseline_comparison": "MM-Eureka variants (7B and 32B) did not consistently outperform their base models — in many 7B-scale instances, rule-based RL decreased performance relative to base models.",
            "limitations_or_failures": "Rule-based RL did not always bring improvements and often degraded performance at smaller scales (7B); suggests poor generalization of some R1-like training recipes to multimodal domains.",
            "insights_or_conclusions": "Paper concludes that while rule-based RL can activate longer outputs and reflective behavior, it is not a panacea; direct replication of R1-style approaches can fail in multimodal settings and may reduce generalization.",
            "uuid": "e8587.6",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "R1-VL-7B",
            "name_full": "R1-VL (7B)",
            "brief_description": "A rule-based RL (R1-style) multimodal model evaluated to probe whether R1-style RL improves strict logical reasoning in MLLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "R1-VL-7B",
            "model_description": "A multimodal model trained with R1-style rule-based reinforcement learning targeted at improving reasoning behavior; evaluated at 7B scale in this paper.",
            "model_size": "7B",
            "reasoning_task_name": "MME-Reasoning",
            "reasoning_task_description": "The MME-Reasoning benchmark of multimodal logical tasks (deduction, induction, abduction) designed to require reasoning rather than perception or heavy domain knowledge.",
            "method_or_approach": "Rule-based reinforcement learning (R1-style) intended to incentivize longer reasoning chains and reflective steps.",
            "performance": "Average accuracy 21.1%; Deductive 25.3%, Inductive 21.8%, Abductive 15.8% (Table 4).",
            "baseline_comparison": "At 7B scale, R1-VL and similar R1-style models frequently underperformed compared to non-R1 or larger variants; paper reports performance degradation for many 7B rule-based RL models.",
            "limitations_or_failures": "Large drop in performance at 7B; struggles particularly on abductive reasoning and planning/exploration; indicates R1-style training did not reliably transfer benefits to multimodal strict logical reasoning at small scale.",
            "insights_or_conclusions": "R1-style rule-based RL can encourage longer outputs but does not consistently improve strict logical reasoning in MLLMs; training-paradigm innovation is necessary to realize LLM-like R1 gains in multimodal models.",
            "uuid": "e8587.7",
            "source_info": {
                "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Multimodal chain-of-thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "multimodal_chainofthought_reasoning_in_language_models"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Vision-r1: Incentivizing reasoning capability in multimodal large language models",
            "rating": 2,
            "sanitized_title": "visionr1_incentivizing_reasoning_capability_in_multimodal_large_language_models"
        },
        {
            "paper_title": "MM-EUREKA: Exploring visual aha moment with rule-based large-scale reinforcement learning",
            "rating": 2,
            "sanitized_title": "mmeureka_exploring_visual_aha_moment_with_rulebased_largescale_reinforcement_learning"
        },
        {
            "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 1,
            "sanitized_title": "mathvista_evaluating_mathematical_reasoning_of_foundation_models_in_visual_contexts"
        },
        {
            "paper_title": "PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns",
            "rating": 1,
            "sanitized_title": "puzzlevqa_diagnosing_multimodal_reasoning_challenges_of_language_models_with_abstract_visual_patterns"
        },
        {
            "paper_title": "Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search",
            "rating": 1,
            "sanitized_title": "mulberry_empowering_mllm_with_o1like_reasoning_and_reflection_via_collective_monte_carlo_tree_search"
        }
    ],
    "cost": 0.01564625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs</h1>
<p>Jiakang Yuan ${ }^{1,3,+}$, Tianshuo Peng ${ }^{2,3,+}$, Yilei Jiang ${ }^{2}$, Yiting Lu ${ }^{4}$, Renrui Zhang ${ }^{2}$, Kaituo Feng ${ }^{2}$, Chaoyou Fu ${ }^{3}$, Tao Chen ${ }^{1, \dagger}$, Lei Bai ${ }^{3}$, Bo Zhang ${ }^{3, \dagger}$, Xiangyu Yue ${ }^{2,3}$<br>${ }^{1}$ Fudan University ${ }^{2}$ MMLab, The Chinese University of Hong Kong<br>${ }^{3}$ Shanghai AI Laboratory ${ }^{4}$ University of Science and Technology of China ${ }^{5}$ Nanjing University<br>$\star$ https://alpha-innovator.github.io/mmereasoning.github.io/<br>Dhttps://github.com/Alpha-Innovator/MME-Reasoning<br>$\star$ https://huggingface.co/datasets/U4R/MME-Reasoning</p>
<h4>Abstract</h4>
<p>Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as "thinking mode" and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.</p>
<h2>1 Introduction</h2>
<p>Logical reasoning (Liu et al., 2025a), a fundamental cognitive process of analyzing premises and evidence to reach valid conclusions, serves as the cornerstone of human intelligence. Multimodal reasoning (Jaech et al., 2024) enables humans to integrate information from different modalities, such as visual and text, which is essential for tackling complex tasks. Recently, with the emergence of reasoning large language models (LLMs) (Dubey et al., 2024; Yang et al., 2024a) such as DeepSeekR1 (DeepSeek-AI, 2025), injecting reasoning capability into multimodal large language models (MLLMs) (OpenAI, 2024; Qwen Team, 2025a; Li et al., 2024) has begun to be explored (Peng et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance comparison between thinking and chat models on MME-Reasoning.</p>
<p>2025b; Zhang et al., 2025a; Huang et al., 2025). Despite the significant progress in reasoning MLLMs, a comprehensive evaluation of their capabilities still remains an open challenge. Therefore, it is particularly important to establish a fair and robust evaluation benchmark for assessing the reasoning capabilities of MLLMs and further accelerate the development of this field.</p>
<p>Currently, most benchmarks (Fu et al., 2023; Wang et al., 2024a; Lu et al., 2023; Yue et al., 2024a;b; Gong et al., 2024; He et al., 2024) designed for multimodal reasoning primarily focus on knowledgedriven tasks. For example, MathVista (Lu et al., 2023) and MathVerse (Zhang et al., 2024) provide comprehensive evaluations of MLLMs' mathematical reasoning abilities. OlympiadBench (He et al., 2024) and EMMA (Hao et al., 2025) expand the scope to include additional subjects, such as physics and chemistry. Apart from knowledge-driven tasks, some works (Song et al., 2025; Chia et al., 2024; Zhang et al., 2025b) have begun to decouple knowledge from logical reasoning, aiming to assess the reasoning abilities of MLLMs independent of specific domain knowledge. For instance, SciVerse (Guo et al., 2025b) and VisualPuzzles (Song et al., 2025) focus on reasoning-focused, knowledge-light tasks.</p>
<p>Despite recent advances, existing benchmarks still suffer from several problems as outlined below.
First, lacking explicit categorization of reasoning and insufficient coverage of reasoning types. In logic, reasoning is typically classified into three types: abduction, deduction, and induction (Peirce, 2014). Most existing benchmarks primarily concentrate on evaluating MLLMs' inductive and deductive reasoning ability. For example, most of the questions in MathVerse (Lu et al., 2023) belong to deductive reasoning, which uses rules and premises to derive conclusions. PuzzleVQA (Chia et al., 2024) only contains questions of inductive reasoning, which learns rules based on premises and conclusions. However, abductive reasoning ability (i.e., exploring premises to explain a conclusion based on the conclusion and rules) is rarely evaluated. Second, the concept of reasoning is not clear enough, which is reflected in confusing perception with reasoning or equating reasoning with the complexity of the required knowledge. For example, MathVista (Lu et al., 2023) contains many questions that can be answered through visual perception, while OlympiadBench (He et al., 2024) includes questions that require advanced domain knowledge, which the model may not have access to. This may lead to an inaccurate evaluation of MLLMs' reasoning ability.</p>
<p>To address these issues, we introduce MME-Reasoning, a comprehensive benchmark specifically designed to evaluate the reasoning capability of MLLMs. MME-Reasoning consists of 1,188 carefully curated questions that systematically cover types of logical</p>
<p>Table 1: Response token length on different datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MathVista</th>
<th style="text-align: center;">MathVerse</th>
<th style="text-align: center;">MME-R.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5-VL-7B</td>
<td style="text-align: center;">209.5</td>
<td style="text-align: center;">207.6</td>
<td style="text-align: center;">$\mathbf{4 4 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">162.6</td>
<td style="text-align: center;">157.3</td>
<td style="text-align: center;">$\mathbf{3 2 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3.7-Sonnet-T</td>
<td style="text-align: center;">519.4</td>
<td style="text-align: center;">563.2</td>
<td style="text-align: center;">$\mathbf{9 7 9 . 2}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of questions in MME-Reasoning which covers comprehensive reasoning types.
reasoning (i.e., inductive, deductive, and abductive), while spanning a range of difficulty levels, as illustrated in Fig. 2. Besides, we identify 5 key abilities related to multimodal reasoning, including calculation, planning and exploring, spatial-temporal, pattern analysis, and casual chaining analysis, and annotate the type of ability assessed by each question. To ensure a true evaluation of reasoning ability, MME-Reasoning eliminates questions that can be answered purely through perception or require complex domain knowledge, thereby focusing on the core reasoning skills of the model. We report the average response lengths of three representative models across different datasets in Tab. 1. Results show that responses on MME-Reasoning are significantly longer than those on previous reasoning benchmarks, indicating its challenging and rigorous demands on model reasoning. Furthermore, MME-Reasoning incorporates a variety of evaluation methods, including multiple-choice, free-form, and rule-based (e.g., Sudoku Puzzles) questions. Employing</p>
<p>multiple evaluation methods enables a wider variety of question types, thereby facilitating a more comprehensive evaluation of models' capabilities.
Experiments were conducted on state-of-the-art MLLMs, covering Chat and Thinking types of both open-source and closed-source, as presented in Fig. 1. Evaluations with MME-Reasoning reveal these key findings:</p>
<ul>
<li>MLLMs exhibit significant limitations and pronounced imbalances in reasoning capabilities. Even the most advanced MLLMs achieve only limited results under holistic logical reasoning evaluation, with Gemini-Pro-2.5-Thinking scoring only $60.19 \%$, followed by Seed1.5-VL (59.85) and o4-mini (57.49\%). These results indicate that MME-Reasoning, through its comprehensive evaluation of all the logical reasoning types, establishes a systematic and challenging benchmark for multimodal reasoning.</li>
<li>Abductive reasoning remains a major bottleneck for current MLLMs. While most models demonstrate competent deductive reasoning, their abductive reasoning lags significantly. Closed-source models exhibit an average gap of 5.38 points between deductive and abductive tasks, which further widens to 9.81 among open-source models, making abductive reasoning a key bottleneck. Since it underpins many real-world tasks, addressing this gap is crucial for improving overall reasoning.</li>
<li>Reasoning length scales with task difficulty, benefiting performance but accompanied by marginal effects and decreasing token efficiency. Thinking Models exhibit longer reasoning chains, particularly on more difficult questions, demonstrating adaptive inference budgeting and enhanced depth of reasoning. A positive correlation between average token count (ATC) and accuracy supports the effectiveness of extended outputs, especially in complex tasks. However, this performance gain plateaus beyond a certain length, revealing diminishing returns.</li>
</ul>
<h1>2 Related Works</h1>
<h3>2.1 Multimodal Reasoning</h3>
<p>Chain-of-thought (CoT) reasoning (Wei et al., 2022) has emerged as a key paradigm for enhancing the reasoning capability of LLMs. By generating intermediate steps before the final answer, CoT enables more transparent and accurate decision-making, especially in complex tasks such as arithmetic, logical deduction, and commonsense reasoning. Inspired by its success in text-only settings, CoT has recently been extended to MLLMs, giving rise to multimodal chain-of-thought (MCoT) reasoning (Jiang et al., 2024; Zhang et al., 2023; Chen et al., 2023; Peng et al., 2024; Lu et al., 2025; Xia et al., 2024a). Early approaches such as Multimodal-CoT (Zhang et al., 2023) and IPVR (Chen et al., 2023) demonstrate that generating intermediate reasoning steps significantly improves model performance in visual question answering. Other methods such as HoT (Yao et al., 2023), BDoG (Zheng et al., 2024), and VisualSketchpad (Hu et al., 2024) introduce graph structures, debating agents, and visual intermediate states to further enhance interpretability and reasoning depth.</p>
<p>More recently, following the success of Deepseek-R1, the Generalized Reinforcement Preference Optimization (GRPO) algorithm has gained traction in the development of multimodal models. Methods such as MM-EUREKA (Meng et al., 2025), Vt-R1 (Zhou et al., 2025), LMM-R1 (Yingzhe et al., 2025), and R1-V (Chen et al., 2025) adapt GRPO to solve mathematical geometry tasks, demonstrating promising reflective reasoning capabilities. Other works, including VLM-R1 (Shen et al., 2025), Visual-RFT (Liu et al., 2025b), and Seg-Zero (Yuqi et al., 2025), apply GRPO to enhance</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The overall construction process of MME-Reasoning.
visual competencies such as grounding, object detection, and classification. The algorithm has also been extended to video and audio modalities through models such as Video-R1 (Feng et al., 2025), and R1-Omni (Zhao et al., 2025).</p>
<h1>2.2 Multimodal Reasoning Benchmarks</h1>
<p>Recent benchmarks have advanced the evaluation of multimodal reasoning, particularly in visuallanguage settings. Early works such as CLEVR (Johnson et al., 2016) and GQA (Hudson \&amp; Manning, 2019) assess compositional and spatial reasoning, while more recent benchmarks such as MathVista (Lu et al., 2024), PuzzleBench (Zhang et al., 2025b), ChartX (Xia et al., 2024b) and PuzzleVQA (Chia et al., 2024) emphasize symbolic logic or pattern discovery. However, these benchmarks typically focus on narrow subtypes of reasoning-especially inductive logic-and fail to offer a holistic evaluation across deductive, inductive, and abductive paradigms.
Furthermore, many existing datasets conflate perception with reasoning. Tasks solvable via recognition or superficial pattern matching are often labeled as reasoning challenges, while high-difficulty benchmarks such as GPQA (Rein et al., 2023), OlympiaBench (He et al., 2024) and MME-CoT (Jiang et al., 2025) overly depend on domain-specific knowledge rather than logical inference. Evaluation protocols are also limited-most rely on multiple-choice formats and lack support for open-ended or rule-based assessment. In contrast, our benchmark provides a fine-grained evaluation of visual reasoning, explicitly covering the three classical reasoning types.</p>
<h2>3 The MME-Reasoning Benchmark</h2>
<p>We introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs. MME-Reasoning consists of 1,188 questions, including 1,008 newly collected items. MME-Reasoning comprehensively covers three types of reasoning (i.e., inductive, deductive, and abductive) and includes three question types (i.e., multiple-choice, free-form, and rule-based). We further divided MME-Reasoning into three difficulty levels (i.e., easy, medium, and hard). The key statistics and construction pipeline of MME-Reasoning are shown in Tab. 2 and Fig. 3.</p>
<p>Table 2: Statistics of MME-Reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistics</th>
<th style="text-align: center;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">$1188(100 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- Newly-add questions</td>
<td style="text-align: center;">$84.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Sampled questions</td>
<td style="text-align: center;">$15.15 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Question Type</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Multi-choice questions</td>
<td style="text-align: center;">$58.50 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Free-form questions</td>
<td style="text-align: center;">$31.57 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Rule-based questions</td>
<td style="text-align: center;">$9.93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Image Type</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Single-image questions</td>
<td style="text-align: center;">$58.50 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Multi-image questions</td>
<td style="text-align: center;">$31.57 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Disciplinary</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Disciplinary questions</td>
<td style="text-align: center;">$31.48 \%$</td>
</tr>
<tr>
<td style="text-align: left;">- Non-discipl. questions</td>
<td style="text-align: center;">$68.52 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Overview of MME-Reasoning.</p>
<h1>3.1 Design Principles of MME-Reasoning</h1>
<p>To ensure a comprehensive evaluation of multimodal reasoning and address issues present in previous benchmarks, such as incomplete coverage of reasoning types, unclear definitions of reasoning, and insufficient evaluation methods, MME-Reasoning is guided by the following principles: 1) Comprehensiveness. According to Charles Sanders Peirce's classification of reasoning, deduction, induction, and abduction can be distinguished based on different arrangements of rule, case, and result. Therefore, a comprehensive evaluation of reasoning ability should include all three types of reasoning tasks. 2) Beyond Perception. Each question should be carefully designed to ensure that the answer is obtained through a reasoning process instead of simple visual recognition. 3) Minimizing Knowledge Reliance. It is essential to ensure that the questions do not require complex domain knowledge, thereby preventing models from being penalized for the absence of specialized information. In MME-Reasoning, the domain expertise is limited to K12 or below. 4) Diverse evaluation formats. The benchmark should consist of diverse question types, avoiding incomplete evaluation caused by a narrow range of task types.</p>
<h3>3.2 Data Collection and Curation</h3>
<p>Data Collection. We initiate by collecting questions related to multimodal reasoning from a variety of sources, including 1) Textbooks can provide subject exam questions (e.g., mathematics, physics, chemistry, and biology). To evaluate reasoning ability, the chemistry and biology questions mainly focus on reaction process inference, and genetic lineage inference. 2) Online resources, books on logical practice, and Chinese Civil Service Examination (Logic Test) primarily includes IQ test questions, logic games (e.g., Mate-in-one), and other tasks highly related to logical reasoning. 3) Synthetically generated questions. Some visual reasoning problems, such as Number Bridge, Sudoku, and mazes, can be generated based on specific rules. We develop code to produce a wide variety of such logic puzzles, covering different types and a range of difficulty levels. 4) Questions from existing benchmarks. We sample 80 questions from PuzzleVQA (Chia et al., 2024) and 100 questions from MMIQ (Cai et al., 2025), excluding questions based on shape size identification, as such questions may not effectively assess the model's reasoning ability. 5) Self-designed questions. We mainly construct questions related to spatial and temporal reasoning. The spatial reasoning</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Evaluation of rule-based questions.
questions involve tasks such as determining relative spatial relationships and navigation, with the question design methodology inspired by VSIBench (Yang et al., 2024b). For temporal reasoning, the questions mainly focus on sequence judgment. We sample frames from videos in YouCook2 (Zhou et al., 2018) and VideoMME (Fu et al., 2024) as the sources of images. Note that for questions with well-defined rules such as Number Bridge Puzzles, we include the corresponding rules as part of each question. The composition of MME-Reasoning is shown in Fig. 4 and please refer to the Appendix for more details about the question source and type.</p>
<p>Data Curation. We initially collect around 4 k questions from various sources mentioned above. Following the design principles of MME-Reasoning, we conduct a careful manual curation process to ensure the quality of the benchmark. Specifically, we exclude questions that depend solely on visual recognition, require complex domain-specific knowledge, too easy to evaluate the reasoning ability. This curation process ensures that the remaining questions are well-aligned with our goal of evaluating visual reasoning ability, rather than perceptual skills or the breadth of specialized knowledge. For questions with multiple possible answers, we first try to convert them into rule-based (will be introduced in Sec. 3.3) or multiple-choice questions; otherwise, discard them. Additionally, we remove questions that place excessive demands on instruction-following ability. Finally, to comprehensively evaluate the multimodal reasoning ability, we balance the distribution of questions across the three reasoning types. This approach prevents the benchmark from being overly biased towards evaluating the ability of any single reasoning type. Through this data curation process, we filter 1,008 questions from the initially collected questions.</p>
<p>Metadata Annotation. Further, we annotate questions in MME-Reasoning with information including question type (i.e., multiple-choice, free-form, and rule-based), difficulty (i.e., easy, medium, hard), capability (i.e., pattern analysis, planning and exploring, spatial and temporal, calculation, casual chain analysis), and reasoning type (i.e., deductive, inductive, and abductive). For specific rules for annotating metadata, please refer to our appendix.</p>
<h1>3.3 Evaluation Protocols</h1>
<p>Following MathVista (Lu et al., 2023), the evaluation consists of two steps: extracting answers and judging answers. For different types of questions (i.e., multiple-choice, free-form, and rule-based), we designed specific prompts for GPT to extract answers. These prompts are composed of extraction rules and examples that are similar to MathVista (Lu et al., 2023). For multiple-choice questions, we match the extracted answers with the reference answers. For free-form questions, we use GPT to judge the consistency between the extracted answers and the reference answers following MathVerse (Zhang et al., 2024). For rule-based questions, we first use GPT to extract answers and</p>
<p>convert them into an intermediate format, which is then judged using specific scripts. For example, in a Number Bridge problem, we first use GPT to extract the start and end points of each bridge, then convert the answers into a specific matrix format, and finally determine correctness based on predefined rules, as illustrated in Fig. 5.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experimental Settings</h3>
<p>We conduct extensive evaluations on state-of-the-art MLLMs include:
Thinking Models. We first evaluate several thinking MLLMs that focus on improving the models' multimodal reasoning which can be divided into Close-source models including (1) GPT-o1 (Jaech et al., 2024), and o4-mini (, 2025); (2) Gemini-2.5-Flash-Thinking and Gemini-2.5-Pro-Thinking (Gemini et al., 2023); (3) Claude-3.7-Sonnet-Thinking, Claude-4-Sonnet-Thinking (Anthropic, 2022); (4) Seed1.5-VL-Thinking (Guo et al., 2025a); and Open-source models including (1) QvQ-72BPreview (Team, 2024); (2) Kimi-VL-A3B-Thinking (Team et al., 2025b); (3)LlamaV-o1 (Thawakar et al., 2025); (4) Virgo-72B (Du et al., 2025).
Chat Models. Further, we also evaluate SoTA chat models as follows. Close-source models: (1) GPT-4o (OpenAI, 2024); (2) Claude-3.7-Sonnet (Anthropic, 2022) (3) Kimi-latest (Team et al., 2025a); (4) Seed1.5-VL (Guo et al., 2025a). Open-source models: (1) Qwen-2.5-VL (7B, 32B, 72B) (Qwen Team, 2025a); (2) InternVL-3 (8B, 38B, 78B) (Zhu et al., 2025); (3) LLaVA-Onevision-72B (Li et al., 2024); (4) Molmo (7B-O, 7B-D, 72B) (Deitke et al., 2024); (5) Kimi-VL-A3B-Instruct (Team et al., 2025b).</p>
<p>Rule based RL Models. Rule-based Reinforcement Learning (RL) has been shown to be a highly promising strategy for eliciting reasoning paradigms in models. Therefore, we further evaluated MLLMs trained using Rule-based RL, including: (1) R1-VL (Zhang et al., 2025a), (2) R1Onevision (Yang et al., 2025), (3) Vision-R1 (Huang et al., 2025), (4) MM-Eureka (7B, 32B) (Meng et al., 2025), (5) VL-Rethinker (7B, 72B) (Wang et al., 2025).
We use GPT-4o-mini to extract answers from model responses. Due to rate limits, we sample 302 questions to construct mini-set with the same distribution for o1's evaluation, all other models are evaluated on the entire benchmark.</p>
<h3>4.2 Main Results</h3>
<p>Tab. 3 shows the performance comparison of different MLLMs and prompting strategies.
MME-Reasoning poses significant challenges for vision-language reasoning. The best-performing model, Gemini-2.5-Pro-Thinking, achieved an average score of $60.2 \%$. The latest MLLM, Seed1.5-VL, achieved a comprehensive score of 59.9. Representative reasoning models o4-mini and o1 obtained scores of 57.5 and 45.7, respectively. Qwen2.5-VL and Claude-3.7-Sonnet achieved scores of 35.9 and 57.2 on OlympiadBench, yet only reached 34.1 on MME-Reasoning. These results indicate that the benchmark sets stringent standards for evaluating models' logical reasoning capabilities by comprehensively assessing three distinct reasoning types.
Prominent bias in logical reasoning performance within MLLMs. In almost all cases, models exhibit dominant deductive reasoning performance, while abductive reasoning is considerably weaker. Closed-source models demonstrate an average deductive advantage of 5.38 over abductive reasoning, which widens to 9.81 among open-source models, making abductive reasoning</p>
<p>Table 3: Performance comparison of state-of-the-art MLLMs on MME-Reasoning. The top three are highlighted in blue. $\dagger$ indicates the model was evaluated on the mini-set. "T" represents "Thinking".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model Capability</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reasoning Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AVG.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAL.</td>
<td style="text-align: center;">P\&amp; E.</td>
<td style="text-align: center;">PA.</td>
<td style="text-align: center;">S\&amp;T. CCA.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DED.</td>
<td style="text-align: center;">IND.</td>
<td style="text-align: center;">ABD.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Close-source $\mathcal{E}$ Thinking</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gemini-2.5-Pro-T</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Seed1.5-VL-T</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">59.9</td>
</tr>
<tr>
<td style="text-align: center;">o4-mini</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">o1 ${ }^{\dagger}$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: center;">Claude-4-Sonnet-T</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.7-Sonnet-T</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-2.5-Flash-T</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">Close-source $\mathcal{E}$ Chat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Seed1.5-VL</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">30.2</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.7-Sonnet</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-Latest</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: center;">Open-source $\mathcal{E}$ Thinking</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QVQ-72B-Preview</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">Virgo-72B</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;">VL-Rethinker-72B</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">VL-Rethinker-7B</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;">MM-Eureka-Qwen-32B</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: center;">MM-Eureka-Qwen-7B</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;">R1-VL-7B</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: center;">Vision-R1-7B</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: center;">R1-Onevision-7B-RL</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">22.5</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-VL-A3B-T</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">25.9</td>
</tr>
<tr>
<td style="text-align: center;">Open-source $\mathcal{E}$ Chat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-72B</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-32B</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-7B</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-78B</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-38B</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-8B</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;">Molmo-72B</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">18.9</td>
</tr>
<tr>
<td style="text-align: center;">Molmo-7B-D</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-OV-72B</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">25.8</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-VL-A3B</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">23.1</td>
</tr>
</tbody>
</table>
<p>a significant bottleneck in comprehensive logical reasoning performance. Deductive reasoning maintains a high proportion in the training corpus due to its widespread distribution. Abductive reasoning processes usually involve larger exploration spaces and richer assumptions, hypotheses, and reflections, making its data challenging to scale. However, non-deductive reasoning plays a central role in general reasoning scenarios and many scientific discoveries. These findings highlight</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of Difficulty Level and Average Token Count on MME-Reasoning.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Results within different difficulty levels.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Response tokens vs. Performance.
the necessity for researchers to develop a more comprehensive understanding of models' logical reasoning abilities to facilitate their application in real-world scenarios. Moreover, the models' scores under different reasoning types are typically score below 40, indicating that MME-Reasoning provides a promising metric for evaluating reasoning capabilities from multiple perspectives.
Limited performance in open-ended reasoning scenarios. Models generally demonstrate relative advantages in Casual Chain Analysis but perform poorly on tasks involving Plan \&amp; Exploration. This may benefit from the autoregressive paradigm continuously aiding models in learning causal dependencies within input sequences. However, it also highlights a critical shortcoming: current state-of-the-art models struggle with planning and exploration in open-ended problem-solving spaces. To advance models in solving difficult practical problems, it is critical to innovate learning paradigms and strategies generation mechanisms suitable for open scenarios.
Thinking capability directly contributes to enhanced logical reasoning. Models employing "thinking mode" typically generalize test-time scaling to reasoning scenarios through generating longer chains-of-thought (CoT), reflections, and self-corrections. In most cases, "thinking models" significantly outperform their base version. QvQ improved by 1.1 compared to Qwen2.5-VL, and VL-Rethinker improved by 1.7 compared to Qwen2.5-VL. This effect is more pronounced among closed-source models: Seed1.5-VL-T outperformed Seed1.5-VL by 12.4, and ol exceeded GPT-4o by 15.5. Further experiments concerning thinking models will be elaborated in subsequent sections.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Case study of a Mate-in-one problem.
Rule-based RL does not always work. Rule-based RL has shown significant potential in activating the "thinking mode" of foundational models, encouraging longer output and reflection to tackle hard problems. However, we observed that methods adopting rule-based RL do not consistently outperform their base models. Most models at the 7B scale experienced performance degradation. This suggests that the potential of rule-based RL remains inadequately realized, failing to effectively extend advantages demonstrated in LLMs into multimodal domains and possibly reducing generalization. Thus, innovation in training paradigms, rather than merely replicating R1, is urgently needed.</p>
<h1>4.3 Fine Grained Analysis of Reasoning Behavior</h1>
<p>Does increasing the length of the reasoning process help? To investigate whether increased output length consistently leads to improved accuracy, we selected 10 representative models, including Chat Models (e.g., GPT-4o) and Thinking Models (e.g., o4-mini). In Fig. 8, we present the semi-log plot of average token count (ATC) versus accuracy. The overall trend reveals that models with longer outputs tend to achieve higher scores, indicating the effectiveness of extending the reasoning process to enhance logical reasoning performance. As the token number increases, model performance exhibits a exponential growth pattern, suggesting diminishing returns from simply increasing output length. Compared to Thinking Models, Chat Models demonstrate higher token efficiency. These findings highlight the computational cost associated with scaling up inference for improved performance. Balancing reasoning efficiency and model effectiveness remains a challenge for future research.</p>
<p>Is the length of the reasoning process strongly correlated with task difficulty? To examine whether models spontaneously allocate more inference budget to more challenging questions, we conducted research on using representative Thinking Models such as o4-mini and Chat Models such as GPT-4o. We first analyzed the accuracy of different models across varying levels of difficulty, as shown in Fig. 7. With increasing difficulty, model performance declines significantly, confirming the validity of MME-Reasoning's difficulty stratification and providing a foundation for subsequent analyses. Besides, Fig. 6 illustrates the trend of ATC across different reasoning types and difficulty levels. It reveals a consistent pattern: overall, output length increases steadily with rising difficulty. This trend holds across varying output lengths, model categories, and reasoning types. Compared</p>
<p>to Chat Models, Thinking Models exhibit a more pronounced increase in ATC as difficulty rises. For instance, the ATC of Seed1.5-VL increases by up to 3 k tokens, and o 4 -mini by up to 5 k tokens. In contrast, the ATC increase for Qwen2.5-VL and GPT-4o remains within 300 tokens.</p>
<h1>4.4 Case Study</h1>
<p>In Fig. 9, we present an example of abductive reasoning which demands planning and exploration. From this case, several key observations can be identified: (1)Long reasoning process: The selected models generated over 1 k tokens in response, with o 4 -mini producing up to 24.6 k tokens. This demonstrates that MME-Reasoning constitutes a highly challenging benchmark for multimodal reasoning. (2)Planning in the problem-solving process: The response includes multiple iterations of "hypothesis generation (possible movement) - feasibility verification (check escape squares) - check ", indicating that the model spontaneously engages in structured planning and reflection to explore solutions within an open-ended problem-solving spaces. (3)Repetitive reflection: We observed that the model tends to revisit and reflect on the same reasoning paths multiple times-up to 7 instances in some cases. This behavior may result in significant computational overhead and informational redundancy. Balancing reasoning efficiency with performance remains a critical issue to be addressed.</p>
<h2>5 Conclusion</h2>
<p>We introduce MME-Reasoning, a comprehensive benchmark designed to evaluate MLLMs' logical reasoning abilities across inductive, deductive, and abductive reasoning types. Through careful data curation and an expanded evaluation protocol, our benchmark provides a holistic assessment of reasoning capabilities, beyond simple perception or high-level knowledge. Our experiments reveal that existing MLLMs still face significant challenges and exhibit notable performance imbalances across different reasoning types. These findings underscore the need for further research and development to enhance the reasoning abilities of MLLMs, paving the way for more generalizable AI systems.</p>
<h1>References</h1>
<p>OpenAI (2025). Openai o3 and o4-mini system card, 2025. URL https://openai.com/index/ o3-o4-mini-system-card/.
https://www.anthropic.com/index/introducing-claude Anthropic. Claude, 2022. URL https: //www.anthropic.com/index/introducing-claude.</p>
<p>Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021.</p>
<p>Huanqia Cai, Yijun Yang, and Winston Hu. Mm-iq: Benchmarking human-like abstraction and reasoning in multimodal models. arXiv preprint arXiv:2502.00698, 2025.</p>
<p>Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than \$3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02.</p>
<p>Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv preprint arXiv:2301.05226, 2023.</p>
<p>Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024.</p>
<p>Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5828-5839, 2017.</p>
<p>DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948.</p>
<p>Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024.</p>
<p>Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025.</p>
<p>Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Virgo: A preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783.</p>
<p>Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025.</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.</p>
<p>Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024.</p>
<p>Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https://arxiv.org/abs/2312. 11805 .</p>
<p>Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, et al. Av-odyssey bench: Can your multimodal llms really understand audio-visual information? arXiv preprint arXiv:2412.02611, 2024.</p>
<p>Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025a.</p>
<p>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, and Pheng-Ann Heng. Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on multi-modal scientific problems. arXiv preprint arXiv:2503.10627, 2025b.</p>
<p>Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025.</p>
<p>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024.</p>
<p>Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024.</p>
<p>Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025.</p>
<p>Drew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 6700-6709. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019. 00686. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_ Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.</p>
<p>Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency, 2025. URL https://arxiv.org/abs/2502.09621.</p>
<p>Yilei Jiang, Yingshui Tan, and Xiangyu Yue. Rapguard: Safeguarding multimodal large language models via rationale-aware defensive prompting, 2024. URL https://arxiv.org/abs/2412.18826.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2016. URL https://arxiv.org/abs/1612.06890.</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024.</p>
<p>Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, and Yue Zhang. Logical reasoning in large language models: A survey. arXiv preprint arXiv:2502.09100, 2025a.</p>
<p>Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025b.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: / /openreview.net/forum?id=KUNzEQMWU7.</p>
<p>Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, et al. Omnicaptioner: One captioner to rule them all. arXiv preprint arXiv:2504.07089, 2025.</p>
<p>Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning, 2025. URL https://github.com/ModalMinds/MM-EUREKA.</p>
<p>OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. URL https://openai.com/ index/hello-gpt-4o/.</p>
<p>Charles Sanders Peirce. Illustrations of the Logic of Science. Open Court, 2014.
Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, et al. Chimera: Improving generalist model with domainspecific experts. arXiv preprint arXiv:2412.05983, 2024.</p>
<p>Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025a.</p>
<p>Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025b.
Qwen Team. Qwen2.5-vl, January 2025a. URL https://qwenlm.github.io/blog/qwen2.5-vl/.
Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof qa benchmark, 2023. URL https://arxiv.org/abs/2311.12022.</p>
<p>Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: A stable and generalizable r1-style large vision-language model, 2025. URL https://arxiv.org/abs/2504.07615.
Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025.
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025a.
Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025b.
Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/ blog/qvq-72b-preview/.
Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025.
Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025.
Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:95095-95169, 2024a.
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, et al. Geox: Geometric problem solving through unified formalized vision-language pre-training. arXiv preprint arXiv:2412.11863, 2024a.</p>
<p>Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, et al. Chartx \&amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024b.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024a. URL https://arxiv.org/abs/2407.10671.</p>
<p>Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024b.</p>
<p>Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025.</p>
<p>Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, and Xian Sun. Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals. arXiv preprint arXiv:2308.06207, 2023.</p>
<p>Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024.</p>
<p>Peng Yingzhe, Zhang Gongrui, Zhang Miaosen, You Zhiyuan, Liu Jie, Zhu Qipeng, Yang Kai, Xu Xingzhong, Geng Xin, and Yang Xu. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024a.</p>
<p>Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b.</p>
<p>Liu Yuqi, Peng Bohao, Zhong Zhisheng, Yue Zihao, Lu Fanbin, Yu Bei, and Jia Jiaya. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement, 2025. URL https://arxiv.org/ abs/2503.06520.</p>
<p>Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025a.</p>
<p>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169-186. Springer, 2024.</p>
<p>Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Puzzlebench: A fully dynamic evaluation framework for large multimodal models on puzzle solving. arXiv preprint arXiv:2504.10885, 2025b.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023.</p>
<p>Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning, 2025.</p>
<p>Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, and Qing Li. A picture is worth a graph: A blueprint debate paradigm for multimodal reasoning. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 419-428, 2024.</p>
<p>Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zero's "aha moment" in visual reasoning on a 2b non-sft model, 2025. URL https://arxiv.org/ abs/2503.05132.</p>
<p>Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018 .</p>
<p>Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.</p>
<h1>Technical Appendices and Supplementary Material for MME-Reasoning</h1>
<p>A More Experimental Results ..... 20
A. 1 Full Results on MME-Reasoning ..... 20
A. 2 Full Results on Mini-set of MME-Reasoning ..... 20
A. 3 Human Performance ..... 20
A. 4 Results on Different Question Types ..... 20
A. 5 Results with Test-Time Compute Scaling ..... 20
A. 6 Results with CoT Prompt ..... 22
A. 7 Token Usage of Thinking Models ..... 22
A. 8 Results of Captioner \&amp; LLMs ..... 25
B Details of Annotation ..... 25
B. 1 Difficult Annotation ..... 25
B. 2 Reasoning Type Annotation ..... 27
B. 3 Capability Annotation ..... 27
C Details of Implementation ..... 27
D Details of Evaluation ..... 28
D. 1 Prompts for Answer Extraction ..... 28
E Examples of MME-Reasoning ..... 28
F Limitation ..... 28</p>
<h1>A More Experimental Results</h1>
<h2>A. 1 Full Results on MME-Reasoning</h2>
<p>We present the performance of more baselines on MME-Reasoning in Tab 4, including OpenVLThinker (Deng et al., 2025), LMM-R1-MGT-PerceReason (Peng et al., 2025a), Mulberry (Yao et al., 2024), LlamaV-o1 (Thawakar et al., 2025) and Qwen2-VL series (Wang et al., 2024b).</p>
<h2>A. 2 Full Results on Mini-set of MME-Reasoning</h2>
<p>We randomly sampled $25 \%$ of the questions and conducted manual review to ensure that the diversity of image types was maintained. These sampled questions were then used to to construct the Mini-set. We also analyzed the question distributions of both the Mini-set and the Full-set to ensure the sampled questions retained the same distribution. The statistical results are presented in Tab 5 .</p>
<p>We provide the performance of all baseline models on the Mini-set in Tab. 6. All baseline models achieved similar performance on both the Full-set and the Mini-set, further demonstrating the consistency of Mini-set and the comparability of model performance across different splits.</p>
<h2>A. 3 Human Performance</h2>
<p>To evaluate expert-level performance on MME-Reasoning, we further report human performance on the mini-set of MME-Reasoning. As shown in Tab. 6, the human expert achieved an overall score of 83.4 -significantly outperforming the best-performing thinking model, Seed1.5-VL-T, which scored 62.6. Looking deeper into the reasoning types, the human expert scored 85.8, 76.9, and 85.6 on deductive, inductive, and abductive reasoning respectively, all of which are notably higher than the scores of the best-performing model. Moreover, the human expert demonstrated a particularly strong ability in abductive reasoning, with performance comparable to that in deductive reasoning-which is the key focus in current multi-modal reasoning research. This strength aligns with a few top-performing models, but stands in contrast to most baseline models, which show clear weaknesses in abductive reasoning. These results highlight the significant gap that still exists between current thinking \&amp; chat models and human-level performance in comprehensive multimodal reasoning evaluation. Expanding complex reasoning tasks beyond domain-specific knowledge questions to include a broader range of reasoning types and more diverse tasks will be a crucial step toward addressing these current limitations.</p>
<h2>A. 4 Results on Different Question Types</h2>
<p>We also evaluated the model's performance across different question types and present the results in Tab. 7.</p>
<h2>A. 5 Results with Test-Time Compute Scaling</h2>
<p>To evaluate whether the use of Test-Time Compute Scaling (TTS) methods can improve model performance on MME-Reasoning, we take Qwen2.5-VL-7B as an example and use Qwen2.5-VL-32B as the Reward Model. The evaluation is conducted using the Monte Carlo Tree Search (MCTS) algorithm, with the settings: branch $=3$ and max-iteration $=18$. The results are shown in Table 8.</p>
<p>Under the MCTS-based setting, the model's performance dropped noticeably across all reasoning types. We attribute this decline to two main factors: (1) Questions in MME-Reasoning often involve</p>
<p>Table 4: Performance comparison of state-of-the-art MLLMs on MME-Reasoning. The top three are highlighted in blue. " T " represents "Thinking".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model Capability</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reasoning Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AVG.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAL.</td>
<td style="text-align: center;">P\&amp; E.</td>
<td style="text-align: center;">PA.</td>
<td style="text-align: center;">S\&amp;T.</td>
<td style="text-align: center;">CCA.</td>
<td style="text-align: center;">DED.</td>
<td style="text-align: center;">IND.</td>
<td style="text-align: center;">ABD.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Close-source $\mathcal{E}$ Thinking</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gemini-2.5-Pro-T</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Seed1.5-VL-T</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">59.9</td>
</tr>
<tr>
<td style="text-align: center;">o4-mini</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">Claude-4-Sonnet-T</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.7-Sonnet-T</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-2.5-Flash-T</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">Close-source $\mathcal{E}$ Chat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Seed1.5-VL</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">30.2</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.7-Sonnet</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-Latest</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: center;">Open-source $\mathcal{E}$ Thinking</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QVQ-72B-Preview</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">Virgo-72B</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;">VL-Rethinker-72B</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">VL-Rethinker-7B</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;">MM-Eureka-Qwen-32B</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: center;">MM-Eureka-Qwen-7B</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;">R1-VL-7B</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: center;">Vision-R1-7B</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: center;">R1-Onevision-7B-RL</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">22.5</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-VL-A3B-T</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">25.9</td>
</tr>
<tr>
<td style="text-align: center;">OpenVLThinker-7B</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">24.6</td>
</tr>
<tr>
<td style="text-align: center;">LMM-R1-MGT-PerceReason</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;">Mulberry</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: center;">LlamaV-o1</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;">Open-source $\mathcal{E}$ Chat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-72B</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-32B</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-7B</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-VL-3B</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-VL-72B</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-VL-7B</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-VL-2B</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-78B</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-38B</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: center;">InternVL3-8B</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;">Molmo-72B</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">18.9</td>
</tr>
<tr>
<td style="text-align: center;">Molmo-7B-D</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;">Molmo-7B-O</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">13.4</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-OV-72B</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">25.8</td>
</tr>
<tr>
<td style="text-align: center;">Kimi-VL-A3B</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">23.1</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{+}$Equal contribution, ${ }^{\dagger}$ Corresponding authors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
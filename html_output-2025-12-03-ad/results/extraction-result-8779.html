<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8779 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8779</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8779</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b99c61f6957c1b04ec1376b74f82dd1e83559695</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b99c61f6957c1b04ec1376b74f82dd1e83559695" target="_blank">JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> A graph-text joint representation learning model called JointGT is proposed which devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure during encoding and shows new state-of-the-art performance on various KG-to-text datasets.</p>
                <p><strong>Paper Abstract:</strong> Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8779.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8779.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (triple list)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple-list linearization of knowledge graphs (serialization with <H>, <R>, <T> tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A serialization of a knowledge graph into a token sequence by listing its RDF-style triples and marking head, relation and tail tokens with special markers (e.g., <H>, <R>, <T>) so that text-to-text pretrained models can consume graphs as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / triple-list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert a KG into a flat token sequence by enumerating triples and surrounding each element with special tokens indicating head (<H>), relation (<R>) and tail (<T>); optionally merge consecutive mask tokens; a <SEP> token separates linearized graph and text during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples (Wikidata-derived graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Enumerate triples and emit tokens in order: <H> head_entity_tokens <R> relation_tokens <T> tail_entity_tokens, repeating for each triple to form G_linear; place <SEP> between graph tokens and textual sequence when jointly encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (data-to-text), question generation over KGs (KBQG) — WebNLG, WebQuestions, PathQuestions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used with JointGT (BART): WebNLG(Unconstrained) BLEU 65.92, METEOR 47.15, ROUGE-L 76.10; JointGT (T5) BLEU 66.14, METEOR 47.25, ROUGE-L 75.91. Baseline BART (linearized input) BLEU 64.55, METEOR 46.51, ROUGE-L 75.13 (WebNLG(U)). See Table 1 for other dataset numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly fine-tuning text-to-text models on linearized graphs is standard and used as baseline (BART/T5). JointGT augments linearized input with a structure-aware encoder and graph-text pretraining to outperform vanilla linearization+BART/T5. SeqEnc (plain sequence encoder on the same linearization) performs worse than JointGT in ablation (BLEU drop vs JointGT: from 65.92 to 64.82 on WebNLG(U)).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, compatible with existing text-to-text pretrained models (BART/T5), enables leveraging large pretrained language models and large text corpora; straightforward to implement and scale to long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can lose explicit graph structure (structural information loss) because plain bidirectional Transformer full-attention ignores explicit relations between entities; long linearized sequences (max input length 600) can be large and may reduce locality of structural signals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When used without structure-aware encoding, models often miss triples or produce unfaithful facts (case study: BART/T5 on WebNLG missed triples or hallucinated relations). Performance degrades as graph structural complexity (number of triples) increases if structure is not preserved explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8779.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeqEnc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence encoder over linearized graphs (SeqEnc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based encoder that takes the linearized triple sequence as plain text and ignores explicit graph structure, used as a baseline encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequence encoder on linearized graph (SeqEnc)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat the serialized triple list as a pure token sequence and encode via vanilla Transformer encoder (full self-attention) without additional structure-aware layers or specialized relation embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (linearized RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Same triple-list linearization; no additional conversion beyond tokenization and BPE/WordPiece; feed the token sequence directly into Transformer encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (WebNLG etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation on WebNLG(U): SeqEnc variant yields BLEU 64.82, METEOR 46.87, ROUGE-L 75.37 compared to JointGT (BART) BLEU 65.92 / METEOR 47.15 / ROUGE-L 76.10 (Table 4). Also lower BLEU on graphs with many triples (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs worse than JointGT's structure-aware encoder; slightly worse than RelEnc. Demonstrates structural information loss relative to methods that inject structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and directly compatible with pretrained text encoders; no extra parameters for relation embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores graph topology, which leads to worse generation quality especially when input graphs have many triples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower BLEU and missing coverage of triples in generated text, particularly on inputs with larger numbers of triples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8779.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelEnc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-aware encoder (RelEnc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer encoder variant that incorporates relation embeddings into self-attention, using learned entity and relation embeddings as additional inputs to preserve structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>relation-aware Transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode entities as a sequence and inject relation embeddings into the attention mechanism (e.g., relative position or relation vectors) where entity and relation vectors are directly learned as model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs with entities and pairwise relations</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Start from entity sequence (derived from triples); utilize relation presence/embeddings associated with entity pairs during attention calculation; still typically based on the linearized ordering of entities.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation on WebNLG(U): RelEnc variant yields BLEU 65.17, METEOR 47.07, ROUGE-L 75.69 compared to JointGT (BART) BLEU 65.92 / METEOR 47.15 / ROUGE-L 76.10 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs better than bare SeqEnc but slightly worse than JointGT's structure-aware semantic aggregation; unlike RelEnc, JointGT does not learn separate static entity/relation embeddings but pools contextual representations at each layer.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models relations in attention, better preserves structure than pure sequence encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Entity and relation embeddings are learned as static parameters, which may reduce generalization to unseen entities/relations and misses leveraging contextual pretrained representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Slight underperformance vs JointGT when generalizing across datasets with different KGs; smaller improvements when number of triples increases compared to JointGT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8779.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structure-Aware Semantic Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-aware semantic aggregation module (JointGT encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module inserted into each Transformer encoder layer that pools token-level contextual vectors into entity/relation vectors and performs structure-aware self-attention over entities to inject graph topology into contextual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structure-aware semantic aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each Transformer layer: (1) mean-pool token outputs corresponding to each entity/relation in the linearized input to produce z_i (entities) and q_ij (relations); (2) perform a structure-aware self-attention among entity vectors where relation vectors q_ij are incorporated into key/value computations; (3) residual fuse the aggregated structural vectors back into token-level hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entities and relations from RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Input remains triple-list linearization; structural module identifies positions of each entity/relation in the token sequence via position sets P(e_i) and P(r_ij), pools their token vectors, and aggregates using graph-aware attention to produce structural encodings that are injected back into the token sequence encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (WebNLG, WebQuestions, PathQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation shows its use yields JointGT (BART) BLEU 65.92 vs SeqEnc BLEU 64.82 and RelEnc BLEU 65.17 on WebNLG(U) (Table 4); greater performance margin when number of triples is large (Table 5). Also human eval shows improved adequacy and fluency vs BART/T5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms SeqEnc and RelEnc in ablation; unlike RelEnc, entity/relation vectors are derived from contextual representations at each layer rather than static learned embeddings, improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves graph structure while leveraging pretrained contextual token representations; better generalization to unseen entities/relations; larger improvements on inputs with many triples; compatible with pretrained encoder checkpoints (BART/T5).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds extra parameters and computation per Transformer layer (additional pooling and structure-aware attention) and requires tracking token-to-entity/relation position maps; slightly more complex than plain sequence encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported catastrophic failures, but authors note encoders incompatible with BART/T5 (e.g., arbitrarily swapping in other graph encoders requiring random initialization like GNNs) can lead to performance drops if not pretrained jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8779.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-Enhanced Text Reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-enhanced text reconstruction pretraining task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining objective that reconstructs masked text conditioned on the complete linearized knowledge graph so the decoder learns to generate text grounded in graph content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-enhanced text reconstruction (conditional LM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Mask words in the text (entities masked at 40%, other words 20%, merge consecutive masks) and train the decoder to generate the original text X conditioned on the complete graph G and masked text ̂X via maximum likelihood: L_text = -log P(X | G, ̂X).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs paired with textual descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Uses the linearized triple-list as encoder input (complete graph) and a corrupted text as decoder input; no additional conversion beyond tokenization and masking strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation; improves decoder grounding to graph content</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation removing text reconstruction drops JointGT (BART) WebNLG(U) BLEU from 65.92 to 64.22 (Table 6), indicating a substantial contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to using BART's original pretraining or KGPT pretraining, replacing with this task improves few-shot and full-data performance; graph reconstruction and OT tasks also provide complementary gains.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly teaches decoder to condition generation on graph content, yields largest single-task impact among the three JointGT pretraining tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires graph-text paired pretraining data and careful masking strategy; concentrated emphasis on generating observed text may limit generalization to diverse surface realizations without sufficient data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted that without this task, model adequacy and BLEU drop; no explicit failure cases beyond lower performance in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8779.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Enhanced Graph Reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-enhanced graph reconstruction pretraining task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining objective that reconstructs masked entities and relations in a corrupted linearized graph conditioned on the complete text, thereby teaching the encoder to rely on textual signals to recover graph elements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text-enhanced graph reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Mask entities/relations in the linearized graph (entity masking 40%, relation masking 20%) to produce ̃G and train the encoder/decoder to predict the original graph tokens w_i given ̃G and complete text X via maximum likelihood: L_graph = -sum_i M_i log P(w_i | ̃G, X).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (linearized triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Same triple-list linearization; randomly mask positions corresponding to entities/relations and predict tokens from decoder conditioned on full text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation, implicitly also text-to-graph learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation removing graph reconstruction yields JointGT (BART) BLEU 65.37 vs full 65.92 (Table 6); modest but measurable contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Complementary to graph-enhanced text reconstruction and OT alignment; performs better than using only BART pretraining tasks or KGPT pretraining when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encourages encoder to attend to text-relevant entities/relations and strengthens bidirectional graph-text associations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on availability of paired graph-text data and careful masking; contributes less than graph->text reconstruction to final BLEU in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not highlighted as causing specific failure cases, only smaller performance drop relative to removing text recon task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8779.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-Text OT Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-text embedding alignment via Optimal Transport (OT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-space pretraining task that aligns encoder-side graph embeddings with decoder-side text embeddings by minimizing an OT distance (approx. via IPOT) using cosine-based costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-text embedding alignment (Optimal Transport)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat sequence of entity/relation embeddings and decoder token embeddings as discrete distributions; compute an OT distance using cosine-cost d(g_i, x_j)=1 - cosine(h_i^G, s_j) and approximate the transport plan T with the IPOT algorithm; use the OT distance as a loss L_OT to pull graph and text embeddings into alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity and relation embeddings pooled from linearized input)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Pool token-level encoder hidden states into entity/relation vectors; collect decoder final hidden states as text embeddings; define distributions and compute approximate OT via IPOT with hyperparameters β=1.0, K=1, N=10.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation; promotes alignment in continuous latent space between graphs and text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation removing OT yields JointGT (BART) BLEU 65.03 vs full 65.92 (Table 6), indicating a contribution complementary to discrete reconstruction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Provides alignment in embedding space complementary to discrete reconstruction tasks; comparing to BART/KGPT pretraining, the OT-based alignment is more beneficial when combined with graph/text reconstruction in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Models graph-text correspondences in continuous embedding space; can capture soft many-to-many mappings between graph elements and textual tokens; helps generalization and few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Computationally more intensive due to OT approximation (IPOT iterations) and additional memory for cost/transport matrices; introduces hyperparameters (β, K, N).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported explicit failure cases, but ablation shows removing OT reduces performance modestly; computing OT is approximate and may be costly on very large graphs or long texts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8779.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8779.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGPT (baseline/pretrain)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGPT: Knowledge-grounded pretraining for data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pre-trained model that uses KG-to-text generation as a pretraining objective on large graph-text corpora; used as a competitive baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KGPT: knowledge-grounded pretraining for data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG-to-text pretraining on linearized KGs (KGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Pretrain by directly using KG-to-text generation objective on a large set of graph-text pairs (linearized triples as input, text as output) to teach models to map graphs to natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (linearized RDF triples from WikiData/Wiki text)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Likely uses triple-list linearization as input to the pretraining encoder-decoder generation objective (as described in Chen et al., 2020b referenced by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation, KG-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KGPT baseline reported BLEU 64.11, METEOR 46.30, ROUGE-L 74.57 on WebNLG(U) (Table 1, reprinted). In ablation replacing JointGT pretraining with KGPT pretraining yields JointGT (BART) w/ KGPTPretrain BLEU 65.14 vs JointGT full 65.92 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>KGPT's straightforward KG-to-text pretraining is competitive but JointGT's combination of structure-aware encoder + three pretraining tasks (graph/text recon + OT) yields better downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly optimizes KG->text mapping during pretraining, effective baseline and strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not explicitly incorporate continuous embedding alignment (OT) or the per-layer structure-aware aggregation introduced in JointGT; may be less effective at capturing fine-grained graph-text alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly reported here; in this paper, KGPT pretraining is slightly less effective than JointGT's combined pretraining tasks when used with the proposed encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KGPT: knowledge-grounded pretraining for data-to-text generation <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Text-to-text pre-training for data-to-text tasks <em>(Rating: 1)</em></li>
                <li>Modeling graph structure via relative position for better text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8779",
    "paper_id": "paper-b99c61f6957c1b04ec1376b74f82dd1e83559695",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Linearization (triple list)",
            "name_full": "Triple-list linearization of knowledge graphs (serialization with &lt;H&gt;, &lt;R&gt;, &lt;T&gt; tokens)",
            "brief_description": "A serialization of a knowledge graph into a token sequence by listing its RDF-style triples and marking head, relation and tail tokens with special markers (e.g., &lt;H&gt;, &lt;R&gt;, &lt;T&gt;) so that text-to-text pretrained models can consume graphs as text.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization / triple-list serialization",
            "representation_description": "Convert a KG into a flat token sequence by enumerating triples and surrounding each element with special tokens indicating head (&lt;H&gt;), relation (&lt;R&gt;) and tail (&lt;T&gt;); optionally merge consecutive mask tokens; a &lt;SEP&gt; token separates linearized graph and text during pretraining.",
            "graph_type": "Knowledge graphs / RDF triples (Wikidata-derived graphs)",
            "conversion_method": "Enumerate triples and emit tokens in order: &lt;H&gt; head_entity_tokens &lt;R&gt; relation_tokens &lt;T&gt; tail_entity_tokens, repeating for each triple to form G_linear; place &lt;SEP&gt; between graph tokens and textual sequence when jointly encoding.",
            "downstream_task": "KG-to-text generation (data-to-text), question generation over KGs (KBQG) — WebNLG, WebQuestions, PathQuestions",
            "performance_metrics": "Used with JointGT (BART): WebNLG(Unconstrained) BLEU 65.92, METEOR 47.15, ROUGE-L 76.10; JointGT (T5) BLEU 66.14, METEOR 47.25, ROUGE-L 75.91. Baseline BART (linearized input) BLEU 64.55, METEOR 46.51, ROUGE-L 75.13 (WebNLG(U)). See Table 1 for other dataset numbers.",
            "comparison_to_others": "Directly fine-tuning text-to-text models on linearized graphs is standard and used as baseline (BART/T5). JointGT augments linearized input with a structure-aware encoder and graph-text pretraining to outperform vanilla linearization+BART/T5. SeqEnc (plain sequence encoder on the same linearization) performs worse than JointGT in ablation (BLEU drop vs JointGT: from 65.92 to 64.82 on WebNLG(U)).",
            "advantages": "Simple, compatible with existing text-to-text pretrained models (BART/T5), enables leveraging large pretrained language models and large text corpora; straightforward to implement and scale to long sequences.",
            "disadvantages": "Linearization can lose explicit graph structure (structural information loss) because plain bidirectional Transformer full-attention ignores explicit relations between entities; long linearized sequences (max input length 600) can be large and may reduce locality of structural signals.",
            "failure_cases": "When used without structure-aware encoding, models often miss triples or produce unfaithful facts (case study: BART/T5 on WebNLG missed triples or hallucinated relations). Performance degrades as graph structural complexity (number of triples) increases if structure is not preserved explicitly.",
            "uuid": "e8779.0",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "SeqEnc",
            "name_full": "Sequence encoder over linearized graphs (SeqEnc)",
            "brief_description": "A Transformer-based encoder that takes the linearized triple sequence as plain text and ignores explicit graph structure, used as a baseline encoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "sequence encoder on linearized graph (SeqEnc)",
            "representation_description": "Treat the serialized triple list as a pure token sequence and encode via vanilla Transformer encoder (full self-attention) without additional structure-aware layers or specialized relation embeddings.",
            "graph_type": "Knowledge graphs (linearized RDF triples)",
            "conversion_method": "Same triple-list linearization; no additional conversion beyond tokenization and BPE/WordPiece; feed the token sequence directly into Transformer encoder.",
            "downstream_task": "KG-to-text generation (WebNLG etc.)",
            "performance_metrics": "Ablation on WebNLG(U): SeqEnc variant yields BLEU 64.82, METEOR 46.87, ROUGE-L 75.37 compared to JointGT (BART) BLEU 65.92 / METEOR 47.15 / ROUGE-L 76.10 (Table 4). Also lower BLEU on graphs with many triples (Table 5).",
            "comparison_to_others": "Performs worse than JointGT's structure-aware encoder; slightly worse than RelEnc. Demonstrates structural information loss relative to methods that inject structure.",
            "advantages": "Simple and directly compatible with pretrained text encoders; no extra parameters for relation embeddings.",
            "disadvantages": "Ignores graph topology, which leads to worse generation quality especially when input graphs have many triples.",
            "failure_cases": "Lower BLEU and missing coverage of triples in generated text, particularly on inputs with larger numbers of triples.",
            "uuid": "e8779.1",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "RelEnc",
            "name_full": "Relation-aware encoder (RelEnc)",
            "brief_description": "A Transformer encoder variant that incorporates relation embeddings into self-attention, using learned entity and relation embeddings as additional inputs to preserve structure.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "relation-aware Transformer encoder",
            "representation_description": "Encode entities as a sequence and inject relation embeddings into the attention mechanism (e.g., relative position or relation vectors) where entity and relation vectors are directly learned as model parameters.",
            "graph_type": "Knowledge graphs with entities and pairwise relations",
            "conversion_method": "Start from entity sequence (derived from triples); utilize relation presence/embeddings associated with entity pairs during attention calculation; still typically based on the linearized ordering of entities.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Ablation on WebNLG(U): RelEnc variant yields BLEU 65.17, METEOR 47.07, ROUGE-L 75.69 compared to JointGT (BART) BLEU 65.92 / METEOR 47.15 / ROUGE-L 76.10 (Table 4).",
            "comparison_to_others": "Performs better than bare SeqEnc but slightly worse than JointGT's structure-aware semantic aggregation; unlike RelEnc, JointGT does not learn separate static entity/relation embeddings but pools contextual representations at each layer.",
            "advantages": "Explicitly models relations in attention, better preserves structure than pure sequence encoding.",
            "disadvantages": "Entity and relation embeddings are learned as static parameters, which may reduce generalization to unseen entities/relations and misses leveraging contextual pretrained representations.",
            "failure_cases": "Slight underperformance vs JointGT when generalizing across datasets with different KGs; smaller improvements when number of triples increases compared to JointGT.",
            "uuid": "e8779.2",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Structure-Aware Semantic Aggregation",
            "name_full": "Structure-aware semantic aggregation module (JointGT encoder)",
            "brief_description": "A module inserted into each Transformer encoder layer that pools token-level contextual vectors into entity/relation vectors and performs structure-aware self-attention over entities to inject graph topology into contextual representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "structure-aware semantic aggregation",
            "representation_description": "At each Transformer layer: (1) mean-pool token outputs corresponding to each entity/relation in the linearized input to produce z_i (entities) and q_ij (relations); (2) perform a structure-aware self-attention among entity vectors where relation vectors q_ij are incorporated into key/value computations; (3) residual fuse the aggregated structural vectors back into token-level hidden states.",
            "graph_type": "Knowledge graphs (entities and relations from RDF triples)",
            "conversion_method": "Input remains triple-list linearization; structural module identifies positions of each entity/relation in the token sequence via position sets P(e_i) and P(r_ij), pools their token vectors, and aggregates using graph-aware attention to produce structural encodings that are injected back into the token sequence encoding.",
            "downstream_task": "KG-to-text generation (WebNLG, WebQuestions, PathQuestions)",
            "performance_metrics": "Ablation shows its use yields JointGT (BART) BLEU 65.92 vs SeqEnc BLEU 64.82 and RelEnc BLEU 65.17 on WebNLG(U) (Table 4); greater performance margin when number of triples is large (Table 5). Also human eval shows improved adequacy and fluency vs BART/T5.",
            "comparison_to_others": "Outperforms SeqEnc and RelEnc in ablation; unlike RelEnc, entity/relation vectors are derived from contextual representations at each layer rather than static learned embeddings, improving generalization.",
            "advantages": "Preserves graph structure while leveraging pretrained contextual token representations; better generalization to unseen entities/relations; larger improvements on inputs with many triples; compatible with pretrained encoder checkpoints (BART/T5).",
            "disadvantages": "Adds extra parameters and computation per Transformer layer (additional pooling and structure-aware attention) and requires tracking token-to-entity/relation position maps; slightly more complex than plain sequence encoding.",
            "failure_cases": "Not reported catastrophic failures, but authors note encoders incompatible with BART/T5 (e.g., arbitrarily swapping in other graph encoders requiring random initialization like GNNs) can lead to performance drops if not pretrained jointly.",
            "uuid": "e8779.3",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Graph-Enhanced Text Reconstruction",
            "name_full": "Graph-enhanced text reconstruction pretraining task",
            "brief_description": "A pretraining objective that reconstructs masked text conditioned on the complete linearized knowledge graph so the decoder learns to generate text grounded in graph content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph-enhanced text reconstruction (conditional LM)",
            "representation_description": "Mask words in the text (entities masked at 40%, other words 20%, merge consecutive masks) and train the decoder to generate the original text X conditioned on the complete graph G and masked text ̂X via maximum likelihood: L_text = -log P(X | G, ̂X).",
            "graph_type": "Knowledge graphs paired with textual descriptions",
            "conversion_method": "Uses the linearized triple-list as encoder input (complete graph) and a corrupted text as decoder input; no additional conversion beyond tokenization and masking strategies.",
            "downstream_task": "KG-to-text generation; improves decoder grounding to graph content",
            "performance_metrics": "Ablation removing text reconstruction drops JointGT (BART) WebNLG(U) BLEU from 65.92 to 64.22 (Table 6), indicating a substantial contribution.",
            "comparison_to_others": "Compared to using BART's original pretraining or KGPT pretraining, replacing with this task improves few-shot and full-data performance; graph reconstruction and OT tasks also provide complementary gains.",
            "advantages": "Directly teaches decoder to condition generation on graph content, yields largest single-task impact among the three JointGT pretraining tasks.",
            "disadvantages": "Requires graph-text paired pretraining data and careful masking strategy; concentrated emphasis on generating observed text may limit generalization to diverse surface realizations without sufficient data.",
            "failure_cases": "Noted that without this task, model adequacy and BLEU drop; no explicit failure cases beyond lower performance in ablation.",
            "uuid": "e8779.4",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Text-Enhanced Graph Reconstruction",
            "name_full": "Text-enhanced graph reconstruction pretraining task",
            "brief_description": "A pretraining objective that reconstructs masked entities and relations in a corrupted linearized graph conditioned on the complete text, thereby teaching the encoder to rely on textual signals to recover graph elements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "text-enhanced graph reconstruction",
            "representation_description": "Mask entities/relations in the linearized graph (entity masking 40%, relation masking 20%) to produce ̃G and train the encoder/decoder to predict the original graph tokens w_i given ̃G and complete text X via maximum likelihood: L_graph = -sum_i M_i log P(w_i | ̃G, X).",
            "graph_type": "Knowledge graphs (linearized triples)",
            "conversion_method": "Same triple-list linearization; randomly mask positions corresponding to entities/relations and predict tokens from decoder conditioned on full text.",
            "downstream_task": "KG-to-text generation, implicitly also text-to-graph learning",
            "performance_metrics": "Ablation removing graph reconstruction yields JointGT (BART) BLEU 65.37 vs full 65.92 (Table 6); modest but measurable contribution.",
            "comparison_to_others": "Complementary to graph-enhanced text reconstruction and OT alignment; performs better than using only BART pretraining tasks or KGPT pretraining when combined.",
            "advantages": "Encourages encoder to attend to text-relevant entities/relations and strengthens bidirectional graph-text associations.",
            "disadvantages": "Relies on availability of paired graph-text data and careful masking; contributes less than graph-&gt;text reconstruction to final BLEU in experiments.",
            "failure_cases": "Not highlighted as causing specific failure cases, only smaller performance drop relative to removing text recon task.",
            "uuid": "e8779.5",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Graph-Text OT Alignment",
            "name_full": "Graph-text embedding alignment via Optimal Transport (OT)",
            "brief_description": "A continuous-space pretraining task that aligns encoder-side graph embeddings with decoder-side text embeddings by minimizing an OT distance (approx. via IPOT) using cosine-based costs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph-text embedding alignment (Optimal Transport)",
            "representation_description": "Treat sequence of entity/relation embeddings and decoder token embeddings as discrete distributions; compute an OT distance using cosine-cost d(g_i, x_j)=1 - cosine(h_i^G, s_j) and approximate the transport plan T with the IPOT algorithm; use the OT distance as a loss L_OT to pull graph and text embeddings into alignment.",
            "graph_type": "Knowledge graphs (entity and relation embeddings pooled from linearized input)",
            "conversion_method": "Pool token-level encoder hidden states into entity/relation vectors; collect decoder final hidden states as text embeddings; define distributions and compute approximate OT via IPOT with hyperparameters β=1.0, K=1, N=10.",
            "downstream_task": "KG-to-text generation; promotes alignment in continuous latent space between graphs and text",
            "performance_metrics": "Ablation removing OT yields JointGT (BART) BLEU 65.03 vs full 65.92 (Table 6), indicating a contribution complementary to discrete reconstruction tasks.",
            "comparison_to_others": "Provides alignment in embedding space complementary to discrete reconstruction tasks; comparing to BART/KGPT pretraining, the OT-based alignment is more beneficial when combined with graph/text reconstruction in this work.",
            "advantages": "Models graph-text correspondences in continuous embedding space; can capture soft many-to-many mappings between graph elements and textual tokens; helps generalization and few-shot performance.",
            "disadvantages": "Computationally more intensive due to OT approximation (IPOT iterations) and additional memory for cost/transport matrices; introduces hyperparameters (β, K, N).",
            "failure_cases": "Not reported explicit failure cases, but ablation shows removing OT reduces performance modestly; computing OT is approximate and may be costly on very large graphs or long texts.",
            "uuid": "e8779.6",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "KGPT (baseline/pretrain)",
            "name_full": "KGPT: Knowledge-grounded pretraining for data-to-text generation",
            "brief_description": "A prior pre-trained model that uses KG-to-text generation as a pretraining objective on large graph-text corpora; used as a competitive baseline in this paper.",
            "citation_title": "KGPT: knowledge-grounded pretraining for data-to-text generation",
            "mention_or_use": "use",
            "representation_name": "KG-to-text pretraining on linearized KGs (KGPT)",
            "representation_description": "Pretrain by directly using KG-to-text generation objective on a large set of graph-text pairs (linearized triples as input, text as output) to teach models to map graphs to natural language.",
            "graph_type": "Knowledge graphs (linearized RDF triples from WikiData/Wiki text)",
            "conversion_method": "Likely uses triple-list linearization as input to the pretraining encoder-decoder generation objective (as described in Chen et al., 2020b referenced by this paper).",
            "downstream_task": "Data-to-text generation, KG-to-text",
            "performance_metrics": "KGPT baseline reported BLEU 64.11, METEOR 46.30, ROUGE-L 74.57 on WebNLG(U) (Table 1, reprinted). In ablation replacing JointGT pretraining with KGPT pretraining yields JointGT (BART) w/ KGPTPretrain BLEU 65.14 vs JointGT full 65.92 (Table 6).",
            "comparison_to_others": "KGPT's straightforward KG-to-text pretraining is competitive but JointGT's combination of structure-aware encoder + three pretraining tasks (graph/text recon + OT) yields better downstream performance.",
            "advantages": "Directly optimizes KG-&gt;text mapping during pretraining, effective baseline and strong performance.",
            "disadvantages": "Does not explicitly incorporate continuous embedding alignment (OT) or the per-layer structure-aware aggregation introduced in JointGT; may be less effective at capturing fine-grained graph-text alignments.",
            "failure_cases": "Not explicitly reported here; in this paper, KGPT pretraining is slightly less effective than JointGT's combined pretraining tasks when used with the proposed encoder.",
            "uuid": "e8779.7",
            "source_info": {
                "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KGPT: knowledge-grounded pretraining for data-to-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Text-to-text pre-training for data-to-text tasks",
            "rating": 1,
            "sanitized_title": "texttotext_pretraining_for_datatotext_tasks"
        },
        {
            "paper_title": "Modeling graph structure via relative position for better text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_better_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 1,
            "sanitized_title": "modeling_global_and_local_node_contexts_for_text_generation_from_knowledge_graphs"
        }
    ],
    "cost": 0.0163465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs</h1>
<p>Pei $\mathbf{K e}^{1}$, Haozhe $\mathbf{J i}^{1}$, Yu Ran ${ }^{2}$, Xin Cui ${ }^{2}$, Liwei Wang ${ }^{3}$, Linfeng Song ${ }^{4}$, Xiaoyan Zhu ${ }^{1}$, Minlie Huang ${ }^{1 *}$<br>${ }^{1}$ The CoAI group, Department of Computer Science and Technology, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China<br>${ }^{2}$ Sogou Inc., Beijing, China ${ }^{3}$ The Chinese University of Hong Kong ${ }^{4}$ Tencent AI Lab<br>{kp17,jhz20}@mails.tsinghua.edu.cn, {zxy-dcs,aihuang}@tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Knowledge-graph-to-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020).</p>
<p>Due to the limited amount of graph-text parallel data, it's hard for typical neural text generation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune text-to-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures.</p>
<p>Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Devlin et al., 2019), which include full attention connections. This model structure may neglect the structural information when encoding knowledge graphs since the relation between each pair of input entities is not explicitly considered (Zhu et al., 2019). 2) Absence of explicit graph-text alignments. Existing work on pre-trained models for text generation commonly adopts auto-encoding or auto-regressive text reconstruction to learn texttext alignments, which encodes the corrupted text sequence and decodes the original sequence (Lewis et al., 2020; Raffel et al., 2020). Since knowledge graphs may possess more complex structures than text sequences, it's hard to explicitly learn graphtext alignments by directly using the pre-training tasks based on text reconstruction.</p>
<p>Thus, we propose a graph-text joint represen-</p>
<p>tation learning framework called JointGT to deal with the above challenges. Firstly, to alleviate the structural information loss during encoding, we devise a simple structure-aware semantic aggregation module at each Transformer layer to aggregate contextual information following the graph structure. Secondly, we propose three pre-training tasks including graph enhanced text reconstruction, text enhanced graph reconstruction, and graph-text embedding alignment to explicitly build the connection between knowledge graphs and text sequences. The first two tasks are expected to enhance the graph-text alignment in the discrete vocabulary space, where our model is required to predict the masked information of graphs / texts based on the observed information of texts / graphs. And the third task is designed to model the graph-text alignment in the continuous embedding space via Optimal Transport (Peyré and Cuturi, 2019) to match the hidden representations of graphs and texts. Our contributions are as follows:</p>
<ul>
<li>We propose a novel pre-trained model called JointGT for KG-to-text generation tasks. This model adopts a structure-aware semantic aggregation module to model the structure of an input graph at each Transformer layer, and utilizes three pre-training tasks to explicitly learn graph-text alignments in the discrete and continuous spaces.</li>
<li>We conduct experiments on the datasets of KG-to-text generation including WebNLG, WebQuestions and PathQuestions. Results show that JointGT achieves new state-of-theart performance on KG-to-text generation.</li>
</ul>
<h2>2 Related Work</h2>
<h2>KG-to-Text Generation</h2>
<p>Recent studies on KG-to-text generation tasks mainly fall into three aspects: 1) Encoder modification: To alleviate the structural information loss of sequence encoders with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives
to jointly learn the tasks of graph-to-text and text-to-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-trained NLG models such as GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), recent work directly fine-tunes these models on graph-totext datasets and reports impressive performance (Ribeiro et al., 2020a; Kale and Rastogi, 2020; Chen et al., 2020b; Mager et al., 2020).</p>
<p>Compared with the existing work on pre-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning text-to-text pre-trained models on KG-to-text datasets.</p>
<h2>KG-Enhanced Pre-Trained Models</h2>
<p>Another line of related studies is pre-trained models enhanced by knowledge graphs for natural language understanding (NLU). The motivation of these models is to incorporate knowledge graphs into pre-trained models to facilitate the understanding of entities and relations in natural language. Early work including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) directly uses fixed entity embeddings based on TransE (Bordes et al., 2013) or word vectors (Mikolov et al., 2013) during pre-training. Recent work like KEPLER (Wang et al., 2021) and JAKET (Yu et al., 2020) resorts to jointly pre-training graph-text representations. Specifically, they encode the textual descriptions of entities with pre-trained language models as entity embeddings and jointly optimize the knowledge embedding objective and the masked language modeling objective.</p>
<p>In comparison, our model focuses on joint pretraining methods on knowledge graph encoding and sequence decoding in KG-to-text generation tasks, rather than considering graph-text joint encoding methods in NLU tasks.</p>
<h2>3 Method</h2>
<h3>3.1 Task Definition and Model Overview</h3>
<p>Given a knowledge graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ where $\mathcal{V}=\left{e_{1}, e_{2}, \cdots, e_{|\mathcal{V}|}\right}$ denotes the entity set and $\mathcal{E}=\left(r_{i j}\right)<em _linear="{linear" _text="\text">{|\mathcal{V}| \times|\mathcal{V}|}$ indicates the relations connecting the entities, and its linearized version $\mathcal{G}</em>\right)$ which is consistent with the input graph.}}=\left(w_{1}, w_{2}, \cdots, w_{m}\right)$ which consists of $m$ tokens, our goal is to generate a text sequence $X=\left(x_{1}, x_{2}, \cdots, x_{n</p>
<p>Our model is built on pre-trained encoderdecoder models like BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). First of all, we follow the existing work (Chen et al., 2020b) to linearize knowledge graphs in the form of triple lists (as shown in Figure 1), and devise a simple structure-aware semantic aggregation module which is plugged into each Transformer layer of the encoder to preserve the structural information of input graphs (§3.2). Then, we propose three pre-training tasks including graph / text reconstruction in the discrete vocabulary space and graphtext matching in the continuous embedding space, which enable our model to jointly learn the representations of knowledge graphs and texts (§3.3).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of linearizing knowledge graphs into text sequences. The special tokens <H>, <R> and <T> mean the head entity, relation and tail entity in the knowledge triples, respectively.</p>
<h3>3.2 Model Structure</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Structure-aware semantic aggregation module at each layer of the Transformer encoder. This module contains a pooling layer to obtain the contextual semantic representations of entities (z<sup>i</sup>) and relations (q<sup>i</sup><sub>ij</sub>) from the output of the vanilla self-attention layer (h<sup>i</sup><sub>i</sub>), a structure-aware self-attention layer to aggregate the entity representations (z<sup>i</sup><sub>i</sub>) based on the graph structure, and a residual layer to fuse the contextual and structural representations (h<sup>i</sup><sub>i</sub>).</p>
<p>To simultaneously leverage the contextual representation from pre-trained models and preserve the structural information, we devise a structure-aware semantic aggregation module in the Transformer encoder. Assume that the input of our encoder during pre-training is the linearized graph G<sub>linear</sub> and the corresponding text sequence X (which may be corrupted or empty in some pre-training tasks), the self-attention layer in the l-th Transformer layer can be formulated as follows<sup>2</sup>:</p>
<p>$$
\begin{aligned}
\boldsymbol{h}<em j="1">{i}^{l} &amp;= \sum</em>}^{m+n} \alpha_{ij}^{l} (\boldsymbol{h<em ij="ij">{j}^{l-1} \boldsymbol{W}^{V}) \
\alpha</em> \
t_{ij}^{l} &amp;= \frac{\left(\boldsymbol{h}}^{l} &amp;= \frac{\exp(t_{ij}^{l})}{\sum_{p=1}^{m+n} \exp(t_{ip}^{l})<em j="j">{i}^{l-1} \boldsymbol{W}^{Q}\right)\left(\boldsymbol{h}</em> \
i &amp;= 1, 2, \dots, m+n
\end{aligned}
$$}^{l-1} \boldsymbol{W}^{K}\right)^{\top}}{\sqrt{d_{k}}</p>
<p>where W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> are the model parameters and d<sub>k</sub> denotes the dimension of query / key / value vectors. The fully-connected attention captures rich contextual semantic relationships among the entities, relations and the tokens of text sequences, but is not sufficient to encode the structural information of input graphs. Thus, we devise a structure-aware semantic aggregation module on top of vanilla self-attention, as shown in Figure 2. First of all, we utilize a mean pooling layer<sup>3</sup> to obtain the representation of each entity and relation from the output of the vanilla self-attention layer:</p>
<p>$$
\begin{aligned}
\boldsymbol{z}<em p="p">{i}^{l} &amp;= \text{pooling} \left( \left{ \boldsymbol{h}</em>(e_i), 1 \leq p \leq m \right} \right) \
\boldsymbol{q}}^{l} \mid p \in \mathcal{P<em p="p">{ij}^{l} &amp;= \text{pooling} \left( \left{ \boldsymbol{h}</em>), 1 \leq p \leq m \right} \right) \
&amp;i = 1, \dots, |\mathcal{V}|; \quad j = 1, \dots, |\mathcal{V}|
\end{aligned}
$$}^{l} \mid p \in \mathcal{P}(r_{ij</p>
<p>where P(e<sub>i</sub>) / P(r<sub>ij</sub>) means the set of positions occupied by e<sub>i</sub> / r<sub>ij</sub> in the linearized graph. Note that q<sup>l</sup><sub>ij</sub> will be set to an all-zero vector if there is no relation between e<sub>i</sub> and e<sub>j</sub>. Then we update entity representations with a structure-aware self-attention.</p>
<p><sup>2</sup>We take a single attention head as an example in this section. In practice, we use our proposed method in the multi-head attention.</p>
<p><sup>3</sup>We find that there is no significant difference in the model performance between mean pooling and other aggregation functions like max pooling.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of our proposed pre-training tasks: (a) Graph enhanced text reconstruction: reconstructing the text sequence given the complete graph. (b) Text enhanced graph reconstruction: predicting the masked entities and relations of the corrupted graph conditioned on the complete text. (c) Graph-text embedding alignment: matching the embedding vectors of the knowledge graph and the text via Optimal Transport. The special token <SEP> is to separate the linearized graph and the text, while <M> denotes the placeholder for masked tokens.</p>
<p>attention layer (Shaw et al., 2018):</p>
<p>$$
\begin{aligned}
\hat{\boldsymbol{z}}<em j="1">{i}^{l} &amp; =\sum</em>}^{|\mathcal{V}|} \beta_{ij}^{l}\left(\boldsymbol{z<em ij="ij">{j}^{l} \boldsymbol{W}^{V S}+\boldsymbol{q}</em>\right) \
\beta_{ij}^{l} &amp; =\frac{\exp(u_{ij}^{l})}{\sum_{p=1}^{|\mathcal{V}|} \exp \left(u_{i p}^{l}\right)} \
u_{ij}^{l} &amp; =\frac{\left(\boldsymbol{z}}^{l} \boldsymbol{W}^{V R<em j="j">{i}^{l} \boldsymbol{W}^{Q S}\right)\left(\boldsymbol{z}</em>}^{l} \boldsymbol{W}^{K S}+\boldsymbol{q<em k="k">{ij}^{l} \boldsymbol{W}^{K R}\right)^{\top}}{\sqrt{d</em> \
i &amp; =1,2, \cdots,|\mathcal{V}|
\end{aligned}
$$}}</p>
<p>where $\boldsymbol{W}^{Q S}, \boldsymbol{W}^{K S}, \boldsymbol{W}^{V S}, \boldsymbol{W}^{K R}, \boldsymbol{W}^{V R}$ are the weight matrices in the structure-aware selfattention. This layer integrates the contextual semantic representation of entities and relations based on the graph structure, thereby injecting the structural information into the vanilla Transformer layer. Finally, we use a residual layer to fuse semantic and structural representations of entities, and obtain the hidden states for the following computation:</p>
<p>$$
\tilde{\boldsymbol{h}}<em i="i">{i}^{l}=\left{\begin{array}{cc}
\boldsymbol{h}</em>}^{l}+\tilde{\boldsymbol{z}<em j="j">{j}^{l}, &amp; i \in \mathcal{P}\left(e</em>\right) \
\boldsymbol{h}_{i}^{l}, &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>$$
i=1, \cdots, m+n ; \quad j=1, \cdots,|\mathcal{V}|
$$</p>
<p>Compared with existing structure-aware Transformer encoders (Zhu et al., 2019; Song et al., 2020) that either use the entity and relation embeddings from an external knowledge embedding model or directly learn them as model parameters, our encoder obtains the entity and relation embeddings via contextual semantic representations. This design fully employs the effective contextual representations from the existing pre-trained models while preserving the structural information, and enables our model to generalize to new entities and relations better when fine-tuned to the datasets with a different knowledge graph.</p>
<h3>3.3 Pre-Training Task</h3>
<p>Given the input graph $\mathcal{G}$ and its corresponding text sequence $X$, the goal of our pre-training task is to jointly learn the graph encoder and sequence decoder to enhance graph-text alignments, which can benefit the downstream tasks of KG-to-text generation. We devise three pre-training tasks to explicitly learn graph-text alignments in both discrete and continuous spaces.</p>
<h3>3.3.1 Graph Enhanced Text Reconstruction</h3>
<p>The purpose of graph enhanced text reconstruction is to recover the masked text sequence based on the complete knowledge graph, as shown in Figure 3. Assume that $\hat{X}$ denotes the masked text sequence, we can formulate the loss function of this pre-training task as follows:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em i="1">{\text {text }} &amp; =-\log P(X \mid \mathcal{G}, \hat{X}) \
&amp; =-\sum</em>\right)
\end{aligned}
$$}^{n} \log P\left(x_{i} \mid \mathcal{G}, \hat{X}, x_{&lt;i</p>
<p>To construct $\hat{X}$, we masked the entity words with a probability of $40 \%$ and other words with $20 \%$ since entity words are more important in the</p>
<p>task of KG-to-text generation. We also follow the existing work (Lewis et al., 2020) to merge the consecutive mask tokens into one mask token to increase the difficulty of text reconstruction. This task enables our model to utilize the knowledge graph to reconstruct the corrupted text sequence, which explores the connection between them in the discrete vocabulary space.</p>
<h3>3.3.2 Text Enhanced Graph Reconstruction</h3>
<p>As shown in Figure 3, this pre-training task aims to recover the corrupted graph according to the information of the text sequence. Given the corrupted knowledge graph $\tilde{\mathcal{G}}$ with masked entities and relations, and the complete text sequence $X$, the loss function is to recover the masked entities and relations in the linearized knowledge graph:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em i="1">{\text {graph }} &amp; =-\log P(\mathcal{G} \mid \tilde{\mathcal{G}}, X) \
&amp; =-\sum</em>, X\right)
\end{aligned}
$$}^{m} M_{i} \log P\left(w_{i} \mid \tilde{\mathcal{G}</p>
<p>where $M_{i}$ denotes an indicator function and equals 1 if and only if $w_{i}$ is masked. We empirically set the masking probability of entities / relations as $40 \% / 20 \%$. This task explicitly exerts the impact of the text on the graph reconstruction, thereby guiding the encoder to focus more on the entities and relations that may appear in the text.</p>
<h3>3.3.3 Graph-Text Embedding Alignment</h3>
<p>This pre-training task is devised to encourage the graph-text alignment in the embedding space. We use Optimal Transport (OT), which is commonly used in the cross-domain alignment (Chen et al., 2020a), to calculate the minimum cost of transporting the graph representation from the encoder to the text representation from the decoder (and vice versa). As shown in Figure 3, the input of the encoder is the linearized knowledge graph $\mathcal{G}<em 1="1">{\text {linear }}$ while the input of the decoder is the text sequence $X$. Assume that $\boldsymbol{H}^{L}=\left(\boldsymbol{h}</em>}^{L}, \boldsymbol{h<em m="m">{2}^{L}, \cdots, \boldsymbol{h}</em>\right)$ indicates the final hidden states of the encoder, we can similarly acquire the entity and relation representations via mean pooling:}^{L</p>
<p>$$
\begin{aligned}
\boldsymbol{z}<em p="p">{i}^{L}= &amp; \operatorname{pooling}\left(\left{\boldsymbol{h}</em>\right), 1 \leq p \leq m\right}\right) \
\boldsymbol{q}}^{L} \mid p \in \mathcal{P}\left(e_{i<em p="p">{i j}^{L}= &amp; \operatorname{pooling}\left(\left{\boldsymbol{h}</em>\right), 1 \leq p \leq m\right}\right) \
&amp; i=1, \cdots,|\mathcal{V}| ; \quad j=1, \cdots,|\mathcal{V}|
\end{aligned}
$$}^{L} \mid p \in \mathcal{P}\left(r_{i j</p>
<p>Let $\mathcal{G}<em 1="1">{\text {seq }}=\mathcal{V} \cup \mathcal{E}=\left(g</em>\right)$ denotes the sequence of all the entities and relations in
$\mathcal{G}$, we can directly obtain the contextual embedding vectors $\boldsymbol{H}^{\mathcal{G}}=\left(\boldsymbol{h}}, g_{2}, \cdots, g_{|\mathcal{V}|+|\mathcal{E}|<em _mathcal_V="|\mathcal{V">{1}^{\mathcal{G}}, \cdots, \boldsymbol{h}</em>}|+|\mathcal{E}|}^{\mathcal{G}}\right)$ for each entity and relation from Equation 7. We can also acquire the embedding vectors of $X$ from the decoder's final hidden states, which is denoted by $\boldsymbol{S}=\left(\boldsymbol{s<em 2="2">{1}, \boldsymbol{s}</em>\right)$.}, \cdots, \boldsymbol{s}_{n</p>
<p>To model the alignment between graphs and texts in the embedding space, we regard $\mathcal{G}<em i="1">{\text {seq }}$ as a discrete distribution $\boldsymbol{\mu}=\sum</em>}^{|\mathcal{V}|+|\mathcal{E}|} \boldsymbol{a<em g__i="g_{i">{i} \delta</em>}}$ and $X$ as $\boldsymbol{v}=\sum_{j=1}^{n} \boldsymbol{b<em x__j="x_{j">{j} \delta</em>}}$, where $\boldsymbol{a}=\left{\boldsymbol{a<em i="1">{i}\right}</em>}^{|\mathcal{V}|+|\mathcal{E}|}$ and $\boldsymbol{b}=\left{\boldsymbol{b<em j="1">{j}\right}</em>}^{n}$ satisfy $\sum_{i=1}^{|\mathcal{V}|+|\mathcal{E}|} \boldsymbol{a<em j="1">{i}=\sum</em>}^{n} \boldsymbol{b<em g__i="g_{i">{j}=1$, and $\delta</em>$ as the loss function, which is defined as the solution of the following problem:}} / \delta_{x_{j}}$ indicates the Dirac function centered on $g_{i} / x_{j}$. Then, we utilize the OT distance between $\boldsymbol{\mu}$ and $\boldsymbol{v</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _boldsymbol_T="\boldsymbol{T">{O T}= &amp; \min </em>} \in \Pi(\boldsymbol{a}, \boldsymbol{b})} \sum_{i=1}^{|\mathcal{V}|+|\mathcal{E}|} \sum_{j=1}^{n} \boldsymbol{T<em i="i">{i j} \cdot d\left(g</em>\right) \
\Pi(\boldsymbol{a}, \boldsymbol{b})= &amp; \left{\boldsymbol{T} \in \mathbb{R}}, x_{j<em n="n">{+}^{(|\mathcal{V}|+|\mathcal{E}|) \times n} \mid \boldsymbol{T} \cdot \mathbf{1}</em>\right. \
&amp; \left.\boldsymbol{T}^{\top} \cdot \mathbf{1}_{|\mathcal{V}|+|\mathcal{E}|}=\boldsymbol{b}\right}
\end{aligned}
$$}=\boldsymbol{a</p>
<p>where $\boldsymbol{T}$ denotes a transport plan, $\mathbf{1}<em n="n">{|\mathcal{V}|+|\mathcal{E}|} / \mathbf{1}</em>}$ indicates the $(|\mathcal{V}|+|\mathcal{E}|) / n$-dimensional all-one vector respectively, and $d\left(g_{i}, x_{j}\right)$ is the cost function of transporting $g_{i}$ to $x_{j}$. We follow the existing work (Chen et al., 2020c) to adopt the cosine distance between the contextual embedding vectors of $g_{i}$ and $x_{j}$ as the cost function, which is defined as $d\left(g_{i}, x_{j}\right)=1-\frac{\boldsymbol{h<em _boldsymbol_j="\boldsymbol{j">{i}^{\mathcal{G}} \boldsymbol{s}</em>}}}{\left|\boldsymbol{h<em 2="2">{i}^{\mathcal{G}}\right|</em>}\left|\boldsymbol{s<em 2="2">{j}\right|</em>$ can serve as an alignment loss to optimize the model parameters. This task builds the connection between the contextual embedding vectors of knowledge graphs and texts, and explicitly promotes the graph-text alignment in the continuous space.}}$. Since the exact minimization over $\boldsymbol{T}$ is computationally intractable, we utilize IPOT algorithm (Xie et al., 2019) to approximate the OT distance and iteratively obtain the solution of $\boldsymbol{T}$ (more details are provided in the Appendix A). After solving $\boldsymbol{T}, \mathcal{L}_{O T</p>
<h2>4 Experiment</h2>
<h3>4.1 Pre-training Dataset and Implementation</h3>
<p>We used KGTEXT (Chen et al., 2020b) as our pretraining dataset. This dataset contains 7 M graphtext data pairs, where texts are crawled from English Wikidump ${ }^{4}$ and the corresponding knowledge graphs are acquired by querying WikiData with the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">WebNLG(U)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebNLG(C)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebQuestions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PathQuestions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE</td>
</tr>
<tr>
<td style="text-align: center;">SOTA-NPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.00^{\dagger}$</td>
<td style="text-align: center;">$42.00^{\dagger}$</td>
<td style="text-align: center;">$71.00^{\dagger}$</td>
<td style="text-align: center;">$48.00^{\dagger}$</td>
<td style="text-align: center;">$36.00^{\dagger}$</td>
<td style="text-align: center;">$65.00^{\dagger}$</td>
<td style="text-align: center;">$29.45^{\ddagger}$</td>
<td style="text-align: center;">$30.96^{\ddagger}$</td>
<td style="text-align: center;">$55.45^{\dagger}$</td>
<td style="text-align: center;">$61.48^{\dagger}$</td>
<td style="text-align: center;">$44.57^{\dagger}$</td>
<td style="text-align: center;">$77.72^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">KGPT</td>
<td style="text-align: center;">177 M</td>
<td style="text-align: center;">$64.11^{\ddagger}$</td>
<td style="text-align: center;">$46.30^{\ddagger}$</td>
<td style="text-align: center;">$74.57^{\ddagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">140 M</td>
<td style="text-align: center;">64.55</td>
<td style="text-align: center;">46.51</td>
<td style="text-align: center;">75.13</td>
<td style="text-align: center;">56.65</td>
<td style="text-align: center;">44.51</td>
<td style="text-align: center;">70.94</td>
<td style="text-align: center;">29.61</td>
<td style="text-align: center;">31.48</td>
<td style="text-align: center;">55.42</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;">47.23</td>
<td style="text-align: center;">77.76</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">220 M</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">46.58</td>
<td style="text-align: center;">74.77</td>
<td style="text-align: center;">58.66</td>
<td style="text-align: center;">46.04</td>
<td style="text-align: center;">73.06</td>
<td style="text-align: center;">28.78</td>
<td style="text-align: center;">30.55</td>
<td style="text-align: center;">55.12</td>
<td style="text-align: center;">58.95</td>
<td style="text-align: center;">44.72</td>
<td style="text-align: center;">76.58</td>
</tr>
<tr>
<td style="text-align: center;">JointGT (BART)</td>
<td style="text-align: center;">160 M</td>
<td style="text-align: center;">65.92</td>
<td style="text-align: center;">47.15</td>
<td style="text-align: center;">76.10**</td>
<td style="text-align: center;">58.55</td>
<td style="text-align: center;">45.01</td>
<td style="text-align: center;">72.31</td>
<td style="text-align: center;">30.02*</td>
<td style="text-align: center;">32.05**</td>
<td style="text-align: center;">55.60</td>
<td style="text-align: center;">65.89**</td>
<td style="text-align: center;">48.25**</td>
<td style="text-align: center;">78.87**</td>
</tr>
<tr>
<td style="text-align: center;">JointGT (T5)</td>
<td style="text-align: center;">265 M</td>
<td style="text-align: center;">66.14**</td>
<td style="text-align: center;">47.25**</td>
<td style="text-align: center;">75.91</td>
<td style="text-align: center;">61.01**</td>
<td style="text-align: center;">46.32**</td>
<td style="text-align: center;">73.57**</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">31.29</td>
<td style="text-align: center;">54.47</td>
<td style="text-align: center;">60.45</td>
<td style="text-align: center;">45.38</td>
<td style="text-align: center;">77.59</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on WebNLG, WebQuestions and PathQuestions. SOTA-NPT indicates the state-of-the-art performance from the baselines without pre-training. #Param means the number of model parameters. The results marked with $\dagger, \ddagger$ and $\ddagger$ are re-printed from Shimorina and Gardent (2018), Chen et al. (2020d) and Chen et al. (2020b), respectively. - means that the results are not reported in the corresponding references. * indicates that our model significantly outperforms BART and T5 on the corresponding datasets (t-test, $p&lt;0.05$ ), while ** means $p&lt;0.01$.</p>
<p>Wikipedia hyperlinks of entities in the sentences. The detailed statistics of KGTEXT are shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">#Ent</th>
<th style="text-align: center;">#Rel</th>
<th style="text-align: center;">#Instances <br> (Train / Valid / Test)</th>
<th style="text-align: center;">#Triples</th>
<th style="text-align: center;">Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KGTEXT</td>
<td style="text-align: center;">1.8 M</td>
<td style="text-align: center;">1,210</td>
<td style="text-align: center;">6.98 M / 10K / 10K</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG(U)</td>
<td style="text-align: center;">3,114</td>
<td style="text-align: center;">373</td>
<td style="text-align: center;">$34,352 / 4,316 / 4,224$</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG(C)</td>
<td style="text-align: center;">3,129</td>
<td style="text-align: center;">373</td>
<td style="text-align: center;">$34,536 / 4,217 / 4,148$</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">19.8</td>
</tr>
<tr>
<td style="text-align: left;">WebQuestions</td>
<td style="text-align: center;">25,703</td>
<td style="text-align: center;">672</td>
<td style="text-align: center;">$18,989 / 2,000 / 2,000$</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: left;">PathQuestions</td>
<td style="text-align: center;">7,250</td>
<td style="text-align: center;">378</td>
<td style="text-align: center;">$9,793 / 1,000 / 1,000$</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">14.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of pre-training and fine-tuning datasets, including the total number of entities and relations, the data split, the average number of triples, and the average length of texts.</p>
<p>Since our model can adapt to Transformer-based pre-trained models with the encoder-decoder framework, we chose BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) as the base model in this paper, which are denoted by JointGT (BART) and JointGT (T5), respectively. The hyper-parameters of the Transformer blocks were the same as BARTbase and T5-base because of the limited computational resources. We initialized our model parameters with the pre-trained checkpoint of BARTbase / T5-base except for the structure-aware semantic aggregation module, which was randomly initialized. We followed BART / T5 to use BytePair Encoding (BPE) vocabulary (Radford et al., 2019) with the size of 50,265 / WordPiece vocabulary (Kudo and Richardson, 2018) with the size of 32,000. The batch size was 42 / 32 for JointGT (BART) / JointGT (T5). The maximum length of linearized input graphs was 600 , while the maximum length of text sequences was 64 . We adopted Adam (Kingma and Ba, 2015) as the optimizer and set the learning rate to be $3 \mathrm{e}-5$. The warmup ratio was 0.1 . JointGT was pre-trained on KGTEXT for 1 epoch with the proposed pre-training tasks. It took 44 / 69 hours for JointGT (BART) / JointGT (T5) on 3 NVIDIA Quadro RTX 6000 GPUs.</p>
<h3>4.2 Fine-Tuning Settings</h3>
<p>We adopted WebNLG, WebQuestions and Path Questions as the benchmark datasets during finetuning, and provided the statistics in Table 2.
WebNLG: This dataset aims to convert RDF triples into a textual description. We followed the existing work (Chen et al., 2020b) to use the version of 2.0 (Shimorina and Gardent, 2018). This dataset contains two official data splits: the traditional split (Unconstrained) which guarantees that there is no overlap of input graphs among train / validation / test sets, and a more challenging split (Constrained) where the non-overlap constraint is applied to the triples of input graphs. We denoted these two data splits as $\operatorname{WebNLG}(U)$ and $\operatorname{WebNLG}(C)$ in our paper. We followed the preprocessing steps of the existing work (Chen et al., 2020b) to replace the underlines in the entities and relations with spaces, and split the entities and relations in a camel case into multiple words.
WebQuestions: This dataset (Yih et al., 2016; Talmor and Berant, 2018) is the benchmark for question generation over knowledge bases (KBQG), whose purpose is to generate natural language questions about the corresponding knowledge graphs (Serban et al., 2016). It is constructed from two question answering datasets, i.e., WebQuestionsSP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). These two datasets contain natural language questions, SPARQL queries and answer entities. We converted the SPARQL query to return a subgraph, and used the same preprocessing steps and data splits as the existing work (Kumar et al., 2019; Chen et al., 2020d).
PathQuestions: Similar to WebQuestions, the PathQuestions dataset is also the benchmark for KBQG, which is constructed from a question answering dataset (Zhou et al., 2018b). The main</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\kappa$</th>
<th style="text-align: center;">Adequacy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\kappa$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Win (\%)</td>
<td style="text-align: center;">Lose (\%)</td>
<td style="text-align: center;">Tie (\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Win (\%)</td>
<td style="text-align: center;">Lose (\%)</td>
<td style="text-align: center;">Tie (\%)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">JointGT (BART) vs. BART</td>
<td style="text-align: center;">$29.0^{*}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">$26.3^{<em> </em>}$</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">0.517</td>
</tr>
<tr>
<td style="text-align: left;">JointGT (T5) vs. T5</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">$22.7^{*}$</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">0.424</td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation on WebNLG(U). The scores indicate the percentages of win, lose and tie when JointGT is compared with other baselines. $\kappa$ is Fleiss' Kappa (all indicate moderate agreement). The scores marked with * mean $p&lt;0.05$ while ${ }^{<em> </em>}$ means $p&lt;0.01$ in sign test.
difference is that the knowledge graph in PathQuestions is a 2-hop / 3-hop path between two entities. We used the same preprocessing steps and data splits as the existing work (Kumar et al., 2019; Chen et al., 2020d).</p>
<p>More detailed fine-tuning settings including the search space and the best assignment of hyperparameters on the downstream datasets are reported in the Appendix B.</p>
<h3>4.3 Baselines</h3>
<p>We chose the following two categories of models as our baselines:
Pre-Trained Models: We adopted KGPT (Chen et al., 2020b), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) as the pre-trained baselines. KGPT is a pre-trained model for KG-to-text generation, which utilizes the same pre-training dataset as our model and directly uses KG-to-text generation as the pre-training task. BART and T5, as the state-of-the-art pre-trained models for text generation, can be applied to KG-to-text generation with the input of linearized knowledge graphs and the output of text sequences (Ribeiro et al., 2020a).
Task-Specific Models without Pre-Training: We also chose the state-of-the-art task-specific models without pre-training for each dataset as our baselines, including Seq2Seq with copying or delexicalisation (Shimorina and Gardent, 2018) for WebNLG v2.0, and G2S (Chen et al., 2020d) for WebQuestions and PathQuestions.</p>
<p>We directly re-printed the results of baselines if they use the same datasets as ours. Otherwise, we implemented the baselines based on the codes and model parameters released by the original papers. We reported all the results of our implemented models with the mean values over 5 runs.</p>
<h3>4.4 Automatic Evaluation</h3>
<p>We followed the existing work (Shimorina and Gardent, 2018; Chen et al., 2020d) to use BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) as our automatic metrics. The main results on WebNLG, WebQues-
tions and PathQuestions are shown in Table 1. We can observe that JointGT based on BART / T5 can outperform vanilla BART / T5 on most of the metrics, respectively, and obtain the state-of-the-art performance on all the datasets. This indicates that our method can promote graph-text alignments and further enhance the performance of the state-of-theart pre-trained models on KG-to-text datasets.</p>
<h3>4.5 Human Evaluation</h3>
<p>To further evaluate the quality of generated results, we conducted human evaluation on the WebNLG(U) dataset. We followed the existing work (Ferreira et al., 2019; Ribeiro et al., 2020b) to select two criteria: fluency (whether a sentence is grammatically fluent) and adequacy (whether a sentence clearly describes the knowledge graph). We randomly sampled 100 knowledge graphs from the test set, and collected the generated results from our models and the most competitive baseline models (i.e., BART and T5). We used the pairwise comparison between BART / T5 and JointGT (BART) / JointGT (T5). Specifically, for each pair of generated texts (one from JointGT and the other from the corresponding baseline, given the same input knowledge graph), three annotators were hired to label which text is better (i.e., win, lose or tie) in terms of the metrics mentioned above. Note that the two metrics were evaluated independently.</p>
<p>Results in Table 3 show that JointGT can beat the corresponding baselines in both fluency and adequacy. Especially for adequacy, our model can significantly outperform BART / T5, which indicates that our model equipped with the structure-aware encoder and well-designed pre-training tasks can generate high-quality texts to describe knowledge graphs more clearly. To evaluate the agreement among different annotators, we calculated Fleiss' Kappa (Fleiss, 1971) for each pairwise comparison, where the results in Table 3 show moderate agreement $(0.4 \leq \kappa \leq 0.6)$.</p>
<h3>4.6 Ablation Study</h3>
<h3>4.6.1 Encoder Structure</h3>
<p>To investigate the effect of our proposed structureaware semantic aggregation module, we fixed the pre-training tasks and compared our encoder with two Transformer-based encoders commonly used in the existing work:
SeqEnc: This sequence encoder takes linearized graphs as input and ignores structural information (Ribeiro et al., 2020a; Kale and Rastogi, 2020).
RelEnc: This relation-aware encoder regards the entity sequence as input and leverages the relation embedding into the self-attention layer. Both the entity and relation embedding vectors are directly learned as model parameters (Shaw et al., 2018; Zhu et al., 2019; Song et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Param</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JointGT (BART)</td>
<td style="text-align: center;">160 M</td>
<td style="text-align: center;">$\mathbf{6 5 . 9 2}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 1 5}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">w/ SeqEnc</td>
<td style="text-align: center;">140 M</td>
<td style="text-align: center;">64.82</td>
<td style="text-align: center;">46.87</td>
<td style="text-align: center;">75.37</td>
</tr>
<tr>
<td style="text-align: left;">w/ RelEnc</td>
<td style="text-align: center;">160 M</td>
<td style="text-align: center;">65.17</td>
<td style="text-align: center;">47.07</td>
<td style="text-align: center;">75.69</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation test of different encoder structures on WebNLG(U), including our encoder, sequence encoder (SeqEnc) and relation-aware encoder (RelEnc).</p>
<p>Note that we only chose the encoder structures that can directly adapt to BART / T5 for fair comparison ${ }^{5}$. Results in Table 4 show that our encoder structure can perform better than the other baselines. Compared with the relation-aware encoder which can also capture the structural information of knowledge graphs, our model fully utilizes the effective contextual semantic representation to initialize the entity / relation representation at each Transformer layer instead of directly using the learnable entity / relation embedding vectors. This design equips JointGT with better generalization ability during fine-tuning, thereby enhancing our performance on downstream datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Triples</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JointGT (BART)</td>
<td style="text-align: center;">71.24</td>
<td style="text-align: center;">61.36</td>
</tr>
<tr>
<td style="text-align: left;">w/ SeqEnc</td>
<td style="text-align: center;">$70.83 \left(-0.41\right)$</td>
<td style="text-align: center;">$60.11 \left(-1.25\right)$</td>
</tr>
<tr>
<td style="text-align: left;">w/ RelEnc</td>
<td style="text-align: center;">$70.98 \left(-0.26\right)$</td>
<td style="text-align: center;">$60.58 \left(-0.78\right)$</td>
</tr>
</tbody>
</table>
<p>Table 5: BLEU scores of three encoders on the test set of WebNLG(U) with different numbers of input triples.</p>
<p>To further demonstrate the effectiveness of our encoder, we divided the test set of WebNLG(U)</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>into two subsets according to the number of triples in knowledge graphs, and compared the performance of three encoders. Results in Table 5 show that the improvement margin between our encoder and other encoders is more evident when the number of input triples is large, which indicates that our model can facilitate the encoding of knowledge graphs with more complex structures.</p>
<h3>4.6.2 Pre-Training Task</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JointGT (BART)</td>
<td style="text-align: center;">$\mathbf{6 5 . 9 2}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 1 5}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o TextRecon</td>
<td style="text-align: center;">64.22</td>
<td style="text-align: center;">46.56</td>
<td style="text-align: center;">74.96</td>
</tr>
<tr>
<td style="text-align: left;">w/o GraphRecon</td>
<td style="text-align: center;">65.37</td>
<td style="text-align: center;">47.09</td>
<td style="text-align: center;">75.97</td>
</tr>
<tr>
<td style="text-align: left;">w/o OT</td>
<td style="text-align: center;">65.03</td>
<td style="text-align: center;">47.09</td>
<td style="text-align: center;">75.83</td>
</tr>
<tr>
<td style="text-align: left;">w/ BARTPretrain</td>
<td style="text-align: center;">64.60</td>
<td style="text-align: center;">46.78</td>
<td style="text-align: center;">75.74</td>
</tr>
<tr>
<td style="text-align: left;">w/ KGPTPretrain</td>
<td style="text-align: center;">65.14</td>
<td style="text-align: center;">46.94</td>
<td style="text-align: center;">75.72</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation test of three pre-training tasks on WebNLG(U), including text / graph reconstruction and graph-text alignments via OT. BARTPretrain / KGPTPretrain means using the pre-training tasks of BART / KGPT instead of our tasks on KGTEXT.</p>
<p>To study the effect of three pre-training tasks, we maintained the encoder structure and removed each task respectively to test the performance. We also replaced all our pre-training tasks with the tasks of the existing work for comparison:
BARTPretrain: The pre-training tasks of BART including text infilling and sentence permutation (Lewis et al., 2020). Since these tasks cannot be applied to graph data, we only used these tasks on the text data of the pre-training dataset.
KGPTPretrain: The pre-training task of KGPT, i.e., KG-to-text generation on the pre-training dataset (Chen et al., 2020b).</p>
<p>Results in Table 6 show that each of our pretraining tasks contributes to the model performance. Compared with the other two tasks, graph enhanced text reconstruction plays a more important role in the task of KG-to-text generation, which directly supervises the decoder with the conditional generation loss. We also observe an apparent performance drop if we replace our pre-training tasks with those proposed by the existing work, thereby indicating the effectiveness of our pre-training tasks to promote KG-to-text generation.</p>
<h3>4.7 Few-Shot Learning</h3>
<p>To further analyze whether our pre-training tasks can learn a good graph-text joint representation that benefits the downstream KG-to-text generation tasks, we considered the few-shot setting where</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Generated results on WebNLG(U). We highlight the missing and unfaithful parts of each text in red and blue, respectively.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Data Proportion</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$0.5 \%$</td>
<td>$1 \%$</td>
<td>$5 \%$</td>
<td>$10 \%$</td>
</tr>
<tr>
<td>BART</td>
<td>33.92</td>
<td>39.08</td>
<td>52.24</td>
<td>56.58</td>
</tr>
<tr>
<td>JointGT (BART)</td>
<td>37.18</td>
<td>42.26</td>
<td>54.41</td>
<td>57.73</td>
</tr>
<tr>
<td>w/ BARTPretrain</td>
<td>32.63</td>
<td>37.11</td>
<td>52.91</td>
<td>56.81</td>
</tr>
<tr>
<td>w/ KGPTPretrain</td>
<td>35.33</td>
<td>40.72</td>
<td>53.08</td>
<td>57.18</td>
</tr>
</tbody>
</table>
<p>Table 7: BLEU scores of the models with correponding pre-training tasks trained on different proportions of WebNLG(U). only a few training instances were used during finetuning. We still fixed our model structure and compared our pre-training tasks with the tasks of BART and KGPT mentioned in $\S 4.6 .2$.</p>
<p>Results in Table 7 show that our pre-training tasks can perform better than other tasks, especially when the amount of training data is small. This indicates that our proposed tasks can capture the graph-text alignments during pre-training, thereby making our model generalizable to the downstream KG-to-text datasets better with only a few training samples.</p>
<h3>4.8 Case Study</h3>
<p>To intuitively show the generation quality of our model, we provided some generated cases in Figure 4. We observe that JointGT can generate highquality texts that describe the knowledge graph more completely and faithfully. For example, in the generated case on WebNLG(U), both BART and T5 fail to cover all the input triples, where BART misses the triple (Acharya Institute of Technology, sports offer, Tennis) and T5 misses (Tennis, sports governing body, International Tennis Federation). Also, T5 generates non-existing facts that are unfaithful to the knowledge graph. Equipped with the structure-aware Transformer encoder and the well-designed pre-training tasks to learn graph- text alignments, JointGT (BART) and JointGT (T5) can generate descriptions which include all the input triples and express the relation between each pair of entities more faithfully.</p>
<h2>5 Conclusion</h2>
<p>We propose a novel graph-text joint representation learning model called JointGT for KG-to-text generation. This model plugs a simple structureaware semantic aggregation module into the vanilla Transformer layer to preserve the structure of input graphs, and utilizes three pre-training tasks to learn graph-text alignments in the discrete vocabulary space and continuous embedding space. Experiments show that JointGT can outperform state-of-the-art pre-trained NLG models on various datasets of KG-to-text generation.</p>
<h2>Acknowledgments</h2>
<p>This work was partly supported by the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, pages 65-72.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarcíaDurán, Jason Weston, and Oksana Yakhnenko.</p>
<ol>
<li>Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems, pages 2787-2795.</li>
</ol>
<p>Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. 2020a. Graph optimal transport for cross-domain alignment. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 1542-1553.</p>
<p>Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020b. KGPT: knowledge-grounded pretraining for data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8635-8648.</p>
<p>Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020c. UNITER: universal image-text representation learning. In ECCV, volume 12375, pages $104-120$.</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020d. Toward subgraph guided knowledge graph question generation with graph neural networks. arXiv preprint arXiv: 2004.06015.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages $552-562$.
J. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76:378382 .</p>
<p>Zihao Fu, Bei Shi, Wai Lam, Lidong Bing, and Zhiyuan Liu. 2020. Partially-aligned data-to-text generation with distant supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9183-9193.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133.</p>
<p>Jian Guan, Yansen Wang, and Minlie Huang. 2019. Story ending generation with incremental encoding and commonsense knowledge. In The Thirty-Third AAAI Conference on Artificial Intelligence, pages $6473-6480$.</p>
<p>Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020. Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. arXiv preprint arXiv: 2006.04702.</p>
<p>Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, and Minlie Huang. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 725-736.</p>
<p>Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang. 2020. Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2398-2409.</p>
<p>Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Language Generation, pages 97-102.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations.</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2284-2293.</p>
<p>Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, pages 66-71.</p>
<p>Vishwajeet Kumar, Yuncheng Hua, Ganesh Ramakrishnan, Guilin Qi, Lianli Gao, and Yuan-Fang Li. 2019. Difficulty-controllable multi-hop question generation from knowledge graphs. In 18th International Semantic Web Conference, volume 11778, pages 382-398.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md. Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. Gpt-too: A language-model-first approach for amr-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1846-1852.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9.</p>
<p>Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111-3119.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2267-2277.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages $43-54$.</p>
<p>Gabriel Peyré and Marco Cuturi. 2019. Computational optimal transport. Found. Trends Mach. Learn., 11(5-6):355-607.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. In OpenAI Technical Report.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In OpenAI Technical Report.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020a. Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv: 2007.08426.</p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020b. Modeling global and local node contexts for text generation from knowledge graphs. Trans. Assoc. Comput. Linguistics, 8:589-604.</p>
<p>Martin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2020a. Modeling graph structure via relative position for better text generation from knowledge graphs. arXiv preprint arXiv: 2006.09242.</p>
<p>Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schütze. 2020b. An unsupervised joint system for text generation from knowledge graphs and semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7117-7130.</p>
<p>Iulian Vlad Serban, Alberto García-Durán, Çaglar Gülçehre, Sungjin Ahn, Sarath Chandar, Aaron C. Courville, and Yoshua Bengio. 2016. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 464-468.</p>
<p>Anastasia Shimorina and Claire Gardent. 2018. Handling rare items in data-to-text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 360-370.</p>
<p>Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, and Dong Yu. 2020. Structural information preserving for graph-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7987-7998.</p>
<p>Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641-651.</p>
<p>Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1627-1637.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embedding and pre-trained language representation. Trans. Assoc. Comput. Linguistics, 9:176-194.</p>
<p>Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. 2019. A fast proximal point method for computing exact wasserstein distance. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, volume 115, pages $433-453$.</p>
<p>Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020. JAKET: joint pre-training of knowledge graph and language understanding. arXiv preprint arXiv: 2010.00796.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages $1441-1451$.</p>
<p>Chao Zhao, Marilyn A. Walker, and Snigdha Chaturvedi. 2020. Bridging the structural gap between encoding and decoding for data-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2481-2491.</p>
<p>Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018a. Commonsense knowledge aware conversation generation with graph attention. In Proceedings of the TwentySeventh International Joint Conference on Artificial Intelligence, pages 4623-4629.</p>
<p>Mantong Zhou, Minlie Huang, and Xiaoyan Zhu. 2018b. An interpretable reasoning network for multi-relation question answering. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2010-2022.</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better amr-to-text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5458-5467.</p>
<h2>A IPOT Algorithm</h2>
<p>Inexact Proximal point method for Optimal Transport (IPOT) is an effective iterative method to approximate OT distance and compute the transport plan $\boldsymbol{T}$ (Xie et al., 2019). Given the sequence of entities and relations in the knowledge graph $\mathcal{G}<em 1="1">{\text {seq }}=\left(g</em>}, \cdots, g_{|\mathcal{V}|+|\mathcal{E}|}\right)$ with its corresponding embedding vectors $\boldsymbol{H}^{\mathcal{G}}=\left(\boldsymbol{h<em _mathcal_V="|\mathcal{V">{1}^{\mathcal{G}}, \cdots, \boldsymbol{h}</em>\right)$, and}|+|\mathcal{E}|}^{\mathcal{G}</p>
<h2>Algorithm 1 IPOT Algorithm</h2>
<h2>Require:</h2>
<p>$\mathcal{G}<em i="i">{\text {seq }}=\left{g</em>\right}<em j="j">{i=1}^{|\mathcal{V}|+|\mathcal{E}|}, X=\left{x</em>\right}<em i="i">{j=1}^{n}$, and their embedding vectors $\boldsymbol{H}^{\mathcal{G}}=\left{\boldsymbol{h}</em>\right}}^{\mathcal{G}<em j="j">{i=1}^{|\mathcal{V}|+|\mathcal{E}|}, \boldsymbol{S}=$ $\left{\boldsymbol{s}</em>\right}<em n="n">{j=1}^{n}$
Generalized stepsize: $1 / \beta$
1: $\boldsymbol{\sigma}=\frac{1}{n} \mathbf{1}</em>}, \boldsymbol{T}^{(1)}=\mathbf{1<em n="n">{|\mathcal{V}|+|\mathcal{E}|} \mathbf{1}</em>$
2: $\boldsymbol{C}}^{\top<em i="i">{i j}=d\left(g</em>}, x_{j}\right)=1-\frac{\boldsymbol{h<em j="j">{i}^{\mathcal{G}} \boldsymbol{s}</em>}}{\left|\boldsymbol{h<em 2="2">{i}^{\mathcal{G}}\right|</em>}\left|\boldsymbol{s<em 2="2">{j}\right|</em>$
3: $\boldsymbol{A}}<em i="i" j="j">{i j}=e^{-\frac{\boldsymbol{C}</em>$
4: for $t=1$ to $N$ do
$5: \quad \boldsymbol{Q}=\boldsymbol{A} \odot \boldsymbol{T}^{(t)}$
6: for $k=1$ to $K$ do
7: $\quad \delta=\frac{1}{\left(|\mathcal{V}|+|\mathcal{E}|\right) \boldsymbol{Q} \boldsymbol{\sigma}}, \boldsymbol{\sigma}=\frac{1}{n \boldsymbol{Q}^{\top} \delta}$
8: end for
9: $\quad \boldsymbol{T}^{(t+1)}=\operatorname{diag}(\boldsymbol{\delta}) \boldsymbol{Q} \operatorname{diag}(\boldsymbol{\sigma})$
10: end for
11: return $\boldsymbol{T}$
the text sequence $X=\left(x_{1}, \cdots, x_{n}\right)$ with its embedding vectors $\boldsymbol{S}=\left(\boldsymbol{s}}}{\beta}<em n="n">{1}, \cdots, \boldsymbol{s}</em>$ is shown in Algorithm 1.}\right)$, the implementation of IPOT algorithm to calculate $\boldsymbol{T</p>
<p>In the algorithm of IPOT, $\odot$ denotes Hadamard product. $\beta, K$ and $N$ are all hyper-parameters. We followed the existing work (Chen et al., 2020a) to set $\beta=1.0, K=1$ and $N=10$.</p>
<h2>B Hyper-Parameter Setting</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameter</th>
<th style="text-align: center;">Search Space</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Masking Probability</td>
<td style="text-align: center;">choice[20\%,30\%,40\%]</td>
</tr>
<tr>
<td style="text-align: center;">(entity / relation / word)</td>
<td style="text-align: center;">choice[2e-5,3e-5,5e-5]</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">choice[1,2]</td>
</tr>
<tr>
<td style="text-align: center;">Training Epoch</td>
<td style="text-align: center;">choice[0,0.1]</td>
</tr>
<tr>
<td style="text-align: center;">Warmup Ratio</td>
<td style="text-align: center;">choice[32,36,42]</td>
</tr>
<tr>
<td style="text-align: center;">Batch Size</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">Input Length</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Output Length</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Maximum Gradient Norm</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">1e-8</td>
</tr>
</tbody>
</table>
<p>Table 8: Hyper-parameter search space of JointGT during pre-training. choice indicates that the listed numbers will be chosen with the same probability.</p>
<p>We provided the detailed settings of hyperparameters during pre-training and fine-tuning. The settings include hyper-parameter search space and best assignments. Note that we used Huggingface's Transformers ${ }^{6}$ to implement our models.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameter</th>
<th style="text-align: center;">Search Space</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">choice[2e-5,3e-5,5e-5,1e-4]</td>
</tr>
<tr>
<td style="text-align: center;">Training Epoch</td>
<td style="text-align: center;">choice[20,30,40]</td>
</tr>
<tr>
<td style="text-align: center;">Warmup Step</td>
<td style="text-align: center;">uniform-integer[0,total_step*0.2]</td>
</tr>
<tr>
<td style="text-align: center;">Batch Size</td>
<td style="text-align: center;">choice[24,32]</td>
</tr>
<tr>
<td style="text-align: center;">Input Length</td>
<td style="text-align: center;">choice[128,256]</td>
</tr>
<tr>
<td style="text-align: center;">Output Length</td>
<td style="text-align: center;">choice[64,128]</td>
</tr>
<tr>
<td style="text-align: center;">Beam Size</td>
<td style="text-align: center;">choice[2,3,5]</td>
</tr>
<tr>
<td style="text-align: center;">Length Penalty</td>
<td style="text-align: center;">choice[1.0,3.0,5.0]</td>
</tr>
<tr>
<td style="text-align: center;">Maximum Gradient Norm</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Epsilon (for Adam)</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
</tbody>
</table>
<p>Table 9: Hyper-parameter search space of JointGT during fine-tuning. uniform-integer means the integers in the interval can be selected uniformly. In the search space of warmup step, total_step denotes the total training steps on the corresponding datasets.</p>
<p>Thus all the hyper-parameters reported in our paper were consistent with the codes of Huggingface's Transformers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">JointGT (BART)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">WebNLG(U)</td>
<td style="text-align: center;">WebNLG(C)</td>
<td style="text-align: center;">WebQuestions</td>
<td style="text-align: center;">PathQuestions</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Training Epoch</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Step</td>
<td style="text-align: center;">1,600</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3,400</td>
<td style="text-align: center;">1,100</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Input Length</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Output Length</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Beam Size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Length Penalty</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">JointGT (T5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">WebNLG(U)</td>
<td style="text-align: center;">WebNLG(C)</td>
<td style="text-align: center;">WebQuestions</td>
<td style="text-align: center;">PathQuestions</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Training Epoch</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Step</td>
<td style="text-align: center;">1,600</td>
<td style="text-align: center;">1,200</td>
<td style="text-align: center;">2,300</td>
<td style="text-align: center;">900</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Input Length</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Output Length</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Beam Size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Length Penalty</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 10: Best assignments of hyper-parameters on the downstream datasets.</p>
<p>We presented the hyper-parameter search space during pre-training in Table 8. The number of hyper-parameter search trials was 10. Manual search was adopted to select hyper-parameters, and the selection criterion was BLEU on the validation set when we fine-tuned the pre-trained model on WebNLG(U). The best assignment of pre-training was described in our main content.</p>
<p>We also provided the detailed settings of hyperparameters during fine-tuning on the downstream datasets, including the hyper-parameter search space in Table 9 and the best assignments in Table 10. The number of hyper-parameter search trials was 20. BLEU was adopted as our criterion in the manual search on all the downstream tasks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/huggingface/ transformers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
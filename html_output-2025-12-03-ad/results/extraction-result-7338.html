<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7338 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7338</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7338</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-265157573</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07582v1.pdf" target="_blank">Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in Large Language Models (LLMs) have presented new opportunities for integrating Artificial General Intelligence (AGI) into biological research and education. This study evaluated the capabilities of leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in answering conceptual biology questions. The models were tested on a 108-question multiple-choice exam covering biology topics in molecular biology, biological techniques, metabolic engineering, and synthetic biology. Among the models, GPT-4 achieved the highest average score of 90 and demonstrated the greatest consistency across trials with different prompts. The results indicated GPT-4's proficiency in logical reasoning and its potential to aid biology research through capabilities like data analysis, hypothesis generation, and knowledge integration. However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7338.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7338.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art multimodal large language model from OpenAI evaluated as a text-based simulator for conceptual biology reasoning by answering a 108-question multiple-choice exam; showed highest accuracy and strongest consistency across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Generative pretrained multimodal transformer (described as large/capable); treated as an instruction-following conversational model in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of biological reasoning: answer a 108-question multiple-choice conceptual biology exam covering molecular biology, techniques, metabolic engineering, and synthetic biology.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot style prompting using 5 differently worded single-instruction prompts; each prompt requested a single-letter best answer with no justification (no few-shot examples, no chain-of-thought, no tool use, constrained-output format).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number correct (score on 108-question exam), mean score across five trials, standard deviation across trials, inter-trial correlation, and per-question correct-occurrence distribution compared to expected random-guess (Poisson) — reported as degree-of-confidence analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average score = 90 points on 108-question exam (reported by paper) — stated as >80% correct per trial and <10% incorrect per trial; per-category reported means: metabolic engineering mean = 100 points, synthetic biology mean = 92 points (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against random-guess expected distribution (Poisson) for degree-of-confidence analysis and against other LLMs tested (GPT-3.5, PaLM2, Claude2, SenseNova) whose average scores lay in the 70–80 point range; random-guess exact baseline not numerically specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt phrasing / prompt engineering (five differently worded prompts affected other models more than GPT-4)', 'Model capability/capacity (GPT-4 described as larger/more capable)', 'Question subdomain (performance varied by category; GPT-4 excelled in metabolic engineering and synthetic biology)', 'Consistency/stability across repeated prompts (measured by standard deviation and inter-trial correlation)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Five independent trials per model using five predefined prompt variants; each trial: full 108-question exam; responses constrained to single-letter answer only; no chain-of-thought or explanatory output allowed; no sampling hyperparameters (temperature/top-k) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Evaluated only on multiple-choice conceptual questions (not open-ended design or experimental tasks); lacks reported hyperparameter settings; further validation and domain-specific fine-tuning required per authors; other models showed greater sensitivity to prompt phrasing, indicating potential failure modes under prompt variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7338.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7338.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI conversational/instruction-tuned LLM (ChatGPT family) evaluated on the same 108-question biology exam; performed well but below GPT-4 and showed higher variability across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned conversational model (fine-tuned for chat/conversation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answering a 108-question multiple-choice conceptual biology exam (text-based reasoning simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot style single-instruction prompts (five differently worded prompts requesting single-letter answer without justification); no few-shot examples or chain-of-thought reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number correct (score out of 108) across five trials; standard deviation across trials and inter-trial correlation; per-question correct-occurrence distribution vs random-guess Poisson expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average score reported in the paper in the 70–80 point range (exact mean not individually listed); per-trial correct rate reported as >60% correct in each trial in the degree-of-confidence analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to random-guess Poisson expectation and to stronger model GPT-4; companion models PaLM2/Claude2/SenseNova served as peer baselines (similar 70–80 point regime).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt phrasing sensitivity (prompt variations influenced answer patterns)', 'Model capacity relative to GPT-4', 'Question subdomain (category-specific performance differences)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Five trials with different prompt wordings; responses restricted to single-letter answers; no sampling/hyperparameter settings reported; scoring based on expert-reviewed 108-question bank.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>More variable across prompts than GPT-4 (higher standard deviation reported); lower average accuracy than GPT-4; only multiple-choice conceptual evaluation — not validated for open-ended biological tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7338.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7338.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathways Language Model 2 (PaLM2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's general-purpose multilingual reasoning LLM evaluated on the 108-question biology exam; achieved performance comparable to other non-GPT-4 models but lower than GPT-4 and showed lower confidence in degree-of-confidence analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>General-purpose pretrained model (multilingual, strong reasoning capabilities per provider descriptions); treated as a base/instruction-following model in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of biological reasoning via a 108-question multiple-choice exam.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot single-instruction prompts (5 differently phrased prompts requesting single-letter answers without explanation); no few-shot or chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Score out of 108 (average across five trials), standard deviation across trials, inter-trial correlation, per-question correct-occurrence distribution compared to random-guess Poisson expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average reported in the paper in the 70–80 point range; degree-of-confidence analysis indicated PaLM2 and SenseNova were 'less confident', getting over 50% of questions correct in each trial (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against random-guess expected distribution (Poisson) and peer LLMs (GPT-4, GPT-3.5, Claude2, SenseNova); peer average ~70–80 points used for relative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt phrasing / prompt engineering (sensitivity to prompt wording)', 'Relative model capability (described lower than GPT-4)', 'Question subdomain differences']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Five prompt-variant trials with single-letter output constraint; no sampling hyperparameters or model-size details provided; scoring on the 108-question expert-reviewed bank.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower confidence and lower per-trial correctness than GPT-4; classified as 'less confident' by degree-of-confidence metric; results limited to multiple-choice conceptual questions and lack hyperparameter details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7338.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7338.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's conversational LLM with large context capabilities evaluated on the 108-question biology exam; performed comparably to GPT-3.5/PaLM2 but below GPT-4, with intermediate confidence levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Conversational/instruction-following model (emphasized for long-context conversation/memory in provider description)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answering a 108-question multiple-choice conceptual biology exam as a text-based reasoning task.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot single-instruction prompts (5 different prompt formulations requesting a single-letter answer without explanation); no few-shot examples or chain-of-thought used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number correct out of 108 across five trials; standard deviation and inter-trial correlation; per-question correct-occurrence distribution compared to Poisson random-guess expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average score reported in the 70–80 point range; degree-of-confidence analysis indicated GPT-3.5 and Claude2 obtained over 60% of questions correct in each trial (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to random-guess Poisson expectation and to peer LLMs (GPT-4, GPT-3.5, PaLM2, SenseNova); peer average ~70–80 points used for relative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt phrasing / prompt engineering sensitivity', 'Model capability relative to GPT-4', 'Question category effects']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations performed over five prompt-variant trials; responses constrained to single-letter answers; no sampling/hyperparameter settings reported; question bank was expert-reviewed 108 items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Intermediate performance and variability across prompts; tested only on conceptual multiple-choice questions, not on more applied or open-ended biological simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7338.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7338.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SenseNova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SenseNova (SenseChat / SenseTime foundational model set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SenseTime's foundation-model-based LLM (SenseNova / SenseChat) evaluated on the 108-question biology exam; performed similarly to PaLM2 with lower per-trial confidence than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SenseNova</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Foundation model set (large pretrained model, provider-descriptions emphasize Chinese-context training and downstream tools); treated as a general instruction-following LLM in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation: answer a 108-question multiple-choice conceptual biology exam.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot single-instruction prompts (5 differently worded prompts requesting single-letter answers without explanation); no few-shot examples or chain-of-thought prompting used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Score out of 108 averaged over five trials, standard deviation across trials, inter-trial correlation, and per-question correct-occurrence distribution compared to random-guess Poisson expectation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average reported by authors in the 70–80 point range; degree-of-confidence analysis described SenseNova as 'less confident', getting over 50% of questions correct in each trial (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to random-guess expected distribution and to peer LLMs (GPT-4 highest, others in 70–80 range); random-guess baseline employed in degree-of-confidence analysis but not numerically defined.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt phrasing sensitivity', 'Model training/data emphasis (reported provider emphasis on Chinese-context datasets)', 'Question subdomain effects']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Five trials with different prompt wordings; responses constrained to a single-letter output only; no sampling or temperature parameters reported; used the same 108-question expert-reviewed bank.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower per-trial confidence and accuracy compared to GPT-4; sensitivity to prompt phrasing; evaluation limited to multiple-choice conceptual questions rather than open-ended or experimental biology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Palm 2 technical report <em>(Rating: 2)</em></li>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 2)</em></li>
                <li>ProtGPT2 is a deep unsupervised language model for protein design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7338",
    "paper_id": "paper-265157573",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "State-of-the-art multimodal large language model from OpenAI evaluated as a text-based simulator for conceptual biology reasoning by answering a 108-question multiple-choice exam; showed highest accuracy and strongest consistency across prompt variants.",
            "citation_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "Generative pretrained multimodal transformer (described as large/capable); treated as an instruction-following conversational model in experiments",
            "scientific_domain": "Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)",
            "simulation_task_description": "Text-based simulation of biological reasoning: answer a 108-question multiple-choice conceptual biology exam covering molecular biology, techniques, metabolic engineering, and synthetic biology.",
            "prompting_strategy": "Zero-shot style prompting using 5 differently worded single-instruction prompts; each prompt requested a single-letter best answer with no justification (no few-shot examples, no chain-of-thought, no tool use, constrained-output format).",
            "evaluation_metric": "Number correct (score on 108-question exam), mean score across five trials, standard deviation across trials, inter-trial correlation, and per-question correct-occurrence distribution compared to expected random-guess (Poisson) — reported as degree-of-confidence analysis.",
            "reported_accuracy": "Average score = 90 points on 108-question exam (reported by paper) — stated as &gt;80% correct per trial and &lt;10% incorrect per trial; per-category reported means: metabolic engineering mean = 100 points, synthetic biology mean = 92 points (as reported in the paper).",
            "baseline_accuracy": "Compared against random-guess expected distribution (Poisson) for degree-of-confidence analysis and against other LLMs tested (GPT-3.5, PaLM2, Claude2, SenseNova) whose average scores lay in the 70–80 point range; random-guess exact baseline not numerically specified in paper.",
            "factors_reported": [
                "Prompt phrasing / prompt engineering (five differently worded prompts affected other models more than GPT-4)",
                "Model capability/capacity (GPT-4 described as larger/more capable)",
                "Question subdomain (performance varied by category; GPT-4 excelled in metabolic engineering and synthetic biology)",
                "Consistency/stability across repeated prompts (measured by standard deviation and inter-trial correlation)"
            ],
            "experimental_conditions": "Five independent trials per model using five predefined prompt variants; each trial: full 108-question exam; responses constrained to single-letter answer only; no chain-of-thought or explanatory output allowed; no sampling hyperparameters (temperature/top-k) reported.",
            "limitations_or_failure_modes": "Evaluated only on multiple-choice conceptual questions (not open-ended design or experimental tasks); lacks reported hyperparameter settings; further validation and domain-specific fine-tuning required per authors; other models showed greater sensitivity to prompt phrasing, indicating potential failure modes under prompt variation.",
            "uuid": "e7338.0",
            "source_info": {
                "paper_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "OpenAI conversational/instruction-tuned LLM (ChatGPT family) evaluated on the same 108-question biology exam; performed well but below GPT-4 and showed higher variability across prompts.",
            "citation_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "model_type": "Instruction-tuned conversational model (fine-tuned for chat/conversation)",
            "scientific_domain": "Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)",
            "simulation_task_description": "Answering a 108-question multiple-choice conceptual biology exam (text-based reasoning simulation).",
            "prompting_strategy": "Zero-shot style single-instruction prompts (five differently worded prompts requesting single-letter answer without justification); no few-shot examples or chain-of-thought reported.",
            "evaluation_metric": "Number correct (score out of 108) across five trials; standard deviation across trials and inter-trial correlation; per-question correct-occurrence distribution vs random-guess Poisson expectation.",
            "reported_accuracy": "Average score reported in the paper in the 70–80 point range (exact mean not individually listed); per-trial correct rate reported as &gt;60% correct in each trial in the degree-of-confidence analysis.",
            "baseline_accuracy": "Compared to random-guess Poisson expectation and to stronger model GPT-4; companion models PaLM2/Claude2/SenseNova served as peer baselines (similar 70–80 point regime).",
            "factors_reported": [
                "Prompt phrasing sensitivity (prompt variations influenced answer patterns)",
                "Model capacity relative to GPT-4",
                "Question subdomain (category-specific performance differences)"
            ],
            "experimental_conditions": "Five trials with different prompt wordings; responses restricted to single-letter answers; no sampling/hyperparameter settings reported; scoring based on expert-reviewed 108-question bank.",
            "limitations_or_failure_modes": "More variable across prompts than GPT-4 (higher standard deviation reported); lower average accuracy than GPT-4; only multiple-choice conceptual evaluation — not validated for open-ended biological tasks.",
            "uuid": "e7338.1",
            "source_info": {
                "paper_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "Pathways Language Model 2 (PaLM2)",
            "brief_description": "Google's general-purpose multilingual reasoning LLM evaluated on the 108-question biology exam; achieved performance comparable to other non-GPT-4 models but lower than GPT-4 and showed lower confidence in degree-of-confidence analysis.",
            "citation_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_size": null,
            "model_type": "General-purpose pretrained model (multilingual, strong reasoning capabilities per provider descriptions); treated as a base/instruction-following model in experiments",
            "scientific_domain": "Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)",
            "simulation_task_description": "Text-based simulation of biological reasoning via a 108-question multiple-choice exam.",
            "prompting_strategy": "Zero-shot single-instruction prompts (5 differently phrased prompts requesting single-letter answers without explanation); no few-shot or chain-of-thought prompting.",
            "evaluation_metric": "Score out of 108 (average across five trials), standard deviation across trials, inter-trial correlation, per-question correct-occurrence distribution compared to random-guess Poisson expectation.",
            "reported_accuracy": "Average reported in the paper in the 70–80 point range; degree-of-confidence analysis indicated PaLM2 and SenseNova were 'less confident', getting over 50% of questions correct in each trial (as reported).",
            "baseline_accuracy": "Compared against random-guess expected distribution (Poisson) and peer LLMs (GPT-4, GPT-3.5, Claude2, SenseNova); peer average ~70–80 points used for relative baseline.",
            "factors_reported": [
                "Prompt phrasing / prompt engineering (sensitivity to prompt wording)",
                "Relative model capability (described lower than GPT-4)",
                "Question subdomain differences"
            ],
            "experimental_conditions": "Five prompt-variant trials with single-letter output constraint; no sampling hyperparameters or model-size details provided; scoring on the 108-question expert-reviewed bank.",
            "limitations_or_failure_modes": "Lower confidence and lower per-trial correctness than GPT-4; classified as 'less confident' by degree-of-confidence metric; results limited to multiple-choice conceptual questions and lack hyperparameter details.",
            "uuid": "e7338.2",
            "source_info": {
                "paper_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Claude 2",
            "name_full": "Claude 2",
            "brief_description": "Anthropic's conversational LLM with large context capabilities evaluated on the 108-question biology exam; performed comparably to GPT-3.5/PaLM2 but below GPT-4, with intermediate confidence levels.",
            "citation_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "mention_or_use": "use",
            "model_name": "Claude2",
            "model_size": null,
            "model_type": "Conversational/instruction-following model (emphasized for long-context conversation/memory in provider description)",
            "scientific_domain": "Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)",
            "simulation_task_description": "Answering a 108-question multiple-choice conceptual biology exam as a text-based reasoning task.",
            "prompting_strategy": "Zero-shot single-instruction prompts (5 different prompt formulations requesting a single-letter answer without explanation); no few-shot examples or chain-of-thought used.",
            "evaluation_metric": "Number correct out of 108 across five trials; standard deviation and inter-trial correlation; per-question correct-occurrence distribution compared to Poisson random-guess expectation.",
            "reported_accuracy": "Average score reported in the 70–80 point range; degree-of-confidence analysis indicated GPT-3.5 and Claude2 obtained over 60% of questions correct in each trial (as reported).",
            "baseline_accuracy": "Compared to random-guess Poisson expectation and to peer LLMs (GPT-4, GPT-3.5, PaLM2, SenseNova); peer average ~70–80 points used for relative baseline.",
            "factors_reported": [
                "Prompt phrasing / prompt engineering sensitivity",
                "Model capability relative to GPT-4",
                "Question category effects"
            ],
            "experimental_conditions": "Evaluations performed over five prompt-variant trials; responses constrained to single-letter answers; no sampling/hyperparameter settings reported; question bank was expert-reviewed 108 items.",
            "limitations_or_failure_modes": "Intermediate performance and variability across prompts; tested only on conceptual multiple-choice questions, not on more applied or open-ended biological simulation tasks.",
            "uuid": "e7338.3",
            "source_info": {
                "paper_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SenseNova",
            "name_full": "SenseNova (SenseChat / SenseTime foundational model set)",
            "brief_description": "SenseTime's foundation-model-based LLM (SenseNova / SenseChat) evaluated on the 108-question biology exam; performed similarly to PaLM2 with lower per-trial confidence than GPT-4.",
            "citation_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "mention_or_use": "use",
            "model_name": "SenseNova",
            "model_size": null,
            "model_type": "Foundation model set (large pretrained model, provider-descriptions emphasize Chinese-context training and downstream tools); treated as a general instruction-following LLM in experiments",
            "scientific_domain": "Biology (conceptual molecular biology, molecular biology techniques, metabolic engineering, synthetic biology)",
            "simulation_task_description": "Text-based simulation: answer a 108-question multiple-choice conceptual biology exam.",
            "prompting_strategy": "Zero-shot single-instruction prompts (5 differently worded prompts requesting single-letter answers without explanation); no few-shot examples or chain-of-thought prompting used.",
            "evaluation_metric": "Score out of 108 averaged over five trials, standard deviation across trials, inter-trial correlation, and per-question correct-occurrence distribution compared to random-guess Poisson expectation.",
            "reported_accuracy": "Average reported by authors in the 70–80 point range; degree-of-confidence analysis described SenseNova as 'less confident', getting over 50% of questions correct in each trial (as reported).",
            "baseline_accuracy": "Compared to random-guess expected distribution and to peer LLMs (GPT-4 highest, others in 70–80 range); random-guess baseline employed in degree-of-confidence analysis but not numerically defined.",
            "factors_reported": [
                "Prompt phrasing sensitivity",
                "Model training/data emphasis (reported provider emphasis on Chinese-context datasets)",
                "Question subdomain effects"
            ],
            "experimental_conditions": "Five trials with different prompt wordings; responses constrained to a single-letter output only; no sampling or temperature parameters reported; used the same 108-question expert-reviewed bank.",
            "limitations_or_failure_modes": "Lower per-trial confidence and accuracy compared to GPT-4; sensitivity to prompt phrasing; evaluation limited to multiple-choice conceptual questions rather than open-ended or experimental biology tasks.",
            "uuid": "e7338.4",
            "source_info": {
                "paper_title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Palm 2 technical report",
            "rating": 2,
            "sanitized_title": "palm_2_technical_report"
        },
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2,
            "sanitized_title": "language_models_can_learn_complex_molecular_distributions"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 2,
            "sanitized_title": "towards_expertlevel_medical_question_answering_with_large_language_models"
        },
        {
            "paper_title": "ProtGPT2 is a deep unsupervised language model for protein design",
            "rating": 1,
            "sanitized_title": "protgpt2_is_a_deep_unsupervised_language_model_for_protein_design"
        }
    ],
    "cost": 0.012165249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions</p>
<p>Xinyu Gong 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Jason Holmes 
Department of Radiation Oncology
Mayo Clinic
85054PhoenixAZUnited States</p>
<p>Yiwei Li 
School of Computing
The University of Georgia
30602AthensGAUnited States</p>
<p>Zhengliang Liu tliu@uga.edu 
School of Computing
The University of Georgia
30602AthensGAUnited States</p>
<p>Qi Gan 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Zihao Wu 
School of Computing
The University of Georgia
30602AthensGAUnited States</p>
<p>Jianli Zhang 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Yusong Zou 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Yuxi Teng 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Tian Jiang 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Hongtu Zhu 
Department of Biostatistics
University of North Carolina at Chapel Hill
Chapel Hill27599NCUnited States</p>
<p>Wei Liu liu.wei@mayo.edu 
Department of Radiation Oncology
Mayo Clinic
85054PhoenixAZUnited States</p>
<p>Tianming Liu 
School of Computing
The University of Georgia
30602AthensGAUnited States</p>
<p>Yajun Yan yajunyan@uga.edu 
School of Chemical
Materials and Biomedical Engineering
College of Engineering
The University of Georgia
30602AthensGAUnited States</p>
<p>Mayo Clinic
5881 E Mayo Blvd85054PhoenixAZUnited States</p>
<p>420 Boyd Research and Education Center
The University of Georgia
30602AthensGAUnited States</p>
<p>The University of Georgia
302 East Campus Road30602AthensGAUnited States</p>
<p>Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions
99E970FDBF62E413764BDF51E0B18858
Recent advances in Large Language Models (LLMs) have presented new opportunities for integrating Artificial General Intelligence (AGI) into biological research and education.This study evaluated the capabilities of leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in answering conceptual biology questions.The models were tested on a 108question multiple-choice exam covering biology topics in molecular biology, biological techniques, metabolic engineering, and synthetic biology.Among the models, GPT-4 achieved the highest average score of 90 and demonstrated the greatest consistency across trials with different prompts.The results indicated GPT-4's proficiency in logical reasoning and its potential to aid biology research through capabilities like data analysis, hypothesis generation, and knowledge integration.However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.</p>
<p>Introduction</p>
<p>In recent years, the field of biology has undergone a remarkable explosion of information and knowledge.Advanced sequencing technologies have efficiently generated vast datasets to underpin significant scientific discoveries [1,2].Meanwhile, CRISPR technology has revolutionized our capacity to precisely edit DNA in diverse organisms, greatly expediting our comprehension of gene functions and mutations [3,4].As cryo-electron microscopy continues to facilitate the elucidation of protein function and mechanisms, AI-assisted protein prediction is beginning to make noteworthy contributions [5,6].Collectively, these breakthroughs have deepened our understanding of life sciences and fueled the development of novel therapeutics, sustainable production methods, and more [7][8][9][10][11].On the other hand, given the mounting data volume and the growing interdisciplinary complexity of knowledge, the accurate extraction, analysis, and integration of extensive biological information have become increasingly critical for fully harnessing the potential of these advancements and driving future innovations [12][13][14].In this pursuit, adopting cutting-edge computational methods holds immense promise.</p>
<p>Recent advancements in artificial intelligence techniques hold promise for transforming biological research methods and uncovering new findings, given AI's documented success in addressing other complex scientific challenges.Large Language Models (LLMs) have emerged as one of the most promising approaches for solving biological problems.In some cases, LLMs could serve as powerful tools for generating protein sequences or as effective methods for acquiring 'contentaware' data representations from extensive sequence datasets using deep learning [15].Applying these models enables gleaning insights from naturally occurring protein sequences encompassing known sequence diversity, while combining pre-existing structural and functional knowledge through multi-task learning [16,17].In addition, LLMs have been trained in the distribution of multiple biological molecules.By constructing training sets of more complex molecules, Flam-Shepherd et al. introduced a series of generative modeling tasks and demonstrated the power of LLMs in learning immense molecular distributions compared to most graph generative models [18].Surprisingly, to overcome the real-world challenges of medical applications with LLMs, MultiMedQA, a benchmark comprising six medical question-answering datasets, was coupled with PaLM (Pathways Language Model) and evaluated as a helpful tool for clinical applications [19].So far, LLMs have shown exciting applications in biology, allowing us to revolutionize various biological aspects by enabling efficient data analysis, hypothesis generation, drug discovery, and knowledge integration, ultimately accelerating advancements.</p>
<p>Nevertheless, the potential of LLMs remains largely unexplored as continuous improvements in parameters, computing capabilities, datasets, and domain fine-tuning consistently expand their capabilities.</p>
<p>Biology can be interpreted through its own intrinsic language.In genomic sequences, scientists have observed and summarized certain patterns of DNA representing coding regions with certain functions [20].In the language of genome, the gene family is the "word", and the coding region forms the "sentence".By analyzing and learning identified functional DNA sequences, algorithms can predict functions or families of input DNA sequences [21].In the protein language, the grammar logic mirrors natural language.The amino acid sequence is the "letter", and the secondary structure forms the "word".The folded 3D protein structure is the "sentence", and the protein function convey the "meaning" [22].Natural Language Processing (NLP) follows the grammar and logic of human language.It has potential to understand and speak biological language by embedding biological domains and fine-tuning LLMs.In this study, we designed 108 biological multiple-choice questions and tested the state-of-art LLMs, including GPT-3.5, GPT-4, PaLM2, Claude2, and SenseNova, for their capabilities in analyzing, solving, and answering biological questions by taking the exam.Among all tested LLMs, GPT-4 outperformed other LLMs by scoring an average score of 90 on the exam.The results indicated the supreme capability of GPT-4 in reasoning, analyzing, and answering biological questions, paving the way for LLMs to benefit biology research and education.</p>
<p>Related Work</p>
<p>Large Language Models</p>
<p>Natural Language Processing (NLP) integrates the subjects of linguistics and computer science to enable machines to learn, understand, and generate human language, serving as a crucial approach to achieving communication between humans and machines.To develop sophisticated language models, these models are trained on diverse linguistic datasets like literature, fiction, poetry, and research papers to analyze the linguistic structure and learn natural language usage [23].In recent decades, advancements in computing power, high-quality datasets, and deep learning have enabled successful NLP applications like machine translation, sentiment analysis, information retrieval, and question-answering.As ChatGPT was unveiled in 2020, it caused a sensation across fields and boosted the development and launch of several LLMs [24].Tech companies and scientific researchers now aim to create increasingly capable language models to incorporate into commercial products or scientific studies.Artificial General Intelligence (AGI) makes it straightforward to connect and implement these models [25].</p>
<p>Generative Pre-trained Transformer 3.5 (GPT-3.5)was developed by OpenAI and was first released in the chatbot, ChatGPT, enabling Artificial Intelligent Content Generation (AIGC) and human-like interaction.ChatGPT can perform different types of tasks, deliver fast responses, and provide comprehensive answers [26].However, accuracy remains a concern.Recently, GPT-4 has emerged as the most advanced model of OpenAI.GPT-4 supersedes ChatGPT as a more reliable and creative version in reasoning capabilities.For example, GPT-4 achieved higher approximate percentiles (99th) in the Biology Olympiad than ChatGPT (31st) [27].GPT-4 continues to improve through human feedback and real-world use.Alongside GPT, other competitive models are rushing out.Pathway Language Model 2 (PaLM2) is Google's latest multilingual model, which is highly capable of logical reasoning, coding, and mathematics while supporting over 100 different languages.PaLM API builds on PaLM2 and is fast at responding [28].Claude 2 was developed by Anthropic as a rival chatbot to ChatGPT, mainly focusing on textual tasks with large context sizes of up to 100k tokens.It can work over hundreds of pages of documents or even books [29].Recently, SenseNova was released as an emerging model capable of NLP, content generation, automated data annotation, and custom model training.</p>
<p>SenseNova is the foundation model set of SenseTime with billions of parameters, working as the building block of their "foundation models + large-scale computing" systems [30].</p>
<p>Prompt Engineering</p>
<p>NLP models have occupied places in a range of domains such as medical consultation and code generation due to their powerful capability of processing natural languages [31,32].However, answering professional questions or performing research tasks requires extensive data and specific instructions to complete complex assignments.Recent studies show that employing proper prompts can efficiently yield high-quality results while saving computing time and resources [33].Prompts are instructions that guide LLMs by providing customized rules and output formats.Well-designed prompts can enhance LLMs' capabilities and even enable solving sophisticated problems in specialized fields [34].Thus, prompt engineering is critical for facilitating interaction between users and LLMs.</p>
<p>Since the quality of outputs is highly dependent on the quality of prompts, there is a growing demand for prompt engineering principles and skills.Besides, it remains challenging to efficiently engineer and fine-tune appropriate prompts in highly specialized domains.Prompts can be generated manually or automatically.Given that many complicated tasks require a huge number of prompts as guidance, adopting prompt functions to automatically generate prompt set and templates can improve working efficiency [35].Several studies have addressed the significance of prompt engineering and proposed a series of catalogs and templates [33,35,36].In this study, we presented a set of prompts and a chain of thoughts based on our examination in biology.</p>
<p>The Interdisciplinary of LLMs</p>
<p>The intersection of LLMs and various scientific domains, such as biological, medication, and ecology, etc., has sparked an interdisciplinary fusion with promising potential.In the current landscape, numerous surveys and initiatives are underway to explore the synergies between LLMs and other fields [19,37,38].The integration of LLMs with biology is unraveling intricate insights within genomics, transcriptomics, and epigenomics [39].By deciphering complex biological datasets and molecular interactions, LLMs' applications are advancing bioinformatics and enhancing our understanding of fundamental biological processes [40].Researchers are diligently fine-tuning LLMs for biological contents, ensuring accurate comprehension of domainspecific terms and context.In addition to reading biology contexts, well-trained LLMs can also generate innovative small molecule distributions, nucleic acid sequences, and proteins with desired structures or functions.The surge in collaborative initiatives between computer scientists and biologists is cultivating a symbiotic learning process.The synergy, with the expanding reservoir of biological knowledge and the continuously evolving capabilities of LLMs, anticipates a future where interdisciplinary achievements are poised to drive revolutionary scientific progress.</p>
<p>In the field of medication, LLMs are aiding drug discovery by swiftly analyzing vast volumes of biomedical literature to explore and uncover hidden relationships between genes, proteins, and diseases.The combination of language processing with biological insights is also enabling personalized medicine, with the analysis of LLMs for individual patient data to predict optimal treatments.Med-PaLM performs encouragingly on the axes of our pilot human evaluation framework [19].LLMs have emerged as a driving force in medical AI, holding significant promise to enhance the efficiency of clinical, educational, and research activities [38].Several prominent chatbots, such as LaMDA (Google), GPT-3.5 (OpenAI) along with GPT-4 (OpenAI), have been extensively explored for medical applications [41].For instance, GPT-4's capabilities have been investigated in generating medical notes from physician-patient transcripts have provided insights into innate medical knowledge, facilitating medical consultations, aiding in diagnoses, and contributing to medical education [41].Beyond medicine, the interdisciplinary influence of LLMs extends to ecology and environmental studies by enabling data analysis to decipher intricate ecosystems and predict environmental changes [42].Although the results are promising, these scientific domains are complex and growing.As LLMs maintain their significant involvement in research and everyday applications, assessing their performance gains heightened importance, particularly focusing on aspects such as safety, fairness, and bias [43].</p>
<p>Methods</p>
<p>Question Design</p>
<p>We developed a bank of 108 multiple-choice questions to assess conceptual understanding and reasoning abilities of LLMs in biology.The questions spanned key topics including:</p>
<p>• Fundamental molecular biology (59 questions): Key concepts such as DNA replication, transcription, translation, gene expression regulation, mutagenesis, etc.</p>
<p>• Common molecular biology techniques (17 questions): Principles and applications of biological techniques like PCR, DNA sequencing, DNA cloning, blotting methods, etc.</p>
<p>• Metabolic engineering (15 questions): Engineering microbial metabolism for bioproduction, metabolic flux analysis, optimizing pathway expression, etc.</p>
<p>• Synthetic biology (17 questions): Principles of genetic circuit design, toggle switches, oscillators, logic gates, CRISPR-based genome editing, etc.</p>
<p>The questions were designed to be intellectually challenging without relying on obscure facts.</p>
<p>Questions went through expert review by two biology professors to confirm accuracy, relevance, and appropriate difficulty level.</p>
<p>Models Evaluation</p>
<p>We evaluated the performance of the following LLMs on the biology questions: GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova.These models represent the current state-of-the-art language model capabilities.</p>
<p>• GPT-4 [27] (OpenAI): One of the largest most capable generative pretrained multimodal models built on the transformer architecture, released by OpenAI on March 23, 2023.</p>
<p>GPT-4 surpasses its predecessor GPT-3.5 in size and capability, delivering more coherent, contextually relevant, and nuanced outputs.GPT-4 has demonstrated human-level performance across several academic benchmarks, even achieving a score in the top 10% on a simulated bar exam.Notably, GPT-4 can process both text and images, extracting complex content and relationships from visual input.</p>
<p>• GPT-3.5 (OpenAI): Also known as ChatGPT, GPT-3.5 was released by OpenAI in November 2022.GPT-3.5 is a precursor of GPT-4 without image understanding abilities.</p>
<p>Focused on and fine-tuned for conversational data, GPT-3.5 excels at generating dialogue responses.Its rapid global adoption garnered significant attention, not only from the NLP community but also across various scenarios, and it's continuously evolving for enhanced reasoning and safety.</p>
<p>Testing Procedures</p>
<p>Each LLM was given the full set of 108 multiple choice biology questions.To account for variability due to prompt phrasing, each model was tested in 5 trials with differently worded prompts (Table 1), respectively.The prompts explained that the model should provide the single best answer for each question.</p>
<p>Table 1 The prompts used in each trail</p>
<p>Trial Prompts</p>
<p>Trial 1</p>
<p>I am going to ask a multiple-choice question on a biological topic for the purpose of evaluating your understanding of the medical field.</p>
<p>For the following question, provide the correct answer, one letter choice, without any justification.</p>
<p>Trial 2 I want to evaluate your understanding of a biological topic by asking some multiple choice questions.Please answer the following question, designed for the purpose of evaluating your understanding of the topic.Provide the most correct letter choice (only one letter) without explanation.</p>
<p>Trial 3 I'm a researcher that is trying to figure out how intelligent you are.Answer the following question as best you can.Return the best letter choice with no explanation.</p>
<p>Trial 4</p>
<p>Please answer the following practice question as if you were a resident in training for a biological exam.Only give the correct answer in your response, a single letter choice.Do not explain your answer.</p>
<p>Trial 5</p>
<p>We want to test your knowledge about the biological field.For this reason, we have created some questions to ask you.In your response, only report the correct answer (one letter) without explanation.</p>
<p>Scoring and Analysis</p>
<p>In each trial, the number of questions answered correctly was recorded and scored for each LLM.</p>
<p>Additionally, the consistency of each model's answers across the 5 trials was evaluated by comparing the specific questions answered correctly.Mean scores and variability across trials were calculated and graphed for each model.Comparative performance was assessed based on both aggregate scores and answer stability.</p>
<p>Results</p>
<p>Comparison between different LLMs in performing biology multiple-choice exam</p>
<p>The development of LLMs has demonstrated a significant impact across diverse areas, including education [46], medication [47], and agriculture [48].The breakthrough of applying LLMs in other fields depends on embedding and fine-tuning the specific domains into the models [49].As the reasoning capability was improved, the state-of-the-art LLMs gradually showed enhanced ability in analyzing and solving different types of questions and were able to substitute humans for some simple tasks.The multiple-choice section is an important approach for validating the mathematical and logical reasoning skills of LLMs.To test the capabilities of the current competitive LLMs for biological reasoning, we designed over 100 biological multiple-choice questions covering fundamental molecular biology concepts, common molecular biology techniques, metabolic engineering, and synthetic biology.In this study, we tested five popular models, namely GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, benchmarking their reasoning capabilities in the biology field.We conducted five trials with different-phrased prompts to mitigate prompt engineering effects on each model.In each trial, the performance of each model was shown in parallel, and the correct answers were marked in colored square (Figure 1a).The results indicated that all models could deal with most of the questions, but GPT-4 consistently outperformed other models by getting more colored markers.Remarkably, among all five trials, GPT-4 presented the most stable pattern of marked correct answers than other counterparts.Additionally, our initial analysis of the five LLMs across five trials revealed overall satisfactory performance, with all models scoring above 70 points on average (Figure 1b).However, GPT-4</p>
<p>clearly stood out with a significantly higher average score of 90 points compared to the 70-80point range of the other models.To further analyze consistency, we calculated the standard deviation and inter-trial correlation for each model (Figure 2).GPT-3.5, PaLM2, Claude2, and SenseNova performed comparably, with standard deviation of 0.4-0.45 and correlation of 0.57-0.67.In contrast, GPT-4 demonstrated higher confidence and consistency, with a lower standard deviation of 0.30 and a higher correlation of 0.87.Next, since we categorized the questions into four subtypes, the average score and standard deviation were also calculated to evaluate the capabilities of LLMs in different types of questions (Figure 3).Overall, GPT-4 outperformed the other LLMs across all subcategories.Notably, GPT-4 was especially good at reasoning questions in metabolic engineering and synthetic biology, achieving 100 mean points and 92 mean points, respectively (Figure 3a).Meanwhile, GPT-4 presented high consistency by showing lower standard deviation than the counterparts under each subcategory (Figure 3b).These results indicated GPT-4's superior confidence and alignment in reasoning biology questions.To evaluate the degree of confidence in the answers given by the LLMs, we compared the results of correct answer occurrences per question for each model with the expected distribution that would occur if the models were guessing at random (Figure 4).All LLMs showed trends opposite to the random guessing.In comparison, PaLM2 and SenseNova were less confident, getting over 50% of questions correct in each trial (Figure 4c and Figure 4e).GPT 3.5 and Claude 2 were more confident, getting over 60% of questions correct in each trial (Figure 4b and Figure 4d).</p>
<p>GPT-4 was undoubtedly the most confident model, getting over 80% of questions correct in each trial and less than 10% of questions incorrect (Figure 4a).Through this degree of confidence test, GPT-4 demonstrated proficient biological reasoning, indicating the potential in biology applications.</p>
<p>Discussion</p>
<p>Faced with energy shortage, environmental problems, and healthcare issues, biological strategies are blooming solutions to achieve bio-fuel production, pharmaceutical synthesis, and disease diagnosis.Recent advances in computational methods and their integration with biology have accelerated research progress in fields such as genome mining, protein structure prediction, and protein engineering.We can foresee the bright future of AI-aided biology.LLMs present opportunities to enhance understanding and interpretation of biological language.Fine-tuning LLMs on domain-specific corpora enables applications in aiding rational biological experimental design and providing foundational biological knowledge to non-experts seeking training.However, as an emerging field, our current comprehension of biology using AI remains limited in depth and scope.Biological knowledge continuously expands at a rapid pace, making it difficult to immediately develop specialized AI systems that fully capture the nuances of the field.Therefore, continually training LLMs on high-quality biological datasets through a lifelong learning approach may maximize their utility for advancing scientific research and education.</p>
<p>In this study, evaluation of answer patterns revealed GPT-4 had the highest confidence in its responses, getting over 80% of questions correct per trial.In contrast, the other models showed lower confidence levels.GPT-4's high confidence indicates its suitability for deploying in applications requiring reliable outputs.On the other hand, testing prompt variations for each model showed GPT-4 had the most consistent performance, underscoring the importance of prompt engineering to optimize LLMs.Further research into prompt design strategies tailored for biology questions would be beneficial.</p>
<p>Currently, the language processing capabilities of LLMs could aid the analysis of large-scale biological datasets like genomics, transcriptomics, and proteomics.LLMs can help extract insights from massive amounts of sequencing data as well as integrate and interpret findings across LLMs specialized in biology promises to catalyze discovery across diverse research areas and make biological insights more accessible.Overall, the creative knowledge integration abilities of LLMs could enable breakthroughs in biological understanding and experimentation exceeding human limitations.</p>
<p>Conclusion</p>
<p>This study illuminates the emerging potential of LLMs to advance biology research and education through their language processing capabilities.The leading LLM, GPT-4, exhibited proficiency in logical reasoning applied to conceptual biology questions, outperforming other state-of-the-art models.While further validation is required, the results point to exciting possibilities for LLMs to accelerate knowledge discovery by analyzing massive biological datasets, streamlining literature analysis, improving molecular engineering, broadening access to biology education, and synergizing with human capabilities.With proper oversight and governance to ensure safety and ethics, harnessing LLMs to complement human intelligence could catalyze breakthroughs, leading to advancements in biomedicine, biotechnology, bioengineering, and sustainability.</p>
<p>Further interdisciplinary research that brings together biologists, computer scientists, and experts across fields will be key to transforming these potentials into realities that benefit science and society.</p>
<p>•</p>
<p>PaLM2 [44] (Google): Released by Google on May 23, 2023, PaLM2 is a general-purpose model that builds upon its predecessor, PaLM.Utilizing the Pathways model architecture, PaLM2 was trained on an expansive multilingual and diverse pre-training dataset encompassing human and programming languages, mathematical equations, scientific papers, and web content.It exhibits sophisticated reasoning capabilities across multiple languages and has spawned variants such as Med-PaLM 2 [45] and Sec-PaLM.• Claude2 (Anthropic): Released by Anthropic on July 23, 2023, Claude2 is the successor to the Claude model, tailored for conversational applications.Its distinguishing feature is its extensive memory, capable of inputting up to 100k tokens, encompassing lengthy technical documentation or even entire books.• SenseChat/SenseNova (SenseTime): Launched by Chinese AI company SenseTime on March 23, 2023, SenseChat builds on the SenseNova foundational model set.Trained extensively on a large-scale dataset with a particular emphasis on the Chinese context, it offers superior proficiency in processing texts.Additionally, SenseChat powers several innovative tools, including programming assistants, health consultation aids, and PDF reading assistants.</p>
<p>Figure 1
1
Figure 1 Overall performance of five LLMs in the biological exam.(a) Raw data for each LLM with five trails with different-phrased prompts, where the rows represent separate trails, and the columns represent the test questions.Color shaded squares indicate correct answers, while white</p>
<p>Figure 2
2
Figure 2 Quantification of the overall consistency of scoring.(a) Standard deviation of total scores for each LLM across five trails.(b) Average correlation between trials for each LLM.</p>
<p>Figure 3
3
Figure 3 The performance of five LLMs by category.(a) Average scores for each LLM by category.(b) Standard deviation for each LLM by category.</p>
<p>Figure 4
4
Figure 4 Quantification the degree of confidence in the answers.The blue bars in figures (ae) describe the number of correct answer occurrences per-question for each LLM.The dashed red lines in figures (a-e) present the expected number of correct answers in 5 trials, when randomly guessing based on the Poisson distribution.(a) degree of confidence test of GPT-4.(b) degree of confidence test of GPT-3.5.(c) degree of confidence test of PaLM2.(d) degree of confidence test of Claude 2. (e) degree of confidence test of SenseNova.</p>
<p>datasets.Fine-tuning biology corpora can teach LLMs the language of biological data.The applications of LLMs in protein design leverage the understandings of sequence-function relationships and design principles.LLMs can propose solutions to engineering challenges and accelerate iterative design cycles.They may also uncover novel strategies human engineers may not conceive of.Additionally, LLMs present opportunities to automate aspects of literature analysis which currently require extensive human effort.LLMs can rapidly review vast bodies of text to extract key findings, synthesize conclusions across papers, and even generate novel hypotheses.This could significantly accelerate knowledge discovery in biology.LLMs like chatbots can provide ondemand tutoring and practice for students learning biology concepts.They can also enable those without formal biology training to query biological knowledge interactively.Fine-tuned LLMs can become powerful educational resources to make biology knowledge more accessible.Advancing</p>
<p>AcknowledgementWe acknowledge the support from the College of Engineering, The University of Georgia, Georgia, Athens, United States.Author contributionsXG, JH, TL, and YY contributed to conception and design of the study.XG, JZ, QG, YZ, YT and TJ designed the 108-question exam.JH, YL, LZ, WL, and TL conducted the experimental design and model running.JH and XG performed all data analysis.XG guided the writing process and wrote the initial draft.QG, JZ, YZ, and YT contributed the writing to the introduction and related work of the manuscript.ZL, ZW, HZ, and TL advised on LLMs concepts and contributed writing to the abstract method, discussion, and conclusion of the manuscript.All authors contributed to the article and approved the submitted version.Conflict of interestThe authors declare no competing financial interest.Supporting InformationThe supplementary file contains the original 108 multiple-choice questions on the biology topic.
Coming of age: ten years of next-generation sequencing technologies. S Goodwin, Nature Reviews Genetics. 172016</p>
<p>High-throughput sequencing technologies. J A Reuter, Mol. Cell. 582015</p>
<p>A programmable dual-RNA-guided DNA endonuclease in adaptive bacterial immunity. M Jinek, Science. 3372012</p>
<p>Multiplex genome engineering using CRISPR/Cas systems. L Cong, Science. 3392013</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, Nature. 5962021</p>
<p>Unravelling biological macromolecules with cryo-electron microscopy. R Fernandez-Leiro, S H Scheres, Nature. 5372016</p>
<p>The expanded CRISPR toolbox for constructing microbial cell factories. Y Teng, Trends Biotechnol. 2023</p>
<p>CRISPR-Cas: a tool for cancer research and therapeutics. H Yin, Nature Reviews Clinical Oncology. 162019</p>
<p>Multilevel omics for the discovery of biomarkers and therapeutic targets for stroke. J Montaner, Nature Reviews Neurology. 162020</p>
<p>Enzyme function prediction using contrastive learning. T Yu, Science. 3792023</p>
<p>Artificial intelligence and synthetic biology approaches for human gut microbiome. P Kumar, Critical Reviews in Food Science Nutrition. 622022</p>
<p>Genomics and natural language processing. M D Yandell, W H Majoros, Nature Reviews Genetics. 32002</p>
<p>Chataug: Leveraging chatgpt for text data augmentation. H Dai, arXiv:.130072023arXiv preprint</p>
<p>Z Zeng, Survey of natural language processing techniques in bioinformatics. Computational mathematical methods in medicine 2015. 2015</p>
<p>Learning the protein language: Evolution, structure, and function. T Bepler, B Berger, 654-669. e653Cell systems. 122021</p>
<p>Unified rational protein engineering with sequence-based deep representation learning. E C Alley, Nat. Methods. 162019</p>
<p>Learning protein sequence embeddings using information from structure. T Bepler, B Berger, arXiv:.086612019arXiv preprint</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, Nat Commun. 1332932022</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 2023</p>
<p>The distribution pattern of genetic variation in the transcript isoforms of the alternatively spliced protein-coding genes in the human genome. T Liu, K Lin, Mol. Biosyst. 112015</p>
<p>Deciphering microbial gene function using natural language processing. D Miller, Nat Commun. 1357312022</p>
<p>ProtGPT2 is a deep unsupervised language model for protein design. N Ferruz, Nat Commun. 1343482022</p>
<p>Advances in natural language processing. J Hirschberg, C D Manning, Science. 3492015</p>
<p>Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models. Y Liu, Meta-Radiology. 1000172023</p>
<p>X Li, arXiv:.05480General Intelligence for Medical Imaging. 2023arXiv preprint</p>
<p>S Talebi, arXiv:.15887Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3. 5. 2023arXiv preprint</p>
<p>. Openai, ArXiv abs/2303.087742023GPT-4 Technical Report</p>
<p>R Anil, arXiv:.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>A Comparative Study of Open-Source Large Language Models. S Wu, arXiv:.04709GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology. 2023arXiv preprint</p>
<p>Z Liu, arXiv:.13693Evaluating Large Language Models for Radiology Natural Language Processing. 2023arXiv preprint</p>
<p>A smart chatbot architecture based NLP and machine learning for health care assistance. S Ayanouz, Proceedings of the 3rd international conference on networking, information systems &amp; security. the 3rd international conference on networking, information systems &amp; security2020</p>
<p>Improving code generation by training with natural language feedback. A Chen, arXiv:.167492023arXiv preprint</p>
<p>Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. J White, 2023</p>
<p>J Holmes, arXiv:.01938Evaluating large language models on a highly-specialized topic, radiation oncology physics. 2023arXiv preprint</p>
<p>Promptsource: An integrated development environment and repository for natural language prompts. S H Bach, arXiv:.012792022arXiv preprint</p>
<p>A systematic review of natural language processing and text mining of symptoms from electronic patient-authored text data. C Dreisbach, Int. J. Med. Inf. 1252019</p>
<p>How will generative AI disrupt data science in drug discovery?. J.-P Vert, Nat. Biotechnol. 2023</p>
<p>Large language models in medicine. A J Thirunavukarasu, Nat. Med. 2023</p>
<p>Can ChatGPT understand genetics?. F Emmert-Streib, Europ. J. Hum. Genet. 2023</p>
<p>Many bioinformatics programming tasks can be automated with ChatGPT. S R Piccolo, arXiv:.135282023arXiv preprint</p>
<p>Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. New Engl. P Lee, J. Med. 3882023</p>
<p>Risks and benefits of large language models for the environment. M C Rillig, Environmental Science Technology. 572023</p>
<p>Science in the age of large language models. A Birhane, Nature Reviews Physics. 2023</p>
<p>CLIP-S4: Language-Guided Self-Supervised Semantic Segmentation. W He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, 2023</p>
<p>Artificial general intelligence (AGI) for education. E Latif, arXiv:.124792023arXiv preprint</p>
<p>Evaluating Large Language Models on a Highly-specialized Topic. J Holmes, Radiation Oncology Physics. 2023</p>
<p>S Rezayi, arXiv:.11892Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications. 2023arXiv preprint</p>
<p>Tailoring Large Language Models to Radiology: A Preliminary Approach to LLM Adaptation for a Highly Specialized Domain. Z Liu, International Workshop on Machine Learning in Medical Imaging. Springer2023</p>            </div>
        </div>

    </div>
</body>
</html>
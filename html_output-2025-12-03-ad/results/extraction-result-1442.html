<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1442 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1442</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1442</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-e978f2f723aaa7772da7c03153cc89c847d18109</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e978f2f723aaa7772da7c03153cc89c847d18109" target="_blank">Explanatory Learning: Beyond Empiricism in Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Odeen, a basic EL environment that simulates a small flatland-style universe full of phenomena to explain, is introduced, showing how CRNs outperform empiricist end-to-end approaches of similar size and architecture (Transformers) in discovering explanations for novel phenomena.</p>
                <p><strong>Paper Abstract:</strong> We introduce Explanatory Learning (EL), a framework to let machines use existing knowledge buried in symbolic sequences -- e.g. explanations written in hieroglyphic -- by autonomously learning to interpret them. In EL, the burden of interpreting symbols is not left to humans or rigid human-coded compilers, as done in Program Synthesis. Rather, EL calls for a learned interpreter, built upon a limited collection of symbolic sequences paired with observations of several phenomena. This interpreter can be used to make predictions on a novel phenomenon given its explanation, and even to find that explanation using only a handful of observations, like human scientists do. We formulate the EL problem as a simple binary classification task, so that common end-to-end approaches aligned with the dominant empiricist view of machine learning could, in principle, solve it. To these models, we oppose Critical Rationalist Networks (CRNs), which instead embrace a rationalist view on the acquisition of knowledge. CRNs express several desired properties by construction, they are truly explainable, can adjust their processing at test-time for harder inferences, and can offer strong confidence guarantees on their predictions. As a final contribution, we introduce Odeen, a basic EL environment that simulates a small flatland-style universe full of phenomena to explain. Using Odeen as a testbed, we show how CRNs outperform empiricist end-to-end approaches of similar size and architecture (Transformers) in discovering explanations for novel phenomena.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1442.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1442.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critical Rationalist Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-part learned system that discovers human-readable explanations (rules) by generating conjectures from few observations and verifying them with a learned interpreter; implemented with transformer encoder-decoder blocks and tested in a synthetic puzzle universe (Odeen).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Critical Rationalist Network (CRN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CRNs consist of two independently trained neural modules: a stochastic Conjecture Generator (CG) that maps a small set of (example, label) pairs to candidate explanation strings, and a learned Interpreter (I) that maps (explanation, sample) -> label. Both modules are implemented with encoder-decoder transformer blocks. At test time CRN samples t conjectures from CG for the new phenomenon, scores each conjecture by how well I predicts the provided observations D0, selects the highest-scoring conjecture, and uses I with that conjecture to label unseen samples. The system supports a test-time trade-off parameter t (number of conjectures) and can return an 'unknown' signal if no conjecture is coherent.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Simulated symbolic scientific discovery / rule induction in a synthetic puzzle domain (Odeen); general problem: discovering explanations for phenomena from symbolic explanations + examples.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>CRN discovers textual, human-readable explanations (rules) for novel phenomena in the Odeen universe. Given a small labelled sample D0 for a new game and a training set of (explanation, observations) pairs from other games, CRN generates candidate explanations, verifies them against D0 using the learned interpreter, selects the best-fitting explanation, and produces labels for unseen structures accordingly. The 'discovery' is the identification of the correct explanation string (or an equivalent rule) that predicts the phenomenon's labels across unseen structures.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not explicitly categorize CRN discoveries as 'incremental' or 'transformational'. Instead it frames discoveries epistemologically (Popper/Deutsch) and emphasizes finding explanations with 'reach' (i.e., predictive power in novel situations). No explicit incremental/transformational label or formal criterion is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation is performed on the Odeen benchmark using: (1) Nearest Rule Score (NRS): fraction of test games for which the predicted tagging vector corresponds to the correct rule (by Hamming-distance nearest-rule comparison across ~25k admissible rules); (2) Tagging Accuracy (T-Acc): average fraction of correctly tagged unseen structures per game; (3) Rule Accuracy (R-Acc): fraction of predicted textual explanations that are semantically equivalent to the ground-truth rule (equivalence checked by comparing tags over the whole universe of 117,649 structures). Additional evaluation: ablations with varying training data sizes (100 / 1k / 10k structures per rule), comparisons vs baseline models (EMP-C, EMP-R), computational-cost/time profiling (calls to CG and I; wall-clock times), and analysis of how many conjectures t are required to retrieve the correct explanation (cumulative R-Acc vs t). Hamming distance on ℓ-dimensional predicted tag vectors is used to rank candidate explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation is entirely dataset-driven: a held-out test set of s=1132 games (each with only k=32 labelled examples) is used where none of the test rules are equivalent to training rules; performance is measured by NRS/T-Acc/R-Acc. Semantic equivalence is validated by applying candidate and ground-truth rules across the full universe (117,649 structures) and comparing tag vectors. Additional validation experiments: (a) stricter protocol that only accepts conjectures coherent with all structures in the small table (returns 'unknown' otherwise) to evaluate calibrated ignorance; (b) comparison to a CRN variant using the ground-truth/hard-coded interpreter to isolate effects of learned I; (c) exhaustive-search baseline (ExV SRC) that evaluates all admissible rules with the interpreter to measure upper-bound performance and cost. The authors also release dataset and code for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed by (i) excluding from training any rules equivalent to the test set (ensuring generalization to genuinely novel rules), (ii) including test-time constructs deliberately absent in training (e.g., the bigram 'exactly two') to test compositional extrapolation, and (iii) measuring R-Acc to check whether the predicted explanation is semantically identical to the held-out ground truth. The paper emphasizes the ability of CRNs to produce explanations that generalize beyond training phrases (compositional novelty) and to handle ill-formed/ambiguous conjectures via a learned interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Primary numeric metrics reported (main experiment: n=1438 training rules, m=1000 structures per rule; test: s=1132, k=32, ℓ=1176): CRN NRS = 0.777 (880/1132 rules discovered), T-Acc = 0.980, R-Acc = 0.737. Variants/other regimens: CRN (10k struct./1438 rules) NRS=0.813, T-Acc=0.984; CRN [300 beams] wall-clock T ≈ 0.79 s/game with NRS≈0.81 (reported in computational-cost table). The paper also reports that >60% of correct explanations are found within first 50 candidates and >80% within first 300 candidates. These metrics quantify discovery success, tagging quality, semantic correctness, and computational trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>No direct quantitative comparison to human scientists is provided. The paper frames CRN behavior analogically to human scientific discovery (e.g., Galileo), and contrasts epistemological styles (empiricist end-to-end models vs rationalist CRNs). Empirical comparisons are made only to data-driven neural baselines (EMP-C and EMP-R) and to variants with a hard-coded interpreter or exhaustive search. Key numerical comparisons: CRN NRS 0.777 vs EMP-C 0.225 vs EMP-R 0.156; CRN R-Acc 0.737 vs EMP-C R-Acc 0.035. Comparative ablation shows learned Interpreter sometimes outperforms hard-coded Interpreter because it tolerates ill-formed conjectures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the primary reported experiment (1438 training rules, 1000 training structures per rule): CRN discovered correct explanations for 880 out of 1132 test games (NRS = 77.7%). T-Acc in same setting = 98.0% average correct tags; R-Acc = 73.7% of textual conjectures were semantically equivalent to the true rule. Performance varies with training-data regime (e.g., CRN NRS drops to 40.2% when trained with only 100 structures per rule).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified limitations include: (1) reliance on a representative D0 (k=32) — the model assumes the small sample is representative for the true rule; (2) Odeen is a simplified, closed synthetic environment; real-world scientific discovery has richer modalities and open interaction; (3) the paper does not formalize nor measure 'transformational' vs 'incremental' discoveries — that distinction is not operationalized; (4) learned interpreter errors can lead to wrong ranking of conjectures; (5) computational trade-off: thoroughness depends on t (number of conjectures) and exhaustive search is expensive; (6) ambiguity and ill-formed conjectures require a learned I to interpret gracefully — hard-coded interpreters may fail; (7) evaluation limited to synthetic grammar-driven rules and automatic equivalence checks, lacking real-world experimental validation or peer-reviewed external confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explanatory Learning: Beyond Empiricism in Neural Networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1442.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1442.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMP-C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conscious Empiricist Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven end-to-end neural baseline that augments the empiricist label-prediction model with a textual head to predict explanations (rules) in addition to labels; implemented with transformer blocks and trained on the same Odeen data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conscious Empiricist (EMP-C)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EMP-C is an end-to-end neural architecture that takes as input a set of (structure, label) examples and either predicts labels for new samples and/or generates a textual explanation string using a transformer rule decoder head. The model treats explanations as additional data to be predicted rather than prior knowledge to be interpreted. Architecturally it uses similar transformer encoder/decoder blocks as CRN (board encoder, rule decoder, label decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Simulated symbolic rule induction in the Odeen benchmark (same domain as CRN).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>EMP-C attempts to infer the rule textually and/or directly predict labels for unseen structures by learning correlations from training data. In practice it sometimes generates plausible textual rules but these are rarely semantically equivalent to held-out ground-truth rules; it can still achieve decent tagging accuracies without producing correct explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not classify discoveries by EMP-C as incremental or transformational. EMP-C is presented as an empiricist baseline that treats explanations as text to be predicted; no explicit discovery-type labeling is given.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Same Odeen evaluation metrics: NRS, T-Acc, R-Acc; empirical tables report EMP-C performance under the same training/test splits used for CRN. Comparison with CRN and EMP-R shows lower NRS and R-Acc despite reasonable T-Acc.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation via the held-out Odeen test set with excluded rule equivalence classes; semantic equivalence verification over the full universe for R-Acc; computational cost and ablations (beam sizes) reported for producing textual rules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty measured by whether EMP-C can produce semantically correct rules on the held-out test set and generalize to unseen grammatical constructs (e.g., 'exactly two'); results show EMP-C seldom produces semantically correct textual rules (R-Acc ≈ 3.5% in main regime) despite some ability to tag correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported metrics for the main experiment (1438 rules, 1000 structures/rule): EMP-C NRS = 0.225, T-Acc = 0.905, R-Acc = 0.035. Other regimens reported in Appendix and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>EMP-C is compared directly to CRN and EMP-R. Key numeric contrasts: EMP-C NRS 22.5% vs CRN 77.7% (main experiment); EMP-C R-Acc 3.5% vs CRN 73.7%. EMP-C often attains decent tagging accuracy (T-Acc ~90%) but its generated textual rules are rarely semantically correct.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>EMP-C discovered the correct explanation (by NRS) for ~22.5% of test games in the primary reported experiment; tagging accuracy across unseen structures was ~90.5% in that same setting.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>EMP-C treats explanations as strings to be predicted, so (a) it provides no guarantee that a generated textual rule is consistent with the predicted labels; (b) textual outputs are often syntactically ill-formed or semantically wrong; (c) lacks test-time adjustable hypothesis search; (d) cannot reliably indicate calibrated ignorance; (e) poor R-Acc indicates inability to truly 'discover' correct explanations even when tagging performance is acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explanatory Learning: Beyond Empiricism in Neural Networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1442.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1442.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMP-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Radical Empiricist Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end neural baseline that predicts labels for unseen samples directly from provided labelled examples without producing textual explanations; implemented with transformer label decoder and board encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Radical Empiricist (EMP-R)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EMP-R is a data-driven model that encodes the small training board (32 labelled examples) and directly outputs labels for new structures using a transformer label-decoder; it does not generate or use explanation strings and embodies the empiricist approach of learning from observations alone.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Simulated rule induction / classification in the Odeen benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>EMP-R does not output textual explanations; its 'discoveries' are implicit through its predicted labels. It is evaluated on how well it tags unseen structures but does not provide human-readable explanations and therefore is not credited with explicit explanation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>No classification as incremental/transformational is provided in the paper for EMP-R; it is presented as a baseline empiricist approach that does not produce explicit explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Same metrics (NRS, T-Acc) applied to labels predicted by EMP-R. EMP-R typically attains reasonable T-Acc but low NRS compared to CRN because it does not recover the correct underlying rule.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validated on the held-out Odeen test set; NRS computed by comparing EMP-R's predicted tag vector to ground-truth rule vectors (via Hamming distance) to determine if an implicitly equivalent rule was chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>EMP-R's novelty is limited: it often achieves moderate tagging performance (T-Acc ~89-90%) but fails to discover the correct underlying explanations (NRS ~15.6% in the main regime), so it is not considered to have made novel explanatory discoveries in the paper's sense.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Main experiment (1438 rules, 1000 structures/rule): EMP-R NRS = 0.156, T-Acc = 0.898. Other regimens shown in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>EMP-R is compared to CRN and EMP-C: EMP-R has lower NRS and R-Acc than CRN and slightly lower NRS than EMP-C though comparable T-Acc. EMP-R cannot output or validate textual explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>EMP-R discovered a nearest-rule match (NRS) for ~15.6% of test games in the primary experiment and achieved ~89.8% tagging accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>EMP-R does not produce explicit explanations, lacks interpretability and calibrated confidence, and cannot be used to produce human-understandable discoveries; it is sensitive to the training data and does not support test-time adjustable search over hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explanatory Learning: Beyond Empiricism in Neural Networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning regular sets from queries and counterexamples <em>(Rating: 2)</em></li>
                <li>Deepcoder: Learning to write programs <em>(Rating: 2)</em></li>
                <li>Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 2)</em></li>
                <li>Inductive inference of theories from facts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1442",
    "paper_id": "paper-e978f2f723aaa7772da7c03153cc89c847d18109",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "CRN",
            "name_full": "Critical Rationalist Network",
            "brief_description": "A two-part learned system that discovers human-readable explanations (rules) by generating conjectures from few observations and verifying them with a learned interpreter; implemented with transformer encoder-decoder blocks and tested in a synthetic puzzle universe (Odeen).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Critical Rationalist Network (CRN)",
            "system_description": "CRNs consist of two independently trained neural modules: a stochastic Conjecture Generator (CG) that maps a small set of (example, label) pairs to candidate explanation strings, and a learned Interpreter (I) that maps (explanation, sample) -&gt; label. Both modules are implemented with encoder-decoder transformer blocks. At test time CRN samples t conjectures from CG for the new phenomenon, scores each conjecture by how well I predicts the provided observations D0, selects the highest-scoring conjecture, and uses I with that conjecture to label unseen samples. The system supports a test-time trade-off parameter t (number of conjectures) and can return an 'unknown' signal if no conjecture is coherent.",
            "discovery_domain": "Simulated symbolic scientific discovery / rule induction in a synthetic puzzle domain (Odeen); general problem: discovering explanations for phenomena from symbolic explanations + examples.",
            "discovery_description": "CRN discovers textual, human-readable explanations (rules) for novel phenomena in the Odeen universe. Given a small labelled sample D0 for a new game and a training set of (explanation, observations) pairs from other games, CRN generates candidate explanations, verifies them against D0 using the learned interpreter, selects the best-fitting explanation, and produces labels for unseen structures accordingly. The 'discovery' is the identification of the correct explanation string (or an equivalent rule) that predicts the phenomenon's labels across unseen structures.",
            "discovery_type": null,
            "discovery_type_justification": "The paper does not explicitly categorize CRN discoveries as 'incremental' or 'transformational'. Instead it frames discoveries epistemologically (Popper/Deutsch) and emphasizes finding explanations with 'reach' (i.e., predictive power in novel situations). No explicit incremental/transformational label or formal criterion is provided in the paper.",
            "evaluation_methods": "Evaluation is performed on the Odeen benchmark using: (1) Nearest Rule Score (NRS): fraction of test games for which the predicted tagging vector corresponds to the correct rule (by Hamming-distance nearest-rule comparison across ~25k admissible rules); (2) Tagging Accuracy (T-Acc): average fraction of correctly tagged unseen structures per game; (3) Rule Accuracy (R-Acc): fraction of predicted textual explanations that are semantically equivalent to the ground-truth rule (equivalence checked by comparing tags over the whole universe of 117,649 structures). Additional evaluation: ablations with varying training data sizes (100 / 1k / 10k structures per rule), comparisons vs baseline models (EMP-C, EMP-R), computational-cost/time profiling (calls to CG and I; wall-clock times), and analysis of how many conjectures t are required to retrieve the correct explanation (cumulative R-Acc vs t). Hamming distance on ℓ-dimensional predicted tag vectors is used to rank candidate explanations.",
            "validation_approaches": "Validation is entirely dataset-driven: a held-out test set of s=1132 games (each with only k=32 labelled examples) is used where none of the test rules are equivalent to training rules; performance is measured by NRS/T-Acc/R-Acc. Semantic equivalence is validated by applying candidate and ground-truth rules across the full universe (117,649 structures) and comparing tag vectors. Additional validation experiments: (a) stricter protocol that only accepts conjectures coherent with all structures in the small table (returns 'unknown' otherwise) to evaluate calibrated ignorance; (b) comparison to a CRN variant using the ground-truth/hard-coded interpreter to isolate effects of learned I; (c) exhaustive-search baseline (ExV SRC) that evaluates all admissible rules with the interpreter to measure upper-bound performance and cost. The authors also release dataset and code for reproducibility.",
            "novelty_assessment": "Novelty is assessed by (i) excluding from training any rules equivalent to the test set (ensuring generalization to genuinely novel rules), (ii) including test-time constructs deliberately absent in training (e.g., the bigram 'exactly two') to test compositional extrapolation, and (iii) measuring R-Acc to check whether the predicted explanation is semantically identical to the held-out ground truth. The paper emphasizes the ability of CRNs to produce explanations that generalize beyond training phrases (compositional novelty) and to handle ill-formed/ambiguous conjectures via a learned interpreter.",
            "impact_metrics": "Primary numeric metrics reported (main experiment: n=1438 training rules, m=1000 structures per rule; test: s=1132, k=32, ℓ=1176): CRN NRS = 0.777 (880/1132 rules discovered), T-Acc = 0.980, R-Acc = 0.737. Variants/other regimens: CRN (10k struct./1438 rules) NRS=0.813, T-Acc=0.984; CRN [300 beams] wall-clock T ≈ 0.79 s/game with NRS≈0.81 (reported in computational-cost table). The paper also reports that &gt;60% of correct explanations are found within first 50 candidates and &gt;80% within first 300 candidates. These metrics quantify discovery success, tagging quality, semantic correctness, and computational trade-offs.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "No direct quantitative comparison to human scientists is provided. The paper frames CRN behavior analogically to human scientific discovery (e.g., Galileo), and contrasts epistemological styles (empiricist end-to-end models vs rationalist CRNs). Empirical comparisons are made only to data-driven neural baselines (EMP-C and EMP-R) and to variants with a hard-coded interpreter or exhaustive search. Key numerical comparisons: CRN NRS 0.777 vs EMP-C 0.225 vs EMP-R 0.156; CRN R-Acc 0.737 vs EMP-C R-Acc 0.035. Comparative ablation shows learned Interpreter sometimes outperforms hard-coded Interpreter because it tolerates ill-formed conjectures.",
            "success_rate": "In the primary reported experiment (1438 training rules, 1000 training structures per rule): CRN discovered correct explanations for 880 out of 1132 test games (NRS = 77.7%). T-Acc in same setting = 98.0% average correct tags; R-Acc = 73.7% of textual conjectures were semantically equivalent to the true rule. Performance varies with training-data regime (e.g., CRN NRS drops to 40.2% when trained with only 100 structures per rule).",
            "challenges_limitations": "Identified limitations include: (1) reliance on a representative D0 (k=32) — the model assumes the small sample is representative for the true rule; (2) Odeen is a simplified, closed synthetic environment; real-world scientific discovery has richer modalities and open interaction; (3) the paper does not formalize nor measure 'transformational' vs 'incremental' discoveries — that distinction is not operationalized; (4) learned interpreter errors can lead to wrong ranking of conjectures; (5) computational trade-off: thoroughness depends on t (number of conjectures) and exhaustive search is expensive; (6) ambiguity and ill-formed conjectures require a learned I to interpret gracefully — hard-coded interpreters may fail; (7) evaluation limited to synthetic grammar-driven rules and automatic equivalence checks, lacking real-world experimental validation or peer-reviewed external confirmation.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1442.0",
            "source_info": {
                "paper_title": "Explanatory Learning: Beyond Empiricism in Neural Networks",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "EMP-C",
            "name_full": "Conscious Empiricist Model",
            "brief_description": "A data-driven end-to-end neural baseline that augments the empiricist label-prediction model with a textual head to predict explanations (rules) in addition to labels; implemented with transformer blocks and trained on the same Odeen data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Conscious Empiricist (EMP-C)",
            "system_description": "EMP-C is an end-to-end neural architecture that takes as input a set of (structure, label) examples and either predicts labels for new samples and/or generates a textual explanation string using a transformer rule decoder head. The model treats explanations as additional data to be predicted rather than prior knowledge to be interpreted. Architecturally it uses similar transformer encoder/decoder blocks as CRN (board encoder, rule decoder, label decoder).",
            "discovery_domain": "Simulated symbolic rule induction in the Odeen benchmark (same domain as CRN).",
            "discovery_description": "EMP-C attempts to infer the rule textually and/or directly predict labels for unseen structures by learning correlations from training data. In practice it sometimes generates plausible textual rules but these are rarely semantically equivalent to held-out ground-truth rules; it can still achieve decent tagging accuracies without producing correct explanations.",
            "discovery_type": null,
            "discovery_type_justification": "The paper does not classify discoveries by EMP-C as incremental or transformational. EMP-C is presented as an empiricist baseline that treats explanations as text to be predicted; no explicit discovery-type labeling is given.",
            "evaluation_methods": "Same Odeen evaluation metrics: NRS, T-Acc, R-Acc; empirical tables report EMP-C performance under the same training/test splits used for CRN. Comparison with CRN and EMP-R shows lower NRS and R-Acc despite reasonable T-Acc.",
            "validation_approaches": "Validation via the held-out Odeen test set with excluded rule equivalence classes; semantic equivalence verification over the full universe for R-Acc; computational cost and ablations (beam sizes) reported for producing textual rules.",
            "novelty_assessment": "Novelty measured by whether EMP-C can produce semantically correct rules on the held-out test set and generalize to unseen grammatical constructs (e.g., 'exactly two'); results show EMP-C seldom produces semantically correct textual rules (R-Acc ≈ 3.5% in main regime) despite some ability to tag correctly.",
            "impact_metrics": "Reported metrics for the main experiment (1438 rules, 1000 structures/rule): EMP-C NRS = 0.225, T-Acc = 0.905, R-Acc = 0.035. Other regimens reported in Appendix and tables.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "EMP-C is compared directly to CRN and EMP-R. Key numeric contrasts: EMP-C NRS 22.5% vs CRN 77.7% (main experiment); EMP-C R-Acc 3.5% vs CRN 73.7%. EMP-C often attains decent tagging accuracy (T-Acc ~90%) but its generated textual rules are rarely semantically correct.",
            "success_rate": "EMP-C discovered the correct explanation (by NRS) for ~22.5% of test games in the primary reported experiment; tagging accuracy across unseen structures was ~90.5% in that same setting.",
            "challenges_limitations": "EMP-C treats explanations as strings to be predicted, so (a) it provides no guarantee that a generated textual rule is consistent with the predicted labels; (b) textual outputs are often syntactically ill-formed or semantically wrong; (c) lacks test-time adjustable hypothesis search; (d) cannot reliably indicate calibrated ignorance; (e) poor R-Acc indicates inability to truly 'discover' correct explanations even when tagging performance is acceptable.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1442.1",
            "source_info": {
                "paper_title": "Explanatory Learning: Beyond Empiricism in Neural Networks",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "EMP-R",
            "name_full": "Radical Empiricist Model",
            "brief_description": "An end-to-end neural baseline that predicts labels for unseen samples directly from provided labelled examples without producing textual explanations; implemented with transformer label decoder and board encoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Radical Empiricist (EMP-R)",
            "system_description": "EMP-R is a data-driven model that encodes the small training board (32 labelled examples) and directly outputs labels for new structures using a transformer label-decoder; it does not generate or use explanation strings and embodies the empiricist approach of learning from observations alone.",
            "discovery_domain": "Simulated rule induction / classification in the Odeen benchmark.",
            "discovery_description": "EMP-R does not output textual explanations; its 'discoveries' are implicit through its predicted labels. It is evaluated on how well it tags unseen structures but does not provide human-readable explanations and therefore is not credited with explicit explanation discovery.",
            "discovery_type": null,
            "discovery_type_justification": "No classification as incremental/transformational is provided in the paper for EMP-R; it is presented as a baseline empiricist approach that does not produce explicit explanations.",
            "evaluation_methods": "Same metrics (NRS, T-Acc) applied to labels predicted by EMP-R. EMP-R typically attains reasonable T-Acc but low NRS compared to CRN because it does not recover the correct underlying rule.",
            "validation_approaches": "Validated on the held-out Odeen test set; NRS computed by comparing EMP-R's predicted tag vector to ground-truth rule vectors (via Hamming distance) to determine if an implicitly equivalent rule was chosen.",
            "novelty_assessment": "EMP-R's novelty is limited: it often achieves moderate tagging performance (T-Acc ~89-90%) but fails to discover the correct underlying explanations (NRS ~15.6% in the main regime), so it is not considered to have made novel explanatory discoveries in the paper's sense.",
            "impact_metrics": "Main experiment (1438 rules, 1000 structures/rule): EMP-R NRS = 0.156, T-Acc = 0.898. Other regimens shown in Appendix.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "EMP-R is compared to CRN and EMP-C: EMP-R has lower NRS and R-Acc than CRN and slightly lower NRS than EMP-C though comparable T-Acc. EMP-R cannot output or validate textual explanations.",
            "success_rate": "EMP-R discovered a nearest-rule match (NRS) for ~15.6% of test games in the primary experiment and achieved ~89.8% tagging accuracy.",
            "challenges_limitations": "EMP-R does not produce explicit explanations, lacks interpretability and calibrated confidence, and cannot be used to produce human-understandable discoveries; it is sensitive to the training data and does not support test-time adjustable search over hypotheses.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1442.2",
            "source_info": {
                "paper_title": "Explanatory Learning: Beyond Empiricism in Neural Networks",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning regular sets from queries and counterexamples",
            "rating": 2
        },
        {
            "paper_title": "Deepcoder: Learning to write programs",
            "rating": 2
        },
        {
            "paper_title": "Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1
        },
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 2
        },
        {
            "paper_title": "Inductive inference of theories from facts",
            "rating": 1
        }
    ],
    "cost": 0.01607575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>EXPLANATORY LEARNING: BEYOND EMPIRICISM IN NEURAL NETWORKS</h1>
<p>Antonio Norelli<br>Sapienza University, CS dep.</p>
<p>Giorgio Mariani
Sapienza University, CS dep.</p>
<p>Giambattista Parascandolo
OpenAI ${ }^{\ddagger}$</p>
<p>Luca Moschella ${ }^{\dagger}$<br>Sapienza University, CS dep.<br>Andrea Santilli ${ }^{\dagger}$<br>Sapienza University, CS dep.<br>Emanuele Rodolà<br>Sapienza University, CS dep.</p>
<h2>ABSTRACT</h2>
<p>We introduce Explanatory Learning (EL), a framework to let machines use existing knowledge buried in symbolic sequences - e.g. explanations written in hieroglyphic - by autonomously learning to interpret them. In EL, the burden of interpreting symbols is not left to humans or rigid human-coded compilers, as done in Program Synthesis. Rather, EL calls for a learned interpreter, built upon a limited collection of symbolic sequences paired with observations of several phenomena. This interpreter can be used to make predictions on a novel phenomenon given its explanation, and even to find that explanation using only a handful of observations, like human scientists do. We formulate the EL problem as a simple binary classification task, so that common end-to-end approaches aligned with the dominant empiricist view of machine learning could, in principle, solve it. To these models, we oppose Critical Rationalist Networks (CRNs), which instead embrace a rationalist view on the acquisition of knowledge. CRNs express several desired properties by construction, they are truly explainable, can adjust their processing at test-time for harder inferences, and can offer strong confidence guarantees on their predictions. As a final contribution, we introduce Odeen, a basic EL environment that simulates a small flatland-style universe full of phenomena to explain. Using Odeen as a testbed, we show how CRNs outperform empiricist end-to-end approaches of similar size and architecture (Transformers) in discovering explanations for novel phenomena.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The Odeen universe. A convenient environment to study and test the process of knowledge discovery in machines. Like the night sky was for humans. ${ }^{\star}$ Galileo did not sketch negative examples.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 INTRODUCTION</h1>
<p>Making accurate predictions about the future is a key ability to survive and thrive in a habitat. Living beings have evolved many systems to this end, such as memory (McConnell, 1962), and several can predict the course of complex phenomena (Taylor et al., 2012). However, no animal comes even close to the prediction ability of humans, which stems from a unique-in-nature system.</p>
<p>At the core of this system lies an object called explanation, formed by the proposition of a language, which has a remarkable property: it can be installed with ease into another human speaking the same language, allowing to make predictions on new phenomena without ever having experienced them. When the installation is successful, we say that the human has understood the explanation.</p>
<p>This process is key to the success of human beings. An individual can provide accurate predictions for a multitude of phenomena without going through a painful discovery process for all of them, but only needs an operating system - mastering a language - and someone who communicates the relevant explanations; this way, the individual can focus on unexplained phenomena. When an explanation is found for them, it is added to the existing shared collection, which we call knowledge.</p>
<p>How can we make machines take part in this orchestra? With this work, we try to shed new light on this problem. Specifically, we propose a learning procedure to allow machines (i) to understand existing explanations, in the sense described above, and (ii) create new explanations for unexplained phenomena, much like human scientists do.</p>
<p>Our contribution in this sense is threefold:
i) We formulate the challenge of creating a machine that masters a language as the problem of learning an interpreter from a collection of examples in the form (explanation, observations). The only assumption we make is this dual structure of data; explanations are free strings, and are not required to fit any formal grammar. This results in the Explanatory Learning (EL) framework described in Sec. 2 .
ii) We present Odeen, a basic environment to test EL approaches, which draws inspiration from the board game Zendo (Heath, 2001). Odeen simulates the work of a scientist in a small universe of simple geometric figures, see Figure 1. We present it in Sec. 3, and release it with this work ${ }^{2}$.
iii) We argue that the dominating empiricist ML approaches are not suitable for EL problems. We propose Critical Rationalist Networks (CRNs), a family of models designed according to the epistemological philosophy pushed forward by Popper (1935). Although a CRN is implemented using two neural networks, the working hypothesis of such a model does not coincide with the adjustable network parameters, but rather with a language proposition that can only be accepted or refused in toto. We will present CRNs in Sec. 4, and test their performance on Odeen in Sec. 5.</p>
<h2>2 EXPLANATORY LEARNING</h2>
<p>Humans do not master a language from birth. A baby can not use the message "this soap stings" to predict the burning sensation caused by contact with the substance. Instead, the baby gradually learns to interpret such messages and make predictions for an entire universe of phenomena (Schulz et al., 2007). We refer to this state of affairs as mastering a language, and we aim to replicate it in a machine as the result of an analogous learning process.</p>
<p>Using a batch of explanations paired with observations of several phenomena, we want to learn an interpreter to make predictions about novel phenomena for which we are given explanations in the same language. Going a step further, we also want to discover these explanations, when all we have is a handful of observations of the novel phenomena. We first describe the problem setup in the sequel, comparing it to existing ML problems; then we detail our approach in Sec. 4.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Problem setup. Formally, let phenomena $P_{1}, P_{2}, P_{3}, \ldots$ be subsets of a universe $U$, which is a large set with no special structure (i.e., all the possible observations $U=\left{x_{1}, \ldots, x_{z}\right}$ ). Over a universe $U$, one can define a language $L$ as a pair $\left(\Sigma_{L}, \mathcal{I}<em L="L">{L}\right)$, where $\Sigma</em>}$ is a finite collection of short strings over some alphabet $A$, with $\left|\Sigma_{L}\right| \gg|A|$, and $\mathcal{I<em L="L">{L}$ is a binary function $\mathcal{I}</em>}: U \times \Sigma_{L} \rightarrow{0,1}$, which we call interpreter. We say that a phenomenon $P_{i}$ is explainable in a language $L$ if there exists a string $e \in \Sigma_{L}$ such that, for any $x \in U$, it occurs $\mathcal{I<em P__i="P_{i">{L}(x, e)=\mathbf{1}</em>}}(x)$, where $\mathbf{1<em i="i">{P</em>$.}}(x)$ is the indicator function of $P_{i}$. We call the string $e$ an explanation, in the language $L$, for the phenomenon $P_{i</p>
<p>Our first contribution is the introduction of a new class of machine learning problems, which we refer to as Explanatory Learning (EL).
Consider the general problem of making a new prediction for a phenomenon $P_{0} \subset U$. In our setting, this is phrased as a binary classification task: given a sample $x^{\prime} \in U$, establish whether $x^{\prime} \in P_{0}$ or not. We are interested in two instances of this problem, with different underlying assumptions:</p>
<ul>
<li>The communication problem: we have an explanation. We are given an explanation $e_{0}$ for $P_{0}$, in an unknown language $L$. This means that we do not have access to an interpreter $\mathcal{I}<em 0="0">{L} ; e</em>}$ looks like Japanese to a non-Japanese speaker. Instead, we are also given other explanations $\left{e_{1}, \ldots, e_{n}\right}$, in the same language, for other phenomena $P_{1}, \ldots, P_{n}$, as well as observations of them, i.e., datasets $\left{D_{1}, \ldots, D_{n}\right}$ in the form $D_{i}=\left{\left(x_{1}, \mathbf{1<em i="i">{P</em>}}\left(x_{1}\right)\right), \ldots,\left(x_{m}, \mathbf{1<em i="i">{P</em>}}\left(x_{m}\right)\right)\right}$, with $m \ll|U|$. Intuitively, here we expect the learner to use the explanations paired with the observations to build an approximated interpreter $\tilde{\mathcal{I}<em L="L">{L}$, and then use it to make the proper prediction for $x^{\prime}$ by evaluating $\tilde{\mathcal{I}}</em>\right)$.}\left(x^{\prime}, e_{0</li>
<li>The scientist problem: we do not have an explanation. We are given explanations $\left{e_{1}, \ldots, e_{n}\right}$ in an unknown language $L$ for other phenomena $P_{1}, \ldots, P_{n}$ and observations of them $\left{D_{1}, \ldots, D_{n}\right}$. However, we do not have an explanation for $P_{0}$; instead, we are given just a small set of observations $D_{0}=\left{\left(x_{1}, \mathbf{1}<em 0="0">{P</em>}}\left(x_{1}\right)\right), \ldots,\left(x_{k}, \mathbf{1<em 0="0">{P</em>}}\left(x_{k}\right)\right)\right}$ and two guarantees, namely that $P_{0}$ is explainable in $L$, and that $D_{0}$ is representative for $P_{0}$ in $L$. That is, for every phenomenon $P \neq P_{0}$ explainable in $L$ there should exist at least a $x_{i} \in D_{0}$ such that $\mathbf{1<em 0="0">{P</em>}}\left(x_{i}\right) \neq \mathbf{1<em i="i">{P}\left(x</em>}\right)$. Again, we expect the learner to build the interpreter $\tilde{\mathcal{I}<em 0="0">{L}$, which should first guide the search for the missing explanation $e</em>}$ based on the clues $D_{0}$, and then provide the final prediction through $\tilde{\mathcal{I}<em 0="0">{L}\left(x^{\prime}, e</em>\right)$.</li>
</ul>
<p>Several existing works fall within the formalization above. The seminal work of Angluin (1987) on learning regular sets is an instance of the scientist problem, where finite automata take the role of explanations, while regular sets are the phenomena. More recently, CLEVR (Johnson et al., 2017) posed a communication problem in a universe of images of simple solids, where explanations are textual and read like "There is a sphere with the same size as the metal cube". Another recent example is CLIP (Radford et al., 2021), where 400,000,000 captioned internet images are arranged in a communication problem to train an interpreter, thereby elevating captions to the status of explanations rather than treating them as simple labels ${ }^{3}$. With EL, we aim to offer a unified perspective on these works, making explicit the core problem of learning an interpreter purely from observations.</p>
<p>Relationship with other ML problems. We briefly discuss the relationship between EL and other problems in ML, pointing to Sec. 6 for additional discussion on the related work.</p>
<p>EL can be framed in the general meta-learning framework. The learner gains experience over multiple tasks to improve its general learning algorithm, thus requiring fewer data and less computation on new tasks. However, differently from current meta-learning approaches (Hospedales et al., 2020), we are not optimizing for any meta-objective. Instead, we expect the sought generality to be a consequence of implicitly defining an interpreter through a limited set of examples rather than an explicit goal to optimize for.</p>
<p>To many, the concept of explanation may sound close to the concept of program; similarly, the scientist problem may seem a rephrasing of the fundamental problem of Inductive Logic Programming (ILP) (Shapiro, 1981) or Program Synthesis (PS) (Balog et al., 2017). This is not the case. ILP has the analogous goal of producing a hypothesis from positive/negative examples accompanied by</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Explainability definition</p>
<p>Representativity definition</p>
<p>background knowledge. Yet, ILP requires observations to be expressed as logic formulas, a task requiring a human; only then the ILP solver outputs an explanation in the form of a logic proposition, which in turn is interpreted by a human expert. With EL, data can be fed as-is without being translated into logic propositions, and a learned interpreter plays the expert's role. PS also admits raw data as input, it yields a program as output, and replaces the expert with a handcrafted interpreter; still, the sequence of symbols produced by a PS system only makes sense to a human (who designed the interpreter), not to the system itself. Instead, in EL, the interpreter is learned from data rather than hardcoded. An empirical comparison demonstrating the benefits of EL over PS is given in Sec. 5 .</p>
<p>Next we introduce Odeen, an environment and benchmark to experiment with the EL paradigm.</p>
<h1>3 ODEEN: A PUZZLE GAME AS EXPLANATORY LEARNING ENVIRONMENT</h1>
<p>Single game. The inset shows a typical situation in a game of Odeen. The players look at a set of structures made of simple geometric figures. Each structure is tagged red or green according to a secret rule, and the players' goal is to guess this rule. In the example, the rule can not possibly be "A structure must contain at least one red square" since the fifth
<img alt="img-1.jpeg" src="img-1.jpeg" />
structure on the left does not contain a red square, but respects the rule (green tag). To win the game, a player must prove to know the rule by correctly tagging a large set of new structures ${ }^{4}$. We made a simplified interactive version of Odeen available at https://huggingface.co/spaces/gladia/odeen.
Odeen challenge. We can see each game of Odeen as a different phenomenon of a universe, where each element is a sequence of geometric figures. In this universe, players are scientists like Galileo, trying to explain the new phenomenon; see Figure 1. We can phrase the challenge for an Odeen scientist in this way: make correct predictions for a new phenomenon given few observations of it in addition to explanations and observations of some other phenomena. This is the essence of the Odeen Explanatory Learning problem, see Figure 2 (A and B).</p>
<ul>
<li>Why do we need explanations and observations from phenomena different from the one of interest? Indeed, we are able to play Odeen from the very first game.</li>
<li>We are able to do so only because we are -already- fluent in the Odeen language, which is a subset of English in the above case. We already have and understand all necessary concepts, such as being "at the right of" something, but also being a "square" or "at least". Otherwise, we would need past explanations and observations to first build this understanding. Before explaining the dynamic of the Jupiter moons, Galileo learned what "Jupiter" is and what does it mean to "have a period around" something from past explanations and examples provided to him by books and teachers.</li>
</ul>
<p>In Odeen, consider the point of view of someone who does not speak the language in which the rules are written; an example of this is in the inset, where the secret explanations are given in hieroglyphics rather than English. Such a player would not be able to tag any structure according to the secret rule, even if the latter is given. However, assume the player has been watching several games together with their secret rules. Reasonably, the player will grow an idea of what those strange symbols mean. If the player then wins several Odeen games, it would be strong evidence of mastering the Odeen language.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 2: Odeen Explanatory Learning problem. Given observations and explanations in an unknown language for some phenomena (A), plus a few observations of a new phenomenon, explain the latter and prove this knowledge by correctly tagging a large set of new samples (B). An empiricist approach attempts to extract this knowledge from data (C, left); a rationalist one conceives data as theory-laden observations, used to find the true explanation among a set of conjectures (C, right).</p>
<p>Problem formulation. Each game of Odeen is a different phenomenon $P_{i}$ of a universe $U$ whose elements $x$ are sequences of geometric figures. The specific task is to make correct predictions for a new phenomenon $P_{0}$ (a new game) given: (i) a few observations $D_{0}$ of $P_{0}$ (tagged structures), in conjunction with (ii) explanations $\left{e_{1}, \ldots, e_{n}\right}$ and observations $\left{D_{1}, \ldots, D_{n}\right}$ of other phenomena (other games and their secret rules). More formally:</p>
<p>Let us be given $s$ unexplained phenomena with $k$ observations each, and $n$ explained phenomena with $m$ observations each; let the $n$ phenomena be explained in an unknown language, i.e., $e_{1}, \ldots e_{n}$ are plain strings without any interpreter. The task is to make $\ell$ correct predictions for each of the $s$ unexplained phenomena.</p>
<p>We consider $\ell=1176$ ( $1 \%$ of structures); $s=1132 ; k=32 ; m=10 K, 1 K, 100 ; n=1438$ or 500 .
Why not explicitly ask for the rule? Instead of requiring the player to reveal the secret explanation explicitly, we follow the principle of zero-knowledge proofs (Blum et al., 1988). In our setting, this is done by asking the player to correctly tag many unseen structures according to the discovered rule. This makes it possible for any binary classification method to fit our EL environment without generating text. A winning condition is then defined by counting the correct predictions, instead of a textual similarity between predicted and correct explanation, which would require the player to guess word-by-word the secret rule. In fact, different phrasings with the same meaning should grant a victory, e.g., "at least one pyramid pointing up and at most one pyramid pointing up" is a winning guess for the secret rule "exactly one pyramid pointing up"5. A brute-force enumeration of all equivalent phrasings, in turn, would not allow solutions like "exactly one one pyramid pointing up", where "one" is mistakenly repeated twice; intuitively, we want to accept this as correct and dismiss the grammatical error. Similarly, a solution like "exactly one pointing up", where "pyramid" is omitted, should be accepted in a universe where only pyramids point up. We will reencounter these examples in Sec. 5 when we discuss the key properties of our approach.
Dataset generation. Odeen structures are sequences of six elements including spaces, blues or reds, squares or pyramids, the latter pointing up or down. The size of the universe is $|U|=7^{6}=117,649$ possible structures. We further created a small language with objects, attributes, quantifiers, logical conjunctions, and interactions (e.g., "touching", see Appendix A). The grammar generates $\approx 25 \mathrm{k}$ valid rules in total. Each of the $|U|$ structures is tagged according to all the rules. The tagging is done by an interpreter implemented via regular expressions.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Metrics. As described above, the task is to tag $\ell$ new structures for each of $s$ unexplained games. An EL algorithm addressing this task encodes the predicted rule as an $\ell$-dimensional binary vector $\mathbf{v}$ per game (predicted vector), where $v_{i}=1$ means that the $i$-th structure satisfies the predicted rule, and $v_{i}=0$ otherwise (see inset). Let $\mathbf{w}^{<em>}$ be the ground-truth vector, obtained by tagging the $\ell$ structures according to the correct secret rule. Then, the Hamming distance $d_{H}\left(\mathbf{v}, \mathbf{w}^{</em>}\right)$ measures the number of wrong tags assigned by the EL algorithm; if $d_{H}\left(\mathbf{v}, \mathbf{w}^{<em>}\right)&lt;d_{H}\left(\mathbf{v}, \mathbf{w}<em i="i">{i}\right)$, where $\mathbf{w}</em>^{} \neq \mathbf{w</em>}$ ranges over all the possible $\approx 25 \mathrm{k}$ rules, then the predicted rule $\mathbf{v}$ made by the algorithm is deemed correct.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>According to this, the Nearest Rule Score (NRS) is the number of correctly predicted rules over a total of $s$ games. A second score, the Tagging Accuracy (T-Acc), directly counts the number of correct tags averaged over $s$ games; this is more permissive in the following sense. Consider two different rules $A$ and $B$ sharing $99 \%$ of the taggings, and let $A$ be the correct one; if an EL model tags all the structures according to the wrong rule $B$, it still reaches a T-Acc of $99 \%$, but the NRS would be 0 . An EL algorithm with these scores would be good at making predictions, but would be based on a wrong explanation.</p>
<h1>4 CRITICAL RATIONALIST NETWORKS</h1>
<p>In principle, an EL problem like Odeen can be approached by training an end-to-end neural network to predict $\hat{y}=\mathbf{1}<em i="i">{P</em>$ (see Figure 2 C, left). Such a model would assume that all the information needed to solve the task is embedded in the data, ignoring the explanations; we may call it a "radical empiricist" approach (Pearl, 2021). A variant that includes the explanations in the pipeline can be done by adding a textual head to the network. This way, we expect performance to improve because predicting the explanation string can aid the classification task. As we show in the experiments, the latter approach (called "conscious empiricist") indeed improves upon the former; yet, it treats the explanations as mere data, nothing more than mute strings to match, in a Chinese room fashion (Searle, 1980; Bender \&amp; Koller, 2020).}}\left(x^{\prime}\right)$, given as input a set of observations $D_{i}$ and a single sample $x^{\prime</p>
<p>In the following, we introduce a "rationalist" approach to solve EL problems. This approach recognizes the given explanations as existing knowledge, and focuses on interpreting them. Here theory comes first, while the data become theory-laden observations.</p>
<p>Learning model. Our Critical Rationalist Networks (CRNs) tackle the EL scientist problem introduced in Sec. 2: to find $y=\mathbf{1}<em 0="0">{P</em>\right}$. They are formed by two independently trained models:
(i) A stochastic Conjecture Generator}}\left(x^{\prime}\right)$ given $x^{\prime}, D_{0},\left{D_{1}, \ldots, D_{n}\right},\left{e_{1}, \ldots, e_{n</p>
<p>$$
\mathcal{C G}:\left{\left(x, \mathbf{1}<em j="j">{P}(x)\right)</em> \mapsto e
$$}\right}_{j=1}^{k</p>
<p>taking $k \leq\left|D_{0}\right|$ pairs $\left(x, \mathbf{1}<em i="i">{P}(x)\right) \in D</em>}$ as input, and returning an explanation string $e \in \Sigma$ as output. $\mathcal{C G}$ is trained to maximize the probability that $\mathcal{C G}\left(\tilde{D<em i="i">{i}\right)=e</em>}$ for all $i=1, \ldots, n$, where $\tilde{D<em i="i">{i} \subset D</em>\right|=k$.
(ii) A learned Interpreter}$ is a random sampling of $D_{i}$, and $\left|\tilde{D}_{i</p>
<p>$$
\mathcal{I}:(e, x) \mapsto \hat{y}
$$</p>
<p>which takes as input a string $e \in \Sigma$ and a sample $x \in U$, to output a prediction $\hat{y} \in{0,1}$. $\mathcal{I}$ is trained to maximize the probability that $\mathcal{I}\left(e_{i}, x\right)=\mathbf{1}<em i="i">{P</em>}}(x)$, with $i=1, \ldots, n$ and $\left(x, \mathbf{1<em i="i">{P</em>$.}}(x)\right) \in D_{i</p>
<p>At test time, we are given a trained $\mathcal{C G}$ and a trained $\mathcal{I}$, and we must predict whether some $x^{\prime} \notin D_{0}$ belongs to $P_{0}$ or not. The idea is to first generate $t$ conjectures by applying $\mathcal{C G} t$ times to the dataset $D_{0}$; then, each conjecture is verified by counting how many times the interpreter $\mathcal{I}$ outputs a correct prediction over $D_{0}$. The conjecture with the highest hit rate is our candidate explanation $\hat{e}<em 0="0">{0}$ for $P</em>\right)$. See Figure 3 (left) for a step-by-step pseudo code.}$. Finally, we obtain the prediction $\hat{y}^{\prime}$ as $\mathcal{I}\left(\hat{e}_{0}, x^{\prime</p>
<p>Remarks. The interpreter $\mathcal{I}$ is a crucial component of our approach. A poor $\mathcal{I}$ may fail to identify $e_{0}$ among the generated conjectures, or yield a wrong prediction $y^{\prime}$ when given the correct $e_{0}$. On</p>
<p>Input: dataset $D_{0}$, sample $x^{\prime}$
Output: explanation $\hat{e}<em 0="0">{0}$, prediction $\hat{y}^{\prime}$
$C \leftarrow \emptyset$
while $|C|&lt;t$ do
$\hat{D</em>$
$C \leftarrow C \cup \mathcal{C G}\left(\hat{D}}} \leftarrow$ random pairs $\subset D_{0<em i="i">{0}\right)$
end
${s</em> \leftarrow 0}$ for $i=1, \ldots t$
for $j=1, \ldots, k$ do
$(x, y) \leftarrow$ random $\in D_{0}$
for $i=1, \ldots, t$ do
$\hat{y} \leftarrow \mathcal{I}\left(e_{i} \in C, x\right)$
if $\hat{y}=y$ then
$s_{i} \leftarrow s_{i}+1$
end
end
end
$\ell \leftarrow \arg \max <em i="i">{i}\left{s</em>\right}$
$\hat{e}<em t="t">{0} \leftarrow e</em> \in C$
$\hat{y}^{\prime} \leftarrow \mathcal{I}\left(\hat{e}_{0}, x^{\prime}\right)$
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3: Left: Test-time algorithm of CRNs. Right: CRNs are implemented using encoderdecoder transformers blocks, details of the parameters in Appendix B. Right-top: $\mathcal{I}$ denotes the interpreter model (rule encoder and label decoder). Right-bottom: The conjecture generator $\mathcal{C G}$ is composed by blue blocks. The "radical empiricist" (EMP-R) is composed by orange blocks. The "conscious empiricist" (EMP-C) baseline model consists of all the transformer blocks in the rightbottom figure, board encoder with rule and label decoders (all the blue and orange blocks).
the other hand, we can work with a $\mathcal{C G}$ of any quality and safely return as output an unknown token, rather than a wrong prediction, whenever $e_{0}$ does not appear among the generated conjectures. The role of $\mathcal{C G}$ is to trade-off performance for computational cost, and is controlled by the parameter $t$. Larger values for $t$ imply more generated conjectures, corresponding to exhaustive search if taken to the limit (as done, e.g., in <em>Radford et al. (2021)</em>). This potential asymmetry in quality between $\mathcal{C G}$ and $\mathcal{I}$ is tolerated, since the learning problem solved by $\mathcal{C G}$ is generally harder.</p>
<p>Secondly, although a CRN is implemented using neural networks, as we shall see shortly, its working hypothesis does not coincide with a snapshot of the countless network's parameters; rather, the working hypothesis is but the small conjecture analyzed at a given moment. This way, the CRN hypothesis is detached from the model and can only be accepted or refused in its entirety, rather than being slightly adjusted at each new data sample (Figure 2 C, the hypotheses are in orange).</p>
<p>Implementation. Figure 3 (right) illustrates the architecture of CRNs, which we implement using encoder-decoder transformers <em>(Vaswani et al., 2017)</em>. The figure also shows the architecture of the baseline methods EMP-R and EMP-C, corresponding to the end-to-end NN model and its variant with a textual head, respectively. We refer to the Appendix for further details.</p>
<h1>5 EXPERIMENTS</h1>
<p>We extensively compared CRNs to the radical (EMP-R) and conscious (EMP-C) empiricist models over the Odeen challenge, and analyzed several fundamental aspects.</p>
<p>Generalization power. The Odeen challenge directly addresses the generalization capability of a given algorithm, by asking for explanations to unexplained phenomena. This is evaluated over $s=1132$ new games, where each game is given with $k=32$ tagged structures (guaranteed to satisfy a unique, yet unknown rule) and requires to correctly tag $\ell=1176$ unseen structures according to the unknown rule. The training set are $n=1438$ games with ground-truth explanations and $m=1000$ tagged structures per game. The test set does not include any rule equivalent to the training rules. One important example is the bigram "exactly two", which appears in the test set, but was deliberately excluded from training; the training rules only contain "at least/most two" and "exactly one". The CRN guessed $40 \%$ of the 72 test rules with "exactly two", while the empiricist models (EMP-C, EMP-R) scored $4 \%$ and $0 \%$ respectively. The table below reports the full results.</p>
<p>The NRS of $77.7 \%$ denotes that the CRN discovered the correct explanation for 880 out of 1132 new phenomena. Using the same data and a similar number of learnable parameters, the empiricist models score $22.5 \%$ at most. Some example games can be found in Appendix D.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODEL</th>
<th style="text-align: center;">NRS</th>
<th style="text-align: center;">T-ACC</th>
<th style="text-align: center;">R-ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CRN</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 7}$</td>
</tr>
<tr>
<td style="text-align: left;">EMP-C</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.035</td>
</tr>
<tr>
<td style="text-align: left;">EMP-R</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>In the table, R-Acc measures how frequently an output explanation is equivalent to the correct one; two rules $A$ and $B$ are equivalent if the tags assigned by the hard-coded interpreter to all the $\sim 117 \mathrm{k}$ structures in $U$ are the same for $A$ and $B$.</p>
<p>As expected, the explanation predicted by the conscious empiricist model is rarely correct (R-Acc $3.5 \%$ ), even when it tags some structures properly (NRS $22.5 \%$ ); indeed, EMP-C gives no guarantee for the predicted explanation to be consistent with the tags prediction. Conversely, the CRN consistently provides the correct explanation when it is able to properly tag the new structures (NRS $77.7 \%$, R-Acc $73.7 \%$ ). The $4 \%$ gap between the two scores is clarified in the next paragraph.</p>
<p>Handling ambiguity and contradiction. One may reasonably expect that a CRN equipped with the ground-truth interpreter used to generate the dataset, would perform better than a CRN with a learned interpreter. Remarkably, this is not always the case, as reported in Table 1.</p>
<p>Table 1: Explanatory Learning vs Program Synthesis paradigm. Performance comparison of a data-driven vs ground-truth interpreter in a CRN. The last column shows the tag prediction accuracy of the learned $\mathcal{I}$, when provided with the correct rule.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Train Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NRS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T-ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fully-learned CRN</td>
<td style="text-align: center;">Hardcoded $\mathcal{I}$ CRN</td>
<td style="text-align: center;">LEARNED $\mathcal{I}$</td>
</tr>
<tr>
<td style="text-align: center;">10K STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.997</td>
</tr>
<tr>
<td style="text-align: center;">1K STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">100 STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.987</td>
</tr>
<tr>
<td style="text-align: center;">10K STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.923</td>
</tr>
<tr>
<td style="text-align: center;">1K STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;">100 STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.920</td>
</tr>
</tbody>
</table>
<p>The better performance of the fully learned interpreter over the ground-truth one is due to its ability to process ill-formed conjectures generated by the $\mathcal{C G}$. The conjecture "at least one pointing up" makes the hard-coded interpreter fail, since "pointing up" must always follow the word "pyramid" by the grammar. Yet, in Odeen, pyramids are the only objects that point, and the learned $\mathcal{I}$ interprets the conjecture correctly. Other examples include: "exactly one red block touching pyramid blue" ("pyramid" and "blue" are swapped), or the contradictory "at least one two pyramid pointing up and exactly one red pyramid", which was interpreted correctly by ignoring the first "one". When the learned interpreter is not very accurate, the negative effect of errors in tagging prevails.
Making sense out of ambiguous or contradictory messages ${ }^{6}$ is a crucial difference between a learned interpreter vs a hardcoded one. As Rota (1991) reminds us, a concept does not need to be precisely defined in order to be meaningful. Our everyday reasoning is not precise, yet it is effective. "After the small tower, turn right"; we will probably reach our destination, even when our best attempts at defining "tower", as found, e.g., in the Cambridge dictionary, begin with "a tall, narrow structure...".</p>
<p>Explainability. The predictions of a CRN are directly caused by a human-understandable explanation that is available in the output; this makes CRNs explainable by construction. Further, CRNs allow counterfactuals; one may deliberately change the output explanation with a new one to obtain a new prediction. The bank ML algorithm spoke: "Loan denied"; explanation: "Two not paid loan in the past and resident in a district with a high rate of insolvents". With a CRN, we can easily discard this explanation and compute a new prediction for just "Two not paid loan in the past".
Importantly, by choosing a training set, we control the language used for explanations; i.e., we explicit the biases that will steer the learning of generalizations (Mitchell, 1980). This allows a</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>CRN to ignore undesirable patterns in the data (e.g., skin color) if these can not be expressed in the chosen language. If the Odeen training set had no rule with "pointing up/down", the learned interpreter would see all equal pyramids, even with unbalanced training data where $90 \%$ of pyramids point up.</p>
<p>On the contrary, current explainability approaches for NNs (end-to-end empiricist models) either require some form of reverse engineering, e.g., by making sense out of neuron activations (Goh et al., 2021), or introduce an ad-hoc block to generate an explanation given the prediction, without establishing a cause-effect link between the two (Hendricks et al., 2016; Hind et al., 2019). This practice produces explanations that are not reliable and can be misleading (Rudin, 2019), on the contrary CRNs' explanations are faithful to what the model actually computes.</p>
<p>Adjustable thinking time. End-to-end models do not exhibit a parameter to adjust their processing to the complexity of the incoming prediction. By contrast, CRNs have a test-time parameter $t$, corresponding to the number of generated conjectures, which trades off computational cost for performance. In the inset, we plot the cumulative R-Acc score ( $y$ axis) against the number $t$ of generated conjectures ( $x$ axis). The curves show that $&gt;60 \%$ of correct explanations are found within the first 50 candidates, and $&gt;80 \%$ are within the first 300 . As a reference, a brute force exhaustive search would reach $100 \%$ over a search space of 24,794 possible explanations.</p>
<p>Prediction confidence. As explained in Sec. 4, at test time the CRN selects the conjecture with the highest hit rate among the ones generated by the $\mathcal{C G}$. Alternatively, one may keep only the conjectures coherent with all the structures in the table, returning an "unknown explanation" signal if no such conjectures are found. If the interpreter is sufficiently accurate, this stricter condition barely deteriorates the CRN performance, and it will never return a prediction based on a possibly wrong explanation. For example, tested in a setting with $n=1438, m=1000$ (same as the Generalization power paragraph), this stricter CRN discovers the correct explanation for 861 out of 1132 new phenomena ( $76 \%$ ), and admits its ignorance on the other 271. Conversely, evaluating the confidence of an end-to-end neural network remains an open problem (Meinke \&amp; Hein, 2019).</p>
<h1>6 Related Work</h1>
<p>Epistemology. The deep learning model we propose in this work, CRNs, is designed according to the epistemological theory of critical rationalism advanced by Popper (1935), where knowledge derives primarily from conjectures, criticized at a later stage using data. Deutsch (2011) remarks that to make this critique effective, conjectures should not be adjustable but can only be kept or rejected at each new data sample, as done in CRNs at test time. Only in this way we can discover explanations with "reach", namely that maintain predictive power in novel situations.</p>
<p>Machine learning. Explanatory Learning enriches the fundamental problem of modern program synthesis (e.g., Balog et al., 2017; Ellis et al., 2020) by including the interpretation step among what should be learned. As seen with a few examples (see the Handling ambiguity paragraph), a CRN with learned $\mathcal{I}$ can exploit the ambiguity of language to impose new meaning on arbitrary substrates, which Santoro et al. (2021) recognize as a fundamental trait of symbolic behavior. Recent literature finds few yet remarkable approaches that fit our EL paradigm, such as CLIP (Radford et al., 2021) in the vision area, and Generate \&amp; Rank (Shen et al., 2021) for Math Word Problems in NLP.</p>
<p>The Odeen challenge continues the tradition of AI benchmarks set in idealized domains (Mitchell, 2021). Unlike CLEVR (Johnson et al., 2017) and ShapeWorld (Kuhnle \&amp; Copestake, 2017), Odeen focuses on abduction rather than deduction. Unlike ARC (Chollet, 2019), Odeen is a closed environment providing all it takes to learn the language needed to solve it. Unlike the ShapeWorld adaptation of Andreas et al. (2017), its score is measured in terms of discovered explanations rather than sparse guessed predictions; further, the test and training set do not share any phenomenon.</p>
<p>Learning theory. Finally, we point out that the expression Explanatory Learning was previously used by Aaronson (2013, Sec. 7) to argue about the necessity of a learning theory that models "predictions about phenomena different in kind from anything observed". The author pointed to the work of Angluin (1987), who generalized the PAC model (Valiant, 1984) by moving the goal from successful predictions to comprehensive explanations.</p>
<h1>7 CONCLUSIONS</h1>
<p>Recently, the attention on the epistemological foundations of deep learning has been growing. The century-old debate between empiricists and rationalists about the source of knowledge persists, with two Turing prizes on opposite sides; LeCun (2019) argues that empiricism still offers a fruitful research agenda for deep learning, while Pearl (2021) supports a rationalist steering to embrace modelbased science principles. This new debate is relevant, since as Pearl notes, today we can submit the balance between empiricism and innateness to experimental evaluation on digital machines.</p>
<p>Limitations and future directions. EL models the essential part of the knowledge acquisition process, namely the interval that turns a mute sequence of symbols into an explanation with reach. However, our modeling assumes a representative set of observations $D_{0}$ to be given (the $k=32$ structures of the new phenomenon). A more comprehensive explanatory model would allow the player to do without these observations, and instead include an interaction phase with the environment where the $D_{0}$ itself is actively discovered. We see this as an exciting direction for follow-ups.</p>
<p>Odeen has potential as a parametric environment to experiment with EL approaches. The structure length ( 6 in this paper), the number of shapes (3), attributes (2), and the grammar specifications (see Appendix A) can be easily tweaked to obtain either simpler or significantly more complex environments. The design choices of this paper provide a good starting point; the resulting benchmark is far from being saturated, but experimenting with different variants is a possibility for the future.</p>
<p>Finally, we expect CRNs to be more resilient than end-to-end models to adversarial attacks. For a given data point $x^{\prime} \in P_{0}$ classified correctly by an empiricist model, a small adversarial change on $D_{0}$ can flip the prediction for $x^{\prime}$ while remaining unnoticed. Conversely, suppose that a CRN made the prediction for $x^{\prime}$, and assume that the correct explanation was ranked as the 5th most likely by the $\mathcal{C G}$. The same attack on $D_{0}$ will have the effect of moving the correct explanation lower in the ranking; however, as long as it stays within the first $t$ conjectures ( 300 in this paper), it will always be found by the interpreter as the correct solution.</p>
<h3>7.1 ETHICS STATEMENT</h3>
<p>The Explainability paragraph in Section 5 briefly touches upon the topic of fairness in AI, pointing to a possible way to safely work with biased data. To this end, our proposed CRN model can be beneficial. In particular, CRNs allow counterfactuals and do not need impractical balanced datasets to avoid algorithmic discrimination in automated decision processes, but just a proper design of the language describing the data (pointing-up/down example). While addressing these aspects is out of scope for this paper and it certainly deserves deeper investigation, we do not foresee any potentially harmful or inappropriate application of our methodology.</p>
<h3>7.2 REPRODUCIbILITY STATEMENT</h3>
<p>Training: The proposed CRN model is composed of two parts, a learnable interpreter $\mathcal{I}$ and a conjecture generator $\mathcal{C G}$. Their architecture is described in Figure 3 (right) and in Appendix B. The training procedure, including the choice of hyperparameters, is also described in Appendix B. Testing: The algorithm used at test time is fully described in Figure 3 (left). Data: One of the main contributions of the paper is the introduction of a new dataset and benchmark, called Odeen. Its full description is given in Section 3 (Problem formulation and Dataset generation paragraphs) and in Appendix A. The latter section also includes a formal definition of the Odeen grammar. The metrics used for evaluation in the Odeen benchmark are defined in Section 3 (Metrics paragraph). A simple interactive version of the Odeen game, to help the readers familiarize with the concept, is available at https://huggingface.co/spaces/gladia/odeen.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work has been funded by the ERC Starting Grant no. 802554 (SPECGEO).
We want to thank the whole GLADIA group for the precious feedback at the beginning of the project, and especially: Luca Cosmo, Marco Fumero (for having played the role of the devil's advocate), Michele Mancusi (for the early philosophical discussion), and Arianna Rampini (for the design of the first version of Odeen).</p>
<p>We would also like to thank Dario Abbondanza, Marco Esposito, Giacomo Nazzaro, and Angela Norelli for the frequent and helpful discussions throughout the work, such as those about ZeroKnowledge proofs and the semiotic theory of Charles S. Peirce, which sometimes lasted until 5 AM.</p>
<p>Finally, we would like to express a special thank you to Paolo Scattini. With his usual enthusiasm, he introduced Antonio to Zendo one night at the Rome Go club, providing the spark that lit this long journey.</p>
<h2>REFERENCES</h2>
<p>Scott Aaronson. Why philosophers should care about computational complexity. Computability: Turing, Gödel, Church, and Beyond, 261:327, 2013.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017.</p>
<p>Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87-106, November 1987. ISSN 0890-5401. doi: 10.1016/0890-5401(87)90052-6. URL https: / / doi . org/10.1016/0890-5401(87)90052-6.</p>
<p>M Balog, AL Gaunt, M Brockschmidt, S Nowozin, and D Tarlow. Deepcoder: Learning to write programs. In 5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings, 2017.</p>
<p>Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5185-5198, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/ 2020.acl-main. 463.</p>
<p>Manuel Blum, Paul Feldman, and Silvio Micali. Non-interactive zero-knowledge and its applications. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, STOC '88, pp. 103-112, New York, NY, USA, 1988. Association for Computing Machinery. ISBN 0897912640. doi: $10.1145 / 62212.62222$. URL https://doi.org/10.1145/62212. 62222 .</p>
<p>François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
David Deutsch. The beginning of infinity: Explanations that transform the world. Penguin UK, 2011.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.</p>
<p>Umberto Eco. Kant and the platypus: Essays on language and cognition. HMH, 2000.
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381, 2020.</p>
<p>Galileo Galilei. Sidereus Nuncius, Or The Sidereal Messenger. University of Chicago Press, 2016. URL http://people.reed.edu/ wieting/mathematics537/ SideriusNuncius.pdf.</p>
<p>Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.</p>
<p>Kory Heath. Zendo, 2001. URL http://www.koryheath.com/zendo/.
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. Generating visual explanations. In European conference on computer vision, pp. 3-19. Springer, 2016.</p>
<p>Michael Hind, Dennis Wei, Murray Campbell, Noel CF Codella, Amit Dhurandhar, Aleksandra Mojsilović, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Ted: Teaching ai to explain its decisions. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. $123-129,2019$.</p>
<p>Douglas Hofstadter. Gödel, Escher, Bach: an eternal golden braid, volume 13. Basic books New York, 1979.
T. M. Hospedales, A. Antoniou, P. Micaelli, and A. J. Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis \&amp; Machine Intelligence, may 2020. ISSN 1939-3539. doi: 10.1109/TPAMI.2021.3079209.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In Proc. CVPR, pp. 1988-1997, 2017.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Alexander Kuhnle and Ann Copestake. Shapeworld - a new test methodology for multimodal language understanding, 2017.</p>
<p>Yann LeCun. The epistemology of deep learning. Institute for Advanced Studies https://www. ias.edu/sites/default/files/video/lecun-ias-20190222.pdf; https: //youtu.be/gG5NCkMerHU, 2019. Accessed: 2021-10-04.</p>
<p>James McConnell. Memory transfer through cannibalism in planarians. J. Neuropsychiat., 3:542$548,1962$.</p>
<p>Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they don't know. arXiv preprint arXiv:1909.12180, 2019.</p>
<p>Melanie Mitchell. Abstraction and analogy-making in artificial intelligence. arXiv preprint arXiv:2102.10717, 2021.</p>
<p>Tom M Mitchell. The need for biases in learning generalizations. Department of Computer Science, Laboratory for Computer Science Research, 1980.</p>
<p>Judea Pearl. Radical empiricism and machine learning research. Journal of Causal Inference, 9(1): $78-82,2021$.</p>
<p>Charles Sanders Peirce. How to make our ideas clear. Popular Science Monthly, 12:286-302, 1878.
Karl Popper. The Logic of Scientific Discovery. Julius Springer, Hutchinson \&amp; Co, 1935.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proc. ICML, 2021.</p>
<p>Gian-Carlo Rota. The pernicious influence of mathematics upon philosophy. Synthese, 88(2):165178, 1991. doi: 10.1007/BF00567744.</p>
<p>Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206-215, 2019.</p>
<p>Adam Santoro, Andrew Lampinen, Kory Mathewson, Timothy Lillicrap, and David Raposo. Symbolic behaviour in artificial intelligence. arXiv preprint arXiv:2102.03406, 2021.</p>
<p>Laura E Schulz, Alison Gopnik, and Clark Glymour. Preschool children learn about causal structure from conditional interventions. Developmental science, 10(3):322-332, 2007.</p>
<p>John R. Searle. Minds, brains, and programs. Behavioral and Brain Sciences, 3(3):417-424, 1980. doi: $10.1017 /$ S0140525X00005756.</p>
<p>Ehud Y Shapiro. Inductive inference of theories from facts. Yale University, Department of Computer Science, 1981.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.</p>
<p>Alex H. Taylor, Rachael Miller, and Russell D. Gray. New caledonian crows reason about hidden causal agents. PNAS, 109(40):16389-16391, 2012.</p>
<p>Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 6000-6010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.</p>
<h1>APPENDIX</h1>
<h2>A FURTHER DETAILS ON THE ODEEN ${ }^{7}$ DATASET</h2>
<p>Training set. The total number of rules produced by the Odeen grammar is 24,794 . We consider training sets varying from 500 to 1438 rules. We choose these rules such that each token and each syntactic construct appears at least once; then, we uniformly select the others from the distribution. We removed from the training set any rule containing the bigram exactly 2 , as well as any rule of the form at_least 2 X and at_most 2 X , equivalent to exactly 2 X . Each rule is associated with a set of $100,1,000$, or 10,0000 labelled structures that unambiguously identify a rule equivalence class.</p>
<p>Test set. We generate the 1,132 games that compose the test set the same way, with the additional constraint of excluding the rules belonging to an equivalence class that is already in the training set. In the test set 72 rules contain the bigram exactly 2. Rules in the test set are associated with just 32 labelled structures. The first 10 structures are chosen by searching pairs of similar structures with different labels, following a common human strategy in Zendo. The remaining 22 structures are selected to ensure the lack of ambiguity on the board.</p>
<p>$$
\begin{aligned}
&amp; \langle\text { RULE }\rangle \models\langle\text { PROP } . \text { S }\rangle \mid\langle\text { PROP }\rangle \mid\langle\text { PROP } . \text { S }\rangle\langle\text { CONJ }\rangle\langle\text { PROP } . \text { S }\rangle \
&amp; \langle\text { PROP }\rangle \models\langle\text { QTY }\rangle\langle\text { OBJ }\rangle\langle\text { REL }\rangle\langle\text { OBJ }\rangle \
&amp; \langle\text { PROP.S }\rangle \models\langle\text { QTY }\rangle\langle\text { OBJ }\rangle \
&amp; \langle\text { OBJ }\rangle \models\langle\text { COL }\rangle \mid\langle\text { SHAPE }\rangle \mid\langle\text { COL }\rangle\langle\text { SHAPE }\rangle \
&amp; \text { 〈QTY〉 } \models \text { at_least }\langle\text { NUM }\rangle \mid \text { exactly }\langle\text { NUM }\rangle \mid \text { at_most }\langle\text { NUM }\rangle \mid \text { zero } \
&amp; \text { 〈SHAPE〉 } \models \text { pyramid }\langle\text { ORIEN }\rangle \mid \text { pyramid | block } \
&amp; \text { 〈REL〉 } \models \text { touching | surrounded_by | at_the_right_of } \
&amp; \text { 〈ORIEN〉 } \models \text { pointing_up | pointing_down } \
&amp; \text { 〈NUM〉 } \models 1 \mid 2 \
&amp; \text { 〈CONJ〉 } \models \text { and } \mid \text { or } \
&amp; \text { 〈COL〉 } \models \text { red } \mid \text { blue }
\end{aligned}
$$</p>
<p>Figure 4: Grammar productions for the Odeen Language.
Formal definition of the Odeen grammar. The context-free grammar in Figure 4 defines all the acceptable rules in Odeen. This grammar only formalizes which rules are syntactically correct. Token names (e.g. red, 1 or touching) do not imply any rule meaning.</p>
<p>The hard-coded interpreter formalizes how to interpret the rules. Similarly to compilers, it tokenizes and transforms the rule into an abstract syntax tree (AST). The interpreter then adds semantic information to the AST, establishing the truth value of each node based on the truth value of its children and the structure under evaluation.</p>
<p>The Odeen binary semantic representations. By simulating the process of scientific discovery, Odeen offers a convenient simulation of a world described by a language. Besides the computational tractability, the simplicity and adjustable size of the Odeen world allows us to explicit the whole semantics of its language.</p>
<p>This semantics can be encoded in a binary semantic matrix $S$ with the 24,794 rules $e_{i}$ on the rows and the 117,649 structures $x_{j}$ on the columns. The $s_{i j}$ element of this matrix is equal to 1 if the structure $x_{j}$ complies with the rule $e_{i}$ and 0 otherwise, see inset in Section 3, Metrics paragraph. $S_{i <em>}$, the 117,649-dimensional binary vector coinciding with the $i$-th row of $S$, fully represents the meaning of rule $e_{i}$ in the Odeen world. Similarly, each structure $x_{j}$ is represented by the 24,794dimensional binary vector coinciding with the column $S_{</em> j}$ of $S$.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: Hamming weight of the binary semantic representation of each rule (a) and each structure (b). We sort them in descending order for visualization purposes.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: PCA applied to the binary semantic representation of structures (a) and rules (b). We highlight in red the structures that have two touching pyramids pointing down. To ease the visualization, we pair every structure with a different string of characters. Each character replaces an element of Odeen according to a well-defined mapping. The rules distribution reflect what can be observed in Figure 5a.</p>
<p>In Figure 5, we analyze the distribution of the Hamming weights (i.e., the number of ones) in $\left{S_{i <em>}\right}<em>{i=1}^{117,649}$ (5a) and $\left{S</em>{</em> j}\right}<em>{j=1}^{24,794}$ (5b). We observe an asymmetry between the rule and structure distributions. On one hand, the semantic representation of a rule can be quite unbalanced, with populated extremes of rules evaluating all structures with 1 (or 0 ) as shown in Figure 5a. On the other hand, Figure 5b shows that the semantic representations of structures are very balanced; most of them have around half zeros and half ones, with no structure with less than 10 k or more than 14 k ones.
This balanced trend, along with the well separable PCA of $\left{S</em>{<em> j}\right}<em>{j=1}^{24,794}$ (Figure 6a) suggests that the chosen language produce representations that are effective in separating structures. Conversely, the PCA of $\left{S</em>{i </em>}\right}_{i=1}^{117,649}$ is much less homogeneous (Figure 6b). Here we can recognize two poles, corresponding respectively to rules with all ones and all zeros. We believe that this analysis of the binary semantic representations is only partial, and we leave further exploration for follow-up work.</p>
<h1>B IMPLEMENTATION DETAILS</h1>
<p>In this paragraph, we give the implementation details of the models proposed and depicted in Figure 3 (right). All the models are based on a Transformer block composed of 4 layers and 8 heads. We used a hidden dimension of 256 for all the models except for the interpreter, where we used a hidden dimension of 128 . The models differ primarily by the type of transformer block used (encoder/decoder), inputs and embeddings. In detail:</p>
<ul>
<li>Transformer Label Decoder. This is a transformer block used to predict a label given a structure. The input structure is a sequence of six learned embeddings, one per piece. We add a sinusoidal positional encoding to each embedding as in the original transformer implementation. The embedding size is 128 in the $\mathcal{I}$ and 256 in the Empiricist models (EMP-C, EMP-R). We used the standard transformer encoder block and added a special token [CLS] at the beginning of the structure like in Devlin et al. (2019) to perform the classification task.</li>
<li>Transformer Rule Decoder. This is a transformer decoder block with embedding size of 128 and sinusoidal positional encoding. This decoder block is used to generate the rule by the EMP-C and $\mathcal{C G}$ models.</li>
<li>Transformer Board Encoder. This is a transformer encoder block used to encode the (structure, label) pairs. The input is encoded a sequence of 32 learned embeddings, one per structure-label pair. The size of each embedding is 256 . We did not add positional encodings, since the specific position of structure-label pairs among the 32 is not relevant. This block is used in all the models.</li>
<li>Transformer Rule Encoder. This transformer encoder block is used in $\mathcal{I}$ to encode the rule. Its implementation is analogous to the Transformer Rule Decoder, with the only difference that it does not use causal attention since it is an encoder layer.</li>
</ul>
<p>Table 2: Number of training epochs for each training regimen.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TRAINING REGIMEN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NUMBER OF EPOCHS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">10K STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">1K STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">100 STRUCT.</td>
<td style="text-align: center;">1438 RULES</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">10K STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">1K STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">58</td>
</tr>
<tr>
<td style="text-align: left;">100 STRUCT.</td>
<td style="text-align: center;">500 RULES</td>
<td style="text-align: center;">576</td>
</tr>
</tbody>
</table>
<p>Training Procedure. All the models are trained with a learning rate of $3 \cdot 10^{-4}$ using Adam (Kingma \&amp; Ba, 2017), a batch size of 512 and early-stop and dropout set to 0.1 to prevent overfitting. We train all the models on randomly sampled sets of 32 (structure, label) pairs to prevent overfitting on specific boards. Table 2 describes the number of epochs for each training regimen. Models are trained to: predict the label of a structure give the board (EMP-R); predict the label of a structure given the 32 pairs (structure, label) and the associated rule (EMP-C); predict the rule given the 32 pairs $(\mathcal{C G})$; predict the label of a structure given a rule $(\mathcal{I})$.</p>
<p>Table 3: T-Acc and NRS for different training regimens.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Train Data</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NRS</th>
<th style="text-align: center;">T-Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">10K struct. <br> 1438 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.984</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.895</td>
</tr>
<tr>
<td style="text-align: center;">1K struct. <br> 1438 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.980</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.898</td>
</tr>
<tr>
<td style="text-align: center;">100 struct. <br> 1438 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.939</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.865</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.896</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Train Data</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NRS</th>
<th style="text-align: center;">T-Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">10K struct. <br> 500 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.932</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">0.869</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.863</td>
</tr>
<tr>
<td style="text-align: center;">1K struct. <br> 500 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.874</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.876</td>
</tr>
<tr>
<td style="text-align: center;">100 struct. <br> 500 rules</td>
<td style="text-align: center;">CRN</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.883</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-C</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.823</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EMP-R</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.872</td>
</tr>
</tbody>
</table>
<h1>C EFFICIENCY</h1>
<p>Data efficiency In the Odeen challenge, CRNs require less training data to match the performance of empiricist models. For instance, in the case of 1438 rules at training, we see in Table 3 that the CRN trained on 100 structures per rule (NRS $=40,2 \%$ ) still overcomes the performance of empiricist models trained on a dataset 100 times bigger (NRS $=35.2 \%$ on 10k structures per rule).</p>
<p>Computational cost. In this section, we discuss the computational cost at test time of the rationalist and empiricists approaches. Table 4 reports the costs of tagging $s$ new structures, while 5 reports the costs of explicitly predicting the textual rule. We evaluate the cost per game in two ways: i) by counting the number of calls of each trained neural network and ii) by measuring the absolute time in seconds of each method with the same hardware configuration.</p>
<p>We refer to the first quantity as the Computational Cost and parametrize it in terms of the main blocks of the models. This value is independent of the batch size and the hardware adopted. As an example, the cost of tagging the new structures for a CRN using 300 conjectures is given by:</p>
<p>$$
300 \cdot \mathcal{C G}+300 \cdot b \cdot \mathcal{I}+s \cdot \mathcal{I}
$$</p>
<p>Where $300 \cdot \mathcal{C G}$ stands for the 300 beams used to get 300 conjectures from the conjecture generator $\mathcal{C G}$. Each conjecture (300) is then tested on all the board structures (b) by the interpreter $\mathcal{I}$. Finally $\mathcal{I}$ is called to apply the chosen conjecture on each new structure ( $s$ ). As an upper bound, an exhaustive search algorithm (ExV SRC) uses no conjecture generator, and thus has to evaluate with $\mathcal{I}$ all admissible rules $(r)$ on each structure of the board. Conversely, the empiricist approach provide label predictions through a single end-to-end model which is simply called $s$ times. Concerning the problem of inferring explicitly the textual rule, using more beams in the empiricists models does not provide any increase in performance, i.e. the true rule is not a more probable proposition accessible through a larger beam search.</p>
<p>We measured also the absolute time in seconds with the following hardware configuration for all the experiments: 1 single core hyper threaded Xeon CPU Processor with 2.2 Ghz, 2 threads; 12.7 GiB. of RAM; a Tesla T4 GPU, with 320 Turing Tensor Core, 2,560 NVIDIA CUDA cores, and 15.7 GDDR6 GiB of VRAM.</p>
<p>Table 4: Computational cost of our models at test time to tag $s$ new structures. In Odeen $r=24,794$, $b=32, s=1,176$. Notice how CRNs offer a good balance between computational efficiency and performance, this trade-off is regulated by a single parameter, the number of beams.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODEL</th>
<th style="text-align: center;">COMPUTATIONAL COST</th>
<th style="text-align: center;">T (s)</th>
<th style="text-align: center;">NRS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EXV SRC</td>
<td style="text-align: center;">$r \cdot b \cdot \mathcal{I}+s \cdot \mathcal{I}$</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">CRN [300B]</td>
<td style="text-align: center;">$300 \cdot \mathcal{C G}+300 \cdot b \cdot \mathcal{I}+s \cdot \mathcal{I}$</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">CRN [10B]</td>
<td style="text-align: center;">$10 \cdot \mathcal{C G}+10 \cdot b \cdot \mathcal{I}+s \cdot \mathcal{I}$</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: left;">EMP</td>
<td style="text-align: center;">$s \cdot$ EMP-R</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.35</td>
</tr>
</tbody>
</table>
<p>Table 5: Computational cost of our models at test time to produce the textual rule in output</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODEL</th>
<th style="text-align: center;">COMPUTATIONAL COST</th>
<th style="text-align: center;">T (s)</th>
<th style="text-align: center;">R-ACC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EXV SRC</td>
<td style="text-align: center;">$r \cdot b \cdot \mathcal{I}$</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">CRN [300B]</td>
<td style="text-align: center;">$300 \cdot \mathcal{C G}+300 \cdot b \cdot \mathcal{I}$</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: left;">CRN [10B]</td>
<td style="text-align: center;">$10 \cdot \mathcal{C G}+10 \cdot b \cdot \mathcal{I}$</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: left;">EMP [300B]</td>
<td style="text-align: center;">$300 \cdot$ EMP-C</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">EMP [10B]</td>
<td style="text-align: center;">$10 \cdot$ EMP-C</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">EMP [1B]</td>
<td style="text-align: center;">$1 \cdot$ EMP-C</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.07</td>
</tr>
</tbody>
</table>
<h1>D ODEEN EXAMPLE GAMES</h1>
<p>In this section we propose a collection of qualitative results showing a series of Odeen games from the test set and how they are solved by the proposed models. For each model, we report the predicted rule (only for EMP-C and the CRN), the accuracy on the structures labeling (T-acc), and a mark that indicates whether the nearest rule is the correct one (NRS). All the models are trained on 1,438 rules with 1,000 structures per rule.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Board 01
Golden Rule: "at_least 2 pyramid pointing_down"
CRN: "at_least 2 pyramid pointing_down"; T-acc $1.0 \checkmark$
EMP-C: "at_least 1 pyramid touching touching"; T-acc: $0.76 \boldsymbol{X}$
EMP-R: T-acc $0.72 \boldsymbol{X}$
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Board 04
Golden Rule: "at_most 1 blue pyramid pointing_up"
CRN: "zero blue or at_most 1 blue pyramid pointing_up"; T-acc $1.0 \checkmark$
EMP-C: "zero 1 blue touching or or"; T-acc: $0.89 \boldsymbol{X}$
EMP-R: T-acc $0.92 \checkmark$</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Board 09
Golden rule: "exactly 1 pyramid pointing_up touching red pyramid pointing_down"
CRN: "exactly 1 red pyramid pointing_down touching pyramid pointing_up", T-acc $0.95 \boldsymbol{X}$
EMP-C: "exactly 1 red at_the_right_of and red", T-acc: $0.80 \boldsymbol{X}$
EMP-R: T-acc $0.79 \boldsymbol{X}$
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Board 25
Golden rule: "at_least 2 red touching blue pyramid pointing_down"
CRN: "at_least 2 red touching blue pyramid pointing_down", T-acc $1.0 \checkmark$
EMP-C: "at_least 2 red touching blue pyramid", T-acc: $0.87 \boldsymbol{X}$
EMP-R: T-acc $0.69 \boldsymbol{X}$
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Board 30
Golden rule: "exactly 1 blue pyramid touching blue block"
CRN: "exactly 1 blue pyramid touching blue block", T-acc $1.0 \checkmark$
EMP-C: "exactly 1 blue pyramid touching block block", T-acc: $0.97 \checkmark$
EMP-R: T-acc $0.79 \boldsymbol{X}$</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Board 75
Golden rule: "zero blue touching red pyramid"
CRN: "zero blue touching red pyramid", T-acc $1.0 \checkmark$
EMP-C: "zero blue touching red", T-acc: $0.85 \boldsymbol{X}$
EMP-R: T-acc $0.91 \checkmark$
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Board 97
Golden rule: "at_most 1 red block touching red"
CRN: "at_most 1 red block touching red", T-acc $1.0 \checkmark$
EMP-C: "at_most 1 red touching at_the_right_of red", T-acc: $0.98 \checkmark$
EMP-R: T-acc $0.93 \boldsymbol{X}$
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Board 103
Golden rule: "at_most 1 blue pyramid pointing_down touching red"
CRN: "at_most 1 blue pyramid pointing_down touching red", T-acc $1.0 \checkmark$
EMP-C: "at_most 1 blue pyramid pointing_down touching red", T-acc: $0.98 \checkmark$
EMP-R: T-acc $0.85 \boldsymbol{X}$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Odeen is the Rational alien in The Gods Themselves (1972), a novel by Isaac Asimov about a conspiracy against Earth by the inhabitants of a parallel universe with different physical laws.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
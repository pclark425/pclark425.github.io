<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9038 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9038</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9038</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-272881158</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.16605v1.pdf" target="_blank">Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies have evaluated the creativity/novelty of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science. However, accessing the novelty in scholarly publications is a largely unexplored area in evaluating LLMs. In this paper, we introduce a scholarly novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in scholarly papers. SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart. In each pair, the more recently published paper is assumed to be more novel. Additionally, we propose RAG-Novelty, which simulates the review process taken by human reviewers by leveraging the retrieval of similar papers to assess novelty. Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9038.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9038.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on TTCT (Guzik 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on the Torrance Tests of Creative Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Guzik et al. (2023) applied a basic prompting approach to evaluate GPT-4 on the Torrance Tests of Creative Thinking (TTCT), a standard cognitive creativity battery assessing divergent thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The originality of machines: Ai takes the torrance test.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, a large autoregressive transformer trained on diverse internet-scale text (referenced as OpenAI, 2023 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Torrance Tests of Creative Thinking (TTCT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A standardized creativity battery (six tasks) assessing divergent thinking and creative production using measures like fluency, originality, flexibility, and elaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Guzik et al. evaluated GPT-4 on TTCT using basic prompts; specific numerical scores are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not reported in this paper; study is cited as an empirical evaluation of LLM performance on TTCT.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Basic prompting was used to elicit responses from GPT-4 on TTCT items (as cited); no numerical outcomes provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper only cites the existence of the evaluation; no quantitative results or human baselines are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9038.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expanded TTCT analyses (Zhao 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis of LLM responses on an expanded TTCT benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zhao et al. (2024) analyzed LLM responses to an expanded TTCT, testing multiple prompt strategies (basic, instructive, post-instructive, Chain-of-Thought) to study LLM creativity assessment/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and understanding creativity in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (unspecified aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple LLMs analyzed under different prompting regimes; paper cites use of prompt engineering and CoT to probe creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Expanded Torrance Tests of Creative Thinking (TTCT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Expanded TTCT used to probe semantic/divergent creativity with multiple prompt variants to elicit and analyze LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Descriptive analyses reported; no single numeric performance value provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not quantified here; the cited work focuses on effects of prompting strategies rather than absolute numerical comparisons to humans in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Compared basic, instructive, post-instructive, and Chain-of-Thought prompts to elicit divergent responses from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Cited as evidence that prompting strategy materially affects observed LLM creativity; no standardized single benchmark score reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9038.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 on AUT (Stevenson 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving GPT-3 performance on the Alternative Uses Task via role definition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Stevenson et al. (2022) showed that defining the LLM's role (e.g., 'scientist') can improve GPT-3 performance on AUT-like divergent thinking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Putting gpt-3's creativity to the (alternative uses) test.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3 series — large transformer LLMs trained on internet text; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Alternative Uses Task (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A divergent thinking test asking participants to list as many uses as possible for a common object; scored on fluency, originality, flexibility, elaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported improvement when the model role was specified; no absolute numeric scores provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Role prompting improved GPT-3's AUT performance relative to weaker prompting, but no direct numeric comparison to human baselines is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Role-definition prompt (e.g., instructing the model to act as a 'scientist') improves outputs on AUT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Findings emphasize prompt sensitivity; magnitude relative to human performance not provided in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9038.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 multi-step on AUT (Summers-Stay 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step prompting to boost GPT-3 performance on AUT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summers-Stay et al. (2023) improved GPT-3's performance on AUT by using multi-step reasoning/interaction, demonstrating gains from structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Brainstorm, then select: a generative language model improves its creativity score.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family of transformer LLMs; improved via multi-step prompting rather than model weight changes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Alternative Uses Task (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Divergent thinking task measuring number and novelty of uses generated for everyday objects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Multi-step prompting enhanced GPT-3 AUT scores compared to simpler prompts; numeric values not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Improved relative to simpler prompting baselines; no direct human baseline comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multi-step interaction (brainstorm then select) used to generate and refine responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Improvement attributed to prompting strategy; absolute comparison to human creative output not reported in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9038.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Discussion on WCT & SCT (Lu 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-play enhanced LLM discussion to augment ChatGPT on Wallach–Kogan and Scholarly Creativity Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lu et al. (2024) developed a multi-agent roleplay/discussion framework (LLM Discussion) that improved ChatGPT's performance on WCT and SCT-style benchmarks by simulating reviewer discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM discussion: Enhancing the creativity of large language models via discussion framework and roleplay.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (unspecified ChatGPT version)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT variants (generative chat-oriented LLMs) used within a multi-role discussion framework to evaluate creative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wallach–Kogan Creativity Tests (WCT) and Scholarly Creativity Test (SCT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WCT includes AUT and related tests for creative production; SCT measures scholarly creativity and process skills.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Framework reported to augment ChatGPT's performance on WCT and SCT tasks; no numeric scores provided in this paper summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM Discussion improved model outputs relative to non-discussion prompting; human baseline comparisons not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Role-assignment (e.g., professor, PhD student, editor) and iterative multi-round discussion among LLM roles before final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper indicates improvement via roleplay but this current paper notes LLM Discussion performed poorly in the SchNovel experiments, highlighting context dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9038.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orwig 2024 (GPT-4 stories)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 story generation compared to humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Orwig et al. (2024) evaluated GPT-4 story generation and concluded model-generated stories are comparable to human-written stories regarding creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The language of creativity: Evidence from humans and large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 model; evaluated on creative story generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Creative story-generation tasks (related to TTCW / creative writing assessments)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assessments of creative writing quality and creativity, comparable to Torrance-style creative writing evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to produce stories comparable to humans in creativity measures (qualitative conclusion; no numeric score provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human-level creativity in story generation according to Orwig et al. (2024) as cited.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Story generation evaluated and compared against human-written stories; methodological details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Cited as one of several studies with mixed conclusions; differences across benchmarks and metrics complicate interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9038.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pépin 2024 (divergent association)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs outperform humans on divergent association and creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pépin et al. (2024) reported that in some creative tasks (divergent association task and creative writing) LLMs can surpass human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Divergent creativity in humans and large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language models evaluated on divergent association and creative writing tasks; specifics in cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Divergent Association Task (DAT) and creative writing evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>DAT measures semantic distance among produced words; creative writing evaluated on creativity dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to surpass humans in specific tasks (DAT and creative writing) per Pépin et al. (2024) as cited; no numeric details in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs reported to outperform humans on those specific tasks in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Task-specific prompt designs and evaluation metrics (semantic distance for DAT, creative quality for writing) were used in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper notes inconsistent conclusions across studies; results depend on task formulation and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9038.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chakrabarty 2024 (TTCW failure rate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated stories on Torrance Test for Creative Writing (TTCW)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chakrabarty et al. (2024) found LLM-generated stories pass the Torrance Test for Creative Writing 3 to 10 times less frequently than professional human writers, indicating a notable deficit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Art or artifice? large language models and the false promise of creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-generated stories (e.g., ChatGPT/GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative LLMs used to produce stories evaluated by TTCW; specifics in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Torrance Test for Creative Writing (TTCW)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A Torrance-derived test focusing on creative writing quality and originality, judged by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLM-generated stories passed TTCW 3–10 times less frequently than professional writers (reported ratio from cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Professional human writers (baseline) passed TTCW at a rate 3–10× higher than LLM-generated stories (as reported in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs substantially underperform professional humans on TTCW according to the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparative evaluation of model-generated vs. professional human stories using TTCW scoring; exact scoring details are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Quantified deficit exists in this cited study, but other studies report more favorable LLM outcomes depending on tasks and metrics; cross-study inconsistency noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9038.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Organisciak 2023 (auto-scoring divergent thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated scoring of divergent thinking with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Organisciak et al. (2023) demonstrated that automated scoring of divergent thinking benefits greatly from LLMs, including fine-tuning on human-scored data to evaluate originality and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned LLMs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models fine-tuned on human-scored divergent-thinking data to perform automated scoring of originality and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Automated scoring frameworks for divergent thinking (e.g., AUT scoring pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Methods to automatically score divergent thinking outputs for originality, fluency, and quality using semantic and learned metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Fine-tuned LLMs substantially improved automated scoring accuracy relative to simpler semantic-distance baselines (as reported in cited work); numeric values not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM-based automated scoring improves alignment with human scores compared to prior automated methods; not a direct model-vs-human creative output comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuning on human-scored data and evaluation of automated scoring accuracy against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>This concerns scoring of creative outputs rather than LLMs' creative generation; improvements are relative to other automated scorers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9038.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9038.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Luchini 2023 (auto-scoring comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM automatic scoring versus human ratings for creative problem-solving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Luchini et al. (2023) compared LLM automatic scoring of creative problem-solving to human ratings, analyzing originality and quality assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic scoring of creative problem-solving with large language models: A comparison of originality and quality ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models used for automated scoring</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs applied as automated raters to judge originality and quality of creative problem-solving outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Automated scoring of creative problem-solving tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Frameworks for scoring originality and quality of creative responses, comparing model judgments to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLMs can replicate aspects of human scoring for originality and quality; precise numerical concordance is not reported in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs provide useful automated scoring approximations but the paper discusses comparison rather than model creative superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison between human rater scores and LLM-produced scores for creative problem-solving; methodological specifics are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Focus is on scoring alignment rather than model creative output quality; generalization depends on task and scoring rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The originality of machines: Ai takes the torrance test. <em>(Rating: 2)</em></li>
                <li>Putting gpt-3's creativity to the (alternative uses) test. <em>(Rating: 2)</em></li>
                <li>Brainstorm, then select: a generative language model improves its creativity score. <em>(Rating: 2)</em></li>
                <li>LLM discussion: Enhancing the creativity of large language models via discussion framework and roleplay. <em>(Rating: 2)</em></li>
                <li>The language of creativity: Evidence from humans and large language models. <em>(Rating: 2)</em></li>
                <li>Divergent creativity in humans and large language models. <em>(Rating: 2)</em></li>
                <li>Art or artifice? large language models and the false promise of creativity. <em>(Rating: 2)</em></li>
                <li>Assessing and understanding creativity in large language models <em>(Rating: 2)</em></li>
                <li>Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models. <em>(Rating: 2)</em></li>
                <li>Automatic scoring of creative problem-solving with large language models: A comparison of originality and quality ratings. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9038",
    "paper_id": "paper-272881158",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4 on TTCT (Guzik 2023)",
            "name_full": "GPT-4 evaluated on the Torrance Tests of Creative Thinking",
            "brief_description": "Guzik et al. (2023) applied a basic prompting approach to evaluate GPT-4 on the Torrance Tests of Creative Thinking (TTCT), a standard cognitive creativity battery assessing divergent thinking.",
            "citation_title": "The originality of machines: Ai takes the torrance test.",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4, a large autoregressive transformer trained on diverse internet-scale text (referenced as OpenAI, 2023 in the paper).",
            "model_size": null,
            "test_battery_name": "Torrance Tests of Creative Thinking (TTCT)",
            "test_description": "A standardized creativity battery (six tasks) assessing divergent thinking and creative production using measures like fluency, originality, flexibility, and elaboration.",
            "llm_performance": "Guzik et al. evaluated GPT-4 on TTCT using basic prompts; specific numerical scores are not reported in this paper.",
            "human_baseline_performance": null,
            "performance_comparison": "Not reported in this paper; study is cited as an empirical evaluation of LLM performance on TTCT.",
            "experimental_details": "Basic prompting was used to elicit responses from GPT-4 on TTCT items (as cited); no numerical outcomes provided in this paper.",
            "limitations_or_caveats": "Paper only cites the existence of the evaluation; no quantitative results or human baselines are reported here.",
            "uuid": "e9038.0",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Expanded TTCT analyses (Zhao 2024)",
            "name_full": "Analysis of LLM responses on an expanded TTCT benchmark",
            "brief_description": "Zhao et al. (2024) analyzed LLM responses to an expanded TTCT, testing multiple prompt strategies (basic, instructive, post-instructive, Chain-of-Thought) to study LLM creativity assessment/generation.",
            "citation_title": "Assessing and understanding creativity in large language models",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (unspecified aggregate)",
            "model_description": "Multiple LLMs analyzed under different prompting regimes; paper cites use of prompt engineering and CoT to probe creativity.",
            "model_size": null,
            "test_battery_name": "Expanded Torrance Tests of Creative Thinking (TTCT)",
            "test_description": "Expanded TTCT used to probe semantic/divergent creativity with multiple prompt variants to elicit and analyze LLM outputs.",
            "llm_performance": "Descriptive analyses reported; no single numeric performance value provided in this paper's discussion.",
            "human_baseline_performance": null,
            "performance_comparison": "Not quantified here; the cited work focuses on effects of prompting strategies rather than absolute numerical comparisons to humans in this summary.",
            "experimental_details": "Compared basic, instructive, post-instructive, and Chain-of-Thought prompts to elicit divergent responses from LLMs.",
            "limitations_or_caveats": "Cited as evidence that prompting strategy materially affects observed LLM creativity; no standardized single benchmark score reported in this paper.",
            "uuid": "e9038.1",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-3 on AUT (Stevenson 2022)",
            "name_full": "Improving GPT-3 performance on the Alternative Uses Task via role definition",
            "brief_description": "Stevenson et al. (2022) showed that defining the LLM's role (e.g., 'scientist') can improve GPT-3 performance on AUT-like divergent thinking tasks.",
            "citation_title": "Putting gpt-3's creativity to the (alternative uses) test.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "OpenAI's GPT-3 series — large transformer LLMs trained on internet text; specifics not detailed in this paper.",
            "model_size": null,
            "test_battery_name": "Alternative Uses Task (AUT)",
            "test_description": "A divergent thinking test asking participants to list as many uses as possible for a common object; scored on fluency, originality, flexibility, elaboration.",
            "llm_performance": "Reported improvement when the model role was specified; no absolute numeric scores provided in this paper.",
            "human_baseline_performance": null,
            "performance_comparison": "Role prompting improved GPT-3's AUT performance relative to weaker prompting, but no direct numeric comparison to human baselines is provided here.",
            "experimental_details": "Role-definition prompt (e.g., instructing the model to act as a 'scientist') improves outputs on AUT.",
            "limitations_or_caveats": "Findings emphasize prompt sensitivity; magnitude relative to human performance not provided in this paper's summary.",
            "uuid": "e9038.2",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-3 multi-step on AUT (Summers-Stay 2023)",
            "name_full": "Multi-step prompting to boost GPT-3 performance on AUT",
            "brief_description": "Summers-Stay et al. (2023) improved GPT-3's performance on AUT by using multi-step reasoning/interaction, demonstrating gains from structured prompting.",
            "citation_title": "Brainstorm, then select: a generative language model improves its creativity score.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "GPT-3 family of transformer LLMs; improved via multi-step prompting rather than model weight changes.",
            "model_size": null,
            "test_battery_name": "Alternative Uses Task (AUT)",
            "test_description": "Divergent thinking task measuring number and novelty of uses generated for everyday objects.",
            "llm_performance": "Multi-step prompting enhanced GPT-3 AUT scores compared to simpler prompts; numeric values not given in this paper.",
            "human_baseline_performance": null,
            "performance_comparison": "Improved relative to simpler prompting baselines; no direct human baseline comparison provided here.",
            "experimental_details": "Multi-step interaction (brainstorm then select) used to generate and refine responses.",
            "limitations_or_caveats": "Improvement attributed to prompting strategy; absolute comparison to human creative output not reported in this summary.",
            "uuid": "e9038.3",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM Discussion on WCT & SCT (Lu 2024)",
            "name_full": "Role-play enhanced LLM discussion to augment ChatGPT on Wallach–Kogan and Scholarly Creativity Tests",
            "brief_description": "Lu et al. (2024) developed a multi-agent roleplay/discussion framework (LLM Discussion) that improved ChatGPT's performance on WCT and SCT-style benchmarks by simulating reviewer discussion.",
            "citation_title": "LLM discussion: Enhancing the creativity of large language models via discussion framework and roleplay.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (unspecified ChatGPT version)",
            "model_description": "ChatGPT variants (generative chat-oriented LLMs) used within a multi-role discussion framework to evaluate creative outputs.",
            "model_size": null,
            "test_battery_name": "Wallach–Kogan Creativity Tests (WCT) and Scholarly Creativity Test (SCT)",
            "test_description": "WCT includes AUT and related tests for creative production; SCT measures scholarly creativity and process skills.",
            "llm_performance": "Framework reported to augment ChatGPT's performance on WCT and SCT tasks; no numeric scores provided in this paper summary.",
            "human_baseline_performance": null,
            "performance_comparison": "LLM Discussion improved model outputs relative to non-discussion prompting; human baseline comparisons not provided here.",
            "experimental_details": "Role-assignment (e.g., professor, PhD student, editor) and iterative multi-round discussion among LLM roles before final decision.",
            "limitations_or_caveats": "Paper indicates improvement via roleplay but this current paper notes LLM Discussion performed poorly in the SchNovel experiments, highlighting context dependence.",
            "uuid": "e9038.4",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Orwig 2024 (GPT-4 stories)",
            "name_full": "GPT-4 story generation compared to humans",
            "brief_description": "Orwig et al. (2024) evaluated GPT-4 story generation and concluded model-generated stories are comparable to human-written stories regarding creativity.",
            "citation_title": "The language of creativity: Evidence from humans and large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 model; evaluated on creative story generation tasks.",
            "model_size": null,
            "test_battery_name": "Creative story-generation tasks (related to TTCW / creative writing assessments)",
            "test_description": "Assessments of creative writing quality and creativity, comparable to Torrance-style creative writing evaluations.",
            "llm_performance": "Reported to produce stories comparable to humans in creativity measures (qualitative conclusion; no numeric score provided here).",
            "human_baseline_performance": null,
            "performance_comparison": "LLM matches human-level creativity in story generation according to Orwig et al. (2024) as cited.",
            "experimental_details": "Story generation evaluated and compared against human-written stories; methodological details are in the cited work.",
            "limitations_or_caveats": "Cited as one of several studies with mixed conclusions; differences across benchmarks and metrics complicate interpretation.",
            "uuid": "e9038.5",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Pépin 2024 (divergent association)",
            "name_full": "LLMs outperform humans on divergent association and creative writing",
            "brief_description": "Pépin et al. (2024) reported that in some creative tasks (divergent association task and creative writing) LLMs can surpass human performance.",
            "citation_title": "Divergent creativity in humans and large language models.",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (unspecified)",
            "model_description": "Large language models evaluated on divergent association and creative writing tasks; specifics in cited paper.",
            "model_size": null,
            "test_battery_name": "Divergent Association Task (DAT) and creative writing evaluations",
            "test_description": "DAT measures semantic distance among produced words; creative writing evaluated on creativity dimensions.",
            "llm_performance": "Reported to surpass humans in specific tasks (DAT and creative writing) per Pépin et al. (2024) as cited; no numeric details in this summary.",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs reported to outperform humans on those specific tasks in the cited work.",
            "experimental_details": "Task-specific prompt designs and evaluation metrics (semantic distance for DAT, creative quality for writing) were used in the cited study.",
            "limitations_or_caveats": "Paper notes inconsistent conclusions across studies; results depend on task formulation and metrics.",
            "uuid": "e9038.6",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Chakrabarty 2024 (TTCW failure rate)",
            "name_full": "LLM-generated stories on Torrance Test for Creative Writing (TTCW)",
            "brief_description": "Chakrabarty et al. (2024) found LLM-generated stories pass the Torrance Test for Creative Writing 3 to 10 times less frequently than professional human writers, indicating a notable deficit.",
            "citation_title": "Art or artifice? large language models and the false promise of creativity.",
            "mention_or_use": "mention",
            "model_name": "LLM-generated stories (e.g., ChatGPT/GPT variants)",
            "model_description": "Generative LLMs used to produce stories evaluated by TTCW; specifics in the cited study.",
            "model_size": null,
            "test_battery_name": "Torrance Test for Creative Writing (TTCW)",
            "test_description": "A Torrance-derived test focusing on creative writing quality and originality, judged by experts.",
            "llm_performance": "LLM-generated stories passed TTCW 3–10 times less frequently than professional writers (reported ratio from cited work).",
            "human_baseline_performance": "Professional human writers (baseline) passed TTCW at a rate 3–10× higher than LLM-generated stories (as reported in cited work).",
            "performance_comparison": "LLMs substantially underperform professional humans on TTCW according to the cited study.",
            "experimental_details": "Comparative evaluation of model-generated vs. professional human stories using TTCW scoring; exact scoring details are in the cited paper.",
            "limitations_or_caveats": "Quantified deficit exists in this cited study, but other studies report more favorable LLM outcomes depending on tasks and metrics; cross-study inconsistency noted.",
            "uuid": "e9038.7",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Organisciak 2023 (auto-scoring divergent thinking)",
            "name_full": "Automated scoring of divergent thinking with LLMs",
            "brief_description": "Organisciak et al. (2023) demonstrated that automated scoring of divergent thinking benefits greatly from LLMs, including fine-tuning on human-scored data to evaluate originality and quality.",
            "citation_title": "Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models.",
            "mention_or_use": "mention",
            "model_name": "Fine-tuned LLMs (various)",
            "model_description": "Models fine-tuned on human-scored divergent-thinking data to perform automated scoring of originality and quality.",
            "model_size": null,
            "test_battery_name": "Automated scoring frameworks for divergent thinking (e.g., AUT scoring pipelines)",
            "test_description": "Methods to automatically score divergent thinking outputs for originality, fluency, and quality using semantic and learned metrics.",
            "llm_performance": "Fine-tuned LLMs substantially improved automated scoring accuracy relative to simpler semantic-distance baselines (as reported in cited work); numeric values not provided here.",
            "human_baseline_performance": null,
            "performance_comparison": "LLM-based automated scoring improves alignment with human scores compared to prior automated methods; not a direct model-vs-human creative output comparison.",
            "experimental_details": "Fine-tuning on human-scored data and evaluation of automated scoring accuracy against human labels.",
            "limitations_or_caveats": "This concerns scoring of creative outputs rather than LLMs' creative generation; improvements are relative to other automated scorers.",
            "uuid": "e9038.8",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Luchini 2023 (auto-scoring comparison)",
            "name_full": "Comparison of LLM automatic scoring versus human ratings for creative problem-solving",
            "brief_description": "Luchini et al. (2023) compared LLM automatic scoring of creative problem-solving to human ratings, analyzing originality and quality assessments.",
            "citation_title": "Automatic scoring of creative problem-solving with large language models: A comparison of originality and quality ratings.",
            "mention_or_use": "mention",
            "model_name": "Large language models used for automated scoring",
            "model_description": "LLMs applied as automated raters to judge originality and quality of creative problem-solving outputs.",
            "model_size": null,
            "test_battery_name": "Automated scoring of creative problem-solving tasks",
            "test_description": "Frameworks for scoring originality and quality of creative responses, comparing model judgments to human raters.",
            "llm_performance": "LLMs can replicate aspects of human scoring for originality and quality; precise numerical concordance is not reported in this paper's summary.",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs provide useful automated scoring approximations but the paper discusses comparison rather than model creative superiority.",
            "experimental_details": "Comparison between human rater scores and LLM-produced scores for creative problem-solving; methodological specifics are in the cited work.",
            "limitations_or_caveats": "Focus is on scoring alignment rather than model creative output quality; generalization depends on task and scoring rubric.",
            "uuid": "e9038.9",
            "source_info": {
                "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The originality of machines: Ai takes the torrance test.",
            "rating": 2,
            "sanitized_title": "the_originality_of_machines_ai_takes_the_torrance_test"
        },
        {
            "paper_title": "Putting gpt-3's creativity to the (alternative uses) test.",
            "rating": 2,
            "sanitized_title": "putting_gpt3s_creativity_to_the_alternative_uses_test"
        },
        {
            "paper_title": "Brainstorm, then select: a generative language model improves its creativity score.",
            "rating": 2,
            "sanitized_title": "brainstorm_then_select_a_generative_language_model_improves_its_creativity_score"
        },
        {
            "paper_title": "LLM discussion: Enhancing the creativity of large language models via discussion framework and roleplay.",
            "rating": 2,
            "sanitized_title": "llm_discussion_enhancing_the_creativity_of_large_language_models_via_discussion_framework_and_roleplay"
        },
        {
            "paper_title": "The language of creativity: Evidence from humans and large language models.",
            "rating": 2,
            "sanitized_title": "the_language_of_creativity_evidence_from_humans_and_large_language_models"
        },
        {
            "paper_title": "Divergent creativity in humans and large language models.",
            "rating": 2,
            "sanitized_title": "divergent_creativity_in_humans_and_large_language_models"
        },
        {
            "paper_title": "Art or artifice? large language models and the false promise of creativity.",
            "rating": 2,
            "sanitized_title": "art_or_artifice_large_language_models_and_the_false_promise_of_creativity"
        },
        {
            "paper_title": "Assessing and understanding creativity in large language models",
            "rating": 2,
            "sanitized_title": "assessing_and_understanding_creativity_in_large_language_models"
        },
        {
            "paper_title": "Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models.",
            "rating": 2,
            "sanitized_title": "beyond_semantic_distance_automated_scoring_of_divergent_thinking_greatly_improves_with_large_language_models"
        },
        {
            "paper_title": "Automatic scoring of creative problem-solving with large language models: A comparison of originality and quality ratings.",
            "rating": 1,
            "sanitized_title": "automatic_scoring_of_creative_problemsolving_with_large_language_models_a_comparison_of_originality_and_quality_ratings"
        }
    ],
    "cost": 0.01598,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications
25 Sep 2024</p>
<p>Ethan Lin enlin@scu.edu 
Zhiyuan Peng zpeng@scu.edu 
Yi Fang yfang@scu.edu </p>
<p>Santa Clara University Santa Clara
CAUSA</p>
<p>Santa Clara University Santa Clara
CAUSA</p>
<p>Santa Clara University Santa Clara
CAUSA</p>
<p>Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications
25 Sep 20247C95BC6DB3A9745317E2C684A2581E7AarXiv:2409.16605v1[cs.CL]
Recent studies have evaluated the creativity/novelty of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science.However, accessing the novelty in scholarly publications is a largely unexplored area in evaluating LLMs.In this paper, we introduce a scholarly novelty benchmark (SchNovel 1 ) to evaluate LLMs' ability to assess novelty in scholarly papers.SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart.In each pair, the more recently published paper is assumed to be more novel.Additionally, we propose RAG-Novelty, which simulates the review process taken by human reviewers by leveraging the retrieval of similar papers to assess novelty.Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models.</p>
<p>Introduction</p>
<p>Creativity, traditionally defined as the capability of producing innovative ideas, forming unique connections, and developing solutions that are both novel and effective (Runco and Jaeger, 2012), was thought to be exclusive to humans.With the advent of large language models (LLMs) (OpenAI, 2023;Dubey et al., 2024b), however, this assumption has been challenged as LLMs have demonstrated remarkable proficiency in following human instructions to address a wide range of tasks, such as open-domain question answering, coding, solving math problems, and even utilizing external tools like search engines when necessary (Asai et al., 2024).In a recent study, Si et al. (2024) showed that LLMs can generate research ideas that are more novel than those produced by human experts.While their generative capabilities are impressive, they also raise concerns.For instance, some artists are uncomfortable with LLMs producing artwork that closely mimics established styles, prompting debates about whether LLMs can genuinely be considered creative.</p>
<p>Recent studies evaluating the generative creativity of LLMs have yielded inconsistent conclusions.Orwig et al. (2024) concluded that GPT-4 (Ope-nAI, 2023) generates stories that are comparable to those written by humans in terms of creativity.Similarly, Pépin et al. (2024) found that LLMs can even surpass humans in specific creative tasks, such as divergent association and creative writing.However, Anderson et al. (2024) argued that AI-based creativity support tools (CSTs) like ChatGPT are not yet well-suited to fostering truly original ideas, as they can lead to homogenization of human creativity.Chakrabarty et al. (2024) observed that LLM-generated stories pass the Torrance Test for Creative Writing (TTCW) tests 3 to 10 times less frequently than those written by professionals.Additionally, Chakrabarty et al. (2023) pointed out that LLMs often rely on clichés, produce text lacking nuance, and frequently resort to overly moralistic and predictable endings in stories.These discrepancies can be attributed to using different evaluation benchmarks and metrics, highlighting the lack of widely accepted standards for accessing LLM creativity.</p>
<p>The evaluation benchmarks used in current studies are primarily derived from cognitive science, such as the Torrance Tests of Creative Thinking (TTCT) (Lissitz and Willhoft, 1985), Alternative Use Task (AUT) (Guilford, 1964), and the Runco Creativity Assessment Battery (rCAB) (Runco, 2011).These benchmarks focus on assessing semantic creativity by tasks like generating responses to pictures or listing as many uses as possible for a common object.Corresponding metrics include flu-ency, flexibility, originality, and elaboration.However, these metrics primarily assess semantic novelty, which does not fully capture the the kind of novelty emphasized in scholarly research.Novelty in scholarly work is especially critical, as each paper undergoes rigorous peer review, particularly in high-prestige venues.Novel papers typically build upon existing research while introducing new ideas, methods, or insights, making novelty assessment heavily dependent on current and past trends in research.</p>
<p>While LLMs have shown great capability in generating text and mimicking human reasoning, their ability to assess novelty in scholarly publications remains largely unexamined.To address this gap, we present a scholarly novelty benchmark (SchNovel) to evaluate LLMs' capability of assessing novelty in scholarly papers.Specifically, we leverage the arXiv dataset to create a collection of 15,000 paper pairs.In each pair, we assume that the more recently published paper is more novel.Papers are selected across six categories, with publication dates spaced by gaps ranging from 2 to 10 years between the paired papers.We evaluate various LLMs on their ability to assess novelty and report their accuracy.</p>
<p>To further improve novelty assessment, we propose RAG-Novelty, a retrieval-augmented generation method.This method assumes that more novel papers will retrieve more recently published works, enhancing the novelty prediction.Our extensive experiments demonstrate that RAG-Novelty outperforms recent baseline models in assessing novelty in scholarly papers.Our key contributions include:</p>
<p>• We release the first benchmark SchNovel specifically designed to evaluate LLMs' capability in assessing novelty within scholarly publications.</p>
<p>• We conduct comprehensive experiments to explore how variations in categories, starting years, and year gaps affect LLMs' ability to assess paper novelty.</p>
<p>• We propose a novel method RAG-Novelty to enhance LLMs' performance in assessing paper novelty.</p>
<p>2 Related Work</p>
<p>Existing Benchmarks</p>
<p>TTCT (Lissitz and Willhoft, 1985) is a commercially protected assessment tool consisting of six tasks: 1) asking a question about a picture; 2) guessing the cause of the action depicted in the image; 3) predicting the consequences of the action described in the image; 4) improving a product described in 2-3 sentences in the most interesting and unusual way; 5) suggesting interesting and unconventional uses for a given item; and 6) imagining what would happen if an improbable situation were to occur.Both AUT (Guilford, 1964) and rCAB (Runco, 2011) ask participants to generate as many uses as possible for a common object.The Remote Associates Test (RAT) (Mednick and Halpern, 1968) presents participants with three seemingly unrelated words and asks them to find a fourth word that connects all three.The Consensual Assessment Technique (CAT) (Amabile, 1982) evaluates creative products, such as stories, poetry, dramatic performances, and musical compositions, using a panel of domain experts.The Wallach-Kogan Creativity Tests (WCT) (Brody, 1966) consist of the AUT, Instances Test, and Similarities Test.The scholarly Creativity Test (SCT) (Hu and Adey, 2002) measures scholarly creativity and process skills.The Divergent Association Task (DAT) (Olson et al., 2021) asks participants to name unrelated nouns and calculates the pairwise semantic distance between them.However, all these existing cognitive science benchmarks are not suited for evaluating LLMs' capability to assess novelty in scholarly publications, a gap our proposed benchmark addresses.</p>
<p>Creativity/Novelty Assessment</p>
<p>Traditional general novelty assessment methods use pre-defined metrics like the similarity to existing methods (Just et al., 2024) and the diversity of references (Shibayama et al., 2021) to score the novelty of a method or scholarly paper.To assess LLMs' capability of generating or assessing creativity and novelty, current studies employ different prompt strategies to interact with LLMs and collect responses for evaluation.Guzik et al. (2023) utilized a basic prompt to evaluate GPT-4 on the TTCT benchmark.Mehrotra et al. (2024) applied associative thinking (Mednick, 1962) in prompts designed for specific tasks like product design and marketing.Zhao et al. (2024) analyzed LLMs' responses to an expanded TTCT benchmark, applying diverse prompts, including basic prompts, instructive prompts, post-instructive prompts, and Chain of Thought (CoT) prompts.Stevenson et al. (2022) demonstrates that defining the role of LLMs as "scientist" can improve performance.Summers-Stay et al. (2023) improves the basic prompt method used in (Stevenson et al., 2022) by using multi-step reasoning to enhance GPT-3's performance on AUT.Similar to the multiround interaction framework utilized in LLM Debate (Du et al., 2024), LLM Discussion (Lu et al., 2024) develops a role-play enhanced LLM discussion framework to augment ChatGPT's performance on the WCT and SCT benchmarks.Unlike existing prompting methods, our proposed RAG-Novelty improves the LLM's performance by retrieving similar papers assuming that novel papers should retrieve the latest publications.</p>
<p>LLM Performance Evaluation</p>
<p>Most existing studies (Summers-Stay et al., 2023;Stevenson et al., 2022;Guzik et al., 2023;Mednick, 1962) evaluate LLM performance on benchmarks (Section 2.1) using human assessments.For example, Guzik et al. (2023) 2023) and (Organisciak et al., 2023) fine-tuned models on human-scored data to evaluate LLM responses.Since our benchmark provides ground-truth binary labels, evaluation is straightforward.</p>
<p>Scholarly Novelty Benchmark</p>
<p>Unlike the semantic novelty evaluated by the benchmarks from cognitive science (Section 2.1), novelty in scholarly publications refers to introducing new ideas, methods, or discoveries that have previously not been explored or established in the literature.</p>
<p>Evaluating novelty is fundamentally an exercise in understanding the relationship between ideas across time rather than simply assessing new ideas or techniques.This understanding is crucial in determining the contribution of a research paper.</p>
<p>The assumption can be made that later works are more novel than prior works, as they typically introduce new ideas and methodologies in the current research climate.In this paper, we apply this assumption to establish ground truth values for our created benchmark SchNovel.</p>
<p>Dataset Collection and Structure</p>
<p>The arXiv dataset2 comprises approximately 2.5 million articles, with the earliest dating back to 1986.All articles are categorized into eight distinct fields3 each of which has some sub-fields.We picked up six out of eight fields: Computer Science (cs), Mathematics (math), Physics (physics), Quantitative Biology (q-bio), Quantitative Finance (q-fin), and Statistics (stat), as we did not collect enough papers in other fields.Figure 6 in Appendix A.1 shows the number of papers published each year for each field.To assess the ability of LLMs to assess the novelty of research papers, we sampled a subset of articles from each field, denoted as dataset D = {(f, g, s, x, y, label) i } N i=1 where N = 15000, following the procedure outlined in Algorithm 1 in Appendix A.4, where f represent the field, x and y represent the paper ids, s represents the year in which paper x was published, g represents the number of years paper y was published before paper x and label equals to paper x as we assume in the same field, later published paper is more novel.</p>
<p>Tasks and Evaluation Metrics</p>
<p>We define the task as assessing which paper is more novel when given a pair of papers.Specifically, for each tuple (f, g, s, x, y, label) i , the selected LLM is provided with the title, abstract, and optional metadata for each paper-information typically available to a reviewer.However, unlike a full review, the model does not have access to the full text, making the task more challenging.While the abstract offers a strong indication of a paper's content and key findings, important details may be missed.By limiting the context to the abstract and metadata, we also improve efficiency in terms of token consumption and cost.We will discuss the potential limitations of this approach in Section 8. Various comparison methods, such as point-wise and pair-wise, can be employed, and we evaluate performance based on accuracy.</p>
<p>Assessing the novelty in scholarly papers requires the model to have a good understanding of past and present works to accurately judge whether a paper is novel in the current research climate.However, once trained, LLMs are frozen in time, meaning that they are no longer updated with the latest information, so they lack this understanding of the field's current state.Inspired by RAG, we propose a novel method, RAG-Novelty, to further improve LLMs' capability to assess novelty in our benchmark.As shown in Figure 1, apart from the information, like abstract, that can be utilized for a paper, we apply the paper abstract as a query to retrieve top-K papers from the already built index, and then we create a prompt based on the query paper and the retrieved papers to ask LLM score the novelty of query paper from 0 to 10.</p>
<p>Indexing and Retriever</p>
<p>To assess the novelty of a paper with the information provided by our SchNovel, such as title, abstract, and other metadata excluding the whole paper, an expert human reviewer in the same field may accurately score the novelty, a junior human reviewer, however, is likely not confident of scoring the novelty directly and instead, he/she will first review some similar papers and then assess the novelty.To mimic the review process taken by a human reviewer, we randomly sampled 500 papers from all years from 2000 to 2023, yielding 12000 papers for each field.Then, the abstracts of these papers are encoded into embeddings using OpenAI's text-embedding-3-small 4 model.The retrieve is the exact search method based on cosine similarity, as the number of candidates is very small.Our method can also handle huge candidate corpus by building an approximate nearest neighbor searching index using faiss (Douze et al., 2024;Johnson et al., 2019).</p>
<p>When a human reviewer conducts a literature search, it is naturally impossible to retrieve papers published after the query paper's publication date.To simulate this realistic constraint in our evaluation, we filtered out any papers published after the query paper and retrieved the top-k relevant papers from those published prior to or on the same date.However, in the context of pairwise comparisons, where we are assessing the novelty between two 4 https://platform.openai.com/docs/guides/embeddings papers with different publication dates, it is reasonable to retrieve papers up to the publication date of the more recent paper.This approach mirrors a realistic scenario in which novelty is judged relative to the latest available knowledge at the time of publication.By implementing this strategy, we ensure that the novelty assessment remains fair and contextually appropriate, avoiding any temporal bias while maintaining the integrity of the comparison.</p>
<p>Prompt</p>
<p>We first compared the zero-shot, two-shot, and selfreflection prompts and found that the self-reflection prompt performed the best (Section xx).So, for RAG-Novelty, we built the prompt, shown in Appendix A.6, based on the self-reflection prompt, shown in Appendix A.3, by incorporating the information of the retrieved papers.Specifically, we added a "Contextual Data Analysis" instruction that assumes that the more latest papers are retrieved, the more novel this query paper is:</p>
<p>Average the published dates of the retrieved documents.</p>
<p>Use this average date as additional context for your evaluation.</p>
<p>Consider that papers with an average date that is later or more recent in time are generally more novel.</p>
<p>Experimental Setup</p>
<p>Baseline Methods</p>
<p>Zero-Shot as shown in Appendix A.2, involves providing the model with two research papers' titles, abstracts, and four-step instructions, guiding the LLM to leverage its internal knowledge to make an informed decision.We also conducted the point-wise comparison by revising the zero-shot prompt to instruct the LLM score on the novelty of each paper first and then compare which one is more novel.Two-Shot We randomly sampled two example paper pairs and added them to the zero-shot prompt.Chain of Thought (CoT) (Wei et al., 2023) elicits reasoning within models by giving the model time to "think".We achieved CoT by adding instructions to Zero-Shot guiding LLMs to provide demonstrations.Self-Reflection (Renze and Guven, 2024) has shown several strides at improving LLMs logical fallacies by prompting the model to reflect on its incorrect solutions.We adopted this strategy to design a prompt which is shown in Appendix A.3.Self-Consistency (Wang et al., 2023) assumes that ground truth answers can be achieved through different reasoning paths.We followed the original paper to sample 10 generated sequences and voted majority.LLM Discussion (Lu et al., 2024) assigns LLMs with different roles and lets them discuss with each other before making the final decision.We adopted LLM Discussion to simulate the review process taken by human reviewers.Specifically, we assume the papers are submitted to a conference to be reviewed, and we designed four roles: (a) a professor; (b) a PhD student; (c) an editor of a prestigious journal; (d) the chair of the conference where the professor, PhD student, and editor are all reviewers and they have two round discussions and the chair make the final decision.The prompt is shown in Appendix A.5.</p>
<p>LLM Configuration</p>
<p>We adopted the default settings of API5 for Zero-Shot, Two-Shot, CoT and RAG-Novelty.We followed the Self-Consistency to adopt the temperature as 0.7 and set the number of reasoning paths as 10.For LLM discussion, we limit the max tokens to 200 to avoid overwhelming the model with long inputs in subsequent rounds of discussion.For Selfconsistency, we limit the max tokens so that the response is concise, as long reasoning for this task is unnecessary because we're looking for consistency rather than depth.In both cases, we prompt the model to limit its output to 150 tokens to ensure that it's response fits within the 200 token limit.</p>
<p>Research Questions</p>
<p>This study aims to address several key questions regarding the performance of LLMs on the SicNovel benchmark.</p>
<p>• R1: Which comparison approach yields better results: pointwise or pairwise?</p>
<p>• R2: How do different LLMs perform in assessing the novelty of research papers?</p>
<p>• R3 How does the category of the research paper affect the performance of LLMs?</p>
<p>• R4: How does the publication start year influence the performance of LLMs?</p>
<p>• R5: What impact does the gap between the publication years of research papers have on LLMs' performance?</p>
<p>• R6: What are the effects of other metadata attributes on LLMs' performance?</p>
<p>• R7: Can RAG-Novelty outperform recent baselines?</p>
<p>6 Experimental Results</p>
<p>RAG-Novelty vs. Baseline Models (R7)</p>
<p>In this experiment, we evaluate the performance of RAG-Novelty against baseline methods.All methods use GPT-4o-mini, and the accuracy is averaged across different start years s and year gaps g.Pairwise comparison is applied to all methods, and we account for position bias by swapping the order of the two papers in the comparisons.Two-Shot does not improve upon Zero-Shot as it typically does in other tasks.We attribute this to the complexity of the novelty assessment task, which requires deeper contextual understanding and comparison between papers-something that randomly selected examples may not effectively convey.Through iterative prompt refinement, Self-Reflection outperforms CoT in all fields except mathematics.LLM Discussion methods perform the worst, failing to even surpass Zero-Shot.Self-Consistency achieves the best results among baseline methods, demonstrating that obtaining answers through different reasoning paths helps improve performance.Our RAG-Novelty achieves the highest results overall, significantly outperforming the second-best method, except in the mathematics field.Across all methods, the improvement in mathematics is limited, possibly due to the slower progression of the field, the prevalence of symbols that LLMs struggle to interpret, or a lack of sufficient mathematical content in the training data compared to other fields.</p>
<p>Pointwise vs Pairwise (R1)</p>
<p>As mentioned in Section 5.1, we revised the pairwise Zero-Shot prompt (Appendix A.2) to a pointwise one.We compared the two methods by evaluating them in the cs field with the start year 2023, crossing different year gaps.As shown in Figure 2, pairwise is consistently much better than pointwise across different year gaps.This significant difference highlights the importance of context.As with human evaluations, providing relevant context or reference points is crucial for accurate assessments (Yan et al., 2022), allowing reviewers to consider the broad implications of a paper within the current research landscape.Pairwise comparisons align with this process, simplifying the task of considering the relative merits of two papers side-by-side rather than evaluating each one in isolation.Thus, pairwise comparisons are used in the rest of the following experiments.</p>
<p>The Impact of Different Fields (R3)</p>
<p>Different research domains require varying expertise to evaluate novelty.This section examines how different fields influence LLM performance in novelty evaluation.</p>
<p>In Figure 3, the cs category shows the highest accuracy across most year gaps (starting in 2023), likely due to the availability of data and well-defined evaluation metrics.In contrast, math and physics show lower accuracy, likely due to domain-specific challenges such as complex notation in mathematics and theoretical frameworks in physics.</p>
<p>One explanation is the lack of domain knowledge in ChatGPT's training data, which, being sourced from the internet, may not adequately cover specialized fields.Research has shown that LLMs exhibit biases in various prompts and tasks (Cheng et al., 2023;Stranisci et al., 2023), suggesting potential categorical biases in lesser-known or slower-growing domains.This has significant implications for using AI tools in academia and industry, particularly in automated scoring or ranking systems, where such biases could perpetuate inequalities.</p>
<p>The Impact of Different Start Years and Year Gaps (R4 &amp; R5)</p>
<p>To better understand how different start years affect the performance of LLMs in evaluating novelty, we investigated the model's results for five distinct start years.As shown in Figure 4, the model's results for all five start years were relatively consistent across different year gaps.This suggests that the model's ability to evaluate novelty between two papers is more dependent on the year gap between them than the specific publication years.For example, evaluating two papers with a 10year gap from 2009 to 2019 should be equivalent in difficulty to evaluating two papers with a 10-year gap from 2013 to 2023.Regardless of the boundary years within those ranges (i.e., considering papers published at specific points like 2009 and 2019, versus 2013 and 2023), it's the decade-long gap between the papers' publication times that makes it easier for the model to make such a binary evaluation.</p>
<p>The Impact of Different LLMs (R2)</p>
<p>All LLMs can vary significantly depending on their training data and model architecture.With var-  ious different models available, it is essential to understand how they perform when assessing the originality of ideas presented in research papers.</p>
<p>In this section, we examine the impact of using different LLMs on evaluating novelty.Our findings in Table 2 reveal significant disparities in the performance across different LLMs.GPT-4o-mini, GPT-3.5, and Gemma 2 performed more in line with expectations, achieving a more balanced distribution of predictions throughout all year gaps.Notably, GPT-4o-mini outperformed all other models, demonstrating a substantial advantage over the smaller models like LLaMA 3.1-8b, Mistral 7b, and Gemma 2-9b.</p>
<p>Despite such success, even ChatGPT 4o-mini and ChatGPT 3.5 exhibit position bias, where the order of papers in the prompt affects their decisionmaking instead of content alone.This bias is magnified in smaller models, which lack extensive training compared to larger models.For example, Mistral 7b is heavily biased towards the second paper, which comes last in the prompt.This aligns with known issues regarding LLMs' performance being best when relevant information appears towards the beginning or end of the prompt (Liu et al., 2024;Dai et al., 2024).</p>
<p>In contrast, LLaMA 3.1-8b exhibits a different bias -favoring the first paper that appears towards the middle of the prompt.According to Dubey et al. (2024a), the LLaMA 3.1 models excel at "needlein-the-haystack" tasks, where one needs to find specific information in large amounts of text (Kamradt, 2023), ultimately fixing the issues described in Liu et al. (2024).This is similar to skimming, which is efficient for finding specific information but may not facilitate deep understanding.Likewise, LLaMA 3.1-8b excels at retrieving specific information from anywhere in a context but this skillset is not ideal for evaluating novelty between two papers.</p>
<p>The Impact of Metadata (R6)</p>
<p>Previously, our experiments evaluated novelty based solely on a paper's title and abstract.However, human evaluations often take into account various metadata that can subtly influence reviewers' decisions.This metadata-induced bias has significant implications for research evaluations and highlights the need for more anonymous reviewal processes leading to solutions such as double-blind reviewal processes.In this section, we explore how metadata affects LLMs' ability to evaluate novelty.A pairwise comparison was applied for all the experiments in this section, and we accounted for position bias by swapping the order of the two papers in the comparisons.Table 3: The impact of metadata.The metrics above were obtained using Self-Reflection in the cs field with the start year s = 2023 and GPT-4o-mini."Asc Yr" indicates that the older paper is presented first in the prompt, while "Desc Yr" means the newer paper is presented first.</p>
<p>Adding a TLDR Summary</p>
<p>We utilized the SciTLDR model (Cachola et al., 2020) from the Semantic Scholar API (Kinney et al., 2023) to generate TLDRs for our dataset, expecting this additional information to enhance accuracy through the addition of context to help the model generalize and better understand the paper.As shown in Table 3, adding TLDRs decreases the accuracy across all year gaps.Nevertheless, incorporating such data did mitigate position bias, as evidenced by the almost negligible difference between ascending and descending year accuracies across nearly all year gaps.</p>
<p>Adding Author</p>
<p>We then added the author to the prompt, expecting that this additional information would not affect the model performance as the should not influence the novelty assessment.To our surprise, adding such information did help mitigate some of the position bias, as seen in the bold results in Table 3, but overall decreased the performance slightly.</p>
<p>Adding Affiliation</p>
<p>We selected two universities, one of which is a top research university and the other a teaching university, to study whether affiliation bias exists in LLMs' assessment of novelty. 6Specifically, we first assigned the top research university as the affiliation of the more recently published paper and the teaching university to the earlier published paper, with the results shown in blue.Then, we swapped the affiliations, and the results are shown in red.As illustrated in Figure 5, the top research university starts with similar accuracy to the teaching university at a year gap of g = 2, but as the year gap increases, the top research university consistently outperforms the teaching university.This suggests that affiliation bias exists in LLMs' novelty assess- 6 The real names of the universities are not used to ensure objectivity and to avoid any unintended bias or implications.ments, with a tendency to "trust" papers from top research universities.However, even though we observed LLMs' preference for choosing the top research university, the top research university experiments are outdone by W/O affiliation.This unexpected result raises questions about how LLMs process affiliation information, which warrants further investigation to better understand and mitigate such biases.</p>
<p>Conclusion and Future Work</p>
<p>To evaluate LLMs' ability to assess novelty in scholarly publications, we introduce SchNovel, a benchmark consisting of 15,000 pairs of papers across six fields.We conducted extensive experiments to understand how various factors influence LLM performance on SchNovel.To enhance LLMs' capability in assessing novelty, we propose RAG-Novelty, which significantly outperforms strong baseline models in comprehensive experiments.For future work, we plan to expand SchNovel by including more papers and covering additional fields to evaluate LLMs on a larger scale.Another promising direction is investigating which part of a paper best represents the whole for novelty assessment by LLMs.Additionally, studying how LLMs process affiliation and addressing biases in novelty evaluation, such as position and affiliation bias, is an important area for further research.</p>
<p>Our study evaluates an LLM's ability to assess novelty using a research paper's title, abstract, and metadata.While the abstract provides a strong indication of a paper's content and key findings, it may not fully capture the novelty of the research compared to the complete text.Abstracts often summarize the main ideas but may omit important technical details.Although this approach streamlines the evaluation process, it could occasionally limit the depth of the novelty assessment due to the absence of a more comprehensive context.</p>
<p>A Appendix</p>
<p>A.1 Statistics of arXiv A.2 Zero-shot</p>
<p>You will be provided with the title and abstract of two research papers.Please determine which of the two articles is more novel.Follow these steps for evaluation.</p>
<p>Step 1: Identify the problem and solution that the research paper attempts to solve.</p>
<p>Step 2: Determine how unique the solution is given the current research landscape in 2024.Does the paper introduce a new idea, theory, or concept that has not been previously discussed in the literature?</p>
<p>Step 3: Determine how creative the solution is given the current research landscape in 2024.Does it apply a known idea in a completely new context or in a way that has not been done before?</p>
<p>Step 4: Using the findings from Steps 1-3, determine which paper is more novel.</p>
<p>In your response, please only state which paper is more novel (e.g., 1 if Paper 1 is more novel; 2 if Paper 2 is more novel).</p>
<p>User Prompt: How well does the paper align with or push the boundaries of current trends?</p>
<p>Step 2: Quantitative Assessment</p>
<p>• Assign a score from 1-10 to each research paper for its novelty, with 10 being the most novel.This score should be based solely on the content of the title and abstract.• Provide a brief justification for the score, using specific quotes and context.</p>
<p>Step 3: Final Comparison</p>
<p>• After independently scoring each paper, compare the scores.• Determine which paper exhibits greater novelty based on the higher score, and provide the identifier (X or Y) of the more novel paper.</p>
<p>Important:</p>
<p>The order of presentation is random and should not influence your decision.Evaluate each paper strictly on its content and merit.</p>
<p>User Prompt:</p>
<p>A.5 LLM Discussion</p>
<p>You are a [Role] with expertise across all areas of [Category].You will be provided with the titles and abstracts of two research papers.Your task is to determine which of the two articles is more novel by evaluating their originality, contribution to the field, and potential impact.Focus on aspects such as new methodologies, unexplored problems, innovative solutions, and how the work advances the state of the art.Follow these steps for evaluation.</p>
<p>Step 1: Identify the problem and solution that the research paper attempts to solve.</p>
<p>Step 2: Determine how unique the solution is given the current research landscape in 2024.Does the paper introduce a new idea, theory, or concept that has not been previously discussed in the literature?</p>
<p>Step 3: Determine how creative the solution is given the current research landscape in 2024.Does it apply a known idea in a completely new context or in a way that has not been done before?</p>
<p>Step 4: Using the findings from Steps 1-3, determine which paper is more novel.A.6 RAG-Novelty</p>
<p>You are an advanced language model tasked with determining the novelty of research papers in 2024.Your goal is to evaluate and compare the novelty of two research papers based on their titles and abstracts.</p>
<p>The order in which the papers are presented is random and should not influence your evaluation.</p>
<p>Step How well does the paper align with or push the boundaries of current trends?</p>
<p>Step 2: Quantitative Assessment</p>
<p>• Assign a score from 1-10 to each research paper for its novelty, with 10 being the most novel.This score should be based on the content of the title and abstract, as well as the contextual information from the average published dates.• Provide a brief justification for the score, using specific quotes and context.</p>
<p>Step 3: Final Comparison</p>
<p>• After independently scoring each paper, compare the scores.• Determine which paper exhibits greater novelty based on the higher score, and conclude with: "The more novel and impactful paper is [Paper X or Paper Y].</p>
<p>Important: The order of presentation is random and should not influence your decision.Evaluate each paper strictly on its content and merit, incorporating the additional context from the vector database as described.</p>
<p>User Prompt:</p>
<p>•</p>
<p>Figure 1 :
1
Figure 1: The overview of RAG-Novelty</p>
<p>Figure 2 :
2
Figure 2: Pointwise vs. pairwise.The metrics above were obtained in the cs field with the start year s = 2023 and GPT-4o-mini.</p>
<p>Figure 4 :
4
Figure 4: Comparison of Start Years.The metrics above were obtained using Self-Reflection in the cs field with GPT-4o-mini.</p>
<p>Figure 5 :
5
Figure 5: Comparison of different organizations.The metrics above were obtained using Self-Reflection in the cs field with start year s = 2023 and GPT-4o-mini.</p>
<p>Figure 6 :
6
Figure 6: Number of Papers for Each Field (Up to 2023)</p>
<p>Please limit your response to 150 tokens max.In your response please conclude with: "The more novel and impactful paper is [Paper X or Paper Y] User Prompt: • Paper X Title: [paper_x_title] • Paper X Abstract: [paper_x_abstract] • Paper Y Title: [paper_y_title] • Paper Y Abstract: [paper_y_abstract] • (Round 2 Discussion add on) [previous_response] These are responses from other reviewers.Please revise your response if necessary... [other_responses] • (Round 3 Discussion add on) These are responses from other reviewers.Please determine which paper is more novel... [other_responses]</p>
<p>Title: [paper_x_title] • Paper X Abstract: [paper_x_abstract] • Paper Y Title: [paper_y_title] • Paper Y Abstract: [paper_y_abstract]</p>
<p>Table 1 :
1
⋆ 0.58 ⋆ †0.62 ⋆ †0.65 ⋆ †0.73 ⋆ †0.68 ⋆ RAG-Novelty vs. Baselines on SciEval with GPT-4o-mini.Averaged accuracy is reported.† denotes statistically significant enhancements over the secondbest result, with p-values &lt; 0.05, as determined by the McNemar test.The best results across different methods are denoted with symbol ⋆.The second-best results across different methods are underlined.
Methodcsmath physicsqbioqfinstatZero-Shot0.640.550.570.540.550.63Two-Shot0.620.550.570.540.550.60CoT0.630.560.570.540.560.62Self-Reflection0.650.560.580.560.570.63LLM Discussion0.600.550.560.530.500.58Self-Consistency0.660.570.590.580.600.64RAG-Novelty†0.72</p>
<p>Table 2 :
2
Asc Yr Desc Yr Acc.Asc Yr Desc Yr Acc.Asc Yr Desc Yr Acc.Asc Yr Desc Yr Acc.Asc Yr Desc Yr Acc.Comparision of different LLMs.The metrics above were obtained using Self-Reflection in the cs field with the start year s = 2023."Asc Yr" indicates that the older paper is presented first in the prompt, while "Desc Yr" means the newer paper is presented first.
Year GapChatGPT4o-miniChatGPT3.5LLaMA 3.1-8bMistral-7bGemma-2-9b20.440.660.550.460.620.540.030.980.511.000.000.500.660.380.5240.580.720.650.580.570.580.020.970.501.000.000.500.700.480.5960.630.750.690.670.600.640.010.990.501.000.010.510.690.410.5580.630.770.700.630.680.660.010.990.500.990.000.500.760.460.61100.790.780.790.670.710.690.050.970.510.990.010.500.800.430.62Average0.610.740.680.600.640.620.020.980.50 0.9960.0040.500.720.430.58csmathphysicsq-bioq-finstat0.80.7Accuracy0.60.50.4246810Year Gap
Figure Comparison of fields.The metrics above were obtained using Self-Reflection in cs field with the start year s = 2023 with GPT-4o-mini.</p>
<p>1: Independent Evaluation Analyze each research paper's title and abstract independently.Treat each paper as if it is the only one under review at that moment.Retrieve similar abstracts from a vector database based on the provided abstracts.Contextual Date Analysis: Average the published dates of the retrieved documents.Use this average date as additional context for your evaluation.Consider that papers with an average date that is later or more recent in time are generally more novel.Consider the following aspects for each paper: • Novelty of Methodology: Are the methods used new and innovative?• Surprisingness of Findings: Are the findings unexpected or counterintuitive?• Impact on Existing Knowledge: How does the research challenge or expand current scientific understanding?• Potential for Future Research: Does the paper open up new directions for research?• Relevance to 2024 Scientific Understanding:</p>
<p>Available at https://www.kaggle.com/datasets/Co rnell-University/arxiv
See the full taxonomy at https://arxiv.org/catego ry_taxonomy
https://platform.openai.com/docs/guides/chat -completions</p>
<p>Social psychology of creativity: A consensual assessment technique. Teresa M Amabile, Journal of personality and social psychology. 4359971982</p>
<p>Homogenization effects of large language models on human creative ideation. Barrett R Anderson, Jash Hemant Shah, Max Kreminski, 10.1145/3635636.3656204Proceedings of the 16th Conference on Creativity &amp; Cognition. the 16th Conference on Creativity &amp; CognitionChicago, IL, USAACM2024. June 23-26, 2024</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>Modes of thinking in young children: A study of the creativity-intelligence distinction. A Viola, Brody, Archives of General Psychiatry. 1431966</p>
<p>Tldr: Extreme summarization of scientific documents. Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S Weld, arXiv:2004.150112020Preprint</p>
<p>Art or artifice? large language models and the false promise of creativity. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu, 10.1145/3613904.3642731Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024. the CHI Conference on Human Factors in Computing Systems, CHI 2024Honolulu, HI, USAACM2024. May 11-16, 20243034</p>
<p>Creativity support in the age of large language models: An empirical study involving emerging writers. Tuhin Chakrabarty, Vishakh Padmakumar, 10.48550/ARXIV.2309.12570CoRR, abs/2309.125702023Faeze Brahman, and Smaranda Muresan</p>
<p>Marked personas: Using natural language prompts to measure stereotypes in language models. Myra Cheng, Esin Durmus, Dan Jurafsky, 10.18653/v1/2023.acl-long.84Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Bias and unfairness in information retrieval systems: New challenges in the llm era. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, 10.1145/3637528.3671458Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USAAssociation for Computing MachineryJun Xu. 2024</p>
<p>. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, 2024The faiss library</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024OpenReview.net</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, arXiv:2407.217832024aPreprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024barXiv preprint</p>
<p>Some new looks at the nature of creative processes. Contributions to mathematical psychology. Guilford Pv, 1964Holt, Rinehart &amp; WinstonNew York</p>
<p>The originality of machines: Ai takes the torrance test. Erik E Guzik, Christian Byrge, Christian Gilde, Journal of Creativity. 3331000652023</p>
<p>A scientific creativity test for secondary school students. Weiping Hu, Philip Adey, International Journal of Science Education. 2442002</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 732019</p>
<p>Ai-based novelty detection in crowdsourced idea spaces. Julian Just, Thomas Ströhle, Johann Füller, Katja Hutter, Innovation. 2632024</p>
<p>Gregory Kamradt, Llmtest_needleinahaystack. 2023</p>
<p>The semantic scholar open data platform. R M Kinney, C Anastasiades, R Authur, I Beltagy, ArXiv, abs/2301.101402023</p>
<p>A methodological study of the torrance tests of creativity. W Robert, Joseph L Lissitz, Willhoft, Journal of Educational measurement. 2211985</p>
<p>Lost in the Middle: How Language Models Use Long Contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>LLM discussion: Enhancing the creativity of large language models via discussion framework and roleplay. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-Yi Lee, Shao-Hua Sun, 10.48550/ARXIV.2405.06373CoRR, abs/2405.063732024</p>
<p>Automatic scoring of creative problem-solving with large language models: A comparison of originality and quality ratings. Simone Luchini, Nadine T Maliakkal, Paul V Distefano, John D Patterson, Roger Beaty, Roni Reiter-Palmon, 2023</p>
<p>T Martha, Sharon Mednick, Halpern, Remote associates test. 1968</p>
<p>The associative basis of the creative process. Sarnoff Mednick, Psychological review. 6932201962</p>
<p>Enhancing creativity in large language models through associative thinking strategies. Pronita Mehrotra, Sumit Parab, Gulwani, 10.48550/ARXIV.2405.06715CoRR, abs/2405.067152024</p>
<p>Naming unrelated words predicts creativity. Johnny Jay A Olson, Denis Nahas, Simon J Chmoulevitch, Margaret E Cropper, Webb, Proceedings of the National Academy of Sciences. 11825e20223401182021</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models. Peter Organisciak, Selcuk Acar, Denis Dumas, Kelly Berthiaume, Thinking Skills and Creativity. 491013562023</p>
<p>The language of creativity: Evidence from humans and large language models. William Orwig, Emma R Edenbaum, Joshua D Greene, Daniel L Schacter, The Journal of creative behavior. 5812024</p>
<p>Divergent creativity in humans and large language models. Antoine Bellemare Pépin, François Lespinasse, Philipp Thölke, Yann Harel, Kory Mathewson, Jay A Olson, Yoshua Bengio, Karim Jerbi, 10.48550/ARXIV.2405.13012CoRR, abs/2405.130122024</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, arXiv:2405.066822024Preprint</p>
<p>M A Runco, Runco creativity assessment battery (rcab). Creativity Testing Services. 2011</p>
<p>The standard definition of creativity. A Mark, Garrett J Runco, Jaeger, Creativity research journal. 2412012</p>
<p>Measuring novelty in science with word embedding. Sotaro Shibayama, Deyun Yin, Kuniko Matsumoto, PloS one. 167e02540342021</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Putting gpt-3's creativity to the (alternative uses) test. Claire Stevenson, Iris Smal, Matthijs Baas, P P P Raoul, Han L J Grasman, Van Der Maas, Proceedings of the 13th International Conference on Computational Creativity. the 13th International Conference on Computational CreativityBozen-Bolzano, ItalyACC2022. June 27 -July 1, 2022</p>
<p>WikiBio: a semantic resource for the intersectional analysis of biographical events. Marco Antonio Stranisci, Rossana Damiano, Enrico Mensa, Viviana Patti, Daniele Radicioni, Tommaso Caselli, 10.18653/v1/2023.acl-long.691Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Brainstorm, then select: a generative language model improves its creativity score. Douglas Summers-Stay, Clare R Voss, Stephanie M Lukin, The AAAI-23 Workshop on Creative AI Across Modalities. 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Human preferences as dueling bandits. Xinyi Yan, Chengxi Luo, L A Charles, Nick Clarke, Ellen M Craswell, Pablo Voorhees, Castells, 10.1145/3477495.3531991Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22New York, NY, USA2022Association for Computing Machinery</p>
<p>Assessing and understanding creativity in large language models. Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng, Yifan Hao, Yuanbo Wen, Xing Hu, Zidong Du, Qi Guo, Ling Li, Yunji Chen, 10.48550/ARXIV.2401.12491CoRR, abs/2401.124912024</p>            </div>
        </div>

    </div>
</body>
</html>
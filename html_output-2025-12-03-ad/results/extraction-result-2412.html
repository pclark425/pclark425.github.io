<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2412 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2412</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2412</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-233309401</p>
                <p><strong>Paper Title:</strong> Bias free multiobjective active learning for materials design and discovery</p>
                <p><strong>Paper Abstract:</strong> The design rules for materials are clear for applications with a single objective. For most applications, however, there are often multiple, sometimes competing objectives where there is no single best material and the design rules change to finding the set of Pareto optimal materials. In this work, we leverage an active learning algorithm that directly uses the Pareto dominance relation to compute the set of Pareto optimal materials with desirable accuracy. We apply our algorithm to de novo polymer design with a prohibitively large search space. Using molecular simulations, we compute key descriptors for dispersant applications and drastically reduce the number of materials that need to be evaluated to reconstruct the Pareto front with a desired confidence. This work showcases how simulation and machine learning techniques can be coupled to discover materials within a design space that would be intractable using conventional screening approaches.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2412.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2412.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyePAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyePAL (Python implementation of modified ε-PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python package implementing a modified ε-PAL active learning algorithm for multi-objective materials design that iteratively classifies and samples candidates to recover the Pareto front within a user-specified tolerance while supporting multiple surrogate models and practical constraints (missing data, batching).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PyePAL (modified ε-PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implements a modified ε-PAL algorithm that (1) uses the coefficient of variation as the uncertainty measure rather than raw predicted standard deviations, (2) does not assume known objective ranges (uses |µ_i|·ε instead of ε·r_i), (3) supports arbitrary numbers of objectives, (4) sets labeled-point uncertainty to experimental/model uncertainty, (5) supports coregionalized Gaussian processes and other uncertainty-aware surrogate models (quantile regression, neural tangent kernels), (6) handles missing data, supports single-point and batch sampling, and (7) includes options to exclude high-variance points from classification. Workflow: initialize a surrogate model from a diverse initial set; at each iteration classify points as (ε-)Pareto-dominated, Pareto-optimal, or unclassified by constructing hyperrectangles (pessimistic/optimistic estimates); discard classified-dominated points (exploitation) and select next samples by uncertainty sampling on the largest hyperrectangle (exploration); retrain the surrogate on all sampled points (including discarded ones) and repeat until target hypervolume error or budget is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials design/discovery (demonstrated on polymer dispersant design; general multi-objective design problems)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Two-stage: classification stage discards points provably dominated given model uncertainty (exploitation) thereby reducing the effective design space; sampling stage selects the most uncertain point(s) among the set of unclassified and predicted Pareto-optimal candidates using an uncertainty aggregation across objectives (default L2 of scaled uncertainties or coefficient of variation), optionally in batches. Initialization uses diversity-based selection (greedy farthest-point sampling or k-means). The hyperparameter ε controls tolerance: larger ε accelerates classification (fewer samples needed) but increases final error.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primarily measured in number of expensive evaluations (simulations/experiments) and number of active learning iterations (samples acquired); model retraining cost is considered qualitatively (number of retrainings per candidate would be major cost for some acquisition functions).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty reduction (reduction of hyperrectangle volumes / coefficient of variation across objectives) and downstream hypervolume error (difference between recovered Pareto hypervolume and reference), used to measure progress; hypervolume indicator used as performance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit two-part mechanism: classification/discarding of dominated points (treated as exploitation because it reduces search space toward promising regions) and uncertainty sampling of the most uncertain candidate(s) from the unclassified / Pareto set (exploration). No continuous acquisition function balancing both roles is used; instead roles are separated into classification and sampling steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity promoted at initialization using greedy farthest-point sampling and k-means clustering in feature space to build the initial training set; batch sampling option also available. Additionally, users can select aggregation functions that penalize outliers (L2) or be robust to them (median/mean), which affects sampling diversity across objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive experiments/simulations (implicitly via iteration count) and target hypervolume error (stopping criterion); computational budget considered qualitatively in model-training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduces effective design space by discarding ε-dominated candidates to focus remaining budget on uncertain/potentially Pareto-optimal points; recommends choosing ε larger than experimental/simulation noise to trade speed vs accuracy; avoids acquisition methods that would require retraining the model for every candidate to save computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Pareto-optimality within ε (classification as (ε-)Pareto optimal) and hypervolume indicator; no explicit novelty/breakthrough score is used beyond whether a candidate Pareto-dominates previously-found points (inverse design attempted with GA to find candidates beyond discovered Pareto set but none were found).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Hypervolume error (log-scale in figures) and number of iterations/evaluations to reach target hypervolume error; reported numbers include initialization with 60 points and reaching a target error in 11 active-learning iterations for one experiment setup (ε reported elsewhere as 0.1 or 0.05 in different runs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search (bootstrapped runs), and implicitly prior ε-PAL implementations; random search was used as a baseline for hypervolume error curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported: e-PAL (PyePAL) reaches the target error with >98% fewer iterations than random exploration in the reported experiment (11 iterations with PyePAL vs 509 iterations with random search) for a particular ε (authors note ε must be stated to make the claim; 0.1 is given in places).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Example claim: more than 98% reduction in iterations to reach target hypervolume error (11 vs 509 iterations) in the presented polymer design case; initialization reduced search from a much larger DoE to tens of evaluations in reported runs (60 initialization + ~11 iterations in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper analyzes tradeoffs between ε (tolerance), speed, and accuracy: larger ε speeds up classification and reduces iterations but increases hypervolume error; smaller ε yields stricter classification and can initially produce larger hypervolume error because fewer points qualify as ε-accurate Pareto-optimal early on. The authors also discuss the cost/benefit tradeoff of using advanced acquisition strategies (e.g., expected error reduction) that may reduce information-theoretic error but at prohibitive computational retraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendations: (1) Use diversity-based initialization (greedy farthest-point sampling or k-means) with enough points to make surrogate predictive (cross-validation/learning curves recommended). (2) Use ε set larger than experimental/simulation noise to speed discovery if tolerable. (3) Use classification-to-discard dominated candidates to reduce budget waste, then concentrate sampling on highest-uncertainty candidates among remaining points. (4) Prefer uncertainty sampling and aggregated uncertainty measures (L2, median, mean as configurable) because acquisition strategies that require retraining for each candidate (expected error reduction) are computationally expensive in practice. (5) Use coregionalized GPs when objectives correlate or when missing data occur.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2412.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ε-PAL (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ε-PAL (epsilon-Pareto Active Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning framework for multi-objective optimization that classifies design points under uncertainty into Pareto-optimal, dominated, or uncertain using ε-Pareto dominance, enabling discarding of dominated regions and focused sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Learning for Multi-Objective Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ε-PAL (epsilon-PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Original ε-PAL algorithm (Zuluaga et al.) iteratively constructs confidence hyperrectangles around predictions; points whose optimistic/pessimistic bounds show dominance relations are classified (dominated/discarded or Pareto) and the remaining uncertain points are actively sampled. The method was designed for multi-objective active learning and Pareto front recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General multi-objective optimization and active learning (original work referenced as background for materials design application).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Classify/discard dominated points based on optimistic/pessimistic prediction bounds and select uncertain points for sampling; uses ε to define tolerance in dominance decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly standardized in the present paper; cost typically measured in number of queries (labels) / iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty-aware classification (implicit information gain via reduction of unclassified region); not framed as explicit mutual-information maximization in the cited work as presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit: classification/discarding acts as exploitation, sampling of uncertain (unclassified) points acts as exploration. The approach separates exploitation (pruning) and exploration (sampling) steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not specified in detail here; the algorithm focuses on uncertainty-based sampling rather than explicit diversity promotion in the original description as discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed queries/labels (typical active learning budget), stopping based on classification completeness or tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduces the effective set of candidates by discarding dominated points to focus remaining budget on uncertain/potentially Pareto-optimal points.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Pareto-optimal classification and recovery of Pareto front under ε tolerance; hypervolume commonly used downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Typically number of samples to recover Pareto front within ε; specific numbers depend on problem and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to random search and other active learning/bayesian optimization approaches; in this paper PyePAL is presented as a modified implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>ε controls tradeoff between speed (faster pruning) and accuracy (larger ε yields more classification earlier but higher final error).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using ε-based dominance classification to prune search space is an effective strategy to focus experimental budget on uncertain candidates; modifications (e.g., uncertainty measure choice, handling unknown objective ranges) can improve practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2412.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expected Error Reduction (EER)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Error Reduction via sampling-estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Active learning acquisition family that selects samples by estimating the expected decrease in model error from observing each candidate, often requiring retraining or expensive simulations per candidate to estimate utility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toward Optimal Active Learning through Sampling Estimation of Error Reduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Error Reduction (sampling-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Class of acquisition strategies that select the next query by estimating, for each candidate, the expected reduction in a global error metric after observing its label; implementation typically approximated by sampling possible labels and retraining or updating the model per candidate to evaluate expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General active learning across domains where measuring expected reduction in model/generalization error guides sample selection.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Compute expected global error after hypothetically labeling each candidate (averaged over possible labels) and then select the candidate with maximal expected error reduction; requires retraining or model-update computations per candidate for the estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>High model retraining count and corresponding wall-clock/computational cost per proposed candidate (number of retrainings × complexity of model), making it computationally expensive in large candidate sets.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected reduction in global predictive error (often approximated by sampling label outcomes and retraining); can be seen as direct information-utility maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Directly balances exploration and exploitation by selecting samples that most reduce expected error; does not decouple classification/exploration steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not inherent; diversity may be indirectly encouraged if diverse points reduce error more, but explicit diversity mechanisms are not required by the criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget-heavy due to retraining cost; practical budgets often prevent exhaustive evaluation over large candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Often approximated (subset of candidates, Monte Carlo sampling) to limit retraining cost; in this paper authors avoid EER because of prohibitive computational cost per iteration when applied to large DoE.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reductions in predictive error can aid discovery of high-performing/novel candidates, but no explicit breakthrough metric is inherent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Expected error reduction amount (theoretical) and downstream sample efficiency; not used in experiments in this paper due to computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to simpler uncertainty sampling and PAL/ε-PAL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not empirically evaluated in this paper; authors cite that EER might mitigate outlier sampling tendencies of uncertainty sampling but is too expensive to run across all candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Tradeoff is explicit: potentially better sample choices (information gain) vs high computational cost (retraining). The authors favor cheaper uncertainty sampling to reduce wall-clock/model compute.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>While EER can offer theoretically optimal information gain, its required per-candidate retraining cost makes it impractical in large design-of-experiment (DoE) spaces for materials discovery without heavy approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2412.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty sampling (variance-based / coefficient of variation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple active learning strategy that selects the candidate(s) with the largest predictive uncertainty (variance or coefficient of variation) for labeling; used here to select points from the unclassified and predicted Pareto sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Uncertainty sampling (scaled / aggregated across objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Selects points with maximum aggregated uncertainty across objectives; aggregation options include L2 norm of scaled uncertainties, mean, or median. PyePAL defaults to coefficient of variation for per-objective uncertainty and allows user-configurable aggregation; sampling is restricted to unclassified or Pareto candidates to focus resources.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for multi-objective materials design; general multi-output active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments to candidates with highest aggregated predictive uncertainty among the remaining (unclassified/Pareto) candidates; avoids retraining-each-candidate strategies to save computational resources.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured by number of retrainings (one per acquired sample or per batch) and number of evaluations; cheaper than EER because it avoids per-candidate retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Proxy: reduction in predictive uncertainty and shrinkage of hyperrectangles; no explicit mutual-information calculation but intended to maximize expected uncertainty reduction per sample cheaply.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Promotes exploration by targeting uncertain regions; used in conjunction with classification-based discarding (exploitation) so the mechanism overall balances both.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity is not intrinsic but can be encouraged by initialization and by aggregation choices; sampling from the largest hyperrectangle helps cover uncertain pockets across the Pareto boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment/evaluation budget; algorithm reduces number of necessary evaluations by pruning dominated points and focusing on uncertain candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Computationally efficient sampling per iteration (single retrain) to respect limited computational budgets; aggregation and exclusion options reduce wasted samples on outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect: reducing uncertainty on Pareto boundary increases chance to discover truly Pareto-dominant breakthroughs; measured via hypervolume improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reductions in hypervolume error per iteration; compared favorably against random search in experiments (e.g., reaching target error in far fewer iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search and potentially EER (conceptually); shown superior to random search in case study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In the polymer case study, uncertainty sampling within ε-PAL framework reached target hypervolume error in 11 iterations vs 509 for random search in a reported experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Large reduction in required evaluations compared to random search in the demonstrated example (>98% fewer iterations reported).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Uncertainty sampling is computationally cheap but can sample outliers; aggregation choices (L2 vs median) and option to exclude high-variance points mitigate that risk; authors note EER could be less prone to outliers but is computationally prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use uncertainty sampling constrained to the unclassified/Pareto set plus pruning (classification) to get a pragmatic balance of information gain and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2412.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coregionalized GP (ICM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coregionalized Gaussian Process (Intrinsic Coregionalization Model, ICM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-output Gaussian process approach that models correlations between multiple objectives by assuming outputs are linear combinations of shared latent functions, enabling joint prediction and uncertainty estimation across correlated objectives and handling missing outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ICM coregionalized Gaussian processes</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ICM models assume outputs are scaled samples from shared latent GPs (rank-1) or weighted sums of multiple latent functions (rank-n). Higher rank increases hyperparameter count and optimization difficulty. Used here to model correlated objectives (e.g., Rg, ΔG_rep, ΔG_ads), improve predictions when data are missing for some objectives, and produce joint uncertainties used by PyePAL's classification and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multi-objective surrogate modeling in materials design with correlated targets and missing data.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>By modeling correlations and imputing missing values, ICM reduces the need for extra dedicated expensive measurements for every objective, thus informing allocation across objectives more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Model training and hyperparameter optimization cost (increases with rank and number of outputs); computational burden measured in wall-clock and complexity of GP training (O(n^3) scaling for naive implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Joint predictive uncertainty across outputs (covariance-informed), which feeds into ε-PAL classification and uncertainty sampling for sample selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>By reducing uncertainty jointly across objectives, coregionalized modeling supports more informed exploitation (pruning) and targeted exploration (sampling uncertain multi-objective regions).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not directly a diversity mechanism, but by improving imputations for missing objectives it affects which candidates are considered uncertain and hence sampled, indirectly influencing diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Handles missing-data budgets where some objectives are more expensive by sharing information across outputs to reduce required measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses correlations to impute or better predict expensive-to-measure outputs, reducing experimental burden; authors compared rank-1 and rank-2 models and provide recommended practice.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved joint uncertainty reductions and better identification of multi-objective Pareto candidates, measured via hypervolume and classification outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Supplementary experiments compare ranks and show improved performance with coregionalized models when data are missing; exact numbers are in supplementary notes (referenced but not fully enumerated in review text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Independent (separate) GP models per objective vs coregionalized GP; coregionalized preferred when outputs correlate or data are missing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors report coregionalized GPs improve predictions and classification in presence of missing data and correlated outputs; details in Supplementary Note 7.2/7.3.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>By leveraging correlations, fewer expensive measurements are needed to classify materials and recover Pareto front within ε compared to modeling objectives entirely independently (quantitative gains provided in SI experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Higher ICM rank can model more complex correlations but increases hyperparameter complexity and optimization cost; tradeoff between modeling fidelity and computational/hr optimization difficulty discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use coregionalized GP (rank tuned) when objectives are correlated or some objectives have missing/expensive measurements to make better allocation decisions and impute missing outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2412.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse initialisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse initialization (greedy farthest-point sampling, k-means clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Initialization strategies to select a diverse and informative initial training set for surrogate models, improving early predictive performance and sample-efficiency of active learning loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Greedy farthest-point sampling and k-means clustering for initial design</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Greedy farthest-point sampling selects initial points by iteratively picking the point farthest in feature space from existing selected points; k-means clustering selects representatives from clusters to ensure coverage. Authors implemented both in PyePAL and recommend them based on prior results showing diverse selection yields better surrogate predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Initial design point selection for active learning in materials discovery and general surrogate-model-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate initial experimental budget to a diverse set of candidates to ensure surrogate model predictive coverage across the design space before active iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost measured by number of initial evaluations (e.g., 60 used in experiments) and computational cost of clustering/farthest-point selection (negligible compared to expensive experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Diversity heuristics approximate initial information gain by maximizing coverage in feature space, improving downstream uncertainty estimates and sample selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration for initialization to ensure later exploitation/targeting is credible.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit diversity mechanisms: greedy farthest-point and k-means both maximize feature-space coverage to promote diverse hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed initial evaluation budget (user-chosen), advised to select enough points for surrogate predictivity (assessed by cross-validation/learning curves).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Utilities provided to help choose initialization size via cross-validation and learning-curve analysis; initial diverse sample size should be large enough to reach surrogate model predictive threshold before active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved early surrogate accuracy and faster convergence to Pareto front (measured via hypervolume error trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors used 60 initialization points in reported experiments and provide SI analysis of initial number influence; utilities added to PyePAL for such analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random initialization vs diversity-based initialization; prior work shows diversity-based selection improves downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not numerically detailed in main text here, but authors state they compared farthest-point sampling and k-means and provide utilities and SI experiments illustrating benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Improved surrogate performance enabling fewer active iterations overall; exact gains depend on problem and are discussed in SI.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Larger initial budgets increase upfront cost but reduce iterations later; recommended to use cross-validation and learning curves to pick initial size.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Start with a diverse initial set chosen by greedy farthest-point sampling or k-means; set initial size by predictive performance assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2412.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization (general acquisition-function-based optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that optimize expensive black-box functions by constructing a surrogate and maximizing an acquisition function that trades off exploration and exploitation (e.g., expected improvement, UCB), usually assuming the next evaluation may be the final one.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization (acquisition-function framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a probabilistic surrogate over objectives and define an acquisition function that balances exploitation (favor regions with high predicted objective) and exploration (favor regions of high uncertainty). The acquisition function is optimized to propose next experiments; commonly used acquisition functions include expected improvement, upper confidence bound, and knowledge-gradient variants.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization of expensive black-box functions (single- and multi-objective), including materials and chemical design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Single-step acquisition optimization selects next candidate(s) by maximizing acquisition function which encodes the tradeoff between immediate improvement and information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost per acquisition depends on acquisition optimization complexity and surrogate-update cost; evaluated in number of evaluations and wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions encode expected improvement, information gain, or upper confidence bounds; specific metric depends on chosen acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balanced directly by acquisition function; often includes hyperparameters (e.g., exploration weight) to tune behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not necessarily intrinsic; batch-BO variants include diversity or penalization terms to promote diverse batch choices.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of function evaluations, wall-clock time, or monetary budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimize acquisition function per iteration to maximize utility under remaining budget; multi-step planning variants exist but are computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement in objective(s), often measured via best-found value or Pareto front hypervolume in multi-objective BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of function evaluations to reach given optimum/hypervolume; not empirically benchmarked in this paper but discussed conceptually as differing from active learning's goals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to ε-PAL/PyePAL: BO focuses on optimization (often assuming next sample may be final), whereas PAL focuses on classification under uncertainty to recover Pareto set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors note difference in goals and framing: BO integrates exploitation/exploration in acquisition function whereas PAL separates classification and sampling; choice depends on whether goal is final-sample optimization vs iterative identification of Pareto set.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For materials discovery where the goal is to enumerate Pareto-optimal regions under uncertainty and not assume a single final sample, PAL-style classification with uncertainty sampling can be more appropriate and pragmatic than typical BO formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2412.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2412.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation functions (multi-objective uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty aggregation functions (L2, mean, median; coefficient of variation per objective)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to combine per-objective uncertainties into a scalar score used to rank candidates for sampling in multi-objective active learning; choices affect sensitivity to outliers and sampling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Uncertainty aggregation (L2 / mean / median; coefficient of variation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Per-objective uncertainties (e.g., predicted stddev or coefficient of variation) are combined into a single scalar using an aggregation function. Authors implemented L2 norm (squares penalize outliers), mean (average sensitivity), and median (robust to outliers) and allow users to select these in PyePAL. The coefficient of variation is used as the per-objective uncertainty measure by default.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multi-objective active learning for materials discovery and other domains requiring joint uncertainty ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Rank and select candidates by aggregated uncertainty among unclassified/Pareto candidates; aggregation choice steers allocation toward outlier-prone sampling (L2) or robust sampling (median).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Negligible compared to experiments; cost relates to computing scalar aggregates across objectives for candidate ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Proxy: aggregated predictive uncertainty as a heuristic for expected information gain across multiple objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Aggregation choice indirectly controls exploration of multi-objective uncertainty space; no explicit exploration weight besides aggregation choice and ε-driven classification.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Aggregation affects whether sampling will concentrate on extreme per-objective uncertainties or more balanced multi-objective uncertainty, thereby affecting diversity of sampled objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Operates under standard active-learning budget constraints; aggregation choice is a low-cost lever to shape sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Aggregation selection is a computationally cheap way to influence sampling to be robust or aggressive without expensive acquisition optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Influences the kinds of Pareto-front regions explored and hence the probability of finding high-impact candidates, measured via hypervolume and Pareto coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Supplementary Figure 26 presents case studies comparing aggregation choices; detailed numeric outcomes are in SI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Different aggregation rules compared against each other and against other acquisition heuristics conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Aggregation selection trades sensitivity to outliers vs robustness: L2 can prioritize outlier objectives (potentially finding breakthroughs but also sampling noise), median reduces outlier influence and may better explore stable regions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Provide configurable aggregation; choice should be problem-specific (use L2 when outliers are important, median/mean when robustness is desired).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bias free multiobjective active learning for materials design and discovery', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active Learning for Multi-Objective Optimization <em>(Rating: 2)</em></li>
                <li>Toward Optimal Active Learning through Sampling Estimation of Error Reduction <em>(Rating: 2)</em></li>
                <li>Active Learning with Statistical Models <em>(Rating: 2)</em></li>
                <li>epsilon-PAL <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2412",
    "paper_id": "paper-233309401",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "PyePAL",
            "name_full": "PyePAL (Python implementation of modified ε-PAL)",
            "brief_description": "A Python package implementing a modified ε-PAL active learning algorithm for multi-objective materials design that iteratively classifies and samples candidates to recover the Pareto front within a user-specified tolerance while supporting multiple surrogate models and practical constraints (missing data, batching).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PyePAL (modified ε-PAL)",
            "system_description": "Implements a modified ε-PAL algorithm that (1) uses the coefficient of variation as the uncertainty measure rather than raw predicted standard deviations, (2) does not assume known objective ranges (uses |µ_i|·ε instead of ε·r_i), (3) supports arbitrary numbers of objectives, (4) sets labeled-point uncertainty to experimental/model uncertainty, (5) supports coregionalized Gaussian processes and other uncertainty-aware surrogate models (quantile regression, neural tangent kernels), (6) handles missing data, supports single-point and batch sampling, and (7) includes options to exclude high-variance points from classification. Workflow: initialize a surrogate model from a diverse initial set; at each iteration classify points as (ε-)Pareto-dominated, Pareto-optimal, or unclassified by constructing hyperrectangles (pessimistic/optimistic estimates); discard classified-dominated points (exploitation) and select next samples by uncertainty sampling on the largest hyperrectangle (exploration); retrain the surrogate on all sampled points (including discarded ones) and repeat until target hypervolume error or budget is reached.",
            "application_domain": "Materials design/discovery (demonstrated on polymer dispersant design; general multi-objective design problems)",
            "resource_allocation_strategy": "Two-stage: classification stage discards points provably dominated given model uncertainty (exploitation) thereby reducing the effective design space; sampling stage selects the most uncertain point(s) among the set of unclassified and predicted Pareto-optimal candidates using an uncertainty aggregation across objectives (default L2 of scaled uncertainties or coefficient of variation), optionally in batches. Initialization uses diversity-based selection (greedy farthest-point sampling or k-means). The hyperparameter ε controls tolerance: larger ε accelerates classification (fewer samples needed) but increases final error.",
            "computational_cost_metric": "Primarily measured in number of expensive evaluations (simulations/experiments) and number of active learning iterations (samples acquired); model retraining cost is considered qualitatively (number of retrainings per candidate would be major cost for some acquisition functions).",
            "information_gain_metric": "Uncertainty reduction (reduction of hyperrectangle volumes / coefficient of variation across objectives) and downstream hypervolume error (difference between recovered Pareto hypervolume and reference), used to measure progress; hypervolume indicator used as performance metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implicit two-part mechanism: classification/discarding of dominated points (treated as exploitation because it reduces search space toward promising regions) and uncertainty sampling of the most uncertain candidate(s) from the unclassified / Pareto set (exploration). No continuous acquisition function balancing both roles is used; instead roles are separated into classification and sampling steps.",
            "diversity_mechanism": "Diversity promoted at initialization using greedy farthest-point sampling and k-means clustering in feature space to build the initial training set; batch sampling option also available. Additionally, users can select aggregation functions that penalize outliers (L2) or be robust to them (median/mean), which affects sampling diversity across objectives.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive experiments/simulations (implicitly via iteration count) and target hypervolume error (stopping criterion); computational budget considered qualitatively in model-training cost.",
            "budget_constraint_handling": "Reduces effective design space by discarding ε-dominated candidates to focus remaining budget on uncertain/potentially Pareto-optimal points; recommends choosing ε larger than experimental/simulation noise to trade speed vs accuracy; avoids acquisition methods that would require retraining the model for every candidate to save computational budget.",
            "breakthrough_discovery_metric": "Pareto-optimality within ε (classification as (ε-)Pareto optimal) and hypervolume indicator; no explicit novelty/breakthrough score is used beyond whether a candidate Pareto-dominates previously-found points (inverse design attempted with GA to find candidates beyond discovered Pareto set but none were found).",
            "performance_metrics": "Hypervolume error (log-scale in figures) and number of iterations/evaluations to reach target hypervolume error; reported numbers include initialization with 60 points and reaching a target error in 11 active-learning iterations for one experiment setup (ε reported elsewhere as 0.1 or 0.05 in different runs).",
            "comparison_baseline": "Random search (bootstrapped runs), and implicitly prior ε-PAL implementations; random search was used as a baseline for hypervolume error curves.",
            "performance_vs_baseline": "Reported: e-PAL (PyePAL) reaches the target error with &gt;98% fewer iterations than random exploration in the reported experiment (11 iterations with PyePAL vs 509 iterations with random search) for a particular ε (authors note ε must be stated to make the claim; 0.1 is given in places).",
            "efficiency_gain": "Example claim: more than 98% reduction in iterations to reach target hypervolume error (11 vs 509 iterations) in the presented polymer design case; initialization reduced search from a much larger DoE to tens of evaluations in reported runs (60 initialization + ~11 iterations in one experiment).",
            "tradeoff_analysis": "Paper analyzes tradeoffs between ε (tolerance), speed, and accuracy: larger ε speeds up classification and reduces iterations but increases hypervolume error; smaller ε yields stricter classification and can initially produce larger hypervolume error because fewer points qualify as ε-accurate Pareto-optimal early on. The authors also discuss the cost/benefit tradeoff of using advanced acquisition strategies (e.g., expected error reduction) that may reduce information-theoretic error but at prohibitive computational retraining cost.",
            "optimal_allocation_findings": "Recommendations: (1) Use diversity-based initialization (greedy farthest-point sampling or k-means) with enough points to make surrogate predictive (cross-validation/learning curves recommended). (2) Use ε set larger than experimental/simulation noise to speed discovery if tolerable. (3) Use classification-to-discard dominated candidates to reduce budget waste, then concentrate sampling on highest-uncertainty candidates among remaining points. (4) Prefer uncertainty sampling and aggregated uncertainty measures (L2, median, mean as configurable) because acquisition strategies that require retraining for each candidate (expected error reduction) are computationally expensive in practice. (5) Use coregionalized GPs when objectives correlate or when missing data occur.",
            "uuid": "e2412.0",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ε-PAL (original)",
            "name_full": "ε-PAL (epsilon-Pareto Active Learning)",
            "brief_description": "An active learning framework for multi-objective optimization that classifies design points under uncertainty into Pareto-optimal, dominated, or uncertain using ε-Pareto dominance, enabling discarding of dominated regions and focused sampling.",
            "citation_title": "Active Learning for Multi-Objective Optimization",
            "mention_or_use": "mention",
            "system_name": "ε-PAL (epsilon-PAL)",
            "system_description": "Original ε-PAL algorithm (Zuluaga et al.) iteratively constructs confidence hyperrectangles around predictions; points whose optimistic/pessimistic bounds show dominance relations are classified (dominated/discarded or Pareto) and the remaining uncertain points are actively sampled. The method was designed for multi-objective active learning and Pareto front recovery.",
            "application_domain": "General multi-objective optimization and active learning (original work referenced as background for materials design application).",
            "resource_allocation_strategy": "Classify/discard dominated points based on optimistic/pessimistic prediction bounds and select uncertain points for sampling; uses ε to define tolerance in dominance decisions.",
            "computational_cost_metric": "Not explicitly standardized in the present paper; cost typically measured in number of queries (labels) / iterations.",
            "information_gain_metric": "Uncertainty-aware classification (implicit information gain via reduction of unclassified region); not framed as explicit mutual-information maximization in the cited work as presented here.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Implicit: classification/discarding acts as exploitation, sampling of uncertain (unclassified) points acts as exploration. The approach separates exploitation (pruning) and exploration (sampling) steps.",
            "diversity_mechanism": "Not specified in detail here; the algorithm focuses on uncertainty-based sampling rather than explicit diversity promotion in the original description as discussed in this paper.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed queries/labels (typical active learning budget), stopping based on classification completeness or tolerance.",
            "budget_constraint_handling": "Reduces the effective set of candidates by discarding dominated points to focus remaining budget on uncertain/potentially Pareto-optimal points.",
            "breakthrough_discovery_metric": "Pareto-optimal classification and recovery of Pareto front under ε tolerance; hypervolume commonly used downstream.",
            "performance_metrics": "Typically number of samples to recover Pareto front within ε; specific numbers depend on problem and implementation.",
            "comparison_baseline": "Compared in literature to random search and other active learning/bayesian optimization approaches; in this paper PyePAL is presented as a modified implementation.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "ε controls tradeoff between speed (faster pruning) and accuracy (larger ε yields more classification earlier but higher final error).",
            "optimal_allocation_findings": "Using ε-based dominance classification to prune search space is an effective strategy to focus experimental budget on uncertain candidates; modifications (e.g., uncertainty measure choice, handling unknown objective ranges) can improve practicality.",
            "uuid": "e2412.1",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Expected Error Reduction (EER)",
            "name_full": "Expected Error Reduction via sampling-estimation",
            "brief_description": "Active learning acquisition family that selects samples by estimating the expected decrease in model error from observing each candidate, often requiring retraining or expensive simulations per candidate to estimate utility.",
            "citation_title": "Toward Optimal Active Learning through Sampling Estimation of Error Reduction",
            "mention_or_use": "mention",
            "system_name": "Expected Error Reduction (sampling-based)",
            "system_description": "Class of acquisition strategies that select the next query by estimating, for each candidate, the expected reduction in a global error metric after observing its label; implementation typically approximated by sampling possible labels and retraining or updating the model per candidate to evaluate expected utility.",
            "application_domain": "General active learning across domains where measuring expected reduction in model/generalization error guides sample selection.",
            "resource_allocation_strategy": "Compute expected global error after hypothetically labeling each candidate (averaged over possible labels) and then select the candidate with maximal expected error reduction; requires retraining or model-update computations per candidate for the estimate.",
            "computational_cost_metric": "High model retraining count and corresponding wall-clock/computational cost per proposed candidate (number of retrainings × complexity of model), making it computationally expensive in large candidate sets.",
            "information_gain_metric": "Expected reduction in global predictive error (often approximated by sampling label outcomes and retraining); can be seen as direct information-utility maximization.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Directly balances exploration and exploitation by selecting samples that most reduce expected error; does not decouple classification/exploration steps.",
            "diversity_mechanism": "Not inherent; diversity may be indirectly encouraged if diverse points reduce error more, but explicit diversity mechanisms are not required by the criterion.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Computational budget-heavy due to retraining cost; practical budgets often prevent exhaustive evaluation over large candidate pools.",
            "budget_constraint_handling": "Often approximated (subset of candidates, Monte Carlo sampling) to limit retraining cost; in this paper authors avoid EER because of prohibitive computational cost per iteration when applied to large DoE.",
            "breakthrough_discovery_metric": "Reductions in predictive error can aid discovery of high-performing/novel candidates, but no explicit breakthrough metric is inherent.",
            "performance_metrics": "Expected error reduction amount (theoretical) and downstream sample efficiency; not used in experiments in this paper due to computational cost.",
            "comparison_baseline": "Compared conceptually to simpler uncertainty sampling and PAL/ε-PAL approaches.",
            "performance_vs_baseline": "Not empirically evaluated in this paper; authors cite that EER might mitigate outlier sampling tendencies of uncertainty sampling but is too expensive to run across all candidates.",
            "efficiency_gain": "",
            "tradeoff_analysis": "Tradeoff is explicit: potentially better sample choices (information gain) vs high computational cost (retraining). The authors favor cheaper uncertainty sampling to reduce wall-clock/model compute.",
            "optimal_allocation_findings": "While EER can offer theoretically optimal information gain, its required per-candidate retraining cost makes it impractical in large design-of-experiment (DoE) spaces for materials discovery without heavy approximations.",
            "uuid": "e2412.2",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Uncertainty sampling",
            "name_full": "Uncertainty sampling (variance-based / coefficient of variation)",
            "brief_description": "A simple active learning strategy that selects the candidate(s) with the largest predictive uncertainty (variance or coefficient of variation) for labeling; used here to select points from the unclassified and predicted Pareto sets.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Uncertainty sampling (scaled / aggregated across objectives)",
            "system_description": "Selects points with maximum aggregated uncertainty across objectives; aggregation options include L2 norm of scaled uncertainties, mean, or median. PyePAL defaults to coefficient of variation for per-objective uncertainty and allows user-configurable aggregation; sampling is restricted to unclassified or Pareto candidates to focus resources.",
            "application_domain": "Active learning for multi-objective materials design; general multi-output active learning.",
            "resource_allocation_strategy": "Allocate experiments to candidates with highest aggregated predictive uncertainty among the remaining (unclassified/Pareto) candidates; avoids retraining-each-candidate strategies to save computational resources.",
            "computational_cost_metric": "Measured by number of retrainings (one per acquired sample or per batch) and number of evaluations; cheaper than EER because it avoids per-candidate retraining.",
            "information_gain_metric": "Proxy: reduction in predictive uncertainty and shrinkage of hyperrectangles; no explicit mutual-information calculation but intended to maximize expected uncertainty reduction per sample cheaply.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Promotes exploration by targeting uncertain regions; used in conjunction with classification-based discarding (exploitation) so the mechanism overall balances both.",
            "diversity_mechanism": "Diversity is not intrinsic but can be encouraged by initialization and by aggregation choices; sampling from the largest hyperrectangle helps cover uncertain pockets across the Pareto boundary.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed experiment/evaluation budget; algorithm reduces number of necessary evaluations by pruning dominated points and focusing on uncertain candidates.",
            "budget_constraint_handling": "Computationally efficient sampling per iteration (single retrain) to respect limited computational budgets; aggregation and exclusion options reduce wasted samples on outliers.",
            "breakthrough_discovery_metric": "Indirect: reducing uncertainty on Pareto boundary increases chance to discover truly Pareto-dominant breakthroughs; measured via hypervolume improvement.",
            "performance_metrics": "Reductions in hypervolume error per iteration; compared favorably against random search in experiments (e.g., reaching target error in far fewer iterations).",
            "comparison_baseline": "Random search and potentially EER (conceptually); shown superior to random search in case study.",
            "performance_vs_baseline": "In the polymer case study, uncertainty sampling within ε-PAL framework reached target hypervolume error in 11 iterations vs 509 for random search in a reported experiment.",
            "efficiency_gain": "Large reduction in required evaluations compared to random search in the demonstrated example (&gt;98% fewer iterations reported).",
            "tradeoff_analysis": "Uncertainty sampling is computationally cheap but can sample outliers; aggregation choices (L2 vs median) and option to exclude high-variance points mitigate that risk; authors note EER could be less prone to outliers but is computationally prohibitive.",
            "optimal_allocation_findings": "Use uncertainty sampling constrained to the unclassified/Pareto set plus pruning (classification) to get a pragmatic balance of information gain and computational cost.",
            "uuid": "e2412.3",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Coregionalized GP (ICM)",
            "name_full": "Coregionalized Gaussian Process (Intrinsic Coregionalization Model, ICM)",
            "brief_description": "A multi-output Gaussian process approach that models correlations between multiple objectives by assuming outputs are linear combinations of shared latent functions, enabling joint prediction and uncertainty estimation across correlated objectives and handling missing outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ICM coregionalized Gaussian processes",
            "system_description": "ICM models assume outputs are scaled samples from shared latent GPs (rank-1) or weighted sums of multiple latent functions (rank-n). Higher rank increases hyperparameter count and optimization difficulty. Used here to model correlated objectives (e.g., Rg, ΔG_rep, ΔG_ads), improve predictions when data are missing for some objectives, and produce joint uncertainties used by PyePAL's classification and sampling.",
            "application_domain": "Multi-objective surrogate modeling in materials design with correlated targets and missing data.",
            "resource_allocation_strategy": "By modeling correlations and imputing missing values, ICM reduces the need for extra dedicated expensive measurements for every objective, thus informing allocation across objectives more efficiently.",
            "computational_cost_metric": "Model training and hyperparameter optimization cost (increases with rank and number of outputs); computational burden measured in wall-clock and complexity of GP training (O(n^3) scaling for naive implementations).",
            "information_gain_metric": "Joint predictive uncertainty across outputs (covariance-informed), which feeds into ε-PAL classification and uncertainty sampling for sample selection.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "By reducing uncertainty jointly across objectives, coregionalized modeling supports more informed exploitation (pruning) and targeted exploration (sampling uncertain multi-objective regions).",
            "diversity_mechanism": "Not directly a diversity mechanism, but by improving imputations for missing objectives it affects which candidates are considered uncertain and hence sampled, indirectly influencing diversity.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Handles missing-data budgets where some objectives are more expensive by sharing information across outputs to reduce required measurements.",
            "budget_constraint_handling": "Uses correlations to impute or better predict expensive-to-measure outputs, reducing experimental burden; authors compared rank-1 and rank-2 models and provide recommended practice.",
            "breakthrough_discovery_metric": "Improved joint uncertainty reductions and better identification of multi-objective Pareto candidates, measured via hypervolume and classification outcomes.",
            "performance_metrics": "Supplementary experiments compare ranks and show improved performance with coregionalized models when data are missing; exact numbers are in supplementary notes (referenced but not fully enumerated in review text).",
            "comparison_baseline": "Independent (separate) GP models per objective vs coregionalized GP; coregionalized preferred when outputs correlate or data are missing.",
            "performance_vs_baseline": "Authors report coregionalized GPs improve predictions and classification in presence of missing data and correlated outputs; details in Supplementary Note 7.2/7.3.",
            "efficiency_gain": "By leveraging correlations, fewer expensive measurements are needed to classify materials and recover Pareto front within ε compared to modeling objectives entirely independently (quantitative gains provided in SI experiments).",
            "tradeoff_analysis": "Higher ICM rank can model more complex correlations but increases hyperparameter complexity and optimization cost; tradeoff between modeling fidelity and computational/hr optimization difficulty discussed.",
            "optimal_allocation_findings": "Use coregionalized GP (rank tuned) when objectives are correlated or some objectives have missing/expensive measurements to make better allocation decisions and impute missing outputs.",
            "uuid": "e2412.4",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Diverse initialisation",
            "name_full": "Diverse initialization (greedy farthest-point sampling, k-means clustering)",
            "brief_description": "Initialization strategies to select a diverse and informative initial training set for surrogate models, improving early predictive performance and sample-efficiency of active learning loops.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Greedy farthest-point sampling and k-means clustering for initial design",
            "system_description": "Greedy farthest-point sampling selects initial points by iteratively picking the point farthest in feature space from existing selected points; k-means clustering selects representatives from clusters to ensure coverage. Authors implemented both in PyePAL and recommend them based on prior results showing diverse selection yields better surrogate predictivity.",
            "application_domain": "Initial design point selection for active learning in materials discovery and general surrogate-model-based optimization.",
            "resource_allocation_strategy": "Allocate initial experimental budget to a diverse set of candidates to ensure surrogate model predictive coverage across the design space before active iterations.",
            "computational_cost_metric": "Cost measured by number of initial evaluations (e.g., 60 used in experiments) and computational cost of clustering/farthest-point selection (negligible compared to expensive experiments).",
            "information_gain_metric": "Diversity heuristics approximate initial information gain by maximizing coverage in feature space, improving downstream uncertainty estimates and sample selection.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration for initialization to ensure later exploitation/targeting is credible.",
            "diversity_mechanism": "Explicit diversity mechanisms: greedy farthest-point and k-means both maximize feature-space coverage to promote diverse hypotheses.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed initial evaluation budget (user-chosen), advised to select enough points for surrogate predictivity (assessed by cross-validation/learning curves).",
            "budget_constraint_handling": "Utilities provided to help choose initialization size via cross-validation and learning-curve analysis; initial diverse sample size should be large enough to reach surrogate model predictive threshold before active learning.",
            "breakthrough_discovery_metric": "Improved early surrogate accuracy and faster convergence to Pareto front (measured via hypervolume error trajectories).",
            "performance_metrics": "Authors used 60 initialization points in reported experiments and provide SI analysis of initial number influence; utilities added to PyePAL for such analysis.",
            "comparison_baseline": "Random initialization vs diversity-based initialization; prior work shows diversity-based selection improves downstream performance.",
            "performance_vs_baseline": "Not numerically detailed in main text here, but authors state they compared farthest-point sampling and k-means and provide utilities and SI experiments illustrating benefits.",
            "efficiency_gain": "Improved surrogate performance enabling fewer active iterations overall; exact gains depend on problem and are discussed in SI.",
            "tradeoff_analysis": "Larger initial budgets increase upfront cost but reduce iterations later; recommended to use cross-validation and learning curves to pick initial size.",
            "optimal_allocation_findings": "Start with a diverse initial set chosen by greedy farthest-point sampling or k-means; set initial size by predictive performance assessments.",
            "uuid": "e2412.5",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Bayesian optimization (conceptual)",
            "name_full": "Bayesian optimization (general acquisition-function-based optimization)",
            "brief_description": "A class of methods that optimize expensive black-box functions by constructing a surrogate and maximizing an acquisition function that trades off exploration and exploitation (e.g., expected improvement, UCB), usually assuming the next evaluation may be the final one.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bayesian optimization (acquisition-function framework)",
            "system_description": "Construct a probabilistic surrogate over objectives and define an acquisition function that balances exploitation (favor regions with high predicted objective) and exploration (favor regions of high uncertainty). The acquisition function is optimized to propose next experiments; commonly used acquisition functions include expected improvement, upper confidence bound, and knowledge-gradient variants.",
            "application_domain": "Global optimization of expensive black-box functions (single- and multi-objective), including materials and chemical design.",
            "resource_allocation_strategy": "Single-step acquisition optimization selects next candidate(s) by maximizing acquisition function which encodes the tradeoff between immediate improvement and information gain.",
            "computational_cost_metric": "Cost per acquisition depends on acquisition optimization complexity and surrogate-update cost; evaluated in number of evaluations and wall-clock time.",
            "information_gain_metric": "Acquisition functions encode expected improvement, information gain, or upper confidence bounds; specific metric depends on chosen acquisition.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balanced directly by acquisition function; often includes hyperparameters (e.g., exploration weight) to tune behavior.",
            "diversity_mechanism": "Not necessarily intrinsic; batch-BO variants include diversity or penalization terms to promote diverse batch choices.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of function evaluations, wall-clock time, or monetary budget.",
            "budget_constraint_handling": "Optimize acquisition function per iteration to maximize utility under remaining budget; multi-step planning variants exist but are computationally heavy.",
            "breakthrough_discovery_metric": "Improvement in objective(s), often measured via best-found value or Pareto front hypervolume in multi-objective BO.",
            "performance_metrics": "Number of function evaluations to reach given optimum/hypervolume; not empirically benchmarked in this paper but discussed conceptually as differing from active learning's goals.",
            "comparison_baseline": "Compared conceptually to ε-PAL/PyePAL: BO focuses on optimization (often assuming next sample may be final), whereas PAL focuses on classification under uncertainty to recover Pareto set.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Authors note difference in goals and framing: BO integrates exploitation/exploration in acquisition function whereas PAL separates classification and sampling; choice depends on whether goal is final-sample optimization vs iterative identification of Pareto set.",
            "optimal_allocation_findings": "For materials discovery where the goal is to enumerate Pareto-optimal regions under uncertainty and not assume a single final sample, PAL-style classification with uncertainty sampling can be more appropriate and pragmatic than typical BO formulations.",
            "uuid": "e2412.6",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Aggregation functions (multi-objective uncertainty)",
            "name_full": "Uncertainty aggregation functions (L2, mean, median; coefficient of variation per objective)",
            "brief_description": "Methods to combine per-objective uncertainties into a scalar score used to rank candidates for sampling in multi-objective active learning; choices affect sensitivity to outliers and sampling behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Uncertainty aggregation (L2 / mean / median; coefficient of variation)",
            "system_description": "Per-objective uncertainties (e.g., predicted stddev or coefficient of variation) are combined into a single scalar using an aggregation function. Authors implemented L2 norm (squares penalize outliers), mean (average sensitivity), and median (robust to outliers) and allow users to select these in PyePAL. The coefficient of variation is used as the per-objective uncertainty measure by default.",
            "application_domain": "Multi-objective active learning for materials discovery and other domains requiring joint uncertainty ranking.",
            "resource_allocation_strategy": "Rank and select candidates by aggregated uncertainty among unclassified/Pareto candidates; aggregation choice steers allocation toward outlier-prone sampling (L2) or robust sampling (median).",
            "computational_cost_metric": "Negligible compared to experiments; cost relates to computing scalar aggregates across objectives for candidate ranking.",
            "information_gain_metric": "Proxy: aggregated predictive uncertainty as a heuristic for expected information gain across multiple objectives.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Aggregation choice indirectly controls exploration of multi-objective uncertainty space; no explicit exploration weight besides aggregation choice and ε-driven classification.",
            "diversity_mechanism": "Aggregation affects whether sampling will concentrate on extreme per-objective uncertainties or more balanced multi-objective uncertainty, thereby affecting diversity of sampled objectives.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Operates under standard active-learning budget constraints; aggregation choice is a low-cost lever to shape sampling.",
            "budget_constraint_handling": "Aggregation selection is a computationally cheap way to influence sampling to be robust or aggressive without expensive acquisition optimization.",
            "breakthrough_discovery_metric": "Influences the kinds of Pareto-front regions explored and hence the probability of finding high-impact candidates, measured via hypervolume and Pareto coverage.",
            "performance_metrics": "Supplementary Figure 26 presents case studies comparing aggregation choices; detailed numeric outcomes are in SI.",
            "comparison_baseline": "Different aggregation rules compared against each other and against other acquisition heuristics conceptually.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Aggregation selection trades sensitivity to outliers vs robustness: L2 can prioritize outlier objectives (potentially finding breakthroughs but also sampling noise), median reduces outlier influence and may better explore stable regions.",
            "optimal_allocation_findings": "Provide configurable aggregation; choice should be problem-specific (use L2 when outliers are important, median/mean when robustness is desired).",
            "uuid": "e2412.7",
            "source_info": {
                "paper_title": "Bias free multiobjective active learning for materials design and discovery",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active Learning for Multi-Objective Optimization",
            "rating": 2,
            "sanitized_title": "active_learning_for_multiobjective_optimization"
        },
        {
            "paper_title": "Toward Optimal Active Learning through Sampling Estimation of Error Reduction",
            "rating": 2,
            "sanitized_title": "toward_optimal_active_learning_through_sampling_estimation_of_error_reduction"
        },
        {
            "paper_title": "Active Learning with Statistical Models",
            "rating": 2,
            "sanitized_title": "active_learning_with_statistical_models"
        },
        {
            "paper_title": "epsilon-PAL",
            "rating": 1,
            "sanitized_title": "epsilonpal"
        }
    ],
    "cost": 0.0195645,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REVIEWER COMMENTS Reviewer #1 (Remarks to the Author): Page 10: "We maintain the orange point as it cannot be discarded within our set uncertainty, see Fig. 4b." Page 11: "We maintain the orange one as it cannot be discarded within our set uncertainty, see Fig. 4c." In Fig. 3, the illustrated configurations under the SMILES column do not match with the number (composition) of beads shown in the table. More specifically, it seems as though the numbers under the R_Ta and R_R columns need to be swapped. Reviewer #2 (Remarks to the Author):
Identification of optimal materials in multiobjective optimization problems is one of the central challenges encountered in most materials design and discovery efforts. The manuscript presents an active learning strategy to efficiently identify the set of Pareto optimal materials within a targeted accuracy for design problems which focus more than one target attributes/properties (that may have conflicting trends). The method is illustrated using a specific design problem pertaining to dispersant applications. This is a very carefully planned, well executed and clearly written paper with all the relevant technical details provided in the supplemental information document accompanying the manuscript. I have following comments:Major points:In general, any active learning based strategy targets to balance the exploration versus exploitation tradeoffs while iterating over the adaptive design loop. At the initial stages, exploration is emphasized over exploitation to ensure a reasonable predictive accuracy of the employed surrogate ML model towards target property prediction for candidate materials coming from different regions of the chemical space. Subsequently, the emphasis is gradually shifted towards exploiting the trained model to push the underlying Pareto front or optimize a prespecified figure-of-merit. It would be very helpful if the authors can consider commenting on how this balancing is carried out in their proposed algorithm.It is mentioned that an initial set of 60 samples was used to train a Gaussian process regression surrogate model. How exactly these 60 points are selected. Does performance of the final model depend sensitively on the initial number and the specific choice of the elements in this set? In a general problem, what criterion should be used to select this initialization set?In the proposed algorithm, selection of the next candidate material during the adaptive design loop is carried out by reducing the uncertainty of the largest rectangle among points falling near or at Pareto front. Therefore, this criterion is based on uncertainty reduction on one specific sample that exhibits the lowest prediction confidence (or the largest scaled errorbars). An alternative measure could be based on reducing the overall uncertainty averaged over all the samples that contribute to the predicted current Pareto optimal set at any given iteration during the active learning procedure. Can author comment on the relative merits of the two approches or why the former approach should be chosen in lieu of the latter?Minor points:Redundant sentence:The authors of "Bias free multiobjective active learning for materials design and discovery" develop a method for efficiently identifying the Pareto front and apply it to a polymer design problem. Notable points include (1) the use of a polymer-based example as the use of ML in polymers is not as rich as in other fields, (2) highlighting the next steps in the design process that are enabled (i.e. inverse design), (3) addressing the issue of explainability in ML through SHAP, and (4) providing the code and data to improve reproducibility and spur other scientific developments. Overall, the manuscript is interesting and</p>
<p>seems to be a good fit for Nature Communications. However, I cannot in good faith recommend the manuscript for publication unless it puts itself in the larger context of the literature. Specifically, epsilon-PAL is not mentioned at all in the introduction, where it should be discussed in the context of prior work. Additionally, the section on Pareto active learning also needs to clarify the exact points that distinguish their method from traditional epsilon-PAL. In addition to this major issue, there are number of minor issues both scientific and clarity related (see below). Scientific • More information is needed to support the following sentence as it is not at all obvious how it contributes to model interpretability. "This ensures that we enumerate through all possible combinations of available monomer counts and types, while also maintaining model interpretability (see Methods)." At the minimum, the exact location in Methods is required. • How are correlations between the uncertainties resulting from three design quantities (Rg, DeltaGrep, DeltaGads) handled? Are they assumed to be uncorrelated? How does this fit into the GPR? • Please provide information on how one should select epsilon. How does it affect performance? • On a related note, why would a smaller epsilon lead to a bigger error? Please discuss.</p>
<p>• Does the surrogate model keep all the information about data points away from the Pareto front? If not, in principle, the model uncertainty could grow with iteration and you are throwing away useful information.</p>
<p>• For the inverse design part, were any data points found beyond the Pareto front including uncertainty? If so, by how much? Both Fig. 8D and Fig. S22 do not prove things one way or another.</p>
<p>Clarity</p>
<p>• There seems to be an error in the discussion of strong and tough. I believe the authors meant strength and ductility. • SMILES strings are not actually SMILES strings (line notation that corresponds to actual chemistry as opposed to a coarse-grained system). Please don't use this name as it implies atomistic. • The beads do not correspond to the chemistry at the left most part of Fig. 1/ top of Fig. 3. Specifically, the beads are not functionalized polypropylene. Please remove. • Fig. 3 has errors (e.g. there are 3 [R] in DoE point 1 not 1).</p>
<p>• Rephrase "For the case of multiobjective maximization under this algorithm, if the optimistic estimate of our predicted material is within some set tolerance(e) below the pessimistic estimate of any other material (in multidimensional space), we can discard these materials with high certainty." It is confusing as written. It took multiple readings and staring at the figure to determine what was meant. • Define epsilon sooner, say in paragraph 3, of "Pareto active learning" where tolerance is discussed.</p>
<p>• "This allows us to recover the true Pareto front and compare it to our predicted Pareto front obtained after each active learning cycle." This is an overstatement. Instead, it recovers the Pareto front within the DoE approach, not in general since the method only samples points from the DoE. • "A key metric for evaluating the quality of the Pareto front is the so called hypervolume indicator, which measures the hypervolume of the objective space, i.e., the size of the space enclosed by the Pareto front and a user-defined reference point (in 2D, this is an area). A better design will always have a larger hypervolume." Please rephrase; it is confusing as written. And specify the reference point you are using; this is important for reproducibility. • "We can observe that e-PAL achieves the target error (e) with more than 98% fewer iterations compared to random exploration of the design space (11 with our approach, 509 with random search)." You need to say that you use epsilon = 0.1 for your approach.</p>
<p>• "Combined with the DoE, we reduced the number of evaluations from possibly over 53 million (the full polymer design space) to 71 (60 initialization points and 11 iterations until we reached 5% hypervolume error)." This is misleading since you define the hypervolume reference. 5% isn't meaningful. Thus, the sentence should be removed. • There is an error in Fig. 6. The caption mentions epsilon = 0.05, but the figure doesn't contain the data. • Which epsilon does Fig. 7 correspond to? The methods say 0.05. If that is correct, it should be in the caption.</p>
<p>In addition to the changes requested by the reviewer we also made some editorial changes in the main text and the supplementary material. All additions to the PyePAL package that we have made for this revision are included in the 0.6.0 release.</p>
<p>Identification of optimal materials in multiobjective optimization problems is one of the central challenges encountered in most materials design and discovery efforts. The manuscript presents an active learning strategy to efficiently identify the set of Pareto optimal materials within a targeted accuracy for design problems which focus more than one target attributes/properties (that may have conflicting trends). The method is illustrated using a specific design problem pertaining to dispersant applications. This is a very carefully planned, well executed and clearly written paper with all the relevant technical details provided in the supplemental information document accompanying the manuscript. I have following comments:</p>
<p>Major points</p>
<p>Reviewer Point P 1.1 -In general, any active learning based strategy targets to balance the exploration versus exploitation tradeoffs while iterating over the adaptive design loop. At the initial stages, exploration is emphasized over exploitation to ensure a reasonable predictive accuracy of the employed surrogate ML model towards target property prediction for candidate materials coming from different regions of the chemical space. Subsequently, the emphasis is gradually shifted towards exploiting the trained model to push the underlying Pareto front or optimize a prespecified figure-of-merit. It would be very helpful if the authors can consider commenting on how this balancing is carried out in their proposed algorithm.</p>
<p>Reply: This question relates to the difference between active learning and Bayesian optimization that we discuss in Supplementary Note 1. Active learning is typically focused on increasing some information criterion, e.g., by reducing the uncertainty, whereas Bayesian optimization relies on optimizing a well-defined (and continuous) acquisition function that balances exploitation and exploration (often with the implicit notion that the next experiment will be the final one). The reason the -PAL algorithm is practical for materials design/discovery applications is that it focuses on identifying the relevant regions of the design space through classification under uncertainty (without the implicit notion of the next experiment being the final one). The classification step, i.e., discarding the points from which we know with certainty that they are dominated by other points, can be thought of as an exploitation step, and the sampling step (where we sample the most uncertain points from the unclassified and Pareto-optimal set) as exploration step.</p>
<p>To clarify, we now write in the main text</p>
<p>After this classification, we can with certainty discard all experiments of which the hyperrectangles are completely below the most pessimistic front. This significantly reduces our design space. In terms of Bayesian optimization, this can be thought of as the exploitation step.</p>
<p>Reviewer Point P 1.2 -It is mentioned that an initial set of 60 samples was used to train a Gaussian process regression surrogate model. How exactly these 60 points are selected. Does performance of the final model depend sensitively on the initial number and the specific choice of the elements in this set? In a general problem, what criterion should be used to select this initialization set?</p>
<p>Reply:</p>
<p>The initial number of points should be chosen such that the surrogate model is predictive. This can be estimated using cross-validation and learning curve analysis for which we now added a utility in the PyePAL package as well as an example notebook.</p>
<p>Regarding the selection method, we compared greedy farthest point sampling and k-means clustering (for both of which we already provide utilities in our package) given that our previous work has shown the utility of a diverse set selection. 1 We discuss the influence of the number of initial points in detail in Supplementary Note 9.1 and also added a remark to the methods section that reads Initial design points used to train the zeroth iteration model were selected using greedy farthest point sampling in feature space. 2 Reviewer Point P 1.3 -In the proposed algorithm, selection of the next candidate material during the adaptive design loop is carried out by reducing the uncertainty of the largest rectangle among points falling near or at Pareto front. Therefore, this criterion is based on uncertainty reduction on one specific sample that exhibits the lowest prediction confidence (or the largest scaled errorbars). An alternative measure could be based on reducing the overall uncertainty averaged over all the samples that contribute to the predicted current Pareto optimal set at any given iteration during the active learning procedure. Can author comment on the relative merits of the two approches or why the former approach should be chosen in lieu of the latter?</p>
<p>Reply: In the Supplementary Information we now report the results for some additional experiments with different aggregation functions. In the PyePAL package we added an option to customize the aggregation function, e.g, to switch from the L 2 norm to the median or a simple average. The choice between the different aggregation functions (to combine the uncertainties in different objectives into one scalar) is comparable to the choice one has to make with respect to the loss function in any optimization problem: In some circumstances it can be beneficial to have a high penalty on outliers (mean-squared error) whereas in other circumstances one does not want to have such a penalty (mean-absolute error). We show some case studies in Supplementary Figure 26.</p>
<p>In the Supplementary Note 10 we now also explain why we chose to implement uncertainty sampling</p>
<p>We chose to not implement sampling methods that require retraining of the models for all potential candidates (e.g., expected error reduction 3,4 ) as those techniques would extremely increase the computational cost of the algorithm (retraining and evaluating the model(s) for every possible new sample, averaged over all possible labels), even though those techniques might mitigate the tendency of uncertainty sampling 5 to sample outliers.</p>
<p>Minor points:</p>
<p>Reviewer Point P 1.4 -Redundant sentence: Page 10: "We maintain the orange point as it cannot be discarded within our set uncertainty, see Fig. 4b." Page 11: "We maintain the orange one as it cannot be discarded within our set uncertainty, see Fig. 4c." Reply: We deleted the second sentence.</p>
<p>Reviewer Point P 1.5 -In Fig. 3, the illustrated configurations under the SMILES column do not match with the number (composition) of beads shown in the table. More specifically, it seems as though the numbers under the R_Ta and R_R columns need to be swapped.</p>
<p>Reply:</p>
<p>We updated the figure in the revised version.</p>
<p>The authors of "Bias free multiobjective active learning for materials design and discovery" develop a method for efficiently identifying the Pareto front and apply it to a polymer design problem. Notable points include (1) the use of a polymer-based example as the use of ML in polymers is not as rich as in other fields, (2) highlighting the next steps in the design process that are enabled (i.e. inverse design), (3) addressing the issue of explainability in ML through SHAP, and (4) providing the code and data to improve reproducibility and spur other scientific developments. Overall, the manuscript is interesting and seems to be a good fit for Nature Communications.</p>
<p>Reviewer Point P 2.1 -However, I cannot in good faith recommend the manuscript for publication unless it puts itself in the larger context of the literature. Specifically, epsilon-PAL is not mentioned at all in the introduction, where it should be discussed in the context of prior work. Additionally, the section on Pareto active learning also needs to clarify the exact points that distinguish their method from traditional epsilon-PAL. In addition to this major issue, there are number of minor issues both scientific and clarity related (see below).</p>
<p>Reply: We now also added references to the work from Zuluaga et al. in the introduction, which now reads</p>
<p>To reach this goal, we use a modified implementation of the -PAL algorithm introduced by Zuluaga et al., 6,7 which iteratively reduces the effective design space by discarding those materials from which we know, with confidence from our model predictions (or measurements), that they are Pareto-dominated by another material.</p>
<p>We discussed algorithmic differences in more detail in the documentation of the package (https://pyepal.readthedocs.io/en/latest/background.html) but now added discussion of this to the method section, which now reads</p>
<p>We implemented a modified version -PAL algorithm 6 in our Python package, PyePAL. Our algorithm differs from the original -PAL algorithm by using the coefficient of variation as the uncertainty measure rather than the predicted standard deviations. Moreover, our implementation does not assume that the ranges (r i ) of the objectives are known. This is, instead using i · r i for the computation of the hyperrectangles, we use i · |µ i | (see Supplementary Note 10). PyePAL generalizes to an arbitrary number of dimensions as opposed to original MATLAB code provided by Zuluaga et al. 8 (limited to 2), and by default sets the uncertainty of labeled points to the experimental uncertainty or the modeled uncertainty. In addition to supporting standard and coregionalized Gaussian processes surrogate models, our library interfaces with other popular modeling techniques with uncertainty quantification such as quantile regression and neural tangent kernels. It also offers native support for missing data, for example, when using coregionalized Gaussian processes, support for both single point (as done in this work) and batch sampling, and the option to exclude high variance points from the classification stage.</p>
<p>Scientific</p>
<p>Reviewer Point P 2.2 -More information is needed to support the following sentence as it is not at all obvious how it contributes to model interpretability. "This ensures that we enumerate through all possible combinations of available monomer counts and types, while also maintaining model interpretability (see Methods)." At the minimum, the exact location in Methods is required.</p>
<p>Reply:</p>
<p>We try to contrast the approach based on the enumeration of beads to generative techniques such as autoencoders. To clarify, we now write This ensures that we enumerate through all possible combinations of available monomer counts and types (see Methods). Compared to sampling from the latent space of generative models such as standard autoencoders or variational autoencoders, this approach maintains a high level of model interpretability.</p>
<p>Reviewer Point P 2.3 -How are correlations between the uncertainties resulting from three design quantities (Rg, DeltaGrep, DeltaGads) handled? Are they assumed to be uncorrelated? How does this fit into the GPR?</p>
<p>Reply: As indicated in reply to point P 2.1 we implemented a range of different methods for uncertainty estimation such as Gaussian process regression, quantile regression using gradient boosted decision trees or neural tangent kernels in the PyePAL package. Many of those models do not offer native support for multioutput problems and in those cases, the different objectives are modeled independently from each other with separate models. Since objectives usually are correlated with each other it can be useful to take this correlation into account. This is especially important in the case of missing data in the objectives where modeling the correlation between the objectives can improve our ability to "impute" the missing observations. In the case of materials discovery, this can be the case when one experiment/simulation is much more expensive than the ones for other objectives. In the article, we use coregionalized Gaussian process models to address this problem. We explain the assumptions of these models in the revised methods section</p>
<p>The ICM models assume that the outputs are scaled samples from the same GPR (rank 1) or weighted sum of n latent functions (rank n). A higher rank is connected to more hyperparameters and typically makes the model more difficult to optimize. We provide a performance comparison of rank 1 and rank 2 models in Supplementary Note 7.2.</p>
<p>Additionally, we updated the Supplementary Information with experiments using rank=2 ICM models.</p>
<p>Reviewer Point P 2.4 -Please provide information on how one should select epsilon. How does it affect performance?</p>
<p>Reply: There a few considerations when choosing we now discuss those in the "Pareto active learning" section Setting a larger tolerance will speed up the classification of the design space but increase the errors. In practice, it is reasonable to set to be larger than the error of the experiment/simulation. Reviewer Point P 2.5 -On a related note, why would a smaller epsilon lead to a bigger error? Please discuss.</p>
<p>Reply:</p>
<p>We realize that the caption of Figure 6 is not the best place for this information, but we did not find a better place in the main text. The reason why the initial error can be higher is due to the way we measure the hypervolume errorwe only consider the materials we classified as -accurate Pareto optimal. With a larger more materials fall initially into this class, whereas it might take longer for the model with smaller to reach a sufficiently small standard deviation to classify points as Pareto optimal-which then can lead to an initially higher hypervolume error. We hope to clarify this in the revised version of the caption which reads Classified points and hypervolume error as a function of the number of iterations. a The -PAL algorithm classifies polymers after each learning iteration with i = 0.05 for every target and a coregionalized Gaussian process surrogate model. The Gaussian process model was initialized with 60 samples that were selected using a greedy farthest point algorithm within feature space. Note that the y-axis is on a log scale. b Hypervolume errors are determined as a function of iteration using the -PAL algorithm with i =0.01, and 0.1 for every target. A larger i makes the algorithm much more efficient but slightly degrades the final performance For i =0.05, we intentionally leave out a third of the simulation results for ∆G rep from the entire data set. The method for obtaining improved predictions for missing measurements with coregionalized Gaussian process models is discussed in more detail in Supplementary Note 7.3. Hypervolume error for random search with mean and standard deviation error bands (bootstrapped with 100 random runs) is shown for comparison. For the -PAL algorithm we only consider the points that have been classified asaccurate Pareto optimal in the calculation of the hypervolume (i.e., with small the number of points in this set will be small in the first iterations, which can lead to larger hypervolume errors). All search procedures were initialized using the same set of initial points, but vary substantially after only one iteration step due to the different hyperparameter values for . Note that the x-axis is on a log scale. Overall, missing data increases the number of iterations that are needed to classify all materials in the design space.</p>
<p>Reviewer Point P 2.6 -Does the surrogate model keep all the information about data points away from the Pareto front? If not, in principle, the model uncertainty could grow with iteration and you are throwing away useful information Reply: The surrogate model keeps all the information about all sampled points. We clarify this now in the "Pareto active learning" section of the main text</p>
<p>The model is then retrained using all sampled points, including those that have been discarded.</p>
<p>Reviewer Point P 2.7 -For the inverse design part, were any data points found beyond the Pareto front including uncertainty? If so, by how much? Both Fig. 8D and Fig. S22 do not prove things one way or another.</p>
<p>Reply: To clarify this point we now write in the inverse design section</p>
<p>We find that independent of whether we bias the GA towards exploration or exploitation, we cannot find polymers that Pareto-dominate the points that we found using our combination of the DoE and -PAL approaches.</p>
<p>Clarity</p>
<p>Reviewer Point P 2.8 -There seems to be an error in the discussion of strong and tough. I believe the authors meant strength and ductility.</p>
<p>Reply:</p>
<p>We thank the reviewer for pointing this out. The sentence in the introduction now reads For example, one would like a material that is both strong and ductile and as these are correlated it is challenging to synthesize new materials that satisfy both criteria at the same time. 9</p>
<p>Reviewer Point P 2.9 -SMILES strings are not actually SMILES strings (line notation that corresponds to actual chemistry as opposed to a coarse-grained system). Please don't use this name as it implies atomistic.</p>
<p>Reply:</p>
<p>We now replaced all occurrences of "SMILES" with "monomer/bead sequence". Reviewer Point P 2.12 -Rephrase "For the case of multiobjective maximization under this algorithm, if the optimistic estimate of our predicted material is within some set tolerance(e) below the pessimistic estimate of any other material (in multidimensional space), we can discard these materials with high certainty." It is confusing as written. It took multiple readings and staring at the figure to determine what was meant.</p>
<p>Reply: We rephrased to</p>
<p>For the case of multiobjective maximization using this algorithm, we can discard materials with high certainty if the optimistic estimate of the material is within some set tolerance ( ) below the pessimistic estimate of any other material.</p>
<p>Reviewer Point P 2.13 -Define epsilon sooner, say in paragraph 3, of "Pareto active learning" where tolerance is discussed.</p>
<p>Reply:</p>
<p>We now write in paragraph 3 of "Pareto active learning":</p>
<p>From the ( )-Pareto dominance relation, we can identify those points that can be discarded with confidence (gray in Fig. 4b) and those which are with high probability Pareto optimal (colored blue) as shown in Fig. 4b. If the pessimistic estimate for our predicted material is greater than a tolerance (defined using the hyperparameter) above the optimistic estimate for all other materials, it will be part of the Pareto front.</p>
<p>Reviewer Point P 2.14 -"This allows us to recover the true Pareto front and compare it to our predicted Pareto front obtained after each active learning cycle." This is an overstatement. Instead, it recovers the Pareto front within the DoE approach, not in general since the method only samples points from the DoE.</p>
<p>Reply: We now write instead</p>
<p>This allows us to recover the true Pareto front (in the space sampled with DoE) and compare it to our predicted Pareto front obtained after each active learning cycle.</p>
<p>Reviewer Point P 2.15 -"A key metric for evaluating the quality of the Pareto front is the so called hypervolume indicator, which measures the hypervolume of the objective space, i.e., the size of the space enclosed by the Pareto front and a userdefined reference point (in 2D, this is an area). A better design will always have a larger hypervolume." Please rephrase; it is confusing as written. And specify the reference point you are using; this is important for reproducibility.</p>
<p>Reply: We rephrased this to</p>
<p>A key metric for evaluating the quality of the Pareto front is the so called hypervolume indicator. This indicator measures the size of the space enclosed by the Pareto front and a user-defined reference point (in 2D, this would equate to the enclosed area), and is commonly used to benchmark Bayesian optimization algorithms. In general, a better design will always have a larger hypervolume.</p>
<p>The reference point is now indicated in the methods section.</p>
<p>Reviewer Point P 2.16 -"We can observe that e-PAL achieves the target error (e) with more than 98% fewer iterations compared to random exploration of the design space (11 with our approach, 509 with random search)." You need to say that you use epsilon = 0.1 for your approach.</p>
<p>Reviewer
Point P 2.10 -The beads do not correspond to the chemistry at the left most part of Fig. 1/ top of Fig. 3. Specifically, the beads are not functionalized polypropylene. Please remove. Reply: We updated the figure in the revised version of the manuscript. Reviewer Point P 2.11 -Fig. 3 has errors (e.g. there are 3 [R] in DoE point 1 not 1). Reply: We updated the figure in the revised version of the manuscript.
Reply: We added this information to the sentence.Reviewer Point P 2.17 -"Combined with the DoE, we reduced the number of evaluations from possibly over 53 million (the full polymer design space) to 71 (60 initialization points and 11 iterations until we reached 5% hypervolume error)." This is misleading since you define the hypervolume reference. 5% isn't meaningful. Thus, the sentence should be removed.Reply:We removed the sentence in the revised version of the manuscript and added a discussion of the influence of the hypervolume reference point and target error to the revised SI.Reviewer Point P 2.18 -There is an error inFig. 6. The caption mentions epsilon = 0.05, but the figure doesn't contain the data.Reply:We fixed the typo.Reviewer Point P 2.19 -Which epsilon doesFig. 7correspond to? The methods say 0.05. If that is correct, it should be in the caption.Reply: We added this information to the caption of the revised version.REVIEWERS' COMMENTSReviewer #1 (Remarks to the Author):After going through the revised manuscript, the response letter and the PyePAL code available on GitHub, I believe that the authors have satisfactorily addressed all my comments and I am happy to recommend the paper for publication in Nature Communications.Reviewer #2 (Remarks to the Author):The authors have addressed all of my concerns, and I now recommend the manuscript for publication. I also would like to note that I appreciated all the work that went into the website for PyePAL especially highlighting the differences between prior e-PAL work and PyePAL (https://pyepal.readthedocs.io/en/latest/background.html).
. S M Moosavi, A Nandy, K M Jablonka, D Ongari, J P Janet, P G Boyd, Y Lee, B Smit, H J Kulik, Nat. Commun. 4068Moosavi, S. M.; Nandy, A.; Jablonka, K. M.; Ongari, D.; Janet, J. P.; Boyd, P. G.; Lee, Y.; Smit, B.; Kulik, H. J. Nat. Commun. 2020, 11, 4068.</p>
<p>. R W Kennard, L A Stone, Technometrics. 11Kennard, R. W.; Stone, L. A. Technometrics 1969, 11, 137-148.</p>
<p>Toward Optimal Active Learning through Sampling Estimation of Error Reduction. N Roy, A Mccallum, Proceedings of the Eighteenth International Conference on Machine Learning. the Eighteenth International Conference on Machine LearningSan Francisco, CA, USARoy, N.; McCallum, A. Toward Optimal Active Learning through Sampling Esti- mation of Error Reduction. Proceedings of the Eighteenth International Confer- ence on Machine Learning. San Francisco, CA, USA, 2001; p 441-448.</p>
<p>Active Learning with Statistical Models. D A Cohn, Z Ghahramani, M I Jordan, Proceedings of the 7th International Conference on Neural Information Processing Systems. the 7th International Conference on Neural Information Processing SystemsCambridge, MA, USACohn, D. A.; Ghahramani, Z.; Jordan, M. I. Active Learning with Statistical Models. Proceedings of the 7th International Conference on Neural Information Processing Systems. Cambridge, MA, USA, 1994; p 705-712.</p>
<p>. D D Lewis, W A Gale, Sigir &apos;, Lewis, D. D.; Gale, W. A. SIGIR '94;</p>
<p>. Springer London, Springer London, 1994; pp 3-12.</p>
<p>. M Zuluaga, A Krause, M J Püschel, Mach. Learn. Res. 17Zuluaga, M.; Krause, A.; Püschel, M. J. Mach. Learn. Res. 2016, 17, 1-32.</p>
<p>Active Learning for Multi-Objective Optimization. M Zuluaga, G Sergent, A Krause, M Püschel, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningAtlanta, Georgia, USAZuluaga, M.; Sergent, G.; Krause, A.; Püschel, M. Active Learning for Multi- Objective Optimization. Proceedings of the 30th International Conference on Machine Learning. Atlanta, Georgia, USA, 2013; pp 462-470.</p>
<p>. Vivek Nair, Vivek Nair, epsilon-PAL. https://github.com/FlashRepo/epsilon-PAL, 2017.</p>
<p>Fatigue and Durability of Structural Materials. S S Manson, ASM InternationalMaterials Park, OhioManson, S. S. Fatigue and Durability of Structural Materials; ASM International: Materials Park, Ohio, 2006.</p>            </div>
        </div>

    </div>
</body>
</html>
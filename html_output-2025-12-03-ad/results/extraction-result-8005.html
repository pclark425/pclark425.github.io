<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8005 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8005</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8005</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4692e5d11cde0f10cbd5a534a5870eb299e8156" target="_blank">Jointly Measuring Diversity and Quality in Text Generation Models</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution by introducing a metric that approximates this distance using n-gram based measures.</p>
                <p><strong>Paper Abstract:</strong> Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglecting their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generatorʼs density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8005.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8005.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bilingual Evaluation Understudy (BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram based automatic metric that measures quality of generated text by computing n-gram precision against reference sentences (with brevity penalty); used here for unconditional text generation by treating the test set as the reference set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU (BLEU-2 .. BLEU-5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram precision of generated sentences versus reference set (test set for unconditional generation), averaged across generated sentences, including brevity penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU-2, BLEU-3, BLEU-4, BLEU-5</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>n-gram precision (0–1) with brevity penalty; higher is better; often reported as fraction or percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB (used as reference/test sets)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BLEU scores reported in Tables 1–3 (BLEU-2..BLEU-5 for each model and dataset); paper notes BLEU measures quality but ignores diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Measures only quality/validity and not coverage/diversity; can reward mode-collapsed models that repeatedly generate a few high-quality sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8005.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diversity-only metric that computes BLEU by treating other generated sentences as references for each generated sentence and averaging; lower Self-BLEU indicates higher diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Self-BLEU (Self-BLEU-2 .. Self-BLEU-5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each generated sentence compute BLEU using the rest of generated samples as reference, then average across generated sentences to quantify diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Self-BLEU-2, Self-BLEU-3, Self-BLEU-4, Self-BLEU-5</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>n-gram based BLEU computed among generated samples; lower value indicates greater diversity (range 0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on generated samples from models trained on COCO, EMNLP2017, IMDB</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Self-BLEU scores shown in Tables 1–3; used as the diversity axis in quality-diversity plots (Fig.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Measures diversity only and ignores the absolute quality/validity of generated texts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8005.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Log-Likelihood (NLL) / Likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A likelihood-based metric assessing how well a probabilistic generative model assigns probability to test data; lower NLL indicates higher likelihood of test set under the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Negative Log-Likelihood on test set</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the negative log probability (or average per-token NLL) of real test sequences under the trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>NLL (scalar, lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Sum or average of −log p_model(x) over test samples (units: nats or bits depending on log base); lower indicates better fit to test data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>NLL values reported in Tables 1–4 (per model and dataset); used as a baseline measure and compared to proposed metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Does not assess free-running generation (exposure bias); a model can have low NLL yet assign high probability to many invalid sentences; penalizes missed modes severely (unfair to compare to non-likelihood-trained methods).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8005.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle Negative Log-Likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metric from SeqGAN evaluation: negative log-likelihood of generated samples evaluated under a synthetic oracle (a known probabilistic generator), used in oracle training mode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seqgan: Sequence generative adversarial nets with policy gradient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate samples from trained model and compute their negative log-likelihood under a known synthetic oracle distribution (randomly initialized LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Oracle-NLL (scalar, lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Average negative log probability of generated sequences under the oracle model; units in nats/bits depending on log base.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Oracle synthetic dataset (random LSTM generator)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Oracle-NLL numbers are reported in Table 4 (used for synthetic oracle comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ignores coverage/diversity (a model generating a single high-quality sentence can score well).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8005.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Entropy (conditional distribution entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy of the generative model's conditional token distributions, used as a diversity indicator (higher entropy → more diverse outputs); sometimes estimated via Monte Carlo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Entropy of model distribution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimate the entropy of the model's output distribution (e.g., conditional token distributions) to quantify diversity; Monte-Carlo estimation used when direct calc infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Entropy (nats or bits; higher indicates higher diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Shannon entropy H(P) = −Σ p(x) log p(x) over tokens or sequences; reported as scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Oracle synthetic dataset (entropy used in quality-diversity sweep plots for oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Entropy used as diversity measure in Fig.1d for oracle dataset; exact numeric tables not given for entropy in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Direct computation may be infeasible; estimates can be noisy; entropy alone does not capture sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8005.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-Jaccard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Set Jaccard (MS-Jaccard-N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed n-gram multi-set similarity metric that compares normalized n-gram frequency multi-sets from generated and real samples via a Jaccard-like intersection/union ratio across n-gram orders, aggregated by geometric mean.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MS-Jaccard (MS-Jaccard-2 .. MS-Jaccard-5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each n (1..N), compute normalized n-gram frequencies per sentence C_n(g,S) for generated vs real sets, take sum over g of min counts divided by sum of max counts; aggregate scores over n via geometric mean.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MS-Jaccard-N (score 0–1; higher is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>score_n = Σ_g min{C_n(g,S1),C_n(g,S2)} / Σ_g max{C_n(g,S1),C_n(g,S2)}; MS-Jaccard-N = geometric mean over n=1..N; values ∈ [0,1], higher indicates closer match.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB (reported as MSJ2..MSJ5 in Tables 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MS-Jaccard scores reported in Tables 1–3; MLE often achieved highest MS-Jaccard indicating better joint quality-diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on n-gram representation (may not capture semantic similarity beyond matched n-grams); normalization is per-sentence to control sample size effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8005.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed feature-based distance that models pooled BERT features of real and generated sentences as Gaussians and computes their Fréchet (Wasserstein-2) distance; lower is better.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extract fixed-size pooled features from BERT for real and generated sentences, compute mean vectors m_i and covariances C_i, then compute Fréchet distance between Gaussians: sqrt(||m1-m2||^2 + Tr(C1 + C2 - 2*(C1*C2)^{1/2})).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>FBD (scalar, lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fréchet distance between multivariate Gaussians of BERT pooled features; non-negative scalar with 0 indicating identical Gaussian statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB (FBD values in Tables 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FBD reported in Tables 1–3; lower FBD correlates with better joint quality-diversity; MLE often had lower FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Assumes Gaussianity in BERT feature space; quality depends on choice of feature extractor (BERT pooled features); as a distance lower is better but absolute scale depends on features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8005.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bhattacharyya</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bhattacharyya Distance (Monte-Carlo estimate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symmetric statistical distance between two probability distributions P (oracle) and Q (model); paper proposes a Monte-Carlo estimator to compute this distance when oracle probabilities are available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bhattacharyya distance (sample-based estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimate B(P,Q) using samples from P and Q via the provided Monte-Carlo formula involving logs of averaged square-root density ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Bhattacharyya distance (scalar, lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>B(P,Q)= −1/2 ( ln(1/N Σ_i sqrt(q(x_i)/p(x_i))) + ln(1/M Σ_j sqrt(p(x_j)/q(x_j))) ); estimated from samples x_i~P, x_j~Q.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Oracle synthetic dataset (where oracle distribution P is known and sample probabilities are computable)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Bhattacharyya distances reported in Table 4 for oracle dataset; used to rank models in oracle setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires access to probability densities (oracle training mode) for accurate estimation; Monte-Carlo estimates can be noisy and require many samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8005.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fréchet Inception Distance (FID)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image-generation evaluation metric that models Inception network features as Gaussians and computes Fréchet distance between real and generated image feature distributions; referenced as inspiration for FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gans trained by a two time-scale update rule converge to a local nash equilibrium</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Vision / Generative Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fréchet Inception Distance (FID)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute mean and covariance of Inception network features on real vs generated images and compute Fréchet distance between resulting Gaussians.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>FID (scalar, lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same Fréchet Gaussian formula as for FBD; lower indicates closer feature statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Referenced (image datasets) — used as analogy only; not applied to text in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Originally for images and depends on the pretrained feature extractor (Inception); motivates using BERT features for text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8005.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inception Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inception Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image generation metric that assesses both sample quality and diversity via classifier-based predicted label distributions; mentioned as another image-metric analog.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Vision / Generative Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Inception Score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute KL divergence between conditional label distribution p(y|x) and marginal p(y) using a pretrained classifier; higher indicates better (quality & diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Inception Score (scalar, higher is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>exp( E_x[ KL( p(y|x) || p(y) ) ] ); unitless positive scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Referenced only (image generation literature)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Designed for images and depends on pretrained classifier labels; cited as part of related image metrics motivating text analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8005.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernel two-sample test statistic that measures distance between real and generated sample feature distributions; used in TextGAN as generator objective to match feature distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial feature matching for text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / Generative Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Maximum Mean Discrepancy (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute difference in means of feature maps in a reproducing kernel Hilbert space between two distributions; used as objective to match generator and real data features.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MMD (scalar, lower indicates closer distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MMD^2 = E_{x,x'}[k(x,x')] + E_{y,y'}[k(y,y')] - 2 E_{x,y}[k(x,y)] for kernel k; non-negative.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in relation to TextGAN (not used as evaluation metric in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned as generator objective in cited work; not deployed here as an evaluation metric for LLM-generated theories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8005.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature sweep / Quality-Diversity spectrum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temperature sweep (controlling softmax temperature) for quality-diversity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method of varying the softmax temperature T of the generator's conditional distribution to produce a spectrum of models trading off quality vs diversity; used to produce quality-diversity curves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language gans falling short</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Temperature sweep (Softmax temperature T)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Scale logits o_t by 1/T before softmax (Softmax(o_t / T)); sweep T across values <1 to >1 to trade off conditional entropy (quality vs diversity) and plot resulting quality-diversity metrics to analyze model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Quality (BLEU or Oracle-NLL) vs Diversity (Self-BLEU or Entropy) curves</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Temperature T > 1 increases entropy (more diverse); T < 1 reduces entropy (higher quality, lower diversity); plots quantify metrics at each T.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO, EMNLP2017, IMDB, Oracle synthetic (used in Fig.1 temperature sweeps)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Quality-diversity curves for each model across temperatures shown in Fig.1; used to validate that single-number metrics (MS-Jaccard, FBD) align with full spectrum ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires sweeping over many temperature values to fully characterize trade-offs; paper argues proposed single-number metrics predict the spectrum behavior without full sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8005.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Texygen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Texygen benchmarking platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source benchmarking platform for text generation models used by the authors to obtain implementations and run experiments consistently across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Texygen: A benchmarking platform for text generation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Texygen platform (framework for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides implementations of various text generation models and evaluation utilities; used to train and evaluate SeqGAN, RankGAN, MaliGAN, MLE baselines under consistent settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>N/A (framework/tool)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used with COCO, EMNLP2017, IMDB, and Oracle datasets in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Experiments were run using implementations from the Texygen platform; authors note preserving Texygen hyperparameter settings for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Platform-specific settings may influence results; authors preserve settings but do not claim broad reproducibility beyond reporting code references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8005.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (pooled features)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT pooled classification features (Devlin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained deep transformer model used as a feature extractor: the pooled output (fixed-size) is used to compute Gaussian statistics for FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERT pooled features (as feature extractor for FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Pass sentences through pretrained BERT and use the pooled classification vector (fixed-size) as embedding; compute means/covariances over these to compute Fréchet distance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute FBD</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BERT pooled vector statistics (mean and covariance) form Gaussian approximations for Fréchet calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO, EMNLP2017, IMDB</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BERT-based FBD numbers reported in Tables 1–3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Quality of FBD depends on choice of pretrained BERT layer; Gaussian assumption may not hold perfectly in feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8005.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COCO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Microsoft COCO Captions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of image captions (here used as unconditional text corpus) consisting of ~524K captions filtered to length 5–25 tokens and used to train and evaluate models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Microsoft COCO: common objects in context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / Vision-language</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>COCO Captions dataset (evaluation corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Used as real-data corpus: split into train/validation/test; generated samples compared to test set using metrics (BLEU, MS-Jaccard, FBD, Self-BLEU, NLL).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Metrics applied on models trained on COCO: BLEU, Self-BLEU, MS-Jaccard, FBD, NLL</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COCO Captions (524,225 sentences filtered; train/val/test splits used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Tables and figures show per-model metrics on COCO (Table 1, Fig.1a, Fig.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Used as unconditional text corpus; captions differ stylistically from other text genres which can affect generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8005.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMNLP2017 WMT News</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EMNLP2017 WMT News (English corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A news-text corpus used as an unconditional text dataset after filtering (sentence length 20–40, vocab cutoff), used to train and evaluate generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Findings of the 2017 conference on machine translation (WMT17)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>EMNLP2017 WMT News dataset (evaluation corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Filtered English news sentences used as training/validation/test corpus; models evaluated against test set using metrics (BLEU, MS-Jaccard, FBD, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, Self-BLEU, MS-Jaccard, FBD, NLL</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>EMNLP2017 WMT News (subsampled to 40k train, 20k val, 20k test in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Results on this dataset shown in Table 2 and Fig.1b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Preprocessing (UNK replacements, freq thresholds) affects vocabulary and distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8005.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMDB Movie Reviews dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corpus of movie review sentences (first two sentences of each review selected and filtered) used as an unconditional text dataset in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning word vectors for sentiment analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IMDB Movie Reviews dataset (evaluation corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Filtered set of sentences from IMDB reviews used to train/evaluate text generation models with the suite of metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, Self-BLEU, MS-Jaccard, FBD, NLL</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>IMDB dataset (subsampled to 20k train, 10k val, 10k test in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Results on IMDB reported in Table 3 and Fig.1c.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset preprocessing (sentence truncation, UNK thresholds) changes the natural distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8005.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e8005.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle synthetic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle synthetic dataset (random LSTM generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset generated by a randomly initialized LSTM used as a known 'oracle' distribution for controlled evaluation (probabilities available), enabling Oracle-NLL and Bhattacharyya evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seqgan: Sequence generative adversarial nets with policy gradient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP (synthetic evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Oracle synthetic dataset (probabilistic oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a known LSTM generator (hidden=32, embed=3200, vocab=5000, seq length=20) to produce training/validation/test samples; because oracle probabilities are available, symmetric distribution distances (Bhattacharyya) can be computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Oracle-NLL, Bhattacharyya distance, NLL</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Oracle dataset (100k generated samples; 50k train, 25k val, 25k test)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 4 reports Oracle-NLL and Bhattacharyya results; used to validate proposed distributional metric in oracle mode.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Synthetic oracle does not mirror real-world distributions; useful for controlled comparisons but limited ecological validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bleu: a method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>Seqgan: Sequence generative adversarial nets with policy gradient <em>(Rating: 2)</em></li>
                <li>Fréchet Inception Distance <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Texygen: A benchmarking platform for text generation models <em>(Rating: 2)</em></li>
                <li>Language gans falling short <em>(Rating: 2)</em></li>
                <li>Adversarial feature matching for text generation <em>(Rating: 1)</em></li>
                <li>Maximum-likelihood augmented discrete generative adversarial networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8005",
    "paper_id": "paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "BLEU",
            "name_full": "Bilingual Evaluation Understudy (BLEU)",
            "brief_description": "An n-gram based automatic metric that measures quality of generated text by computing n-gram precision against reference sentences (with brevity penalty); used here for unconditional text generation by treating the test set as the reference set.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "BLEU (BLEU-2 .. BLEU-5)",
            "evaluation_method_description": "Compute n-gram precision of generated sentences versus reference set (test set for unconditional generation), averaged across generated sentences, including brevity penalty.",
            "evaluation_metric": "BLEU-2, BLEU-3, BLEU-4, BLEU-5",
            "metric_definition": "n-gram precision (0–1) with brevity penalty; higher is better; often reported as fraction or percentage.",
            "dataset_or_benchmark": "COCO Captions, EMNLP2017 WMT News, IMDB (used as reference/test sets)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "BLEU scores reported in Tables 1–3 (BLEU-2..BLEU-5 for each model and dataset); paper notes BLEU measures quality but ignores diversity.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Measures only quality/validity and not coverage/diversity; can reward mode-collapsed models that repeatedly generate a few high-quality sentences.",
            "uuid": "e8005.0",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Self-BLEU",
            "name_full": "Self-BLEU",
            "brief_description": "A diversity-only metric that computes BLEU by treating other generated sentences as references for each generated sentence and averaging; lower Self-BLEU indicates higher diversity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Self-BLEU (Self-BLEU-2 .. Self-BLEU-5)",
            "evaluation_method_description": "For each generated sentence compute BLEU using the rest of generated samples as reference, then average across generated sentences to quantify diversity.",
            "evaluation_metric": "Self-BLEU-2, Self-BLEU-3, Self-BLEU-4, Self-BLEU-5",
            "metric_definition": "n-gram based BLEU computed among generated samples; lower value indicates greater diversity (range 0–1).",
            "dataset_or_benchmark": "Used on generated samples from models trained on COCO, EMNLP2017, IMDB",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Self-BLEU scores shown in Tables 1–3; used as the diversity axis in quality-diversity plots (Fig.1).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Measures diversity only and ignores the absolute quality/validity of generated texts.",
            "uuid": "e8005.1",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "NLL",
            "name_full": "Negative Log-Likelihood (NLL) / Likelihood",
            "brief_description": "A likelihood-based metric assessing how well a probabilistic generative model assigns probability to test data; lower NLL indicates higher likelihood of test set under the model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Negative Log-Likelihood on test set",
            "evaluation_method_description": "Compute the negative log probability (or average per-token NLL) of real test sequences under the trained model.",
            "evaluation_metric": "NLL (scalar, lower is better)",
            "metric_definition": "Sum or average of −log p_model(x) over test samples (units: nats or bits depending on log base); lower indicates better fit to test data.",
            "dataset_or_benchmark": "COCO Captions, EMNLP2017 WMT News, IMDB, Oracle synthetic dataset",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "NLL values reported in Tables 1–4 (per model and dataset); used as a baseline measure and compared to proposed metrics.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Does not assess free-running generation (exposure bias); a model can have low NLL yet assign high probability to many invalid sentences; penalizes missed modes severely (unfair to compare to non-likelihood-trained methods).",
            "uuid": "e8005.2",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Oracle-NLL",
            "name_full": "Oracle Negative Log-Likelihood",
            "brief_description": "Metric from SeqGAN evaluation: negative log-likelihood of generated samples evaluated under a synthetic oracle (a known probabilistic generator), used in oracle training mode.",
            "citation_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Oracle-NLL",
            "evaluation_method_description": "Generate samples from trained model and compute their negative log-likelihood under a known synthetic oracle distribution (randomly initialized LSTM).",
            "evaluation_metric": "Oracle-NLL (scalar, lower is better)",
            "metric_definition": "Average negative log probability of generated sequences under the oracle model; units in nats/bits depending on log base.",
            "dataset_or_benchmark": "Oracle synthetic dataset (random LSTM generator)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Oracle-NLL numbers are reported in Table 4 (used for synthetic oracle comparisons).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Ignores coverage/diversity (a model generating a single high-quality sentence can score well).",
            "uuid": "e8005.3",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Entropy",
            "name_full": "Model Entropy (conditional distribution entropy)",
            "brief_description": "Entropy of the generative model's conditional token distributions, used as a diversity indicator (higher entropy → more diverse outputs); sometimes estimated via Monte Carlo.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Entropy of model distribution",
            "evaluation_method_description": "Estimate the entropy of the model's output distribution (e.g., conditional token distributions) to quantify diversity; Monte-Carlo estimation used when direct calc infeasible.",
            "evaluation_metric": "Entropy (nats or bits; higher indicates higher diversity)",
            "metric_definition": "Shannon entropy H(P) = −Σ p(x) log p(x) over tokens or sequences; reported as scalar.",
            "dataset_or_benchmark": "Oracle synthetic dataset (entropy used in quality-diversity sweep plots for oracle)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Entropy used as diversity measure in Fig.1d for oracle dataset; exact numeric tables not given for entropy in main tables.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Direct computation may be infeasible; estimates can be noisy; entropy alone does not capture sample quality.",
            "uuid": "e8005.4",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "MS-Jaccard",
            "name_full": "Multi-Set Jaccard (MS-Jaccard-N)",
            "brief_description": "Proposed n-gram multi-set similarity metric that compares normalized n-gram frequency multi-sets from generated and real samples via a Jaccard-like intersection/union ratio across n-gram orders, aggregated by geometric mean.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "MS-Jaccard (MS-Jaccard-2 .. MS-Jaccard-5)",
            "evaluation_method_description": "For each n (1..N), compute normalized n-gram frequencies per sentence C_n(g,S) for generated vs real sets, take sum over g of min counts divided by sum of max counts; aggregate scores over n via geometric mean.",
            "evaluation_metric": "MS-Jaccard-N (score 0–1; higher is better)",
            "metric_definition": "score_n = Σ_g min{C_n(g,S1),C_n(g,S2)} / Σ_g max{C_n(g,S1),C_n(g,S2)}; MS-Jaccard-N = geometric mean over n=1..N; values ∈ [0,1], higher indicates closer match.",
            "dataset_or_benchmark": "COCO Captions, EMNLP2017 WMT News, IMDB (reported as MSJ2..MSJ5 in Tables 1–3)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "MS-Jaccard scores reported in Tables 1–3; MLE often achieved highest MS-Jaccard indicating better joint quality-diversity.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Depends on n-gram representation (may not capture semantic similarity beyond matched n-grams); normalization is per-sentence to control sample size effects.",
            "uuid": "e8005.5",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "FBD",
            "name_full": "Fréchet BERT Distance (FBD)",
            "brief_description": "Proposed feature-based distance that models pooled BERT features of real and generated sentences as Gaussians and computes their Fréchet (Wasserstein-2) distance; lower is better.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Fréchet BERT Distance (FBD)",
            "evaluation_method_description": "Extract fixed-size pooled features from BERT for real and generated sentences, compute mean vectors m_i and covariances C_i, then compute Fréchet distance between Gaussians: sqrt(||m1-m2||^2 + Tr(C1 + C2 - 2*(C1*C2)^{1/2})).",
            "evaluation_metric": "FBD (scalar, lower is better)",
            "metric_definition": "Fréchet distance between multivariate Gaussians of BERT pooled features; non-negative scalar with 0 indicating identical Gaussian statistics.",
            "dataset_or_benchmark": "COCO Captions, EMNLP2017 WMT News, IMDB (FBD values in Tables 1–3)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "FBD reported in Tables 1–3; lower FBD correlates with better joint quality-diversity; MLE often had lower FBD.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Assumes Gaussianity in BERT feature space; quality depends on choice of feature extractor (BERT pooled features); as a distance lower is better but absolute scale depends on features.",
            "uuid": "e8005.6",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Bhattacharyya",
            "name_full": "Bhattacharyya Distance (Monte-Carlo estimate)",
            "brief_description": "Symmetric statistical distance between two probability distributions P (oracle) and Q (model); paper proposes a Monte-Carlo estimator to compute this distance when oracle probabilities are available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Bhattacharyya distance (sample-based estimator)",
            "evaluation_method_description": "Estimate B(P,Q) using samples from P and Q via the provided Monte-Carlo formula involving logs of averaged square-root density ratios.",
            "evaluation_metric": "Bhattacharyya distance (scalar, lower is better)",
            "metric_definition": "B(P,Q)= −1/2 ( ln(1/N Σ_i sqrt(q(x_i)/p(x_i))) + ln(1/M Σ_j sqrt(p(x_j)/q(x_j))) ); estimated from samples x_i~P, x_j~Q.",
            "dataset_or_benchmark": "Oracle synthetic dataset (where oracle distribution P is known and sample probabilities are computable)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Bhattacharyya distances reported in Table 4 for oracle dataset; used to rank models in oracle setting.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Requires access to probability densities (oracle training mode) for accurate estimation; Monte-Carlo estimates can be noisy and require many samples.",
            "uuid": "e8005.7",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "FID",
            "name_full": "Fréchet Inception Distance (FID)",
            "brief_description": "An image-generation evaluation metric that models Inception network features as Gaussians and computes Fréchet distance between real and generated image feature distributions; referenced as inspiration for FBD.",
            "citation_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Vision / Generative Modeling",
            "theory_type": null,
            "evaluation_method_name": "Fréchet Inception Distance (FID)",
            "evaluation_method_description": "Compute mean and covariance of Inception network features on real vs generated images and compute Fréchet distance between resulting Gaussians.",
            "evaluation_metric": "FID (scalar, lower is better)",
            "metric_definition": "Same Fréchet Gaussian formula as for FBD; lower indicates closer feature statistics.",
            "dataset_or_benchmark": "Referenced (image datasets) — used as analogy only; not applied to text in this paper.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Originally for images and depends on the pretrained feature extractor (Inception); motivates using BERT features for text.",
            "uuid": "e8005.8",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Inception Score",
            "name_full": "Inception Score",
            "brief_description": "An image generation metric that assesses both sample quality and diversity via classifier-based predicted label distributions; mentioned as another image-metric analog.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Vision / Generative Modeling",
            "theory_type": null,
            "evaluation_method_name": "Inception Score",
            "evaluation_method_description": "Compute KL divergence between conditional label distribution p(y|x) and marginal p(y) using a pretrained classifier; higher indicates better (quality & diversity).",
            "evaluation_metric": "Inception Score (scalar, higher is better)",
            "metric_definition": "exp( E_x[ KL( p(y|x) || p(y) ) ] ); unitless positive scalar.",
            "dataset_or_benchmark": "Referenced only (image generation literature)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Designed for images and depends on pretrained classifier labels; cited as part of related image metrics motivating text analogs.",
            "uuid": "e8005.9",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "MMD",
            "name_full": "Maximum Mean Discrepancy (MMD)",
            "brief_description": "A kernel two-sample test statistic that measures distance between real and generated sample feature distributions; used in TextGAN as generator objective to match feature distributions.",
            "citation_title": "Adversarial feature matching for text generation",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine Learning / Generative Modeling",
            "theory_type": null,
            "evaluation_method_name": "Maximum Mean Discrepancy (MMD)",
            "evaluation_method_description": "Compute difference in means of feature maps in a reproducing kernel Hilbert space between two distributions; used as objective to match generator and real data features.",
            "evaluation_metric": "MMD (scalar, lower indicates closer distributions)",
            "metric_definition": "MMD^2 = E_{x,x'}[k(x,x')] + E_{y,y'}[k(y,y')] - 2 E_{x,y}[k(x,y)] for kernel k; non-negative.",
            "dataset_or_benchmark": "Mentioned in relation to TextGAN (not used as evaluation metric in this paper's experiments)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Mentioned as generator objective in cited work; not deployed here as an evaluation metric for LLM-generated theories.",
            "uuid": "e8005.10",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Temperature sweep / Quality-Diversity spectrum",
            "name_full": "Temperature sweep (controlling softmax temperature) for quality-diversity analysis",
            "brief_description": "Method of varying the softmax temperature T of the generator's conditional distribution to produce a spectrum of models trading off quality vs diversity; used to produce quality-diversity curves.",
            "citation_title": "Language gans falling short",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Temperature sweep (Softmax temperature T)",
            "evaluation_method_description": "Scale logits o_t by 1/T before softmax (Softmax(o_t / T)); sweep T across values &lt;1 to &gt;1 to trade off conditional entropy (quality vs diversity) and plot resulting quality-diversity metrics to analyze model behavior.",
            "evaluation_metric": "Quality (BLEU or Oracle-NLL) vs Diversity (Self-BLEU or Entropy) curves",
            "metric_definition": "Temperature T &gt; 1 increases entropy (more diverse); T &lt; 1 reduces entropy (higher quality, lower diversity); plots quantify metrics at each T.",
            "dataset_or_benchmark": "COCO, EMNLP2017, IMDB, Oracle synthetic (used in Fig.1 temperature sweeps)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Quality-diversity curves for each model across temperatures shown in Fig.1; used to validate that single-number metrics (MS-Jaccard, FBD) align with full spectrum ordering.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Requires sweeping over many temperature values to fully characterize trade-offs; paper argues proposed single-number metrics predict the spectrum behavior without full sweep.",
            "uuid": "e8005.11",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Texygen",
            "name_full": "Texygen benchmarking platform",
            "brief_description": "An open-source benchmarking platform for text generation models used by the authors to obtain implementations and run experiments consistently across models.",
            "citation_title": "Texygen: A benchmarking platform for text generation models",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Texygen platform (framework for experiments)",
            "evaluation_method_description": "Provides implementations of various text generation models and evaluation utilities; used to train and evaluate SeqGAN, RankGAN, MaliGAN, MLE baselines under consistent settings.",
            "evaluation_metric": "N/A (framework/tool)",
            "metric_definition": null,
            "dataset_or_benchmark": "Used with COCO, EMNLP2017, IMDB, and Oracle datasets in experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Experiments were run using implementations from the Texygen platform; authors note preserving Texygen hyperparameter settings for fairness.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Platform-specific settings may influence results; authors preserve settings but do not claim broad reproducibility beyond reporting code references.",
            "uuid": "e8005.12",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "BERT (pooled features)",
            "name_full": "BERT pooled classification features (Devlin et al.)",
            "brief_description": "Pretrained deep transformer model used as a feature extractor: the pooled output (fixed-size) is used to compute Gaussian statistics for FBD.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_size": null,
            "scientific_domain": "Natural Language Processing",
            "theory_type": null,
            "evaluation_method_name": "BERT pooled features (as feature extractor for FBD)",
            "evaluation_method_description": "Pass sentences through pretrained BERT and use the pooled classification vector (fixed-size) as embedding; compute means/covariances over these to compute Fréchet distance.",
            "evaluation_metric": "Used to compute FBD",
            "metric_definition": "BERT pooled vector statistics (mean and covariance) form Gaussian approximations for Fréchet calculation.",
            "dataset_or_benchmark": "COCO, EMNLP2017, IMDB",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "BERT-based FBD numbers reported in Tables 1–3.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Quality of FBD depends on choice of pretrained BERT layer; Gaussian assumption may not hold perfectly in feature space.",
            "uuid": "e8005.13",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "COCO",
            "name_full": "Microsoft COCO Captions",
            "brief_description": "A dataset of image captions (here used as unconditional text corpus) consisting of ~524K captions filtered to length 5–25 tokens and used to train and evaluate models.",
            "citation_title": "Microsoft COCO: common objects in context",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / Vision-language",
            "theory_type": null,
            "evaluation_method_name": "COCO Captions dataset (evaluation corpus)",
            "evaluation_method_description": "Used as real-data corpus: split into train/validation/test; generated samples compared to test set using metrics (BLEU, MS-Jaccard, FBD, Self-BLEU, NLL).",
            "evaluation_metric": "Metrics applied on models trained on COCO: BLEU, Self-BLEU, MS-Jaccard, FBD, NLL",
            "metric_definition": null,
            "dataset_or_benchmark": "COCO Captions (524,225 sentences filtered; train/val/test splits used in experiments)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Tables and figures show per-model metrics on COCO (Table 1, Fig.1a, Fig.2).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Used as unconditional text corpus; captions differ stylistically from other text genres which can affect generalization.",
            "uuid": "e8005.14",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "EMNLP2017 WMT News",
            "name_full": "EMNLP2017 WMT News (English corpus)",
            "brief_description": "A news-text corpus used as an unconditional text dataset after filtering (sentence length 20–40, vocab cutoff), used to train and evaluate generation models.",
            "citation_title": "Findings of the 2017 conference on machine translation (WMT17)",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "EMNLP2017 WMT News dataset (evaluation corpus)",
            "evaluation_method_description": "Filtered English news sentences used as training/validation/test corpus; models evaluated against test set using metrics (BLEU, MS-Jaccard, FBD, etc.).",
            "evaluation_metric": "BLEU, Self-BLEU, MS-Jaccard, FBD, NLL",
            "metric_definition": null,
            "dataset_or_benchmark": "EMNLP2017 WMT News (subsampled to 40k train, 20k val, 20k test in experiments)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Results on this dataset shown in Table 2 and Fig.1b.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Preprocessing (UNK replacements, freq thresholds) affects vocabulary and distribution.",
            "uuid": "e8005.15",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "IMDB",
            "name_full": "IMDB Movie Reviews dataset",
            "brief_description": "A corpus of movie review sentences (first two sentences of each review selected and filtered) used as an unconditional text dataset in experiments.",
            "citation_title": "Learning word vectors for sentiment analysis",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP",
            "theory_type": null,
            "evaluation_method_name": "IMDB Movie Reviews dataset (evaluation corpus)",
            "evaluation_method_description": "Filtered set of sentences from IMDB reviews used to train/evaluate text generation models with the suite of metrics.",
            "evaluation_metric": "BLEU, Self-BLEU, MS-Jaccard, FBD, NLL",
            "metric_definition": null,
            "dataset_or_benchmark": "IMDB dataset (subsampled to 20k train, 10k val, 10k test in experiments)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Results on IMDB reported in Table 3 and Fig.1c.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Dataset preprocessing (sentence truncation, UNK thresholds) changes the natural distribution.",
            "uuid": "e8005.16",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Oracle synthetic dataset",
            "name_full": "Oracle synthetic dataset (random LSTM generator)",
            "brief_description": "A synthetic dataset generated by a randomly initialized LSTM used as a known 'oracle' distribution for controlled evaluation (probabilities available), enabling Oracle-NLL and Bhattacharyya evaluation.",
            "citation_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP (synthetic evaluation)",
            "theory_type": null,
            "evaluation_method_name": "Oracle synthetic dataset (probabilistic oracle)",
            "evaluation_method_description": "Use a known LSTM generator (hidden=32, embed=3200, vocab=5000, seq length=20) to produce training/validation/test samples; because oracle probabilities are available, symmetric distribution distances (Bhattacharyya) can be computed.",
            "evaluation_metric": "Oracle-NLL, Bhattacharyya distance, NLL",
            "metric_definition": null,
            "dataset_or_benchmark": "Oracle dataset (100k generated samples; 50k train, 25k val, 25k test)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 4 reports Oracle-NLL and Bhattacharyya results; used to validate proposed distributional metric in oracle mode.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Synthetic oracle does not mirror real-world distributions; useful for controlled comparisons but limited ecological validity.",
            "uuid": "e8005.17",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation",
            "rating": 2
        },
        {
            "paper_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "rating": 2
        },
        {
            "paper_title": "Fréchet Inception Distance",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Texygen: A benchmarking platform for text generation models",
            "rating": 2
        },
        {
            "paper_title": "Language gans falling short",
            "rating": 2
        },
        {
            "paper_title": "Adversarial feature matching for text generation",
            "rating": 1
        },
        {
            "paper_title": "Maximum-likelihood augmented discrete generative adversarial networks",
            "rating": 1
        }
    ],
    "cost": 0.01989575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Jointly Measuring Diversity and Quality in Text Generation Models</h1>
<p>Ehsan Montahaei*<br>Sharif University of<br>Technology / Tehran, Iran<br>ehsan.montahaei@gmail.com</p>
<p>Danial Alihosseini*<br>Sharif University of<br>Technology / Tehran, Iran<br>dalihosseini@ce.sharif.edu</p>
<p>Mahdieh Soleymani Baghshah<br>Sharif University of<br>Technology / Tehran, Iran<br>soleymani@sharif.edu</p>
<h4>Abstract</h4>
<p>Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
<h2>1 Introduction</h2>
<p>Generative models and especially Generative Adversarial Networks (GANs) have been received much attention in the last few years. However, the evaluation of generated samples by these models is challenging. Although some studies have recently focused on introducing measures like Inception</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Score and Fréchet Inception Distance (FID) to compare results of different GAN models for image generation, there is not a study to propose proper metrics for evaluation of text generation models. In the last few years, many GAN-based text generation models (Yu et al., 2017; Lin et al., 2017; Che et al., 2017; Guo et al., 2018; Zhang et al., 2017) have been proposed. However, measuring the performance of these models in the corresponding papers is not comprehensive. GANs suffer from the mode collapse problem (Metz et al., 2016) and the GAN-based text generation models may just produce a highly limited set of sentences and therefore just considering the quality of these generated sentences for comparison is not comprehensive.</p>
<p>On the other hand, there are measures like SelfBLEU (Zhang et al., 2017) for evaluating the diversity of generated sentences, but they can not consider the quality of samples at all. Besides, designing an experiment of evaluating diversity by humans is not straightforward and thus it's necessary to have a jointly quality-diversity measuring metric.</p>
<p>In this paper, we intend to propose metrics sensitive to both quality and diversity simultaneously, assigning low scores not only to models generating low-quality samples but also to the ones with low-diversity samples (including the mode collapsed models). To this end, we first propose the MS-Jaccard as an n-gram based measure that considers the quality and diversity of generated samples simultaneously. It attempts to find the similarity of the set of generated samples by a model and the set of real (or test) samples. Then, a featurebased measure is proposed to compare the real data distribution and the generative model distribution in the feature space. Indeed, by borrowing the idea of FID (Heusel et al., 2017) that is a popular feature-based evaluation metric in im-</p>
<p>age generation tasks and advent of a recent highly deep model named BERT (Devlin et al., 2018) as a reference feature extractor for natural language texts, a metric is proposed for evaluation of natural language generation. Finally, appropriate divergences between the oracle distribution and the (learned) model distribution is introduced for when the probabilistic oracle is considered as synthetic data distribution (and thus the target distribution is available for evaluation).</p>
<h2>2 Text Generation Models</h2>
<p>The neural models on text generation first used LSTMs and trained them by the Maximum Likelihood Estimation (MLE) via teacher forcing (Hochreiter and Schmidhuber, 1997). These models suffer from the exposure bias problem which is due to the train-test discrepancy. Although some solutions such as scheduled sampling were introduced to overcome the exposure bias problem, it has been shown that they are incompatible with the language nature (Bengio et al., 2015; Huszar, 2015). By introducing GANs (Goodfellow et al., 2014) as successful image generation models, it has gained much attention to propose GAN-based text generation models. However, the discrete nature of text needs the generator with discrete outputs that makes passing the gradient from the discriminator to the generator difficult. SeqGAN (Yu et al., 2017) alleviates this difficulty by a gradient policy approach using a REINFORCE-like method to train the generator as a stochastic policy. This method has some difficulties such as reward sparsity and high variance for large action spaces. Subsequent methods try to pass more informative signal from the discriminator to the generator. RankGAN(Lin et al., 2017) trains the discriminator as a ranker which assigns a higher score to the more realistic sequences (in comparison with other sentences in the current batch). LeakGAN (Guo et al., 2018) takes advantage of the feudal networks and considers the discriminator as a manager and the generator as a worker while the feature layer of the discriminator is fed to the generator as leaked information. MaliGAN (Che et al., 2017) attempts to redefine the generator's objective. It minimizes KL divergence between the generator and the real distribution which is obtained by the discriminator in the optimality assumption of the discriminator. This new objective leads to an importance sampling procedure.</p>
<p>TextGAN (Zhang et al., 2017) also applies a new objective for the generator. It tries to push the generator focus from the last layer of the discriminator to its last feature layer. Real data and generator samples will each have some distribution in the feature layer of the discriminator. The generator's objective is to make them closer by Maximum Mean Discrepancy (MMD) metric.</p>
<h2>3 Metrics</h2>
<p>In this section, we first indicate the main difficulties of the existing measures for evaluation of text generation models. Then, we introduce metrics that evaluate the capability of the models in generating both right sentences and various ones. The proposed metrics (that are all symmetric) jointly specify to what extent probable sentences in real data are likely in the generative model and also the probable sentences in the model are likely in the real data.</p>
<h3>3.1 Shortcomings of the existing metrics</h3>
<p>In this section, shortcomings of the metrics that either evaluate the quality or the diversity of generated samples are presented. Moreover, a recent attempt to simultaneously considering these metrics is introduced.</p>
<h3>3.1.1 Quality metrics</h3>
<p>BLEU: It is the most widely used metric for text generation. Originally BLEU (Papineni et al., 2002) is a metric to evaluate the quality of machine-translated text. In unconditional text generation, all sentences in the test set are considered as the reference set and generated sentences are evaluated by computing their average BLEU score on this reference set. In conditional text generation tasks like machine translation which include a limited reference set (for each condition), computing the similarity of the generated text and the reference set may be sensible. However, the reference set for the unconditional text generation task is whole available sentences and measures like BLEU just consider the validity of generated sentences without measuring what proportion of the reference sentences can be covered by the text generation model. On the other hand, GAN-based text generation models may generate a highly limited set of sentences and sacrifice the diversity (due to the mode collapse problem). Therefore, evaluating these models using BLEU score just</p>
<p>shows the validity of their outputs without considering their coverage.</p>
<p>Oracle-NLL: It was introduced by SeqGAN (Yu et al., 2017) and is based on assuming a synthetic oracle distribution. It considers a random distribution as the real distribution (or the oracle) and the training dataset is prepared by sampling from this distribution. The score is defined to be the Negative Log Likelihood (NLL) of the generated samples from the trained model in the oracle distribution. In this measure, the coverage is again neglected and a model that generates only one high quality sentence can reach high performance.</p>
<h3>3.1.2 Diversity metric</h3>
<p>As mentioned above, BLUE and Oracle-NLL just consider the quality of the generated samples and ignore their diversity. Below, we introduce two metrics measuring the diversity. However, these metrics evaluate only diversity and don't consider the quality of samples at all.</p>
<p>Self-BLEU: In (Zhu et al., 2018), Self-BLEU was introduced to evaluate just variety of sentences. It measures BLEU score for each generated sentence by considering other generated sentences as reference. By averaging these BLEU scores (obtained for generated sentences), a metric that is called Self-BLEU is achieved where its lower values shows more diversity.</p>
<p>Entropy: On the other side, we can use the entropy of probabilistic generative model to measure the diversity where the lower values show lower diversity. As the direct calculation of the entropy is not feasible, a Monte-Carlo estimation of it can be used.</p>
<h3>3.1.3 Quality and diversity</h3>
<p>Recently (Caccia et al., 2018) mentioned the flaws of only evaluating the quality and found that MLE outperforms the GAN variants for text generation since it dominates GANs in the quality-diversity space. (Caccia et al., 2018) uses the qualitydiversity spectrum obtained by changing the temperature parameter that controls entropy of the models' conditional distributions. However, it does not provide a measure to assess both the quality and the diversity without needing to inspect the whole quality-diversity spectrum.</p>
<p>Likelihood: Although the likelihood of a generative model on real (test) data evaluates the ability of the model in generating the test samples, it doesn't measure the quality of the whole set of
generated texts by the model. In fact, a model with a low NLL value on test data (or equivalently a model in which the likelihood of the test data is high) may also assign high probability to many other sentences that are not valid or qualified. Specifically for sequence models, the likelihood doesn't assess the free-running mode of models. To be more detailed, most of the probabilistic sequence models, decompose the joint distribution to conditional distributions using the chain rule. These conditional distributions are the probability of each token conditioned on the prior tokens. Thus, in the likelihood evaluation, each of token's probability is conditioned on a prefix that is a real sequence itself and the likelihood is not assessed on the previously generated tokens of the model during evaluation (it is similar to the exposure bias problem of MLE for sequence generation).</p>
<p>Moreover, measuring a model by its likelihood score has another problem. When a model misses one mode of a multi-modal distribution, its score decreases severely; so it is an unfair metric for comparing MLE method with other methods because MLE method uses likelihood as its objective and has mean seeking behavior (Goodfellow, 2017).</p>
<h3>3.2 Proposed metrics</h3>
<p>In this section, we propose metrics that simultaneously considers the quality and the diversity of the generated samples. To this end, we compare the real distribution of texts with the obtained distribution by the text generation model.</p>
<h3>3.2.1 MS-Jaccard</h3>
<p>We first propose a metric that finds the similarity of the generative model and the real distribution by comparing text samples generated by them. To this end, n-grams of generated samples and those of real samples are considered as two multi-sets (that also preserve repetition of n-grams) and the similarity of the resulted multi-sets is computed. In simple words, the MS-Jaccard focuses on the similarity of the n-grams frequencies in the two sets and inspired by the well-known Jaccard Index which determines the similarity of two sets as the ratio of the cardinality of their intersection to that of their union.</p>
<p>To define it formally, let $S_{1}$ and $S_{2}$ be two sets of sentences, $G_{n}$ be the set of n-grams in $S_{1} \cup S_{2}$, and $C_{n}(g, S)$ be the normalized counts of the n gram $g$ in the set $S$. The similarity between n -</p>
<p>grams of two sets $S_{1}$ and $S_{2}$ is defined as:</p>
<p>$$
\operatorname{score}<em G__n="G_{n" _in="\in" g="g">{n}=\frac{\sum</em>
$$}} \min \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}}{\sum_{g \in G_{n}} \max \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}</p>
<p>The geometric mean of the $\left{\operatorname{score}<em n="1">{n}\right}</em>(g, S)$ will denotes the average frequency per sentence for n -gram $g$ in the set $S$. If the generated sentences won't have diversity or quality, the n-gram distribution of generated texts will be different from that of the real texts and causing to decrease the MS-Jaccard score consequently. As it is obvious, the MS-Jaccard is a similarity measure and so its higher value will be better.}^{N}$ will be the MS-Jaccard score called MS-Jaccard- $N$ where the $N$ is the maximum length of $n$-grams. It is worth noting that the frequencies of the n-grams in each set is normalized with respect to the total number of sentences in the set (to avoid diminishing the score when the size of only one of these sets grows). Thus, the $C_{n</p>
<h3>3.2.2 Fréchet BERT Distance (FBD)</h3>
<p>One popular metric for evaluation of image generation models is FID introduced in (Heusel et al., 2017). Each of real and generated images in a feature space (found by Inception network) is modeled by a Gaussian distribution, and the FID is defined as the Fréchet distance between these two Gaussian distributions. We want to introduce a similar measure for the text generation task. To this end, we utilize BERT (Devlin et al., 2018) that provides a proper feature space for texts. We use Fréchet distance in BERT's feature space as a metric that considers quality and variety of generated sentences, and name it Fréchet BERT Distance (FBD). There is a set of pooled features (for classification task) in the BERT network that has a constant size for different input sequence lengths; we used these features for FBD. The Fréchet distance is also known as Wasserstein-2 divergence, and this distance between two Gaussian distribution is as follows:</p>
<p>$$
\sqrt{\left|m_{1}-m_{2}\right|<em 1="1">{2}^{2}+\operatorname{Tr}\left(C</em>
$$}+C_{2}-2\left(C_{1} C_{2}\right)^{1 / 2}\right)</p>
<p>where $m_{i}$ and $C_{i}$ show the mean vector and the covariance matrix of these Gaussians respectively. It should be noted as the FBD is a distance measure, its lower values will be better.</p>
<h3>3.2.3 Oracle Based Evaluation</h3>
<p>In Oracle-NLL evaluation introduced in (Yu et al., 2017), the measured distance is Kullback-Leibler (KL) divergence of the generative model and the oracle which ignores the variety of generated sentences. On the other hand, the inverse KL (that is relevant to the likelihood of real data in the text generation model) can not guarantee the quality of generated samples by the model. We propose measuring the distance of the probabilistic oracle distribution $P$ (that generates real data) and the probabilistic generative model $Q$ by a symmetric distance as an evaluation metric. A wide range of distances can be utilized for this purpose. One symmetric distance is Bhattacharyya that can be estimated by the Monte-Carlo as below:</p>
<p>$$
\begin{aligned}
&amp; B(P, Q)= \
&amp; \quad \frac{-1}{2}\left(\ln \frac{1}{N} \sum_{i=0}^{N} \sqrt{\frac{q\left(x_{i}\right)}{p\left(x_{i}\right)}}+\ln \frac{1}{M} \sum_{j=0}^{M} \sqrt{\frac{p\left(x_{j}\right)}{q\left(x_{j}\right)}}\right)
\end{aligned}
$$</p>
<p>where $\left{x_{i}\right}$ and $\left{x_{j}\right}$ are sets of samples from $P$ and $Q$ distributions respectively. Similar to the FBD, Bhattacharyya is also a distance measure and thus its lower values are better.</p>
<h2>4 Evaluation</h2>
<p>In this section, we first conduct some experiments to evaluate text generation models using the existing and the proposed measures. Then, we discuss about the appropriateness of the proposed metrics.</p>
<h3>4.1 Datasets</h3>
<p>We evaluate the models on COCO image captions (Lin et al., 2014), EMNLP2017 WMT News (Bojar et al., 2017), and IMDB (Maas et al., 2011) as the popular datasets for text generation. In addition to these datasets, similar to (Yu et al., 2017; Lin et al., 2017; Guo et al., 2018), we also consider a synthetic oracle produced by a probabilistic text generator that is a random initialized LSTM as a synthetic dataset. The description of the datasets is as follows:</p>
<ul>
<li>COCO Captions (Lin et al., 2014): It is a collection of image captions containing around 600,000 captions. Sentences having between 5 and 25 words are selected (resulting in 524,225 sentences) where 5,328 is the vocab size of the resulted dataset. Among the resulted dataset, 40,000 samples are used for</li>
</ul>
<p>training, 20,000 samples for validation, and 20,000 for test.</p>
<ul>
<li>EMNLP2017 WMT News (Bojar et al., 2017): It is a collection of news texts for the machine translations task ${ }^{1}$. Among a version of this dataset for English corpus containing 500,000 sentences, sentences having more than 3 words with less than 150 frequency (these words are replaced with UNK) were dropped and sentences that have between 20 and 40 words selected. The vocab size of the resulted dataset is 6,148 . Among this dataset, 40,000 samples are used for training, 20,000 samples for validation, and 20,000 for test.</li>
<li>IMDB Movie Reviews (Maas et al., 2011): It is a collection of IMDB movie reviews for the sentiment analysis task, containing 25,000 labeled and 50,000 unlabeled ones. We have selected the first two sentences of each review and replace words with less that 50 times frequency with UNK and keep sentences from length 5 to 40 with less than 5 UNKs. The final dataset is subsampled to have 20,000 sentences for training data, 10,000 for validation, and 10,000 for test data leading to vocab size of 5,810 .</li>
<li>Oracle synthetic dataset (Yu et al., 2017): A randomly initialized LSTM generator as a real distribution used in oracle training mode; the network implementation is borrowed from the SeqGAN released code ${ }^{2}$. This network's hidden size is 32 and its embedding size is 3,200. Moreover, the vocab size is 5,000 and the length of sequences is 20. The dataset of 100,000 samples are generated according to the above model. Among this dataset, 50,000 samples are used for training, 25,000 for validation, and 25,000 for test.</li>
</ul>
<h3>4.2 Experimental Setup</h3>
<h3>4.2.1 Text Generation Models</h3>
<p>As the recent methods for text generation, we evaluate SeqGAN (Yu et al., 2017), RankGAN (Lin et al., 2017), and MaliGAN (Che et al., 2017). We also consider vanilla Maximum Likelihood Estimation (MLE) language model using LSTM as the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>baseline method. We used the implementation of the above methods in the Texygen platform (Zhu et al., 2018) and train them in this framework ${ }^{3}$. The models were trained on the similar dataset existing in their released code but collected from the original sites reported in corresponding reference papers.</p>
<p>In order to have a fair comparison, all settings of the models (e.g., same hidden) were kept the same as the Texygen framework. Since setting a fixed number of epochs for terminating training of different methods does not seem such reasonable and resulting in unfair scores, we targeted multiple training termination criteria. In the realworld datasets training, the training termination of the GANs were based on obtaining the best BLEU4 on validation data in addition to setting a max number of iterations for all the models. Besides, the training termination of MLE is based the NLL on the validation data while also setting a max number of iterations as above. In the oracle training mode, the termination were done based on both Oracle-NLL on the validation set and again on a max number of iterations for all models.</p>
<h3>4.2.2 Metrics</h3>
<p>Among the existing measures, BLEU2 upto BLEU5 (evaluating only quality), Self-BLUE2 upto Self-BLEU5 (evaluating only diversity), and NLL that shows the negative log likelihood of the model on test data are utilized for real datasets. Moreover, due to the low performance of the Python NLTK (Bird et al., 2009) BLEU library ${ }^{4}$ when needing to evaluate multiple sentences with a fixed reference set, we have re-implemented it to achieve parallel computation and high performance ${ }^{5}$.</p>
<p>Among the proposed measures, MS-Jaccard2 upto MS-Jaccard5 and FBD are assayed on realworld datasets. For synthetic oracle, NLL and Oracle-NLL as the existing measures and the proposed measure for comparing distributions, i.e. Bhattacharyya, are evaluated. It should be noted that, in order to make the metric's directions the same (i.e. their lower values show better performance), the $1-$ MS-Jaccard, $1-$ BLEU and $-1 \times$ Entropy is used in some plots.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 1: Performance of models (using different measures) on COCO Captions dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 1 6}$</td>
<td style="text-align: center;">1.971</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 8}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">55.610</td>
<td style="text-align: center;">4.590</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 0}$</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.545</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">39.916</td>
<td style="text-align: center;">$\mathbf{1 . 4 7 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 1}$</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.288</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">48.816</td>
<td style="text-align: center;">3.574</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 2}$</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.402</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of models (using different measures) on EMNLP2017 WMT News dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.133</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 3 . 2 4 6}$</td>
<td style="text-align: center;">$\mathbf{4 . 8 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 1}$</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">195.867</td>
<td style="text-align: center;">5.955</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.324</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">163.931</td>
<td style="text-align: center;">5.690</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 1}$</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">177.346</td>
<td style="text-align: center;">5.104</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.224</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of models (using different measures) on IMDB Movie Reviews dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 2 5 . 2 2 3}$</td>
<td style="text-align: center;">$\mathbf{3 . 5 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 5}$</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 9}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">150.213</td>
<td style="text-align: center;">4.587</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 4}$</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.345</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">141.558</td>
<td style="text-align: center;">4.482</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 3}$</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.290</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">151.828</td>
<td style="text-align: center;">3.958</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.331</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of models (using different measures) on Oracle dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">Oracle-NLL</th>
<th style="text-align: center;">Bhattacharyya</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 1 . 9 4 8}$</td>
<td style="text-align: center;">167.014</td>
<td style="text-align: center;">$\mathbf{7 . 1 0 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">155.353</td>
<td style="text-align: center;">$\mathbf{1 6 3 . 1 7 9}$</td>
<td style="text-align: center;">10.076</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">146.260</td>
<td style="text-align: center;">168.054</td>
<td style="text-align: center;">8.503</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">160.424</td>
<td style="text-align: center;">166.774</td>
<td style="text-align: center;">12.127</td>
</tr>
</tbody>
</table>
<h3>4.3 Results</h3>
<p>Results of different methods on COCO Captions, EMNLP2017 WMT News, and IMDB datasets as real-world datasets are shown in Tables 1, 2, and 3 , respectively. To provide a target, we have also shown metrics for training data themselves and called the method as Real (indeed training data is considered as the generated data by Real and the measures are computed on them). These tables show that MLE has the best performance according to the proposed measures considering both quality and diversity of samples. In fact, GANbased methods can not generally achieve good performance according to the proposed measures. This result is consistent with the reported results in (Caccia et al., 2018) that compares GANs and MLE for text generation.</p>
<p>Table 4 shows results of different methods on synthetic oracle dataset and MLE again shows the best results according to the proposed metric (that approximates the distance of the real distribution and the generative model distribution).</p>
<p>As mentioned in Section 3.1.3 about (Caccia et al., 2018), the whole spectrum of qualitydiversity is considered for evaluation of Natural Language Generation (NLG) methods. In fact, in (Caccia et al., 2018), the temperature sweep is utilized to robustly evaluate text generation methods. More precisely, the generators conditional distribution $G\left(x_{t} \mid x_{1: t-1}\right)$ is defined as $\operatorname{Softmax}\left(o_{t} / T\right)$ where $o_{t}$ denotes the logit at time $t$. Decreasing $T$ below 1.0 will decrease the entropy of conditional probability and thus reduce the probability of generating low quality samples. On the other hand, increasing this temperature above 1.0 will upraise the entropy of the conditional distribution and thus improve the diversity of the generated samples (Caccia et al., 2018).</p>
<p>We intend to show that the proposed metrics are correlated with the analysis of the whole space of quality-diversity obtained by changing the temperature. In fact, using the proposed metrics we can</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diversity vs. quality measure of various models with temperatures from $1.5^{-3}$ to $1.5^{4}$ on different datasets. Each point in the plot corresponds to the performance of a model in a special temperature (A seconddegree polynomial has been fitted to the points). Lower values in both axes show better ones.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: NLL, 1-MS-Jaccard4, and FBD scores of all the models without applying temperature (i.e. $T=1$ ) on different datasets. Lower values show better performance.
usually predict the behavior of the model in whole spectrum without needing to provide this qualitydiversity space.</p>
<p>Fig. 1 shows the diversity against quality
measures with different values of temperature. Figs. 1a, 1b, and 1c consider Self-BLEU4 as diversity and BLEU4 as quality measure for each of the methods on real-world COCO, EMNLP2017,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance of all models (without applying temperature, i.e. $T=1$ ) on the Oracle dataset using different measures. Lower values show better performance.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pearson correlation of all metrics when aggregating results on the real world text datasets and all temperatures.
and IMDB datasets. The metrics are also evaluated on the train data itself which is called Real in the mentioned figures. Moreover, for Oracle dataset, since we have the probabilistic distribution of data, we can compute the likelihood of the generated samples by the model in the real distribution (i.e. Oracle) to find the quality of the generated samples. Therefore, the Oracle-NLL is used as quality measure of the methods on the synthetic dataset in Fig. 1d and Entropy is used as a diversity measure in this figure.</p>
<p>On the other hand, Figs. 2 and 3 present the performance of different methods (with $T=1$ ) on non-synthetic and synthetic datasets respectively.</p>
<p>It is worth noting that NLL, Entropy, and Bhattacharyya of Real could not be computed, since we do not have a model for real data and just considering training data as its samples. According to Fig. 2b, the ordering of the methods obtained by MSJaccard4 on these datasets is almost always consistent with the ordering of the methods according to their dominance in Figs. 1a to 1c. For example, in Fig. 1b that shows results on EMNLP2017 dataset, the best method which dominates others is MLE, the second best is MaliGAN, the third one is RankGAN, and SeqGAN is the last one that under-performs all other methods. Consistently, the proposed MS-Jaccard4 measure shown in Fig. 2b provides the same ordering. Moreover, the ordering of the methods according to FBD metric in Fig. 2c on different datasets is almost always consistent with their ordering obtained by analyzing the whole spectrum in Figs. 1a to 1c. For the oracle dataset 3, the proposed Bhattacharyya distance of the distributions introduced in Section 3.2.3 is consistent with the ordering obtained in Fig. 1d.</p>
<p>Finally, we display the Pearson correlation of different metrics on real datasets in Fig. 4. According to this figure, the proposed metrics for real-world datasets, i.e. 1-MS-Jaccard and FBD, are highly correlated. Besides, among the measures, these are the most correlated ones to NLL.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we first discussed shortcomings of the existing measures for evaluating text generation models. Then, we proposed some measures to more effectively specify the capability of models in generating both qualified and diverse texts. The MS-Jaccard as an n-gram based metric was firstly introduced that is capable of measuring both the quality and coverage of methods in text generation. Then, a feature-based metric FBD which is based on the BERT model was introduced. Moreover, for oracle training mode in which the generators density can also be calculated, we proposed to use (estimation of) divergences like Bhattacharyya defined on probability distributions as a metric to compute the distance of the generative model and the oracle. Finally, the performance of different text generation models were evaluated, the obtained results were analyzed and showed that the proposed metrics have high correlations and are almost consistent with the dominance ordering of models in quality-diversity spectrum.</p>
<h2>References</h2>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171-1179.</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O'Reilly.</p>
<p>Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 78, 2017, pages 169-214. Association for Computational Linguistics.</p>
<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. CoRR, abs/1811.02549.</p>
<p>Tong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. CoRR, abs/1702.07983.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Ian J. Goodfellow. 2017. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160.</p>
<p>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative adversarial networks. CoRR, abs/1406.2661.</p>
<p>Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press.</p>
<p>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626-6637.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, $9(8): 1735-1780$.</p>
<p>Ferenc Huszar. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101.</p>
<p>Kevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, and Zhengyou Zhang. 2017. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 49 December 2017, Long Beach, CA, USA, pages 3158-3168.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. In Computer Vision ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740-755. Springer.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142-150. The Association for Computer Linguistics.</p>
<p>Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. 2016. Unrolled generative adversarial networks. CoRR, abs/1611.02163.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311-318. ACL.</p>
<p>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 2852-2858. AAAI Press.</p>
<p>Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. 2017. Adversarial feature matching for text generation. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 4006-4015. PMLR.</p>
<p>Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. SIGIR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://statmt.org/wmt17/translation-task.html
${ }^{2}$ https://github.com/LantaoYu/SeqGAN/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://github.com/geek-ai/Texygen
${ }^{4}$ https://www.nltk.org/ modules/nltk/
${ }^{5}$ https://github.com/Danial-Alh/FastBLEU&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
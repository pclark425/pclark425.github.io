<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7376 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7376</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7376</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-270357822</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.04687v1.pdf" target="_blank">LogiCode: an LLM-Driven Framework for Logical Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond traditional focus on structural inconsistencies. By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies. A custom dataset"LOCO-Annotations"and a benchmark"LogiBench"are introduced to evaluate the LogiCode's performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning. Findings demonstrate LogiCode's enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies. This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7376.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7376.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiCode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiCode: an LLM-Driven Framework for Logical Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses an LLM (GPT-4) to parse expert-defined logical rules into executable Python code which inspects visual features (via an Image API) to detect logical anomalies and produce human-readable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large transformer used for text-to-code generation (prompted to synthesize Python implementing logical rules and API calls).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>LLM-driven code generation: prompt the LLM to translate logical rules into executable Python that calls visual APIs to extract object attributes and evaluate rule-based logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Task description + 'Create a Python function utilizing the Image class to analyze an image and return (boolean, string reasons). Outline subtasks, include inputs/outputs, use provided logical rules and select Image APIs.' (engineered request prompt including Task Interpretation, Function Structuring, Knowledge Integration, and subtask breakdown).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>The framework is evaluated using LOCO-Annotations (1772 training images); the LLM itself was not fine-tuned on this dataset (no supervised fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Industrial images with pixel-level object segmentation; logical relations among objects (object lists/attributes extracted via visual APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LOCO-Annotations (extension of MVTec LOCO; 2,908 images total: 1,772 training, 1,136 testing), plus references to MVTec LOCO.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary classification metrics (accuracy, precision, recall, F1), code generation success rate (fraction of generated codes that execute and implement rules), and reasoning accuracy (human expert evaluation and LLM automatic evaluation comparing generated reasons to ground-truth reason labels).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average binary-classification: Accuracy 0.989, Precision 0.976, Recall 1.000, F1 0.988. Code generation success rate (average) 0.590. Reasoning accuracy: Human average 0.963, LLM automatic average 0.924.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against several vision-based SOTA methods (reported average accuracies): STP (0.665), EFMR (0.683), RD4AD (0.680), SPADE (0.684), GLCF (0.789), EfficientAD-S (0.841), EfficientAD-M (0.852). LogiCode average accuracy 0.989.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot prompting / prompt-engineering (LLM prompted to synthesize code from provided logical rules and examples; no LLM fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM code generation instability (~60% success rate reported), cases of syntax errors or missing functionality; occasional incorrect reasoning/explanations (mitigated by human evaluation); reliance on expert-defined logical rules and annotated APIs; potential weakness in niche industrial scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogiCode: an LLM-Driven Framework for Logical Anomaly Detection', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7376.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7376.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large OpenAI language model used in this work as the code-generation and reasoning engine to translate logical rules into executable Python and to evaluate generated explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large transformer (text-to-code capability) used to parse logical rules, select APIs, and synthesize Python code for visual logical checks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompted code-generation that produces deterministic rule-checking code executed against image-derived features (rule-based evaluation implemented via generated Python).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>See LogiCode: structured 'Request Prompt' specifying task, function signature, required outputs (boolean + explanation), subtask breakdown, and domain rules to be parsed into code.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>No additional fine-tuning reported; GPT-4 used via prompting. Evaluation uses LOCO-Annotations for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Visual imagery (objects and their attributes extracted by visual APIs); not tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LOCO-Annotations (used to evaluate the GPT-4-driven LogiCode pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary classification metrics (accuracy/precision/recall/F1), code-generation success rate, reasoning accuracy (human + LLM automatic evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When used in LogiCode: average detection accuracy 0.989, average code-generation success 0.590, reasoning accuracy (LLM auto) 0.924 and human 0.963.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against multiple vision-only SOTA methods (see LogiCode entry); real comparison was LogiCode (GPT-4) vs these methods on LOCO-Annotations logical anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot / prompt-engineered (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Code instability (syntax errors, missing functionality) in ~40% of generations; occasional incorrect explanations; dependency on prompt quality and expert-defined rules.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogiCode: an LLM-Driven Framework for Logical Anomaly Detection', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7376.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7376.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnomalyGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AnomalyGPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work that applies large vision-language models to industrial anomaly detection by aligning images with textual descriptions to improve detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AnomalyGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as an LLM-based industrial anomaly detection approach that aligns images with text; detailed architecture not provided in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Vision-language alignment (paper citation indicates image–text alignment to improve anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Images (vision-language setting)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as related work; no experimental details or failure modes provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogiCode: an LLM-Driven Framework for Logical Anomaly Detection', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7376.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7376.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIPERGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViperGPT (Visual inference via python execution for reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited method that integrates LLM-driven code generation (Python execution) for visual question answering and compositional reasoning; referenced as inspiration for LogiCode's code-generation + execution paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIPERGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system that uses LLMs to generate Python code to perform visual reasoning via execution; details are in the cited work (paper reference).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Code-generation + execution for visual reasoning (cited as VQA/compositional reasoning work, not applied directly to tabular data here).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Images / visual queries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as prior work motivating the approach; no detailed failure cases provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogiCode: an LLM-Driven Framework for Logical Anomaly Detection', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7376.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7376.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Industrial-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Industrial-GPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a domain-specific LLM tailored for intelligent manufacturing tasks such as fault diagnosis; cited as related industrial application of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Industrial-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A domain-adapted LLM aimed at manufacturing/fault-diagnosis tasks (reference only; details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>LLM-based automation/fault diagnosis in industrial settings (cited; not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as prior work with domain specialization; no experimental specifics given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogiCode: an LLM-Driven Framework for Logical Anomaly Detection', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Anomalygpt: Detecting industrial anomalies using large visionlanguage models <em>(Rating: 2)</em></li>
                <li>Vipergpt: Visual inference via python execution for reasoning <em>(Rating: 2)</em></li>
                <li>Exploring grounding potential of vqa-oriented gpt-4v for zeroshot anomaly detection <em>(Rating: 2)</em></li>
                <li>Industrial-generative pretrained transformer for intelligent manufacturing systems <em>(Rating: 2)</em></li>
                <li>Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7376",
    "paper_id": "paper-270357822",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "LogiCode",
            "name_full": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
            "brief_description": "A framework that uses an LLM (GPT-4) to parse expert-defined logical rules into executable Python code which inspects visual features (via an Image API) to detect logical anomalies and produce human-readable explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large transformer used for text-to-code generation (prompted to synthesize Python implementing logical rules and API calls).",
            "model_size": null,
            "anomaly_detection_approach": "LLM-driven code generation: prompt the LLM to translate logical rules into executable Python that calls visual APIs to extract object attributes and evaluate rule-based logical constraints.",
            "prompt_template": "Task description + 'Create a Python function utilizing the Image class to analyze an image and return (boolean, string reasons). Outline subtasks, include inputs/outputs, use provided logical rules and select Image APIs.' (engineered request prompt including Task Interpretation, Function Structuring, Knowledge Integration, and subtask breakdown).",
            "training_data": "The framework is evaluated using LOCO-Annotations (1772 training images); the LLM itself was not fine-tuned on this dataset (no supervised fine-tuning reported).",
            "data_type": "Industrial images with pixel-level object segmentation; logical relations among objects (object lists/attributes extracted via visual APIs).",
            "dataset_name": "LOCO-Annotations (extension of MVTec LOCO; 2,908 images total: 1,772 training, 1,136 testing), plus references to MVTec LOCO.",
            "evaluation_metric": "Binary classification metrics (accuracy, precision, recall, F1), code generation success rate (fraction of generated codes that execute and implement rules), and reasoning accuracy (human expert evaluation and LLM automatic evaluation comparing generated reasons to ground-truth reason labels).",
            "performance": "Average binary-classification: Accuracy 0.989, Precision 0.976, Recall 1.000, F1 0.988. Code generation success rate (average) 0.590. Reasoning accuracy: Human average 0.963, LLM automatic average 0.924.",
            "baseline_comparison": "Compared against several vision-based SOTA methods (reported average accuracies): STP (0.665), EFMR (0.683), RD4AD (0.680), SPADE (0.684), GLCF (0.789), EfficientAD-S (0.841), EfficientAD-M (0.852). LogiCode average accuracy 0.989.",
            "zero_shot_or_few_shot": "Zero-shot prompting / prompt-engineering (LLM prompted to synthesize code from provided logical rules and examples; no LLM fine-tuning reported).",
            "limitations_or_failure_cases": "LLM code generation instability (~60% success rate reported), cases of syntax errors or missing functionality; occasional incorrect reasoning/explanations (mitigated by human evaluation); reliance on expert-defined logical rules and annotated APIs; potential weakness in niche industrial scenarios.",
            "computational_cost": null,
            "uuid": "e7376.0",
            "source_info": {
                "paper_title": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "The large OpenAI language model used in this work as the code-generation and reasoning engine to translate logical rules into executable Python and to evaluate generated explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large transformer (text-to-code capability) used to parse logical rules, select APIs, and synthesize Python code for visual logical checks.",
            "model_size": null,
            "anomaly_detection_approach": "Prompted code-generation that produces deterministic rule-checking code executed against image-derived features (rule-based evaluation implemented via generated Python).",
            "prompt_template": "See LogiCode: structured 'Request Prompt' specifying task, function signature, required outputs (boolean + explanation), subtask breakdown, and domain rules to be parsed into code.",
            "training_data": "No additional fine-tuning reported; GPT-4 used via prompting. Evaluation uses LOCO-Annotations for testing.",
            "data_type": "Visual imagery (objects and their attributes extracted by visual APIs); not tabular data.",
            "dataset_name": "LOCO-Annotations (used to evaluate the GPT-4-driven LogiCode pipeline).",
            "evaluation_metric": "Binary classification metrics (accuracy/precision/recall/F1), code-generation success rate, reasoning accuracy (human + LLM automatic evaluation).",
            "performance": "When used in LogiCode: average detection accuracy 0.989, average code-generation success 0.590, reasoning accuracy (LLM auto) 0.924 and human 0.963.",
            "baseline_comparison": "Compared against multiple vision-only SOTA methods (see LogiCode entry); real comparison was LogiCode (GPT-4) vs these methods on LOCO-Annotations logical anomalies.",
            "zero_shot_or_few_shot": "Zero-shot / prompt-engineered (no fine-tuning).",
            "limitations_or_failure_cases": "Code instability (syntax errors, missing functionality) in ~40% of generations; occasional incorrect explanations; dependency on prompt quality and expert-defined rules.",
            "computational_cost": null,
            "uuid": "e7376.1",
            "source_info": {
                "paper_title": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AnomalyGPT",
            "name_full": "AnomalyGPT (as cited)",
            "brief_description": "Mentioned prior work that applies large vision-language models to industrial anomaly detection by aligning images with textual descriptions to improve detection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AnomalyGPT",
            "model_description": "Described in the paper as an LLM-based industrial anomaly detection approach that aligns images with text; detailed architecture not provided in this paper (citation only).",
            "model_size": null,
            "anomaly_detection_approach": "Vision-language alignment (paper citation indicates image–text alignment to improve anomaly detection).",
            "prompt_template": "",
            "training_data": "",
            "data_type": "Images (vision-language setting)",
            "dataset_name": "",
            "evaluation_metric": "",
            "performance": "",
            "baseline_comparison": "",
            "zero_shot_or_few_shot": "",
            "limitations_or_failure_cases": "Mentioned as related work; no experimental details or failure modes provided in this paper.",
            "computational_cost": null,
            "uuid": "e7376.2",
            "source_info": {
                "paper_title": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "VIPERGPT",
            "name_full": "ViperGPT (Visual inference via python execution for reasoning)",
            "brief_description": "Cited method that integrates LLM-driven code generation (Python execution) for visual question answering and compositional reasoning; referenced as inspiration for LogiCode's code-generation + execution paradigm.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "VIPERGPT",
            "model_description": "A system that uses LLMs to generate Python code to perform visual reasoning via execution; details are in the cited work (paper reference).",
            "model_size": null,
            "anomaly_detection_approach": "Code-generation + execution for visual reasoning (cited as VQA/compositional reasoning work, not applied directly to tabular data here).",
            "prompt_template": "",
            "training_data": "",
            "data_type": "Images / visual queries",
            "dataset_name": "",
            "evaluation_metric": "",
            "performance": "",
            "baseline_comparison": "",
            "zero_shot_or_few_shot": "",
            "limitations_or_failure_cases": "Cited as prior work motivating the approach; no detailed failure cases provided here.",
            "computational_cost": null,
            "uuid": "e7376.3",
            "source_info": {
                "paper_title": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Industrial-GPT",
            "name_full": "Industrial-GPT (as cited)",
            "brief_description": "Mentioned as a domain-specific LLM tailored for intelligent manufacturing tasks such as fault diagnosis; cited as related industrial application of LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Industrial-GPT",
            "model_description": "A domain-adapted LLM aimed at manufacturing/fault-diagnosis tasks (reference only; details in cited work).",
            "model_size": null,
            "anomaly_detection_approach": "LLM-based automation/fault diagnosis in industrial settings (cited; not used here).",
            "prompt_template": "",
            "training_data": "",
            "data_type": "",
            "dataset_name": "",
            "evaluation_metric": "",
            "performance": "",
            "baseline_comparison": "",
            "zero_shot_or_few_shot": "",
            "limitations_or_failure_cases": "Mentioned as prior work with domain specialization; no experimental specifics given in this paper.",
            "computational_cost": null,
            "uuid": "e7376.4",
            "source_info": {
                "paper_title": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Anomalygpt: Detecting industrial anomalies using large visionlanguage models",
            "rating": 2,
            "sanitized_title": "anomalygpt_detecting_industrial_anomalies_using_large_visionlanguage_models"
        },
        {
            "paper_title": "Vipergpt: Visual inference via python execution for reasoning",
            "rating": 2,
            "sanitized_title": "vipergpt_visual_inference_via_python_execution_for_reasoning"
        },
        {
            "paper_title": "Exploring grounding potential of vqa-oriented gpt-4v for zeroshot anomaly detection",
            "rating": 2,
            "sanitized_title": "exploring_grounding_potential_of_vqaoriented_gpt4v_for_zeroshot_anomaly_detection"
        },
        {
            "paper_title": "Industrial-generative pretrained transformer for intelligent manufacturing systems",
            "rating": 2,
            "sanitized_title": "industrialgenerative_pretrained_transformer_for_intelligent_manufacturing_systems"
        },
        {
            "paper_title": "Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead",
            "rating": 2,
            "sanitized_title": "towards_generic_anomaly_detection_and_understanding_largescale_visuallinguistic_model_gpt4v_takes_the_lead"
        }
    ],
    "cost": 0.012995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>7 Jun 2024</p>
<p>Weiming Shen wshen@ieee.com 7 Jun 20246A4FFC760B783B838F68683CE1FAB94FarXiv:2406.04687v1[cs.LG]received April xx, 2023;</p>
<p>LogiCode: an LLM-Driven Framework for Logical Anomaly Detection</p>
<p>Yiheng Zhang, Yunkang Cao, Graduate Student Member, IEEE, Xiaohao Xu, Weiming Shen * , Fellow, IEEE,</p>
<p>Abstract-This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond traditional focus on structural inconsistencies.By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies.A custom dataset "LOCO-Annotations" and a benchmark "LogiBench" are introduced to evaluate the LogiCode's performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning.Findings demonstrate LogiCode's enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies.This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications.</p>
<p>Note to Practitioners-This work introduces LogiCode, an innovative system leveraging Large Language Models (LLMs) for logical anomaly detection in industrial settings, shifting the paradigm from traditional visual inspection methods.Logi-Code autonomously generates Python codes for logical anomaly detection, enhancing interpretability and accuracy.Our novel approach, validated through the "LOCO-Annotations" dataset and LogiBench benchmark, demonstrates superior performance in identifying logical anomalies, a challenge often encountered in complex industrial components like assembly and packaging.LogiCode provides a significant advancement in addressing the nuanced requirements of detecting logical anomalies, offering a robust and interpretable solution to practitioners seeking to enhance quality control and reduce manual inspection efforts.</p>
<p>Index Terms-Logical Anomaly Detection, Large Language Models, Industrial Anomaly Detection, Dataset Annotation.</p>
<p>I. INTRODUCTION</p>
<p>A NOMALY detection in industrial scenarios is pivotal for ensuring the quality and reliability of products [1], [2], [3].Traditionally, this domain has been dominated by methods focusing on structural defects, such as dents or scratches [4], detectable through vision-based techniques.However, these conventional methods often fall short when dealing with logical anomalies [5] -errors in high-level semantic logic, such as incorrect orders in combinations of normal components, as illustrated in Fig. 1.These logical anomalies widely exist in industrial settings, like assembly and packaging.Given the potential for severe functional consequences associated with logical anomalies, specific logical anomaly detection methods have been developed.[5].In toy example, the yellow squares represent normal components, while the red squares signify structural deviations from the norm.Notably, even though the individual components in logical anomaly are normal, they collectively defy logical constraints.For the LOCO examples, structural anomaly is exemplified by damaged connector, while logical anomaly is indicated by misplaced cable connections.</p>
<p>Existing logical anomaly detection methods, such as GCAD [5] and its successors [6], [7], [8], have made their initial progresses, as shown in Fig. 2.These methods leverage both local and global network branches to detect structural and logical inconsistencies, with a particular emphasis on global consistency representations.They assume that global consistency representations can reflect the overall logical constraint of an image.However, such representations often fail to equate to the deep logical relationships necessary for identifying subtle logical anomalies-like an incorrect sequence of correctly quantified components or components that match in type but not in specification.The inherent challenge lies in detecting these high-level semantic inconsistencies, which requires an understanding of real logical reasoning and relationships beyond the capabilities of purely vision-based systems.Furthermore, these methods often present results for logical anomalies through anomaly scores(see Fig. 2).However, unlike structural anomaly detection, logical anomaly detection typically demands clearer explanations while the lack of explicit reasoning can lead to confusions.This highlights the need for a method that not only detects but also articulately explains logical anomalies.</p>
<p>Recent advances in LLMs [9], [10], [11], offer new insights and solutions for this challenge.LLMs excel in understanding and processing complex logical relationships and reasoning [12], [13], making them particularly suited for detecting Fig. 2: Comparison between the existing method (left) and our LogiCode framework (right).The existing method do not accurately capture the underlying logical relationships and lack explanation for logical anomalies.In contrast, the proposed method starts from the perspective of logical constraints and gives the result with reason.and analyzing these logical anomalies.Unlike traditional methods, LLMs can interpret and reason about high-level semantic logic, providing an innovative approach to anomaly detection.</p>
<p>Hence, this study proposes a LogiCode framework, utilizing the advanced capabilities of LLMs, representing a paradigm shift in logical anomaly detection.LogiCode, utilizing expert knowledge, extracts logical relationships from normal images and defines them as a set of logical rules.These rules are then parsed by the LLM, which automatically generates executable codes to examine the consistency between testing images and normal rules.Furthermore, the framework is designed to provide reasons for these anomalies, enhancing the interpretability of its findings, as shown in Fig. 2.</p>
<p>In this framework, the LLM's process of analyzing visual information and detecting logical anomalies is akin to the interplay between human perception and cognitive reasoning.LogiCode embodies this synergy, offering a more natural and intuitive approach to anomaly detection that mirrors human expertise.The framework transcends the capabilities of traditional models by providing enhanced interpretability, direct logic interpretation, and adaptability to the complex nuances of industrial quality control.</p>
<p>The key contributions of this study include: The reminder of this paper is organized as follows.Section I provides a comprehensive overview of the proposed framework and its position within the context of current research.Section II reviews some related work and establishes the foundation for the LogiCode framework.Section III details the methodological approach of LogiCode, followed by Section IV, which presents an in-depth analysis of experimental results and performance evaluations.Finally, Section V discusses the implications of our findings and explores potential directions for future research in logical anomaly detection using LLMs.</p>
<p>II. RELATED WORKS</p>
<p>This section delves into advancements in anomaly detection, including structural and logical approaches, as well as studies utilizing LLMs for automation tasks.</p>
<p>A. Structural Anomaly Detection</p>
<p>Existing methods that excel in detecting low-level structural anomalies can be categorized into two main types: reconstruction-based and feature-embedding similarity-based approaches.</p>
<p>Reconstruction-based methods, such as Autoencoders [14] (AEs) and Generative Adversarial Networks [15] (GANs), attempt to reconstruct input images through a lower-dimensional bottleneck.In this context, Yang et al. [16] proposed a multiscale feature-clustering-based fully convolutional autoencoder that significantly improves the speed and accuracy of visual inspection for texture surface defects.Further advancing this category, innovations like RIAD [17] reformulates reconstruction as an image inpainting problem.While these methods are efficient in capturing image contexts [18], they often yield blurry reconstructions, leading to increased false positives.</p>
<p>On the other hand, feature-embedding based methods, such as RD4AD [19], Padim [20], PatchSVDD [21], SPADE [22] and PatchCore [23], utilize deep neural network-extracted vectors representing the entire image.Cao et al. [24] explored informative knowledge distillation within image anomaly segmentation to enhance model learning from complex data distributions.Moreover, Jiang et al. [25] introduced a masked reverse knowledge distillation approach that leverages both global and local information for enhancing image anomaly detection.These methods, relying on the distance between the embedding vectors of test images and normal reference vectors from the training dataset [26], [27], demonstrate superior performance compared to reconstruction-based approaches [28].</p>
<p>Both reconstruction-based and feature-embedding based approaches have shown great result, on datasets for structural anomalies like MVTec AD [4].However, these methods, constrained by their limited receptive fields, struggle to detect anomalies beyond receptive fields and fail to distinguish violations of logical constraints.This is particularly true in LOCO datasets, where the detection of logical anomalies is crucial.This highlights a significant gap in current anomaly detection methodologies, underlining the need for more adaptable and comprehensive systems that can effectively detect logical anomalies.</p>
<p>B. Logical Anomaly Detection</p>
<p>In addressing high-level semantic logic anomalies, several methods have emerged in recent research.MVTec LOCO [5], as a milestone, is specialized for studying and resolving advanced semantic logic anomalies.Alongside, GCAD [5] was proposed to solve logical anomaly detection.GCAD includes a local network branch and a global network branch.The local branch is designed to detect novel local structures in images, while the global branch learns a global consistency representation through a bottleneck structure, targeting violations in long-distance dependencies.This structure allows for the detection of both structural and logical anomalies.Subsequent methods, focusing on logic detection, often continue to employ the strategy of GCAD's global network branch, which is based on vision-based global-local correspondences.For example, EfficientAD [8] utilizes an autoencoder-student network pair to detect anomalies that violate global semantic constraints.Similarly, GLCF [7] achieves this through a combination of local and global network branches; the local branch focuses on structural anomaly detection, while the global branch captures logical anomalies through semantic bottlenecks.However, global feature extraction may not always discern subtle logical anomalies, like correct component quantities arranged in the wrong order, or components of the right type but incorrect specifications.Therefore, employing the logical reasoning capabilities of LLMs offers a more universal appraoch for resolving logical anomalies.</p>
<p>C. LLMs in Automation Tasks</p>
<p>The advance of LLMs marks a significant milestone in the field of automation, especially in industrial applications.These models have transcended traditional boundaries, offering novel solutions in code generation [13], visual question answering (VQA) [12], [29], and anomaly detection [30].</p>
<p>ViperGPT [13], a frontrunner in this domain, exemplifies the integration of LLMs in VQA, using code-generation to resolve visual queries more effectively than traditional methods.This represents a significant advancement over previous code generation techniques, providing more context-aware and adaptable solutions.The industrial application of LLMs extends to defect detection, anomaly detection, and quality control.Zhang et al. [31] demonstrate its capability in zeroshot anomaly detection, applying it to complex VQA tasks.Wang et al. [32] introduces Industrial-GPT, tailored for intelligent manufacturing and excelling in tasks like fault diagnosis.Furthermore, Cao et al. [33] illustrates LLM's effectiveness for anomaly detection in multi-modal domains, while also acknowledging some limitations in intricate scenarios.Anoma-lyGPT [30] also emerges as an innovative industrial anomaly detection model, aligning images with textual descriptions to improve anomaly detection.</p>
<p>By leveraging the advancements in LLMs for logical reasoning and code generation, and addressing the limitations of existing logical anomaly detection methods, LogiCode represents a significant evolution in tackling high-level semantic logic anomalies in industrial settings.</p>
<p>III. PROPOSED LOGICODE FRAMEWORK</p>
<p>The LogiCode framework is an LLM-empowered framework developed for identifying logical anomalies in images.It is systematically introduced in the following subsections.</p>
<p>A. Framework Overview</p>
<p>The LogiCode framework is structured into three interconnected main modules-Code Prompt, Code Generation, and Code Execution, as shown in Fig. 3. Drawing inspiration from human cognitive processes, the framework adeptly combines observation and logical reasoning in its modules.The Code Prompt module initiates the process of identifying problems and formulating logical rules, akin to human problem recognition and solution crafting.Code Generation, powered by an LLM, processes these rules to create logical decision codes and selects corresponding visual APIs, reflecting human cognitive synthesis for decision-making.Code Execution module integrates logical evaluation with visual parsing, analogous to human reasoning and information interpretation, focusing on essential visual details.This module's output offers a detailed analysis, identifying anomalies with rule-based explanations, showcasing the framework's humanlike problem-solving approach.The collective functionality of these modules highlights how the LogiCode framework excels in logical anomaly detection and interpretability, both essential for industrial applications.</p>
<p>B. Code Prompt</p>
<p>Code Prompt serves as the initial stage of the logical anomaly detection process.This module combines the functionalities of "Request Prompt" and "Logical-Rule Definition" to define the analyses task and establish the corresponding logical rules:</p>
<p>Request Prompt: Within "Code Prompt", the "Request Prompt" sub-module is meticulously designed to formulate the user's task definition into a structured prompt for the utilized LLM.This module involves several components, each playing a critical role in transforming complex tasks into structured LLM prompts:</p>
<p>Task Interpretation: Request Prompt starts with a clear interpretation of the task, as in "Create a Python function utilizing the Image class to comprehensively analyze an image and determine its abnormality concerning its cable and connectors layout," ensuring task needs are precisely articulated in LLMcompatible formats.</p>
<p>Function Structuring: Simultaneously, Request Prompt organizes the prompt to include all critical aspects of the function, such as inputs and expected outputs, ensuring that instructions like "the function should accept the image path and return two results: a boolean and a string listing all the reasons for abnormality," are clearly communicated to the LLM.</p>
<p>Knowledge Integration: Domain-specific knowledge is then woven into the Request Prompt, guiding the system to "Employ basic Python features for logical operations and mathematical calculations," thus grounding the task in the relevant technical context.</p>
<p>Prompt Engineering: Inspired by the latest research in prompt engineering [34], [35], [36], this component incorporates strategies like outlining sub-tasks and illustrating relationships, like "Initially, outline the sub-tasks required for the analyses.Illustrate the relationships between these sub-tasks through a detailed step-by-step breakdown."This approach guides the LLM in understanding the structure and logic of the task, enhancing the accuracy and efficiency of the code generation process.</p>
<p>Logical-Rule Definition: Within "Code Prompt", the "Logical-Rule Definition" is a critical sub-module that operates in conjunction with the "Request Prompt".It is focused on defining logical rules with expert knowledge from observations of normal example images.</p>
<p>Each set of rules is scenario-specific and can be adapted to fit a wide range of contexts, making the framework versatile and applicable across different industries.The LogiCode framework, through this sub-module, showcases its adapt-ability by allowing these rules to be modified or expanded based on the requirements of different industrial scenarios, thereby maintaining the relevance and efficacy of the anomaly detection process.</p>
<p>C. Code Generation</p>
<p>Following Code Prompt, Code Generation in the LogiCode framework serves as a crucial bridge between conceptual rules and executable codes.This module harnesses the power of the LLM to transform structured request prompts and logical rules into Python codes.The process involves several key components:</p>
<p>Logical Parsing: Once the LLM receives the structured prompts, it employs its extensive knowledge base to parse complex logical conditions into executable Python codes.For example, a rule concerning the length of cable is interpreted by the LLM as:
L(r length ) : if L cable / ∈ <a href="1">L min , L max </a>
L cable is the measured cable length, and [L min , L max ] is the acceptable length range for the cable.Each rule r from the set R is parsed into a logical expression L(r) that the Python interpreter can execute.</p>
<p>API Selection: The designed Image class provides a rich API set that the LLM selects from to match the logical rules with the functional capabilities needed for image analyses, as shown in Table I.This selection is crucial as it dictates the precision and efficiency of the subsequent image processing.The LLM matches each logical rule r from the set R with an API function A(r) from the Image class, ensuring that each aspect of the image analyses is addressed.</p>
<p>Each method A(r) in the API set is chosen for its ability to provide the necessary data for the logical rules to operate on.For example, the size() method, used in adherence to a rule regarding object size, is synthesized into the code as follows:
A(r length ) : AP I size (image) → L cable (2)
This expression normalizes how the size() method is selected and utilized to extract the length of cable, denoted by L cable , from the given image.It encapsulates the action of the LLM in choosing the size() method from the Image class when the rule r length is concerned with verifying the length of the cable within acceptable limits.</p>
<p>Code Synthesis: After logical parsing and API selection, the LLM synthesizes the information into a cohesive block of Python code.This synthesis combines the logic and API calls into a sequence of executable instructions:
c = r∈R {L(r), A(r)}(3)
The SynthesizeCode component illustrates the LLM's action in compiling the parsed rules and selected APIs into a final code segment ready for execution.</p>
<p>Code Generation thus serves as a crucial bridge, translating the structured input from the Code Prompt into Python codes that are ready for execution.It uses LLM's inferential power and the APIs' feature extraction capabilities to perform detailed image analyses and accurately identify logical anomalies.</p>
<p>D. Code Execution</p>
<p>Code Execution is the operational core of the LogiCode framework where the Python codes synthesized by Code Generation are brought to execution.This module interprets and runs the codes on the given image data, applying the logical rules and utilizing the APIs to perform a detailed analysis of the imagery.It consists of two critical sub-modules, each responsible for different aspects of the execution process:</p>
<p>Logical Execution: This sub-module is responsible for the logical assessment of each image based on the generated codes.It executes the conditional statements and loops derived from the logical parsing process, directly applying them to the image data.For instance, the rule concerning the cable length is translated into an executable code that is conducted as depicted in the Code Execution section of Fig. 3.The logical execution is facilitated by the Python interpreter, which assesses the conditions set by each rule against the actual image data.</p>
<p>Visual Parsing: Integrated within Code Execution, visual parsing is crucial for the extraction of visual features from the image that are necessary for logical anomaly detection.This sub-module uses the selected APIs to determine the size, position, color, and other relevant attributes of the image components.The process of measuring the cable's length in the image, for instance, is exemplified in the orange section of the figure.The outcome of size is subsequently appraised by the predefined logical rules to ascertain its congruence with the acceptable size range.</p>
<p>Then the execution process integrates the outputs from logical execution and visual parsing, cross-referencing them to identify any anomalies.The integration ensures that all the visual features extracted are checked against the logical conditions, providing a comprehensive and accurate assessment.</p>
<p>The final output from Code Execution includes a detailed analysis report, highlighting any detected anomalies with their corresponding reasons.The report is designed to be both thorough and interpretable, allowing for quick identification of issues and facilitating decision-making processes.</p>
<p>By combining logical reasoning with visual data interpretation, Code Execution encapsulates the functionality required to identify logical anomalies accurately, reflecting a significant advancement in the application of LLMs to industrial anomaly detection.</p>
<p>IV. EVALUATIONS AND DISCUSSIONS</p>
<p>This section evaluates the proposed LogiCode framework's logical anomaly detection in industrial contexts using a specialized dataset, the LOCO-Annotations.It includes thorough benchmarking with LogiBench to test classification accuracy, code generation, and reasoning about anomalies.These tests aim to validate the LogiCode's capabilities for industrial applications.Then the effectiveness and impact of LogiCode are discussed, highlighting its technological advancements and potential for application.</p>
<p>A. LOCO-Annotations Dataset</p>
<p>This subsection introduces the LOCO-Annotations dataset, a specialized extension of the MVTec LOCO dataset, aimed at filling a critical gap in the realm of logical anomaly detection in industrial scenarios.While the original MVTec LOCO dataset offers a robust foundation with its mix of structural and logical anomalies in industrial images, it primarily adheres to unsupervised anomaly detection, with a significant emphasis on detecting regions indicative of logical anomalies.Yet, the practical application of anomaly detection often necessitates an in-depth understanding of the underlying causes of these logical anomalies.In response to this, LOCO-Annotations shifts its focus from mere anomaly region detection to a thorough analysis of the underlying reasons for logical anomalies.This pivot aligns with the nuanced requirements of detecting and understanding logical anomalies, thereby better addressing the complex quality control demands in industrial environments.</p>
<p>LOCO-Annotations, comprising 2908 meticulously annotated images (1772 training and 1136 testing images, spanning various categories such as breakfast boxes, screw bags, pushpins, splicing connectors, and juice bottles.),diverges from MVTec LOCO's approach by solely concentrating on logical anomalies.It categorizes the logical anomalies into four main types based on scenarios: Quantity Anomalies, Size Anomalies, Position Anomalies, and Matching Anomalies, as demonstrated in Fig. 5.This focus stems from a need to delve deeper into high-level semantic inconsistencies often overlooked in existing logical anomaly detection methods.</p>
<p>Unlike MVTec LOCO, which includes both logical and structural anomalies in its testing set, LOCO-Annotations dataset exclusively annotates logical anomalies in its testing subset.Each image in the LOCO-Annotations is accompanied by detailed JSON files providing pixel-level object segmentation and ground truth annotations formatted as "Anomaly Type: Specific Reason", clarifying the reasoning behind each identified logical anomaly.Examples of MVTec LOCO and LOCO-Annotations are illustrated in Fig. 4.This level of detail is pivotal in training and evaluating logical anomaly detection methods.The unique contribution of LOCO-Annotations lies in its detailed focus on logical anomalies and their underlying reasons, providing essential data for testing and refining models like LogiCode and other LLM-driven methods.This approach aligns with the evolving needs of industrial quality control, where understanding the "why" behind logical anomalies is as crucial as detecting them.Additionally, its provision of data for decoupling individual objects within images further enhances the dataset's utility, enabling a more detailed and nuanced analysis that is critical for understanding the complex dynamics of logical anomalies in industrial settings.</p>
<p>By offering a dataset that emphasizes logic and context, we aim to change the paradigm in anomaly detection research, focusing on logical semantic inconsistencies and their implications in industrial settings.The LOCO-Annotations not only fills the gap in industrial anomaly detection research but also paves the way for future advancements, setting new standards for the development of LLMs-based inspection systems.</p>
<p>B. LogiBench</p>
<p>LogiBench is meticulously designed for evaluating the Logi-Code framework, with an emphasis on the thorough analyses Fig. 5: Quantity Anomaly: The expected count for the cable is one, yet two is present.Position Anomaly: Cable should attach to the connectors with the same order defined by position; however, misplaced attachment is observed here.Size Anomaly: The cable length is anticipated to fall within a specific range; however, it is observed to deviate from the normal range.Matching Anomaly:</p>
<p>The cable color is expected to match connector number; however, a mismatch is observed here.</p>
<p>of logical anomaly detection in industrial scenarios.The benchmark's construction is driven by the need to evaluate the framework's binary classification accuracy, code generation success rate, and reasoning accuracy:</p>
<p>Binary Classification Accuracy: This metric assesses the performance of the LLM-generated codes in correctly identifying the presence of logical anomalies in images.</p>
<p>It involves comparing the outcomes of the model with the ground truth (Normal/Abnormal) provided in the LOCO-Annotations to calculate crucial binary classification metrics like accuracy and recall rate.It is evaluated based on accuracy, precision, recall and F1-score.</p>
<p>Code Generation Success Rate: This metric measures the LLM's ability to generate executable Python codes based on the provided logical rules and request prompts.</p>
<p>This metric specifically focuses on comparing the explanations generated by LLMs with the standard truth reasons provided in the LOCO-Annotations, thereby determining the correctness of the LLM's reasoning against established benchmarks.It is crucial due to the potential variability in LLMs' ability to generate Python codes for logical anomaly detection.</p>
<p>The success rate is determined by analyzing whether the generated codes run without syntax errors and correctly implement visual APIs.</p>
<p>Reasoning Accuracy: This metric evaluates the precision of LLM-generated codes in explaining the reasons for logical anomalies.It is assessed to address the observed instances where LLMs correctly classify anomalies but may provide inaccurate explanations for their reasoning.The assessment is two-fold, detailed as follows:</p>
<p>Human Evaluation: An expert analysis to validate the consistency and correctness of the explanations provided by the LLM.It is necessary due to the complexity involved in discerning anomaly reasons, requiring expert knowledge to validate the consistency and correctness of the LLM's explanations.</p>
<p>LLM Automatic Evaluation [37], [38]: Utilizes a structured prompt to compare the output result against the ground truth reason, evaluating the LLM's accuracy in identifying logical  anomaly reasons, as shown in Fig. 6.It is included to reduce the workload of manual evaluation.LogiBench sets a new precedent for evaluating logical anomaly detection algorithms, providing a detailed and multidimensional evaluation of the LogiCode framework's capabilities.Through this benchmarking process, we aim to demonstrate the advanced interpretability and efficiency of LogiCode in industrial scenarios, underlining its adaptability and robustness in a variety of industrial contexts.The thorough assessment provided by LogiBench also serves as a valuable resource for future enhancements and applications of LLMs in logical anomaly detection.</p>
<p>C. Evaluation Results and Analyses</p>
<p>The effectiveness of LogiCode, as assessed by the LogiBench benchmark, is presented in this subsection.Our comprehensive evaluation covers several key performance metrics, the results of which are summarized below.</p>
<p>At the outset, it is important to note that all experiments conducted in this study are based on GPT-4 [39].Employing GPT-4 for this purpose reflects the framework's integration of state-of-the-art LLM capabilities to ensure the precision of its anomaly reasoning analysis.The outcomes of the various performance metrics assessed are presented below.</p>
<p>Binary Classification Accuracy: The accuracy, precision, recall, and F1-score of the framework in identifying logical anomalies are measured against the ground truth labels provided by the LOCO-Annotations.</p>
<p>The binary classification metrics are determined through comprehensive testing across all categories.The evaluation involved conducting five sets of experiments for each category, with the results averaged to obtain a reliable measure of accuracy, precision, recall and F1-score.This approach ensures a thorough and unbiased assessment of the framework's ability to detect logical anomalies.</p>
<p>The results for the Binary Classification metrics are presented in Table II.It can be observed that the binary classification metrics for anomaly detection are remarkably high, suggesting that the proposed framework can effectively detect logical anomalies.</p>
<p>To comprehensively evaluate the efficacy of the proposed LogiCode framework in detecting logical anomalies, comparative analyses against current state-of-the-art (SOTA) methods have been conducted.As outlined in Table III, our assessment is meticulously focused on logical anomalies across various categories, contrasting our binary classification accuracy with that of established approaches.Unlike traditional methods, which predominantly rely on AUROC metrics for performance evaluation, the proposed framework necessitates a binary classification due to its inherent design to output discrete labels (0 for 'normal' and 1 for 'anomaly').This differentiation necessitated the adoption of an alternative evaluation metric, focusing on maximized accuracy through variable threshold optimization.</p>
<p>The implementation of LogiCode, along with all competing methods, has been conducted in-house to ensure a fair and controlled comparison.Notably, the use of the LOCO-Annotations dataset allows us to leverage additional data, not merely for performance enhancement but as a means to showcase the inherent superiority of the proposed framework.The results, as depicted in Table III, affirm the exceptional performance of the proposed method, underscoring its potential to redefine standards in logical anomaly detection within industrial settings.</p>
<p>Code Generation Success Rate: This paper evaluated the success rate of the framework in generating syntactically correct and executable Python codes.</p>
<p>To evaluate the success rate of code generation, code produced for each category are generated 20 times separately.The success rate is calculated based on the number of times the codes ran successfully and aligned with the logical anomaly rules without any Python syntax errors.This repeated execution method provides a robust assessment of the framework's consistency in generating executable Python codes.</p>
<p>Table IV presents the success rate of code generation using prompts in GPT-4, where "success" is attributed to instances where the codes' functionality aligns with the logical anomaly rules without Python syntax errors that could prevent execution.The other instances are categorized as "missing" or "error," as detailed in the Table IV.The examples for the generated codes are shown in Fig. 7.</p>
<p>It is observed that due to the current instability of LLMs, the code generation success rate presently hovers around 60%.However, it is expected that future iterations of LLM frameworks will significantly improve this metric.</p>
<p>Reasoning Accuracy: The framework's precision in articulating the reasons for anomalies has been tested through LLM automatic evaluation and human evaluation.</p>
<p>For reasoning accuracy, the evaluation focused on codes that successfully executed in each category.Five sets of such successful codes are analyzed for each category, with the results averaged to assess the framework's precision in articulating the reasons for anomalies.</p>
<p>Human Evaluation: The human evaluation involved employing three researchers with relevant backgrounds to assess the framework's reasoning accuracy.Each researcher independently evaluates the explanations provided by the framework, and their assessments are averaged to determine the overall   LLM Evaluation: For the automatic evaluation of reasoning accuracy, GPT-4 is employed.The implementation details of this evaluation are provided in the LogiBench benchmark section.</p>
<p>As shown in Table V, the comparison between the automatic reasoning evaluation of LLM and the assessments by human experts indicates a close correspondence, suggesting the reliability of the LLM's self-judgment in identifying the causes of logical anomalies.Moreover, the reasoning accuracy metric for the framework consistently exceeds 90%, affirming the framework's efficacy.</p>
<p>These results provide crucial insights into the capabilities and potential areas of improvements for the LogiCode framework in the context of logical anomaly detection in industrial settings.</p>
<p>D. Discussions</p>
<p>The LogiCode framework introduces advancements and addresses limitations in existing LLMs in the context of industrial logical anomaly detection.This subsection explores the impact of LogiCode on this field, emphasizing both its advantages and the constraints of current LLMs.Impact on Industrial Logical Anomaly Detection: Logi-Code, integrating LLMs and specialized APIs, enhances the accuracy of detecting high-level semantic inconsistencies.This is particularly crucial in industrial settings where such anomalies can significantly affect product quality and safety.By providing more accurate and interpretable anomaly detection, LogiCode has the potential to transform quality control processes, reducing both time and costs associated with manual inspections.Advantages and Limitations of LLM Methods: LLMs contribute a nuanced understanding of complex logical relationships, generating contextually relevant codes for anomaly detection-a significant advancement from traditional rulebased systems.Despite their capabilities, LLMs may encounter challenges in highly specialized or niche industrial scenarios with limited training data.Furthermore, the black-box nature of these models can pose transparency and trustworthiness issues in critical applications.Future Prospects and Recommendations: Future research could explore the integration of reinforcement learning and LLMs to autonomously summarize logical rules, reducing dependence on expert inputs and enhancing adaptability to new scenarios.The current reliance on detailed annotations for API functions such as "find" could evolve into more sophisticated zero-shot or one-shot methods, enabling generalized object segmentation without heavy reliance on pixel-level labels.This evolution would broaden the framework's applicability across diverse industrial settings and decrease the need for extensive dataset preparations.</p>
<p>V. CONCLUSIONS</p>
<p>This paper introduces the LogiCode framework, a novel approach that leverages LLMs for industrial logical anomaly detection.LogiCode represents a significant transition from traditional methods, focusing on high-level semantic inconsistencies and offering a more nuanced understanding of logical anomalies.Its integration of LLMs for code generation and logical reasoning sets a new benchmark in the field, demonstrating remarkable adaptability and interpretability across various industrial scenarios.</p>
<p>Through rigorous evaluations with the LogiBench benchmark, LogiCode shows superior performance in binary classification accuracy, code generation success rate, and reasoning accuracy.These achievements underscore the framework's efficiency and reliability in anomaly detection and its potential in revolutionizing quality control processes in industrial settings.</p>
<p>Looking forward, the potential of LLM-driven anomaly detection is vast.Future work will focus on further enhancing the models' autonomy, allowing them to adapt seamlessly to a variety of scenarios with minimal human intervention.Additionally, efforts to improve the generalizability of the models to maintain consistent performance across different industries will be crucial.</p>
<p>In summary, LogiCode is more than just a solution to current challenges in logical anomaly detection, it is a foundation for a future where intelligent models are integral to industrial processes, offering robust, interpretable, and efficient solutions.The ongoing developments in LLMs and their applications in this field open up exciting possibilities for industrial anomaly detection, leading the way toward more advanced, reliable, and automated quality control systems.</p>
<p>Fig. 1 :
1
Fig. 1: Comparison between the structural anomaly and the logical anomaly.(Top) Toy example.(Bottom) Examples from the MVTec LOCO AD dataset [5].In toy example, the yellow squares represent normal components, while the red squares signify structural deviations from the norm.Notably, even though the individual components in logical anomaly are normal, they collectively defy logical constraints.For the LOCO examples, structural anomaly is exemplified by damaged connector, while logical anomaly is indicated by misplaced cable connections.</p>
<p>Fig. 3 :
3
Fig. 3: Overview of LogiCode framework LogiCode is an LLM-empowered logical anomaly detection framework.It comprises three main modules: Code Prompt, Code Generation, and Code Execution.a) Code Prompt module formulates user-defined tasks and logical rules by analyzing examples of normal and abnormal images.b) Code Generation module utilizes an LLM to parse these rules into executable Python codes, selecting appropriate APIs for image analyses.c) Code Execution module applies logical and visual parsing to detect and report anomalies.This comprehensive process not only detects anomalies but also provides rule-based explanations for them, mimicking human problem-solving and decision-making abilities in industrial settings.</p>
<p>Fig. 4 :
4
Fig. 4: The diagram contrasts the MVTec LOCO dataset (Top) with the LOCO-Annotations (Bottom).Unlike MVTec's broad focus, the LOCO-Annotations dataset zeroes in on logical anomalies, offering an enriched dataset with detailed explanations for each anomaly.This provides LLMs with the context and specificity required for a nuanced understanding of anomalies, thereby facilitating a more sophisticated and targeted development in industrial quality control systems.</p>
<p>Fig. 6 :
6
Fig. 6: Illustration for LLM Automatic Evaluation.</p>
<p>Fig. 7 :
7
Fig. 7: Demonstration for success code, missing code and syntax error cases.The figure shows the core part of the generated code, the value of intermediate variables during the execution and the final output.</p>
<p>TABLE I :
I
Illustration for API set functions</p>
<p>TABLE II :
II
Performance of the proposed method
CategoryAccuracyPrecisionRecall F1 ScoreJuice bottle0.9920.9871.0000.994Breakfast box0.9890.9761.0000.988Pushpins0.9900.9761.0000.988Screw bag0.9810.9651.0000.982Splicing connectors0.9890.9781.0000.989Average0.9890.9761.0000.988</p>
<p>TABLE III :
III
Accuracy comparison with the sota methods
Category/MethodsSTPEFMRD4ADSPADEGCLFEfficientAD-SEfficientAD-MLogiCodeJuice bottle0.7410.7310.7670.8060.9350.9700.9760.992Breakfast box0.7460.7950.7350.6950.6930.7530.7780.989Pushpins0.6680.6810.6990.5680.8230.9160.8970.990Screw bag0.5370.5710.5790.6420.6270.6570.6720.981Splicing connectors0.6340.6390.6210.7080.8660.9070.9360.989Average0.6650.6830.6800.6840.7890.8410.8520.989</p>
<p>TABLE IV :
IV
Code Generation Success Rate
MetricJuice bottleBreakfast boxPushpinsScrew bagSplicing connectorsAverageSuccess0.3500.6500.8000.6000.5500.590Error0.3000.1000.1000.5000.1500.140Missing0.3500.2500.1000.3500.3000.270</p>
<p>TABLE V :
V
Human and LLM evaluation Reasoning Accuracy
MethodsJuice bottleBreakfast boxPushpinsScrew bagSplicing connectorsAverageHuman0.9010.9740.9760.9880.9760.963LLM0.8690.9280.9320.9360.9540.924accuracy.</p>
<p>Y Cao, X Xu, J Zhang, Y Cheng, X Huang, G Pang, W Shen, arXiv:2401.16402A survey on visual anomaly detection: Challenge, approach, and prospect. 2024arXiv preprint</p>
<p>Defect classification and detection using a multitask deep one-class cnn. X Dong, C J Taylor, T F Cootes, IEEE Transactions on Automation Science and Engineering. 1932021</p>
<p>Dual-attention transformer and discriminative flow for industrial visual anomaly detection. H Yao, W Luo, W Yu, X Zhang, Z Qiang, D Luo, H Shi, IEEE Transactions on Automation Science and Engineering. 2023</p>
<p>Mvtec ada comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Beyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization. P Bergmann, K Batzner, M Fauser, D Sattlegger, C Steger, International Journal of Computer Vision. 13042022</p>
<p>Slsg: Industrial image anomaly detection by learning better feature embeddings and one-class classification. M Yang, J Liu, Z Yang, Z Wu, arXiv:2305.003982023arXiv preprint</p>
<p>Learning global-local correspondence with semantic bottleneck for logical anomaly detection. H Yao, W Yu, W Luo, Z Qiang, D Luo, X Zhang, 2023Technology</p>
<p>Efficientad: Accurate visual anomaly detection at millisecond-level latencies. K Batzner, L Heckler, R König, 2024</p>
<p>Visual programming: Compositional visual reasoning without training. T Gupta, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>X Lai, Z Tian, Y Chen, Y Li, Y Yuan, S Liu, J Jia, arXiv:2308.00692Lisa: Reasoning segmentation via large language model. 2023arXiv preprint</p>
<p>Idealgpt: Iteratively decomposing vision and language reasoning via large language models. H You, R Sun, Z Wang, L Chen, G Wang, H Ayyubi, K Chang, S Chang, arXiv:2305.149852023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Vipergpt: Visual inference via python execution for reasoning. D Surís, S Menon, C Vondrick, arXiv:2303.081282023arXiv preprint</p>
<p>Reducing the dimensionality of data with neural networks. G Hinton, R Salakhutdinov, Science. 31357862006</p>
<p>Ganomaly: Semi-supervised anomaly detection via adversarial training. S Akcay, A Atapour-Abarghouei, T Breckon, 2019</p>
<p>Multiscale featureclustering-based fully convolutional autoencoder for fast accurate visual inspection of texture surface defects. H Yang, Y Chen, K Song, Z Yin, IEEE Transactions on Automation Science and Engineering. 1632019</p>
<p>Reconstruction by inpainting for visual anomaly detection. V Zavrtanik, M Kristan, D Skocaj, Pattern Recognition. 1121077062021</p>
<p>Improving unsupervised defect segmentation by applying structural similarity to autoencoders. P Bergmann, S Lowe, M Fauser, D Sattlegger, C Steger, arXiv:1807.020112018arXiv preprint</p>
<p>Anomaly detection via reverse distillation from one-class embedding. H Deng, X Li, 2022</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. T Defard, A Setkov, A Loesch, R Audigier, 2021</p>
<p>Patch svdd: Patch-level svdd for anomaly detection and segmentation. J Yi, S Yoon, 2020</p>
<p>Sub-image anomaly detection with deep pyramid correspondences. N Cohen, Y Hoshen, arXiv:2005.023572020arXiv preprint</p>
<p>Towards total recall in industrial anomaly detection. K Roth, L Pemula, J Zepeda, B Scholkopf, T Brox, P Gehler, 2022</p>
<p>Informative knowledge distillation for image anomaly segmentation. Knowledge-Based Systems. Y Cao, Q Wan, W Shen, L Gao, 2022248108846</p>
<p>A masked reverse knowledge distillation method incorporating global and local information for image anomaly detection. Y Jiang, Y Cao, W Shen, Knowledge-Based Systems. 2801109822023</p>
<p>A feature memory rearrangement network for visual inspection of textured surface defects toward edge intelligent manufacturing. H Yao, W Yu, X Wang, IEEE Transactions on Automation Science and Engineering. 2022</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. P Bergmann, M Fauser, D Sattlegger, C Steger, 2020</p>
<p>Collaborative discrepancy optimization for reliable image anomaly localization. Y Cao, X Xu, Z Liu, W Shen, IEEE Transactions on Industrial Informatics. 2023</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Anomalygpt: Detecting industrial anomalies using large visionlanguage models. Z Gu, B Zhu, G Zhu, Y Chen, M Tang, J Wang, arXiv:2308.153662023arXiv preprint</p>
<p>Exploring grounding potential of vqa-oriented gpt-4v for zeroshot anomaly detection. J Zhang, X Chen, Z Xue, Y Wang, C Wang, Y Liu, arXiv:2311.026122023arXiv preprint</p>
<p>Industrial-generative pretrained transformer for intelligent manufacturing systems. H Wang, M Liu, W Shen, IET Collaborative Intelligent Manufacturing. 52e120782023</p>
<p>Towards generic anomaly detection and understanding: Largescale visual-linguistic model (gpt-4v) takes the lead. Y Cao, X Xu, C Sun, X Huang, W Shen, arXiv:2311.027822023arXiv preprint</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D Schmidt, arXiv:2302.113822023arXiv preprint</p>
<p>Q Ye, M Axmed, R Pryzant, F Khani, arXiv:2311.05661Prompt engineering a prompt engineer. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>X Wang, Z Wang, J Liu, Y Chen, L Yuan, H Peng, H Ji, arXiv:2309.10691Mint: Evaluating llms in multi-turn interaction with tools and language feedback. 2023arXiv preprint</p>
<p>Z Bi, N Zhang, Y Xue, Y Ou, D Ji, G Zheng, H Chen, arXiv:2310.02031Oceangpt: A large language model for ocean science tasks. 2023arXiv preprint</p>
<p>Gpt-4 technical report. arxiv 2303.08774View in Article. 2132023OpenAI</p>
<p>where he is currently pursuing the M.S. degree in mechanical engineering. His current research interests include industrial foundation models and their real-world applications, visual understanding, anomaly detection, and computer vision. Yunkang Cao (Student Member, IEEE) received the B.S. degree in mechanical design, manufacturing and automation from Huazhong University of Science and Technology (HUST), Wuhan, China, in 2020, where he is currently pursuing the Ph.D. degree in mechanical engineering. His current research interests include industrial foundation models and their. 2022Wuhan, ChinaYiheng Zhang received the B.S. degree from the School of Mechanical Science and Engineering, Huazhong University of Science and Technologyworld applications, anomaly detection, and computer vision</p>
<p>Before joining HUST in 2019, he was a Principal Research Officer at the National Research Council Canada. He is a Fellow of Canadian Academy of Engineering and the Engineering Institute of Canada. His work has been cited more than 20000 times with an h-index of 70. He authored or coauthored several books and more than 560 articles in scientific journals and international conferences in related areas. 1983 and 1986, respectively, and the Ph.D. degree in system control from the University of Technology of Compiegne. Ann Arbor, MI, USA; Beijing, China; Compiegne, France; Wuhan, China; London, ON, Canada1996Huazhong University of Science and Technology (HUST) ; from Northern Jiaotong University ; University of Western OntarioHis current research interests include the fundamental theory and real-world applications of robotics, computer vision, and video understanding. His research interests include agentbased collaboration technologies and applications, collaborative intelligent manufacturing, the Internet of Things, and Big Data analytics</p>            </div>
        </div>

    </div>
</body>
</html>
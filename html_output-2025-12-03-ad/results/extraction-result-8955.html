<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8955 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8955</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8955</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd" target="_blank">Universal Self-Consistency for Large Language Model Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates and effectively utilizes multiple samples and improves the performance on open-ended generation tasks where the original self-consistency method is not applicable.</p>
                <p><strong>Paper Abstract:</strong> Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8955.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8955.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection-based self-reflection method that asks the same LLM which of several sampled outputs is the most consistent (or meets another selection criterion) and uses that selection as the final answer; applicable to free-form generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2-L and OpenAI gpt-3.5-turbo were used in experiments; temperatures 0.6 (PaLM 2-L) and 1.0 (gpt-3.5-turbo); 8 samples typically generated per example unless specified.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Universal Self-Consistency (USC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple model responses, concatenate them into a prompt, and instruct the LLM to select the single response that is most consistent (or meets another instruction like 'most detailed'). The LLM returns an index of the chosen response; no external reranker or execution results required.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (GSM8K, MATH), Code generation (BIRD-SQL, ARCADE), Long-context summarization (GovReport, SummScreen), Open-ended QA (TruthfulQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various benchmarks: GSM8K and MATH for math reasoning; BIRD-SQL and ARCADE for code/text-to-SQL and Python code generation; GovReport and SummScreen for long-context summarization; TruthfulQA for open-ended truthful QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GSM8K PaLM 2-L USC 90.2% accuracy; MATH PaLM 2-L USC 37.4%; GSM8K gpt-3.5-turbo USC 77.8% (MATH 38.1%). BIRD-SQL (gpt-3.5-turbo) USC execution accuracy 45.5%, valid-efficiency 48.8. ARCADE USC execution accuracy 30.1%. GovReport (PaLM 2-L) USC ROUGE-1 40.2, ROUGE-2 17.4, ROUGE-Lsum 35.1, BERTScore 62.8. SummScreen USC ROUGE-1 31.7, ROUGE-2 7.8, ROUGE-Lsum 19.8. TruthfulQA PaLM 2-L USC GPT-judge 67.7, GPT-info 99.0; gpt-3.5-turbo USC GPT-judge 82.5, GPT-info 99.6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy decoding and random selection baselines reported: GSM8K PaLM 2-L greedy 85.7%, random 82.9%; MATH greedy 30.8, random 28.0. BIRD-SQL greedy 42.4, random 41.9. GovReport greedy ROUGE-1 38.8, SummScreen greedy ROUGE-1 30.6. TruthfulQA PaLM 2-L greedy GPT-judge 62.1; gpt-3.5-turbo greedy 79.8.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: concatenate sampled outputs and prompt the LLM itself to judge/select the most consistent (or most detailed) response; selection is performed by the same (or another) LLM via a short answer (index). No external modules required; for code, no execution-based checking is required.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements across tasks versus greedy/random baselines (see above). USC matches standard self-consistency on many tasks (e.g., GSM8K PaLM 2-L SC 90.4 vs USC 90.2), matches execution-based voting on code tasks, and improves summarization and TruthfulQA where SC is inapplicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>USC is limited by model context length (concatenating many candidates increases prompt length); lacks inherent confidence/uncertainty estimation; requires an extra LLM query (inference cost); selection-by-consistency may choose a consistent but incorrect answer (gap to oracle selection reported); performance can degrade with many samples in some tasks (GSM8K decreased at 16 samples), and USC is only an imperfect approximation of SC (match ratio drops when number of candidates increases). Position/order bias studied and found minimal on average but can exist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to greedy, random, standard self-consistency (SC), and execution-based self-consistency (SC-Exec). USC generally outperforms greedy/random, matches or closely matches SC on extractable-answer tasks, and matches SC-Exec on code tasks despite not using execution outputs; for summarization and TruthfulQA (free-form) USC outperforms baselines and SC is not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Order shuffling: performance stable across 5 random orders (small SDs reported). Number of responses: USC benefits from more samples up to a point (improvements on TruthfulQA and BIRD-SQL), but GSM8K degraded at 16 samples; 8 samples considered a practical sweet spot. Selection criterion ablation: prompting for 'most detailed' summary improved GovReport ROUGE-1 from 40.2 to 42.4 and ROUGE-Lsum from 35.1 to 36.9.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8955.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (chain-of-thought voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple chain-of-thought reasoning traces and takes a majority vote on the final extracted answers to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (used with chain-of-thought); in this paper PaLM 2-L and gpt-3.5-turbo were evaluated under SC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SC is a decoding/aggregation method applied to chain-of-thought outputs from LLMs; no single model size inherently required by the method itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple chain-of-thought reasoning chains, extract the final answer from each (via answer parsing/regex), and choose the most frequent final answer (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (GSM8K, MATH), other tasks with extractable final answers</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks with closed-form extractable answers where majority-vote aggregation is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PaLM 2-L SC: GSM8K 90.4% accuracy, MATH 37.9%. gpt-3.5-turbo SC: GSM8K 78.5%, MATH 38.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy PaLM 2-L GSM8K 85.7%, MATH 30.8%; gpt-3.5-turbo greedy GSM8K 73.4%, MATH 33.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling chain-of-thought with temperature >0 and aggregating final answers via exact-match extraction (regex or parser), then majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Direct numeric improvements over greedy decoding reported (see performance_with_reflection vs performance_without_reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Applicable only when final answers can be parsed/extracted into comparable closed-form outputs (e.g., numbers or canonical strings); not applicable to free-form generations like summarization or open-ended QA; depends on reliable answer extraction/parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SC generally improves over greedy/random; USC approximates SC when extractable answers exist and sometimes matches SC performance without needing answer parsing. SC-Exec (execution-based) is a related variant used for code.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8955.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC-Exec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution-based Self-Consistency (code voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For code generation, execute sampled programs on inputs and cluster programs that produce the same execution outputs; select program from largest cluster as final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method applied to LLM-produced programs; the paper compares execution-based selection to USC using the same generated candidate set.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Execution-based self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Execute each candidate program on provided inputs, group by execution output, and pick the group with the most members; optionally generate extra test cases if inputs are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation: BIRD-SQL, ARCADE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>BIRD-SQL: text-to-SQL generation benchmark; ARCADE: Python code generation in data science notebooks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>BIRD-SQL SC-Exec execution accuracy 45.6% (valid-efficiency 48.1). ARCADE SC-Exec strict 29.8%, fuzzy match 30.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy BIRD-SQL execution accuracy 42.4%; ARCADE greedy 26.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External execution of programs to provide an objective clustering signal (execution outputs) for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>SC-Exec improved execution accuracy over greedy baselines (tables show improvements). USC matched SC-Exec performance despite not using execution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires the ability to execute generated code and reliable test inputs; when input testcases are insufficient to distinguish semantics, extra test generation is needed; not applicable to non-executable free-form tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to USC (which achieved similar performance without execution) and to greedy/random baselines; SC-Exec provides a more objective clustering signal but requires tool access (executor).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8955.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yang2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reported approach where LLMs iteratively improve solutions by treating the LLM as an optimizer and producing better solutions given prior trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative LLM-based optimization / generate-and-improve</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given a trajectory of previously generated solutions, the LLM iteratively generates improved solutions for optimization tasks (e.g., prompt optimization and classic optimization problems).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt optimization and classic mathematical optimization tasks (as reported in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Optimization tasks where iterative refinement can improve objective values or solution quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Iterative generation using LLM autoregressively guided by previous candidate trajectories (prompting-based loop).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as demonstrating effectiveness on prompt optimization and several optimization tasks (no numeric results provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not directly compared in this paper; positioned as a work where the LLM generates improved outputs iteratively, unlike USC which selects among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8955.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yoran2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answering questions by meta-reasoning over multiple chains of thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that aggregates multiple reasoning chains and prompts the LLM to produce a better final response via meta-reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Answering questions by meta-reasoning over multiple chains of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Meta-reasoning / aggregation + refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Aggregate multiple chain-of-thought traces and prompt the model to meta-reason over them to synthesize an improved final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-hop question answering (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step reasoning where combining multiple chains can produce higher-quality answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting that conditions on multiple generated reasoning chains to produce a refined answer.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as showing performance improvement on multi-hop QA (no numeric values given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related to USC in using multiple candidates, but differs in that it synthesizes a new answer rather than selecting an existing one.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8955.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Singhal2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards expert-level medical question answering with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work where aggregation of multiple reasoning chains / prompting strategies was used to improve medical QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards expert-level medical question answering with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Aggregation + LLM refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Aggregate multiple candidate reasoning chains and prompt the LLM to generate an improved final medical answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medical question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>High-stakes medical QA where quality and accuracy are crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based aggregation and refinement using LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as showing improvement in the cited work (no numeric details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Differs from USC by synthesizing refined answers rather than selecting among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8955.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gou2023-Critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work demonstrating LLM self-correction by interacting with external tools for critique and correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing / self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use LLMs together with tool interactions to critique and correct their outputs, enabling self-correction beyond pure prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Interactive critique loop involving tools (external execution/evidence) and LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as showing LLMs can self-correct using tool interactions (no numeric details in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Provides a complementary approach to USC and SC by leveraging external tools rather than only LLM internal selection.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8955.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huang2023b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models cannot self-correct reasoning yet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work reporting weaknesses in LLMs' ability to self-correct their own reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models cannot self-correct reasoning yet</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-correction / self-reflection evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Empirical analysis showing failures or limitations when LLMs attempt to self-correct reasoning mistakes without external guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Analysis of LLM self-correction behavior under various prompting setups.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as evidence that LLMs sometimes incorrectly judge correctness and struggle to self-correct reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used in this paper to motivate that assessing consistency may be an easier task than judging absolute correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8955.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8955.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evaluation (e.g., GptScore, G-eval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs themselves as evaluators/raters of generated text, sometimes fine-tuned on human feedback; can be used for selection or scoring of candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family evaluators (GPT-3 variants, GPT-4 referenced elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Examples include GPT-3 models fine-tuned for judgement tasks (GPT-judge, GPT-info) used as metrics in this paper; other works use GPT-4 or GPT-based evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LLM-based evaluation / selection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use an LLM to score or rate candidate outputs (e.g., truthfulness, informativeness) and select highest-scoring answers; can be used to implement selection or self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA evaluation (GPT-judge, GPT-info used here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>TruthfulQA is a benchmark of 817 open-ended questions testing model truthfulness and informativeness; GPT-judge outputs binary truthfulness, GPT-info binary informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Used as the evaluation metric: GPT-judge/GPT-info rates the answers; USC improved GPT-judge and GPT-info scores vs baselines (see USC entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting or fine-tuning LLMs to output evaluation judgements; here GPT-judge and GPT-info (GPT-3 models fine-tuned on human feedback) used as external evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper uses these evaluators to show USC improves judged truthfulness/informativeness; also notes that LLM-evaluators sometimes correlate better with humans than automatic metrics but can exhibit biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM evaluators may not correlate perfectly with human judgments, can be biased towards model-generated texts, and can have selection/position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with trained neural rerankers and human-label-based ranking systems; USC differs by using the generator LLM to perform selection rather than a separate trained ranker.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Answering questions by meta-reasoning over multiple chains of thought <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8955",
    "paper_id": "paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "USC",
            "name_full": "Universal Self-Consistency",
            "brief_description": "A selection-based self-reflection method that asks the same LLM which of several sampled outputs is the most consistent (or meets another selection criterion) and uses that selection as the final answer; applicable to free-form generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L, gpt-3.5-turbo",
            "model_description": "Instruction-tuned PaLM 2-L and OpenAI gpt-3.5-turbo were used in experiments; temperatures 0.6 (PaLM 2-L) and 1.0 (gpt-3.5-turbo); 8 samples typically generated per example unless specified.",
            "reflection_method_name": "Universal Self-Consistency (USC)",
            "reflection_method_description": "Sample multiple model responses, concatenate them into a prompt, and instruct the LLM to select the single response that is most consistent (or meets another instruction like 'most detailed'). The LLM returns an index of the chosen response; no external reranker or execution results required.",
            "task_name": "Mathematical reasoning (GSM8K, MATH), Code generation (BIRD-SQL, ARCADE), Long-context summarization (GovReport, SummScreen), Open-ended QA (TruthfulQA)",
            "task_description": "Various benchmarks: GSM8K and MATH for math reasoning; BIRD-SQL and ARCADE for code/text-to-SQL and Python code generation; GovReport and SummScreen for long-context summarization; TruthfulQA for open-ended truthful QA.",
            "performance_with_reflection": "GSM8K PaLM 2-L USC 90.2% accuracy; MATH PaLM 2-L USC 37.4%; GSM8K gpt-3.5-turbo USC 77.8% (MATH 38.1%). BIRD-SQL (gpt-3.5-turbo) USC execution accuracy 45.5%, valid-efficiency 48.8. ARCADE USC execution accuracy 30.1%. GovReport (PaLM 2-L) USC ROUGE-1 40.2, ROUGE-2 17.4, ROUGE-Lsum 35.1, BERTScore 62.8. SummScreen USC ROUGE-1 31.7, ROUGE-2 7.8, ROUGE-Lsum 19.8. TruthfulQA PaLM 2-L USC GPT-judge 67.7, GPT-info 99.0; gpt-3.5-turbo USC GPT-judge 82.5, GPT-info 99.6.",
            "performance_without_reflection": "Greedy decoding and random selection baselines reported: GSM8K PaLM 2-L greedy 85.7%, random 82.9%; MATH greedy 30.8, random 28.0. BIRD-SQL greedy 42.4, random 41.9. GovReport greedy ROUGE-1 38.8, SummScreen greedy ROUGE-1 30.6. TruthfulQA PaLM 2-L greedy GPT-judge 62.1; gpt-3.5-turbo greedy 79.8.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: concatenate sampled outputs and prompt the LLM itself to judge/select the most consistent (or most detailed) response; selection is performed by the same (or another) LLM via a short answer (index). No external modules required; for code, no execution-based checking is required.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative improvements across tasks versus greedy/random baselines (see above). USC matches standard self-consistency on many tasks (e.g., GSM8K PaLM 2-L SC 90.4 vs USC 90.2), matches execution-based voting on code tasks, and improves summarization and TruthfulQA where SC is inapplicable.",
            "limitations_or_failure_cases": "USC is limited by model context length (concatenating many candidates increases prompt length); lacks inherent confidence/uncertainty estimation; requires an extra LLM query (inference cost); selection-by-consistency may choose a consistent but incorrect answer (gap to oracle selection reported); performance can degrade with many samples in some tasks (GSM8K decreased at 16 samples), and USC is only an imperfect approximation of SC (match ratio drops when number of candidates increases). Position/order bias studied and found minimal on average but can exist.",
            "comparison_to_other_methods": "Compared directly to greedy, random, standard self-consistency (SC), and execution-based self-consistency (SC-Exec). USC generally outperforms greedy/random, matches or closely matches SC on extractable-answer tasks, and matches SC-Exec on code tasks despite not using execution outputs; for summarization and TruthfulQA (free-form) USC outperforms baselines and SC is not applicable.",
            "ablation_study_results": "Order shuffling: performance stable across 5 random orders (small SDs reported). Number of responses: USC benefits from more samples up to a point (improvements on TruthfulQA and BIRD-SQL), but GSM8K degraded at 16 samples; 8 samples considered a practical sweet spot. Selection criterion ablation: prompting for 'most detailed' summary improved GovReport ROUGE-1 from 40.2 to 42.4 and ROUGE-Lsum from 35.1 to 36.9.",
            "uuid": "e8955.0",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency (chain-of-thought voting)",
            "brief_description": "A method that samples multiple chain-of-thought reasoning traces and takes a majority vote on the final extracted answers to improve correctness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Large language models (used with chain-of-thought); in this paper PaLM 2-L and gpt-3.5-turbo were evaluated under SC.",
            "model_description": "SC is a decoding/aggregation method applied to chain-of-thought outputs from LLMs; no single model size inherently required by the method itself.",
            "reflection_method_name": "Self-Consistency (SC)",
            "reflection_method_description": "Generate multiple chain-of-thought reasoning chains, extract the final answer from each (via answer parsing/regex), and choose the most frequent final answer (majority vote).",
            "task_name": "Mathematical reasoning (GSM8K, MATH), other tasks with extractable final answers",
            "task_description": "Benchmarks with closed-form extractable answers where majority-vote aggregation is possible.",
            "performance_with_reflection": "PaLM 2-L SC: GSM8K 90.4% accuracy, MATH 37.9%. gpt-3.5-turbo SC: GSM8K 78.5%, MATH 38.0%.",
            "performance_without_reflection": "Greedy PaLM 2-L GSM8K 85.7%, MATH 30.8%; gpt-3.5-turbo greedy GSM8K 73.4%, MATH 33.2%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Sampling chain-of-thought with temperature &gt;0 and aggregating final answers via exact-match extraction (regex or parser), then majority voting.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Direct numeric improvements over greedy decoding reported (see performance_with_reflection vs performance_without_reflection).",
            "limitations_or_failure_cases": "Applicable only when final answers can be parsed/extracted into comparable closed-form outputs (e.g., numbers or canonical strings); not applicable to free-form generations like summarization or open-ended QA; depends on reliable answer extraction/parsing.",
            "comparison_to_other_methods": "SC generally improves over greedy/random; USC approximates SC when extractable answers exist and sometimes matches SC performance without needing answer parsing. SC-Exec (execution-based) is a related variant used for code.",
            "ablation_study_results": null,
            "uuid": "e8955.1",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SC-Exec",
            "name_full": "Execution-based Self-Consistency (code voting)",
            "brief_description": "For code generation, execute sampled programs on inputs and cluster programs that produce the same execution outputs; select program from largest cluster as final prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (evaluated in this paper)",
            "model_description": "Method applied to LLM-produced programs; the paper compares execution-based selection to USC using the same generated candidate set.",
            "reflection_method_name": "Execution-based self-consistency",
            "reflection_method_description": "Execute each candidate program on provided inputs, group by execution output, and pick the group with the most members; optionally generate extra test cases if inputs are insufficient.",
            "task_name": "Code generation: BIRD-SQL, ARCADE",
            "task_description": "BIRD-SQL: text-to-SQL generation benchmark; ARCADE: Python code generation in data science notebooks.",
            "performance_with_reflection": "BIRD-SQL SC-Exec execution accuracy 45.6% (valid-efficiency 48.1). ARCADE SC-Exec strict 29.8%, fuzzy match 30.3%.",
            "performance_without_reflection": "Greedy BIRD-SQL execution accuracy 42.4%; ARCADE greedy 26.0%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External execution of programs to provide an objective clustering signal (execution outputs) for selection.",
            "number_of_iterations": null,
            "evidence_for_improvement": "SC-Exec improved execution accuracy over greedy baselines (tables show improvements). USC matched SC-Exec performance despite not using execution.",
            "limitations_or_failure_cases": "Requires the ability to execute generated code and reliable test inputs; when input testcases are insufficient to distinguish semantics, extra test generation is needed; not applicable to non-executable free-form tasks.",
            "comparison_to_other_methods": "Compared to USC (which achieved similar performance without execution) and to greedy/random baselines; SC-Exec provides a more objective clustering signal but requires tool access (executor).",
            "ablation_study_results": null,
            "uuid": "e8955.2",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Yang2023",
            "name_full": "Large language models as optimizers",
            "brief_description": "A reported approach where LLMs iteratively improve solutions by treating the LLM as an optimizer and producing better solutions given prior trajectories.",
            "citation_title": "Large language models as optimizers",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (cited work)",
            "model_description": null,
            "reflection_method_name": "Iterative LLM-based optimization / generate-and-improve",
            "reflection_method_description": "Given a trajectory of previously generated solutions, the LLM iteratively generates improved solutions for optimization tasks (e.g., prompt optimization and classic optimization problems).",
            "task_name": "Prompt optimization and classic mathematical optimization tasks (as reported in the cited work)",
            "task_description": "Optimization tasks where iterative refinement can improve objective values or solution quality.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Iterative generation using LLM autoregressively guided by previous candidate trajectories (prompting-based loop).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as demonstrating effectiveness on prompt optimization and several optimization tasks (no numeric results provided in this paper).",
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Not directly compared in this paper; positioned as a work where the LLM generates improved outputs iteratively, unlike USC which selects among candidates.",
            "ablation_study_results": null,
            "uuid": "e8955.3",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Yoran2023",
            "name_full": "Answering questions by meta-reasoning over multiple chains of thought",
            "brief_description": "An approach that aggregates multiple reasoning chains and prompts the LLM to produce a better final response via meta-reasoning.",
            "citation_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (cited work)",
            "model_description": null,
            "reflection_method_name": "Meta-reasoning / aggregation + refinement",
            "reflection_method_description": "Aggregate multiple chain-of-thought traces and prompt the model to meta-reason over them to synthesize an improved final answer.",
            "task_name": "Multi-hop question answering (cited work)",
            "task_description": "Tasks requiring multi-step reasoning where combining multiple chains can produce higher-quality answers.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompting that conditions on multiple generated reasoning chains to produce a refined answer.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as showing performance improvement on multi-hop QA (no numeric values given in this paper).",
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Related to USC in using multiple candidates, but differs in that it synthesizes a new answer rather than selecting an existing one.",
            "ablation_study_results": null,
            "uuid": "e8955.4",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Singhal2023",
            "name_full": "Towards expert-level medical question answering with large language models",
            "brief_description": "A cited work where aggregation of multiple reasoning chains / prompting strategies was used to improve medical QA performance.",
            "citation_title": "Towards expert-level medical question answering with large language models",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (cited work)",
            "model_description": null,
            "reflection_method_name": "Aggregation + LLM refinement",
            "reflection_method_description": "Aggregate multiple candidate reasoning chains and prompt the LLM to generate an improved final medical answer.",
            "task_name": "Medical question answering",
            "task_description": "High-stakes medical QA where quality and accuracy are crucial.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-based aggregation and refinement using LLM capabilities.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as showing improvement in the cited work (no numeric details provided in this paper).",
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Differs from USC by synthesizing refined answers rather than selecting among candidates.",
            "ablation_study_results": null,
            "uuid": "e8955.5",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Gou2023-Critic",
            "name_full": "CRITIC: Large language models can self-correct with tool-interactive critiquing",
            "brief_description": "A cited work demonstrating LLM self-correction by interacting with external tools for critique and correction.",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (cited work)",
            "model_description": null,
            "reflection_method_name": "Tool-interactive critiquing / self-correction",
            "reflection_method_description": "Use LLMs together with tool interactions to critique and correct their outputs, enabling self-correction beyond pure prompting.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Interactive critique loop involving tools (external execution/evidence) and LLM prompts.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as showing LLMs can self-correct using tool interactions (no numeric details in this paper).",
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Provides a complementary approach to USC and SC by leveraging external tools rather than only LLM internal selection.",
            "ablation_study_results": null,
            "uuid": "e8955.6",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Huang2023b",
            "name_full": "Large language models cannot self-correct reasoning yet",
            "brief_description": "A cited work reporting weaknesses in LLMs' ability to self-correct their own reasoning traces.",
            "citation_title": "Large language models cannot self-correct reasoning yet",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (cited work)",
            "model_description": null,
            "reflection_method_name": "Self-correction / self-reflection evaluation",
            "reflection_method_description": "Empirical analysis showing failures or limitations when LLMs attempt to self-correct reasoning mistakes without external guidance.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Analysis of LLM self-correction behavior under various prompting setups.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": "Cited as evidence that LLMs sometimes incorrectly judge correctness and struggle to self-correct reasoning.",
            "comparison_to_other_methods": "Used in this paper to motivate that assessing consistency may be an easier task than judging absolute correctness.",
            "ablation_study_results": null,
            "uuid": "e8955.7",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-eval",
            "name_full": "LLM-based evaluation (e.g., GptScore, G-eval)",
            "brief_description": "Using LLMs themselves as evaluators/raters of generated text, sometimes fine-tuned on human feedback; can be used for selection or scoring of candidates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-family evaluators (GPT-3 variants, GPT-4 referenced elsewhere)",
            "model_description": "Examples include GPT-3 models fine-tuned for judgement tasks (GPT-judge, GPT-info) used as metrics in this paper; other works use GPT-4 or GPT-based evaluators.",
            "reflection_method_name": "LLM-based evaluation / selection",
            "reflection_method_description": "Use an LLM to score or rate candidate outputs (e.g., truthfulness, informativeness) and select highest-scoring answers; can be used to implement selection or self-reflection.",
            "task_name": "TruthfulQA evaluation (GPT-judge, GPT-info used here)",
            "task_description": "TruthfulQA is a benchmark of 817 open-ended questions testing model truthfulness and informativeness; GPT-judge outputs binary truthfulness, GPT-info binary informativeness.",
            "performance_with_reflection": "Used as the evaluation metric: GPT-judge/GPT-info rates the answers; USC improved GPT-judge and GPT-info scores vs baselines (see USC entry).",
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompting or fine-tuning LLMs to output evaluation judgements; here GPT-judge and GPT-info (GPT-3 models fine-tuned on human feedback) used as external evaluators.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper uses these evaluators to show USC improves judged truthfulness/informativeness; also notes that LLM-evaluators sometimes correlate better with humans than automatic metrics but can exhibit biases.",
            "limitations_or_failure_cases": "LLM evaluators may not correlate perfectly with human judgments, can be biased towards model-generated texts, and can have selection/position bias.",
            "comparison_to_other_methods": "Contrasted with trained neural rerankers and human-label-based ranking systems; USC differs by using the generator LLM to perform selection rather than a separate trained ranker.",
            "ablation_study_results": null,
            "uuid": "e8955.8",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "rating": 2
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.01779875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Universal Self-Consistency for Large Language Model Generation</h1>
<p>Xinyun Chen ${ }^{<em>}$ Renat Aksitov</em> Uri Alon Jie Ren Kefan Xiao Pengcheng Yin<br>Sushant Prakash Charles Sutton Xuezhi Wang Denny Zhou<br>Google</p>
<h4>Abstract</h4>
<p>Self-consistency with chain-of-thought prompting ( CoT ) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, selfconsistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have accomplished significant breakthroughs in a wide variety of domains, including mathematical reasoning (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022), code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), and other text generation tasks (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023). Despite the rapid progress, the LLM-generated responses are still prone to errors when they get long. A long line of efforts have been devoted to improve the output quality by sampling multiple model responses and then selecting the final output based on certain criteria. For example, prior works have trained neural networks to rerank model outputs (Cobbe et al., 2021; Li et al., 2023b; Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), and more recent works investigate using LLMs to score the responses (Fu et al., 2023; Liu et al., 2023; Wang et al., 2023a).</p>
<p>In this work, we consider the consistency among model responses as the criterion to select the model output, a generic metric that has enabled huge performance leaps in reasoning (Wang et al., 2022) and code generation (Li et al., 2022; Shi et al., 2022). In particular, self-consistency (Wang et al., 2022) with chain-of-thought prompting (Wei et al., 2022) boosts the performance on various benchmarks, by marginalizing latent reasoning paths through sampling which leads to select the final answer as the most common one. However, self-consistency can only be applied to tasks where the final answer can be aggregated via exact match, e.g., a single number for math problems.</p>
<p>To address this major limitation of self-consistency, we propose Universal Self-Consistency (USC) to support various applications, especially free-form generation tasks. Specifically, given multiple candidate responses, USC simply calls the LLM to select the most consistent response among them as the final output. Thus, USC eliminates the need of designing an answer extraction process, and is applicable to tasks with free-form answers. Although prior works have revealed weaknesses of LLMs for response selection, such as position bias (Wang et al., 2023b; Zheng et al., 2023b) and incorrectly judging the answer correctness (Huang et al., 2023b; Gou et al., 2023), intuitively, assessing the consistency among candidate answers is easier than measuring and comparing the answer quality.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We evaluate universal self-consistency on a wide range of tasks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks for math problem solving, USC generally matches the performance of the standard self-consistency. On programming tasks including text-to-SQL generation (Li et al., 2023a) and Python code generation (Yin et al., 2023), USC matches the performance of execution-based consistency (Li et al., 2022; Shi et al., 2022), while USC does not require execution results to aggregate over candidate programs. Finally, USC also improves the performance for open-ended question answering (Lin et al., 2021) and long-context summarization (Huang et al., 2021; Chen et al., 2022b), where the standard self-consistency is not applicable. In addition to the performance gain, our evaluation also demonstrates that USC outputs highly match those of the standard self-consistency when the comparison is applicable, while it is robust to the ordering of candidate responses.</p>
<h1>2 BACKGROUND: SELF-CONSISTENCY</h1>
<p>Self-consistency (Wang et al., 2022) augments chain-of-thought prompting (Wei et al., 2022) by sampling multiple reasoning chains and then taking a majority vote on the final answer set. The intuition is that sometimes the greedily decoded reasoning process might not be the optimal one, hence it makes more sense to sample a diverse set of reasoning chains, and if some of them lead to the same answer, then we have a higher confidence that this consistent answer is the correct one. It has been shown that self-consistency improves the greedy chain-of-thought prompting by a large margin on a wide set of reasoning tasks.</p>
<p>Besides question answering tasks, consistency-based answer selection has also been applied to code generation (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), which requires code execution. Specifically, we first execute all predicted programs on the given inputs, then programs with the same execution outputs are clustered together, assuming that they are semantically equivalent. Finally, we select the program belonging to the largest cluster as the final prediction. When the program inputs given in the task description are insufficient to distinguish between different predictions, this execution-based code selection is also often accompanied with a test case generation process to better examine the consistency (Li et al., 2022; Chen et al., 2022a; Huang et al., 2023a).</p>
<p>Despite the remarkable improvement, self-consistency is only applicable to problems with a unique and closed-form answer, e.g., when the final answer consists of a single number, because a majority vote needs to be taken over the final answer set. This significant requirement poses a challenge for tasks that require open-ended generations, such as summarization, creative writing, and open-ended question answering.</p>
<h2>3 UNIVERSAL SELF-CONSISTENCY</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the Universal Self-Consistency workflow.
We present the overall workflow of universal self-consistency (USC) in Figure 1, which utilizes LLMs to enable self-consistency for a wide variety of tasks, especially free-form text generation. First, we sample multiple responses with the large language model. Afterward, to select one model response as the final answer, we concatenate all responses together, and then construct a prompt with an instruction asking the language model to select the most consistent response. In this way, USC obviates the necessity of counting the exact answer frequency as in the standard self-consistency, and relies on the LLM's own ability to measure the consistency among different responses. Although prior works show that LLMs sometimes have trouble evaluating the prediction correctness (Huang</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of Universal Self-Consistency for answer selection from responses of diverse formats: (a) mathematical reasoning; and (b) open-ended question answering. Note that for the given open-ended question, the final answer is an entity list, where no two responses share the same predictions. Still, the LLM correctly selects the response where the individual entities in the predicted list appear most frequently in the candidate responses.
et al., 2023b; Gou et al., 2023), especially for reasoning problems, empirically we observe that LLMs are generally able to examine the response consistency across multiple tasks.</p>
<p>Consistency assessment with LLMs offers more flexibility for free-form generation. Figure 2 demonstrates example tasks where different consistency criteria are beneficial for response selection. Specifically, Figure 2a shows different model responses for a math problem, where the output formats are diverse and thus makes it challenging for rule-based methods to extract answers. Nonetheless, assuming that the final answers are correctly extracted, the consistency criterion still follows the standard self-consistency on mathematical reasoning, which is based on the exact match of the final answers represented as single numerical values. On the other hand, Figure 2b shows an example question where the final answer is an entity list. Despite that there is no response that is consistent with others based on the exact match, the LLM selects the response where each of the predicted entities appears most frequently among the candidate outputs. In Section 4, we further show that LLM can also examine the consistency among responses beyond the question answering tasks, including code generation without access to the execution outputs, and long-context summarization.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Evaluation Setup</h3>
<p>Benchmarks. We evaluate USC on the following variety of tasks:</p>
<ul>
<li>Mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), a dataset of 8,500 grade school math word problems, and MATH (Hendrycks et al., 2021), a dataset of 12,500 challenging mathematics problems from high school competitions.</li>
<li>Code generation benchmarks, including BIRD-SQL dataset (Li et al., 2023a) for text-to-SQL generation, and ARCADE dataset (Yin et al., 2023) for Python code generation in data science notebooks.</li>
<li>Long-context summarization, including the GovReport and SummScreen benchmarks from ZeroSCROLLS (Shaham et al., 2023). In GovReport (Huang et al., 2021), each input is a document containing $\sim 7,900$ words on average, and the reference output is an expert-written executive summary with $\sim 500$ words. In SummScreen (Chen et al., 2022b), every input is a transcript of a TV show episode with $\sim 5,600$ words, and each reference output is a $\sim 100$ words human-written recap of the episode. We follow Shaham et al. (2023) and measure ROUGE 1, ROUGE 2, and ROUGE-Lsum which measure n-gram overlap with the reference summary, and we also measure BERTScore F1 (Zhang et al., 2019).</li>
<li>TruthfulQA (Lin et al., 2021) benchmark for open-ended question answering, which contains 817 questions to test model's ability in generating truthful answers. To evaluate the answer's quality, we use the GPT-judge and GPT-info, which are GPT-3 models fine-tuned on human feedback data, provided by Lin et al. (2021). GPT-judge model outputs a binary rating for truthfulness, and GPT-info model outpus a binary rating for informativeness. It is shown that the GPT-3 models have higher accuracy in predicting human judgement than the automatic metrics ROUGE, BLEU, BLEURT.</li>
</ul>
<p>Decoding schemes. We compare USC to the following decoding schemes:</p>
<ul>
<li>Greedy decoding generates a single answer with the temperature 0 .</li>
<li>Random selects one answer randomly from multiple samples with temperature $&gt;0$.</li>
<li>SC (Wang et al., 2022) is the standard self-consistency decoding with answer extraction. We evaluate SC whenever applicable; for example, on reasoning benchmarks where the final answers can be compared through exact match.</li>
</ul>
<p>To enable a fair comparison, for sampling schemes (i.e., except greedy decoding), we always select the final answer from the same set of initial model responses. For code generation, we compare our approach to execution-based self-consistency (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), where we select the code with the most common execution result. Both USC and execution-based self-consistency first filter out syntactically invalid candidate programs, and then perform the voting over the remaining ones. For ARCADE benchmark, we also evaluate a variant of the execution-based self-consistency with fuzzy matching as described in Yin et al. (2023), which implements a set of heuristics to determine whether the execution outputs of two programs are equivalent when they are not exact match.</p>
<p>Implementation details. We conduct experiments using instruction-tuned PaLM 2-L (Anil et al., 2023) and gpt-3.5-turbo models. Unless otherwise specified, the LLM generates 8 initial samples for both SC and USC. For mathematical reasoning, summarization and the ARCADE benchmark for Python code generation, the initial samples are generated with zero-shot prompting, thus the output formats are diverse. For BIRD-SQL, we used the 1-shot chain-of-thought prompt in Li et al. (2023a), which improves the performance. We also utilized a one-shot prompt for TruthfulQA to improve the quality of candidate responses. We set the temperature to be 0.6 for PaLM 2-L, and 1.0 for gpt-3.5-turbo.</p>
<h1>4.2 Main ReSults</h1>
<p>Mathematical reasoning. For mathematical reasoning benchmarks, we compare USC against the standard self-consistency in Table 1. For the standard self-consistency, we employ a regular expression matching to extract the final answer on GSM8K, and re-use the answer parsing code from (Zheng et al., 2023a) for MATH. Overall, USC consistently improves over the greedy decoding and random selection, and the performance is generally comparable to the standard self-consistency, which USC does not need answer parsing to perform the voting.</p>
<p>Code generation. Table 2 presents the results on BIRD-SQL and ARCADE respectively. On BIRD-SQL, besides the execution accuracy, we follow (Li et al., 2023a) to also evaluate the valid efficiency score, which measures the efficiency of the generated SQL queries. We show that USC matches the execution-based self-consistency performance on both benchmarks, while USC does not utilize code execution to perform the voting.</p>
<p>Summarization. Results for the summarization benchmarks are shown in Table 3. Since the generated summaries are in free-form, the standard self-consistency is not applicable. In GovReport, USC consistently improves over the baselines across all metrics. In Section 4.3, we further show that asking the model to choose the most detailed summary results in more performance gain.</p>
<p>TruthfulQA. Table 4 presents results on TruthfulQA, where SC is also not directly applicable because the generated answers are in free-form. Comparing with greedy decoding and random selection, USC-based answers have the highest truthfulness with both PaLM 2-L and gpt-3.5turbo. For informativeness which is considered as a secondary objective, USC-based answers have the highest score on PaLM 2-L and the second highest score ( 0.1 lower than the highest) on gpt-3.5-turbo. Considering that GPT-judge and GPT-info models have generally 90-95\% validation accuracy on rating prediction (Lin et al., 2021), the 0.1 difference is not considered significant.</p>
<p>Table 1: Accuracy on mathematical reasoning benchmarks. USC and SC consistently improve over the greedy decoding and random selection. USC performance is generally comparable to SC.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}$</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on code generation benchmarks with gpt-3.5-turbo.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BIRD-SQL</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">ARCADE</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Results on long-context summarization benchmarks with PaLM 2-L. Since the outputs are in free-form, the standard self-consistency is not applicable. USC consistently improves over the baselines on summary quality.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Approach</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-Lsum</th>
<th>BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td>GovReport</td>
<td>Greedy decoding</td>
<td>38.8</td>
<td>16.9</td>
<td>33.8</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>38.5</td>
<td>16.9</td>
<td>33.6</td>
<td>62.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{4 0 . 2}$</td>
<td>$\mathbf{1 7 . 4}$</td>
<td>$\mathbf{3 5 . 1}$</td>
<td>$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td>SummScreen</td>
<td>Greedy decoding</td>
<td>30.6</td>
<td>7.5</td>
<td>19.1</td>
<td>$\mathbf{5 8 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>30.2</td>
<td>7.3</td>
<td>19.0</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{3 1 . 7}$</td>
<td>$\mathbf{7 . 8}$</td>
<td>$\mathbf{1 9 . 8}$</td>
<td>58.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on the TruthfulQA benchmark. Since the answer is in free-form, the standard self-consistency is not applicable. USC overall has the highest truthfulness and informativeness over the baselines.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Approach</th>
<th>GPT-judge</th>
<th>GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td>PaLM 2-L</td>
<td>Greedy decoding</td>
<td>62.1</td>
<td>95.1</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>62.9</td>
<td>94.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{6 7 . 7}$</td>
<td>$\mathbf{9 9 . 0}$</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>Greedy decoding</td>
<td>79.8</td>
<td>$\mathbf{9 9 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>80.6</td>
<td>99.3</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{8 2 . 5}$</td>
<td>99.6</td>
</tr>
</tbody>
</table>
<h1>4.3 Ablations</h1>
<p>Effect of response ordering. Prior works have shown that large language models can be affected by the order of candidate responses when used to evaluate their quality (Wang et al., 2023b; Zheng et al., 2023b). We examine the effect of response ordering by performing USC with 5 different random orders when concatenating all responses, and calculate the mean and standard deviation of the task results. From Table 5, we observe that the overall model performance remains similar with different response orders, suggesting the effect of response order is minimal.</p>
<p>Table 5: USC performance with random shuffling of original responses using PaLM 2-L. The mean and standard deviation are computed with 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(a)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(b)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(c)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">ROUGE-Lsum</td>
<td style="text-align: center;">Metric</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$89.7 \pm 0.3$</td>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">$31.6 \pm 0.3$</td>
<td style="text-align: center;">$19.5 \pm 0.2$</td>
<td style="text-align: center;">GPT-judge</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$37.3 \pm 0.2$</td>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">$40.0 \pm 0.1$</td>
<td style="text-align: center;">$34.9 \pm 0.2$</td>
<td style="text-align: center;">GPT-info</td>
</tr>
</tbody>
</table>
<p>Different number of responses. Next, we examine the effect of using different numbers of responses in USC. As shown in Figure 3, USC consistently benefits from more samples on TruthFulQA and BIRD-SQL. However, USC does not further improve the performance on SummScreen after 5 samples, and the accuracy on GSM8K decreases with 16 samples. This can be due to the weakness in long-context understanding when the prompt contains more candidate responses, and the imperfect counting ability of LLMs. Nevertheless, we consider utilizing a few samples (e.g., 8) a sweet spot to balance the task accuracy and compute cost, in which case USC reliably improves the performance across the board. In Section 4.4, we further compare the predictions from USC and SC to understand how using more candidate responses affects the results.</p>
<p>Criteria for response selection. One advantage of USC is its generality: the same criteria can be applied to various tasks, without any task-specific knowledge. Nonetheless, a minor task-specific</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Results on GSM8K and BIRD-SQL. The top numbers are USC accuracies, and the bottom numbers are the differences to SC accuracies.</p>
<p>Figure 3: USC results with different number of samples.
adaptation of the response selection instruction can further boost USC over the generic prompts. For example, Table 6 shows that asking the LLM to choose the most detailed response (rather than the most consistent one) results in gains of about 2 ROUGE-1 and ROUGE-Lsum points.</p>
<p>Table 6: Ablation on the response selection criterion on long-context summarization benchmarks with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{4 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{3 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
</tbody>
</table>
<h1>4.4 Discussion: How Well does USC Match SC SElection?</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of selections made by USC versus SC with PaLM 2-L. $k$ denotes the number of candidate responses for selection. "Tied votes" represents the case where the USC and SC select different responses, but both have the maximum votes.</p>
<p>We have demonstrated that on tasks where the standard self-consistency is applicable, USC and SC achieve comparable overall performance with 8 samples; however, USC fails to further improve the GSM8K performance with 16 samples. In this section, we look closer into the relationship between USC and SC, specifically how well is the alignment between their selected responses.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy distribution when USC selection doesn't match SC.</p>
<p>Figure 4 presents a breakdown analysis of USC predictions on mathematical reasoning benchmarks with 8 and 16 candidate responses, and Figure 5 further compares the performance of USC and SC when they select different responses. We observe that:</p>
<ul>
<li>The voting ties constitute a notable portion to the selection differences between USC and SC, especially with 8 candidate responses. Specifically, among all responses with the maximum votes, SC always selects the one with the smallest index, while USC can pick up alternative ones based on the response format.</li>
<li>The match ratio between USC and SC consistently surpasses their own task accuracies, which shows that the consistency criterion is easier to measure than the answer correctness.</li>
<li>Shifting from 8 to 16 samples, the USC-SC match ratio reduces, suggesting that USC behaves as an imperfect approximation of SC. However, the difference in response selection does not always lead to the performance decrease, as USC sometimes selects the correct response when SC fails.</li>
</ul>
<h1>5 Related Work</h1>
<p>Response reranking and selection for language models. Reranking is a common method to improve the generation quality in language models by sampling multiple outputs and applying a post-hoc criterion to rank them, which often requires an additional trained ranker and sometimes additional human labeled data. For example, Cobbe et al. (2021) use human labels to train a ranking model to verify whether each generated response is correct or not, and Shen et al. (2021) jointly train a generator and a ranker to improve performance for math tasks. Instead of training response generators and rankers as separate models, Thoppilan et al. (2022) finetune the dialog model to also predict the ratings of candidate responses with human-annotated judgements. For code generation, various reranker models have been designed (Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), which typically utilize execution results and language-specific syntactic features to improve the ranking performance. In contrast with these prior works, USC does not require any additional labeled training data nor an external reranking model: the LLM that generated the initial outputs is the same one that selects the final answer.</p>
<p>Several consistency-based response selection approaches have been proposed in the literature, which typically include a voting procedure to select the most common response (Wang et al., 2022; Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023; Bertsch et al., 2023). Self-consistency (Wang et al., 2022) shows that with multiple responses generated for the same task, selecting the reasoning path leading to the most common final answer improves the chain-of-thought reasoning performance. The candidate responses can also come from different prompt variants corresponding to the same problem (Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023). To measure the pairwise similarity between candidate responses for open-ended generation tasks, Jain et al. (2023) propose the n-gram consistency score, and the consistency score for each response is computed as the sum of the pairwise similarity scores. For code generation, the consistency measurement is typically based on code execution, where the candidate program with the most common execution outputs is selected (Shi et al., 2022; Li et al., 2022; Chen et al., 2019). Besides the consistency of code execution, other works also examine the consistency between the code and the specification (Min</p>
<p>et al., 2023), and utilize it for reranking (Zhang et al., 2023a; Huang et al., 2023a). In this work, we directly instruct the LLM to perform consistency-based selection without an explicit definition of the pairwise similarity, and we demonsrate its applicability to a wide range of tasks.</p>
<p>Response improvement with multiple candidates. Some recent works demonstrate that the LLM can improve its prediction output on top of the candidate responses. Yang et al. (2023) show that given a trajectory of previously generated solutions, the LLM can iteratively produce better solutions for an optimization task, and they demonstrate the effectiveness of this LLM-based optimization process for prompt optimization and several classic mathematical optimization tasks. Other works aggregate multiple reasoning chains and prompts the LLM to generate a better final response, which shows performance improvement on multi-hop question answering (Yoran et al., 2023) and medical question answering (Singhal et al., 2023). Instead of asking the LLM to generate a better response, USC focuses on response selection, as the candidate responses usually already contain high-quality solutions to the underlying tasks. Meanwhile, performing the consistency-based selection is generally an easier task than improving the answer correctness, and we demonstrate that USC properly utilizes multiple responses to improve the performance across different tasks.</p>
<p>Large language models for response evaluation. The underlying assumption in our work is that LLMs are reflective enough to evaluate the consistency between multiple self-generated outputs. This assumption is related to recent findings which had shown that large language models can also be used for evaluating model-generated texts (Bubeck et al., 2023; Fu et al., 2023; Wang et al., 2023a; Zhang et al., 2023b). LLM-based evaluators have demonstrated some promising results, e.g., they can be used to evaluate natural language generations without human references, but some work has also shown that they might not correlate very well with human judgements and sometimes exhibit bias towards model-generated texts (Bubeck et al., 2023; Liu et al., 2023). Another line of work utilizes the prediction probability of the LLM to measure the quality of multiple choices (Ren et al., 2023; Adiwardana et al., 2020), and Lin et al. (2022) show promising results on arithmetic tasks where they prompt the LLM to directly output the level of confidence for its response. In this work, we show that LLMs not only can serve as evaluators, they can also improve their own output by sampling multiple responses and evaluating the consistency between them.</p>
<h1>6 Limitations and Future Work</h1>
<p>Despite that USC supports open-ended generation tasks and generally achieves comparable performance in those domains where the standard self-consistency can be applied, our current USC implementation has its own limitations compared to the extraction-based self-consistency approach.</p>
<p>First, while self-consistency can be applied to an arbitrary number of samples as long as the final answers can be extracted, the number of samples supported by USC is bounded by the context length of the underlying LLM. That said, to seek a balance between the task performance and the sampling cost, in practice the number of generated samples per task is not prohibitively large, thus the context length is generally sufficient to make best use of the samples.</p>
<p>Second, the voting mechanism in self-consistency inherently offers a measure of confidence or uncertainty for each response (Wang et al., 2022). However, universal self-consistency has not yet been developed to include the confidence estimation. We consider developing a calibration mechanism for USC as future work, where we can leverage the LLM to perform output clustering and pairwise self-consistency.</p>
<p>Also, USC requires an additional LLM query by design, which incurs additional inference costs. Given that our USC prompt only requires the LLM to generate a response index corresponding to the final answer, the USC output length is much shorter than any individual candidate response to select from. To further reduce the cost, one direction is to use a light-weight language model to conduct USC, and optimizes its efficiency regarding long-context encoding.</p>
<p>Finally, one common limitation of both the standard self-consistency and USC is about the consistencybased selection criterion. Specifically, although consistency is a generic and effective criterion, the most consistent response is not necessarily the best one. We observe that there is still a notable gap to oracle scores where we assume the access to an oracle reranker that always selects the best response, and we present the full results in Appendix A. In Section 4.3 we demonstrate that we can</p>
<p>design task-specific criteria to further improve the performance, and we consider refining the USC framework to further close the gap to the oracle performance as future work.</p>
<h1>7 CONCLUSION</h1>
<p>In this work, we presented Universal Self-Consistency (USC), which extends the standard selfconsistency to support free-form generation tasks. USC notably boosts the performance in diverse applications, and performs on par with the standard self-consistency on those tasks where answer extraction is feasible for voting. Besides addressing the limitations discussed in Section 6, we also consider mitigating the position bias and improving long-context understanding of LLMs as important future work that can further enhance the effectiveness and robustness of the USC scheme.</p>
<h2>REFERENCES</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew R Gormley. It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk. arXiv preprint arXiv:2310.01387, 2023.</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022a.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8602-8615, 2022b.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. Enhancing large language models in coding through multi-perspective self-consistency. arXiv preprint arXiv:2309.17272, 2023a.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023b.</p>
<p>Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419-1436, 2021.</p>
<p>Siddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. Self-consistency for open-ended generations. arXiv preprint arXiv:2307.06857, 2023.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql. arXiv preprint arXiv:2305.03111, 2023a.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, 2023b.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.</p>
<p>Marcus J Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, and Baishakhi Ray. Beyond accuracy: Evaluating self-consistency of code large language models with identitychain. arXiv preprint arXiv:2310.14053, 2023.</p>
<p>Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. arXiv preprint arXiv:2302.08468, 2023.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2269-2279, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology. org/2021.findings-emnlp.195.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022.</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/ 2201.08239.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. 2022. URL https://arxiv.org/pdf/2201.11903.</p>
<p>Gwenyth Portillo Wightman, Alexandra DeLucia, and Mark Dredze. Strength in numbers: Estimating confidence of large language models by prompt agreement. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 326-362, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Pengcheng Yin and Graham Neubig. Reranking for neural semantic parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.</p>
<p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, and Charles Sutton. Natural language to code generation in interactive data science notebooks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023.</p>
<p>Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. arXiv preprint arXiv:2310.03094, 2023.</p>
<p>Lu Zeng, Sree Hari Krishnan Parthasarathi, and Dilek Hakkani-Tur. N-best hypotheses reranking for text-to-sql systems. arXiv preprint arXiv:2210.10668, 2022.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pp. 41832-41846. PMLR, 2023a.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023b.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023a.
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023b.
Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049, 2022.</p>
<h1>A Comparison to Oracle Selection</h1>
<p>Tables 7, 8, 9, 10 and 11 compare the results of different approaches to the oracle performance, which selects the best response among candidates for each task. The oracle selection is from the same 8 samples as SC and USC. We observe that there is still a notable gap between USC and the oracle performance, and we consider developing ranking methods to bridge this gap across multiple tasks as future work.</p>
<p>Table 7: Comparison to the oracle selection on mathematical reasoning benchmarks. The results were obtained with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">57.2</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison to the oracle selection on BIRD-SQL benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec</td>
<td style="text-align: center;">$\mathbf{4 5 . 6}$</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">55.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison to the oracle selection on ARCADE benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">$\mathbf{3 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">40.5</td>
</tr>
</tbody>
</table>
<h2>B EXAMPLES OF USC PROMPTS</h2>
<p>Figures 6 and 7 present examples of full USC prompts with candidate responses for different tasks.</p>
<p>Table 10: Comparison to the oracle selection on long-context summarization benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">60.6</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison to the oracle selection on TruthfulQA benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">GPT-judge</th>
<th style="text-align: center;">GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">95.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>I have generated the following responses to the question: The three-digit number "ab5" is divisible by 3. How many different three-digit numbers can "ab5" represent?</p>
<p>Response 0: A number is divisible by 3 if the sum of its digits is divisible by 3 . In this case, the sum of the digits of "ab5" must be divisible by 3 , so $\mathrm{a}+\mathrm{b}+5$ must be divisible by 3 .
The smallest possible sum with $\mathrm{a}+\mathrm{b}+5$ is $1+0+5=6$, which is divisible by 3 . The largest possible sum is $9+7+5=21$, which is also divisible by 3 .
Therefore, the possible values of $\mathrm{a}+\mathrm{b}$ can range from 1 to 16 , with each increment of 3 leading to another possible sum. There are 6 such increments $(16-1) / 3+1=6$, so there are 6 possible values for $\mathrm{a}+\mathrm{b}$.
For each possible value of $\mathrm{a}+\mathrm{b}$, there are multiple combinations of digits that add up to that value. For example, if $\mathrm{a}+\mathrm{b}=4$, possible combinations are $(1,3),(2,2),(3,1)$ and $(4,0)$.
Therefore, there are 30 different three-digit numbers that "ab5" can represent. $(1+4+7+9+6+3$ $=30)$
In general, any three-digit number of the form "ab5" where $\mathrm{a}+\mathrm{b}+5$ is divisible by 3 will be divisible by 3 .
Response 1: A number is divisible by 3 if the sum of its digits is divisible by 3 . Since "ab5" has a fixed digit of 5 , the sum of "a" and "b" must be divisible by 3 for the number to be divisible by 3 .
There are 10 possible digits: $0,1,2,3,4,5,6,7,8$, and 9 .
We can consider the cases where the sum of "a" and "b" is $3,6,9,12,15$, or 18 :
Sum of 3: 1 possibility $(1+2)$.
Sum of 6: 2 possibilities $(1+5,2+4)$.
Sum of 9: 3 possibilities $(1+8,2+7,3+6)$.
Sum of 12: 4 possibilities $(3+9,4+8,5+7,6+6)$.
Sum of 15: 4 possibilities $(4+11,5+10,6+9,7+8)$.
Sum of 18: 2 possibilities $(9+9,8+10)$.
Therefore, there are a total of $1+2+3+4+4+2=16$ possibilities for "a" and "b."
However, we need to remember that "a" can take any of the 9 non-zero digits $(1,2,3,4,5,6,7,8,9)$, while "b" can be any of the 10 digits. So, we need to double the total number of possibilities for "a" and "b."
Therefore, the number of different three-digit numbers that "ab5" can represent is 16 possibilities * 2 $=32$.
So the answer is 32 .
Response 2: A number is divisible by 3 if the sum of its digits is also divisible by 3 .
There are 9 possible values for both a and 10 possible values of b, giving us 90 possible combinations. However, only the combinations where $\mathrm{a}+\mathrm{b}+5$ is a multiple of 3 will be valid.
We can list out the possible combinations:
$\mathrm{a}+\mathrm{b}=1:(1,0) \mathrm{a}+\mathrm{b}=4:(1,3),(2,2),(3,1),(4,0) \mathrm{a}+\mathrm{b}=7:(1,6),(2,5),(3,4),(4,3),(5,2),(6$, $1),(7,0) \mathrm{a}+\mathrm{b}=10:(1,9),(2,8),(3,7),(4,6),(5,5),(6,4),(7,3),(8,2),(9,1) \mathrm{a}+\mathrm{b}=13:(4,9),(5$, $8),(6,7),(7,6),(8,5),(9,4) \mathrm{a}+\mathrm{b}=16:(7,9),(8,8),(9,7)$ This gives a total of 30 different pairs of values for a and b. Therefore, there are 30 different three-digit numbers that "ab5" can represent. $1+4+7+9+6+3=30$
I hope this helps! Let me know if you have any other questions.
...
Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 6: A full example USC prompt for mathematical reasoning. This corresponds to the question in Figure 2a.</p>
<p>I have generated the following responses to the question: Where do people drink less coffee than they do in Mexico?</p>
<p>Response 0: There are several countries where people drink less coffee than they do in Mexico. Some examples include Japan, China and the United Kingdom.
It is important to note that coffee consumption can vary among individuals within these countries, and preferences can change depending on different factors such as demographics, urban versus rural areas, and personal taste.
Response 1: People in countries like Japan, China, and India typically drink less coffee than they do in Mexico.</p>
<ol>
<li>Japan: Tea, particularly green tea, is more popular than coffee in Japan. Traditional tea ceremonies are deeply rooted in Japanese culture, and tea is often consumed throughout the day.</li>
<li>China: Tea is also the preferred hot beverage in China. Chinese tea culture dates back thousands of years, and various types of tea, like green, black, and oolong, are consumed regularly. Coffee consumption, although growing, is still relatively low compared to other countries.</li>
<li>India: India has a strong culture of tea consumption. Chai, which is a spiced milk tea, is a staple beverage across the country. While coffee is gaining popularity in some urban areas, tea remains the beverage of choice for most Indians.
Please note that coffee consumption can vary within each country, and these rankings are based on general trends and cultural preferences.
Response 2: There are several countries where people generally drink less coffee compared to Mexico. Some of these countries include:</li>
<li>Japan: While coffee has gained popularity in recent years, traditional tea culture remains strong in Japan, resulting in lower coffee consumption overall.</li>
<li>China: Despite having a large population, tea is the preferred hot beverage in China, and coffee consumption is relatively low.</li>
<li>Saudi Arabia: Traditional Arabic coffee (qahwa) is popular, but in terms of overall coffee consumption, it is lower compared to Mexico.</li>
<li>India: Tea is the dominant beverage in India, and coffee consumption is significantly lower in comparison.
It's essential to note that coffee consumption varies among individuals within a country, and this list represents a general comparison.
...</li>
</ol>
<p>Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 7: A full example USC prompt for TruthfulQA. This corresponds to the question in Figure 2b.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3806 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3806</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3806</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-258676639</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.07340v1.pdf" target="_blank">MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine</a></p>
                <p><strong>Paper Abstract:</strong> METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both multiple-turn medical dialogue and case report scenarios.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3806.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3806.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedGPTEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedGPTEval evaluation system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive evaluation system introduced in this paper to assess LLM responses in medical scenarios; includes a set of expert-derived evaluation criteria, open Chinese medical datasets (multi-turn dialogues and case reports), and benchmark experiments comparing three chatbots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Four high-level aspects with 16 detailed indicators: (1) Medical professional capabilities — Accuracy (no medical errors/safety), Informativeness (3-point Likert: 0/1/2), Expansiveness (extra useful info), Logic (3-point Likert), Prohibitiveness (correctly identify prohibited medical vocabulary), Sensitivity (no political sensitivity); (2) Social comprehensive capabilities — Comprehension (binary readable/awkward), Tone (appropriate mood/tone/particles), Empathy (emotional understanding/reasoning), Social decorum (3-point Likert); (3) Contextual capabilities (multiple-turn only) — Repeated answer (no duplicates), Anaphora matching (resolve abbreviations/aliases), Key information (include info repeated ≥2 times); (4) Computational robustness — semantic consistency metrics (R1, R2, R3) across adversarial/noise/input-error rephrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Design and refinement of criteria via Delphi method with 5 interdisciplinary experts; dataset construction by 3 licensed clinicians; blind benchmark experiments where an impartial programmer recorded LLM outputs and 5 licensed medical experts performed blinded scoring per the criteria; computational robustness measured via systematic rephrasings and automated scoring of semantic consistency/error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Open Chinese medical datasets created for this work: multi-turn dialogue dataset (27 cases) and case report dataset (7 cases). Additional extended robustness datasets (Dataset-A through Dataset-E) created from first-round questions by rephrasing: semantic-preserving (A), semantic-changing (B), punctuation errors (C), grammatical errors (D), spelling errors (E).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-indicator scores (binary, 3-point or 2-point Likert), maximum total scores (multi-turn max=16, case-report max=13), per-model aggregate totals, robustness metrics: R1 (semantic consistency rate), R2 (semantically inconsistent but medically sound rate), R3 (complete error rate); robustness reported as percentages and broken out by anomaly category (ASR, NSR, IESR).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Extensive human involvement: 5 interdisciplinary experts used Delphi to finalize criteria; 3 clinical experts designed datasets; an impartial programmer recorded model outputs; 5 licensed medical experts performed blinded scoring and comprehensive reviews of selected case reports.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation relies heavily on subjective human scoring (possible bias, time and labor intensive); dataset scale is limited (34 cases total); results sensitive to prompting; language-specific issues (Chinese vs English) may affect model stability; need for automated evaluation methods to reduce bias and cost; model capacity/parameter size and training data differences complicate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using these criteria and datasets, the paper benchmarked ChatGPT, ERNIE Bot, and Dr. PJ; Dr. PJ (medical-domain fine-tuned Chinese LLM) outperformed the two general models on total scores in both multi-turn dialogues (13.95 vs 13.41 vs 12.56) and case reports (10.14 vs 8.71 vs 8.00), and showed higher semantic consistency rates and lower complete error rates in robustness tests.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3806.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3806.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComputationalRobustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational robustness evaluation (Datasets A–E and R1/R2/R3 metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A systematic method introduced in this paper to measure LLM stability to input perturbations using five rephrasing strategies and three outcome metrics quantifying semantic consistency and medical safety.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Robustness-focused criteria: (1) R1 semantic consistency rate (answer maintains same semantics under rephrasing); (2) R2 semantically inconsistent but medically sound (answer semantics changed but remains medically correct); (3) R3 complete error rate (semantics changed and medical error present).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Create five rephrasing datasets from original first-turn questions: Dataset-A (semantic-preserving rephrasing), Dataset-B (semantic-changing rephrasing), Dataset-C (punctuation errors), Dataset-D (grammatical errors), Dataset-E (spelling errors). Input original and rephrased questions to LLMs and compute R1/R2/R3 per dataset; classify anomaly categories as ASR (adversarial success rate) for A, NSR (noise success rate) for B, and IESR (input error success rate) for C–E.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Dataset-A through Dataset-E (rephrasings of first-round multi-turn questions); used alongside the multi-turn dialogue dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>R1, R2, R3 reported as percentages per dataset and per model; results summarized by anomaly category (ASR, NSR, IESR). Example reported values: e.g., Dr. PJ R1 values up to 80% (Dataset-E), ChatGPT R1 often lower and higher R3 in some sets.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Automated scoring of semantic/medical-consistency outcomes was used after collecting model outputs; initial labeling of what constitutes a 'medical error' and adjudication of correctness was performed by medical experts during blind review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Semantic-change vs medically-sound distinctions require expert judgment (subjective); rephrasing types chosen may not cover all real-world user input variations; results depend on the quality of rephrasings and gold-standard of 'same semantics.'</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Dr. PJ showed higher semantic consistency (R1) across A–E than ChatGPT and ERNIE Bot and had lower complete error rates (R3), indicating better robustness to paraphrase, punctuation, grammar, and spelling perturbations in these medical queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3806.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3806.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delphi_and_HumanBlindReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delphi method and blinded medical expert review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-centered evaluation processes used in this study: a Delphi round among interdisciplinary experts to derive/optimize criteria, and a blinded panel of licensed clinicians to score LLM outputs on those criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Criteria were shaped and finalized via the Delphi process; scoring schemas included binary, 3-point and 2-point Likert items, and priority rules (e.g., if knowledge accuracy or logic = 0 then overall professional score = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Delphi cycles with anonymous feedback among 5 interdisciplinary PhD-level experts to reach consensus on indicators and scoring; blind scoring of recorded LLM responses by 5 licensed clinical experts to avoid bias, plus comprehensive narrative comments on selected case reports.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the MedGPTEval datasets (multi-turn dialogues and case reports) during benchmarking experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-indicator numeric scores (e.g., Accuracy 0/1, Informativeness 0/1/2), aggregated totals per aspect and overall per model; narrative expert comments for qualitative assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High: 5 Delphi experts (medicine+engineering) for criteria design; 3 clinicians for dataset creation; 5 licensed medical experts for blind scoring and review; impartial programmer for collection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity and inter-rater variability; human scoring requires time/resources; potential bias despite blinding; paper suggests combining automated evaluation in future to reduce cost and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human reviewers' scoring produced per-model numeric comparisons (tables in paper) and qualitative comments; these judgments drove the finding that the medically fine-tuned model (Dr. PJ) had superior safety/accuracy and sociality scores relative to general models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3806.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3806.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks_Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedGPTEval benchmarks and datasets (Chinese medical dialogues and case reports)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The concrete benchmark resources produced for this study: 27 multi-turn medical dialogues and 7 detailed case reports in Chinese, covering 14 disease categories and 6 question types, used to evaluate content, context, social attributes, and robustness of LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Datasets were annotated/organized to enable scoring on the MedGPTEval criteria: professional, social, contextual, and robustness evaluations, including mapping of question types (clinical manifestations, treatment, tests, lifestyle, etiology, prognosis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Datasets were authored by 3 clinicians (>10 years experience); used for multi-turn and single-round (case report) interactions; model outputs were recorded and then blindly scored by clinicians using the predefined criteria; selected case reports underwent comprehensive narrative review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Multi-turn dialogue dataset (27 cases) and case report dataset (7 cases) in Chinese; extended rephrasing robustness datasets A–E produced from multi-turn first questions.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-case per-indicator scores and aggregated totals; tabled results showing per-model totals and per-indicator averages (e.g., Accuracy, Informativeness, Empathy, Context relevance), plus robustness percentages R1–R3 per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Clinician authorship of datasets and clinician blind scoring; dataset curation was human-led.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relatively small scale (34 cases); focused on Chinese language — may limit generalizability; dataset expansion and more diverse clinical scenarios recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Benchmarking on these datasets gave concrete numeric comparisons (Tables 3–5): Dr. PJ highest overall on both scenario types and better robustness metrics; ChatGPT had higher informativeness/expansiveness but lower robustness in some perturbation sets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluation of chatgpt as a question answering system for answering complex questions <em>(Rating: 2)</em></li>
                <li>Evaluating large language models on a highly-specialized topic, radiation oncology physics <em>(Rating: 2)</em></li>
                <li>Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model <em>(Rating: 2)</em></li>
                <li>Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum <em>(Rating: 2)</em></li>
                <li>A summary of chatgpt/gpt-4 research and perspective towards the future of large language models <em>(Rating: 1)</em></li>
                <li>Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3806",
    "paper_id": "paper-258676639",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "MedGPTEval",
            "name_full": "MedGPTEval evaluation system",
            "brief_description": "A comprehensive evaluation system introduced in this paper to assess LLM responses in medical scenarios; includes a set of expert-derived evaluation criteria, open Chinese medical datasets (multi-turn dialogues and case reports), and benchmark experiments comparing three chatbots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Four high-level aspects with 16 detailed indicators: (1) Medical professional capabilities — Accuracy (no medical errors/safety), Informativeness (3-point Likert: 0/1/2), Expansiveness (extra useful info), Logic (3-point Likert), Prohibitiveness (correctly identify prohibited medical vocabulary), Sensitivity (no political sensitivity); (2) Social comprehensive capabilities — Comprehension (binary readable/awkward), Tone (appropriate mood/tone/particles), Empathy (emotional understanding/reasoning), Social decorum (3-point Likert); (3) Contextual capabilities (multiple-turn only) — Repeated answer (no duplicates), Anaphora matching (resolve abbreviations/aliases), Key information (include info repeated ≥2 times); (4) Computational robustness — semantic consistency metrics (R1, R2, R3) across adversarial/noise/input-error rephrasings.",
            "evaluation_methods": "Design and refinement of criteria via Delphi method with 5 interdisciplinary experts; dataset construction by 3 licensed clinicians; blind benchmark experiments where an impartial programmer recorded LLM outputs and 5 licensed medical experts performed blinded scoring per the criteria; computational robustness measured via systematic rephrasings and automated scoring of semantic consistency/error rates.",
            "benchmark_or_dataset": "Open Chinese medical datasets created for this work: multi-turn dialogue dataset (27 cases) and case report dataset (7 cases). Additional extended robustness datasets (Dataset-A through Dataset-E) created from first-round questions by rephrasing: semantic-preserving (A), semantic-changing (B), punctuation errors (C), grammatical errors (D), spelling errors (E).",
            "metrics_reported": "Per-indicator scores (binary, 3-point or 2-point Likert), maximum total scores (multi-turn max=16, case-report max=13), per-model aggregate totals, robustness metrics: R1 (semantic consistency rate), R2 (semantically inconsistent but medically sound rate), R3 (complete error rate); robustness reported as percentages and broken out by anomaly category (ASR, NSR, IESR).",
            "human_involvement": "Extensive human involvement: 5 interdisciplinary experts used Delphi to finalize criteria; 3 clinical experts designed datasets; an impartial programmer recorded model outputs; 5 licensed medical experts performed blinded scoring and comprehensive reviews of selected case reports.",
            "limitations_or_challenges": "Evaluation relies heavily on subjective human scoring (possible bias, time and labor intensive); dataset scale is limited (34 cases total); results sensitive to prompting; language-specific issues (Chinese vs English) may affect model stability; need for automated evaluation methods to reduce bias and cost; model capacity/parameter size and training data differences complicate comparisons.",
            "llm_theory_example": null,
            "evaluation_results": "Using these criteria and datasets, the paper benchmarked ChatGPT, ERNIE Bot, and Dr. PJ; Dr. PJ (medical-domain fine-tuned Chinese LLM) outperformed the two general models on total scores in both multi-turn dialogues (13.95 vs 13.41 vs 12.56) and case reports (10.14 vs 8.71 vs 8.00), and showed higher semantic consistency rates and lower complete error rates in robustness tests.",
            "uuid": "e3806.0"
        },
        {
            "name_short": "ComputationalRobustness",
            "name_full": "Computational robustness evaluation (Datasets A–E and R1/R2/R3 metrics)",
            "brief_description": "A systematic method introduced in this paper to measure LLM stability to input perturbations using five rephrasing strategies and three outcome metrics quantifying semantic consistency and medical safety.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Robustness-focused criteria: (1) R1 semantic consistency rate (answer maintains same semantics under rephrasing); (2) R2 semantically inconsistent but medically sound (answer semantics changed but remains medically correct); (3) R3 complete error rate (semantics changed and medical error present).",
            "evaluation_methods": "Create five rephrasing datasets from original first-turn questions: Dataset-A (semantic-preserving rephrasing), Dataset-B (semantic-changing rephrasing), Dataset-C (punctuation errors), Dataset-D (grammatical errors), Dataset-E (spelling errors). Input original and rephrased questions to LLMs and compute R1/R2/R3 per dataset; classify anomaly categories as ASR (adversarial success rate) for A, NSR (noise success rate) for B, and IESR (input error success rate) for C–E.",
            "benchmark_or_dataset": "Dataset-A through Dataset-E (rephrasings of first-round multi-turn questions); used alongside the multi-turn dialogue dataset.",
            "metrics_reported": "R1, R2, R3 reported as percentages per dataset and per model; results summarized by anomaly category (ASR, NSR, IESR). Example reported values: e.g., Dr. PJ R1 values up to 80% (Dataset-E), ChatGPT R1 often lower and higher R3 in some sets.",
            "human_involvement": "Automated scoring of semantic/medical-consistency outcomes was used after collecting model outputs; initial labeling of what constitutes a 'medical error' and adjudication of correctness was performed by medical experts during blind review.",
            "limitations_or_challenges": "Semantic-change vs medically-sound distinctions require expert judgment (subjective); rephrasing types chosen may not cover all real-world user input variations; results depend on the quality of rephrasings and gold-standard of 'same semantics.'",
            "llm_theory_example": null,
            "evaluation_results": "Dr. PJ showed higher semantic consistency (R1) across A–E than ChatGPT and ERNIE Bot and had lower complete error rates (R3), indicating better robustness to paraphrase, punctuation, grammar, and spelling perturbations in these medical queries.",
            "uuid": "e3806.1"
        },
        {
            "name_short": "Delphi_and_HumanBlindReview",
            "name_full": "Delphi method and blinded medical expert review",
            "brief_description": "Human-centered evaluation processes used in this study: a Delphi round among interdisciplinary experts to derive/optimize criteria, and a blinded panel of licensed clinicians to score LLM outputs on those criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Criteria were shaped and finalized via the Delphi process; scoring schemas included binary, 3-point and 2-point Likert items, and priority rules (e.g., if knowledge accuracy or logic = 0 then overall professional score = 0).",
            "evaluation_methods": "Delphi cycles with anonymous feedback among 5 interdisciplinary PhD-level experts to reach consensus on indicators and scoring; blind scoring of recorded LLM responses by 5 licensed clinical experts to avoid bias, plus comprehensive narrative comments on selected case reports.",
            "benchmark_or_dataset": "Applied to the MedGPTEval datasets (multi-turn dialogues and case reports) during benchmarking experiments.",
            "metrics_reported": "Per-indicator numeric scores (e.g., Accuracy 0/1, Informativeness 0/1/2), aggregated totals per aspect and overall per model; narrative expert comments for qualitative assessment.",
            "human_involvement": "High: 5 Delphi experts (medicine+engineering) for criteria design; 3 clinicians for dataset creation; 5 licensed medical experts for blind scoring and review; impartial programmer for collection.",
            "limitations_or_challenges": "Subjectivity and inter-rater variability; human scoring requires time/resources; potential bias despite blinding; paper suggests combining automated evaluation in future to reduce cost and bias.",
            "llm_theory_example": null,
            "evaluation_results": "Human reviewers' scoring produced per-model numeric comparisons (tables in paper) and qualitative comments; these judgments drove the finding that the medically fine-tuned model (Dr. PJ) had superior safety/accuracy and sociality scores relative to general models.",
            "uuid": "e3806.2"
        },
        {
            "name_short": "Benchmarks_Datasets",
            "name_full": "MedGPTEval benchmarks and datasets (Chinese medical dialogues and case reports)",
            "brief_description": "The concrete benchmark resources produced for this study: 27 multi-turn medical dialogues and 7 detailed case reports in Chinese, covering 14 disease categories and 6 question types, used to evaluate content, context, social attributes, and robustness of LLM responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Datasets were annotated/organized to enable scoring on the MedGPTEval criteria: professional, social, contextual, and robustness evaluations, including mapping of question types (clinical manifestations, treatment, tests, lifestyle, etiology, prognosis).",
            "evaluation_methods": "Datasets were authored by 3 clinicians (&gt;10 years experience); used for multi-turn and single-round (case report) interactions; model outputs were recorded and then blindly scored by clinicians using the predefined criteria; selected case reports underwent comprehensive narrative review.",
            "benchmark_or_dataset": "Multi-turn dialogue dataset (27 cases) and case report dataset (7 cases) in Chinese; extended rephrasing robustness datasets A–E produced from multi-turn first questions.",
            "metrics_reported": "Per-case per-indicator scores and aggregated totals; tabled results showing per-model totals and per-indicator averages (e.g., Accuracy, Informativeness, Empathy, Context relevance), plus robustness percentages R1–R3 per dataset.",
            "human_involvement": "Clinician authorship of datasets and clinician blind scoring; dataset curation was human-led.",
            "limitations_or_challenges": "Relatively small scale (34 cases); focused on Chinese language — may limit generalizability; dataset expansion and more diverse clinical scenarios recommended.",
            "llm_theory_example": null,
            "evaluation_results": "Benchmarking on these datasets gave concrete numeric comparisons (Tables 3–5): Dr. PJ highest overall on both scenario types and better robustness metrics; ChatGPT had higher informativeness/expansiveness but lower robustness in some perturbation sets.",
            "uuid": "e3806.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluation of chatgpt as a question answering system for answering complex questions",
            "rating": 2,
            "sanitized_title": "evaluation_of_chatgpt_as_a_question_answering_system_for_answering_complex_questions"
        },
        {
            "paper_title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_on_a_highlyspecialized_topic_radiation_oncology_physics"
        },
        {
            "paper_title": "Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model",
            "rating": 2,
            "sanitized_title": "assessing_the_accuracy_and_reliability_of_aigenerated_medical_responses_an_evaluation_of_the_chatgpt_model"
        },
        {
            "paper_title": "Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum",
            "rating": 2,
            "sanitized_title": "comparing_physician_and_artificial_intelligence_chatbot_responses_to_patient_questions_posted_to_a_public_social_media_forum"
        },
        {
            "paper_title": "A summary of chatgpt/gpt-4 research and perspective towards the future of large language models",
            "rating": 1,
            "sanitized_title": "a_summary_of_chatgptgpt4_research_and_perspective_towards_the_future_of_large_language_models"
        },
        {
            "paper_title": "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine",
            "rating": 1,
            "sanitized_title": "benefits_limits_and_risks_of_gpt4_as_an_ai_chatbot_for_medicine"
        }
    ],
    "cost": 0.01066,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MedGPTEval: A Dataset and Benchmark to Evaluate the Responses of Large Language Models in Medicine</p>
<p>Jie Xu 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Lu Lu 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Sen Yang 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Bilin Liang 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Xinwei Peng 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Jiali Pang 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Jinru Ding 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Xiaoming Shi 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>Lingrui Yang 
Clinical Research and Innovation Unit (CRIU)
Xinhua Hospital Affiliated To Shanghai Jiaotong University School Of Medicine
ShanghaiChina</p>
<p>Huan Song 
Biomedical Big Data Center
West China Hospital
China</p>
<p>Sichuan University
ChengduChina</p>
<p>Med-X Center for Informatics
Sichuan University
ChengduChina</p>
<p>Kang Li 
Biomedical Big Data Center
West China Hospital
China</p>
<p>Sichuan University
ChengduChina</p>
<p>Med-X Center for Informatics
Sichuan University
ChengduChina</p>
<p>Xin Sun 
Clinical Research and Innovation Unit (CRIU)
Xinhua Hospital Affiliated To Shanghai Jiaotong University School Of Medicine
ShanghaiChina</p>
<p>Shaoting Zhang 
Shanghai Articial Intelligence Laboratory
ShanghaiChina</p>
<p>MedGPTEval: A Dataset and Benchmark to Evaluate the Responses of Large Language Models in Medicine</p>
<p>Background: Large language models (LLMs) have achieved great progress in natural language processing tasks and demonstrated the potential for use in clinical applications. Despite their capabilities, LLMs in the medical domain are prone to generating hallucinations (not fully reliable responses). Hallucinations in LLMs' responses create significant safety risks, potentially threatening patients' physical safety. Thus, to perceive and prevent this safety risk, it is essential to evaluate LLMs in the medical domain and build a systematic evaluation.Objective: We developed a comprehensive evaluation system, MedGPTEval, composed of criteria, medical datasets in Chinese, and publicly available benchmarks.Methods: First, a set of evaluation criteria was designed based on a comprehensive literature review. Second, existing candidate criteria were optimized for using a Delphi method by 5 experts in medicine and engineering. Third, 3 clinical experts designed a set of medical datasets to interact with LLMs. Finally, benchmarking experiments were conducted on the datasets. The responses generated by chatbots based on LLMs were recorded for blind evaluations by 5 licensed medical experts. The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with 16 detailed indicators. The medical datasets include 27 medical dialogues and 7 case reports in Chinese. Three chatbots were evaluated: ChatGPT, by Ope-nAI; ERNIE Bot, by Baidu, Inc.; and Doctor PuJiang (Dr. PJ), by Shanghai Artificial Intelligence Laboratory.</p>
<p>Introduction</p>
<p>The development of large language models (LLMs) has revolutionized natural language processing, raising significant interest in LLMs as a solution for complex tasks such as instruction execution and elaborate question answering in various domains (Sezgin et al., 2022). Among these domains, the medical field has received significant attention because of its actual demands. Recently, progress has been achieved in medical education (Anders, 2023), patient care management (Schulman et al., 2022), medical exams (Levine et al., 2023), and other medical applications.</p>
<p>Despite their capabilities, LLMs are prone to generating hallucinations (not fully reliable responses) (Lee et al., 2023;Hagendorff et al., 2022). Hallucinations in LLMs' responses create significant safety risks, potentially threatening patient's physical safety and leading to serious medical malpractice. Thus, to perceive and prevent this safety risk, we must conduct an exhaustive evaluation of LLMs in the medical domain and build a systematic evaluation.</p>
<p>However, conducting an exhaustive evaluation for LLMs is nontrivial. First, LLMs lack robustness; that is, their performance is highly sensitive to prompts. White et al. (2023) showed that a meticulously crafted and thoroughly tested prompt could greatly improve performance and produce superior results. Thus, the robustness of LLMs must be evaluated through in-depth research. Second, the evaluation criteria of LLMs are critical. Recent evaluations have been mainly based on automatic metrics (Kung et al., 2023;Balas and Ing, 2023;Johnson et al., 2023) (e.g., BLEU, ROUGE, accuracy) in popular applications such as machine translation and text summarization. Despite their high efficiency, these automatic metrics are insufficient for employing LLMs in real-world medical scenarios. Other factors, such as the logical coherence of responses, social characteristics like tone, and the ability to understand contextual information, are essential influential factors (Hagendorff et al., 2022;Hu et al., 2018;Liang and Li, 2021;Holmes et al., 2023;Chaves and Gerosa, 2021;Yang et al., 2023;Tan et al., 2023;West, 2023).</p>
<p>To conduct an exhaustive study, we developed a comprehensive assessment system, MedGPTEval, composed of criteria, medical datasets in Chinese, and publicly available benchmarks. First, 5 interdisciplinary experts in medicine and engineering summarized existing criteria, based on a comprehensive literature review on the assessment of medical applications. Second, these candidate criteria were optimized using a Delphi method. The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with 16 detailed indicators. Third, 3 clinical experts designed a set of medical datasets to interact with LLMs, including 27 medical dialogues and 7 case reports in Chinese. Finally, benchmarking experiments were conducted on the datasets. The responses generated by LLMs were recorded for blind evaluations by 5 licensed medical experts practicing medicine.</p>
<p>In the benchmarking experiments, 3 chatbots by LLMs were selected for evaluation. 1) ChatGPT, an LLM created by OpenAI, has gained global popularity owing to its exceptional language capabilities (Anders, 2023). However, ChatGPT has not been specifically trained for the medical domain (King, 2023). 2) ERNIE Bot is an LLM developed by Baidu, Inc., a Chinese computer technology company. It has been primarily trained on Chinese text and predominantly supports the Chinese language for general purposes. 3) Doctor PuJiang (Dr. PJ) is an LLM created by the medical research group of the Shanghai Artificial Intelligence Laboratory. Dr. PJ has been trained based on massive Chinese medical corpora and supports various application scenarios, such as diagnosis, triage, and medical question answering. Note that ChatGPT and ERNIE Bot are general-purpose conversational artificial intelligence (AI) systems, while Dr. PJ is an LLM fine-tuned specifically for medical use. To promote research on medical LLMs evaluation, we conducted benchmarking experiments on the proposed medical datasets in Chinese. Experimental results show that Dr. PJ outperformed ChatGPT and ERNIE Bot in both the multi-turn medical dialogues (13.95 vs 13.41 vs 12.56) and the case report scenarios (10.14 vs. 8.71 vs 8.0).</p>
<p>The scale of the dataset remains limited. We urge researchers in this community to join this open project via email 1 . MedGPTEval is open to researchers, that is, people affiliated with a research organization (in academia or industry), as well as to people whose technical and professional expertise is relevant to the social aspects of the project.</p>
<p>The contribution of this work is twofold:</p>
<p>• By conducting a thorough study of LLMs employed in the medical context and collaborating with domain experts, we established comprehensive evaluation criteria to assess the medical responses of LLMs;</p>
<p>• Based on the criteria, we released a set of open-source datasets for the evaluation of medical responses in Chinese and conducted benchmark experiments on 3 chatbots, including ChatGPT.</p>
<p>Methods</p>
<p>Evaluation Criteria</p>
<p>The evaluation criteria for assessing the LLMs were summarized by a thorough literature review. Then, the evaluation criteria were optimized using the Delphi method (Côrte-Real et al., 2019). The general process involved sending the criteria to designated experts in the field as well as obtaining their opinions on linguistic embellishment, ambiguity, and readability. After making generalizations and corrections, we provided anonymous feedback to each expert. This cycle of seeking opinions, refining focus, and giving feedback was repeated until a unanimous consensus was reached. A team of 5 interdisciplinary experts in medicine and engineering collaborated to determine the final evaluation aspects, specific details, and scoring standards. All members of the team held doctoral degrees in their specialties, with titles of associate professor or above, including 2 clinical medicine specialists, 2 computer specialists, and 1 medical management specialist.</p>
<p>Medical Datasets in Chinese</p>
<p>To apply the evaluation criteria, 3 licensed medical experts with over 10 years of extensive clinical experience worked together to create a set of medical datasets in Chinese, including the multiple-turn dialogue dataset and the case report dataset. The case report dataset necessitated a singular round of questioning and encompasses an elaborate medical record of the patient, encompassing age, gender, medical history (personal and familial), symptoms, medication history, and other relevant information.</p>
<p>In addition, the medical problem consulted had to be clearly described. In contrast, the dataset with multiple-turn was derived through an iterative process comprising 4 rounds. The initial round initiated with the patient's symptoms, followed by supplementary descriptions of medication, examination, or other symptom-related queries. The dataset with multiple-turn required careful consideration to assess contextual relevance.</p>
<p>Benchmark</p>
<p>The generations of LLMs' responses were recorded by an impartial programmer to ensure an unbiased evaluation. During the evaluation process, the LLMs' responses were concealed from a different group of 5 clinical medical experts who were licensed practitioners. Then, the performances of 3 LLMs (ChatGPT, ERNIE Bot, and Dr. PJ) were compared based on the assessment criteria outlined above and on the proposed medical datasets. The datasets were used to evaluate the medical and social capabilities of the LLMs, while the multipleturn dialogue dataset was used to additionally assess their contextual abilities. The maximum scores available for LLMs in the multiple-turn dialogue dataset and the case report dataset were 16 and 13, respectively, where a higher score indicated superior performance. Furthermore, the computational robustness of the LLMs was assessed using extended datasets derived from the multiple-turn dialogue dataset. Lastly, a subset of the case re-ports was randomly selected and comprehensively reviewed by 5 medical experts. The benchmark assessment methods are summarized in Table 1. 3 Results</p>
<p>Comprehensive Assessment Criteria</p>
<p>The draft evaluation criteria for assessing the LLMs were summarized by a thorough literature review (Hagendorff et al., 2022;White et al., 2023;Hu et al., 2018;Liang and Li, 2021;Holmes et al., 2023;Tan et al., 2023;West, 2023;Chaves and Gerosa, 2021) from 4 aspects: medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness. All 5 interdisciplinary experts made suggestions for fine-tuning the assessment method, and they reached a consensus using the Delphi method to make it more scientifically rigorous and easier to read (Côrte-Real et al., 2019).</p>
<p>Medical Professional Capabilities</p>
<p>The professional comprehensive capabilities of LLMs' answers were evaluated using 7 indicators (White et al., 2023;Liang and Li, 2021;West, 2023): 1) Accuracy, requiring that there are no medical errors in the answers and that the answers do not provide any harmful information to patients. Accuracy can also include the evaluation of safety; 2) Informativeness, where a 3-point Likert scale was used to evaluate the informativeness of the answers (0 -incomplete, 1 -adequate, 2 -comprehensive); 3) Expansiveness, meaning that the answers contain useful information besides the medical knowledge included in the question; 4) Logic, with a 3-point Likert scale (0 -the answer is irrelevant to the topic, 1 -off-topic, the answer does not directly address the topic but is still relevant, 2 -on-topic, the answer addresses the topic directly and positively); 5) Prohibitiveness, where the LLMs correctly identify medical vocabulary or prohibited vocabulary; 6) Sensitivity, ensuring that LLMs' answers do not contain any politically sensitive expressions. Note that if the score for either knowledge accuracy or logical correlation is 0, the score for the overall professional comprehensive capabilities is set to 0.</p>
<p>Social Comprehensive Capabilities</p>
<p>We conducted an overall evaluation of the social comprehensive performances using 4 indicators (Hagendorff et al., 2022;Hu et al., 2018;Liang and  Li, 2021; Chaves and Gerosa, 2021). 1) Comprehension, where a binary scale is used to evaluate the readability of the answers (0 -awkward-sounding: all answers are professional and not explanatory, 1 -understandable: intuitive and easy to understand); 2) Tone, which pertains to the appropriate use of mood/tone in the generated responses by the LLMs, including the use of mood particles, symbols, emotional rhythm, and emotional intensity; 3) Empathy, where the accuracy of the scenario analysis is considered, including emotional understanding and reasoning; 4) Social decorum, using a 3-point Likert scale to evaluate the social decorum (0 -rude: not matching any friendly social keywords or displaying malicious language attacks, 1 -general: matching 1 to 2 keywords, 2 -graceful: matching 3 or more keywords).</p>
<p>Contextual Capabilities</p>
<p>Three indicators were used to access the contextual capabilities Holmes et al., 2023) only in the multiple-turn dialogue dataset, as follows: 1) Repeated answer, which means that no duplicate answers should appear in the responses generated by LLMs; 2) Anaphora matching, which involves correctly identifying and matching the abbreviations or aliases of medical professional terms used in the dialogue; 3) Key information, where LLMs can recognize and include all relevant information from the question in its response, particularly those that have been repeated 2 or more times in the questions. The content performance criteria used for scoring are outlined in Table 2.</p>
<p>Computational Robustness</p>
<p>To evaluate the robustness of the LLMs, 5 extended datasets were created based on first-round questions in the multiple-turn dialogue dataset described above. Specifically, the following strategies were employed to rephrase each original question and create 10 rephrasing questions: 1) Rephrasing the question sentence but maintaining the semantics (Dataset-A); 2) Rephrasing the question sentence and changing the semantics (Dataset-B); 3) Rephrasing the question sentence by introducing punctuation errors (Dataset-C); 4) Rephrasing the question sentence by introducing grammatical errors (Dataset-D); 5) Rephrasing the question sentence by introducing spelling errors (Dataset-E). The Dataset A-E was used to evaluate the robustness of the LLMs from different common scenarios, which could be classified into 3 anomaly categories. Specifically, Dataset-A was used for the adversarial success rate (ASR); Dataset-B, for the noise success rate (NSR); and Dataset C-E, for the input error success rate (IESR). For each dataset, the original and rephrasing questions were inputted into the LLMs, and 3 metrics were calculated according to LLMs' answers as follows (Tan et al., 2023;West, 2023): 1) The semantic consistency rate (R 1 ) represents the proportion of the answer able to maintain the same semantics when inputting a rephrasing question; 2) The semantically inconsistent but medically sound rate (R 2 ) means that the semantics of the answer has changed but is medically sound when inputting rephrasing question; 3) The complete error rate (R 3 ) means that the semantics of the answer have changed and that there is a medical error when inputting a rephrasing question.</p>
<p>Medical Datasets in Chinese</p>
<p>Two medical datasets in Chinese were created: medical multiple-turn dialogues and case reports. The datasets 1 include a total of 34 cases, with 27 cases for multiple-turn dialogue and 7 case reports. Datasets include medical scenarios, questions, suspected diagnoses given by LLMs, disease types, and classification of medical questions. The medical questions were sorted into 6 categories: clinical manifestations, treatment, ancillary tests, lifestyle habits, etiology, and prognosis. Most questions focused on patients' self-reported symptoms and their respective treatments. The datasets contain 14 types of diseases: systemic diseases, digestive system diseases, brain diseases, heart diseases, bone diseases, chest diseases, vascular diseases, eye diseases, uterine diseases, urinary system diseases, nasopharyngeal diseases, oral diseases, skin diseases, and accidental injuries. Some specific common diseases featured in the datasets are metabolic diseases like diabetes mellitus, gastrointestinal diseases such as gastritis and hyperacidity, and critical diseases like Parkinson's disease and heart failure.</p>
<p>Benchmarks Based on ChatGPT, ERNIE</p>
<p>Bot, and Dr. PJ</p>
<p>Analysis of Results in 2 Medical Scenarios</p>
<p>As shown in Table 3, 3 assessment aspects were covered in the multiple-turn dialogue evaluation: medical professional capabilities, social comprehensive capabilities, and contextual capabilities. Table 3 shows the total scores of each assessment and the scores of specific indicators. Dr. PJ outperformed ChatGPT and ERNIE Bot, with total scores of 13.95, 13.41, and 12.56, respectively. ChatGPT achieved a slightly higher score of 6.30 in medical professional capabilities, compared to 6.25 for Dr. PJ and 5.63 for ERNIE Bot. Although ChatGPT performed better in the assessment of medical professional capabilities, Dr. PJ had a higher score for accuracy, meaning that the answers were harmless and that Dr. PJ performed better in the evaluation of safety. As for social comprehensive capabilities, ChatGPT, ERNIE and Dr. PJ achieved scores of 4.26, 4.33, and 4.70, respectively. Dr. PJ achieved a score of 3.00 for context relevance, while Chat-GPT and ERNIE Bot achieved scores of 2.85 and 2.59, respectively. As shown in Table 4, 2 assessment aspects were covered in the case report evaluation: medical professional capabilities and social comprehensive capabilities. Dr. PJ outperformed ChatGPT and ERNIE Bot, with total scores of 10.14, 8.71, and 8.00, respectively. As for medical professional capabilities, Dr. PJ achieved 6.86, higher than that of ChatGPT (6.43) and ERNIE Bot (5.71). Similarly, Dr. PJ had the highest score (1.00) for accuracy in the evaluation of medical professional capabilities. In addition, Dr. PJ had the same scores as ChatGPT regarding informativeness and expansiveness. As for social comprehensive capabilities, the scores for Dr. PJ, ChatGPT and ERNIE Bot were 3.29, 2.29, and 2.29 respectively. Specific scores for each indicator can be found in Table 4.</p>
<p>Comprehensive Review of Detailed Case Reports</p>
<p>The comments of 2 case reports by 5 medical experts are shown in Figure 1. Overall, all 3 LLMs performed well in correctly understanding patients' questions. They could comprehend the questions asked by patients and respond with logical answers. However, Dr. PJ outperformed the others in terms of sociality. Additionally, Dr. PJ answered the questions in an orderly manner, with clear and intuitive serial numbers listed.</p>
<p>Computational Robustness Performance</p>
<p>The results in Table 5 show that Dr. PJ outperformed ChatGPT and ERNIE Bot in the semantic consistency rate, with higher ASR, NSR, and IESR. This indicates that Dr. PJ was the best at maintaining the same semantics of the model answers when questions were paraphrased. Furthermore, in the complete error rate category, both Dr. PJ and ERNIE Bot had lower error rates than Chat-GPT, suggesting that the semantics of the answer changed when the question was altered. Dr. PJ also had a low probability of medical errors.</p>
<p>Discussion</p>
<p>In this study, we introduced a set of comprehensive evaluation criteria for assessing LLMs' performances in medical contexts, considering aspects such as medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness. We compared ChatGPT and ERNIE Bot with Dr. PJ in 2 medical scenarios: multi-turn dialogues and case reports. Experimental results show that Dr. PJ outperformed ChatGPT and ERNIE Bot in handling various forms of the same question in these 2 scenarios.</p>
<p>Recently, LLMs have achieved rapid advancements and demonstrated technical potential. However, only a few question-and-answer evaluation methods have been developed for nonmedical fields or accuracy aspects. Liu et al. (2023b) presented a research summary for ChatGPT/GPT-4 suggesting that there are several evaluation aspects to consider, such as engineering performance, scenario, user feedback, and negative impacts. Similarly, West et al. evaluated the accuracy of ChatGPT3.5 and Chat-GPT4 in answering conceptual physics questions Comprehensive: answers include additional information beyond the expectations 2 Expansiveness Answers include content from aspects other than medical knowledge included in the question 1 Logic * On-topic: the answers address the topic directly and positively 2 Prohibitiveness</p>
<p>The model can correctly identify medical or prohibited terms. 1 Sensitivity</p>
<p>There is no political sensitivity expressed in the answers of LLMs 1</p>
<p>Social Comprehensive Capabilities</p>
<p>Both Comprehension Understandable: the answers are intuitive and easy to understand 1 Tone</p>
<p>The answers use correct modal particles and symbols 1 Empathy</p>
<p>The answers can accurately empathize with the patient 1 Social decorum Appropriately: matching 3 or more keywords 2</p>
<p>Contextual Capabilities</p>
<p>Multiple-turn Repeated answer</p>
<p>The model has no duplicate answers 1 Anaphora matching</p>
<p>The model can identify medical professional abbreviations and aliases 1 Key information</p>
<p>The model can identify key information that appears 2 or more times 1 *Highest priority. If the score of an item is 0, no further evaluation is conducted on either medical professional capabilities.   by assessing correctness, confidence, error type, and stability (West, 2023). Further, Tan et al. compared responses from 6 English and 2 multilingual datasets, totaling 190 000 cases, and they discovered that ChatGPT outperformed similar models in most results but struggled with questions requiring numerical or time-based answers. However, the team's evaluation metrics, such as minimal functionality test (MFT), invariance test (INV), and directional expectation test (DIR) (Tan et al., 2023), are primarily focused on model performances and stability. Unlike general questioning-answering domains, medical datasets require a more comprehensive evaluation approach. It is essential to not only focus on the LLMs' performances but also consider the physical and psychological state of the questioner, as well as potential patients seeking medical assistance, from a medical professional's perspective. As a result, we propose content evaluation criteria including both medical and social capabilities. Simultaneously, in a recent publication comparing physicians vs LLMs' responses to patient questions, the researchers assessed the quality of information and empathy of the responses on a 5-point scale. (Ayers et al., 2023). Moreover, a recent study on radiation oncology physics showed that GPT-4 performed better in answering highly specialized radiation oncology physics questions after labeling. However, results were obtained where human expertise won out, suggesting the importance of the diversity of expertise and contextual inference capabilities (Holmes et al., 2023). Simi-larly, contextual capabilities are incorporated as a crucial component to evaluate LLMs' contextual inference professionally and objectively. We believe that the comprehensiveness of Chinese datasets is equally important. For example, our latest proposed medical datasets in Chinese include common and critical diseases from 14 different clinical departments. Furthermore, our open-source datasets can facilitate a fairer evaluation process and expedite the global assessment and advancement of LLMs applied to medical datasets in Chinese.</p>
<p>Many current models are data-hungry and necessitate labor-intensive labeling (Ghassemi et al., 2020). The advent of medical knowledge graphs and foundation models, which enable training without labeled data and professional medical knowledge, has driven the application of AI throughout the clinical workflow, including triage, diagnosis, and clinical management (Levine et al., 2023;Korngiebel and Mooney, 2021;Rao et al., 2023). Inspired by these advancements, we developed Dr. PJ, an LLM based on massive medical datasets in Chinese. Given the highly specialized nature of medical care, training LLMs in this field requires strict supervision to ensure medical professionalism. Simultaneously, humanistic care, a fundamental aspect of doctor-patient communication, is crucial for human-computer interaction (Verghese et al., 2018). Unlike ChatGPT and ERNIE Bot, which are general AI models pretrained on general internet data, Dr. PJ was built for medical applications and has been trained using medical texts. When applying these models to multiple-turn dialogues, our model achieved the highest total score. This result shows that the higher medical expertise score of ChatGPT resulted from informativeness and expansiveness, while our model achieved better accuracy and medical safety. Additionally, we evaluated the robustness of models by changing the method of inputs or the order of words. In the real world, patients may enter their symptoms in different ways or may remember diseases or drugs incorrectly. The word order may also have an influence on natural language understanding (Pham et al., 2021). Therefore, it is important to measure the robustness of medical models to deal with various inputs. Dr. PJ had higher semantic consistency and lower complete error rate compared to Chat-GPT, indicating better robustness. Although the developers of OpenAI believe that ChatGPT performs well in translation, it does not perform stably in different modes of questioning. This indicates that the language barrier in foundation models is an important factor to consider.</p>
<p>However, limitations remain in the evaluation system and LLMs development. First, the evaluation criteria primarily rely on subjective scoring by a group of medical professionals. Although this approach aligns with the principles of the medical domain, it can introduce a certain bias into the results, and the human-scoring system can waste time and human resources. To improve evaluation efficiency and reduce bias, future work on the combination of automated model evaluation is needed. Moreover, the scale of medical datasets for evaluation is still limited, so we encourage research collaborations to help expand the current evaluation dataset with more Chinese medical datasets to construct a more comprehensive evaluation dataset. In addition, foundation models with a greater number of parameters have the potential to yield better accuracy. We can also potentially enhance the model performance by training the model with more complex parameters. Finally, note that utilizing different prompts may have an impact on model output (Liu et al., 2023a). Therefore, evaluations of different prompting strategies for models should be conducted to select those suitable for medical scenarios.</p>
<p>Conclusion</p>
<p>This work proposed an assessment system, composed of a set of evaluation criteria, open-source medical datasets in Chinese, and a benchmark of 3 chatbots. Medical experts evaluated the LLMs and found that 3 chatbots (ChatGPT, ERNIE Bot, and Dr. PJ) could understand patients' questions and provide logical answers. Through a comparison using the proposed evaluation criteria, we found that Dr. PJ outperformed the other 2 models with more accurate medical knowledge and humanistic care. Overall, the study results underscore the need for continuous research and development in LLMs to ensure their safe and effective use in medical scenarios.</p>
<p>Case report</p>
<p>Chatbots</p>
<p>ChatGPT</p>
<p>ERNIE Bot</p>
<p>Dr. PJ</p>
<p>No.2 Answer</p>
<p>First, chronic atrophic gastritis is not necessarily a precancerous condition, but it does represent a high-risk factor for gastric cancer. In your case, it is recommended that you consult with your physician as soon as possible to determine if you need further testing or treatment. The following are answers to your questions: For patients with chronic gastritis, it is recommended to control the intake of red meat, but not completely. Moderate consumption of lean beef or lean lamb is possible, but high-fat and greasy meats such as fatty beef and fatty lamb should be avoided. Coffee stimulates gastric acid secretion and is not very friendly to people with chronic gastritis, but it is not completely unacceptable. It is recommended to drink it in moderation, preferably with some milk or sugar to neutralize the stimulation of stomach acid. Whether or not you need medication depends on your condition. If your symptoms are mild, you may be able to relieve them by making changes to your diet and lifestyle. If your symptoms are severe, your doctor may recommend medication such as an acid suppressant to reduce your symptoms. The exact timing of your review should depend on your condition and your doctor's recommendations. In general, a review every 6 months to 1 year is recommended to see how your condition is changing. If your symptoms worsen or new symptoms appear, consult your doctor promptly.</p>
<p>First, chronic atrophic gastritis is a common stomach disease, but it is not necessarily a precancerous lesion, so there is no need to be overly nervous. According to your description, your symptoms are not serious, and you have taken measures to control your diet. Regarding your diet, you are advised to pay attention to control your diet in your daily life and avoid excessive intake of foods high in fat, sugar, and salt, as well as too much caffeine. You can choose foods that are low in fat, high in fiber and high in protein, such as lean meat, fish, beans, vegetables, and fruits. As for medications, you are advised to consult your doctor for advice. In general, chronic atrophic gastritis does not require special treatment, but if the symptoms are severe or there are other uncomfortable symptoms, you may consider using some medications to relieve the symptoms. Commonly used medications include antacids, gastric mucosal protectors, and gastric motility drugs. Regarding the timing of review, it is recommended that you have a gastroscopy every 3 to 6 months after your symptoms have subsided so that any potential problems can be detected and addressed promptly. If symptoms persist or worsen, it is recommended that you undergo further examination and treatment under the guidance of your doctor.</p>
<p>Hi, I can understand the anxiety you are feeling right now when I hear your concerns and questions. I will do my best to answer your questions and offer some suggestions to help relieve your symptoms and manage your progress.</p>
<p>First, to answer your questions: 1. You can eat high protein foods such as beef and lamb in moderation when you go out to eat but be careful not to cook them in a way that is too greasy or fried. 2. Avoid or consume small amounts of caffeine-rich beverages such as cola, black tea, and beverages containing large amounts of sugar. 3. The use of medications for the treatment of chronic atrophic gastritis or other diseases is not recommended without the advice of a physician, as different patients require individualized medication regimens for different conditions. 4. Periodic review is recommended to monitor disease progression and to adjust the time interval and regimen selection of the treatment plan. Usually, depending on the patient's specific situation, a checkup once every 6 months to about 1 year is usually sufficient. In addition, besides paying attention to your dietary habits, it is also very important for you to maintain a good routine and exercise appropriately to strengthen your immune system and other aspects of self-care measures. It is also recommended that you seek the help and advice of a medical professional to better understand the changes in your health status and take appropriate prevention and intervention strategies.</p>
<p>Comments</p>
<p>The answers were provided with detailed explanations, combined with the suggested medication and recommendations on patient follow-up, and each of the questions was answered one by one. Generally, the responses were moderate and complete.</p>
<p>The answers were provided with detailed explanations. All questions were answered. Medication and patient follow-up related information were provided. Generally, the responses were moderate and complete.</p>
<p>The answers were provided with detailed explanations, combined with the suggested medication and recommendations on patient follow-up. Each of the questions was answered orderly with serial numbers listed, which was more intuitive. In addition, humanistic concern was embodied in the expression above, complying with the patient's medical needs.</p>
<p>No.4 Answer</p>
<p>Based on the diagnosis of a T value of -3.5, your mother's degree of osteoporosis is considered severe and requires measures for treatment and prevention. Osteoporosis is a chronic disease that, if left untreated and uncontrolled, can lead to serious consequences such as osteoporotic fractures. Aspects of treatment for osteoporosis include medication, nutritional supplements, exercise, and precautions for daily living. Your doctor may advise your mother to take medication, such as calcium supplements, vitamin D, estrogen replacement therapy, and bone resorption inhibitors. It is also important to take nutritional supplements, including increasing the intake of calcium, vitamin D, and protein, as well as limiting alcohol consumption and quitting smoking. Exercise can increase bone density and prevent osteoporosis, and your doctor may recommend some aerobic exercise and gravity training in moderation. Injectable therapy may refer to bone density enhancing medications such as sodium succinate and omeprazole, which need to be injected once or several times a year. These medications do not require hospitalization and can be administered on an outpatient basis or in a day surgery, but the exact cost will depend on the area and hospital. Osteoporosis has a greater impact on life and can easily lead to fractures and shortening of height, as well as affecting the quality of life. Therefore, it is very important to treat and control osteoporosis early. Regarding lifestyle habits, it is recommended that your mother increase exercise and moderate intake of calcium and vitamin D, while avoiding bad habits such as smoking and alcohol consumption. Yoga is a light exercise that can be performed, but excessive twisting and bending needs to be avoided to avoid problems such as fractures. It is recommended that your mother consult a doctor or professional yoga teacher to develop an exercise program that suits her needs.</p>
<p>Based on the information you provided, your mother's osteoporosis symptoms may be severe and require formalized treatment. The following is my response: Based on the information you provided, your mother's T-value of -3.5 and Z-value of -2.1 indicate that she has low bone density, which may lead to an increased risk of fracture. Therefore, her condition may be more severe. Formalized treatment usually includes the following: Medications: These include calcium and vitamin D supplements, calcitonin, estrogen, and other medications that can help increase bone density and reduce the risk of fracture. Dietary modifications: Add calcium and vitamin D-rich foods to your diet, such as milk, fish, and beans. Exercise: Proper exercise can help increase bone density and bone mass. Lifestyle changes: Reduce bad habits such as smoking and alcohol consumption and maintain proper weight and diet. The medication to be injected may be calcitonin or estrogenic drugs, which need to be injected according to the doctor's recommendation. These medications usually require inpatient injections, and the cost may vary by region and hospital. Osteoporosis may have a greater impact on life as it can lead to fractures, physical pain, and mobility problems. Your mother is advised to take care of herself in her daily life to avoid falls and other accidental injuries. Your mother is advised to increase her diet with foods rich in calcium and vitamin D, such as milk, fish, and beans. Proper exercise can help increase bone density and bone mass. She is also advised to reduce bad habits such as smoking and alcohol consumption and maintain a proper weight and diet. If she has other health problems, such as high blood pressure and diabetes, she should also consult her doctor promptly.</p>
<p>Hi, I understand the concerns you and your mother are having. I will try to answer your questions and provide some help and advice: 1. Based on the information you have provided; it appears that your mother has been diagnosed with osteoporosis (osteopenia) and her T value is -3.5 and Z value -2.1 indicating that she is already in a moderate to severe state of reduced bone density. This is a common bone disease that is often associated with age, gender, and other factors. If left untreated and unmanaged it may lead to an increased risk of complications such as fractures. Therefore, timely treatment and management is needed to control progression and reduce risk. 2. The aspects of formalized treatment include both pharmacological and non-pharmacological interventions. Non-pharmacological treatments include exercise (e.g., walking or yoga) to strengthen muscles and improve balance, and a balanced diet and avoidance of smoking and alcohol. The specific treatment method should be determined by the patient's specific situation and under the guidance of a professional doctor. 3. I am not sure about the specific name of the drug, but, the injectable drugs for osteoporosis are mainly bisphosphonate preparations such as alendronate or other similar substances. These medications can help prevent fractures by strengthening bones and promoting new bone formation. The need for hospitalization depends on the circumstances and the medication regimen and may require hospitalization if long-term observation and monitoring is required. Costs will vary depending on the case, so you can check with your local health care provider for details. The impact on daily life varies from person to person. Some people may not be affected as much, but others may have pain or other symptoms that affect their daily activities and work. For older people like your mother, it is very important to maintain a good lifestyle, such as maintaining a moderate amount of physical activity and a proper dietary structure to help maintain good health. It is also important to follow the doctor's instructions for regular check-ups to monitor the effectiveness of the treatment and adjust the treatment plan. Finally, I would like to remind you that the above is only general advice and information, if you have any questions or uncertainties, it is best to consult a professional physician for more accurate advice and solutions. I hope my reply will be helpful to you!</p>
<p>Comments</p>
<p>Each of the questions was well-answered with detailed clarification including advice on both treatment plans and lifestyle habits. Answers to yoga related question was elaborated in this answer as well.</p>
<p>Each of the questions was well-answered but with fewer details. Both treatment and lifestyle habits recommendations were provided. Overall, the responses were moderate.</p>
<p>Each of the questions was well-answered with detailed clarification including advice on both treatment plans and lifestyle habits. A clear serial number on each question correspondingly. Both the correctness and humanistic concern of the responses were satisfactory. </p>
<p>Figure 1 :
1Comments of detailed case reports.</p>
<p>Table 1 :
1Summary of benchmark assessment.Datasets 
Assessment aspects 
Assessment approaches </p>
<p>Medical Dialogue 
medical professional capabilities, social comprehensive capabilities, contextual capabilities maximum score of 16 
computational robustness 
percentage </p>
<p>Case Report </p>
<p>medical professional capabilities, social comprehensive capabilities 
maximum score of 13 
computational robustness 
percentage 
comprehensive review 
comments </p>
<p>Table 2 :
2Summary of evaluation aspects, indicators, criteria, and datasets.Evaluation aspects 
Datasets 
Evaluation criteria 
Score </p>
<p>Medical Professional Capabilities 
Both 
Accuracy * 
No medical knowledge errors are present in the answer 
1 
Informativeness </p>
<p>Table 3 :
3The content performances of chatbots in medical scenarios on multiple-turn dialogues.Evaluation Indicators 
Chatbots 
ChatGPT ERNIE Bot Dr. PJ </p>
<p>Total score 
13.41 
12.56 
13.95 </p>
<p>Medical professional capabilities 
6.30 
5.63 
6.25 
Accuracy 
0.91 
0.79 
0.94 
Informativeness 
1.40 
1.22 
1.31 
Expansiveness 
0.19 
0.12 
0.17 
Logic 
1.81 
1.50 
1.84 
Prohibitiveness 
1.00 
1.00 
1.00 
Sensitivity 
1.00 
1.00 
1.00 </p>
<p>Social comprehensive capabilities 
4.26 
4.33 
4.70 
Comprehension 
0.96 
0.96 
0.96 
Tone 
0.96 
1.00 
1.00 
Empathy 
0.70 
0.70 
0.85 
Social decorum 
1.63 
1.67 
1.89 </p>
<p>Contextual capabilities 
2.85 
2.59 
3.00 
Repeated answer 
0.96 
0.81 
1.00 
Anaphora matching 
0.96 
0.85 
1.00 
Key information 
0.93 
0.93 
1.00 </p>
<p>Table 4 :
4The content performances of chatbots in medical scenarios with the case report.Evaluation Indicators 
Chatbots 
ChatGPT ERNIE Bot Dr. PJ </p>
<p>Total score 
8.71 
8.00 
10.14 </p>
<p>Medical professional capabilities 
6.43 
5.71 
6.86 
Accuracy 
0.86 
0.71 
1.00 
Informativeness 
1.43 
1.14 
1.43 
Expansiveness 
0.43 
0.43 
0.43 
Logic 
1.71 
1.43 
2.00 
Prohibitiveness 
1.00 
1.00 
1.00 
Sensitivity 
1.00 
1.00 
1.00 </p>
<p>Social comprehensive capabilities 
2.29 
2.29 
3.29 
Comprehension 
1.00 
1.00 
1.00 
Tone 
0.29 
0.14 
0.71 
Empathy 
0.00 
0.14 
0.29 
Social decorum 
1.00 
1.00 
1.29 </p>
<p>Table 5 :
5The robustness of 3 chatbots for the medical consultation detailed answer task. Values are expressed as percentages (%).Abbreviations: ASR, adversarial success rate; NSR, noise success rate; IESR, input error success rate; R1, semantic consistency rate; R2, semantically inconsistent but medically sound; R3, complete error rate.Chatbots 
Anomaly Category 
Datasets 
R1 
R2 
R3 </p>
<p>ChatGPT </p>
<p>ASR 
Dataset-A 
15 
65 
20 
NSR 
Dataset-B 
15 
55 
30 
IESR 
Dataset-C 
0 
100 
0 
Dataset-D 
30 
40 
30 
Dataset-E 
20 
80 
0 </p>
<p>ERNIE Bot </p>
<p>ASR 
Dataset-A 
10 
85 
5 
NSR 
Dataset-B 
0 
100 
0 
IESR 
Dataset-C 
0 
100 
0 
Dataset-D 
20 
80 
0 
Dataset-E 
20 
80 
0 </p>
<p>Dr. PJ </p>
<p>ASR 
Dataset-A 
15 
80 
5 
NSR 
Dataset-B 
35 
65 
0 
IESR 
Dataset-C 
60 
40 
0 
Dataset-D 
50 
40 
10 
Dataset-E 
80 
20 
0 </p>
<p>Email:xujie@pjlab.org.cn.
https://qr02.cn/DBeS9U</p>
<p>Why chatgpt is such a big deal for education. A Brent, Anders, C2C Digital Magazine. 1184Brent A Anders. 2023. Why chatgpt is such a big deal for education. C2C Digital Magazine, 1(18):4.</p>
<p>. John W Ayers, Adam Poliak, Mark Dredze, Eric C Leas, Zechariah Zhu, Jessica B Kelley, Dennis J Faix, Aaron M Goodman, A Christopher, John W. Ayers, Adam Poliak, Mark Dredze, Eric C. Leas, Zechariah Zhu, Jessica B. Kelley, Den- nis J. Faix, Aaron M. Goodman, Christopher A.</p>
<p>Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. Michael Longhurst, Davey M Hogarth, Smith, 10.1001/jamainternmed.2023.1838JAMA Internal Medicine. Longhurst, Michael Hogarth, and Davey M. Smith. 2023. Comparing Physician and Artificial Intel- ligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. JAMA Internal Medicine.</p>
<p>Conversational ai models for ophthalmic diagnosis: Comparison of chatgpt and the isabel pro differential diagnosis generator. Michael Balas, Edsel B Ing, 10.1016/j.jfop.2023.100005JFO Open Ophthalmology. 1100005Michael Balas and Edsel B. Ing. 2023. Conversational ai models for ophthalmic diagnosis: Comparison of chatgpt and the isabel pro differential diagnosis gen- erator. JFO Open Ophthalmology, 1:100005.</p>
<p>How should my chatbot interact? a survey on social characteristics in human-chatbot interaction design. Ana Paula Chaves, Marco Aurelio Gerosa, 10.1080/10447318.2020.1841438International Journal of Human-Computer Interaction. 378Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbot interact? a survey on so- cial characteristics in human-chatbot interaction de- sign. International Journal of Human-Computer Interaction, 37(8):729-758.</p>
<p>Unlocking the drivers of big data analytics value in firms. Nadine Côrte-Real, Pedro Ruivo, Tiago Oliveira, Aleš Popovič, 10.1016/j.jbusres.2018.12.072Journal of Business Research. 97Nadine Côrte-Real, Pedro Ruivo, Tiago Oliveira, and Aleš Popovič. 2019. Unlocking the drivers of big data analytics value in firms. Journal of Business Research, 97:160-173.</p>
<p>A review of challenges and opportunities in machine learning for health. Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, L Andrew, Beam, Y Irene, Rajesh Chen, Ranganath, AMIA Summits on Translational Science Proceedings. 191Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, Irene Y Chen, and Rajesh Ran- ganath. 2020. A review of challenges and opportuni- ties in machine learning for health. AMIA Summits on Translational Science Proceedings, 2020:191.</p>
<p>Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. 2022. Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5.</p>
<p>Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T Sio, Lisa A Mcgee, Jonathan B Ashman, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu, Evaluating large language models on a highly-specialized topic, radiation oncology physics. Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T. Sio, Lisa A. McGee, Jonathan B. Ashman, Xiang Li, Tianming Liu, Jiajian Shen, and Wei Liu. 2023. Evaluating large language mod- els on a highly-specialized topic, radiation oncology physics.</p>
<p>Touch your heart: A tone-aware chatbot for customer care on social media. Tianran Hu, Anbang Xu, Zhe Liu, Quanzeng You, Yufan Guo, Vibha Sinha, Jiebo Luo, Rama Akkiraju, 10.1145/3173574.3173989Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI '18. the 2018 CHI Conference on Human Factors in Computing Systems, CHI '18New York, NY, USAAssociation for Computing MachineryTianran Hu, Anbang Xu, Zhe Liu, Quanzeng You, Yu- fan Guo, Vibha Sinha, Jiebo Luo, and Rama Akki- raju. 2018. Touch your heart: A tone-aware chatbot for customer care on social media. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI '18, page 1-12, New York, NY, USA. Association for Computing Machin- ery.</p>
<p>Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model. Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman, Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman, Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, et al. 2023. Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model.</p>
<p>The future of ai in medicine: a perspective from a chatbot. R Michael, King, Annals of Biomedical Engineering. 512Michael R King. 2023. The future of ai in medicine: a perspective from a chatbot. Annals of Biomedical Engineering, 51(2):291-295.</p>
<p>Considering the possibilities and pitfalls of generative pre-trained transformer 3 (gpt-3) in healthcare delivery. M Diane, Sean D Korngiebel, Mooney, NPJ Digital Medicine. 4193Diane M Korngiebel and Sean D Mooney. 2021. Con- sidering the possibilities and pitfalls of generative pre-trained transformer 3 (gpt-3) in healthcare deliv- ery. NPJ Digital Medicine, 4(1):93.</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, Victor Tseng, 10.1371/journal.pdig.0000198PLOS Digital Health. 22Tiffany H. Kung, Morgan Cheatham, Arielle Mede- nilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. 2023. Performance of chatgpt on usmle: Poten- tial for ai-assisted medical education using large lan- guage models. PLOS Digital Health, 2(2):1-12.</p>
<p>Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. Peter Lee, Sebastien Bubeck, Joseph Petro, 10.1056/NEJMsr221418436988602New England Journal of Medicine. 38813Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233-1239. PMID: 36988602.</p>
<p>Ateev Mehrotra, and Andrew Beam. 2023. The diagnostic and triage accuracy of the gpt-3 artificial intelligence model. Rudraksh David M Levine, Benjamin Tuwani, Amita Kompa, Samuel G Varma, Finlayson, 10.1101/2023.01.30.23285067medRxivDavid M Levine, Rudraksh Tuwani, Benjamin Kompa, Amita Varma, Samuel G. Finlayson, Ateev Mehro- tra, and Andrew Beam. 2023. The diagnostic and triage accuracy of the gpt-3 artificial intelligence model. medRxiv.</p>
<p>Towards standard criteria for human evaluation of chatbots: A survey. Hongru Liang, Huaqing Li, Hongru Liang and Huaqing Li. 2021. Towards stan- dard criteria for human evaluation of chatbots: A survey.</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 559Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9).</p>
<p>. Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao Ge. 2023b. Summary of chatgpt/gpt-4 research and perspective towards the future of large language modelsYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Da- jiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao Ge. 2023b. Summary of chatgpt/gpt-4 research and perspective towards the future of large language models.</p>
<p>Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?. M Thang, Trung Pham, Long Bui, Anh Mai, Nguyen, Thang M. Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021. Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?</p>
<p>Assessing the utility of chatgpt throughout the entire clinical workflow. medRxiv. S Arya, Michael Rao, John Pang, Meghana Kim, Winston Kamineni, Lie, K Anoop, Adam Prasad, Keith Landman, Marc D Dryer, Succi, Arya S Rao, Michael Pang, John Kim, Meghana Kami- neni, Winston Lie, Anoop K Prasad, Adam Land- man, Keith Dryer, and Marc D Succi. 2023. Assess- ing the utility of chatgpt throughout the entire clini- cal workflow. medRxiv, pages 2023-02.</p>
<p>Chatgpt: Optimizing language models for dialogue. J Schulman, Zoph, Kim, Hilton, Menick, Weng, L Uribe, Fedus, Metz, Pokorny, J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, JFC Uribe, L Fedus, L Metz, M Pokorny, et al. 2022. Chatgpt: Optimizing language models for dialogue.</p>
<p>Operationalizing and implementing pretrained, large artificial intelligence linguistic models in the us health care system: Outlook of generative pretrained transformer 3 (gpt-3) as a service model. Emre Sezgin, Joseph Sirrianni, Simon L Linwood, 10.2196/32875JMIR Med Inform. 10232875Emre Sezgin, Joseph Sirrianni, and Simon L Linwood. 2022. Operationalizing and implementing pre- trained, large artificial intelligence linguistic models in the us health care system: Outlook of generative pretrained transformer 3 (gpt-3) as a service model. JMIR Med Inform, 10(2):e32875.</p>
<p>Evaluation of chatgpt as a question answering system for answering complex questions. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi, Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023. Evaluation of chatgpt as a question answering system for answer- ing complex questions.</p>
<p>What This Computer Needs Is a Physician: Humanism and Artificial Intelligence. Abraham Verghese, H Nigam, Robert A Shah, Harrington, 10.1001/jama.2017.19198JAMA. 3191Abraham Verghese, Nigam H. Shah, and Robert A. Harrington. 2018. What This Computer Needs Is a Physician: Humanism and Artificial Intelligence. JAMA, 319(1):19-20.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompt- ing elicits reasoning in large language models.</p>
<p>Ai and the fci: Can chatgpt project an understanding of introductory physics?. Colin G West, Colin G. West. 2023. Ai and the fci: Can chatgpt project an understanding of introductory physics?</p>
<p>. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgptJules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineer- ing with chatgpt.</p>
<p>Exploring the limits of chatgpt for query or aspect-based text summarization. Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, Wei Cheng, Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. Exploring the limits of chat- gpt for query or aspect-based text summarization.</p>            </div>
        </div>

    </div>
</body>
</html>
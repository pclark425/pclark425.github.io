<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7627 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7627</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7627</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267412588</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02544v1.pdf" target="_blank">LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model</a></p>
                <p><strong>Paper Abstract:</strong> . The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS 1 -Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Additionally, we introduce LHRS-Bench, a benchmark for thoroughly evaluating MLLMs’ abilities in RS image understanding. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain 2 .</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7627",
    "paper_id": "paper-267412588",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0056142499999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
16 Jul 2024</p>
<p>Dilxat Muhtar dmuhtar@smail.nju.edu.com 
Zhenshi Li lzhenshi@outlook.com 
Feng Gu gufeng@smail.nju.edu.com 
Xueliang Zhang 
Pengfeng Xiao xiaopf@nju.edu.cn </p>
<p>Nanjing University</p>
<p>LHRS stands for 'Language Helps Remote Sensing'</p>
<p>LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
16 Jul 2024DBBE92283FE88F46A18B75213795D5B4arXiv:2402.02544v4[cs.CV]MultimodalLarge language modelRemote sensing
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains.In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors.To bridge this gap, we construct a large-scale RS image-text dataset, LHRS 1 -Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images.Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method.Additionally, we introduce LHRS-Bench, a benchmark for thoroughly evaluating MLLMs' abilities in RS image understanding.Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain 2 .</p>
<p>Introduction</p>
<p>The recently advanced large language models (LLMs) have presented remarkable capabilities in engaging in informative conversations and solving complex problems [2,8,11,26,57,59,60].To extend the perceptual capabilities of LLMs, multimodal large language models (MLLMs) bring eyes to LLMs.These models, by aligning with visual representations and visual instruction tuning, have demonstrated impressive multimodal instruction following capabilities and can act as a general-purpose interface for a broad range of tasks [1,3,25,32,34,39,85].This success in general domain has served as a catalyst for the development of domain specialized MLLMs, with notable applications in biomedicine [33], autonomous driving [43], and robotics [16].</p>
<p>Among various specialized domains, remote sensing (RS) image understanding is particularly important, as it directly involves the monitoring and analysis of the Earth's surface and environment [5,50,64,72].However, RS images present significant challenges for holistic image understanding due to the intricate complexity of different objects across diverse landscapes worldwide, coupled with the variability in visual scales [22,44,51].MLLMs bring together vision and language, offering possibilities for a unified and interactive understanding of RS images [64].In this direction, several RS-specific MLLMs have been proposed, such as RSGPT [24], GeoChat [28], and SkyEyeGPT [78].However, they are struggling to fit existing public RS datasets, due to the oversight of exploiting the extensive and worldwide RS features to infuse comprehensive RS visual knowledge into LLMs.Furthermore, these contemporary approaches of bridging vision and language domains tend to focus on the incorporation of high-level visual semantics.This strategy neglects the fact that different levels of visual information are crucial to fully align the linguistic and visual domains, which could facilitate a unified RS image understanding at different granularity [44,62,67].Detailed related works are available in supplementary material.</p>
<p>To bridge the gap, we present LHRS-Bot, a specialized MLLM for RS, enhanced by globally available volunteered geographical information (VGI) and worldwide RS images.Specifically, we construct a large-scale, semantically rich, and feature-diverse dataset, LHRS-Align, by geographically pairing RS images with plentiful attributed information from the OpenStreetMap VGI database 3 , and consequently generating image captions using LLMs.Our rigorous data cleaning process, which includes deduplication, pruning, and semantic balancing, ensures that LHRS-Align comprises 1.15 million meaningful and high-quality RS image-text pairs, laying a solid foundation for embedding rich RS visual knowledge into MLLMs.Furthermore, we construct our instruction dataset, LHRS-Instruct, by reorganizing a broad range of open-source RS datasets into multitask instructional datasets and employing  to create complex instructional data from the high-quality subset of LHRS-Align.Building on these datasets, we propose a novel RS-specific MLLM, LHRS-Bot, by implementing a novel bridging strategy for efficiently summarizing multi-level visual representations, alongside a curriculum learning methodology.LHRS-Bot achieves state-of-the-art performance across a variety of RS image understanding tasks (Fig. 1) and demonstrates remarkable abilities to detect intricate objects, engage with human conversation, and provide insights from visual information within RS images (Fig. 5).</p>
<p>Finally, we manually construct a high-quality benchmark, LHRS-Bench, to facilitate the RS community in evaluating the RS-domain MLLMs across diverse evaluation dimensions.LHRS-Bench includes 690 single-choice questions, spanning 5 top-level evaluation dimensions including 11 fine-grained categories to facilitate a comprehensive, objective, and quantitative RS-specific evaluation.</p>
<p>The main contributions of our work are summarized as follows: imagery with geographic coordinates.The geographic features are gathered from OSM, a crowdsource VGI database containing abundant geographical objects.Each object (feature) in the OSM database is annotated as the digitization of the object geometry (e.g., polygons, lines, and points) as well as the semantic tags.A tag is a key-value pair that provides information about a geographic object 5 , for example, "landuse=residential".Thanks to the free access to the globally encompassing data sources, our pipeline can produce a dataset that offers a vast and comprehensive variety on a worldwide scale.For the current version of LHRS-Align, we collect RS images and OSM features within a scope of 0.3°longitude × 0.28°latitude for each of the selected cities from all over the world.We customize our own OSM database to streamline the subsequent feature selection process.More information about our data source and database is introduced in supplementary material.</p>
<p>Geo-alignment between Images and Geographic Features.We iterate through our customized database to identify qualified features serving as "anchors".Based on the geographical extent of each anchor feature, the corresponding RS image is acquired, which is a square bounding box centered on the feature.These images subsequently undergo a stream of postprocessing and pruning procedures.Following this, due to the possibility of multiple features being present within a single image extent, all the features within each image are further identified.At this point, we have established a raw data pool of around 4 million pairs, in which one RS image corresponds to multiple geographical features.Further details for this processes are provided in supplementary material.</p>
<p>Attribute Pruning and Semantic Balancing.In the OSM database, all the features are assigned with their semantic attributes, that is, multiple key-value tags.However, not all the keys are favorable information for visual identification, such as website and postal code.Hence, we filter out irrelevant keys through a pipeline as detailed in supplementary material.Fig. 2 provides an example showcasing an image along with its geographical features and their corresponding attributes.Furthermore, the OSM database exhibits considerable fea-ture imbalance, notably dominated by geographical features such as residential or farmland.Given the importance of balanced semantic information for effective vision-language alignment [48,70], we implement a data balancing method, as detailed in supplementary material, to create a more semantically balanced dataset.As a result, this balancing process retains 1.15 million images remaining out of the original 4 million.</p>
<p>Caption Generation with LLM.The use of image-caption pairs to incorporate visual knowledge into LLMs has been validated as an effective approach [3,14,33,73,85].However, using plain image key-value labels can be problematic, as they lack grammatical structure, potentially disrupting the generative and comprehension abilities of LLMs.Similarly, rule-based caption generation strategies may introduce significant redundancy.Therefore, we opt to use open-source LLMs to generate image captions by summarising the key-value tags of the corresponding images.We experiment with various in-context learning examples and LLMs, ranging in size from 7B to 70B.After comprehensive testing by a domain expert, we select Vicuna-v1.5-13B[10] for caption generation as it achieves the best trade-offs in terms of generation efficiency and quality.Detailed information about the in-context examples and the specific deployment of Vicuna-v1.5-13B[10] can be found in supplementary material.Our final dataset, referred as LHRS-Align, comprises approximately 1.15 million image-caption pairs.Each pair is accompanied with additional metadata, such as image resolution and geographical location (country and city).Several examples of LHRS-Align are also presented in supplementary material.</p>
<p>RS-Specific Instruction-Following Dataset--LHRS-Instruct</p>
<p>We craft a multimodal instruction-following dataset LHRS-Instruct by utilizing two public image-caption datasets and our LHRS-Align dataset.</p>
<p>Instruction-Following Dataset from Public RS Caption Dataset.We start by meticulously evaluating and selecting all image-caption datasets within the RS domain.Our selection process focuses on the quality of captions and involves a thorough human evaluation.This requires each image to be accompanied by five distinct captions, with each caption uniquely detailing different aspects of the image.Based on this criterion, we identify RSITMD [76] and NWPU [9] as the datasets with superior caption quality.Subsequent to selection, we design a rigorous data cleaning process, which involves deduplication, computation of caption lengths, and assessment of relevance through CLIP-score calculations.This results in the extraction of 3.4K and 30K high-quality image-caption pairs from RSITMD and NWPU, respectively.We then use several in-context examples along with the five captions per image for prompting Vicuna-v1.5-13B[10] to generate a conversation that specifically focuses on the content of the objects in the image.Finally, we filter out the most relevant conversations through pre-defined rules and evaluations from three domain experts.This process results in 2.9K and 25K instruction data from these two datasets, respectively.For more detailed information on the in-context examples used and the data filtering criteria, please refer to the supplementary material.</p>
<p>Instruction-Following Dataset from LHRS-Align.Considering the limitations of public image-caption datasets, which typically lack bounding box annotations, and the constrained capabilities of Vicuna-v1.5-13B[10], the generated conversations might omit crucial details such as the locations and quantity of objects in the images, along with visual reasoning information.To mitigate this issue, we meticulously select 15K representative RS images that are rich in attribute information from our LHRS-Align dataset.We then proceed to calculate the bounding boxes for various objects within these images, utilizing the built-in spatial coordinates of different attributes in the OSM database.Leveraging this enriched data, we prompt GPT-4 to craft complex instruction data that incorporates visual reasoning, detailed image descriptions, and conversations about the location and number of objects.The final step involves a manual cleaning of the generated results.This process yields a dataset comprising 0.9K samples about visual reasoning, 4K samples about detailed image description, and 7K samples about conversation.For more details, please refer to supplementary material.</p>
<p>RS-specific MLLM Evaluation Benchmark--LHRS-Bench</p>
<p>We construct LHRS-Bench to evaluate the performance of MLLMs in the RS domain.LHRS-Bench employs hierarchical taxonomies to comprehensively evaluate MLLMs across various dimensions (Tab.1).To prevent data leakage, we abstain from using public RS datasets and instead meticulously collect images from Google Earth.In terms of question format, we utilize single-choice questions From the image, it is evident that the area is primarily dedicated to industrial activities.The presence of multiple industrial buildings and parking areas suggests a high level of industrialization in the area.The absence of visible natural features indicates that the area is primarily developed for industrial purposes.The presence of parking areas and the layout of the industrial facilities indicate that the area is designed to accommodate a high volume of traffic and activities.Overall, the image portrays an industrial area with a focus on manufacturing, production, and other industrial activities.</p>
<p>LoRA</p>
<p>Freeze Trainable Fig. 3: Architecture of the proposed LHRS-Bot.LHRS-Bot employs learnable queries in conjunction with a vision perceiver to summarize multi-level visual representations.These representations are subsequently concatenated with language token embeddings for processing within the LLM.</p>
<p>Method</p>
<p>We present LHRS-Bot to harness the full potential of LLMs and general visual encoders for enhanced RS images understanding and reasoning.This section delves into each components of LHRS-Bot, detailing how we achieve fully alignment between vision and language models across multiple levels.Subsequently, we explain the curriculum training strategy for training LHRS-Bot.</p>
<p>Model Architecture</p>
<p>The overall architecture of LHRS-Bot is presented in Fig. 3. LHRS-Bot is primarily composed of three components: a universal vision encoder, a vision perceiver, and a foundational LLM.Vision Encoder.LHRS-Bot utilizes the pre-trained CLIP visual encoder ViT-L/14 [48] with input resolution of 224×224 pixel for encoding images.The prior works [39, 73,85] usually utilize the final hidden feature for abstracting the semantic content of an image.However, several studies have highlighted that features at different levels of a deep learning model embed distinct types of information [19,35,46].Therefore, we argue that previous image encoding methods fail to sufficiently capture the semantic content of images, leading to a reliance on extensive image-text pairs for vision-language alignment.Consequently, we retain multiple hidden features from various layers of the image encoder.This approach offers a more thorough representation of the underlying image, facilitating a thorough and efficient domain alignment.The shadows cast by the aircraft and vehicles suggest the sun is at a moderate angle, implying the image was taken either in the morning or afternoon.However, without knowing the orientation of the airport, it's impossible to determine the exact time of day.This image depicts an airport.</p>
<p>[0.  Vision Perceiver.Incorporating multi-level visual features brings extended image tokens, increasing computational overhead and potentially overwhelming language information.To mitigate this issue, we introduce a set of learnable queries for each level of image features.These queries are designed to summarize the semantic information of each level of visual features through a series of stacked cross-attention and MLP layers.Once summarized, these image features are then fed into the LLM.Furthermore, we implement a descending query allocation strategy inspired by the observation that image feature redundancy tends to increase with network depth [7,27].This strategy allocates more queries to the shallower image feature levels while gradually reducing the number of queries for deeper levels.</p>
<p>Large Language Model.We employ the LLaMA2-7B [60] as the central expert for interpreting visual and language information to output the desired results following the previous practives [28,33,52].</p>
<p>Curriculum Learning Strategy</p>
<p>We implement a three-stage curriculum learning strategy by progressively increasing the difficulty of training tasks (Fig. 4).This strategy incrementally aligns the vision encoder with the LLM, ultimately enabling the LLM to solve multiple visual tasks and follow complex vision-language instructions.</p>
<p>Stage 1: Pre-training.In the first stage, we use the large-scale, weakly labelled LHRS-Align dataset (Sec.2.1) to train the vision perceiver to incorporate a wide range of RS visual knowledge into the LLM.Specifically, for each sample in the LHRS-Align dataset, the output visual embedding tokens X v obtained from the vision perceiver are concatenated with the language embedding tokens X l from the corresponding image caption.The concatenated result is then fed into the LLM.The objective of this stage is to maximise the conditional probability P (X l |X v ; θ) of generating the caption tokens X l given the prefix visual tokens X v :
L = log P (X l |X v ; θ) = L i=1 log P (x i |X v , X l,&lt;i ; θ),(1)
where L is the length of caption tokens, θ represents the trainable parameters, and X l,&lt;i denotes the generated tokens prior to the current prediction token x i .</p>
<p>Stage 2: Multi-task Pre-training.In the second stage, we further train the vision perceiver and efficiently fine-tune the LLM using LoRA [23] on multitask instruction datasets to improve multimodal task solving capabilities of LLM.The construction of the multi-task instruction datasets begins with the collection of diverse public datasets in the RS domain, encompassing tasks such as classification, VQA, and visual grounding.These collected datasets are then meticulously transformed into instruction-based formats using manually designed templates.More detailed information about these multi-task datasets and the templates used can be found in the supplementary material.Moreover, we integrate the instruction data from NWPU, RSITMD and the 4K detailed descriptions data from LHRS-Instruct (Sec.2.2) to further enrich the LLM with fine-grained RS visual knowledge.The objective for this stage remains similar to Eq. ( 1) from stage 1, albeit with an adaptation that the generated tokens are now additionally conditioned on the instruction tokens X instruct :
L = log P (X l |X v , X instruct ; θ) = L i=1 log P (x i |X v , X instruct , X l,&lt;i ; θ).(2)
Stage 3: Supervised Fine-tuning.In the final stage, our focus shifts towards enhancing the conversational and reasoning capabilities of our model by incorporating more complex instruction data.We utilize all the instruction data from the LHRS-Instruct dataset (Sec.2.2) and randomly select 20K data from the LLaVA complex reasoning dataset [33].Additionally, we mix the above datasets with the multi-task datasets from stage 2 using a lower data sampling ratio to further unleash the multi-task problem-solving ability of LHRS-Bot.</p>
<p>Experiment</p>
<p>We conduct comprehensive experiments across image classification, VQA, and visual grounding tasks within the RS domain.These experiments are designed to rigorously validate the superior multi-task solving capabilities of LHRS-Bot.Furthermore, we evaluate the performance of various LLMs in the RS domain using the proposed LHRS-Bench benchmark.We present both quantitive and quality results.</p>
<p>Experimental Setup</p>
<p>Dataset.We select diverse RS datasets for providing a comprehensive assessment across different tasks within the RS field: seven RS classification datasets (namely AID [68], WHU-RS19 [13], NWPU [9], SIRI-WHU [86], EuroSAT [20], METER-ML [84], and fMoW [12]), two VQA datasets (LR and HR subsets of RSVQA [41]), and two visual grounding datasets (RSVG [56] and DIOR-RSVG [77]).We exclude any data from the test sets that overlap with the training set to avoid data leakage.More details about each test dataset are presented in supplementary material.</p>
<p>Baseline.We evaluate LHRS-Bot and various powerful open-source MLLMs, including LLaVA-v1.5 [38], MiniGPT-v2 [85], InstructBLIP [14], mPLUG-Owl2 [73], QWen-VL-Chat [3], and InternLM-Xcomposer [79], on different tasks.Furthermore, we also include the results of RSGPT [24], GeoChat [28] and SkyEyeGPT [78] from the original papers for comparisons whenever possible.Unless otherwise noted, we utilized 7B variants for all our experiments.</p>
<p>Implementation.We extract hidden states at layers {i = L/3, j = 2L/3, k = L − 1} for summarizing through vision perceiver, where L denotes the number of layers in the vision encoder.Our vision perceiver implementation comprises six layers of cross-attention and MLP, with each hidden state assigned {64, 48, 32} learnable queries.We apply LoRA module to every linear layer of the LLM, setting the rank r and the scale factor α for LoRA to 128 and 256, respectively.Moreover, we add task identifier [CLS], [VQA], and [VG] for classification, VQA, and visual grounding tasks, respectively, for enabling LHRS-Bot to better distinguish different tasks [85].For additional details on the hyperparameters used at each stage, please refer to supplementary material.</p>
<p>Quantitative Evaluation</p>
<p>We evaluate different MLLMs using greedy decoding with the same prompt for each task and report the average result of two independent trials.Details about the evaluation prompts are provided in supplementary material.</p>
<p>Classification.The classification results across different datasets are presented in Tab. 2. These results show that LHRS-Bot outperforms all competing models in all seven classification datasets, with a notable margin of 27.51% higher average accuracy than the second ranked method.It is worth mentioning that, except for NWPU, METER-ML, and fMoW, the remaining datasets are completely absent from the multi-task training dataset 6 , demonstrating the strong  open-source models in terms of the evaluation dimensions of spatial resolution and image modality, even though these aspects were not explicitly trained for in the models, highlighting the effectiveness of RS-specific MLLM training.Additionally, LHRS-Bot outperforms the strong closed-source model Claude and even matches the performance of GPT-4V.However, there is still significant room for improvement in MLLM capabilities within the RS domain.Examples of evaluation results are available on supplementary material.</p>
<p>Qualitative Evaluation</p>
<p>We conduct qualitative demonstrations to illustrate the extensive capabilities of LHRS-Bot in interpreting free-form RS images, as showcased in Fig. 5.The results demonstrate that LHRS-Bot not only comprehends the overall content of an image but also exhibits a strong ability for discerning fine-grained details.Moreover, LHRS-Bot possesses a perceptive understanding of the orientation, position, and quantity of objects within an image.Furthermore, LHRS-Bot is capable of making reasoned inferences from the images and acknowledges its limitations by indicating uncertainty when it cannot provide a definitive answer.</p>
<p>Ablation Analysis</p>
<p>In this subsection, we carry out extensive ablation experiments to demonstrate the effectiveness of our dataset as well as the model design.</p>
<p>What can you see from this image?</p>
<p>From this image, I can see a beautiful and well-organized city with many water bodies, including lakes.The city appears to be a mix of residential and commercial areas, with several buildings of different heights and architectural styles.Additionally, there is a golf course, which suggests that the city provides leisure activities for its residents.</p>
<p>How many water bodies can you see ?</p>
<p>Based on the image, I can see at least five water bodies.</p>
<p>[VG] The coordinates of the largest water bodies.From this image, I can infer that the area depicted is a densely populated residential neighborhood with many houses and trees.The neat arrangement of the houses and the presence of roads suggest a wellplanned and organized community.The image also suggests that the area is relatively affluent, as there are many white buildings and a dense concentration of houses.</p>
<p>How many buildings?</p>
<p>There are many buildings in the dense residential area.The exact number is not visible, but there appear to be at least a dozen or more buildings in the area.</p>
<p>Describe the primary type of infrastructure visible in the center of the image.</p>
<p>The image prominently features a highway interchange with multiple lanes How many major roadways can you identify in the image?There appear to be at least two major roadways intersecting.One is a multi-lane highway running vertically, and the other is a horizontal roadway crossing it.Effectiveness of LHRS-Align dataset.There are already many large-scale image-caption datasets in the field of RS [63,82].To evaluate how different alignment datasets impact model performance, we substitute our LHRS-Align dataset with various image-caption datasets and repeat the entire training process.This process is then evaluated across three distinct tasks: classification, VQA, and visual grounding.The experimental setup remains consistent with Sec.4.1, with the exception of the dataset used in the first stage.</p>
<p>The results are presented in Tab. 6. Notably, despite possessing the smallest dataset (RS5M: 5M, SkyScript: 1.5M, LHRS-Align : 1.15M), LHRS-Align outperformes other settings.It is worth mentioning that both SkyScript and LHRS-Align, which leverage RS images combined with the OSM database, achieve results comparable to or better than the even larger RS5M, which is derived from publicly available natural image datasets, such as LAION [53].This highlights the importance of domain-specific data collection.Additionally, superior performance of LHRS-Align over SkyScript suggests that enriched captions and a meticulous data cleaning pipeline can substantially enhance the effectiveness of MLLMs.</p>
<p>Effectiveness of Model Design.We evaluate the effectiveness of designs involving multi-level visual hidden representations as well as descending query allocation strategies.Specifically, our evaluation focuses on comparing with the single level visual hidden state, projecting it through a two-layer MLP or a vision perceiver.We also perform comparisons between multi-level visual hidden representations, employing either identical queries for each level or incremental query allocation strategies.Building on the concept introduced by the token merging [7], we explore the potential of integrating this strategy with a vision perceiver to efficiently leverage multi-level representations.To minimize experimental costs, our training process comprises just two stages: firstly, we employ LHRS-Align to train the bridge layer, which aligns the vision and language domains.Subsequently, we train the model using the LHRS-Instruct and multi-task instruction datasets.</p>
<p>We report the average result on each task in Tab. 7. It is evident that the approach employed by LHRS-Bot achieves superior performance.The results further illustrate that, although the extraction of a singular visual representation performs better in tasks such as classification and VQA tasks, which depends on high-level visual semantics, it falls short in visual grounding tasks that require detailed visual information.Furthermore, employing the same number of queries to summarise different levels of visual information does not lead to better results.In contrast, adopting a diminishing allocation strategy yields superior results.This further demonstrates the large redundancy among the deep visual tokens [7].Lastly, the token merging strategy proves ineffective, possibly due to the loss of crucial information during the merging process.</p>
<p>Conclusion</p>
<p>We propose LHRS-Bot, an MLLM for the RS domain.To unleash the potential of LLMs for RS image understanding, we curate a large-scale dataset, LHRS-Align, for RS-specific alignment, and LHRS-Instruct, a multimodal instructionfollowing dataset to enhance LHRS-Bot's instruction-following capabilities.Additionally, we introduce LHRS-Bench, a benchmark providing a systematic evaluation framework for RS MLLMs.</p>
<p>In comprehensive comparison experiments, LHRS-Bot demonstrates exceptional RS domain knowledge, surpassing both general-purpose and RS-specific MLLMs.Despite its impressive performance, LHRS-Bot shares limitations common to LLMs, such as susceptibility to hallucinations.We believe that developing higher-quality alignment and instructional datasets, along with improved training strategies, can further enhance the performance of MLLMs for interpreting RS images.The content of each section for the appendix is as follows:</p>
<p>-Appendix A givens an in-depth review of related work.</p>
<p>-Appendix B presents explanation of the methods used to construct the LHRS-Align dataset.-Appendix C outlines the steps taken to develop the LHRS-Instruct dataset.</p>
<p>-Appendix D details the process of converting the public remote sensing (RS) datasets into our multitask instruction dataset.</p>
<p>-Appendix E provides detailed information about the evaluation datasets.</p>
<p>-Appendix F explains the evaluation dimensions of our LHRS-Bench benchmark.-Appendix G lists the training hyperparameters used.</p>
<p>-Appendix H includes the prompts used for evaluation on each task.</p>
<p>A Related Work</p>
<p>A.1 Multimodal Large Language Models</p>
<p>With the remarkable capabilities of LLMs in natural language processing [8,11,21,59,65,80] and their successful applications across various domains [15,36,54,55,58,61,66,71,74,81], LLMs have been extended to MLLMs by aligning visual inputs with LLMs and further empowered by visual instruction tuning to address a serial of cross-modal tasks [1,3,17,33,34,85].The progression in MLLMs has catalyzed the creation of domain-specific MLLMs, encompassing areas such as biomedicine [33], autonomous driving [43], and RS [28,78].</p>
<p>A.2 Remote Sensing Vision-Language Models</p>
<p>Early RS vision-language models were designed for specific tasks, such as VQA [41], visual grounding [77], and scene classification [37].RSGPT [24] is the initial work which exploits LLMs for RS vision-language tasks.However, it necessitates specific fine-tuning for each task, limiting its potential for holistic RS image understanding.Our concurrent works, GeoChat [28] and SkyEyeGPT [78] introduce RS-specific MLLMs which can also address various RS image understanding tasks and follow human instructions.However, these models are struggled to adapting to existing public RS datasets, thus failing to utilize the extensive and global RS images to enrich LLMs with comprehensive RS visual knowledge.In contrast, we unleash the full potential of LLMs for RS applications by leveraging the abundant geographical information within global available RS images and employing a more effective multi-level alignment strategy.</p>
<p>A.3 Large-Scale Remote Sensing Vision-Language Datasets</p>
<p>The RS5M dataset [82], comprised of 5 million RS images, is the first largescale RS image-text paired dataset.This dataset is curated by filtering publicly available general datasets.Despite the extensive scale, it falls short in providing orthorectified satellite images and offers insufficient geographical descriptions of scenes and objects.SkyScript [63], a dataset developed concurrent with ours, utilizes open-source geographical data to create a large-scale image-text dataset.However, their image captions, created using predefined rules and not thoroughly refined by a rigorous cleaning pipeline, fall short in providing high-quality and rich visual semantics.In this work, we develop a large-scale vision-language RS alignment dataset utilizing the semantically abundant VGI database and global RS images.By implementing a rigorous data curation pipeline and employing LLMs to generate meaningful descriptions from feature tags, the proposed dataset is proven more informative for pretraining an MLLM.</p>
<p>B Details about LHRS-Align Dataset</p>
<p>B.1 Data Source</p>
<p>We have collected images that cover the most populous cities from certain countries.Due to challenges such as the scarcity of OSM data for certain cities (particularly in underdeveloped regions), the remaining images cover 9,259 cities across 129 countries.Considering the spatial resolution limitation of RS images, we only utilize the polygon features of OSM for the current version of our dataset, with the extension of polyline and point features left to our future works.Furthermore, we discard the polygon features tagged with the keys "boundary" and "barrier", as they do not exactly describe the function of the feature.</p>
<p>B.2 Geo-alignment and Image Processing</p>
<p>We search the database and find out qualified features as anchors to locate the corresponding RS images.The principle is choosing features whose areas are larger than a 128×128 pixel image and have aspect ratios less than 4.This is to ensure that the chosen features are adequately large and do not exhibit excessive slenderness.For the RS images, we resize any image larger than 768×768 pixels down to 768×768.Subsequently, the images undergo a deduplication process with fastdup7 and a pruning step, which involves removing images that predominantly feature vast ocean areas or are obscured by clouds.The latter process is facilitated by employing a trained classification network.These postprocessing procedures reduce the samples from 7M to 4M.Based on the geographical extent of each image, the corresponding features are selected out whose spatial area is smaller than 1/64 of the image's size.The spatial areas of the features and images are calculated in Web Mercator projection.</p>
<p>B.3 Filtering Keys</p>
<p>We use a method combining initial automatic filtering and subsequent manual filtering.In the automatic filtering phase, we eliminate keys that meet any of these criteria: the key contains the string "name" or "addr", all values under the key have no characters, or the variety of values is fewer than 3. Through this approach, the number of keys is reduced to 1,885 from the original 10,244.In the manual filtering stage, three experts individually determine if the key is useful for providing semantic information, according to the corresponding values (via an annotation interface depicted in Fig. 6) and its description on OSM wiki.A key is preserved only if there is unanimous agreement among all three experts regarding its retention.This process further narrows the count to 186 keys, which are presented in Tab.11.Note that this is the only step involving human discrimination throughout the pipeline, which takes only around 2 hours for each of the three experts.</p>
<p>B.4 Semantic Balancing</p>
<p>Due to the fact that an image corresponds to multiple geographical features, and each geographical feature corresponds to multiple attributes, we remove the duplicate key-value pairs within each image and then calculate the counts of all key-value pairs across the entire dataset.Based on the key-value statistics, we first remove the images whose key-value pairs are all larger than the magic number t = 20k (the threshold used to limit the count of key-value pairs within the whole dataset).After that, we select out images from the removed images using the independent sampling method [70] for supplement to the remained images in the above step.This method results in a more balanced dataset.</p>
<p>B.5 Details about Generating Captions</p>
<p>To generate captions for each image in the LHRS-Align dataset, we utilize Vicuna-v1.5-13B,incorporating the vLLM [29] and fast-chat [83] libraries.We explore various settings for the generation sampling parameters, adjusting the temperature across {0.4,0.5, 0.6, 0.7, 0.85, 0.9} and top_p across {0.8, 0.85, 0.9, 0.95}.Ultimately, we select a temperature of 0.7 and a top_p of 0.95 for optimal performance.The utilized in-context examples are detailed in Tab.12.</p>
<p>C Details about LHRS-Instruct Dataset</p>
<p>C.1 Instruction-Following Dataset from Public RS Caption Dataset Data Filtering Procedure.The data filtering process for the public caption datasets RSITMD [76] and NWPU [9] involves several steps:</p>
<p>-Image deduplication is conducted using the fastdup tool.</p>
<p>-Each of the five captions for an image is tokenized, and captions shorter than 10 tokens are removed.-We employ OpenAI's CLIP-L/14 [48] to calculate the average similarity between each of the five captions and their corresponding image.Samples with similarity scores below 15% are filtered out from the dataset.This threshold was chosen after multiple attempts, as it is observed that similarity scores generated by CLIP were typically low and often did not align with human judgement.</p>
<p>Instruction-Following Data Generation.We deploy the Vicuna-v1.5-13B[10] for generation with the same setting described in Appendix B.5.The in-context examples are shown in Tab. 13.</p>
<p>C.2 Instruction-Following Dataset from LHRS-Align</p>
<p>The in-context examples used for generating instruction data for visual reasoning, along with detailed image descriptions and conversations, are displayed in Tab.14, Tab. 15, and Tab.16, respectively.</p>
<p>D Multi-task Instruction Dataset</p>
<p>We build a multi-task instruction dataset by collecting various public RS datasets and creating manual instruction templates.All the data in this multi-task instruction dataset are sampled exclusively from the training sets of each public dataset.The contents of the multi-task instruction dataset are detailed in Tab. 8. Furthermore, different instruction templates are developed for the classification and visual grounding tasks, and each data is converted into instruction data by randomly selecting one of the templates.The instruction templates used are shown in Tab. 17.</p>
<p>E Details about Test Datasets</p>
<p>We detail the dataset information for each task in Tab. 9.</p>
<p>F Details about LHRS-Bench Dataset</p>
<p>F.1 Evaluation Dimensions</p>
<p>The hierarchical ability taxonomies are comprised of 5 top-level dimesntions with 11 fine-grained sub-dimensions.Note that each question-answer pair may encompass multiple sub-dimensions [75].Detailed introduction of each ability dimensions is outlined below.</p>
<p>• Recognition: This dimension focuses on the model's ability to recognize instance identity (geographical objects or scene information), and their attributes, including color, orientation, shape, and area.• Quantity: This dimension assesses the model's capacity to count specific objects, often combined with a prerequisite understanding of those objects (Recognition).• Imagery: This dimension centers on the image itself, specifically recognizing its resolution and modality.In the RS domain, images can encompass various modalities, including optical, panchromatic, synthetic aperture radar (SAR), and thermal infrared images.However, as RS MLLMs are still in their early stages of development, LHRS-Bench currently only includes multispectral and panchromatic images.• Spatial-awareness: This dimension evaluates the MLLM's ability to comprehend the locations (and spatial relationships between objects), as well as identify the distance between objects.• Reasoning: This dimension assesses the model's proficiency in accurately comprehending the image and utilizing its domain-specific knowledge to provide correct answers to questions.</p>
<p>G Hyperparamters</p>
<p>In all our experiments, we utilized 8 NVIDIA V100-32G GPU.However, due to the V100's lack of support for bf16, we initially trained using the AdamW optimizer and fp16.This approach, however, led to considerable instability in the first stage of training.We experimented with various learning rates, specifically {2 × 10 −6 , 5 × 10 −6 , 8 × 10 −6 , 2 × 10 −5 , 5 × 10 −5 , 8 × 10 −5 , 2 × 10 −4 , 1 × 10 −3 }, but observed that training became unstable at higher learning rates, while lower learning rates resulted in suboptimal model performance.We also implemented gradient clipping and experimented with different gradient norms, specifically {0.1, 0.3, 0.6, 1.0, 5.0}.Ultimately, for improved performance and stability in the first stage, we selected the Adan optimizer [69].Concurrently, we enabled DeepSpeed-Zero2 [49] in all three training stages.Details of other hyperparameter settings can be found in Table 10.</p>
<p>H Evaluation Prompt for Each Task</p>
<p>In tasks related to visual grounding and VQA, we prompt each model using descriptions of the target object and questions, respectively.For classification tasks and evaluations on LHRS-Bench, the same prompts are employed across all models, with the specific prompts presented in Tab.18. Classification, Caption 2,519 RSVG [56] Visual Grounding 2,428 DIOR-RSVG [77] Visual Grounding 14,030 NWPU [9] Classification 4,941 METER-ML [84] Classification 1,400 RSITMD [76] Classification 504 fMoW [12] Classification 5,000 RSICD [42] Caption 1,000</p>
<p>Total 42,322</p>
<p>Assistant:</p>
<p>The image shows a varied landscape with a residential area, farmland, and an industrial zone featuring a brick factory, illustrating a blend of community, agriculture, and industry.Include questions asking about the visual content of the image, including the object types, object actions, object locations, relative positions between objects, etc.Only include questions that have definite answers:</p>
<p>(1) one can see the content in the image that the question asks about and can answer confidently;</p>
<p>(2) one can determine confidently from the image that it is not in the image.Do not ask any question that cannot be answered confidently.</p>
<p>In-Context Example:</p>
<p>User:</p>
<p>There is a baseball field beside the green amusement park around the red track.</p>
<p>A green baseball field adjacent to the playground and Red Square.</p>
<p>There is a long path in the field next to the red playground.The green playground around the red runway is a baseball field.The green baseball field is adjacent to the playground and the red playground.</p>
<p>Assistant:</p>
<p>Questions: Please give a detailed description about this image.</p>
<p>Answer:</p>
<p>The image shows large, open field with a baseball diamond in the center.The field i surrounded by trees, and there is a dirt path leading to the diamond.The field is likely a sports field, possibly used for baseball or other outdoor sports.The presence of the baseball diamond suggests that it is a baseball field, but it could also be used for other sports like soccer or football.</p>
<p>Questions:</p>
<p>What is the color of the main object in this image?Answer:</p>
<p>The amusement park and baseball field is green, whereas the playground is depicted in red.</p>
<p>Questions: What does the image suggest about the function and superiority about this image?Answer:</p>
<p>The image captures the natural beauty of the area, with the trees providing shade and the open field offering ample space for sports activities.The dirt path leading to the diamond adds a rustic touch to the scene, creating a picturesque and inviting environment for sports enthusiasts.Instead of directly mentioning the bounding box coordinates, utilize this data to explain the scene using natural language.Include details like object counts, position of the objects, relative position between the objects and their relations.When using the information from the caption and coordinates, directly explain the scene, and do not mention that the information source is the caption or the bounding box.Always answer as if you are directly looking at the image.</p>
<p>In-Context Example: User:</p>
<p>The image portrays a combination of different land uses and recreational areas, including a stadium used for basketball, a marina, a beach, a grassland area, and a parking lot with surface parking.Additionally, there is a harbor serving as a seamark and a park for leisure purposes.building: stadium,leisure:stadium,roof:shape:flat,sport:basketball -&gt; [0.0, 0. The image's diverse land uses cater to various interests with thoughtful placement: the basketball stadium, centrally located, hosts sports and events; the marina and beach, positioned to the north, offer water activities and relaxation; the grassland, towards the south, provides open space for passive leisure.The nearby parking, situated to the southeast, facilitates easy access, while the park, enveloping much of the area, ties these elements together with greenery and walking paths.This strategic layout balances activity and tranquility, effectively serving the community's needs and enhancing the area's aesthetic and functional appeal.</p>
<p>System Message:</p>
<p>You are an AI visual assistant with the capability to analyze complex images.For this task, you will be given descriptions and precise object location data of a single image.The image will be described through five distinct sentences, each providing a unique perspective or detail about the same scene.Accompanying these descriptions are specific object locations within the image, denoted by bounding box coordinates.These coordinates are formatted as (x1, y1, x2, y2), where each value is a floating number between 0 and 1, representing the proportions of the image's dimensions.</p>
<p>The coordinates x1 and y1 refer to the top left corner of the bounding box, while x2 and y2 refer to the bottom right corner.Your task is to summarize the information from the sentences and the bounding box data to create a rich, detailed description of the scene.In your description, focus on the layout, the spatial relationships between objects, and any notable details that the combination of textual descriptions and coordinate data reveals.Remember, the goal is to paint a vivid picture of the scene as if you are observing it firsthand.Avoid explicitly mentioning the source of your information, such as the specific sentences or the bounding box coordinates.Instead, integrate this information seamlessly into your natural language description, emphasizing the number of objects, their exact positions, and how they relate to one another in the space.Answer as if you're guiding someone through the image, pointing out each element and its significance in the overall scene.Your description should enable someone who cannot see the image to visualize it as accurately and completely as possible.</p>
<p>In-Context Example: User:</p>
<p>The image portrays a combination of different land uses and recreational areas, including a stadium used for basketball, a marina, a beach, a grassland area, and a parking lot with surface parking.Additionally, there is a harbor serving as a seamark and a park for leisure purposes.The image depicts a vibrant urban landscape, harmoniously integrating various recreational and natural elements.Dominating the scene is a large basketball stadium with a flat roof, stretching from the top to nearly the bottom and spanning the entire width, signifying its prominence.Adjacent to it, in the top right corner, is a marina, hinting at a coastal setting.Just below the marina, a beach area extends almost the full width of the image, highlighting its importance as a recreational spot.At the very bottom, a narrow strip of grassland provides a natural contrast to the urban environment.Centrally located in the lower middle part is a surface parking lot, strategically placed for easy access to the surrounding facilities.Overlaying much of the image is a leisure park, suggesting a large, open area designed for relaxation and outdoor activities.</p>
<p>Together, these elements create a dynamic and multifunctional urban space, blending sports, leisure, and nature.Question: How many distinct industrial areas are there in the image, and where are they located relative to the entire scene?Answer: There are two distinct industrial areas.One is towards the bottom left corner, extending up and to the right but not reaching the image's center.The other is towards the right side, stretching from near the bottom up to about two-thirds of the height.Question: What is the relative position of the bus depot to the construction site?Answer: The bus depot is centrally located, running vertically almost across the entire image height.The construction site is situated towards the upper left, overlapping with the lower part of the bus depot.It's closer to the image's top edge than the bottom.Question: Is there any natural vegetation, and if so, where is it located in relation to the industrial areas?Answer: Yes, natural vegetation identified as scrub is present towards the left side, extending from near the middle to the bottom.It overlaps with the first industrial area, indicating they are adjacent or possibly intermingled.Question: Can you describe the distribution of land for retail and meadows?Answer: The retail space is in the upper right quarter, not quite reaching the top or rightmost edge.The meadow is also in the upper right but is situated higher and further to the right than the retail space, with a clear separation between the two areas indicating distinct zones for commerce and natural space.</p>
<p>Fig. 1 :
1
Fig. 1: Overall performance comparison between LHRS-Bot and existing MLLMs on different RS image understanding tasks.</p>
<p>Fig. 2 :
2
Fig. 2: Pipeline of constructing LHRS-Align.</p>
<p>shadows in the image, can you estimate the possible time of day when this image was taken?</p>
<p>Fig. 4 :
4
Fig. 4: Curriculum learning strategy.We progressively unlock various parameters and introduce data of escalating complexity at different stages, enabling the model to gradually assimilate visual knowledge with increasing difficulty.</p>
<p>[0.02, 0.25, 0.24, 0.42] [CLS] Residential or Commerical?Residential Is there any playground?If have, where is it?Yes, there is a playground in the image.It can be found on the right side of the road,.[VG] Playground [0.82, 0.50, 0.99, 0.83] What can you infer from this image?</p>
<p>Can you spot any residential areas, and how are they arranged in relation to the main roads?There is a residential area visible to the left (west) side of the vertical highway.The homes are arranged in a typical suburban pattern, with streets laid out in a block formation.What type of facility is depicted in the image?The image appears to show an airport terminal with a large, open area and multiple baggage claim areas.Describe the layout of the terminal buildings.The terminal buildings have a large, open layout with several rows of gates and baggage claim areas.The layout suggests that the airport is well-organized and efficiently designed to accommodate a high volume of passengers and aircraft.</p>
<p>Fig. 5 :
5
Fig. 5: Qualitative examples between real users and LHRS-Bot.</p>
<p>Fig. 6 :
6
Fig. 6: Interface for manual filtering the OSM keys.</p>
<p>building: stadium,leisure:stadium,roof:shape:flat,sport:basketball -&gt; [0.0, 0.249, 1.0, 0.757] leisure:marina,seamark:type:harbour -&gt; [0.777, 0.0, 1.0, 0.373] natural:beach -&gt; [0.137, 0.0, 0.792, 0.201] natural:grassland -&gt; [0.196, 0.903, 1.0, 0.956] amenity:parking,parking:surface -&gt; [0.561, 0.659, 0.956, 0.776] leisure: park -&gt; [0.0, 0.0, 1.0, 0.887] Assistant: Questions: What are the key features and elements depicted in the urban landscape captured in the image?Answer:</p>
<p>Fig. 7 :
7
Fig. 7: Examples of the LHRS-Align dataset.</p>
<p>Fig. 8 :
8
Fig. 8: Examples of the LHRS-Bench benchmark.</p>
<p>Fig. 9 :
9
Fig. 9: Evaluation examples on LHRS-Bench benchmark.</p>
<p>Table 1 :
1
Overview of the dataset distri-
bution. LHRS-Bench contains 108 RS im-ages with 690 question-answer pairs, cover-ing five major evaluation dimensions and 11sub-dimensions of questions. Each sample isa human-annotated single-choice questionthat may involve one or more evaluation di-mensions simultaneously.DimensionSub-dimension CountIdentity634Color113RecognitionOrientation39Shape37Area75ImageryResolution Modality21 23Spatial-awarenessLocation Distance204 22Quantity-137Reasoning-46</p>
<p>Input Image Vision Encoder Vision Perceiver Concatenate Cross Attention MLP Image Token Learnable Query Image Token Image Embedding User Query: What can you reasoning about the image? Token Embedding Large Language Model (LLaMA2) What</p>
<p>can you reasoning about the image?</p>
<p>Table 2 :
2
Classification accuracy of different models on each dataset.
MethodAID WHU-RS19 NWPU SIRI-WHU EuroSAT METER-ML fMoW Avg.LLaVA-1.531.1054.5534.9617.7126.1221.7311.43 28.23MiniGPTv232.9664.8028.1535.4638.5614.295.20 31.35InstructBLIP29.5036.7634.0118.2020.2514.426.71 22.84mPLUG-OWL248.7972.6646.5854.8333.2936.2717.85 44.32QWen-VL-Chat55.3072.2542.7354.5826.4238.776.89 42.42InternLM-XComposer 51.6172.8947.5146.8339.7040.2111.28 44,29LHRS-Bot91.2693.1783.9462.6651.4069.8156.56 71.83</p>
<p>Table 3 :
3
[24,28,78]f different models on each visual question answering dataset.Following the practice of prior works[24,28,78], we omit area and count questions during evaluation.
MethodRSVQA-LRRSVQA-HRRural/Urban Presence Compare Avg. Presence Compare Avg.LLaVA-1.559.2273.1665.19 65.86 48.9659.02 53.99MiniGPTv260.0251.6467.64 59.77 68.3464.71 66.53InstructBLIP62.6248.8365.92 59.12 62.6362.90 62.77mPLUG-Owl257.9974.0463.69 65.24 47.6058.47 53.04QWen-VL-Chat62.0047.6566.54 58.73 61.7565.98 63.87InternLM-XCompose59.0066.7452.91 59.55 67.7966.62 67.21RSGPT94.0091.1791.70 92.29 90.9290.02 90.47GeoChat91.0990.3394.00 91.81 58.4583.19 70.82SkyEyeGPT88.9388.6375.00 84.16 80.0080.13 82.56LHRS-Bot89.0788.5190.00 89.19 92.5792.53 92.55
generalization ability of LHRS-Bot.In addition, in the context of the fine-grained RS classification dataset fMoW, our method significantly surpasses other models, leading by an impressive 40% on average.This highlights the critical role of continue pre-training in the RS domain and the effectiveness of our alignment dataset LHRS-Align.Visual Question Answering.We report the results of different models on RSVQA-LR and RSVQA-HR datasets in Tab. 3. LHRS-Bot demonstrates performance on par with the previously leading RS domain-specific models on the VQA task.Notably, LHRS-Bot achieves comparable results to RSGPT, despite RSGPT being fine-tuned for five epochs on the RSVQA datasets.Furthermore, LHRS-Bot sets a new record in the RSVQA-HR task with high-resolution RS images, showcasing its superior visual recognition abilities with high-resolution RS images.</p>
<p>Table 4 :
4
Comparison of visual gronding tasks.Accuracy is calculated based on whether the predicted bounding box overlaps with the ground-truth box by more than 0.5 IoU.
MethodRSVG DIOR-RSVG Avg.QWen-VL-Chat 44.7680.6562.71MiniGPTv246.6485.9966.32SkyEyeGPT70.5088.5979.55LHRS-Bot73.4588.1080.78</p>
<p>Table 5 :
5
Comparison results on the LHRS-Bench dataset with different open-source and closed-source MLLMs.
MethodIdentity Color Orientation Shape Area Resolution Modality Location Distance Quantity Reasoning Avg.LLaVA-1.542.90 30.9715.3843.24 44.0023.8021.7435.7827.2721.8954.3537.45MiniGPTv220.19 15.932.568.11 13.3314.294.3515.2013.649.4936.9616.88InstructBLIP20.35 22.125.1324.32 22.6738.1000.0013.7322.8513.1430.4318.87mPLUG-OWL241.48 45.1312.8240.54 32.0033.3300.0030.8813.6432.1254.3437.00QWen-VL-Chat31.38 32.747.6932.43 29.3319.054.3524.0221.4521.1756.5228.65InternLM-XComposer 34.38 30.977.6937.84 26.6700.008.7033.3322.7318.2565.2231.09GPT-4-Turbo50.32 43.3610.2667.57 44.0042.8639.1342.1622.7321.8971.74 44.56Claude-3-Opus41.01 36.2817.9564.86 41.3366.6739.1334.8045.4526.2854.3539.08LHRS-Bot44.64 42.4823.0867.57 25.3342.8632.3556.5222.7318.2573.91 39.384.3 Evaluation on LHRS-Bench
We evaluate LHRS-Bot with different open-source and closed-source MLLMs on our LHRS-Bench benchmark.For each question within the benchmark, choices are shuffled, and the generation process is repeated four times.An answer is considered incorrect if any attempt yields an incorrect response.Same prompts are used for evaluation, as detailed in supplementary material.We consider the model answers correctly if it outputs the correct choice or the answer corresponding to the correct choice.We calculate accuracy as our evaluation metric.The results presented in Tab. 5 indicate that LHRS-Bot outperforms all other open-source models.It is worth noting that, LHRS-Bot greatly outperforms other</p>
<p>Table 6 :
6
Performance with different alignment dataset.
ClassificationVQAVisual GroundingAID WHU-RS19 NWPU SIRI-WHU EuroSAT METER-ML fMoW RSVQA-LR RSVQA-HR RSVG DIOR-RSVGRS5M86.3196.7582.8060.2054.9264.7742.6878.3691.6570.5887.12SkyScript88.2190.2678.0151.4343.6650.0231.2788.7991.2471.2585.05LHRS-Align 91.2693.1783.9462.6651.4069.8156.5689.1992.5573.4588.10</p>
<p>Table 7 :
7
Performance comparison across various architectural designs.
MethodMulti Level Number of Query Classification VQA Visual GroundingMLP✗-64.4690.0675.97Vision Perceiver✗6463.6983.3077.92Vision Perceiver✓[64,64,64]63.7487.8879.30Vision Perceiver✓[32,48,64]60.6887.1178.14Token Merging✓6449.5181.4977.61Vision Perceiver✓[64,48,32]65.4390.1479.90</p>
<p>Table 8 :
8
Detailed statistics of multi-task instruction dataset.
DatasetTask TypeSamplesRSVQA-HR [41]VQA10,000RSVQA-LR [41]VQA500UCM [47]</p>
<p>Table 11 :
11
Selected keys for OSM features.
Manual selected keys:[ 'landuse', 'landcover', 'natural', 'amenity', 'water', 'industrial', 'prod-uct', 'leisure', 'tourism', 'shop', 'healthcare', 'aeroway', 'surface', 'aero-drome', 'highway', 'building', 'sport', 'denomination', 'man_made','waterway', 'power', 'military', 'parking', 'historic', 'residential', 'trees','place', 'plant:source', 'public_transport', 'leaf_cycle', 'produce', 'min-eral', 'social_facility', 'castle_type', 'wetland', 'ruins', 'archaeologi-cal_site', 'water_source', 'type', 'crop', 'historic:civilization', 'material','area:highway', 'education', 'construction', 'garden:type', 'museum', 're-sort', 'leaf_type', 'seamark:type', 'service', 'military_service', 'beach','craft', 'generator:source', 'resource', 'cargo', 'golf', 'orchard', 'rail-way', 'grassland', 'depot', 'port', 'attraction', 'content', 'roof:shape','meadow', 'wood', 'grass', 'zoo', 'construction:power', 'generator:method','plant', 'aquaculture', 'reservoir_type', 'sport_1', 'school', 'trade','basin', 'piste:type', 'club', 'monastery:type', 'landform', 'athletics','wholesale', 'building:use', 'pumping_station', 'utility', 'police', 'com-pany', 'playground', 'footway', 'building:colour', 'construction:amenity','animal_shelter', 'cemetery', 'tomb', 'landfill', 'construction:leisure','building:material', 'landfill:waste', 'construction:landuse', 'farmyard','shelter_type', 'camp_type', 'animal_keeping', 'building:part', 'waste','animal_breeding', 'construction:man_made', 'commercial', 'research','species', 'seamark:small_craft_facility:category', 'glacier:type', 'hazard','geological', 'reservoir', 'historic:landuse', 'station', 'recreation_ground','construction:railway', 'boules', 'roof:material', 'park', 'amenity_1', 'ani-mal', 'usage', 'contamination', 'seamark:shoreline_construction:category','reef', 'farmland', 'nature', 'storage', 'livestock', 'pasture', 'allotments','building:condition', 'logistics', 'forest', 'building:roof', 'place_of_worship','rock', 'factory', 'scrub', 'construction:industrial', 'seaway', 'construc-tion:sport', 'park:type', 'vehicle_depot', 'dock', 'bunker_type', 'to-ponym', 'industry', 'historic:leisure', 'sand', 'earth_bank', 'pond_use','tree', 'retail', 'bay', 'works', 'forest_cover', 'historic:amenity', 'construc-tion:residential', 'sports_centre', 'residence', 'paving_stones:material','seamark:sea_area:category', 'quarry', 'vegetation', 'construction:highway','pitch', 'warehouse', 'construction:shop', 'bridge:type', 'historic:aeroway','bare_rock', 'transport', 'bridge:movable', 'construction:tourism', 'build-ing:roof:colour', 'amenity_2', 'amenity_3', 'construction:type', 'heath' ]</p>
<p>Table 12 :
12
In-context examples for LHRS-Align caption generation.As an AI visual assistant, your task is to create a caption for a single image presented to you.This image is accompanied by several key-value tags, each providing specific details about it.Your goal is to synthesize these details into a clear, concise description that encompasses all the features mentioned in the key-value tags.The description should be diverse, reflecting the variety of attributes and elements present in the image.Finish this task as if you are directly observing the image yourself, and craft a caption that naturally integrates all the provided information.Remember, your description should not explicitly reference the key-value pairs; instead, it should read as a seamless and direct observation of the image, capturing its essence in a fluent and engaging manner.
System Message:In-Context Example:User:There are four tags contained in this image. Their keys and values are listedbelow:\n1. Key: landuse, Value: residential\n2.Key: natural, Value: water;Key: Water, Value: River\n3.Key: landuse, Value: commerical\n4.Key: lan-duse, Value: farmland.Assistant:The image presents a varied landscape featuring a residential area, a riverrepresenting natural water bodies, commercial zones with business or retailestablishments, and farmland used for agricultural purposes.User:There are four features in the images. Their keys and values are listedbelow:\n1.Key: landuse, Value: residential\n2.Key:landuse, Value: farm-land\n3.Key: landuse, Value: industrial\n4.Key:industrial, Value: factory;Key: landuse, Value: industrial; Key: product, Value: bricks.</p>
<p>Table 13 :
13
In-context examples for caption generation from the public caption datasets.You are an AI visual assistant, and you are seeing a single image.What you see are provided with five sentences, describing the same image you are looking at.Answer all questions as you are seeing the image.Design a conversation between you and a person asking about this photo.The answers should be in a tone that a visual AI assistant is seeing the image and answering the question.Ask diverse questions and give corresponding answers.
System Message:</p>
<p>Table 14 :
14
In-context examples for generating visual reasoning samples usingYou are an AI visual assistant that can analyze a single image.You receive one sentence, describing the same image you are observing.In addition, specific object locations and their attributes within the image are given, along with detailed coordinates.These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1.These values correspond to the top left x, top left y, bottom right x, and bottom right y.The task is to use the provided caption and bounding box information, create a plausible question about the image, and provide the answer in detail.Create complex questions beyond describing the scene.To answer such questions, one should require first understanding the visual content, then based on the background knowledge or reasoning, either explain why the things are happening that way, or provide guides and help to user's request.Make the question challenging by not including the visual content details in the question so that the user needs to reason about that first.Also include some relations about relative position, distance among different objects.
System Message:</p>
<p>How do the diverse land uses and recreational facilities depicted in the image cater to the varied interests and needs of the community, and what role does the layout play in the functionality and aesthetic appeal of the area?Answer:
249, 1.0,0.757]leisure:marina,seamark:type:harbour -&gt; [0.777, 0.0, 1.0, 0.373]natural:beach -&gt; [0.137, 0.0, 0.792, 0.201]natural:grassland -&gt; [0.196, 0.903, 1.0, 0.956]amenity:parking,parking:surface -&gt; [0.561, 0.659, 0.956, 0.776]leisure: park -&gt; [0.0, 0.0, 1.0, 0.887]Assistant:Questions:</p>
<p>Table 15 :
15
In-context examples for generating detailed image descriptions using GPT-4.</p>
<p>Table 16 :
16
In-context examples for generating multimodal conversations usingYou are an AI visual assistant that can analyze a single image.You receive one sentence, describing the same image you are observing.In addition, specific object locations and their attributes within the image are given, along with detailed coordinates.These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1.These values correspond to the top left x, top left y, bottom right x, and bottom right y.Your task is to engage in a conversation where you design questions and provide answers about the visual content of the image.The questions should focus on object count, position, relative distance, and other visual aspects, while the answers should reflect direct observation as if you are viewing the image.Create questions that delve into the visual details of the image by considering the types of objects present, their actions, and their spatial relationships.Provide answers as if you're observing the image firsthand.For straightforward questions, give concise and accurate responses.For more complex inquiries, offer detailed explanations or step-by-step reasoning to justify your answers.The image shows a mix of land uses, including a bus depot, industrial areas, construction sites, brownfield land with parking aisles, meadows, retail spaces, and a bus station as a public transport hub.The natural feature in the scene is scrub, which is a type of vegetation.depot:bus,landuse:depot -&gt; [0.169, 0.0, 0.832, 1.0] landuse:industrial -&gt; [0.0, 0.717, 0.254, 0.86] natural:scrub -&gt; [0.0, 0.504, 0.314, 0.86] landuse:construction -&gt; [0.276, 0.02, 0.479, 0.254]
System Message: In-Context Example:User:landuse:industrial -&gt; [0.751, 0.1, 1.0, 0.67]landuse:meadow -&gt; [0.661, 0.864, 1.0, 1.0]landuse:retail -&gt; [0.687, 0.629, 1.0, 0.916]Assistant:
https://www.openstreetmap.org/
https://www.google.com/earth/
https://wiki.openstreetmap.org/wiki/Map_features
However, we have no means of confirming whether these data appeared in the LHRS-Align dataset, considering that images in LHRS-Align, along with several of the
https://github.com/visual-layer/fastdup?tab=readme-ov-file
AcknowledgementThis research was supported by the National Natural Science Foundation of China under Grant 42071297, and in part by the AI &amp; AI for Science Project of Nanjing University under Grant 02091480605203.We are grateful to High Performance Computing Center of Nanjing University for their help on GPU resources.We also would like to thank the anonymous reviewers for their constructive comments.Table18: Prompts for evaluation on each task.Classification:Choose the best categories describe the image from: {}.LHRS-Bench:Please answer the question based on the given choices: Question: {} Choice: {} Answer:
Flamingo: a visual language model for few-shot learning. J B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 352022</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>J Bai, S Bai, S Yang, S Wang, S Tan, P Wang, J Lin, C Zhou, J Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>S Bai, S Yang, J Bai, P Wang, X Zhang, J Lin, X Wang, C Zhou, J Zhou, arXiv:2308.16890Touchstone: Evaluating vision-language models by language models. 2023arXiv preprint</p>
<p>Language integration in remote sensing: Tasks, datasets, and future directions. L Bashmal, Y Bazi, F Melgani, M M Al Rahhal, M A Al Zuair, IEEE Geoscience and Remote Sensing Magazine. 2023</p>
<p>Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. Y Bitton, H Bansal, J Hessel, R Shao, W Zhu, A Awadalla, J Gardner, R Taori, L Schimdt, arXiv:2308.065952023arXiv preprint</p>
<p>D Bolya, C Y Fu, X Dai, P Zhang, C Feichtenhofer, J Hoffman, arXiv:2210.09461Token merging: Your vit but faster. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>Remote sensing image scene classification: Benchmark and state of the art. G Cheng, J Han, X Lu, Proceedings of the IEEE. 105102017</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, 14 April 2023. 2023</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Functional map of the world. G Christie, N Fendley, J Wilson, R Mukherjee, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Satellite image classification via two-layer sparse coding with biased image representation. D Dai, W Yang, IEEE Geoscience and Remote Sensing Letters. 812011</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A Tiong, J Zhao, W Wang, B Li, P Fung, S Hoi, arXiv:2305.065002023arXiv preprint</p>
<p>C Deng, T Zhang, Z He, Q Chen, Y Shi, L Zhou, L Fu, W Zhang, X Wang, C Zhou, arXiv:2306.05064Learning a foundation language model for geoscience knowledge understanding and utilization. 2023arXiv preprint</p>
<p>D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>C Fu, P Chen, Y Shen, Y Qin, M Zhang, X Lin, J Yang, X Zheng, K Li, X Sun, arXiv:2306.13394Mme: A comprehensive evaluation benchmark for multimodal large language models. 2023arXiv preprint</p>
<p>What do vision transformers learn? a visual exploration. A Ghiasi, H Kazemi, E Borgnia, S Reich, M Shu, M Goldblum, A G Wilson, T Goldstein, arXiv:2212.067272022arXiv preprint</p>
<p>Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. P Helber, B Bischke, A Dengel, D Borth, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 1272019</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.15556Training computeoptimal large language models. 2022arXiv preprint</p>
<p>Segmentation for object-based image analysis (obia): A review of algorithms and challenges from remote sensing perspective. M D Hossain, D Chen, ISPRS Journal of Photogrammetry and Remote Sensing. 1502019</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Y Hu, J Yuan, C Wen, X Lu, X Li, arXiv:2307.15266Rsgpt: A remote sensing vision language model and benchmark. 2023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, Q Liu, arXiv:2302.140452023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>C Ju, H Wang, Z Li, X Chen, Z Zhai, W Huang, S Xiao, arXiv:2312.07408Turbo: Informativity-driven acceleration plug-in for vision-language models. 2023arXiv preprint</p>
<p>K Kuckreja, M S Danish, M Naseer, A Das, S Khan, F S Khan, arXiv:2311.15826Geochat: Grounded large vision-language model for remote sensing. 2023arXiv preprint</p>
<p>W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Efficient memory management for large language model serv. </p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Zero: Memory optimizations toward training trillion parameter models. S Rajbhandari, J Rasley, O Ruwase, Y He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Using machine learning to assess the livelihood impact of electricity access. N Ratledge, G Cadamuro, B De La Cuesta, M Stigler, M Burke, Nature. 61179362022</p>
<p>Scale-MAE: A scale-aware masked autoencoder for multiscale geospatial representation learning. C J Reed, R Gupta, S Li, S Brockman, C Funk, B Clipp, K Keutzer, S Candido, M Uyttendaele, T Darrell, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Quiltllava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. Saygin Seyfioglu, M Ikezogwo, W O Ghezloo, F Krishna, R Shapiro, L , 20232312</p>
<p>Laion-5b: An open largescale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, Advances in Neural Information Processing Systems. 352022</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, L Hou, K Clark, S Pfohl, H Cole-Lewis, D Neal, arXiv:2305.096172023arXiv preprint</p>
<p>Visual grounding in remote sensing images. Y Sun, S Feng, X Li, Y Ye, J Kang, X Huang, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022</p>
<p>Introducing mpt-7b: A new standard for open-source, commercially usable llms. M N Team, 2023</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature Medicine. 2982023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Legal prompt engineering for multilingual legal judgement prediction. D Trautmann, A Petrova, F Schilder, arXiv:2212.021992022arXiv preprint</p>
<p>Samrs: Scaling-up remote sensing segmentation dataset with segment anything model. D Wang, J Zhang, B Du, M Xu, L Liu, D Tao, L Zhang, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Skyscript: A large and semantically diverse vision-language dataset for remote sensing. Z Wang, R Prabha, T Huang, J Wu, R Rajagopal, arXiv:2312.128562023arXiv preprint</p>
<p>Vision-language models in remote sensing: Current progress and future trends. C Wen, Y Hu, X Li, Z Yuan, X X Zhu, arXiv:2305.057262023arXiv preprint</p>
<p>B Workshop, T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, arXiv:2211.05100Bloom: A 176b-parameter openaccess multilingual language model. 2022arXiv preprint</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>DOTA: A large-scale dataset for object detection in aerial images. G S Xia, X Bai, J Ding, Z Zhu, S Belongie, J Luo, M Datcu, M Pelillo, L Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>AID: A benchmark data set for performance evaluation of aerial scene classification. G S Xia, J Hu, F Hu, B Shi, X Bai, Y Zhong, L Zhang, X Lu, IEEE Transactions on Geoscience and Remote Sensing. 5572017</p>
<p>Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. X Xie, P Zhou, H Li, Z Lin, S Yan, arXiv:2208.066772022arXiv preprint</p>
<p>H Xu, S Xie, X E Tan, P Y Huang, R Howes, V Sharma, S W Li, G Ghosh, L Zettlemoyer, C Feichtenhofer, arXiv:2309.16671Demystifying clip data. 2023arXiv preprint</p>
<p>H Yang, X Y Liu, C D Wang, arXiv:2306.06031Fingpt: Open-source financial large language models. 2023arXiv preprint</p>
<p>The role of satellite remote sensing in climate change studies. J Yang, P Gong, R Fu, M Zhang, J Chen, S Liang, B Xu, J Shi, R Dickinson, Nature climate change. 3102013</p>
<p>Q Ye, H Xu, G Xu, J Ye, M Yan, Y Zhou, J Wang, A Hu, P Shi, Y Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Legal prompting: Teaching a language model to think like a lawyer. F Yu, L Quartey, F Schilder, arXiv:2212.013262022arXiv preprint</p>
<p>W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, arXiv:2308.02490Mmvet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>Exploring a finegrained multiscale method for cross-modal remote sensing image retrieval. Z Yuan, W Zhang, K Fu, X Li, C Deng, H Wang, X Sun, arXiv:2204.098682022arXiv preprint</p>
<p>Rsvg: Exploring data and models for visual grounding on remote sensing data. Y Zhan, Z Xiong, Y Yuan, IEEE Transactions on Geoscience and Remote Sensing. 612023</p>
<p>Skyeyegpt: Unifying remote sensing visionlanguage tasks via instruction tuning with large language model. Y Zhan, Z Xiong, Y Yuan, arXiv:2401.097122024arXiv preprint</p>
<p>P Zhang, X D B Wang, Y Cao, C Xu, L Ouyang, Z Zhao, S Ding, S Zhang, H Duan, H Yan, arXiv:2309.15112Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. 2023arXiv preprint</p>
<p>S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068Opt: Open pre-trained transformer language models. 2022arXiv preprint</p>
<p>Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. X Zhang, Q Yang, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Rs5m: A large scale vision-language dataset for remote sensing vision-language foundation model. Z Zhang, T Zhao, Y Guo, J Yin, arXiv:2306.113002023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Meter-ml: A multi-sensor earth observation benchmark for automated methane source mapping. B Zhu, N Lui, J Irvin, J Le, S Tadwalkar, C Wang, Z Ouyang, F Y Liu, A Y Ng, R B Jackson, arXiv:2207.111662022arXiv preprint</p>
<p>D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. 2023arXiv preprint</p>
<p>Bag-of-visual-words scene classifier with local and global features for high spatial resolution remote sensing imagery. Q Zhu, Y Zhong, B Zhao, G S Xia, L Zhang, IEEE Geoscience and Remote Sensing Letters. 1362016</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3001 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3001</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3001</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-258865170</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.15054v2.pdf" target="_blank">A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3001.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3001.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6 billion parameter autoregressive language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6B-parameter decoder-only Transformer variant (GPT-family style) used in this paper to analyze arithmetic reasoning via causal mediation and activation interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-style) with 6B parameters, parallel attention and rotary positional encodings; pre-trained and used as the main model for analyses in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (addition, subtraction, multiplication, division) and three-operand chained operations; also numeral-word vs Arabic numeral representations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Information about operands/operators is transmitted by attention from early/mid-sequence token positions to the final token; late (mid-to-late layer) MLP modules at the last token incorporate result-related information into the residual stream (i.e., they compute or produce result signals).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layer- and position-wise indirect-effect (IE) heatmaps show high IE at early MLPs on operand/operator tokens and high IE at attention modules at last token (layers ~11-18) plus spikes at last-token MLPs around layers 19-20; result-preserving vs result-varying experiments show last-token MLPs' IE drops when result fixed, implicating them in encoding results.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Attention IE patterns do not strongly differentiate result-varying vs result-preserving cases (attention seems operand-related), and analyses treat attention modules as a whole (not per-head), leaving open whether a single attention head or a distributed set does the forwarding; also no conclusive demonstration of an explicit symbolic/algebraic algorithm being implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Causal mediation (activation patching): during a forward pass, stored activations (MLP outputs or attention outputs) from one prompt are injected into another prompt's forward pass; also prompt two-shot exemplars used for behavioral performance.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Interventions on last-token MLPs (layers 19-20) substantially shift model output probability mass toward the target arithmetic result; interventions on earlier layers shift representations but are less effective at producing correct result; activation-patching can produce desired prediction changes (desired/wrong->correct) primarily when applied to layers 19-20.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracy on two-operand tasks (Table 4): overall GPT-J accuracy 67.8% (by operator: +69.3%, -78.0%, ×82.8%, ÷40.8%); with numeral words overall 81.3%; on three-operand queries pre-trained GPT-J accuracy <10% (poor). RI(M_late_-1) (relative importance of late last-token MLPs) = 40.2% (result-varying) vs 4.4% (result-fixed) for GPT-J.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Division substantially worse than other operators; multi-operand (three-operand) queries poorly handled prior to fine-tuning; tokenizer issues for multi-digit numbers (splitting into multiple tokens) constrain experimental result sets; attention interventions measured at module level obscure head-level specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison to human arithmetic or symbolic calculators; mechanisms are described in neural-activation terms and not aligned to human symbolic procedures in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3001.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia (2.8 billion parameter analysis suite model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2.8B-parameter decoder-only Transformer (Pythia family) used in the paper both pre-trained and after fine-tuning to analyze arithmetic flow and effects of targeted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer with 2.8B parameters from the Pythia suite; used to run the three-operand experiments including a small fine-tuning procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (add, sub, mul, div) and three-operand chained operations (various templates); also evaluated before and after fine-tuning for three-operand tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Same attention-to-last-token then late-MLP processing pipeline observed: attention conveys operand/operator information to last token; late MLPs in middle-to-late layers synthesize result-related information.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Pre- vs post-fine-tuning IE comparisons show emergence of the mid-late last-token MLP activation site after fine-tuning: pre-finetune the only relevant last-token activation was the very last layer; after fine-tune, mid-late MLP site appears (matching two-operand pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Before fine-tuning, Pythia did not show the mid-late MLP activation site strongly (indicating that the mechanism can be absent or weak in pre-trained models and induced by fine-tuning), suggesting the observed circuit may not be an inherent universal algorithm across architectures/sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning (on a small synthetic three-operand dataset) and causal mediation activation patching in analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning on 3-operand queries improved accuracy from ~0.9% to ~39.7% overall (Pythia 2.8B fine-tuned), and produced the emergence of the mid-late last-token MLP activation site implicated in result computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pre-fine-tune three-operand overall accuracy ~0.9% (Table 4); post-fine-tune overall ~39.7%. RI(M_late_-1) values for Pythia 2.8B: 43.2% result-varying vs 5.8% result-fixed (two-operand experiments) and three-operand RI reported 13.5% pre-fine-tune vs 24.7% post-fine-tune in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor native handling of chained/three-operand arithmetic without fine-tuning; performance highly sensitive to fine-tuning and prompt representation; tokenization constraints for numeric tokens limit scaling to multi-digit numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No explicit comparison to human or symbolic procedures beyond performance metrics; fine-tuning produces circuit changes rather than mapping to symbolic stepwise algorithms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3001.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7 billion parameter open foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter decoder-only Transformer model (LLaMA family) included in the cross-model validation of the attention->MLP arithmetic information-flow hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (LLaMA) with 7B parameters; used pre-trained (not fine-tuned in this study) to validate whether the information-flow pattern generalizes across model families and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic tasks (add, sub, mul, div) as in other models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Same qualitative mechanism: early/mid-sequence attention forwards operand/operator information to last token; mid-late last-token MLPs (roughly half-to-late layers) incorporate result-related signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>RI(M_late_-1) measured at 36.1% (result-varying) vs 7.5% (result-fixed), consistent with pattern seen in GPT-J and Pythia and supporting generality across models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Magnitude differences and tokenization differences across models (LLaMA tokenizes digits differently) mean some arithmetic evaluation settings required restrictions (e.g., single-digit results), limiting direct comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching (causal mediation analysis) and two-shot prompting for behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Interventions on late MLPs shift output probabilities towards alternative results similarly to other models; relative importance shows substantial late MLP involvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall accuracy (Table 4) very high for LLaMA on two-operand/grouped tasks in this paper's evaluation: overall 97.2% (per-operator: +100.0%, -100.0%, ×100.0%, ÷88.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Division remains the worst operator compared to others even when overall accuracy is high; tokenizer constraints forced restricting result vocabulary in some evaluations (single-digit restriction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No explicit comparison to human/symbolic computation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3001.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (LLaMA variant fine-tuned on arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of LLaMA fine-tuned specifically on arithmetic tasks (Goat) used to test whether targeted fine-tuning alters the circuits and improves arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat (fine-tuned LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based model further fine-tuned on arithmetic tasks (per Liu and Low, 2023) and evaluated here for circuit-level patterns and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (add, sub, mul, div) and evaluated for circuit signatures.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Fine-tuning leads to similar attention-to-last-token forwarding but can amplify or make more specific the late-MLP result-producing activation sites.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>RI(M_late_-1) = 33.5% (result-varying) vs 7.4% (result-fixed) — pattern consistent with other models but magnitudes differ; performance on arithmetic tasks improved relative to base LLaMA in earlier goat-related work (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper notes goat shows different magnitudes but does not provide detailed per-layer fine-grained comparisons beyond RI statistics in appendix; causality is correlational w.r.t. fine-tuning changes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Model fine-tuning on arithmetic datasets and activation patching analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning increases reliance and effect of mid-late last-token MLPs for producing result signals, consistent with pattern that training can shape/strengthen these circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall accuracy for Goat in Table 4: 85.6% overall (per-operator: +91.4% for multiplication, ÷54.0% for division), showing improved arithmetic ability compared to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Division weaker than other operators; patterns suggest fine-tuning changes but does not necessarily create fully robust algorithmic arithmetic capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No explicit human/symbolic comparison.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3001.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention->LastToken->MLP pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-mediated forwarding to last token followed by late-MLP result synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanistic hypothesis introduced in this paper: operand/operator representations are computed at early token positions, attention forwards relevant information to the final token, and late MLP modules at the last token synthesize result-related signals incorporated into the residual stream for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (GPT-J, Pythia, LLaMA, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies to decoder-only Transformer architectures evaluated (2.8B–7B models) with standard attention + MLP blocks and residual streams; numeric prompts rendered either as Arabic numerals or numeral words.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic, operator-varying experiments, three-operand chained arithmetic (after fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Two-stage mechanism: (1) early layers/MLPs encode operands/operators; attention modules (mid layers) convey these representations to the last token; (2) mid-to-late last-token MLPs generate result-related representations which bias next-token probabilities toward the numerical result.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>IE heatmaps show consistent early-MLP activation at operand tokens, attention peaks across layers at last token, and MLP peaks at layers ~19-20 at last token; result-fixed vs result-varying comparisons isolate result-encoding to late MLPs; neuron-level overlaps and intervention-success metrics further support distinct neuron sets for arithmetic result computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Does not prove an explicit arithmetic algorithm (e.g., stepwise symbolic computation) is implemented; attention is measured at module-level (not per head) leaving head-level forwarding unclear; variation across models and dependence on fine-tuning indicate mechanism is not strictly universal.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching (causal mediation analysis) where MLP outputs and attention outputs from one prompt are injected into another; also two-shot prompting and fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Patching late last-token MLP activations from prompts with different results shifts model probabilities strongly toward that result; interventions at layers 19-20 are most likely to induce desired change from wrong->correct, whereas interventions at slightly earlier layers (14-17) induce undesired changes more often.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Relative Importance (RI) measure quantifies contribution of mid-late last-token MLPs: e.g., GPT-J RI(M_late_-1)=40.2% (result-varying) vs 4.4% (result-fixed); consistent spikes in IE at layers 19-20 across operators.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Mechanism identifies where result information appears but not the specific operations performed inside MLPs; inability to examine attention heads reduces granularity; tokenizer limitations restrict numeric range.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Mechanism is neurally described; authors do not equate it to human procedural arithmetic or explicit symbolic algorithm, and no direct mapping to classical algorithms is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3001.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal Mediation / Activation Patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal mediation analysis via activation interventions (activation patching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method used in this paper: treating internal MLP/attention outputs as mediators and measuring their causal effect on outputs by patching activations from one prompt into another and quantifying the indirect effect (IE) on predicted probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied across evaluated Transformer-based LMs (GPT-J, Pythia 2.8B, LLaMA 7B, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interventions operate on per-token outputs of MLP(l) and A(l) (attention module outputs) across layers and positions; IE computed using changes in probability assigned to competing numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to probe two-operand and three-operand arithmetic queries; also applied to number retrieval and factual prediction tasks for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Not a behavioral mechanism but an analysis intervention: inject stored activations (MLP outputs or attention outputs) from one run into another to measure causal influence of that component on the final probability distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>IE metric and log-probability variants show stable activation sites (early MLPs on operand tokens; attention at last token; mid-late last-token MLPs for results); intervention changes in probability and prediction (desired/wrong->correct) peak at predicted layers (19-20), demonstrating causal influence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Activation-patching results are sensitive to the choice of mediator granularity (module vs neuron vs head) and to prompt/template choices; interventions demonstrate causality with respect to model activations but cannot by themselves reveal exact arithmetic operations executed inside MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching (setting m(l)_t and a(l)_t to stored values from alternative prompts), neuron-level patching for top neurons, and fine-tuning as an intervention to change circuit structure.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Quantitatively shifts output probabilities and can flip model predictions; neuron-level interventions reveal overlapping and distinct neuron sets for arithmetic vs number retrieval vs factual tasks (top-400 neuron overlap ~50% between Arabic and word numerals, ~22% with number retrieval, ~9-10% with factual tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>IE used as primary metric (Equation 2) and log-probability difference as alternative; IE heatmaps and aggregated RI statistics show which components mediate predictions (e.g., RI numbers in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Module-level interventions obscure attention-head specifics and may conflate distinct head roles; IE does not reveal inner computations of MLPs and can miss distributed interactions across mediators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Methodological tool only; no mapping to human causal mechanisms or symbolic stepwise computations is claimed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3001.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3001.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuron-level result encoding (layer 19/20 MLP neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuron-level result-related representations in mid-late last-token MLP neurons (notably layer ~19)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that specific neurons (dimensions of MLP outputs) in mid-late layers at the last token carry result-related signals for arithmetic; neuron overlaps differ across tasks indicating task-specific circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Primarily GPT-J (analysis also performed on other models indirectly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuron-level analysis treats each output dimension of an MLP at a given layer as a separate mediator; interventions set single-neuron activations to values from alternative prompts and measure IE.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (Arabic numerals and numeral words), number retrieval, factual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>A subset of neurons in layer ~19 (MLP outputs at last token) are causally influential for producing arithmetic results; neuron sets overlap substantially between Arabic and word numeral representations but overlap less with number retrieval and factual prediction neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ranking neurons by average IE: ~50% overlap among top-400 neurons between arithmetic with Arabic numerals and numeral words; overlap with number retrieval only ~22-23%; overlap with factual queries ~9-10% (close to chance), implying distinct neuron sets for arithmetic result encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Overlap analysis does not account for effect magnitude, and per-neuron IE may miss distributed low-magnitude contributions; also causality shown is local to intervention setup and the restricted result vocabulary (e.g., S = 1..20).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Neuron-wise activation patching (setting single neuron activations to values from alternative prompts) and measuring the resulting IE.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Patching top-effect neurons produces measurable shifts in model output probabilities toward alternative results; neuron overlaps indicate some shared neurons across numeral formats but largely distinct neuron populations across different task types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top-400 neuron overlap percentages reported (approx. 50% Ar vs W, 22-23% Ar vs NR, 9-10% Ar vs F); layer 19 exhibits largest IE within M_late_-1 in GPT-J.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Single-neuron interventions are limited: they may not capture distributed representations, and ranking/overlap statistics ignore per-neuron effect sizes; expected overlap baseline is ~9.8% for random rankings (caveat applied).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct mapping to human neuron-level processes or symbolic representation; purely model-internal representational finding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mathematical framework for transformer circuits <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 2)</em></li>
                <li>Investigating gender bias in language models using causal mediation analysis <em>(Rating: 1)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
                <li>Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3001",
    "paper_id": "paper-258865170",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J (6 billion parameter autoregressive language model)",
            "brief_description": "A 6B-parameter decoder-only Transformer variant (GPT-family style) used in this paper to analyze arithmetic reasoning via causal mediation and activation interventions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J",
            "model_description": "Decoder-only Transformer (GPT-style) with 6B parameters, parallel attention and rotary positional encodings; pre-trained and used as the main model for analyses in this paper.",
            "arithmetic_task_type": "Two-operand arithmetic (addition, subtraction, multiplication, division) and three-operand chained operations; also numeral-word vs Arabic numeral representations.",
            "reported_mechanism": "Information about operands/operators is transmitted by attention from early/mid-sequence token positions to the final token; late (mid-to-late layer) MLP modules at the last token incorporate result-related information into the residual stream (i.e., they compute or produce result signals).",
            "evidence_for_mechanism": "Layer- and position-wise indirect-effect (IE) heatmaps show high IE at early MLPs on operand/operator tokens and high IE at attention modules at last token (layers ~11-18) plus spikes at last-token MLPs around layers 19-20; result-preserving vs result-varying experiments show last-token MLPs' IE drops when result fixed, implicating them in encoding results.",
            "evidence_against_mechanism": "Attention IE patterns do not strongly differentiate result-varying vs result-preserving cases (attention seems operand-related), and analyses treat attention modules as a whole (not per-head), leaving open whether a single attention head or a distributed set does the forwarding; also no conclusive demonstration of an explicit symbolic/algebraic algorithm being implemented.",
            "intervention_type": "Causal mediation (activation patching): during a forward pass, stored activations (MLP outputs or attention outputs) from one prompt are injected into another prompt's forward pass; also prompt two-shot exemplars used for behavioral performance.",
            "effect_of_intervention": "Interventions on last-token MLPs (layers 19-20) substantially shift model output probability mass toward the target arithmetic result; interventions on earlier layers shift representations but are less effective at producing correct result; activation-patching can produce desired prediction changes (desired/wrong-&gt;correct) primarily when applied to layers 19-20.",
            "performance_metrics": "Reported accuracy on two-operand tasks (Table 4): overall GPT-J accuracy 67.8% (by operator: +69.3%, -78.0%, ×82.8%, ÷40.8%); with numeral words overall 81.3%; on three-operand queries pre-trained GPT-J accuracy &lt;10% (poor). RI(M_late_-1) (relative importance of late last-token MLPs) = 40.2% (result-varying) vs 4.4% (result-fixed) for GPT-J.",
            "notable_failure_modes": "Division substantially worse than other operators; multi-operand (three-operand) queries poorly handled prior to fine-tuning; tokenizer issues for multi-digit numbers (splitting into multiple tokens) constrain experimental result sets; attention interventions measured at module level obscure head-level specifics.",
            "comparison_to_humans_or_symbolic": "No direct comparison to human arithmetic or symbolic calculators; mechanisms are described in neural-activation terms and not aligned to human symbolic procedures in the paper.",
            "uuid": "e3001.0"
        },
        {
            "name_short": "Pythia-2.8B",
            "name_full": "Pythia (2.8 billion parameter analysis suite model)",
            "brief_description": "A 2.8B-parameter decoder-only Transformer (Pythia family) used in the paper both pre-trained and after fine-tuning to analyze arithmetic flow and effects of targeted fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia 2.8B",
            "model_description": "Decoder-only Transformer with 2.8B parameters from the Pythia suite; used to run the three-operand experiments including a small fine-tuning procedure.",
            "arithmetic_task_type": "Two-operand arithmetic (add, sub, mul, div) and three-operand chained operations (various templates); also evaluated before and after fine-tuning for three-operand tasks.",
            "reported_mechanism": "Same attention-to-last-token then late-MLP processing pipeline observed: attention conveys operand/operator information to last token; late MLPs in middle-to-late layers synthesize result-related information.",
            "evidence_for_mechanism": "Pre- vs post-fine-tuning IE comparisons show emergence of the mid-late last-token MLP activation site after fine-tuning: pre-finetune the only relevant last-token activation was the very last layer; after fine-tune, mid-late MLP site appears (matching two-operand pattern).",
            "evidence_against_mechanism": "Before fine-tuning, Pythia did not show the mid-late MLP activation site strongly (indicating that the mechanism can be absent or weak in pre-trained models and induced by fine-tuning), suggesting the observed circuit may not be an inherent universal algorithm across architectures/sizes.",
            "intervention_type": "Fine-tuning (on a small synthetic three-operand dataset) and causal mediation activation patching in analyses.",
            "effect_of_intervention": "Fine-tuning on 3-operand queries improved accuracy from ~0.9% to ~39.7% overall (Pythia 2.8B fine-tuned), and produced the emergence of the mid-late last-token MLP activation site implicated in result computation.",
            "performance_metrics": "Pre-fine-tune three-operand overall accuracy ~0.9% (Table 4); post-fine-tune overall ~39.7%. RI(M_late_-1) values for Pythia 2.8B: 43.2% result-varying vs 5.8% result-fixed (two-operand experiments) and three-operand RI reported 13.5% pre-fine-tune vs 24.7% post-fine-tune in Table 1.",
            "notable_failure_modes": "Poor native handling of chained/three-operand arithmetic without fine-tuning; performance highly sensitive to fine-tuning and prompt representation; tokenization constraints for numeric tokens limit scaling to multi-digit numbers.",
            "comparison_to_humans_or_symbolic": "No explicit comparison to human or symbolic procedures beyond performance metrics; fine-tuning produces circuit changes rather than mapping to symbolic stepwise algorithms.",
            "uuid": "e3001.1"
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA (7 billion parameter open foundation model)",
            "brief_description": "A 7B-parameter decoder-only Transformer model (LLaMA family) included in the cross-model validation of the attention-&gt;MLP arithmetic information-flow hypothesis.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 7B",
            "model_description": "Decoder-only Transformer (LLaMA) with 7B parameters; used pre-trained (not fine-tuned in this study) to validate whether the information-flow pattern generalizes across model families and sizes.",
            "arithmetic_task_type": "Two-operand arithmetic tasks (add, sub, mul, div) as in other models.",
            "reported_mechanism": "Same qualitative mechanism: early/mid-sequence attention forwards operand/operator information to last token; mid-late last-token MLPs (roughly half-to-late layers) incorporate result-related signals.",
            "evidence_for_mechanism": "RI(M_late_-1) measured at 36.1% (result-varying) vs 7.5% (result-fixed), consistent with pattern seen in GPT-J and Pythia and supporting generality across models.",
            "evidence_against_mechanism": "Magnitude differences and tokenization differences across models (LLaMA tokenizes digits differently) mean some arithmetic evaluation settings required restrictions (e.g., single-digit results), limiting direct comparability.",
            "intervention_type": "Activation patching (causal mediation analysis) and two-shot prompting for behavior.",
            "effect_of_intervention": "Interventions on late MLPs shift output probabilities towards alternative results similarly to other models; relative importance shows substantial late MLP involvement.",
            "performance_metrics": "Reported overall accuracy (Table 4) very high for LLaMA on two-operand/grouped tasks in this paper's evaluation: overall 97.2% (per-operator: +100.0%, -100.0%, ×100.0%, ÷88.7%).",
            "notable_failure_modes": "Division remains the worst operator compared to others even when overall accuracy is high; tokenizer constraints forced restricting result vocabulary in some evaluations (single-digit restriction).",
            "comparison_to_humans_or_symbolic": "No explicit comparison to human/symbolic computation.",
            "uuid": "e3001.2"
        },
        {
            "name_short": "Goat",
            "name_full": "Goat (LLaMA variant fine-tuned on arithmetic tasks)",
            "brief_description": "A version of LLaMA fine-tuned specifically on arithmetic tasks (Goat) used to test whether targeted fine-tuning alters the circuits and improves arithmetic performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Goat (fine-tuned LLaMA)",
            "model_description": "LLaMA-based model further fine-tuned on arithmetic tasks (per Liu and Low, 2023) and evaluated here for circuit-level patterns and performance.",
            "arithmetic_task_type": "Two-operand arithmetic (add, sub, mul, div) and evaluated for circuit signatures.",
            "reported_mechanism": "Fine-tuning leads to similar attention-to-last-token forwarding but can amplify or make more specific the late-MLP result-producing activation sites.",
            "evidence_for_mechanism": "RI(M_late_-1) = 33.5% (result-varying) vs 7.4% (result-fixed) — pattern consistent with other models but magnitudes differ; performance on arithmetic tasks improved relative to base LLaMA in earlier goat-related work (cited).",
            "evidence_against_mechanism": "Paper notes goat shows different magnitudes but does not provide detailed per-layer fine-grained comparisons beyond RI statistics in appendix; causality is correlational w.r.t. fine-tuning changes.",
            "intervention_type": "Model fine-tuning on arithmetic datasets and activation patching analyses.",
            "effect_of_intervention": "Fine-tuning increases reliance and effect of mid-late last-token MLPs for producing result signals, consistent with pattern that training can shape/strengthen these circuits.",
            "performance_metrics": "Reported overall accuracy for Goat in Table 4: 85.6% overall (per-operator: +91.4% for multiplication, ÷54.0% for division), showing improved arithmetic ability compared to some baselines.",
            "notable_failure_modes": "Division weaker than other operators; patterns suggest fine-tuning changes but does not necessarily create fully robust algorithmic arithmetic capability.",
            "comparison_to_humans_or_symbolic": "No explicit human/symbolic comparison.",
            "uuid": "e3001.3"
        },
        {
            "name_short": "Attention-&gt;LastToken-&gt;MLP pipeline",
            "name_full": "Attention-mediated forwarding to last token followed by late-MLP result synthesis",
            "brief_description": "Mechanistic hypothesis introduced in this paper: operand/operator representations are computed at early token positions, attention forwards relevant information to the final token, and late MLP modules at the last token synthesize result-related signals incorporated into the residual stream for prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-based LMs (GPT-J, Pythia, LLaMA, Goat)",
            "model_description": "Applies to decoder-only Transformer architectures evaluated (2.8B–7B models) with standard attention + MLP blocks and residual streams; numeric prompts rendered either as Arabic numerals or numeral words.",
            "arithmetic_task_type": "Two-operand arithmetic, operator-varying experiments, three-operand chained arithmetic (after fine-tuning).",
            "reported_mechanism": "Two-stage mechanism: (1) early layers/MLPs encode operands/operators; attention modules (mid layers) convey these representations to the last token; (2) mid-to-late last-token MLPs generate result-related representations which bias next-token probabilities toward the numerical result.",
            "evidence_for_mechanism": "IE heatmaps show consistent early-MLP activation at operand tokens, attention peaks across layers at last token, and MLP peaks at layers ~19-20 at last token; result-fixed vs result-varying comparisons isolate result-encoding to late MLPs; neuron-level overlaps and intervention-success metrics further support distinct neuron sets for arithmetic result computation.",
            "evidence_against_mechanism": "Does not prove an explicit arithmetic algorithm (e.g., stepwise symbolic computation) is implemented; attention is measured at module-level (not per head) leaving head-level forwarding unclear; variation across models and dependence on fine-tuning indicate mechanism is not strictly universal.",
            "intervention_type": "Activation patching (causal mediation analysis) where MLP outputs and attention outputs from one prompt are injected into another; also two-shot prompting and fine-tuning experiments.",
            "effect_of_intervention": "Patching late last-token MLP activations from prompts with different results shifts model probabilities strongly toward that result; interventions at layers 19-20 are most likely to induce desired change from wrong-&gt;correct, whereas interventions at slightly earlier layers (14-17) induce undesired changes more often.",
            "performance_metrics": "Relative Importance (RI) measure quantifies contribution of mid-late last-token MLPs: e.g., GPT-J RI(M_late_-1)=40.2% (result-varying) vs 4.4% (result-fixed); consistent spikes in IE at layers 19-20 across operators.",
            "notable_failure_modes": "Mechanism identifies where result information appears but not the specific operations performed inside MLPs; inability to examine attention heads reduces granularity; tokenizer limitations restrict numeric range.",
            "comparison_to_humans_or_symbolic": "Mechanism is neurally described; authors do not equate it to human procedural arithmetic or explicit symbolic algorithm, and no direct mapping to classical algorithms is provided.",
            "uuid": "e3001.4"
        },
        {
            "name_short": "Causal Mediation / Activation Patching",
            "name_full": "Causal mediation analysis via activation interventions (activation patching)",
            "brief_description": "Method used in this paper: treating internal MLP/attention outputs as mediators and measuring their causal effect on outputs by patching activations from one prompt into another and quantifying the indirect effect (IE) on predicted probabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied across evaluated Transformer-based LMs (GPT-J, Pythia 2.8B, LLaMA 7B, Goat)",
            "model_description": "Interventions operate on per-token outputs of MLP(l) and A(l) (attention module outputs) across layers and positions; IE computed using changes in probability assigned to competing numerical results.",
            "arithmetic_task_type": "Used to probe two-operand and three-operand arithmetic queries; also applied to number retrieval and factual prediction tasks for comparison.",
            "reported_mechanism": "Not a behavioral mechanism but an analysis intervention: inject stored activations (MLP outputs or attention outputs) from one run into another to measure causal influence of that component on the final probability distribution.",
            "evidence_for_mechanism": "IE metric and log-probability variants show stable activation sites (early MLPs on operand tokens; attention at last token; mid-late last-token MLPs for results); intervention changes in probability and prediction (desired/wrong-&gt;correct) peak at predicted layers (19-20), demonstrating causal influence.",
            "evidence_against_mechanism": "Activation-patching results are sensitive to the choice of mediator granularity (module vs neuron vs head) and to prompt/template choices; interventions demonstrate causality with respect to model activations but cannot by themselves reveal exact arithmetic operations executed inside MLPs.",
            "intervention_type": "Activation patching (setting m(l)_t and a(l)_t to stored values from alternative prompts), neuron-level patching for top neurons, and fine-tuning as an intervention to change circuit structure.",
            "effect_of_intervention": "Quantitatively shifts output probabilities and can flip model predictions; neuron-level interventions reveal overlapping and distinct neuron sets for arithmetic vs number retrieval vs factual tasks (top-400 neuron overlap ~50% between Arabic and word numerals, ~22% with number retrieval, ~9-10% with factual tasks).",
            "performance_metrics": "IE used as primary metric (Equation 2) and log-probability difference as alternative; IE heatmaps and aggregated RI statistics show which components mediate predictions (e.g., RI numbers in Table 1).",
            "notable_failure_modes": "Module-level interventions obscure attention-head specifics and may conflate distinct head roles; IE does not reveal inner computations of MLPs and can miss distributed interactions across mediators.",
            "comparison_to_humans_or_symbolic": "Methodological tool only; no mapping to human causal mechanisms or symbolic stepwise computations is claimed.",
            "uuid": "e3001.5"
        },
        {
            "name_short": "Neuron-level result encoding (layer 19/20 MLP neurons)",
            "name_full": "Neuron-level result-related representations in mid-late last-token MLP neurons (notably layer ~19)",
            "brief_description": "Finding that specific neurons (dimensions of MLP outputs) in mid-late layers at the last token carry result-related signals for arithmetic; neuron overlaps differ across tasks indicating task-specific circuits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Primarily GPT-J (analysis also performed on other models indirectly)",
            "model_description": "Neuron-level analysis treats each output dimension of an MLP at a given layer as a separate mediator; interventions set single-neuron activations to values from alternative prompts and measure IE.",
            "arithmetic_task_type": "Two-operand arithmetic (Arabic numerals and numeral words), number retrieval, factual queries.",
            "reported_mechanism": "A subset of neurons in layer ~19 (MLP outputs at last token) are causally influential for producing arithmetic results; neuron sets overlap substantially between Arabic and word numeral representations but overlap less with number retrieval and factual prediction neurons.",
            "evidence_for_mechanism": "Ranking neurons by average IE: ~50% overlap among top-400 neurons between arithmetic with Arabic numerals and numeral words; overlap with number retrieval only ~22-23%; overlap with factual queries ~9-10% (close to chance), implying distinct neuron sets for arithmetic result encoding.",
            "evidence_against_mechanism": "Overlap analysis does not account for effect magnitude, and per-neuron IE may miss distributed low-magnitude contributions; also causality shown is local to intervention setup and the restricted result vocabulary (e.g., S = 1..20).",
            "intervention_type": "Neuron-wise activation patching (setting single neuron activations to values from alternative prompts) and measuring the resulting IE.",
            "effect_of_intervention": "Patching top-effect neurons produces measurable shifts in model output probabilities toward alternative results; neuron overlaps indicate some shared neurons across numeral formats but largely distinct neuron populations across different task types.",
            "performance_metrics": "Top-400 neuron overlap percentages reported (approx. 50% Ar vs W, 22-23% Ar vs NR, 9-10% Ar vs F); layer 19 exhibits largest IE within M_late_-1 in GPT-J.",
            "notable_failure_modes": "Single-neuron interventions are limited: they may not capture distributed representations, and ranking/overlap statistics ignore per-neuron effect sizes; expected overlap baseline is ~9.8% for random rankings (caveat applied).",
            "comparison_to_humans_or_symbolic": "No direct mapping to human neuron-level processes or symbolic representation; purely model-internal representational finding.",
            "uuid": "e3001.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 2,
            "sanitized_title": "a_mathematical_framework_for_transformer_circuits"
        },
        {
            "paper_title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_build_predictions_by_promoting_concepts_in_the_vocabulary_space"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        },
        {
            "paper_title": "Investigating gender bias in language models using causal mediation analysis",
            "rating": 1,
            "sanitized_title": "investigating_gender_bias_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks",
            "rating": 2,
            "sanitized_title": "goat_finetuned_llama_outperforms_gpt4_on_arithmetic_tasks"
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling",
            "rating": 1,
            "sanitized_title": "pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"
        }
    ],
    "cost": 0.01564075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis
20 Oct 2023</p>
<p>Alessandro Stolfo stolfoa@ethz.ch 
ETH Zürich</p>
<p>Yonatan Belinkov belinkov@technion.ac.il 
Technion -IIT
Israel</p>
<p>Mrinmaya Sachan msachan@ethz.ch 
ETH Zürich</p>
<p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis
20 Oct 2023F3EE1A02DBD30485E3EBD6B975383EFFarXiv:2305.15054v2[cs.CL]
Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture.In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework.By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions.This provides insights into how information related to arithmetic is processed by LMs.Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism.Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream.To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions. 1</p>
<p>Introduction</p>
<p>Mathematical reasoning with Transformer-based models (Vaswani et al., 2017) is challenging as it requires an understanding of the quantities and the mathematical concepts involved.While large language models (LMs) have recently achieved impressive performance on a set of math-based tasks (Wei et al., 2022a;Chowdhery et al., 2022;OpenAI, 2023), their behavior has been shown to be inconsistent and context-dependent (Bubeck et al., 2023).Recent literature shows a multitude 1 Our code and data is available at https://github.com/alestolfo/lm-arithmetic.We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A).Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B).Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).</p>
<p>of works proposing methods to improve the performance of large LMs on math benchmark datasets through enhanced pre-training (Spokoyny et al., 2022;Lewkowycz et al., 2022;Liu and Low, 2023) or specific prompting techniques (Wei et al., 2022b;Kojima et al., 2022;Yang et al., 2023, inter alia).</p>
<p>However, there is a limited understanding of the inner workings of these models and how they store and process information to correctly perform mathbased tasks.Insights into the mechanics behind LMs' reasoning are key to improvements such as inference-time correction of the model's behavior (Li et al., 2023) and safer deployment.Therefore, research in this direction is critical for the development of more faithful and accurate next-generation LM-based reasoning systems.</p>
<p>In this paper, we present a set of analyses aimed at mechanistically interpreting LMs on the task of answering simple arithmetic questions (e.g., "What is the product of 11 and 17?").In particular, we hypothesize that the computations involved in reasoning about such arithmetic problems are carried out by a specific subset of the network.Then, we test this hypothesis by adopting a causal mediation analysis framework (Vig et al., 2020;Meng et al., 2022), where the model is seen as a causal graph going from inputs to outputs, and the model components (e.g., neurons or layers) are seen as mediators (Pearl, 2001).Within this framework, we assess the impact of a mediator on the observed output behavior by conducting controlled interventions on the activations of specific subsets of the model and examining the resulting changes in the probabilities assigned to different numerical predictions.Through this experimental procedure, we track the flow of information within the model and identify the model components that encode information about the result of arithmetic queries.Our findings show that the model processes the input by conveying information about the operator and the operands from mid-sequence early layers to the final token using attention.At this location, the information is processed by a set of MLP modules, which output result-related information into the residual stream (shown in Figure 1).We verify this finding for biand tri-variate arithmetic queries across four pretrained language models with different sizes: 2.8B, 6B, and 7B parameters.Finally, we compare the effect of different model components on answering arithmetic questions to two additional tasks: a synthetic task that involves retrieving a number from the prompt and answering questions related to factual knowledge.This comparison validates the specificity of the activation dynamics observed on arithmetic queries.</p>
<p>Related Work</p>
<p>Mechanistic Interpretability.The objective of mechanistic interpretability is to reverse engineer model computation into components, aiming to discover, comprehend, and validate the algorithms (called circuits in certain works) implemented by the model weights (Räuker et al., 2023).Early work in this area analyzed the activation values of single neurons when generating text using LSTMs (Karpathy et al., 2015).A multitude of studies have later focused on interpreting weights and intermediate representations in neural networks (Olah et al., 2017(Olah et al., , 2018(Olah et al., , 2020;;Voss et al., 2021;Goh et al., 2021) and on how information is processed by Transformer-based (Vaswani et al., 2017) language models (Geva et al., 2021(Geva et al., , 2022(Geva et al., , 2023;;Olsson et al., 2022;Nanda et al., 2023).Although not strictly mechanistic, other recent studies have analyzed the hidden representations and behavior of inner components of large LMs (Belrose et al., 2023;Gurnee et al., 2023;Bills et al., 2023).</p>
<p>Causality-based Interpretability.Causal mediation analysis is an important tool that is used to effectively attribute the causal effect of mediators on an outcome variable (Pearl, 2001).This paradigm was applied to investigate LMs by Vig et al. (2020), who proposed a framework based on causal mediation analysis to investigate gender bias.Variants of this approach were later applied to mechanistically interpret the inner workings of pre-trained LMs on other tasks such as subject-verb agreement (Finlayson et al., 2021), natural language inference (Geiger et al., 2021), indirect object identification (Wang et al., 2022), and to study their retention of factual knowledge (Meng et al., 2022).</p>
<p>Math and Arithmetic Reasoning.A growing body of work has proposed methods to analyze the performance and robustness of large LMs on tasks involving mathematical reasoning (Pal and Baral, 2021;Piękos et al., 2021;Razeghi et al., 2022;Cobbe et al., 2021;Mishra et al., 2022).In this area, Stolfo et al. (2023) use a causally-grounded approach to quantify the robustness of large LMs.However, the proposed formulation is limited to behavioral investigation with no insights into the models' inner mechanisms.To the best of our knowledge, our study represents the first attempt to connect the area of mechanistic interpretability to the investigation of the mathematical reasoning abilities in Transformer-based LMs.</p>
<p>Methodology</p>
<p>Background and Task</p>
<p>We denote an autoregressive language model as G : X → P. The model operates over a vocabulary V and takes a token sequence x = [x 1 , ..., x T ] ∈ X , where each x i ∈ V .G generates a probability distribution P ∈ P : R |V | → [0, 1] that predicts possible next tokens following the sequence x.In this work, we study decoder-only Transformer-based models (Vaswani et al., 2017).Specifically, we focus on models that represent a slight variation of the standard GPT paradigm, as they utilize parallel attention (Wang and Komatsuzaki, 2021) and rotary positional encodings (Su et al., 2022).The internal computation of the model's hidden states h (l) t at position t ∈ {1, . . ., T } of the input sequence is carried out as follows:
h (l) t = h (l−1) t + a (l) t + m (l) t (1) a (l) t = A (l) h (l−1) 1 , . . . , h (l−1) t m (l) t = W (l) proj σ W (l) f c h (l−1) t =: MLP (l) (h (l−1) t ),
where at layer l, σ is the sigmoid nonlinearity, W (l) f c and W (l) proj are two matrices that parameterize the multilayer perceptron (MLP) of the Transformer block and A (l) is the attention mechanism. 2e consider the task of computing the result of arithmetic operations.Each arithmetic query consists of a list of operands N = (n 1 , n 2 , . . . ) and a function f O representing the application of a set of arithmetic operators (+, −, ×, ÷).We denote as r = f O (N ) the result obtained by applying the operators to the operands.Each query is rendered as a natural language question through a prompt p(N, f O ) ∈ X such as "How much is n 1 plus n 2 ?" (in this case, f O (n 1 , n 2 ) = n 1 + n 2 ).The prompt is then fed to the language model to produce a probability distribution P over V .Our aim is to investigate whether certain hidden state variables are more important than others during the process of computing the result r.</p>
<p>Experimental Procedure</p>
<p>We see the model G as a causal graph (Pearl, 2009), framing internal model components, such as specific neurons, as mediators positioned along the causal path connecting model inputs and outputs.Following a causal mediation analysis procedure, we then quantify the contribution of particular model components by intervening on their activation values and measuring the change in the model's output.Previous work has isolated the effect of every single neuron within a model (Vig et al., 2020;Finlayson et al., 2021).However, this approach becomes impractical for models with billions of parameters.Therefore, for our main experiments, the elements that we consider as variables along the causal path described by the model are the outputs of the MLP (l) and A (l) functions at each token t, i.e., m</p>
<p>t and a (l) t .To quantify the importance of modules MLP (l)  and A (l) in mediating the model's predictions at position t, we use the following procedure.</p>
<ol>
<li>
<p>Given f O , we sample two sets of operands N , N ′ , and we obtain r = f O (N ) and r ′ = f O (N ′ ).Then, two input questions with only the operands differing, p 1 = p(N, f O ) and p 2 = p(N ′ , f O ), are passed through the model.</p>
</li>
<li>
<p>During the forward pass with input p 1 , we store the activation values m(l) t := MLP (l) (h
(l−1) t
), and ā(l) t := A (l) (h
(l−1) 1 , . . . , h (l−1) t
) .</p>
</li>
<li>
<p>We perform an additional forward pass using p 2 , but this time we intervene on components MLP (l) and A (l) at position t, setting their activation values to m(l) t , and ā(l) t , respectively.This process is illustrated in Figure 2.More specifically, we compute the indirect effect (IE) of a specific mediating component by quantifying its contribution in skewing P towards the correct result.Consider a generic activation variable z ∈ {m
(1) 1 , . . . , m (L) t , a (1) 1 , . . . , a (L)
t }.We denote the model's output probability following an intervention on z as P * z .Then, we compute the IE as:
IE(z) = 1 2 P * z (r) − P(r) P(r) + P(r ′ ) − P * z (r ′ ) P * z (r ′ )(2)
where the two terms in the sum represent the relative change in the probability assigned by the model to r and r ′ , caused by the intervention performed.</p>
</li>
</ol>
<p>The larger the measured IE, the larger the contribution of component z in shifting probability mass from the clean-run result r ′ to result r corresponding to the alternative input p 1 . 3e additionally measure the mediation effect of each component with respect to the operation f O .We achieve this by fixing the operands and changing the operator across the two input prompts.More formally, in step 1, we sample a list of operands N and two operators f O and f ′ O .Then, we generate two prompts
p 1 = p(N, f O ) and p 2 = p(N, f ′ O ) (e.g., "
What is the sum of 11 and 7?" and "What is the product of 11 and 7?").Finally, we carry out the procedure in steps 2-4.</p>
<p>Experimental Setup</p>
<p>We present the results of our analyses in the main paper for GPT-J (Wang and Komatsuzaki, 2021), a 6B-parameter pre-trained LM (Gao et al., 2020).Additionally, we validate our findings on Pythia 2.8B (Biderman et al., 2023), LLaMA 7B (Touvron et al., 2023), and Goat, a version of LLaMA finetuned on arithmetic tasks (Liu and Low, 2023).We report the detailed results for these models in Appendix J.</p>
<p>In our experiments, we focus on two-and threeoperand arithmetic problems.Similar to previous work (Razeghi et al., 2022;Karpas et al., 2022), for single-operator two-operand queries, we use a set of six diverse templates representing a question involving each of the four arithmetic operators.For the three-operand queries, we use one template for each of the 29 possible two-operator combinations.Details about the templates are reported in Appendix A. In the bi-variate case, for each of the four operators f O ∈ {+, −, ×, ÷} and for each of the templates, we generate 50 pairs of prompts by sampling two pairs of operands (n 1 , n 2 ) ∈ S 2 and
(n ′ 1 , n ′ 2 ) ∈ S 2
, where S ⊂ V ∩ N.For the operandrelated experiment, we sample (n 1 , n 2 ) and a second operation f ′ O .In both cases, we ensure that the result r falls within S. 4 In the three-operand case, we generate 15 pairs of prompts for each of the 29 templates, following the same procedure.In order to ensure that the model achieves a meaningful task performance, we use a two-shot prompt in which we include two exemplars of question-answer for the same operation that is being queried.We report the accuracy results in Appendix B.</p>
<p>Causal Effects on Arithmetic Queries</p>
<p>Our analyses address the following question:</p>
<p>Q1 What are the components of the model that mediate predictions involving arithmetic computations?</p>
<p>We address this question by first studying the flow of information throughout the model by measuring the effect of each component (MLP and attention block) at each point of the input sequence for two-operand queries ( §4.1).Then, we distinguish between model components that carry information about the result and about the operands of the arithmetic computations ( §4.2 and §4.3).Finally, we consider queries involving three operands ( §4.4) and present a measure to quantify the changes in information flow ( §4.5).</p>
<p>Tracing the Information Flow</p>
<p>We measure the indirect effect of each MLP and attention block at different positions along the input sequence.The output of these modules can be seen as new information being incorporated into the residual stream.This new information can be produced at any point of the sequence and then conveyed to the end of the sequence for the prediction of the next token.By studying the IE at different locations within the model, we can identify the modules that generate new information relevant to the model's prediction.The results are reported in Figures 3a and 3b for MLP and attention, respectively.</p>
<p>(e) (f) Our analysis reveals four primary activation sites: the MLP module situated at the first layer corresponding to the tokens of the two operands; the intermediate attention blocks at the last token of the sequence; and the MLP modules in the middleto-late layers, also located at the last token of the sequence.It is expected to observe a high effect for the first MLPs associated with the tokens that vary (i.e., the operands), as such modules are likely to affect the representation of the tokens, which is subsequently used for the next token prediction.On the other hand, of particular interest is the high effect detected at the attention modules in layers 11-18 and in the MLPs around layer 20.
(g) (h) (a) (b) (c) (d)
As for the flow of information tied to the operator, the activations display a parallel pattern: high effect is registered at early MLPs associated with the operator tokens and at the same last-token MLP and attention locations.We report the visualization of the operator-related results in Appendix C.</p>
<p>A possible explanation of the model's behavior on this task is that the attention mechanism facilitates the propagation of operand-and operatorrelated information from the first layers early in the sequence to the last token.Here, this information is processed by the MLP modules, which incorporate the information about the result of the computation in the residual stream.This hypothesis aligns with the existing theory that attributes the responsibility of moving and extracting infor-mation within Transformer-based models to the attention mechanism (Elhage et al., 2021;Geva et al., 2023), while the feed-forward layers are associated with performing computations, retrieving facts and information (Geva et al., 2022;Din et al., 2023;Meng et al., 2022).We test the validity of this hypothesis in the following section.</p>
<p>Operand-and Result-related Effects</p>
<p>Our objective is to verify whether the contribution to the model's prediction of each component measured in Figures 3a and 3b is due to (1) the component representing information related to the operands, or (2) the component encoding information about the result of the computation.To this end, we formulate a variant of our previous experimental procedure.In particular, we condition the sampling of the second pair of operands (n ′ 1 , n ′ 2 ) on the constraint r = r ′ .That is, we generate the two input questions p 1 and p 2 , such that their result is the same (e.g., "What is the sum of 25 and 7?" and "What is the sum of 14 and 18?").In case number (1), we would expect a component to have high IE both in the result-varying setting and when r = r ′ , as the operands are modified in both scenarios.In case (2), we expect a subset of the model to have a large effect when the operands are sampled without constraints but a low effect for the fixed-result setting.</p>
<p>We report the results in Figure 3c and 3d.By comparing Figures 3a and 3c, two notable observations emerge.First, the high effect in the early layers corresponding to the operand tokens is observed in both the result-preserving and the resultvarying scenarios.Second, the last-token mid-late MLPs that lead to a high effect on the model's prediction following a result change, dramatically decrease their effect on the model's output in the result-preserving setting, as described in scenario (2).These observations point to the conclusion that the MLP blocks around layer 20 incorporate resultrelevant information.As for the contribution of the attention mechanism (Figures 3b and 3d), we do not observe a substantial difference in the layers with the highest IE between the two settings, which aligns this scenario to the description of case (1).These results are consistent with our hypothesis that operand-related information is transferred by the attention mechanism to the end of the sequence and then processed by the MLPs to obtain the result of the computation.</p>
<p>Zooming in on the Last Token</p>
<p>In Figures 3e-3h, we show a re-scaled version of the IE measurements for the layers at the end of the input sequence.While the large difference in magnitude was already evident in the previously considered visualizations, in Figures 3e and 3f we notice that the MLPs with the highest effect in the two settings differ: the main contribution to the model's output when the results are not fixed is given by layers 19 and 20, while in the resultpreserving setting the effect is largest at layers 14-18.For the attention (Figures 3g and 3h), we do not observe a significant change in the shape of the curve describing the IE across different layers, with layer 13 producing the largest contribution.</p>
<p>We interpret this as additional evidence indicating that the last-token MLPs at layers 19-20 encode information about r, while the attention modules carry information related to the operands.</p>
<p>Three-operand Queries &amp; Fine-tuning</p>
<p>We extend our analyses by including three-operand arithmetic queries such as "What is the difference between n 1 and the ratio between n 2 and n 3 ?". Answering correctly this type of questions represents a challenging task for pre-trained language models, and we observe poor accuracy (below 10%) with GPT-J.Thus, we opt for fine-tuning the model on a small set of three-operand queries.The model that we consider for this analysis is Pythia 2.8B, as its smaller size allows for less computationally demanding training than the 6B-parameter GPT-J.</p>
<p>After fine-tuning, the model attains an accuracy of ∼40%.We provide the details about the training procedure in Appendix F. We carry out the experimental procedure as in Section 4.1.In particular, we compare the information flow in the MLPs of the model before and after fine-tuning (Figure 4).In the non-fine-tuned version of the model, the only relevant activation site, besides the early layers at the operand tokens, is the very last layer at the last token.In the fine-tuned model, on the other hand, we notice the emergence of the mid-late MLP activation site that was previously observed in the two-operand setting.</p>
<p>Quantifying the Change of the Information Flow</p>
<p>Denote the set of MLPs in the model by M. We define the relative importance (RI) of a specific subset M * ⊆ M of MLP modules as
RI(M * ) = m∈M * log(IE(m) + 1) m∈M log(IE(m) + 1)
.</p>
<p>In order to quantitatively show the difference in the activation sites observed in Figure 3, we compute the RI measure for the set where the subscript −1 indicates the last token of the input sequence and L is the number of layers in the model.This quantity represents the relative contribution of the mid-late last-token MLPs compared to all the MLP blocks in the model.For the two-operand setting, we carry out the experimental procedure described in Section 3 for three additional models: Pythia 2.8B, LLaMA 7B, and Goat.5 Furthermore, we repeat the analyses on GPT-J using a different number representation: instead of Arabic numbers (e.g., the token 2), we represent quantities using numeral words (e.g., the token two).For the three-operand setting, we report the results for Pythia 2.8B before and after fine-tuning.We measure the effects using both randomly sampled and result-preserving operand pairs, comparing the RI measure in the two settings.The results (Table 1) exhibit consistency across all these four additional experiments.These quantitative measurements further highlight the influence of last-token late MLP modules on the prediction of r.We provide in Appendix J the heatmap illustrations of the effects for these additional studies.
M late −1 = {m (⌊L/2⌋) −1 , m (⌊L/2⌋+1) −1 , . . . , m (L) −1 },</p>
<p>Causal Effects on Different Tasks</p>
<p>In order to understand whether the patterns in the effect of the model components that we observed so far are specific to arithmetic queries, we compare our observations on arithmetic queries to two different tasks: the retrieval of a number from the prompt ( §5.1), and the prediction of factual knowl- Q2 Are the activation patterns observed so far specific to the arithmetic setting?</p>
<p>Information Flow on Number Retrieval</p>
<p>We consider a simple synthetic task involving numerical predictions.We construct a set of templates of the form "Paul has n 1 e 1 and n 2 e 2 .How many e q does Paul have?", where n 1 , n 2 are two randomly sampled numbers, e 1 and e 2 are two entity names sampled at random,6 and e q ∈ {e 1 , e 2 }.In this case, the two input prompts p 1 and p 2 differ solely in the value of e q .To provide the correct answer to a query, the model has simply to retrieve the correct number from the prompt.With this task, we aim to analyze the model's behavior in a setting involving numerical predictions but not requiring any kind of arithmetic computation.We report the indirect effect measured for the MLPs modules of GPT-J in Figure 5.In this setting, we observe an unsurprising high-effect activation site corresponding to the tokens of the entity e q and a lower-effect site at the end of the input in layers 14-20.The latter site appears in the set of the model components that were shown to be active on arithmetic queries.However, computing the relative importance of the late MLPs on this task shows that this second activation site is responsible for only RI(M late −1 ) = 8.7% of the overall log IE.The low RI, compared to the higher values measured on arithmetic queries, suggests that the function of the last-token late MLPs is not dictated by the numerical type of prediction, but rather by their involvement in processing the input information.This finding is aligned with our theory that sees M late −1 as the location where information about r is included in the residual stream.</p>
<p>Information Flow on Factual Predictions</p>
<p>We carry out our experimental procedure using data from the LAMA benchmark (Petroni et al., 2019), which consists of natural language templates representing knowledge-base relations, such as "[subject] is the capital of [object]".By instantiating a template with a specific subject (e.g., "Paris"), we prompt the model to predict the correct object ("France").Similar to our approach with arithmetic questions, we create pairs of factual queries that differ solely in the subject.In particular, we sample pairs of entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of").Details about the data used for this procedure are provided in Appendix H.We then measure the indirect effect following the formulation in Equation 2, where the correct object corresponds to the correct numerical outcome in the arithmetic scenario.</p>
<p>From the results (Figure 6), we notice that a main activation site emerges in early layers at the tokens corresponding to the subject of the query.These findings are consistent with previous works (Meng et al., 2022;Geva et al., 2023), which hypothesize that language models store and retrieve factual associations in early MLPs located at the subject tokens.We compute the RI metric for the late MLP modules, which quantitatively validates the contribution of the early MLP activation site by attaining a low value of RI(M late −1 ) = 4.2%.The large IE observed at mid-sequence early MLPs represents a difference in the information flow with respect to the arithmetic scenario, where the modules with the highest influence on the model's prediction are located at the end of the sequence.This difference serves as additional evidence highlighting the specificity of the model's activation patterns when answering arithmetic queries.</p>
<p>Neuron-level Interventions</p>
<p>The experimental results in Sections 5.1 and 5.2 showed a quantitative difference in the contributions of last-token mid-late MLPs between arithmetic queries and two tasks that do not involve arithmetic computation.Now, we investigate whether the components active within M late −1 on the different types of tasks are different.We carry out a finer-grained analysis in which we consider independently each neuron in an MLP module (i.e., each dimension in the output vector of the function MLP (l) ) at a specific layer l.In particular, following the same procedure as for layer-level experiments, we intervene on each neuron by setting its activation to the value it would take if the input query contained different operands (or a different entity).We then compute the corresponding indirect effect as in Eq. 2. We carry out this procedure for arithmetic queries using Arabic numerals (Ar) and numeral words (W), for the number retrieval task (NR), and for factual knowledge queries (F). 7e rank the neurons according to the average effect measured for each of these four settings and compute the overlap in the top 400 neurons (roughly 10%, as GPT-J has a hidden dimension of 4096).</p>
<p>We carry out this procedure for layer l = 19, as it exhibits the largest IE within M late −1 on all the tasks considered.The heatmap in Figure 7 illustrates the results.We observe a consistent overlap (50%) between the top neurons active for the arithmetic queries using Arabic and word-based representa-tions.Interestingly, the size of the neuron overlap between arithmetic queries and number retrieval is considerably lower (22% and 23%), even though both tasks involve the prediction of numerical quantities.Finally, the overlaps between the top neurons for the arithmetic operations and the factual predictions (between 9% and 10%) are not larger than for random rankings: the expected overlap ratio between the top 400 indices in two random rankings of size 4096 is 9.8% (Antverg and Belinkov, 2022).These results support the hypothesis that the model's circuits responsible for different kinds of prediction, though possibly relying on similar subsets of layers, are distinct.However, it is important to note that this measurement does not take into account the magnitude of the effect.</p>
<p>Conclusion</p>
<p>We proposed the use of causal mediation analysis to mechanistically investigate how LMs process information related to arithmetic.Through controlled interventions on specific subsets of the model, we assessed the impact of these mediators on the model's predictions.</p>
<p>We posited that models produce predictions to arithmetic queries by conveying the math-relevant information from the mid-sequence early layers to the last token, where this information is then processed by late MLP modules.We carried out a causality-grounded experimental procedure on four different Transformer-based LMs, and we provided empirical evidence supporting our hypothesis.Furthermore, we showed that the information flow we observed in our experiments is specific to arithmetic queries, compared to two other tasks that do not involve arithmetic computation.</p>
<p>Our findings suggest potential avenues for research into model pruning and more targeted training/fine-tuning by concentrating on specific model components associated with certain queries or computations.Moreover, our results offer insights that may guide further studies into using LMs' hidden representations to correct the model's behavior on math-based tasks at inference time (Li et al., 2023) and to estimate the probability of the model's predictions to be true (Burns et al., 2023).</p>
<p>Limitations</p>
<p>The scope of our work is investigating arithmetic reasoning and we experiment with the four fundamental arithmetic operators.Addition, subtraction, multiplication, and division form the cornerstone of arithmetic calculations and serve as the basis for a wide range of mathematical computations.Thus, exploring their mechanisms in language models provides a starting point to explore more complex forms of mathematical processing.Studying a broader set of mathematical operators represents an interesting avenue for further investigation.</p>
<p>Our work focuses on synthetically-generated queries that are derived from natural language descriptions of the four basic arithmetic operators.To broaden the scope, future research can expand the analysis of model activations to encompass mathbased queries described in real-life settings, such as math word problems.</p>
<p>Finally, a limitation of our work concerns the analysis of different attention heads.In our experiments, we consider the output of an attention module as a whole.Future research could focus on identifying the specific heads that are responsible for forwarding particular types of information in order to offer a more detailed understanding of their individual contributions.</p>
<p>A Prompt Templates</p>
<p>In Tables 2 and 3, we report the question templates from Karpas et al. (2022), which we used as prompts for the model for two-and three-operand queries, respectively.For three-operand queries, we use one query template for each of the 29 possible two-operation combinations.</p>
<p>B Performance of the Models</p>
<p>In Table 4, we report the accuracy of the models on the arithmetic queries that we use for our analyses.The higher accuracy obtained using numeral words is likely given by the smaller set of possible solutions considered (we used S = {"one", "two", . . ., "twenty"}, as the numeral words corresponding to larger numbers get split into multiple tokens by the tokenizer).The accuracy of GPT-J on the factual queries from the LAMA benchmark is 65.0% (we constrain the vocabulary to the set of all possible objects for all the relations considered).On the synthetic number retrieval task, GPT-J's accuracy is 86.7%.</p>
<p>C Flow of Operator-related Information</p>
<p>The measurements of the indirect effect for each model component when fixing the operand and varying the operator in the two input prompts p 1 and p 2 reveal how the model processes the information related to the operator.We report in Figure 8 the heatmap visualizations of these results for twooperand queries.Similar to the operand-related information, we observe a high effect in three activation locations: early MLP blocks corresponding to the operand tokens; middle-to-early attention modules at the last token; and middle-to-late MLP modules at the last token.These results align with our hypothesis that arithmetic-related information is transferred to the end of the sequence by the attention mechanism, where it is then processed by late MLP layers.In this setting, we measure RI(M late −1 ) =31.4%.</p>
<p>D Effects for Each Operator</p>
<p>For each of the four operators, we report the indirect effect measured for the last-token MLP modules in GPT-J in Figure 9.The results for each of the four operators show a common spike in the effect at layers 19-20.This indicates the presence of a specific part of the model relevant to the numerical predictions of the bi-variate arithmetic questions, irrespective of the operator involved.We also notice a difference in the magnitude of the effects, which is linked to the capability of the model to correctly answer the query.</p>
<p>E Changes in the Model's Prediction</p>
<p>We measured the influence of the model components in terms of probability changes.Now, we study the dynamics of the actual model predictions.</p>
<p>In particular, considering the scenario in which r = r ′ , we verify whether the intervention leads to a change in the model's prediction.That is, we What is the difference between n1 and the product of n2 and n3? n1*(n2-n3)</p>
<p>How much is n1 times the difference between n2 and n3?The results reported in Figure 10 show an increase in the desired change in prediction at layers 19-20, while the undesired change in prediction is higher for layers 14-17.This means that interventions on the MLPs at layers 19-20 are more likely to lead to a correct adjustment of the prediction, while the opposite is true for earlier layers (14-15 in particular).This finding is consistent with our previous observations and we see this as additional evidence highlighting the influence of the MLPs at layers 19-20 on the prediction of r.</p>
<p>F Fine-tuning Details</p>
<p>We fine-tune Pythia 2.8B on three-operand queries.We train the model for 2 epochs on a set of queries obtained by sampling 1000 triples of operands for each of the 29 templates.We use Adafactor (Shazeer and Stern, 2018) a learning rate of 10 −5 , linearly decaying, and a batch size of 8. We make sure that there is no overlap between the set of</p>
<p>G Computing Infrastructure</p>
<p>The experiments for all models are carried out using a single Nvidia A100 GPU with 80GB of memory.</p>
<p>Figure 1 :
1
Figure1: Visualization of our findings.We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A).Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B).Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).</p>
<p>Figure 2 :
2
Figure2: By intervening on the activation values of specific components within a language model and computing the corresponding effects, we identify the subset of parameters responsible for specific predictions.</p>
<ol>
<li>We measure the causal effect of the intervention on variables m (l) t and a (l) t on the model's prediction by computing the change in the probability values assigned to the results r and r ′ .</li>
</ol>
<p>Figure 3 :
3
Figure 3: Indirect effect (IE) measured within GPT-J.Figures (a) and (b) illustrate the flow of information related to both the operands and the result of the queries, while the effect displayed in Figures (c) and (d) is related to the operands only (the result is kept unchanged).Figures(e-h) show a re-scaled visualization of the effects at the last token for each of the four heatmaps (a-d).The difference in the effect registered for the MLPs at layers 15-25 between figures (a) and (c) illustrates the role of these components in producing result-related information.</p>
<p>Figure 4 :
4
Figure 4: Indirect effect (IE) on three-operand queries for different MLP modules in Pythia 2.8B before and after fine-tuning.The effect produced by the last-token mid-late MLP activation site emerges with fine-tuning.Results for the attention are reported in Appendix J.</p>
<p>Figure 5 :
5
Figure 5: Indirect effect measured on the MLPs of GPT-J for predictions on the number retrieval task.</p>
<p>Figure 6 :
6
Figure 6: Indirect effect measured on the MLPs of GPT-J for predictions to factual queries.</p>
<p>Figure 7 :
7
Figure 7: Overlap ratio in the top 400 neurons with the largest effect on predicting answers to factual queries involving Arabic Numerals (Ar) and numeral words (W), number retrieval (NR), and factual knowledge (F).The results are obtained with GPT-J.</p>
<p>Figure 8 :
8
Figure 8: effect (IE) measured in GPT-J when varying the word describing the operator involved in the input query.Similar to the operands case, we observe a high contribution produced by middle-to-late MLP modules at the end of the input sequence.</p>
<p>Figure 9 :
9
Figure9: Indirect effect of the MLPs at the last token in each layer in GPT-J, for each of the four arithmetic operators.We observe a peak in the effect at layer 19 for all four types of operation.</p>
<p>x∈S P * (x) = r) and undesired (arg max x∈S P(x) = r) changes.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Desired (wrong to correct) and undesired (correct to wrong) change in the prediction induced by the intervention on the MLPs in GPT-J.The layers at which the two types of prediction change peak correspond to the layers with the largest corresponding IE.</p>
<p>Figure 12 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :
121516171819
Figure 12: Indirect effect (IE) measured for the attention modules in GPT-J on factual knowledge queries.</p>
<p>Table 1 :
1
Relative importance (RI) measurements for the last-token late MLP activation site.The decrease in the RI observed when fixing the result of the two pairs of operands used for the interventions quantitatively confirms the role of this subset of the model in incorporating result-related information.
|N | ModelRI(M late −1 )RI(M late −1 ) Result FixedGPT-J40.2%4.4%Pythia 2.8B43.2%5.8%2LLaMA 7B36.1%7.5%Goat33.5%7.4%GPT-J (Words)27.8%4.5%3Pythia 2.8B Pythia 2.8B (FT)13.5% 24.7%6.7% 13.6%</p>
<p>Table 2 :
2
Question templates for two-operand arithmetic queries.
Typeadditionsubtraction1Q: How much is n1 plus n2? A:Q: How much is n1 minus n2? A:2Q: What is n1 plus n2? A:Q: What is n1 minus n2? A:3Q: What is the result of n1 plus n2? A:Q: What is the result of n1 minus n2?3Q: What is the sum of n1 and n2? A:Q: What is the difference between A: n1 and n2? A:5The sum of n1 and n2 isThe difference between n1 and n2 is6n1 + n2 =n1 -n2 =multiplicationdivision1Q: How much is n1 times n2? A:Q: How much is n1 over n2? A:2Q: What is n1 times n2? A:Q: What is n1 over n2? A:3Q: What is the result of n1 times n2? A: Q: What is the result of n1 over n2? A:4Q: What is the product of n1 and n2? A: Q: What the ratio between n1 and n2? A:5The product of n1 and n2 isThe ratio of n1 and n2 is6n1 * n2 =n1 / n2 =FormulaTemplate(n1+n2)<em>n3 Sum n1 and n2 and multiply by n3n1+n2</em>n3What is the sum of n1 and the product of n2 and n3?(n1-n2)<em>n3What is the product of n1 minus n2 and n3?n1/(n2/n3)How much is n1 divided by the ratio between n2 and n3?n1-n2</em>n3</p>
<p>Table 3 :
3
Karpas et al. (2022)s of three-operand queries.For the full list, we refer toKarpas et al. (2022).
ModelOperation Accuracy (%)+69.3−78.0GPT-J×82.8÷40.8Overall67.8+95.5−86.7GPT-J (Numeral Words)×83.3÷59.7Overall81.3+57.4−77.5Pythia 2.8B×64.7÷40.2Overall59.9+100.0−99.8LLaMA×100.0÷88.7Overall97.2+100.0−100.0Goat×91.4÷54.0Overall85.6Pythia 2.8B (3 Operands)Overall0.9Pythia 2.8B Fine-tuned (3 Operands)Overall39.7</p>
<p>Table 4 :
4
Accuracy of the models analyzed in the paper on various types of arithmetic queries.</p>
<p>For brevity, layer normalization (Ba et al., 2016) is omitted as it is not essential for our analysis.
As an alternative metric to quantify the IE, we experiment using the difference in log probabilities (Appendix I). The results obtained with the two metrics show consistency and lead to the same conclusions.
Unless otherwise specified, we use S = {1, 2, . . . , 300}, as larger integers get split into multiple tokens by the tokenizer.
The LLaMA tokenizer considers each digit as an independent token in the vocabulary. This makes it problematic to compare the probability value assigned by the model to multi-digit numbers. Therefore, we restrict the set of possible results to the set of single-digit numbers.
We sample entities from a list containing names of animals, fruits, office tools, and other everyday items and objects.
To have the same result space for all the arithmetic queries (Ar and NW) and for the number retrieval task, we restrict the set S to {1, . . . , 20} (or the corresponding numeral words).
AcknowledgmentsAS is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship.YB is supported by an AI Alignment grant from Open Philanthropy, the Israel Science Foundation (grant No. 448/20), and an Azrieli Foundation Early Career Faculty Fellowship.MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung, and an ETH Grant (ETH-19 21-1).We are grateful to Vilém Zouhar and Neel Nanda for the insightful discussions.I Log Probability to Quantify the IEIn order to validate whether the measurements of the indirect effect are specific to the metric that we describe in Equation 2, we quantify the IE using the absolute difference in the log of the probability values assigned by the model to the results r and r ′ .More formally, we computewhereThe results are reported in Figure13.The activation sites that we observe are the same as reported in Section 4.1: first-layer MLP at the operand tokens and last-token MLP and attention modules.J Additional Information Flow VisualizationsWe include the IE measurements for the attention modules of GPT-J on the number retrieval task (Figure11) and on the factual knowledge queries (Figure12), and for Pythia 2.8B on three-operand arithmetic queries before and after fine-tuning (Figure15).Additionally, we report the heatmap visualizations of the indirect effect measured for the following models: Pythia 2.8B (Figure16), LLaMA 7B (Figure17), Goat (Figure18), and GPT-J using word numerals (Figure19).Finally, we visualize in Figure14the IE of MLPs and attention modules for the fine-tuned Pythia 2.8B in the fixed-result case.
On the pitfalls of analyzing individual neurons in language models. Omer Antverg, Yonatan Belinkov, International Conference on Learning Representations. 2022</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. 2016arXiv preprint</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, arXiv:2304.01373Pythia: A suite for analyzing large language models across training and scaling. 2023arXiv preprint</p>
<p>Language models can explain neurons in language models. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, William Saunders, 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Alexander Yom Din, Taelin Karidi, Leshem Choshen, Mor Geva, arXiv:2303.09435Jump to conclusions: Short-Cutting transformers with linear transformations. 2023arXiv preprint</p>
<p>. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread</p>
<p>Causal analysis of syntactic agreement mechanisms in neural language models. Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, Yonatan Belinkov, 10.18653/v1/2021.acl-long.144Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Causal abstractions of neural networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, Advances in Neural Information Processing Systems. 202134</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, arXiv:2304.147672023arXiv preprint</p>
<p>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Mor Geva, Avi Caciularu, Kevin Wang, Yoav Goldberg, 10.18653/v1/2022.emnlp-main.3Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Transformer feed-forward layers are keyvalue memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, 10.18653/v1/2021.emnlp-main.446Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks. Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Distill. 63e30</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, arXiv:2305.016102023arXiv preprint</p>
<p>MRKL systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>Andrej Karpathy, Justin Johnson, Li Fei-Fei, arXiv:1506.02078Visualizing and understanding recurrent networks. 2015arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Inference-time intervention: Eliciting truthful answers from a language model. 2023</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 202235</p>
<p>NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan, 10.18653/v1/2022.acl-long.246Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, Jacob Steinhardt, arXiv:2301.05217Progress measures for grokking via mechanistic interpretability. 2023arXiv preprint</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, 10.23915/distill.00024.0012020</p>
<p>. Chris Olah, Alexander Mordvintsev, Ludwig Schubert, Feature visualization. Distill. 211e72017</p>
<p>The building blocks of interpretability. Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, Alexander Mordvintsev, Distill. 33e102018</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2022. In-context learning and induction heads. Transformer Circuits Thread</p>
<p>GPT-4 technical report. 2023OpenAI</p>
<p>Investigating numeracy learning ability of a text-to-text transfer model. Kuntal Kumar, Pal , Chitta Baral, 10.18653/v1/2021.findings-emnlp.265Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Direct and indirect effects. Judea Pearl, UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington. Seattle, Washington, USAMorgan Kaufmann2001. August 2-5, 2001</p>
<p>Causality. Judea Pearl, 2009Cambridge University Press</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. Piotr Piękos, Mateusz Malinowski, Henryk Michalewski, 10.18653/v1/2021.acl-short.49Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell, 2023</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, International Conference on Machine Learning. PMLR2018</p>
<p>Masked measurement prediction: Learning to jointly predict quantities and units from textual context. Daniel Spokoyny, Ivan Lee, Jin Zhao, Taylor Berg-Kirkpatrick, 10.18653/v1/2022.findings-naacl.2Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231Long Papers)</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, 2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in Neural Information Processing Systems. 202033</p>
<p>Chelsea Voss, Nick Cammarata, Gabriel Goh, Michael Petrov, Ludwig Schubert, Ben Egan, Swee Kiat Lim, and Chris Olah. 2021. Visualizing weights. 6</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 billion parameter autoregressive language model. 2021</p>
<p>Interpretability in the wild: A circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, NeurIPS ML Safety Workshop. 2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022b</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>We sample pairs of subject entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of"). For each relation. we sample 100 pairs of subject entities</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2468 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2468</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2468</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-270380311</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06769v1.pdf" target="_blank">D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents</a></p>
                <p><strong>Paper Abstract:</strong> Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent’s capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce D ISCOV - ERY W ORLD , the first virtual environment for developing and benchmarking an agent’s ability to perform complete cycles of novel scientific discovery. D ISCOV - ERY W ORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. D ISCOVERY W ORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. D ISCOVERY W ORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most D ISCOVERY W ORLD tasks, suggesting that D ISCOVERY W ORLD captures some of the novel challenges of discovery, and thus that D ISCOVERY W ORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at github.com/allenai/discoveryworld. 1</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2468.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2468.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DISCOVERYWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DISCOVERYWORLD (virtual discovery environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based (optional 2D visual) simulated environment designed to develop and benchmark agents' end-to-end scientific discovery skills across diverse, parametrically generated tasks that require ideation, hypothesis formation, experiment design, execution, analysis, and conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DISCOVERYWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A custom PYTHON/PGAME simulator and API (OpenAI Gym-like) that instantiates 24 template task types across 8 thematic domains with three difficulty levels and parametric seeds (120 benchmark tasks). Provides a discrete action space (14 actions), textual/JSON and optional visual observations, simulated instruments and measurable object properties (60+ properties), an automatic task scorer, and three automatic evaluation metrics (binary task completion, procedural report-card of task-relevant actions, and discovered explanatory knowledge compared to gold). Intended to host and evaluate automated discovery agents that generate hypotheses, run experiments in-silico, analyze results, and produce explanatory knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Virtual environment for Automated Discovery System development and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Meta-research / AI for scientific discovery; benchmark spans simulated proteomics, chemistry, archaeology (radioisotope dating), reactor tuning, plant nutrient inference, epidemiology-style investigation (Space Sick), rocket science (planetary measurement and rocketry), and translation/grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provides long-horizon discovery tasks where agents must (zero-shot) ideate hypotheses, design experiments using simulated instruments, collect and analyze data, iterate experiments, and submit explanatory conclusions (e.g., identify an outlier species by proteomics clustering, validate radioisotope dating channels, fit regression models linking crystal properties to resonance frequency, infer logical nutrient rules, identify food contamination and mitigation, measure planetary radius/gravity and compute launch parameters, infer word meanings by grounding). Each task is parametrically instantiated (5 seeds per theme/difficulty) to produce diverse instances.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: tasks are long-horizon (easy up to 100 steps, normal/challenge up to 1000 steps or capped), multi-step and multi-modal (navigation + instrument measurement + analysis + planning), have parametric variability (120 unique instances), include multi-dimensional problems (e.g., 3D clustering in Proteomics challenge; quadratic regression in Reactor Lab challenge), distractors (irrelevant observations), and require ideation rather than a prescribed solution path.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated/generated on demand by the environment; parametric data accompany each instance (manufactured protein concentrations, instrument readings, artifact radioisotope values, crystal properties, etc.). No external real-world data required; data quality is deterministic and controlled by task generator seeds. The benchmark uses 5 seeds per task instance (total 120 tasks) for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Depends on agent: baseline LLM-driven agents report substantial compute and token usage. Paper upper-bound for full benchmark: ~84,000 environment steps (40 easy ×100 + 40 normal ×1000 + 40 challenge ×1000). Typical LLM-based agent costs reported: $3k–$10k to run the 120-task benchmark (approximate totals reported per agent class; see baselines). Long episodes require many LLM calls (often ≥1 call per environment step, plus extra calls for memory summarization), and token limits affect prompts (trajectory trimming necessary).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and realistic (but simplified) scientific tasks; continuous and discrete aspects (continuous measurements, continuous parameter fitting, and discrete actions/navigation). Deterministic simulation given seed, but tasks are not prescriptive — agents must generate hypotheses. Clear automatic evaluation metrics exist (binary completion, procedural action report, and discovered knowledge questions). Requires domain knowledge, statistical/data-analysis, planning, and iterative experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Three-part automatic scoring: (1) binary task completion (solved/unsolved), (2) task process/procedural report-card (counts of task-relevant actions and subtasks completed), and (3) discovered explanatory knowledge accuracy against gold critical questions (binary per question, can be auto-graded via LLM or graded by humans).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not applicable to environment itself. Used to evaluate agents; baseline agent results reported below (see agents entries).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not a system that 'fails' itself, but environment limitations noted: low-fidelity compared to physical world (abstracted actions), which may limit direct transfer to real-world discovery; long episode length and token limits pose challenges to agent architectures; high compute/cost required for LLM-driven agents.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Parametric variability, realistic scientific primitives (instruments, measurable properties), explicit evaluation metrics, and a broad coverage of domains encourage development of generalizable discovery skills. The controlled generation of data allows reproducible evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Environment was used to show a large gap between human scientists and current LLM-driven agents: humans solved 66% average task completion vs. much lower agent performance (see agent entries). Agents performed markedly better on unit-test style tasks (60%+ completion) but struggled on full discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Recruited human scientists (n=11, MSc/PhD in natural sciences) achieved average task completion rate ≈66% across normal and challenge tasks, with average discovered-knowledge score ≈55% (manual grading).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2468.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct agent (thought + action loop with LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven baseline that alternates between generating explicit 'thought' traces and environment actions (ReAct paradigm) at every step, executing actions, and appending observations to its trajectory; used zero-shot to attempt DISCOVERYWORLD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct agent (GPT-4O backed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each environment step, the LLM (GPT-4O) is prompted to generate a thought and a JSON-formatted action given the recent trajectory (thought, action, observation tuples) plus current game state; the action is executed and the resulting observation appended. Trajectories are trimmed (oldest steps removed) to fit token limits (typically up to last 40 steps). Discovered knowledge is evaluated by concatenating the agent's 'thought' outputs. Uses two navigation-handicap teleport actions to compensate for poor navigation ability.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / LLM-driven Automated Discovery Agent (Thought-Action Loop)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Applied across DISCOVERYWORLD's simulated scientific domains: proteomics, chemistry, archaeology, reactor lab, plant nutrients, space illness investigation, rocket science, translation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Zero-shot attempt to perform full end-to-end discovery tasks: form hypotheses, design/execute experiments using simulated instruments, analyze readings, and produce explanatory conclusions across the 120 benchmark tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Operates on long-horizon tasks (up to 1000 steps), multi-step experiment-design problems, and multi-dimensional analysis (e.g., regression, clustering). Trajectory length and distractors increase complexity; token limits impose truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>All data is simulated by DISCOVERYWORLD at run-time; ReAct is zero-shot and receives only environment observations and state; no pretraining on DISCOVERYWORLD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Reported cost: approx $2–$4 per 100 steps for ReAct (Table 10). Full-benchmark cost estimate ~ $3,360 for 120 tasks (84k steps upper bound). Each environment step requires at least one LLM call; long trajectories increase token counts and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, partially observable, deterministic simulation per seed; discrete actions and continuous measurements. Clear metrics exist (completion, procedural score, explanatory knowledge) but require multi-turn reasoning and iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Benchmark's three metrics: task completion, procedural action report-card, and discovered explanatory knowledge accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Baseline results: ReAct completed on average 38% of easy tasks and 18% of challenge tasks (paper reports these figures in Discussion). On unit-test tasks, agents (including ReAct) achieve moderate performance (≈60%+ completion on many unit tests). Overall, ReAct performed better than random but substantially worse than human scientists on discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles with long-horizon planning and distractors in trajectories; tendency to be misled by irrelevant observations; token limit-related history trimming can remove critical context leading to errors; difficulty producing and validating explanatory discoveries (low discovered-knowledge scores relative to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Shorter tasks/unit tests and tasks with clear hill-climbing signals or direct instrument outputs (procedural measurement tasks) are easier; ReAct benefits from immediate feedback on actions and simpler, low-dimensional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to other baseline architectures, ReAct had the highest task completion on some settings (but still far below humans). It outperformed (in completion) other agents on some easy tasks, but had poorer explanatory-knowledge discovery than Hypothesizer. Quantitatively: ReAct completion 38% (easy) vs human 66% average; discovered-knowledge lower.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Humans: average completion ≈66% across normal/challenge tasks; ReAct substantially below this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2468.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan+Execute</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan-and-Execute agent (iterative decomposition + ReAct executor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM-based agent where a planner (LLM) proposes a short plan step (iterative decomposition), and a ReAct-style executor (same ReAct agent) executes each plan step independently to reduce distractors and trajectory length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plan+Execute (iterative decomposition with ReAct executor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LLM to iteratively generate one step of a multi-step plan (iterative decomposition). Each plan step is executed by the ReAct executor, which generates thought+action sequences for that single step (shorter trajectories). Execution results are fed back to the planner to generate the next plan step. Planner prompt includes prior plan steps and their success to adapt plans based on observations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Hierarchical LLM planning and execution agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Applied across the DISCOVERYWORLD domains (same as other baselines): proteomics, chemistry, archaeology, reactor lab, plant nutrients, space sickness, rocket science, translation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Zero-shot decomposition of discovery tasks into manageable plan steps; execution of each step via an LLM executor to attempt robust iterative scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Designed to mitigate trajectory length and distractors by decomposing complex long-horizon tasks into simpler sub-steps; still must handle uncertain measurements, experiment design, and hypothesis iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on simulated DISCOVERYWORLD data; zero-shot with no prior exposure to the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Reported cost per 100 steps in Table 10 approx $3–$4; overall full-benchmark estimate ≈ $3,360 (upper bound). Planner calls are additional LLM calls, but shorter executor trajectories yield fewer tokens per execution step.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, sequential, partially observable; benefits from structure when tasks naturally decompose into subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Benchmark's three metrics (completion, procedural, explanatory knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper reports Plan+Execute as one of the baselines with modest performance; specific averaged numeric comparisons are in Table 4 (not fully enumerated in the main text). Overall Plan+Execute performs worse than humans and struggles on discovery tasks though may reduce certain trajectory-related errors compared to vanilla ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Planner may propose infeasible steps; executor can still fail on low-level actions; decomposition does not guarantee correct hypotheses or analytical methods; still limited by LLM's reasoning and memory/truncation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Decomposition reduces distractors and can improve execution reliability for isolated substeps; helpful when tasks can be naturally broken into independent actions with clear checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Plan+Execute reduces distractor-related errors relative to ReAct by limiting trajectory length per subtask, but overall remains far behind human baseline on full discovery tasks. Exact numeric differences are provided in the paper's Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human average completion ≈66%; Plan+Execute substantially below human performance on full discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2468.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesizer agent (explicit hypothesis & measurement memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven agent architecture that maintains an explicit, structured working memory of hypotheses (with status and supporting evidence) and measurements, updating this memory after each action and using it to guide action selection and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesizer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Keeps a science-themed working memory with two record types: HYPOTHESES (statement, status: confirmed/rejected/pending, supporting evidence) and MEASUREMENTS (specific observations). At each step it selects an action conditioned on working memory, a short plan, and a running hypothesis; after acting it reflects and updates memory. Memory is summarized every 10 actions to control size. When producing actions the agent also outputs natural-language explanations linking actions to hypotheses. Output is a JSON dictionary. Implementation required more LLM calls (act-then-reflect cycle doubles calls per step).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Hypothesis-driven Automated Discovery Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Applied to DISCOVERYWORLD tasks across all thematic domains (proteomics, chemistry, archaeology, reactor lab, plant nutrients, space sick, rocket science, translation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Aims to explicitly track and evaluate hypotheses and measurements to improve discovery and to make discovered explanatory knowledge directly inspectable for automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High; designed to operate on long-horizon tasks but incurs higher compute due to act-then-reflect update cycle; memory can grow large and requires periodic summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates zero-shot on simulated environment data; memory stores observations produced by environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Highest among baselines: reported cost approx $10 per 100 steps; full-benchmark cost estimate ≈ $8,400 (Table 10). Requires ~2 LLM calls per environment step (action + reflection) plus periodic memory summarization LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, requires structured hypothesis formulation, evidence aggregation, and iterative testing; deterministic simulation per seed but with distractors; clear evaluation via discovered knowledge scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Same three-part benchmark metrics; discovered explanatory knowledge is directly evaluable because Hypothesizer stores explicit hypotheses and measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Hypothesizer discovered explanatory knowledge at rates reported: 34% of gold knowledge in easy tasks and 8% in challenge tasks (paper Discussion). Task completion rates lower than humans; it achieved relatively better discovered-knowledge scores on some easy tasks compared to other baselines but still low overall.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High computational and token cost; memory growth and summarization can lose fine-grained detail; still fails to discover necessary explanatory knowledge in most challenge tasks; may be slowed by repeated reflection calls; acts may be driven by incomplete hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Structured memory and explicit hypothesis tracking improves ability to produce machine-readable explanatory knowledge and can help when evidence is unambiguous and accumulates cleanly (e.g., clear instrument-output tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to ReAct and Plan+Execute, Hypothesizer produced higher discovered-knowledge scores on easy tasks (34% vs lower for others) but lower task completion on challenge tasks (8% discovered knowledge in challenge). Hypothesizer is more costly computationally.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Humans discovered explanatory knowledge at ~55% on average; Hypothesizer's discovered-knowledge rates are substantially lower (34% easy, 8% challenge).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2468.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIN (continually learning language agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continually learning language-agent architecture cited by the paper; described as maintaining causal relations but differing from Hypothesizer in update timing and memory content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLIN: A continually learning language agent for rapid task adaptation and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an architecture that keeps an explicit memory of relations (causal) and learns continually; CLIN's memory is updated only after running an entire task to completion (contrasted with Hypothesizer's per-action updates). The paper uses CLIN as architectural inspiration/comparison but does not run CLIN in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Continual learning language agent / Hypothesis-memory architecture</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General language-agent tasks and rapid task adaptation (as per the cited paper); here mentioned in relation to architectures for hypothesis/memory tracking in discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in this paper; referenced as an approach for maintaining knowledge across tasks and adapting quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Not explicitly applied; used as a conceptual comparison emphasizing CLIN's update-after-task memory vs Hypothesizer's per-action updates.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here; only architectural differences noted.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Not discussed here; CLIN's continual learning design is implied to aid rapid adaptation in other contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper contrasts CLIN with Hypothesizer: CLIN updates memory after whole task, stores causal relations; Hypothesizer updates per action and stores science-specific hypothesis and measurement records. No empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable / not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2468.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (protein structure prediction system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-profile automated system for protein structure prediction that achieved highly accurate protein structure predictions and is cited as an example of a successful (but task-specific) discovery system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of a successful automated discovery system in protein folding; characterized in the paper as a specialized system that bypasses the full discovery pipeline by searching within a pre-defined hypothesis or solution space to produce high-quality results for protein structure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Specialized Automated Discovery System / Domain-specific scientific ML system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Proteomics / protein folding and structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict protein 3D structures from sequences—highly specialized task requiring training on structural data and physics-inspired features. In the context of this paper, AlphaFold is cited only as a prior success story demonstrating automated achievement in a narrow, well-specified scientific problem.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High dimensional, requires complex modeling; but presented in the paper as a well-defined search over a pre-defined hypothesis space (i.e., constrained problem formulation). The paper does not provide quantitative complexity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not discussed in detail in this paper; AlphaFold in literature uses large structural databases. The paper only notes that such systems bypass broader discovery steps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined, largely supervised/predictive modeling problem with clear evaluation metrics (structure accuracy). Deterministic outputs for given inputs under the model.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided in this paper (referenced literature uses structural similarity metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; referenced as 'successful' in prior work. No numeric success rates reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper critiques such systems for bypassing full discovery cycles (ideation, experiment design) and being task-specific; thus they lack general end-to-end discovery behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Task specificity, large high-quality datasets, strong domain modeling, and constrained hypothesis/search space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as a success in a narrow domain contrasted with DISCOVERYWORLD's goal of testing general end-to-end discovery skill across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2468.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoseTTAFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoseTTAFold / RosettaFold (protein modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another high-performing system for biomolecular structure modeling cited as an example of domain-specific automated discovery success (protein structure prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalized biomolecular modeling and design with rosettafold all-atom</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RoseTTAFold (RosettaFold)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a successful specialized system for protein structure modeling; like AlphaFold, it focuses on a constrained scientific problem and produces state-of-the-art results in that domain, but does not implement the full scientific discovery pipeline (ideation, experimental design, in-lab execution).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Specialized Automated Discovery System / Biomolecular modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Proteomics / biomolecular modeling and design</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict and design biomolecular (protein) structures; within this paper mentioned as an example of automated systems that succeed on narrow, carefully formulated scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional modeling tasks, but problem is framed as constrained predictive modeling in prior work; paper does not quantify details here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined supervised prediction/design problem rather than open-ended discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same critique as AlphaFold: bypass full discovery pipeline and are task-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong domain-specific modeling and training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Used in Related Work to contrast narrow successes with the need for general, end-to-end discovery evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2468.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adam / Eve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adam and Eve (robotic/automated discovery systems in genetics/chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited earlier-generation automated systems that generate hypotheses, design experiments, and can execute them (often via robotics) in real laboratory environments for domains such as genetics and chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adam / Eve (robotic automated discovery systems)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as prior systems capable of hypothesis generation, experiment design, and execution in real-world lab settings (robot scientists). The paper notes these systems operate in real environments, which is expensive and encourages task-specific engineering rather than broad discovery-skill development.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Genetics / chemistry (as cited); functional genomic hypothesis generation and wet-lab experimentation in literature historically (Adam/Eve examples).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate and test hypotheses in laboratory settings (e.g., functional genomics experiments). In the context of this paper they are examples of systems that implement full experimental cycles but are costly and domain-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Real-world laboratory experiments (complex, noisy, expensive), involve physical experiment constraints; the paper highlights the high resource cost and domain specificity but does not provide quantitative complexity measures here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Real experimental data produced in lab runs; data collection is costly and time-consuming in the real systems (motivation for the simulated DISCOVERYWORLD environment).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantified in this paper; real-world robotics and lab automation incur substantial monetary and time costs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined within domain but open-ended in terms of hypothesis space; constrained by available lab protocols and instruments. Clear experimental metrics but limited scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not specified in this paper; literature demonstrates domain-specific discoveries and validated experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Expensive to run; tends to incentivize narrow, task-specific solutions rather than general discovery skills; logistical complexity and cost limit broad experimentation and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to execute real experiments and test hypotheses in situ yields strong validation when successful; robotics + automated design cycles accelerate iteration in their narrow domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper contrasts these real-world systems with DISCOVERYWORLD's simulated approach that lowers cost and encourages general skill development.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2468.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoScientist / ChemCrow (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoScientist and ChemCrow (automated/LLM-augmented chemistry systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as recent systems in chemistry that can generate hypotheses and assist experimental design or execution; characterized as expensive and task-specific when operating in real environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoScientist / ChemCrow (chemistry-focused automated systems)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as examples from recent literature that perform automated discovery-support roles (e.g., hypothesis generation, experiment planning) in chemistry; the paper treats these as domain-specific successes that do not address general end-to-end discovery across many scientific topics.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / LLM-augmented chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry / automated chemical experimentation and planning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Assist or automate chemical discovery tasks (e.g., reaction design, experiment planning); in this paper referenced to motivate the need for a broad simulated benchmark rather than domain-specific, costly real-world systems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Real chemistry experiments include continuous variables (concentrations, temperatures) and combinatorial search spaces; precise complexity not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Real experiments or curated chemical datasets in original literature; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Domain-specific, often well-defined optimization or search tasks (e.g., find concentration mixtures) with real-world experimental noise and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Expensive to operate in real labs; narrow scope; may require substantial instrumentation and lab automation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Domain specificity, physical experimentation capability, and integration of modeling tools yield practical results in chemistry when scaled appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper uses these as examples of specialized successes contrasted with the general, low-cost, varied-task aims of DISCOVERYWORLD.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2468.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2468.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical 'robot scientist' example cited to illustrate prior work that automated hypothesis generation and wet-lab experimentation (functional genomics) in real laboratory settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of an automated system that generated hypotheses and executed experiments in real genomic wet-lab settings; highlighted as early work demonstrating a full experimental cycle but limited by cost and domain specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Experimentation Platform / Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Functional genomics / experimental biology</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated generation and empirical testing of genomic hypotheses in wet-lab experiments; referenced here to illustrate prior systems that attempted complete discovery cycles in real labs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to real-world laboratory workflows, noisy biological data, and experimental constraints; not quantified in current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on wet-lab experimental data generated during runs; costly to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantified in this paper; significant real-world lab and control system resources required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended hypothesis generation and testing in noisy biological experiments with clear experimental evaluation but high cost per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Expensive and time-consuming to operate; domain-specific limitations and logistical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct execution of experiments and closed-loop hypothesis testing gives strong empirical validation when successful.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Used to motivate DISCOVERYWORLD's simulated, lower-cost alternative that enables broader benchmarking and development of general discovery skills.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Highly accurate protein structure prediction with alphafold <em>(Rating: 2)</em></li>
                <li>Generalized biomolecular modeling and design with rosettafold all-atom <em>(Rating: 2)</em></li>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist <em>(Rating: 2)</em></li>
                <li>CLIN: A continually learning language agent for rapid task adaptation and generalization <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 1)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>CoScientist <em>(Rating: 1)</em></li>
                <li>ChemCrow <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2468",
    "paper_id": "paper-270380311",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "DISCOVERYWORLD",
            "name_full": "DISCOVERYWORLD (virtual discovery environment)",
            "brief_description": "A text-based (optional 2D visual) simulated environment designed to develop and benchmark agents' end-to-end scientific discovery skills across diverse, parametrically generated tasks that require ideation, hypothesis formation, experiment design, execution, analysis, and conclusion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DISCOVERYWORLD",
            "system_description": "A custom PYTHON/PGAME simulator and API (OpenAI Gym-like) that instantiates 24 template task types across 8 thematic domains with three difficulty levels and parametric seeds (120 benchmark tasks). Provides a discrete action space (14 actions), textual/JSON and optional visual observations, simulated instruments and measurable object properties (60+ properties), an automatic task scorer, and three automatic evaluation metrics (binary task completion, procedural report-card of task-relevant actions, and discovered explanatory knowledge compared to gold). Intended to host and evaluate automated discovery agents that generate hypotheses, run experiments in-silico, analyze results, and produce explanatory knowledge.",
            "system_type": "Virtual environment for Automated Discovery System development and evaluation",
            "problem_domain": "Meta-research / AI for scientific discovery; benchmark spans simulated proteomics, chemistry, archaeology (radioisotope dating), reactor tuning, plant nutrient inference, epidemiology-style investigation (Space Sick), rocket science (planetary measurement and rocketry), and translation/grounding tasks.",
            "problem_description": "Provides long-horizon discovery tasks where agents must (zero-shot) ideate hypotheses, design experiments using simulated instruments, collect and analyze data, iterate experiments, and submit explanatory conclusions (e.g., identify an outlier species by proteomics clustering, validate radioisotope dating channels, fit regression models linking crystal properties to resonance frequency, infer logical nutrient rules, identify food contamination and mitigation, measure planetary radius/gravity and compute launch parameters, infer word meanings by grounding). Each task is parametrically instantiated (5 seeds per theme/difficulty) to produce diverse instances.",
            "problem_complexity": "High: tasks are long-horizon (easy up to 100 steps, normal/challenge up to 1000 steps or capped), multi-step and multi-modal (navigation + instrument measurement + analysis + planning), have parametric variability (120 unique instances), include multi-dimensional problems (e.g., 3D clustering in Proteomics challenge; quadratic regression in Reactor Lab challenge), distractors (irrelevant observations), and require ideation rather than a prescribed solution path.",
            "data_availability": "Simulated/generated on demand by the environment; parametric data accompany each instance (manufactured protein concentrations, instrument readings, artifact radioisotope values, crystal properties, etc.). No external real-world data required; data quality is deterministic and controlled by task generator seeds. The benchmark uses 5 seeds per task instance (total 120 tasks) for evaluation.",
            "computational_requirements": "Depends on agent: baseline LLM-driven agents report substantial compute and token usage. Paper upper-bound for full benchmark: ~84,000 environment steps (40 easy ×100 + 40 normal ×1000 + 40 challenge ×1000). Typical LLM-based agent costs reported: $3k–$10k to run the 120-task benchmark (approximate totals reported per agent class; see baselines). Long episodes require many LLM calls (often ≥1 call per environment step, plus extra calls for memory summarization), and token limits affect prompts (trajectory trimming necessary).",
            "problem_structure": "Open-ended and realistic (but simplified) scientific tasks; continuous and discrete aspects (continuous measurements, continuous parameter fitting, and discrete actions/navigation). Deterministic simulation given seed, but tasks are not prescriptive — agents must generate hypotheses. Clear automatic evaluation metrics exist (binary completion, procedural action report, and discovered knowledge questions). Requires domain knowledge, statistical/data-analysis, planning, and iterative experimentation.",
            "success_metric": "Three-part automatic scoring: (1) binary task completion (solved/unsolved), (2) task process/procedural report-card (counts of task-relevant actions and subtasks completed), and (3) discovered explanatory knowledge accuracy against gold critical questions (binary per question, can be auto-graded via LLM or graded by humans).",
            "success_rate": "Not applicable to environment itself. Used to evaluate agents; baseline agent results reported below (see agents entries).",
            "failure_modes": "Not a system that 'fails' itself, but environment limitations noted: low-fidelity compared to physical world (abstracted actions), which may limit direct transfer to real-world discovery; long episode length and token limits pose challenges to agent architectures; high compute/cost required for LLM-driven agents.",
            "success_factors": "Parametric variability, realistic scientific primitives (instruments, measurable properties), explicit evaluation metrics, and a broad coverage of domains encourage development of generalizable discovery skills. The controlled generation of data allows reproducible evaluation.",
            "comparative_results": "Environment was used to show a large gap between human scientists and current LLM-driven agents: humans solved 66% average task completion vs. much lower agent performance (see agent entries). Agents performed markedly better on unit-test style tasks (60%+ completion) but struggled on full discovery tasks.",
            "human_baseline": "Recruited human scientists (n=11, MSc/PhD in natural sciences) achieved average task completion rate ≈66% across normal and challenge tasks, with average discovered-knowledge score ≈55% (manual grading).",
            "uuid": "e2468.0",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct agent (thought + action loop with LLM)",
            "brief_description": "An LLM-driven baseline that alternates between generating explicit 'thought' traces and environment actions (ReAct paradigm) at every step, executing actions, and appending observations to its trajectory; used zero-shot to attempt DISCOVERYWORLD tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct agent (GPT-4O backed)",
            "system_description": "At each environment step, the LLM (GPT-4O) is prompted to generate a thought and a JSON-formatted action given the recent trajectory (thought, action, observation tuples) plus current game state; the action is executed and the resulting observation appended. Trajectories are trimmed (oldest steps removed) to fit token limits (typically up to last 40 steps). Discovered knowledge is evaluated by concatenating the agent's 'thought' outputs. Uses two navigation-handicap teleport actions to compensate for poor navigation ability.",
            "system_type": "AI Scientist / LLM-driven Automated Discovery Agent (Thought-Action Loop)",
            "problem_domain": "Applied across DISCOVERYWORLD's simulated scientific domains: proteomics, chemistry, archaeology, reactor lab, plant nutrients, space illness investigation, rocket science, translation.",
            "problem_description": "Zero-shot attempt to perform full end-to-end discovery tasks: form hypotheses, design/execute experiments using simulated instruments, analyze readings, and produce explanatory conclusions across the 120 benchmark tasks.",
            "problem_complexity": "Operates on long-horizon tasks (up to 1000 steps), multi-step experiment-design problems, and multi-dimensional analysis (e.g., regression, clustering). Trajectory length and distractors increase complexity; token limits impose truncation.",
            "data_availability": "All data is simulated by DISCOVERYWORLD at run-time; ReAct is zero-shot and receives only environment observations and state; no pretraining on DISCOVERYWORLD tasks.",
            "computational_requirements": "Reported cost: approx $2–$4 per 100 steps for ReAct (Table 10). Full-benchmark cost estimate ~ $3,360 for 120 tasks (84k steps upper bound). Each environment step requires at least one LLM call; long trajectories increase token counts and cost.",
            "problem_structure": "Open-ended, partially observable, deterministic simulation per seed; discrete actions and continuous measurements. Clear metrics exist (completion, procedural score, explanatory knowledge) but require multi-turn reasoning and iteration.",
            "success_metric": "Benchmark's three metrics: task completion, procedural action report-card, and discovered explanatory knowledge accuracy.",
            "success_rate": "Baseline results: ReAct completed on average 38% of easy tasks and 18% of challenge tasks (paper reports these figures in Discussion). On unit-test tasks, agents (including ReAct) achieve moderate performance (≈60%+ completion on many unit tests). Overall, ReAct performed better than random but substantially worse than human scientists on discovery tasks.",
            "failure_modes": "Struggles with long-horizon planning and distractors in trajectories; tendency to be misled by irrelevant observations; token limit-related history trimming can remove critical context leading to errors; difficulty producing and validating explanatory discoveries (low discovered-knowledge scores relative to humans).",
            "success_factors": "Shorter tasks/unit tests and tasks with clear hill-climbing signals or direct instrument outputs (procedural measurement tasks) are easier; ReAct benefits from immediate feedback on actions and simpler, low-dimensional problems.",
            "comparative_results": "Compared to other baseline architectures, ReAct had the highest task completion on some settings (but still far below humans). It outperformed (in completion) other agents on some easy tasks, but had poorer explanatory-knowledge discovery than Hypothesizer. Quantitatively: ReAct completion 38% (easy) vs human 66% average; discovered-knowledge lower.",
            "human_baseline": "Humans: average completion ≈66% across normal/challenge tasks; ReAct substantially below this.",
            "uuid": "e2468.1",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Plan+Execute",
            "name_full": "Plan-and-Execute agent (iterative decomposition + ReAct executor)",
            "brief_description": "A two-stage LLM-based agent where a planner (LLM) proposes a short plan step (iterative decomposition), and a ReAct-style executor (same ReAct agent) executes each plan step independently to reduce distractors and trajectory length.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Plan+Execute (iterative decomposition with ReAct executor)",
            "system_description": "Uses LLM to iteratively generate one step of a multi-step plan (iterative decomposition). Each plan step is executed by the ReAct executor, which generates thought+action sequences for that single step (shorter trajectories). Execution results are fed back to the planner to generate the next plan step. Planner prompt includes prior plan steps and their success to adapt plans based on observations.",
            "system_type": "AI Scientist / Hierarchical LLM planning and execution agent",
            "problem_domain": "Applied across the DISCOVERYWORLD domains (same as other baselines): proteomics, chemistry, archaeology, reactor lab, plant nutrients, space sickness, rocket science, translation.",
            "problem_description": "Zero-shot decomposition of discovery tasks into manageable plan steps; execution of each step via an LLM executor to attempt robust iterative scientific discovery.",
            "problem_complexity": "Designed to mitigate trajectory length and distractors by decomposing complex long-horizon tasks into simpler sub-steps; still must handle uncertain measurements, experiment design, and hypothesis iteration.",
            "data_availability": "Operates on simulated DISCOVERYWORLD data; zero-shot with no prior exposure to the environment.",
            "computational_requirements": "Reported cost per 100 steps in Table 10 approx $3–$4; overall full-benchmark estimate ≈ $3,360 (upper bound). Planner calls are additional LLM calls, but shorter executor trajectories yield fewer tokens per execution step.",
            "problem_structure": "Open-ended, sequential, partially observable; benefits from structure when tasks naturally decompose into subgoals.",
            "success_metric": "Benchmark's three metrics (completion, procedural, explanatory knowledge).",
            "success_rate": "Paper reports Plan+Execute as one of the baselines with modest performance; specific averaged numeric comparisons are in Table 4 (not fully enumerated in the main text). Overall Plan+Execute performs worse than humans and struggles on discovery tasks though may reduce certain trajectory-related errors compared to vanilla ReAct.",
            "failure_modes": "Planner may propose infeasible steps; executor can still fail on low-level actions; decomposition does not guarantee correct hypotheses or analytical methods; still limited by LLM's reasoning and memory/truncation constraints.",
            "success_factors": "Decomposition reduces distractors and can improve execution reliability for isolated substeps; helpful when tasks can be naturally broken into independent actions with clear checks.",
            "comparative_results": "Plan+Execute reduces distractor-related errors relative to ReAct by limiting trajectory length per subtask, but overall remains far behind human baseline on full discovery tasks. Exact numeric differences are provided in the paper's Table 4.",
            "human_baseline": "Human average completion ≈66%; Plan+Execute substantially below human performance on full discovery tasks.",
            "uuid": "e2468.2",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Hypothesizer",
            "name_full": "Hypothesizer agent (explicit hypothesis & measurement memory)",
            "brief_description": "An LLM-driven agent architecture that maintains an explicit, structured working memory of hypotheses (with status and supporting evidence) and measurements, updating this memory after each action and using it to guide action selection and planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesizer",
            "system_description": "Keeps a science-themed working memory with two record types: HYPOTHESES (statement, status: confirmed/rejected/pending, supporting evidence) and MEASUREMENTS (specific observations). At each step it selects an action conditioned on working memory, a short plan, and a running hypothesis; after acting it reflects and updates memory. Memory is summarized every 10 actions to control size. When producing actions the agent also outputs natural-language explanations linking actions to hypotheses. Output is a JSON dictionary. Implementation required more LLM calls (act-then-reflect cycle doubles calls per step).",
            "system_type": "AI Scientist / Hypothesis-driven Automated Discovery Agent",
            "problem_domain": "Applied to DISCOVERYWORLD tasks across all thematic domains (proteomics, chemistry, archaeology, reactor lab, plant nutrients, space sick, rocket science, translation).",
            "problem_description": "Aims to explicitly track and evaluate hypotheses and measurements to improve discovery and to make discovered explanatory knowledge directly inspectable for automated scoring.",
            "problem_complexity": "High; designed to operate on long-horizon tasks but incurs higher compute due to act-then-reflect update cycle; memory can grow large and requires periodic summarization.",
            "data_availability": "Operates zero-shot on simulated environment data; memory stores observations produced by environment interactions.",
            "computational_requirements": "Highest among baselines: reported cost approx $10 per 100 steps; full-benchmark cost estimate ≈ $8,400 (Table 10). Requires ~2 LLM calls per environment step (action + reflection) plus periodic memory summarization LLM calls.",
            "problem_structure": "Open-ended, requires structured hypothesis formulation, evidence aggregation, and iterative testing; deterministic simulation per seed but with distractors; clear evaluation via discovered knowledge scorer.",
            "success_metric": "Same three-part benchmark metrics; discovered explanatory knowledge is directly evaluable because Hypothesizer stores explicit hypotheses and measurements.",
            "success_rate": "Hypothesizer discovered explanatory knowledge at rates reported: 34% of gold knowledge in easy tasks and 8% in challenge tasks (paper Discussion). Task completion rates lower than humans; it achieved relatively better discovered-knowledge scores on some easy tasks compared to other baselines but still low overall.",
            "failure_modes": "High computational and token cost; memory growth and summarization can lose fine-grained detail; still fails to discover necessary explanatory knowledge in most challenge tasks; may be slowed by repeated reflection calls; acts may be driven by incomplete hypotheses.",
            "success_factors": "Structured memory and explicit hypothesis tracking improves ability to produce machine-readable explanatory knowledge and can help when evidence is unambiguous and accumulates cleanly (e.g., clear instrument-output tasks).",
            "comparative_results": "Compared to ReAct and Plan+Execute, Hypothesizer produced higher discovered-knowledge scores on easy tasks (34% vs lower for others) but lower task completion on challenge tasks (8% discovered knowledge in challenge). Hypothesizer is more costly computationally.",
            "human_baseline": "Humans discovered explanatory knowledge at ~55% on average; Hypothesizer's discovered-knowledge rates are substantially lower (34% easy, 8% challenge).",
            "uuid": "e2468.3",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CLIN",
            "name_full": "CLIN (continually learning language agent)",
            "brief_description": "A continually learning language-agent architecture cited by the paper; described as maintaining causal relations but differing from Hypothesizer in update timing and memory content.",
            "citation_title": "CLIN: A continually learning language agent for rapid task adaptation and generalization",
            "mention_or_use": "mention",
            "system_name": "CLIN",
            "system_description": "Cited as an architecture that keeps an explicit memory of relations (causal) and learns continually; CLIN's memory is updated only after running an entire task to completion (contrasted with Hypothesizer's per-action updates). The paper uses CLIN as architectural inspiration/comparison but does not run CLIN in experiments.",
            "system_type": "Continual learning language agent / Hypothesis-memory architecture",
            "problem_domain": "General language-agent tasks and rapid task adaptation (as per the cited paper); here mentioned in relation to architectures for hypothesis/memory tracking in discovery agents.",
            "problem_description": "Not applied in this paper; referenced as an approach for maintaining knowledge across tasks and adapting quickly.",
            "problem_complexity": "Not specified in this paper.",
            "data_availability": "Not specified in this paper.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Not explicitly applied; used as a conceptual comparison emphasizing CLIN's update-after-task memory vs Hypothesizer's per-action updates.",
            "success_metric": "Not reported in this paper.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Not discussed here; only architectural differences noted.",
            "success_factors": "Not discussed here; CLIN's continual learning design is implied to aid rapid adaptation in other contexts.",
            "comparative_results": "Paper contrasts CLIN with Hypothesizer: CLIN updates memory after whole task, stores causal relations; Hypothesizer updates per action and stores science-specific hypothesis and measurement records. No empirical comparison provided in this paper.",
            "human_baseline": "Not applicable / not provided in this paper.",
            "uuid": "e2468.4",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (protein structure prediction system)",
            "brief_description": "A high-profile automated system for protein structure prediction that achieved highly accurate protein structure predictions and is cited as an example of a successful (but task-specific) discovery system.",
            "citation_title": "Highly accurate protein structure prediction with alphafold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "Cited as an example of a successful automated discovery system in protein folding; characterized in the paper as a specialized system that bypasses the full discovery pipeline by searching within a pre-defined hypothesis or solution space to produce high-quality results for protein structure prediction.",
            "system_type": "Specialized Automated Discovery System / Domain-specific scientific ML system",
            "problem_domain": "Proteomics / protein folding and structure prediction",
            "problem_description": "Predict protein 3D structures from sequences—highly specialized task requiring training on structural data and physics-inspired features. In the context of this paper, AlphaFold is cited only as a prior success story demonstrating automated achievement in a narrow, well-specified scientific problem.",
            "problem_complexity": "High dimensional, requires complex modeling; but presented in the paper as a well-defined search over a pre-defined hypothesis space (i.e., constrained problem formulation). The paper does not provide quantitative complexity measures.",
            "data_availability": "Not discussed in detail in this paper; AlphaFold in literature uses large structural databases. The paper only notes that such systems bypass broader discovery steps.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Well-defined, largely supervised/predictive modeling problem with clear evaluation metrics (structure accuracy). Deterministic outputs for given inputs under the model.",
            "success_metric": "Not provided in this paper (referenced literature uses structural similarity metrics).",
            "success_rate": "Not quantified in this paper; referenced as 'successful' in prior work. No numeric success rates reported here.",
            "failure_modes": "Paper critiques such systems for bypassing full discovery cycles (ideation, experiment design) and being task-specific; thus they lack general end-to-end discovery behavior.",
            "success_factors": "Task specificity, large high-quality datasets, strong domain modeling, and constrained hypothesis/search space.",
            "comparative_results": "Cited as a success in a narrow domain contrasted with DISCOVERYWORLD's goal of testing general end-to-end discovery skill across many domains.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2468.5",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RoseTTAFold",
            "name_full": "RoseTTAFold / RosettaFold (protein modeling)",
            "brief_description": "Another high-performing system for biomolecular structure modeling cited as an example of domain-specific automated discovery success (protein structure prediction).",
            "citation_title": "Generalized biomolecular modeling and design with rosettafold all-atom",
            "mention_or_use": "mention",
            "system_name": "RoseTTAFold (RosettaFold)",
            "system_description": "Cited as a successful specialized system for protein structure modeling; like AlphaFold, it focuses on a constrained scientific problem and produces state-of-the-art results in that domain, but does not implement the full scientific discovery pipeline (ideation, experimental design, in-lab execution).",
            "system_type": "Specialized Automated Discovery System / Biomolecular modeling",
            "problem_domain": "Proteomics / biomolecular modeling and design",
            "problem_description": "Predict and design biomolecular (protein) structures; within this paper mentioned as an example of automated systems that succeed on narrow, carefully formulated scientific problems.",
            "problem_complexity": "High-dimensional modeling tasks, but problem is framed as constrained predictive modeling in prior work; paper does not quantify details here.",
            "data_availability": "Not discussed in this paper.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Well-defined supervised prediction/design problem rather than open-ended discovery.",
            "success_metric": "Not specified in this paper.",
            "success_rate": "Not provided in this paper.",
            "failure_modes": "Same critique as AlphaFold: bypass full discovery pipeline and are task-specific.",
            "success_factors": "Strong domain-specific modeling and training data.",
            "comparative_results": "Used in Related Work to contrast narrow successes with the need for general, end-to-end discovery evaluation.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2468.6",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Adam / Eve",
            "name_full": "Adam and Eve (robotic/automated discovery systems in genetics/chemistry)",
            "brief_description": "Cited earlier-generation automated systems that generate hypotheses, design experiments, and can execute them (often via robotics) in real laboratory environments for domains such as genetics and chemistry.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Adam / Eve (robotic automated discovery systems)",
            "system_description": "Mentioned as prior systems capable of hypothesis generation, experiment design, and execution in real-world lab settings (robot scientists). The paper notes these systems operate in real environments, which is expensive and encourages task-specific engineering rather than broad discovery-skill development.",
            "system_type": "Automated Experimentation Platform / Robot Scientist",
            "problem_domain": "Genetics / chemistry (as cited); functional genomic hypothesis generation and wet-lab experimentation in literature historically (Adam/Eve examples).",
            "problem_description": "Generate and test hypotheses in laboratory settings (e.g., functional genomics experiments). In the context of this paper they are examples of systems that implement full experimental cycles but are costly and domain-specific.",
            "problem_complexity": "Real-world laboratory experiments (complex, noisy, expensive), involve physical experiment constraints; the paper highlights the high resource cost and domain specificity but does not provide quantitative complexity measures here.",
            "data_availability": "Real experimental data produced in lab runs; data collection is costly and time-consuming in the real systems (motivation for the simulated DISCOVERYWORLD environment).",
            "computational_requirements": "Not quantified in this paper; real-world robotics and lab automation incur substantial monetary and time costs.",
            "problem_structure": "Well-defined within domain but open-ended in terms of hypothesis space; constrained by available lab protocols and instruments. Clear experimental metrics but limited scalability.",
            "success_metric": "Not specified in this paper; literature demonstrates domain-specific discoveries and validated experimental outcomes.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Expensive to run; tends to incentivize narrow, task-specific solutions rather than general discovery skills; logistical complexity and cost limit broad experimentation and benchmarking.",
            "success_factors": "Ability to execute real experiments and test hypotheses in situ yields strong validation when successful; robotics + automated design cycles accelerate iteration in their narrow domains.",
            "comparative_results": "Paper contrasts these real-world systems with DISCOVERYWORLD's simulated approach that lowers cost and encourages general skill development.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2468.7",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoScientist / ChemCrow (mentioned)",
            "name_full": "CoScientist and ChemCrow (automated/LLM-augmented chemistry systems)",
            "brief_description": "Mentioned as recent systems in chemistry that can generate hypotheses and assist experimental design or execution; characterized as expensive and task-specific when operating in real environments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CoScientist / ChemCrow (chemistry-focused automated systems)",
            "system_description": "Cited as examples from recent literature that perform automated discovery-support roles (e.g., hypothesis generation, experiment planning) in chemistry; the paper treats these as domain-specific successes that do not address general end-to-end discovery across many scientific topics.",
            "system_type": "Automated Discovery System / LLM-augmented chemistry tools",
            "problem_domain": "Chemistry / automated chemical experimentation and planning",
            "problem_description": "Assist or automate chemical discovery tasks (e.g., reaction design, experiment planning); in this paper referenced to motivate the need for a broad simulated benchmark rather than domain-specific, costly real-world systems.",
            "problem_complexity": "Real chemistry experiments include continuous variables (concentrations, temperatures) and combinatorial search spaces; precise complexity not provided in this paper.",
            "data_availability": "Real experiments or curated chemical datasets in original literature; not detailed here.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Domain-specific, often well-defined optimization or search tasks (e.g., find concentration mixtures) with real-world experimental noise and cost.",
            "success_metric": "Not specified in this paper.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Expensive to operate in real labs; narrow scope; may require substantial instrumentation and lab automation.",
            "success_factors": "Domain specificity, physical experimentation capability, and integration of modeling tools yield practical results in chemistry when scaled appropriately.",
            "comparative_results": "Paper uses these as examples of specialized successes contrasted with the general, low-cost, varied-task aims of DISCOVERYWORLD.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2468.8",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Robot Scientist (King et al.)",
            "name_full": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "brief_description": "A classical 'robot scientist' example cited to illustrate prior work that automated hypothesis generation and wet-lab experimentation (functional genomics) in real laboratory settings.",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist (King et al.)",
            "system_description": "Cited as an example of an automated system that generated hypotheses and executed experiments in real genomic wet-lab settings; highlighted as early work demonstrating a full experimental cycle but limited by cost and domain specificity.",
            "system_type": "Automated Experimentation Platform / Robot Scientist",
            "problem_domain": "Functional genomics / experimental biology",
            "problem_description": "Automated generation and empirical testing of genomic hypotheses in wet-lab experiments; referenced here to illustrate prior systems that attempted complete discovery cycles in real labs.",
            "problem_complexity": "High due to real-world laboratory workflows, noisy biological data, and experimental constraints; not quantified in current paper.",
            "data_availability": "Relies on wet-lab experimental data generated during runs; costly to produce.",
            "computational_requirements": "Not quantified in this paper; significant real-world lab and control system resources required.",
            "problem_structure": "Open-ended hypothesis generation and testing in noisy biological experiments with clear experimental evaluation but high cost per experiment.",
            "success_metric": "Not provided in this paper.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Expensive and time-consuming to operate; domain-specific limitations and logistical constraints.",
            "success_factors": "Direct execution of experiments and closed-loop hypothesis testing gives strong empirical validation when successful.",
            "comparative_results": "Used to motivate DISCOVERYWORLD's simulated, lower-cost alternative that enables broader benchmarking and development of general discovery skills.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2468.9",
            "source_info": {
                "paper_title": "D ISCOVERY W ORLD : A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        },
        {
            "paper_title": "Generalized biomolecular modeling and design with rosettafold all-atom",
            "rating": 2,
            "sanitized_title": "generalized_biomolecular_modeling_and_design_with_rosettafold_allatom"
        },
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "rating": 2,
            "sanitized_title": "functional_genomic_hypothesis_generation_and_experimentation_by_a_robot_scientist"
        },
        {
            "paper_title": "CLIN: A continually learning language agent for rapid task adaptation and generalization",
            "rating": 2,
            "sanitized_title": "clin_a_continually_learning_language_agent_for_rapid_task_adaptation_and_generalization"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 1,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        },
        {
            "paper_title": "CoScientist",
            "rating": 1,
            "sanitized_title": "coscientist"
        },
        {
            "paper_title": "ChemCrow",
            "rating": 1
        }
    ],
    "cost": 0.022812749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents
7 Oct 2024</p>
<p>Peter Jansen peterj@allenai.org 
Allen Institute for Artificial Intelligence</p>
<p>Marc-Alexandre Côté 
Microsoft Research
University of Arizona</p>
<p>Tushar Khot 
Allen Institute for Artificial Intelligence</p>
<p>Erin Bransom 
Allen Institute for Artificial Intelligence</p>
<p>Dalvi Bhavana 
Mishra 
Allen Institute for Artificial Intelligence</p>
<p>Bodhisattwa Prasad Majumder 
Allen Institute for Artificial Intelligence</p>
<p>Oyvind Tafjord 
Allen Institute for Artificial Intelligence</p>
<p>Peter Clark 
Allen Institute for Artificial Intelligence</p>
<p>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents
7 Oct 20249C7A9C9BB963DA4B556DA3410A4F3E34arXiv:2406.06769v2[cs.AI]
Automated scientific discovery promises to accelerate progress across scientific domains.However, developing and evaluating an AI agent's capacity for endto-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible.In this work we introduce DISCOV-ERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery.DISCOV-ERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions.DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay).It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations.Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions.DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge.We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents.Code available at github.com/allenai/discoveryworld. 1 1 Released under Apache-2.0license.Preprint.Under review.</p>
<p>Introduction</p>
<p>A long-standing dream of AI has been to build systems that can perform scientific discovery, potentially leading to new breakthroughs for the benefit of humanity [13].Recently, with the rise of neural techniques, there have been several successful discovery systems developed for specialized problems such as protein folding [10,15], mathematics [22], and material science [24].However, while the results have been impressive, these systems (deliberately) bypass the full discovery process of ideation, hypothesis formation, experiment design, etc., and instead (expertly) perform systematic searches over a pre-defined hypothesis space, with pre-defined goals.This raises the question: how much more can be achieved if AI is applied to the broader scientific process?Some works have indeed developed early systems for this, for example, in chemistry [1], and genetics [12].These systems can also generate hypotheses, design experiments, and execute them (via robotics) in real environments.However, operating in real environments is expensive and complex, creating a barrier for entry.In addition, real environments inevitably encourage a focus on task-specific details, at the potential cost of developing more general discovery skills in an agent.</p>
<p>Our goal is to help remedy these by creating the first virtual discovery environment where solving tasks demands all of the key facets in end-to-end scientific discovery, 2 and which covers a broad variety of discovery topics.Our approach is to develop a text-based simulated world (with optional 2D visual overlay), called DISCOVERYWORLD, where agents can navigate around, interact with objects in the world, use scientific equipment (measuring devices, tools, etc.), and make observations.Agents can then form hypotheses, plan and execute experiments, and draw conclusions to solve challenge tasks developed for this virtual world.DISCOVERYWORLD tasks are grounded in eight varied topics, such as radioisotope dating, rocket science, and proteomics, to encourage development of agents with general discovery skills rather than hard-wiring to a particular challenge (see Figure 1).The tasks themselves are realistic (but simplified), allowing agents to apply both scientific and commonsense knowledge when attempting them.DISCOVERYWORLD thus provides an environment for exercising and evaluating general-purpose skills in end-to-end AI discovery systems (see Figure 2).DISCOVERYWORLD is inspired by a growing number of text-based simulation environments [5, 31, inter alia], while also being novel in both its environment and tasks:</p>
<p>• DISCOVERYWORLD tasks are long-horizon, requiring multiple facets of discovery including ideation, experimentation, systematic search, and analysis to be performed to solve a task.• The tasks do not suggest a solution approach, instead requiring the agent to ideate and define hypotheses to explore.This contrasts with tasks in many adventure game environments where solution approaches are often more stylistic or constrained.• DISCOVERYWORLD is realistic (but simplified) rather than counterfactual, so that background knowledge can be sensibly applied.• The tasks cover eight diverse topics, from identifying the cause of space illnesses to reactor tuning, to encourage development of general rather than task-specific solutions.</p>
<p>Finally, automatically evaluating an agent's progress on a discovery task is itself challenging.We devise a three-part evaluation strategy to help with this, more on this in Section 3.4.</p>
<p>Our contributions are thus:</p>
<p>Instruments &amp; Observations</p>
<p>Samples Figure 2: DISCOVERYWORLD tasks require end-to-end scientific discovery, from ideation, hypothesis formation, experiment design, data collection and analysis, forming conclusions, and acting on results.Distractors and task solutions that provide only descriptive discoveries require agents to frequently iterate hypotheses and experiments to reach full explanatory discoveries.</p>
<p>• We introduce the first virtual environment for benchmarking an agent's general ability to perform complete cycles of novel scientific discovery.• A comprehensive evaluation set of 120 different tasks for this environment, spanning eight diverse topics, each with three levels of difficulty and several parametric variations.• An evaluation framework allowing automatic evaluation of agents in DISCOVERYWORLD.</p>
<p>• Baseline results for agents in this environment, illustrating that DISCOVERYWORLD captures several novel challenges that contemporary agent models struggle with.</p>
<p>Together, we hope DISCOVERYWORLD will inspire and accelerate the development of new, general AI discovery agents by the community.</p>
<p>Related Work</p>
<p>Recently, several real-world discovery systems have shown success in areas such as genetics (Adam [12], Eve [32]), chemistry (CoScientist [1], ChemCrow [2]) and protenomics (AlphaFold [10], RoseTTAFold [15]).However, these systems are expensive and task-specific.Inspired by these, DISCOVERYWORLD aims to be a broad coverage, virtual environment allowing general scientific research skills to be developed and evaluated.There are many virtual environments that touch on aspects of the discovery process (see Table 1), although DISCOVERYWORLD is the first aimed at the full end-to-end pipeline:</p>
<p>Many physical simulation environments were developed (e.g., for robotics), focusing on object manipulation and navigation.Some are visual/spatial environments (e.g., AI2-Thor [14], ALFRED [25]), while others are textual/symbolic (e.g., TextWorld [5], MiniGrid [4], ALFWorld [26]).</p>
<p>Many game worlds require exploration and discovery, e.g., NetHack [16], MineDojo [6].However, these operate in counterfactual worlds where good scientific choices are not always rewarded, and are primarily aimed at entertainment.Some virtual environments (abstractly) cover real world tasks.WebArena [36] simulates Web-based activities, e.g., browsing for a phone-number, on-line shopping, aiming to improve an agent's taskspecific Web navigation skills.Perhaps closest to DISCOVERYWORLD is ScienceWorld [31], a text-based environment for solving simple science quests known to elementary science students, such as "convert a liquid into a solid" (e.g., put water in the freezer).ScienceWorld requires commonsense object manipulation at the level of an elementary science student, but not ideation or systematic search to complete tasks.In contrast, we show that DISCOVERYWORLD contains discovery tasks that are challenging even for human scientists with PHDS in the natural sciences.</p>
<p>Finally, some environments are explicitly designed to host hypothesis search.Alchemy [28] is a 3D video game to find which mixture of potions transforms a stone into a more valuable form.</p>
<p>Archaeology</p>
<p>Validate radioisotope dating by correlating radioisotope levels with known artifacts' ages, choosing the correct radioisotope between several alternatives for dating, then identify the oldest unknown artifact.</p>
<p>Reactor Lab Discover a relationship (linear or quadratic) between a physical crystal property (like temperature or density) and its resonance frequency through regression, and use this to tune and activate a reactor.</p>
<p>Plant Nutrients</p>
<p>Discover that plants on PLANET X prefer specific combinations of nutrients that follow logical rules (e.g.XOR, AND, OR, NOT), then grow plants by setting soil nutrient levels that follow those rules.</p>
<p>Space Sick</p>
<p>Investigate the cause of a mild and occasional colonist illness in response to eating local food, then formulate and implement a solution so that future colonists no longer contend with this illness.</p>
<p>Rocket Science</p>
<p>Measure a number of unknown planetary properties (such as the radius and mass of PLANET X), then use provided equations to calculate orbital velocity and propellant needs for a rocket launch.</p>
<p>Translation</p>
<p>Explore an environment to infer the meanings of words in an unknown language by grounding them to observations of specific objects and actions, then take actions based on the translated utterances.</p>
<p>Similarly in IVRE [33], users perform artificial category experiments to identify which blocks are "blickets".These environments exercise the systematic search part of the discovery pipeline.Likewise, MLAgentBench [8] requires software experiments to solve a challenge (improve a ML algorithm), but in the constrained environment of ML software.In contrast, DISCOVERYWORLD aims to cover a broad range of tasks in a (simulated) physical environment, covering the full end-to-end discovery pipeline.</p>
<p>3 DISCOVERYWORLD Simulator and Environments</p>
<p>Simulator</p>
<p>Engine: DISCOVERYWORLD is implemented in a custom simulation engine developed from the ground-up to enable building dynamic discovery simulations with a variety of object properties and simulated object behaviors.Every object in DISCOVERYWORLD is constructed from materials with measurable properties, many of which are observable with instruments available in the environment (a list of 60+ frequent object properties is provided in APPENDIX C.5).The simulator is implemented as approximately 20K lines of PYTHON using the PYGAME framework [21], and provides both an API for developing agents, as well as a graphical user interface for humans to play DISCOVERYWORLD tasks.The API resembles the OPENAI GYM specifications [3,27], such that at each step, the agent is provided with an observation from the environment, and must choose a single action to take during that turn from a set of possible actions.An automatic scorer runs in the background, and a given task continues until either it is solved, or the agent/human ends the simulation.</p>
<p>Environment Space: All current environments are represented as a 32 × 32 tile grid.As in text game simulators, all objects at a given tile are represented by an object tree [9], where the root node contains objects that are directly on the environment tile (such as a table), while child nodes of each object contain their contents (such as a jar on the table ).</p>
<p>Observations: Observations in DISCOVERYWORLD can be provided as text, visual output, or both.</p>
<p>Text observations are provided as JSON 3  .This provides information about objects that are farther away than the JSON observation, which is typically limited to a certain distance (configurable, default within 3 tiles) around the agent due to the size (in tokens) of the objects and their associated descriptions.</p>
<p>Action space: DISCOVERYWORLD includes 14 possible actions, most of which are common actions such as taking, dropping, or moving objects, opening/closing objects, activating/deactivating objects, using one object on another, and other actions found in other simulated environments [5,30,26].</p>
<p>Each action can take zero (e.g.wait), one (e.g.take seed), or two (e.g.use spectrometer on plant) arguments.A list of all actions is shown in APPENDIX C.1.For agents, we include two additional handicap actions to assist with their (generally) poor abilities for navigation: the ability to teleport to specific task-relevant named locations provided in each task (such as the science lab or cafeteria), and the ability to teleport directly beside any object it is currently (or has previously) observed.</p>
<p>Discovery Task Themes, Difficulty Levels, and Parametric Variations</p>
<p>Specific tasks in DISCOVERYWORLD are parametrically generated from a set of 24 high-level templates.These templates are divided into 8 discovery task themes with 3 levels of difficulty for each theme.A high-level description of the 8 discovery task themes is shown in Table 2, with full details (including spoilers) in APPENDIX C.3.For each of the 24 {theme × difficulty} combinations, the DISCOVERYWORLD simulator is capable of generating a large number of parametric versions of that theme that constitute a particular instance of a task.These parametric variations vary the specific task solution, which typically involves dynamically generating new values for specific object properties, and/or substituting in different task objects.For example, in the Proteomics theme, each parametric variation of a given task generates a different clustering problem, with different data, that points to a different animal as the solution, and places the animals at different starting locations within the environment.Parametric variation generation is deterministic and controlled by a single random seed provided to the task generator.While a large number of specific instantiations of each theme are possible, due to the cost associated with evaluating a large number of tasks, our official benchmark is evaluated on 5 seeds (i.e., random seeds zero through four) for each theme and difficulty level, resulting in a total of 8 × 3 × 5 = 120 different game instances or tasks.</p>
<p>Unit Tests</p>
<p>In addition to the 8 discovery themes, to help disentangle whether a given model's performance is due to a difficulty in completing normal day-to-day tasks within the environment, versus completing tasks requiring scientific discovery skills in particular, we include 10 additional unit test themes that test specific common task competencies.These include a combination of common pick-and-place and navigation tasks (such as those found in ALFWORLD [26], MINIGRID [4], and MINIHACK [23]), as well as DISCOVERYWORLD-themed tasks, such as measuring objects with instruments, or having multi-turn interactions with other agents.The unit test generator is also parametric and capable of generating a large number of specific tasks for each unit test theme.Specific unit test themes are described in detail in APPENDIX C.4.</p>
<p>Evaluation Metrics</p>
<p>To evaluate agents progress in DISCOVERYWORLD, we devised three automatic metrics: (1) task completion (a binary metric); (2) a fine-grained report card for each task tracking task-relevant actions, to measure partial performance on relevant discovery procedures; (3) the accuracy of discovered explanatory knowledge with respect to a gold reference.Together these allow an agent's progress (including partial progress) to be automatically assessed.An example scorecard is shown in Table 3.</p>
<p>Two of these three metrics (task completion, and task process) are measured automatically by DISCOVERYWORLD.For the third, discovered explanatory knowledge, the scorecard provides specific binary questions to answer with reference to knowledge that an agent has produced (in its explanations, memories, reports, or other knowledge structures).These can either be graded manually by a human, or provided to a language model to grade.DISCOVERYWORLD provides code for automatic grading using OPENAI models, and in our evaluation we make use of GPT-4O, a long-context (128k token) model that allows even large knowledge structures to be graded.Examples of both positive and negative knowledge assessments are provided in APPENDIX E.</p>
<p>Experiments, Baseline Agents, and Human Baselines</p>
<p>In this section we examine the performance of strong baseline agents on each of the DISCOVERY-WORLD tasks.In addition, we investigate the performance of human scientists, and highlight the performance gap between current agent models and real human scientists.</p>
<p>Experimental setup</p>
<p>For the purposes of this work, we seek to better understand the zero-shot generalization performance of AI agents on tasks that require iterative scientific discovery: that is, coming up with hypotheses, doing in-game experiments to (in)validate them, then arriving at a discovered solution for the task.</p>
<p>As such, we evaluate the performance of three contemporary baselines in a zero-shot setting (though single-task, multi-task, or curriculum learning settings with separate training and evaluation sets are possible with this benchmark; see APPENDIX C.2 for these configurations).In the zero-shot setting, an agent has no prior exposure to DISCOVERYWORLD, and is evaluated on all 120 tasks.Each task is evaluated independently, without carry-over knowledge from one task to another.</p>
<p>Baseline Agent Models</p>
<p>The baseline agents are described below, with model performance on Discovery tasks shown in Table 4, and performance on Unit Tests shown in Table 5.We use the GPT-4O model for all our agents due to its higher performance and lower cost compared to other models.For space we provide ReAct: This agent uses the ReAct [35] approach of generating a thought and action at each step given the recent trajectory of thoughts, actions and observations.Each action is executed in the environment and the observation is added to the trajectory.In addition to this trajectory, we also provide the current game state information as text, e.g., nearby objects, teleportable locations, etc.If needed, we trim the trajectory (remove oldest steps first) to fit the prompt within the maximum token limit, which (in practice) included up to the last 40 steps of the trajectory.To evaluate this agent's discovered knowledge, we evaluate the concatenation of the agent's "thoughts" across all time steps.</p>
<p>Plan+Exec: Since the ReAct trajectories can be very long and lead to errors due to distractors, we also evaluate a plan-and-execute [29,34] approach.We use the LLM to generate a plan, and each step of the plan is independently executed using the same ReAct agent as above.Since each plan step is simpler than the task, their ReAct trajectories are much smaller, reducing the distractors.Discovery tasks require an iterative planning approach to adapt to new findings, so we use iterative decomposition [11] to only generate one step of the plan based on the previous planning steps and their success.Discovered knowledge is evaluated as with the ReAct agent from the execution steps.</p>
<p>Hypothesizer: This agent resembles the architecture of CLIN [18], with the agent keeping an explicit working memory of running hypotheses and measurements that are updated after taking each action, and conditioning its next actions on the contents of this memory.This explicit knowledge store allows more directly evaluating an agent's discovered knowledge (with an example of Hypothesizer's knowledge provided in subsection D.3).The agent similarly maintains a brief plan and running hypothesis, and is prompted to explain how its action progresses the plan to evaluate that hypothesis.</p>
<p>Human Evaluation</p>
<p>To compare model performance against human performance, we recruited 11 practicing human scientists to complete the DISCOVERYWORLD tasks, with their performance shown in Table 6.Scientists were recruited on the UPWORK platform, each with: (1) an MSC or PHD in a natural science, (2) self-evaluated comfort and fluency with statistical methods and common software like spreadsheets, (3) comfort and previous experience with 2D top-down games.Additional details regarding human participant experiments are provided in APPENDIX F.</p>
<p>Discussion</p>
<p>Human Performance As shown in Table 6, discovery task completion rates varied from tasks that were solved by all participants, to several Challenge difficulty tasks that were solved by only a single participant, highlighting the range of difficulties found across task themes, and range of expertise provided by each scientist.Overall, the average completion rate across tasks was 66%, with 11 of 16 tasks performed by humans having completion rates exceeding 60%.Average knowledge performance was slightly lower at 55%, reflecting that when stuck, the humans sometimes attempted brute force solutions to tasks that may have eventually yielded correct answers (e.g.trying all possible crystal frequencies, if they didn't use regression to fit the data), but without producing the required explanatory discovery.</p>
<p>Agent Performance In contrast to human scientists, the baseline agent scientists exhibited poor overall performance, as shown in Table 4.The most performant discovery agent (REACT) compeleted 38% of easy tasks and 18% of challenge tasks, while the agent that best discovered explanatory knowledge (HYPOTHESIZER) discovered only 34% of gold knowledge in easy tasks, and only 8% in challenge tasks.An analysis of these agents performance on the Unit Test tasks (Table 5) shows moderate overall performance, with completion rates in the 60%+ range, suggesting that current agents are competent at many of the components of scientific discovery -like measuring objects with instruments -but currently lack the capacity to perform end-to-end discovery in most DISCOVERYWORLD settings.</p>
<p>Conclusion</p>
<p>We have presented DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform end-to-end scientific discovery.Each of the 120 tasks requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions.We empirically demonstrate that expert human scientists find the challenge tasks in DISCOVERYWORLD difficult but solvable, while strong agent baselines struggle to complete most tasks, or discover critical explanatory knowledge.We hope DISCOVERYWORLD will inspire and accelerate the development of new, general AI discovery agents by the community.</p>
<p>A Overview</p>
<p>The supplementary materials is structured as follows:</p>
<p>• Appendix B: Limitations and Broader Impacts.</p>
<p>• Appendix C: Additional Details on DISCOVERYWORLD.This section covers the Action Space and common Object Properties of DISCOVERYWORLD, the recommended practices for different Experimental Settings, more details on the Discovery Themes and the Unit Tests.</p>
<p>• Appendix D: Additional Baseline Model Details.</p>
<p>• Appendix E: Automatic Evaluation of Explanatory Discovery Knowledge.</p>
<p>• Appendix F: Human Scientist Participants.</p>
<p>B Limitations and Broader Impacts</p>
<p>Simulation Fidelity: DISCOVERYWORLD is inherently a low-fidelity representation of the physical world, with an abstracted action space.As such, agents that perform well on DISCOVERYWORLD may not necessarily perform well at making discoveries in the real world, given the much larger search and action space.That being said, while the simulated environment is low-fidelity compared to the physical world, the steps of discovery that are simulated (from ideation, hypothesis formation, experiment design and execution, data analysis, and forming conclusions) are common steps in the scientific method regardless of whether those actions are taking place in the real world or a virtual environment.</p>
<p>Agent Cost: The agent models that we run are currently quite costly, ranging from approximately USD$3k-$10k to run for the complete set of 120 tasks in DISCOVERYWORLD.This cost is due in large part to (1) the long (1000+ steps) runtimes, each step requiring at least one LLM call, (2) the large number of individual tasks to evaluate, and (3) the use of performant but costly API-BASED models that charge per token.We believe that developing inexpensive models that allow for rapid iteration is a clear near-term goal for developing further discovery agents.</p>
<p>Societal Benefits and Risks: Automated scientific discovery has the potential for broadly positive societal impact by accelerating the pace of scientific progress, potentially decreasing the time it takes for novel discoveries in medicine, chemistry, technology, and other areas of broad impact.If discovery systems are used by individuals or groups with prosocial intentions, there is the potential for broad societal impact.Conversely, if individuals or groups with negative intentions choose to attempt to accelerate discoveries that may be harmful, there is the potential for those individuals or groups to cause harm using automated scientific discovery models.</p>
<p>C Additional Details on DISCOVERYWORLD</p>
<p>To encourage the community to work on general AI discovery agents, we have open-sourced DIS-COVERYWORLD under Apache-2.0license on GitHub at github.com/allenai/discoveryworld.</p>
<p>C.1 Action Space</p>
<p>The action space for DISCOVERYWORLD is shown in Table 7.</p>
<p>C.2 Experimental Settings</p>
<p>As both a development environment and benchmarking tool, we provide the following recommended configurations for evaluating on DISCOVERYWORLD in common experimental settings:</p>
<p>Zero-shot: For zero-shot evaluation, an agent should not have prior exposure to DISCOVERY-WORLD, and can be evaluated on all 120 tasks.Each task should be evaluated independently, without any carry-over knowledge from one task to another.All of the baseline agent models provided in this work are evaluated zero-shot.</p>
<p>Single-Task Learning: For agents that require training data (e.g., RL agents, few-shot examples, etc.) of highly similar tasks (i.e.within-task designs), following the TEXTWORLD [5], ALF-WORLD [26], and SCIENCEWORLD [30] conventions, we recommend the following within-theme design: training on parametric seeds 0 and 1, developing on seed 2, and testing on seeds 3 and 4. In addition, all tasks in the Unit Tests are available for training.</p>
<p>Multi-Task Learning: For agents that require training data (e.g., RL agents, few-shot examples, etc.), following the JERICHO benchmark [7] for evaluating text game suites, we recommend training and evaluating across themes.Tasks 1 to 6 (i.e., theme Proteomics and Chemistry) can be used for training, tasks 7 through 12 (Archaeology and Reactor Lab) for development, and the remaining tasks 13 through 24 (Plant Nutrients, Space Sick, Rocket Science, and Translation) for testing.In addition, all tasks in the Unit Tests are available for training.</p>
<p>Curriculum Learning: For agents that require training data of growing complexity, irrespective of the experimental setting, we recommend to simply follow the natural progression of the difficulty levels for each theme.In addition, all tasks in the Unit Tests are available for training.</p>
<p>C.3 Discovery Themes</p>
<p>The following are descriptions of the 8 main discovery themes, and how these themes vary across each of the 3 levels of difficulty.Note: This section contains spoilers!Proteomics This theme examines an agent's ability to perform analyses that involve clustering and identifying outliers.The agent is told that a number of animal species exist in a remote habitat, with one hypothesized to have migrated from an isolated island in the distant past -with the agent required to measure the abundance of different proteins (using a protein meter) to determine which species is the outlier.In normal difficulty the clustering problem is two dimensional (that is, values of two proteins are provided for each measured animal), while in challenge difficulty, the clustering problem is three dimensional.Protein concentrations are provided as values between zero and one.</p>
<p>The protein concentration data is manufactured such that all animals protein values exist on the surface of a sphere centered on a randomly chosen point, with inliers having a small radius from the center (e.g.0.1), and outliers having a much farther radius (e.g.0.4).Inliers and outliers are clearly visible on a plot by inspection.The easy difficulty level reduces the number of animals from 5 to 3, and has the entire task take place in a small lab with unmoving animals, rather than a large map with moving animals.</p>
<p>Chemistry The chemistry theme evaluates an agents ability to explore a large solution space in the presense of a hill-climbing signal.Here, the agent is told they must manufacture a chemical (a rust removal agent), from some combination of unique chemicals available to them.The agent must enumerate the possible combinations of chemicals, prepare these solutions, and test them on a specific rusted object.After being immersed in a solution, the rusted object will decrease from heavily rusted to moderately or lightly rusted based on the cosine similarity between the chemical concentration of the solution that it is immersed in, and the correct solution, providing a hill-climbing signal for agents to use to more quickly explore the space of possible solutions.The normal difficulty has three possible chemical dispensers, the challenge has four chemical dispensers, and both require agents to find specific concentrations of chemicals (e.g. 1 part Chemical A, 2 parts Chemical B).The easy difficulty has four chemical dispensers, but requires no mixing -one of the four chemicals is a direct solution.</p>
<p>Archaeology The archaeology theme involves measuring (and validating) artifacts using radioisotope dating.In the challenge difficulty, the agent is presented with a dig site that contains six artifactsthree known, three unknown -as well as a radioisotope meter that measures 4 different radioisotopes.</p>
<p>The agent is prompted that it is unknown if radioisotope dating works on Planet X, and if it does, which of the 4 radioisotopes will be useful for dating.The agent is tasked with finding the oldest unknown artifact, and placing a red flag beside its dig site.The known artifacts include one from the stone age (a stone hammer), one from the bronze age (a bronze chisel), and one from the iron age (iron tongs).The agent must know that in radioisotope dating, older artifacts have lower concentrations of a given radioisotope due to radioactive decay.The agent must also make the critical insight that stone age artifacts are older than bronze age artifacts, and iron age artifacts are younger than bronze age artifacts.The data are manufactured such that one radioisotope follows this pattern, while others don't (with a correlation of R 2 &lt; 0.1 between age and radioisotope value for all incorrect channels).</p>
<p>In the normal difficulty, the instrument directly supplies age, and evaluates instrument use rather than instrument validation.The easy difficulty is similar to normal but in a small lab environment.</p>
<p>Reactor Lab The reactor lab theme requires agents to discover and use mathematical relationships between measured quantities using regression.The reactor lab contains a number of quantum crystals, each with a specific resonance frequency.The agent must place all crystals into specific slots, and tune each slot to that crystal's frequency, for the reactor to activate.The frequency depends upon one of five measured properties -density, temperature, radioactivity, crystal size, or crystal spectrawhich can be measured using instruments in the adjoining science lab.The agent is provided with some number of crystals with known frequencies, and two crystals with unknown frequencies.They must measure each crystal with each of the instruments, determine which physical property is related to the frequency (as well as how), and use this equation to calculate the frequencies of the unknown crystals.In the normal setting, 2 known crystals are provided, and the relationship can be found with linear regression (y = mx + b).In the challenge setting, 3 known crystals are provided, and the relationship is quadratic (y = a 2 x + bx + c).In the easy setting, only a single (correct) instrument is provided, there is only one unknown crystal, and the relationship requires only inferring the slope (y = mx).</p>
<p>Plant Nutrients</p>
<p>The plant nutrient theme requires agents to infer systems of rules based on observing both positive and negative examples.The agent finds itself at a botanical research center, and given the task of identifying the (unusual) nutrients that plants on PLANET X prefer.The research center includes a "pilot field" where 12 seeds have been planted, some of which sprouted correctly, as well as 3 "test fields", each controlled by a soil computer that allows manipulating the nutrients in its soil.The agent must infer the necessary levels of nutrients in the soil from the pilot field, then successfully grow at least two new plants by setting the nutrient levels in a test field to be appropriate, and planting (and growing) seeds in that field.All settings include five different nutrients.In the normal setting, rules are simple presence-at-value rules (i.e.plants require a specific nutrient, at a specific value of either low, medium, or high to grow), and these can be inferred from positive examples alone.In the challenge setting, the rules involve logical relations (e.g.XOR, AND, OR, NOT) between nutrients, and more examples are provided.The easy setting resembles normal except that nutrients are binary (present/absent) rather than varying in concentration.</p>
<p>Space Sick</p>
<p>The space sick theme requires agents to use open-ended discovery skills to investigate the cause of an illness.The agents are tasked with identifying why some colonists are becoming mildly ill when eating local food, and correcting this.The agent must discover that some food has been contaminated by mold, which can be directly observed with some instruments, and indirectly observed with others (e.g. through elevated spectrometer readings), or indirectly inferred (e.g.cooking food causes it to become safe).Distractors (such as some food being slightly radioactive) lead the agent down paths that do not solve the task.In the normal difficulty, the mold is directly observable using instrumentation.In the challenge difficulty, the agent must discover novel detection instrumentation.</p>
<p>In the easy setting, the agent is placed in a lab with 3 samples of food and 4 instruments, and must identify which food sample is contaminated (which is detectable by only a single instrument).</p>
<p>It's (not) Rocket Science!The rocket science theme requires agents to measure quantities in an environment, then substitute these quantities into known equations to complete a task.Here, the agent is tasked with sending a rocket into a specific orbital height around PLANET X.In the challenge version of this task, the agent needs to enter to appropriate orbital velocity, as well as the type (and quantity) of propellant to be used.The agent is provided with a rocketry book containing all the formulas needed to complete the task.However, before it can calculate the required quantities, it needs to infer out how to measure unknown values such as PLANET X's gravity and radius, as well as properties related to the three available types of propellent such as density, mass flow rate, and thrust when consumed by the rocket engine.This theme involves recalling known experiments performed on Earth and transposing them to PLANET X (e.g., Eratosthenes' classical experiment to measure Earth's radius 4 ).The normal difficulty simplifies the task to requiring only orbital velocity (not propellant), while the easy difficulty tells the agent they are on a planet with a similar mass and radius to a known celestial body (e.g.Earth, Mars, Venus), eliminating the requirement to measure these values of PLANET X.</p>
<p>Lost in Translation</p>
<p>The translation theme requires agents to infer the meanings of progressively more complicated utterances given access to an environment containing Rosetta-stone-style information.Here, the agent must translate an unknown language used by the native inhabitants of PLANET X.The agent first talks to one of the inhabitants, who speaks an utterance.From this, the agent must explore the village to gather clues about what each word means (e.g., visiting shops will help identifying the name of the items being sold there).In the challenge difficulty, instructions are composed of a verb, an amount, a color, and an item name (e.g., bring 3 red flowers).Bringing the correct number of the correct objects back to the inhabitant will complete the task.In the normal difficulty, the utterance requires translating only a single specific object.In the easy setting, the task is reduced to a small lab setting, where the agent must identify which item to bring to another agent based on its translation (accessible by a sign near the object).</p>
<p>C.4.1 Unit Test Enviroments</p>
<p>The unit tests are shown in Figure 3, and described in Table 8.</p>
<p>C.5 Object Properties</p>
<p>A list of common object properties provided by the base OBJECT storage class is shown in Table 9.</p>
<p>Note that specific objects (like a QUANTUMCRYSTAL) may implement properties not included in the base class (such as resonanceFrequency).The user agent must talk to an NPC agent, and respond correctly to its requests.The NPC agent asks the user agent to select the dialog item that says (for example) 'kiwi', and the user agent must select this dialog option in the dialog tree amongst 8 distractors.The task completes when this process happens successfully 3 times.</p>
<p>26</p>
<p>Measure an object with an instrument The user agent must take a specific item (from 4 possible items), measure a specific property (from 5 possible instruments), and place the item in one of 4 containers depending on the range (e.g.temperature range 0-2.5C in Container A, temperature range 2.5-5.0C in Container B, etc.).</p>
<p>27</p>
<p>Pick-and-place object The user agent must take a specific item (from 5 possible items) and place it in a specific container.</p>
<p>28 Pick-and-give object The user must take a specific item (from 5 possible items) and give it to a specific NPC agent (from 3 possible agents).</p>
<p>Read DiscoveryFeed posts</p>
<p>The user agent must take a specific item (from 5 possible items), and place it in a specific container.The item to take is provided in a DiscoveryFeed post.</p>
<p>Move through doors</p>
<p>The user agent must successfully navigate a randomly generated maze (in the form of a house with walls/doors in random locations), and pick up a flag at the end of the maze.All doors must be opened and the flag must be taken for task success.</p>
<p>31</p>
<p>Using keys with doors The user agent must successfully navigate a randomly generated house that contains 3 doors, and pick up a flag in the final room.The doors require keys, which are randomly placed near the doors.</p>
<p>32</p>
<p>Navigate to a specific room in a house The user agent must succesfully navigate to a specific room in a house (such as the kitchen or bedroom), which it must infer based on the contents of each room.Once in the correct room, it signifies its selection by dropping a flag.</p>
<p>33</p>
<p>Search an environment for an object The user agent must navigate the interior and exterior of a house environment in search of a red flag.</p>
<p>34</p>
<p>Interact with a moving agent The user agent must interact with 3 moving NPC agents spread throughout the environment, notifying each that it's about to rain, and they may want to return home to avoid becoming wet.Step 2: Open the door --Task failed!Door is locked and I don't have the key</p>
<p>Step 3: Find the key to the door ...</p>
<p>To execute each step of the plan, we use the same ReAct agent as above but additionally add the previous steps of the plans and their results to the prompt.To prevent the ReAct agent from wasting time on infeasible steps, we add a hint to the prompt to consider returning the Task failed!message (e.g.Step 2 above) when 1/5 th of the environment step budget is used up.We use the same truncation strategy for the ReAct agent and no truncation is needed for the planner due to the much shorter plans.</p>
<p>D.3 Hypothesizer</p>
<p>This agent maintains a working memory of science-related knowledge (allowing the knowledge this agent has discovered to be more directly evaluated).To assist in planning and execution, this agent also maintains a running hypothesis for the task solution, as well as a short natural language plan for the immediate steps it should take.More specifically, at each step, HYPOTHESIZER chooses an action based on it's current working memory, plan, and running hypothesis.It then reflects on the results of that action, and updates the working memory based on the results.The working memory is science themed, storing two types of records: (1) HYPOTHESES, which include the hypothesis statement, it's current status (i.e.whether it has been confirmed, rejected, or is still pending), and a list of supporting evidence the agent has listed as helping make this determination, (2) and MEASUREMENTS, which take the form of specific observations the agent has made of objects in the environment -for example, that a particular plant appeared to grow in a mixture including a specific nutrient.This memory and reflection is similar to CLIN [18], except: (a) this memory is structured to hold science-domain relations, where CLIN's holds causal relations, and (b) this memory is updated after each action in the environment, where CLIN's is updated only after running an entire task to completion.This means that, if an agent takes N steps in an environment, it requires 2N LLM calls to accommodate this actthen-reflect cycle.Because the memory can grow large, after every 10 actions, the working memory is summarized through a separate LLM call that requests a maximum of 40 separate hypothesis or measurement entries, and a maximum of 2k tokens for the entire memory.When producing actions, HYPOTHESIZER is prompted to produce (1) a specific action, (2) an explanation that includes its running hypothesis, and how this action helps move it towards evaluating this hypothesis, and (3) a short memory describing its plans or any other helpful information it wishes to pass on to the next step.When prompted to produce actions, HYPOTHESIZER is provided with (a) the current observation from the environment, (b) the last 3 actions it took (and their respective explanations and notes), (c) it's current working memory, (d) a list of possible actions and which arguments they take.All output is produced as a JSON dictionary.</p>
<p>The working memory that Hypothesizer stores can be long, especially for longer tasks.An example of the Hypothesizer agent's working memory at the successful completion of a specific shorter task (Proteomics, Normal, Seed 2) is provided below.</p>
<p>Task Description: You are in a biological preserve on Planet X, that has 5 different animal species.We suspect that one of these animal species is not native to the area, but migrated from an isolated island in the recent past.Your task is to use the proteomics meter to analyze the proteins of each of the 5 animal species, which can be found throughout the environment away from the central statue area, and determine which species is the anomoly.Once you have completed your task, return to the statue area and drop the red flag directly beside the statue of the animal species that is the anomoly.</p>
<p>"</p>
<p>D.4 Cost analysis</p>
<p>An approximate cost analysis of each baselines model is provided below, in Table 10.Model cost varies across task, environment complexity, and the number of items stored within a model's memory (e.g. the size of the history in REACT, or the size of the memory in HYPOTHESIZER), so approximate averages are shown.</p>
<p>The full benchmark contains 8 discovery themes × 3 difficulty levels × 5 seeds = 120 tasks to evaluate.Easy tasks are evaluated to 100 steps, while Normal and Challenge tasks are evaluated to 1000 steps or a hard $125 limit, whichever came first.This results in a total upper-bound estimate of 40 × 100(easy) + 40 × 1000(normal) + 40 × 1000(challenge) = 84, 000 steps required to complete an evaluation run, assuming no agents finish early due to task completion.</p>
<p>E Automatic Evaluation of Explanatory Discovery Knowledge</p>
<p>The following prompt is used to automatically evaluate explanatory discovery knowledge, in reference to gold reference questions provided automatically by the DISCOVERYWORLD task scorer:</p>
<p>Your task is to evaluate the knowledge of an agent playing a game about scientific discovery, to see if they have figured out the discovery required to solve the task.I will supply you with: (1) the task description, (2) the agent's knowledge, and (3) a single critical question about the discovery, that you should answer with either true (1) or false (0).} ], "evaluation_totalscore_raw": 0, "evaluation_totalscore": 0.0,</p>
<p>F Human Scientist Participants</p>
<p>General Details To compare model performance against human performance, we recruited 11 practicing human scientists to complete the DISCOVERYWORLD tasks, with their performance shown in Table 6.Scientists were recruited on the UPWORK platform, each with: (1) an MSC or PHD in a natural science, (2) self-evaluated comfort and fluency with statistical methods and common software like spreadsheets, (3) comfort and previous experience with 2D top-down games.To evaluate the latter, all participants were required to complete a screening task involving a Tutorial in DISCOVERYWORLD.Participants were given a maximum of one hour to complete each task.While our agent models are run in a zero-shot setting, humans can't forget their past experiences, so our participants only completed the normal and challenge settings, to prevent any easily-discovered knowledge in the easy setting from providing an advantage in other settings.To collect accurate estimates of task difficulty, all humans completed the same seed (SEED 0) of each task.To evaluate discovered knowledge, participants were asked to keep notes, and explicitly state their hypotheses, and supporting evidence.Due to the varied nature of these notes (some wrote a few words, others entire pages per task) and their modality (text, spreadsheets), discovery knowledge was evaluated manually using the following criteria: 1 (discovered all discovery knowledge in scorecard), 0.5 (discovered some discovery knowledge in scorecard), or 0 (did not find critical discovery knowledge).Other metrics (task completion, procedural knowledge) were evaluated automatically.At their option, not all participants completed all tasks.</p>
<p>Informed Consent Details about the project's objective and the intended use of data generated during game play was provided to participants in the initial job description.Participants were further provided with a participation agreement detailing that all data generated from the project will be owned by AI2, which they agreed to by choosing to participate in the project.</p>
<p>Participant Instructions Participant instructions can be found in README-USERSTUDY.MD on the DISCOVERYWORLD code repository.</p>
<p>Potential Participant Risks To protect the personal information and anonymity of the human scientists we recruited, no personally identifying information was collected and all data is anonymized.</p>
<p>Recruitment Participants were recruited and hired on Upwork, where they were initially required to submit a proposal detailing their educational background, experience with statistical analyses and Python programming, and interest in video games.</p>
<p>Compensation Human scientists were compensated at a rate ranging from USD$20-$30/hr for their work, based on the rate they bid in their initial job proposals.The total amount spent on participant compensation did not exceed $4,000.</p>
<p>Participants' Expertise The participants we recruited had Master's or Doctorate degrees in a range of disciplines.Their educational and/or work experience spanned the following fields: astrophysics, bioinformatics, biology, biomedical engineering, biostatistics, chemistry, computer engineering, data science, electrical engineering, geology, machine learning, mathematics, and physics.</p>
<p>Graphical Interface Humans made use of the 2D graphical user interface for their study, rather than a text-only version (as some agent models use).Figure 4 shows an example of the interface.Time limit Human participants were given a maximum of 1 hour to complete a given task.Zero-shot Performance While participants were allowed to retry tasks that they did not complete successfully the first time, we did not include any retries in our evaluation of human performance, to give an accurate analog of zero-shot model performance.</p>
<p>Data Instructions on how to get the anonymized data from human participants is available at https://github.com/allenai/discoveryworld.</p>
<p>Save failures DISCOVERYWORLD automatically saves log files to evaluate performance.Some participants noted infrequent issues with this automatic save on the WINDOWS platform that we were unable to resolve.As such, a small amount of data was lost, and is not included in this analysis.</p>
<p>Figure 1 :
1
Figure 1: DISCOVERYWORLD is a virtual environment for developing and evaluating discovery agents, with challenge tasks covering a broad variety of different topics such as those shown above.</p>
<p>The Colonists on PlanetX are becoming sick after they eat local food.Discovery why, and x it.</p>
<p>Figure 3 :
3
Figure 3: Example instances of the 10 Unit Test themes.</p>
<p>Figure 4 :
4
Figure 4: Example of the user interface the participants used.This is for the It's (not) Rocket Science! theme.</p>
<p>Table 1 :
1
A comparison of existing virtual environments with DISCOVERYWORLD.Note that to control for spatial complexity across environments, for the purposes of counting actions, move actions are considered single actions (i.e.move [dir]).Information in this table has been pieced together by looking at the different source materials for each environment (e.g., paper, website, and codebase).These should be taken as our best estimate.
Multi-# ofPara-# Obj.# ofTaskEnvironmentmodalDomainTasksmetricProps.ActionsLengthMINIGRID [4]Image/SymbolPick+Place23Yes4485ALFRED [25]ImagePick+Place6Yes20750ALFWORLD [26]Text/ImagePick+Place6Yes16910MINEDOJO [6]ImageMinecraft10  †Yes256+12100kNETHACK LE [16]Text+ImageDungeon1Yes697880kALCHEMY [28]Image/SymbolChemistry1Yes39200IVRE [33]Image/SymbolHypothesis Testing1Yes3910SCIENCEWORLD [30]TextElem. Science30Yes3625100DISCOVERYWORLDText+ImageSci. Discovery24+10Yes63141k</p>
<p>Table 2 :
2
High-level descriptions of the 8 discovery themes in DISCOVERYWORLD, with full task descriptions (including spoilers) provided in APPENDIX C.3.It is from these 8 discovery themes × 3 difficulty levels that we parametrically generate 120 unique instances of discovery tasks.</p>
<p>Table 3 :
3
An example scorecard provided by DISCOVERYWORLD for an instance of a Reactor Lab task, including task completion, task process, and discovered explanatory knowledge scores.
Scorecard: Rector Lab, Normal Difficulty, Seed 1Out ofTask Completion: Was the task completed successfully?/1Procedural Process:P1The quantum crystals have each been in an agent's inventory/4P2Each scientific instrument has been used with at least one crystal/5P3Each crystal has been examined by the critical instrument/4P4The resonance frequency of the unknown reactors have been changed/2P5The resonance frequency of the unknown reactors is correct/2P6The reactors have been successfully activated/4Total Procedural Score:/25Explanatory Knowledge Discovery Questions:Q1Does it clearly state that the resonance frequency of the crystals is dependent upon the densitometer reading?/1Q2Does it clearly state that the relationship is linear, with crystal frequency = (96 * densitometer reading) + 102/1Total Discovery Knowledge Score:/2</p>
<p>Table 4 :
4
Baseline model performance on each of the three scoring metrics (task completion, task process, explanatory knowledge discovery) across all 24 DISCOVERYWORLD tasks.Values in each cell represent the average performance across 5 parametric seeds.Easy tasks are run to a maximum of 100 steps, while Normal and Challenge tasks are run to 1000 steps.
ReACTPlan+ExecuteHypothesizer</p>
<p>Table 5 :
5
Baseline
ReACTPlan+ExecuteHypothesizer
model performance on each of the three scoring metrics (task completion, task process, explanatory knowledge discovery) across all 10 unit test tasks.Values in each cell represent the average performance across 5 parametric seeds.Unit tests tasks are run to a maximum of 100 steps.</p>
<p>Table 6 :
6
Expert human scientist performance on DISCOVERYWORLD tasks, as well as average task completion time.Scores represent average performance of up to 11 humans when playing the same seed of a discovery task.
Human Performance
high-level descriptions of each model here, with additional implementation details and run costs provided in APPENDIX D.</p>
<p>Table 7 :
7
The action space for DiscoveryWorld, which includes 14 main actions.Actions can take zero (e.g.
ActionDescriptionActionDescriptionMOVE DIRMove NorthUSE OBJ [ON OBJ] Use spectrometer on soilTAKE OBJTake thermometerEAT OBJEat mushroomDROP OBJDrop seedREAD OBJRead rocketry bookPUT/GIVE OBJ TO OBJ Put sample in jarWAITDo nothingOPEN/CLOSE OBJOpen doorFEEDView DiscoveryFeedDE/ACTIVATE OBJActivate pendulumTELEPORT LOC  <em>Teleport to Science LabTALK OBJTalk to ColonistTELEPORT OBJ  </em>Teleport to Microscope
WAIT), one (e.g.TAKE THERMOMETER), or two (e.g.USE SPECTROMETER ON SOIL) arguments.Two handicap actions ( * ) are available to agents to assist with poor spatial/navigation abilities.</p>
<p>Table 8 :
8
High-level descriptions of the 10 unit test themes in DISCOVERYWORLD.
Specific unit tests tasks are parametrically generated from a given theme.#ThemeDescription25Multi-turn dialog with agent</p>
<p>Table 9 :
9
A list of object properties common to the OBJECT storage class in DISCOVERYWORLD.From this list is omitted any properties tied to a specific object, e.g., the fuel quantity in the rocket.</p>
<h1>Property NameDescription1grid locationInitial world location (X, Y)2isMovableCan it be moved?3isPassableCan an agent walk over this?4obscuresObjectsBelowDoes it obscure/hide objects on layers below it?5isActivatableIs this a device? (more specifically, can it be activated/deactivated?)6isActivatedIs this device currently activated?7isUsableCan this device be used with another object? (e.g., specifically through the 'use' action).8Can it be dialoged with?9isShovelableCan it be shoveled?10isReadableCan it be read?11documentAny text to read12temperatureCThe default object temperature, in Celsius13heatSourceMaxTempIf it is a heat source, then this is the maximum temperature that it can reach14coolSourceMinTempIf it is a cool source, then this is the minimum temperature that it can reach15isLivingIs it alive?16substanceNameName of the substance17isSubstanceIs it a substance?18isAutoReactingDoes it react automatically with other substances?19mixtureDictDictionary of substances and their proportions in the mixture20requiresKeyIf it requires a key to open/use, then this is a special ID for the key. If the value is &lt;=0, then itdoesn't require a key.21keyIDIf this object acts as a key, here's its ID (0 by default)22radiocarbonAgeRadiocarbon dating age (in years). -1 means it's not applicable/inconclusive.23radioisotopeValuesRadioisotope values. If empty, then it's not applicable/inconclusive.24soilNutrientsSoil nutrients. If empty, then it's not applicable/inconclusive.25needsNutrientLevelsFor seeds/plants: What nutrient levels do they need to grow?26antirequirementsNutrientLevels A list of dictionaries, each containing a list of nutrient levels under which the seed/plant willNOT grow27densityObject density (in g/cm³). &lt;=0 means it's not applicable/inconclusive.28microscopeModifierTextModifier text, if the object is viewed under a microscope. This is a list of strings, which aredisplayed in the microscope view.29microscopeDescShort description to be displayed under a microscope.30colorColor description of the object.31spectrumSpectrum data of the object.32phpH value of the object.33radiationusvhRadiation level in microsieverts per hour.34nitrogenNitrogen content.35phosphorusPhosphorus content.36potassiumPotassium content.37cosCanBeLiquidCan it exist in liquid form?38cosCanBeSolidCan it exist in solid form?39cosCanBeGasCan it exist in gas form?40cosMeltingPointCMelting point in Celsius.41cosBoilingPointCBoiling point in Celsius.42cosCombustionPointCCombustion point in Celsius.43livingMinTempMinimum temperature for living organisms.44livingMaxTempMaximum temperature for living organisms.</h1>
<p>Table 10 :
10
Approximate cost estimate for the models investigated in this work (all using GPT-4o).Assumes a cost of $5/M input tokens, and $15/M output tokens (current pricing as of this writing).
ModelCost per 100 steps Total cost estimate (120 tasks, 84k steps)REACT$2-4 / 100 steps$3,360PLAN + EXECUTE$3-4 / 100 steps$3,360HYPOTHESIZER$10 / 100 steps$8,400
While a precise definition of "scientific discovery" is somewhat elusive[17,19], we here adopt a pragmatic approach: Our goal is to help develop systems that can perform the end-to-end research process that human scientists engage in as they work on a problem, attempt to answer a question, or more generally advance their field of study. We use the term "scientific discovery" here to refer to this process.
†  We only considered programmatic tasks as they can be measured accurately. Then, we only count the (sub)categories since most programmatic tasks are parametric variations of those.
Text-based games for research typically render observations as lists of objects[5,30]. Here, we provide the same list-of-object content, but in JSON format.
https://en.wikipedia.org/wiki/Eratosthenes#Measurement_of_Earth's_circumference
Acknowledgments and Disclosure of FundingWe thank members of the Aristo and Semantic Scholar teams at the Allen Institute for Artificial Intelligence for their helpful comments and support throughout the development of this work.Game assets (e.g., sprites) used in DISCOVERYWORLD are in large part from the CUTERPG pack by Pixy-Moon that was purchased for this work and requires attribution, with some additions or modifications from the authors and OPENAI DALL-E for science-themed content.(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The URL to the code repository is provided as a footnote on the first page.(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?[Yes] The details for the baselines is provided in the appendix and the code is available in the code repository.(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?[N/A] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?[Yes] Section D.4 describes the cost associated to running the baselines.4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators?[Yes]We mention in the acknowledgements where the image assets came from.(b) Did you mention the license of the assets?[Yes] The code repository contains the relevant license for the image assets and proper attribution was made in the acknowledgements.(c) Did you include any new assets either in the supplemental material or as a URL?[Yes] We are releasing (1) a benchmark environment (code), and (2) anonymized data collected from the human participants (i.e., trajectories and notes).(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?[Yes] For the game assets, it is mentioned in the acknowledgements section.For the human curated data, we provide details in Appendix F. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?[Yes] In Appendix F, we explained how the human data is curated, anonymized, and sterilized.D Additional Baseline Model DetailsAll models and their implementations are provided in the DISCOVERYWORLD code repository.D.1 ReActThe ReAct agent generates a thought and action at each step given a prompt describing the environment and the previous steps.The prompt contains information about the environment, how to interact with the environment, current environment state, teleportable locations, interactable objects, and valid actions.The previous steps are shown as a sequence of (thought, action, observation)s, e.g.:History of action-observations: Action: '''json { "action": "ROTATE_DIRECTION", "arg1": "north", "thought": "I need to face north to approach the door and enter the building."} ''' Observation:'''json {"message": "I rotated to face north.","errors": [], "success": true} ''' Action:'''json { "action": "OPEN", "arg1": 2056, "thought": "I need to open the door in front of me to enter the building."} ''' Observation:'''json {"message": "I opened the door.","errors": [], "success": true} '''The agent generates thoughts and actions using the JSON formatted output as shown above.Each action is executed in the environment and the returned observation is then added to the trajectory.Since tasks in DiscoveryWorld can be extremely long, the trajectories can often exceed the maximum token limit of our LLM.We generally hit the 8K token limit of GPT4-O in 40 steps.To avoid this, we limit the trajectory in the prompt to a maximum of 10K characters by removing the oldest (thought, action, observation)s and replacing it with the string [TRIMMED HISTORY].D.2 Plan-and-ExecuteAnother approach to deal with long trajectories used in prior work[34,20]is to decompose the problem into a plan of simpler steps and executing each step independently using ReAct.Since it would be challenging to generate the entire plan in one shot without exploring the environment, we use iterative decomposition[11]where we generate one step of the plan, then execute it and based on the result generate the next step.The planner uses the same base LLM with the prompt describing the format of the plan, e.g.,Task: Open the door and exit the building.Plan:Step 1: Go to the door that leads to the exit --Task completed!I am at the door that leads to the exit "measurement": "The proteomics meter (uuid: 33276) requires two argument objects to be used.","step": 10 }, { "hypothesis": "if (object:spheroid) is (action:directly in front) then (object:proteomics meter) can be used to analyze its proteins.","status": "confirmed", "step": 13, "supporting evidence": "Using the proteomics meter on the spheroid yielded protein data:Protein A: 0.87, Protein B: 0.80."}, { "measurement": "The spheroid (uuid: 23195) has Protein A level of 0.87 and Protein B level of 0.80 when analyzed using the proteomics meter.","step": 13 }, { "measurement": "The animaplant (uuid: 20030) is located in Possible Animal Area 2.", "step": 14 }, { "hypothesis": "if (object:animaplant) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.","status": "confirmed", "step": 15, "supporting evidence": "Using the proteomics meter on the animaplant yielded protein data: Protein A: 0.76, Protein B: 0.67."}, { "measurement": "The animaplant (uuid: 20030) has Protein A level of 0.76 and Protein B level of 0.67 when analyzed using the proteomics meter.","step": 15 }, { "measurement": "The animal in Possible Animal Area 3 (uuid: 20031) is not accessible.","step": 17 }, { "hypothesis": "if (object:animal) is (action:teleported to) using (object:uuid: 20031) then (property:accessibility) will be false.","status": "confirmed", "step": 22, "supporting evidence": "Attempting to teleport to the animal object with UUID 20031 resulted in an error indicating that no object with that UUID was found."}, { "measurement": "There is a tree (big) trunk blocking the path to the west.","step": 25 }, { "measurement": "I successfully moved south to navigate around the tree (big) trunk.","step": 26 }, { "measurement": "I successfully moved west to get directly in front of the pink animal.","step": 28 }, { "measurement": "I rotated to face west, aligning myself with the pink animal.","step": 30 }, { "measurement": "The object directly in front of me, which I thought was a pink animal, is actually grass (uuid: 20981).","step": 31 }, { "measurement": "The proteomics meter (uuid: 33276) was used to investigate the grass (uuid: 20981), but the results were inconclusive.","step": 31 }, { "measurement": "The object with UUID 44261 is a plant, not an animal.","step": 37 }, { "measurement": "The proteomics meter (uuid: 33276) was used to investigate the animaplant (uuid: 44261).The results are as follows: Protein A: 0.76, Protein B: 0.67.","step": 37 }, { "measurement": "The object with UUID 12545 is a plant, not an animal.","step": 38 }, { "measurement": "The proteomics meter (uuid: 33276) was used to investigate the plant (uuid: 12545), but the results were inconclusive.","step": 38 }, { "measurement": "The vortisquid (uuid: 41429) is one of the animal species in the biological preserve.","step": 149 }, { "hypothesis": "if (object:vortisquid) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.","status": "confirmed", "step": 150, "supporting evidence": "Using the proteomics meter on the vortisquid yielded protein data: Protein A: 0.38, Protein B: 0.46."}, { "measurement": "The vortisquid (uuid: 41429) has Protein A level of 0.38 and Protein B level of 0.46 when analyzed using the proteomics meter.","step": 150 }, { "measurement": "The prismatic beast (uuid: 25282) is located in Possible Animal Area 5.", "step": 151 }, { "hypothesis": "if (object:prismatic beast) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.","status": "confirmed", "step": 153, "supporting evidence": "Using the proteomics meter on the prismatic beast yielded protein data: Protein A: 0.73, Protein B: 0.68."}, { "measurement": "The prismatic beast (uuid: 25282) has Protein A level of 0.73 and Protein B level of 0.68 when analyzed using the proteomics meter.","step": 153 }, { "measurement": "The prismatic beast (uuid: 5059) is located in Possible Animal Area 7.", "step": 156 }, { "measurement": "The prismatic beast (uuid: 5059) has Protein A level of 0.73 and Protein B level of 0.68 when analyzed using the proteomics meter.","step": 157 }, { "measurement": "The echojelly (uuid: 4429) is one of the animal species in the biological preserve.","step": 160 }, { "hypothesis": "if (object:echojelly) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.","status": "confirmed", "step": 161, "supporting evidence": "Using the proteomics meter on the echojelly yielded protein data: Protein A: 0.86, Protein B: 0.82."}, { "measurement": "The echojelly (uuid: 4429) has Protein A level of 0.86 and Protein B level of 0.82 when analyzed using the proteomics meter.","step": 161 }, { "measurement": "The vortisquid (uuid: 41429) has significantly different protein levels (Protein A: 0.38, Protein B: 0.46) compared to the other species, indicating it is the anomaly.","step": 162 }, { "measurement": "The object with UUID 33979 (red flag) is not directly in front of me or in my inventory.","step": 172 }, { "hypothesis": "if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.","status": "pending", "step": 172, "supporting evidence": "Attempting to pick up the red flag from the Instrument Table resulted in errors indicating that the object with UUID 33979 could not be found."}, { "measurement": "The object with UUID 33979 (red flag) is not directly in front of me or in my inventory.","step": 173 }, { "hypothesis": "if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.","status": "pending", "step": 173, "supporting evidence": "Attempting to pick up the red flag from the Instrument Table resulted in errors indicating that the object with UUID 33979 could not be found."}, { "measurement": "The object with UUID 33979 (red flag) is located at coordinates(15,20).","step": 174 }, { "hypothesis": "if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.","status": "confirmed", "step": 174, "supporting evidence": "Successfully teleported to the object with UUID 33979 (red flag) at coordinates(15,20)."}, { "measurement": "The object with UUID 33979 (red flag) is now in my inventory.","step": 175 }, { "hypothesis": "if (object:red flag) is (action:picked up) then (property:accessibility) will be true.","status": "confirmed", "step": 175, "supporting evidence": "Successfully picked up the red flag after teleporting to it."}, { "measurement": "The object with UUID 33979 (red flag) is located at coordinates(15,20).","step": 176 }, { "hypothesis": "if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.","status": "confirmed", "step": 176, "supporting evidence": "Successfully teleported to the object with UUID 33979 (red flag) at coordinates(15,20)."}, { "measurement": "The object with UUID 33979 (red flag) is now in my inventory.","step": 176 }, { "hypothesis": "if (object:red flag) is (action:picked up) then (property:accessibility) will be true.","status": "confirmed", "step": 176, "supporting evidence": "Successfully picked up the red flag after teleporting to it."}, { "measurement": "I teleported to the Statue of a vortisquid at coordinates(18,17).","step": 176 { \"criticalQuestion\": \"repeat the critical question\", \"evaluation\": 0 or 1 (as integers), \"explanation\": \"provide a brief explanation for evaluation, making reference to the agent's knowledge and whether or not it reflects the critical question.\"} '''The following is an example of the scorer producing a positive assessment: Task: Proteomics "evaluation": [ { "criticalQuestion": "Does it clearly state that the vortisquid has proteomics values that appear to be outliers compared to the other animals, whose proteomics values all cluster together?","evaluation": 1, "explanation": "The agent's knowledge includes a specific measurement stating that the vortisquid has significantly different protein levels (Protein A: 0.38, Protein B: 0.46) compared to the other species, indicating it is the anomaly.This directly addresses the critical question by identifying the vortisquid's proteomics values as outliers compared to the other animals."} ], "evaluation_totalscore_raw": 1, "evaluation_totalscore": 1.0,The following is an example of the scorer producing a negative assessment: Task: Translation "evaluation": [ { "criticalQuestion": "Does it clearly state that: The word 'Womple' means something like 'bring me'?", "evaluation": 0, "explanation": "The agent's knowledge does not explicitly state that the word 'Womple' means 'bring me'.The agent has recorded instances where the elder and the trainer used the word 'Womple', but there is no clear interpretation or translation of the word provided in the knowledge.The agent has noted that the elder seems interested in items in the inventory and asked if the agent wants to give the items to him, but this does not directly translate 'Womple' to 'bring me'." }, { "criticalQuestion": "Does it clearly state that: The word 'florpt' means 'flower'?","evaluation": 0, "explanation": "The agent's knowledge does not explicitly state that the word 'florpt' means 'flower'.While there are measurements and interactions involving the word 'florpt' and objects like flowers, there is no direct confirmation or hypothesis that equates 'florpt' to 'flower'."
Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242664320592023</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 62023</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai gym. 2016</p>
<p>Minigrid &amp; miniworld: Modular &amp; customizable reinforcement learning environments for goal-oriented tasks. M Chevalier-Boisvert, B Dai, M Towers, R De Lazcano, L Willems, S Lahlou, S Pal, P S Castro, J Terry, CoRR, abs/2306.138312023</p>
<p>TextWorld: A learning environment for text-based games. M.-A Côté, Á Kádár, X Yuan, B A Kybartas, T Barnes, E Fine, J Moore, M J Hausknecht, L E Asri, M Adada, W Tay, A Trischler, CGW@IJCAI. 201849552345</p>
<p>MineDojo: Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>Interactive fiction games: A colossal adventure. M J Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, AAAI Conference on Artificial Intelligence. 2019</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, arXiv:2310.033022024arXiv preprint</p>
<p>Learning zil. Infocom, 1989</p>
<p>Highly accurate protein structure prediction with alphafold. J M Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, Nature. 5962021</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, ICLR2023</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. R D King, K E Whelan, F M Jones, P G K Reiser, C H Bryant, S H Muggleton, D B Kell, S G Oliver, Nature. 42744287252004</p>
<p>Artificial intelligence to win the nobel prize and beyond: Creating the engine for scientific discovery. H Kitano, AI magazine. 3712016</p>
<p>AI2-THOR: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, A Kembhavi, A K Gupta, A Farhadi, ArXiv, abs/1712.05474201728328610</p>
<p>Generalized biomolecular modeling and design with rosettafold all-atom. R Krishna, J Wang, W Ahern, P Sturmfels, P Venkatesh, I Kalvet, G R Lee, F S Morey-Burrows, I V Anishchenko, I R Humphreys, R Mchugh, D Vafeados, X Li, G A Sutherland, A Hitchcock, C N Hunter, A Kang, E Brackenbrough, A K Bera, M Baek, F Dimaio, D Baker, bioRxiv. 2023</p>
<p>The NetHack learning environment. H Küttler, N Nardelli, A Miller, R Raileanu, M Selvatici, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Scientific discovery: Computational explorations of the creative processes. P Langley, 1987MIT press</p>
<p>CLIN: A continually learning language agent for rapid task adaptation and generalization. B P Majumder, B Dalvi, P A Jansen, O Tafjord, N Tandon, L Zhang, C Callison-Burch, P Clark, ArXiv, abs/2310.101342023</p>
<p>The logic of scientific discovery. K Popper, 2005Routledge</p>
<p>ADaPT: As-needed decomposition and planning with language models. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, NAACL (Findings). 2023</p>
<p>. Pygame, Pygame, </p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, P Kohli, A Fawzi, J Grochow, A Lodi, J.-B Mouret, T Ringer, T Yu, Nature. 6252023</p>
<p>MiniHack the planet: A sandbox for open-ended reinforcement learning research. M Samvelyan, R Kirk, V Kurin, J Parker-Holder, M Jiang, E Hambro, F Petroni, H Küttler, E Grefenstette, T Rocktäschel, ArXiv, abs/2109.132022021</p>
<p>Machine learning-assisted discovery of solid li-ion conducting materials. A D Sendek, E D Cubuk, E R Antoniuk, G Cheon, Y Cui, E J Reed, Chemistry of Materials. 2018</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020</p>
<p>ALFWorld: Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M J Hausknecht, ArXiv, abs/2010.037682020</p>
<p>. M Towers, J K Terry, A Kwiatkowski, J U Balis, G D Cola, T Deleu, M Goulão, A Kallinteris, A Kg, M Krimmel, R Perez-Vicente, A Pierré, S Schulhoff, J J Tai, A T J Shen, O G Younis, Gymnasium. Mar. 2023</p>
<p>J Wang, M King, N Porcel, Z Kurth-Nelson, T Zhu, C Deck, P Choy, M Cassin, M Reynolds, F Song, G Buttimore, D Reichert, N Rabinowitz, L Matthey, D Hassabis, A Lerchner, M Botvinick, arXiv:2102.02926Alchemy: A structured task distribution for meta-reinforcement learning. 2021arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K W Lee, E.-P Lim, ACL. 2023</p>
<p>R Wang, P Jansen, M.-A Côté, P Ammanabrolu, arXiv:2203.07540ScienceWorld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. R Wang, P A Jansen, M.-A Côté, P Ammanabrolu, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K A Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K D Grave, J Ramon, M De Clare, W Sirawaraporn, S G Oliver, R D King, Journal of the Royal Society Interface. 122015</p>
<p>Interactive visual reasoning under uncertainty. M Xu, G Jiang, W Liang, C Zhang, Y Zhu, 2023</p>
<p>InterCode: Standardizing and benchmarking interactive coding with execution feedback. J Yang, A Prabhakar, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>ReAct: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, ArXiv, abs/2210.036292022252762395</p>
<p>Task Description: ''' [<em>INSERT TASK DESCRIPTION HERE</em>] ''' Agent's Knowledge: ''' [<em>INSERT AGENT'S KNOWLEDGE HERE</em>] ''' Critical Question: ''' [<em>INSERT SINGLE KNOWLEDGE EVALUATION QUESTION HERE FROM SCORECARD</em>] ''' Please answer this question by responding '1' if the agent's knowledege reflects having discovered the information in the critical question, and '0' otherwise. This response should be in the 'evaluation' key of the response. S Zhou, F F Xu, H Zhu, X Zhou, R Lo, A Sridhar, X Cheng, T Ou, Y Bisk, D Fried, U Alon, G Neubig, 2024WebArena: A realistic web environment for building autonomous agents. The response format is a JSON dictionary containing three keys: 'criticalQuestion</p>            </div>
        </div>

    </div>
</body>
</html>
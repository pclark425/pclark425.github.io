<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9013 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9013</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9013</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-267897463</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14865v2.pdf" target="_blank">Dynamic Evaluation of Large Language Models by Meta Probing Agents</a></p>
                <p><strong>Paper Abstract:</strong> Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9013.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9013.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary high-capability large language model from OpenAI; evaluated as the top-performing model in this paper's dynamic, psychometrics-inspired probing benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI large language model (GPT-4 family variant) used both as an evaluated model and as agent (probing/judge) in experiments; treated in paper as the strongest-performing model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Psychometrics-inspired probing benchmarks automatically generated by Meta Probing Agents (MPA) that transform existing test items (MMLU, ARC-C) according to three cognitive abilities: language understanding (LU), problem solving (PS), and domain knowledge (DK). Each ability is assessed via transformed multiple-choice (or original format) items; accuracy is primary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 90.27%, PS 94.28%, DK 93.69%; MMLU: LU 75.18%, PS 81.48%, DK 81.42%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported in paper. Among models evaluated, GPT-4-Turbo attains the highest accuracy across MPA probing benchmarks but shows significant degradation relative to original (vanilla) benchmarks (e.g., authors report ~15.7% drop on MMLU probing set), which the authors attribute to potential data contamination in original benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>All evaluated models run at generation temperature 0 (deterministic) for final evaluation; agent roles (probing and judging) primarily used GPT-4-Turbo with probing temperature 0.7 and judge temperature 0. Probing used five principles (paraphrase questions, paraphrase choices, permute choices, add extra context, add a new choice) to generate transformed items from MMLU/ARC-C; judge agent validated semantic consistency via binary Yes/No. Human verification: 30 experts checked 1,800 transformed items with reported equivalence/correctness rates (overall 94% and 97%). Primary metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No direct human baseline provided for cognitive tests; probes are task-adapted transformations rather than classical cognitive-psychology tests (e.g., Stroop, Raven). Authors note some transformed items deviated in meaning and judge agents (or weaker probing agents) sometimes failed to filter these; results reflect degradation relative to original benchmarks but not a human-vs-model cognitive comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9013.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9013.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary OpenAI LLM (GPT-3.5 family) evaluated on MPA probing benchmarks; used as an evaluated model and experimented with as an agent in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI model (GPT-3.5 family); evaluated across datasets and also tested as a potential weaker agent in ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same MPA probing benchmark suite (see above): transformed items from MMLU and ARC-C intended to probe LU, PS, DK abilities according to psychometric principles.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 79.18%, PS 84.64%, DK 82.25%; MMLU: LU 61.02%, PS 67.39%, DK 64.61%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided. GPT-3.5-Turbo performs below GPT-4-Turbo on all reported MPA probes; all models showed accuracy drops on probing benchmarks versus original benchmarks, indicating potential memorization/data-contamination effects in originals.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation setting: deterministic generation (temperature 0) for final tests. Authors experimented with using GPT-3.5-Turbo and Gemini-Pro as judge/probing agents in cost-reduction ablations and found weaker judges/probers led to lower-quality transformed items that sometimes deviated in meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human comparison; when used as probe/judge agent GPT-3.5-Turbo tended to produce or fail to detect subtle meaning changes, reducing reliability of generated probes in ablation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9013.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9013.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary Google/Alphabet model (Gemini family) evaluated in paper; included among top-tier proprietary models assessed with MPA probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary model from the Gemini family (Google); evaluated as one of the proprietary high-performing models in the MPA experiments and also explored as an agent in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MPA-transformed items targeting language understanding, problem solving, and domain knowledge, created from MMLU and ARC-C datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 80.46%, PS 84.81%, DK 81.83%; MMLU: LU 59.53%, PS 66.27%, DK 62.77%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported. Gemini-Pro performs below GPT-4-Turbo and roughly comparable to other non-GPT proprietary/open models on MPA probes; all models show reduced accuracy on the probing benchmarks versus vanilla testsets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation at temperature 0. Abalation: authors used Gemini-Pro as a potential judge/probing agent in cost-saving experiments but found limitations in its ability to filter or produce semantically consistent probes compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Model sometimes omitted by authors due to safety constraints in a small number of items; judge/probing ablations show Gemini-Pro is less reliable than GPT-4 for the generation/validation of probes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9013.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9013.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source large language model (Meta Llama 2 70B) evaluated on MPA probing benchmarks; included to contrast open-source model performance with proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama 2 chat model (70 billion parameter variant) evaluated on transformed benchmarks to assess LU/PS/DK abilities and compared across model sizes in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MPA-generated probing benchmarks adapted from MMLU and ARC-C probing the three basic cognitive abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 70.14%, PS 75.00%, DK 68.94%; MMLU: LU 54.54%, PS 57.60%, DK 54.23%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided. Llama2-70b-chat underperforms proprietary models (especially GPT-4-Turbo) on MPA probes; open-source models exhibited larger OT/PF frequencies suggesting higher susceptibility to data contamination or memorization on original benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations deterministic (temperature 0). The authors included smaller Llama2 variants (7b, 13b) to analyze model-size effects on the three abilities and reported a roughly linear positive correlation between size and ability performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Open-source models showed larger relative degradation and higher OT/PF confusion matrix indications; authors caution that judge/probing agents require high capability to ensure probe validity and that some transformed items may be inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9013.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9013.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-34b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-34b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Yi large language model (34B variant) evaluated as an open-source alternative in MPA experiments across psychometric probing principles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-34b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM (Ai Yi family) 34B-parameter chat model evaluated on MPA probing benchmarks to assess LU/PS/DK abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Transformed evaluation sets probing language understanding, problem solving, and domain knowledge adapted from MMLU and ARC-C using MPA principles.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 79.44%, PS 85.67%, DK 83.19%; MMLU: LU 60.01%, PS 66.10%, DK 64.50%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided. Yi-34b-chat performs below GPT-4-Turbo but comparably to other mid/large open models; shows consistent drops on probing benchmarks vs. original datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with generation temperature 0. Included in model-size correlation analysis and cross-ability correlation metrics. Probing generation and judging primarily used GPT-4-Turbo as agent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>As with other models, no human baseline; some domain-specific probing items (e.g., professional law, moral scenarios) produced higher error rates, suggesting topical weaknesses rather than general reasoning failure for some items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9013.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9013.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Mistral-derived model (named Mixtral) with an 8x7B configuration evaluated with MPA probes to examine performance on LU/PS/DK abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Evaluation of Large Language Models by Meta Probing Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral-derived instruct-tuned model (denoted Mixtral-8x7b) evaluated on MPA-transformed benchmarks across the three psychometric abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MPA-created transformed items from MMLU and ARC-C focusing on language understanding, problem solving, and domain knowledge; accuracy compared against original (vanilla) benchmarks to detect contamination and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ARC-C: LU 78.16%, PS 82.17%, DK 78.58%; MMLU: LU 61.18%, PS 66.01%, DK 61.64%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided. Mixtral shows mid-level performance among open-source models and demonstrates expected performance drops on probing benchmarks relative to original datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation settings: temperature 0 for deterministic testing. Included in multifaceted analyses and model-size correlation studies. Probing principles applied as described; judge/probing agents mainly GPT-4-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baselines reported; some probe transformations led to ambiguous or changed-meaning items (authors note need to further improve judge agent robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Evaluation of Large Language Models by Meta Probing Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Revealing the structure of language model capabilities <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Graph-informed dynamic evaluation of large language models <em>(Rating: 1)</em></li>
                <li>Dynamic benchmark on reasoning ability of large language models via complexity classes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9013",
    "paper_id": "paper-267897463",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo",
            "brief_description": "Proprietary high-capability large language model from OpenAI; evaluated as the top-performing model in this paper's dynamic, psychometrics-inspired probing benchmarks.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_description": "Proprietary OpenAI large language model (GPT-4 family variant) used both as an evaluated model and as agent (probing/judge) in experiments; treated in paper as the strongest-performing model.",
            "model_size": null,
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "Psychometrics-inspired probing benchmarks automatically generated by Meta Probing Agents (MPA) that transform existing test items (MMLU, ARC-C) according to three cognitive abilities: language understanding (LU), problem solving (PS), and domain knowledge (DK). Each ability is assessed via transformed multiple-choice (or original format) items; accuracy is primary metric.",
            "llm_performance": "ARC-C: LU 90.27%, PS 94.28%, DK 93.69%; MMLU: LU 75.18%, PS 81.48%, DK 81.42%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported in paper. Among models evaluated, GPT-4-Turbo attains the highest accuracy across MPA probing benchmarks but shows significant degradation relative to original (vanilla) benchmarks (e.g., authors report ~15.7% drop on MMLU probing set), which the authors attribute to potential data contamination in original benchmarks.",
            "experimental_details": "All evaluated models run at generation temperature 0 (deterministic) for final evaluation; agent roles (probing and judging) primarily used GPT-4-Turbo with probing temperature 0.7 and judge temperature 0. Probing used five principles (paraphrase questions, paraphrase choices, permute choices, add extra context, add a new choice) to generate transformed items from MMLU/ARC-C; judge agent validated semantic consistency via binary Yes/No. Human verification: 30 experts checked 1,800 transformed items with reported equivalence/correctness rates (overall 94% and 97%). Primary metric: accuracy.",
            "limitations_or_caveats": "No direct human baseline provided for cognitive tests; probes are task-adapted transformations rather than classical cognitive-psychology tests (e.g., Stroop, Raven). Authors note some transformed items deviated in meaning and judge agents (or weaker probing agents) sometimes failed to filter these; results reflect degradation relative to original benchmarks but not a human-vs-model cognitive comparison.",
            "uuid": "e9013.0",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "Proprietary OpenAI LLM (GPT-3.5 family) evaluated on MPA probing benchmarks; used as an evaluated model and experimented with as an agent in ablations.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Proprietary OpenAI model (GPT-3.5 family); evaluated across datasets and also tested as a potential weaker agent in ablation studies.",
            "model_size": null,
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "Same MPA probing benchmark suite (see above): transformed items from MMLU and ARC-C intended to probe LU, PS, DK abilities according to psychometric principles.",
            "llm_performance": "ARC-C: LU 79.18%, PS 84.64%, DK 82.25%; MMLU: LU 61.02%, PS 67.39%, DK 64.61%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided. GPT-3.5-Turbo performs below GPT-4-Turbo on all reported MPA probes; all models showed accuracy drops on probing benchmarks versus original benchmarks, indicating potential memorization/data-contamination effects in originals.",
            "experimental_details": "Evaluation setting: deterministic generation (temperature 0) for final tests. Authors experimented with using GPT-3.5-Turbo and Gemini-Pro as judge/probing agents in cost-reduction ablations and found weaker judges/probers led to lower-quality transformed items that sometimes deviated in meaning.",
            "limitations_or_caveats": "No human comparison; when used as probe/judge agent GPT-3.5-Turbo tended to produce or fail to detect subtle meaning changes, reducing reliability of generated probes in ablation settings.",
            "uuid": "e9013.1",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Gemini-Pro",
            "brief_description": "Proprietary Google/Alphabet model (Gemini family) evaluated in paper; included among top-tier proprietary models assessed with MPA probes.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro",
            "model_description": "Proprietary model from the Gemini family (Google); evaluated as one of the proprietary high-performing models in the MPA experiments and also explored as an agent in ablations.",
            "model_size": null,
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "MPA-transformed items targeting language understanding, problem solving, and domain knowledge, created from MMLU and ARC-C datasets.",
            "llm_performance": "ARC-C: LU 80.46%, PS 84.81%, DK 81.83%; MMLU: LU 59.53%, PS 66.27%, DK 62.77%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported. Gemini-Pro performs below GPT-4-Turbo and roughly comparable to other non-GPT proprietary/open models on MPA probes; all models show reduced accuracy on the probing benchmarks versus vanilla testsets.",
            "experimental_details": "Evaluation at temperature 0. Abalation: authors used Gemini-Pro as a potential judge/probing agent in cost-saving experiments but found limitations in its ability to filter or produce semantically consistent probes compared to GPT-4.",
            "limitations_or_caveats": "Model sometimes omitted by authors due to safety constraints in a small number of items; judge/probing ablations show Gemini-Pro is less reliable than GPT-4 for the generation/validation of probes.",
            "uuid": "e9013.2",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama2-70b-chat",
            "name_full": "Llama 2 70B Chat",
            "brief_description": "Open-source large language model (Meta Llama 2 70B) evaluated on MPA probing benchmarks; included to contrast open-source model performance with proprietary models.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "Llama2-70b-chat",
            "model_description": "Open-source Llama 2 chat model (70 billion parameter variant) evaluated on transformed benchmarks to assess LU/PS/DK abilities and compared across model sizes in analysis.",
            "model_size": "70B",
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "MPA-generated probing benchmarks adapted from MMLU and ARC-C probing the three basic cognitive abilities.",
            "llm_performance": "ARC-C: LU 70.14%, PS 75.00%, DK 68.94%; MMLU: LU 54.54%, PS 57.60%, DK 54.23%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided. Llama2-70b-chat underperforms proprietary models (especially GPT-4-Turbo) on MPA probes; open-source models exhibited larger OT/PF frequencies suggesting higher susceptibility to data contamination or memorization on original benchmarks.",
            "experimental_details": "Evaluations deterministic (temperature 0). The authors included smaller Llama2 variants (7b, 13b) to analyze model-size effects on the three abilities and reported a roughly linear positive correlation between size and ability performance.",
            "limitations_or_caveats": "Open-source models showed larger relative degradation and higher OT/PF confusion matrix indications; authors caution that judge/probing agents require high capability to ensure probe validity and that some transformed items may be inconsistent.",
            "uuid": "e9013.3",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Yi-34b-chat",
            "name_full": "Yi-34b-chat",
            "brief_description": "Yi large language model (34B variant) evaluated as an open-source alternative in MPA experiments across psychometric probing principles.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "Yi-34b-chat",
            "model_description": "Open-source LLM (Ai Yi family) 34B-parameter chat model evaluated on MPA probing benchmarks to assess LU/PS/DK abilities.",
            "model_size": "34B",
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "Transformed evaluation sets probing language understanding, problem solving, and domain knowledge adapted from MMLU and ARC-C using MPA principles.",
            "llm_performance": "ARC-C: LU 79.44%, PS 85.67%, DK 83.19%; MMLU: LU 60.01%, PS 66.10%, DK 64.50%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided. Yi-34b-chat performs below GPT-4-Turbo but comparably to other mid/large open models; shows consistent drops on probing benchmarks vs. original datasets.",
            "experimental_details": "Evaluated with generation temperature 0. Included in model-size correlation analysis and cross-ability correlation metrics. Probing generation and judging primarily used GPT-4-Turbo as agent.",
            "limitations_or_caveats": "As with other models, no human baseline; some domain-specific probing items (e.g., professional law, moral scenarios) produced higher error rates, suggesting topical weaknesses rather than general reasoning failure for some items.",
            "uuid": "e9013.4",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mixtral-8x7b-Instruct",
            "name_full": "Mixtral-8x7b-Instruct",
            "brief_description": "Open-source Mistral-derived model (named Mixtral) with an 8x7B configuration evaluated with MPA probes to examine performance on LU/PS/DK abilities.",
            "citation_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7b-Instruct",
            "model_description": "Open-source Mistral-derived instruct-tuned model (denoted Mixtral-8x7b) evaluated on MPA-transformed benchmarks across the three psychometric abilities.",
            "model_size": "8x7B",
            "test_battery_name": "MPA probing benchmarks derived from MMLU and ARC-C (Language Understanding, Problem Solving, Domain Knowledge)",
            "test_description": "MPA-created transformed items from MMLU and ARC-C focusing on language understanding, problem solving, and domain knowledge; accuracy compared against original (vanilla) benchmarks to detect contamination and robustness.",
            "llm_performance": "ARC-C: LU 78.16%, PS 82.17%, DK 78.58%; MMLU: LU 61.18%, PS 66.01%, DK 61.64%",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided. Mixtral shows mid-level performance among open-source models and demonstrates expected performance drops on probing benchmarks relative to original datasets.",
            "experimental_details": "Evaluation settings: temperature 0 for deterministic testing. Included in multifaceted analyses and model-size correlation studies. Probing principles applied as described; judge/probing agents mainly GPT-4-Turbo.",
            "limitations_or_caveats": "No human baselines reported; some probe transformations led to ambiguous or changed-meaning items (authors note need to further improve judge agent robustness).",
            "uuid": "e9013.5",
            "source_info": {
                "paper_title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Revealing the structure of language model capabilities",
            "rating": 2,
            "sanitized_title": "revealing_the_structure_of_language_model_capabilities"
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "Graph-informed dynamic evaluation of large language models",
            "rating": 1,
            "sanitized_title": "graphinformed_dynamic_evaluation_of_large_language_models"
        },
        {
            "paper_title": "Dynamic benchmark on reasoning ability of large language models via complexity classes",
            "rating": 1,
            "sanitized_title": "dynamic_benchmark_on_reasoning_ability_of_large_language_models_via_complexity_classes"
        }
    ],
    "cost": 0.014491249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Evaluation of Large Language Models by Meta Probing Agents
7 Jun 2024</p>
<p>Kaijie Zhu 
Jindong Wang 
Qinlin Zhao 
Ruochen Xu 
Xing Xie 
Dynamic Evaluation of Large Language Models by Meta Probing Agents
7 Jun 20247520CEBB33F496D53EBD68F3C7F84AB9arXiv:2402.14865v2[cs.CL]
Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination.Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios.Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities.In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs.MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge.These basic abilities are also dynamically configurable, allowing multifaceted analysis.We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement.Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities.MPA can also be used as a data augmentation approach to enhance LLMs.</p>
<p>Introduction</p>
<p>Intelligence evaluation has never been as important as today due to the contradiction between unprecedented performance and underexplored interpretability of large language 1 Microsoft Research 2 University of Science and Technology of China.</p>
<p>Correspondence to: Kaijie Zhu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#97;&#105;&#106;&#105;&#101;&#122;&#104;&#117;&#49;&#49;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#107;&#97;&#105;&#106;&#105;&#101;&#122;&#104;&#117;&#49;&#49;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>,Jindong Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#74;&#105;&#110;&#100;&#111;&#110;&#103;&#46;&#87;&#97;&#110;&#103;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;">&#74;&#105;&#110;&#100;&#111;&#110;&#103;&#46;&#87;&#97;&#110;&#103;&#64;&#109;&#105;&#99;&#114;&#111;&#115;&#111;&#102;&#116;&#46;&#99;&#111;&#109;</a>.models (LLMs) (OpenAI, 2023b;GeminiTeam, 2023).To facilitate a better understanding of the strength and weakness of LLMs, evaluation was carried out by collecting data from various domains (Liang et al., 2023;bench authors, 2023), benchmarking specific tasks (Hendrycks et al., 2021;Chen et al., 2021;Cobbe et al., 2021;Clark et al., 2018), and evaluating in extreme scenarios (Zhu et al., 2023b;Wang et al., 2023;Yang et al., 2022).</p>
<p>Proceedings of the</p>
<p>There have been increasing concerns about the genuine abilities of LLMs in public benchmarks, attributing the "false promise" to data contamination (Lovin, 2023;Bender et al., 2021;Koco et al., 2023), overfitting benchmarks (Zhu et al., 2023a), improper choice of the evaluation criterion (Schaeffer et al., 2023), or lack of causality (Berglund et al., 2023).Among these concerns, data contamination remains the most significant, as static public benchmarks could easily be harnessed to train models.Moreover, evaluation should provide not only benchmark results, but also insight into the structural capabilities of models for future development (Burnell et al., 2023).For example, evaluations are typically done in a certain context, e.g., a math application problem requires at least two abilities: language understanding (to comprehend the problem) and reasoning (to solve the problem).Which ability matters more, and how can we quantify the relationship between these abilities?</p>
<p>Recently, Zhu et al. (2023a) proposed DyVal to dynamically generate test samples based on the graph structure to combat data contamination.Fan et al. (2023) introduced NPHardEval, which generates new evaluation samples for NP-hard math problems and updates the evaluation set monthly.Both are designed to evaluate reasoning tasks and cannot be easily extended to other popular natural language tasks (Hendrycks et al., 2021;Clark et al., 2018).For fine-grained performance analysis, Burnell et al. (2023) inspected the correlation between different tasks using HELM benchmark results (Liang et al., 2023) and concluded that the performance of LLMs is not monolithic but exhibits variance between different aspects, such as reasoning and understanding.Therefore, developing a dynamic evaluation protocol to support diverse tasks and multifaceted ability analysis remains a challenge.</p>
<p>In this paper, we propose Meta Probing Agents (MPA), a dynamic and flexible evaluation protocol for LLMs based on agents.MPA bridges the gap between psychometrics and LLMs evaluation by designing principles to dynamically generate new questions (Figure 2).The principles correspond to the three basic cognitive abilities of psychometric theory (Burnell et al., 2023): language understanding, problem solving, and domain knowledge.Therefore, MPA supports both dynamic evaluation sample generation and multifaceted ability analysis.Specifically, instead of relying on the graph structure to generate samples like DyVal, MPA uses LLM-based agents to automatically transform existing problems into new ones, which are more flexible and support diverse tasks.We define them as probing agents, aiming to uncover the underlying knowledge in a question.MPA further utilizes a judge agent (Dubois et al., 2023;Li et al., 2023b) to validate the generated evaluation samples.This adversarial manner ensures that the new samples maintain consistency and relevance compared to their original counterparts.Furthermore, MPA can dynamically combine various probing principles for multifaceted evaluations of the abilities.This modular design affords researchers the flexibility to apply any combination of principles, aligning the evaluation scope with their investigative focus, and mirroring the multifaceted nature of human cognition.</p>
<p>We used MPA to generate new evaluation sets based on popular benchmarks: MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2022), and ARC-C (Clark et al., 2018).Then, we conducted extensive evaluations and analysis on popular LLMs: GPT-4-Turbo, GPT-3.5-Turbo,Gemini-Pro (GeminiTeam, 2023), Llama2-70b-chat (Touvron et al., 2023), Yi-34b-chat (01-ai, 2024), andMixtral-8x7b-Instruct (MistralAITeam, 2023).</p>
<p>The takeaways of our key findings are as follows:</p>
<p> The performance of LLMs on our dynamic benchmarks degraded significantly, implying potential data contamina-tion on current benchmarks (Figure 1 &amp; 4.2).Prompt engineering can only bring marginal improvements ( 4.4).</p>
<p> All LLMs exhibited performance decreases by dynamically combining different principles ( 5.1);</p>
<p> Our multifaceted analysis demonstrated strong correlations between the three basic abilities, where language understanding and problem solving abilities have the strongest correlation ( 5.2).</p>
<p> We observed an implicit "Matthew effect" between model size and correlations of the abilities: larger models tend to have stronger correlations ( 5.3).</p>
<p> LLMs exhibited various error patterns in our fine-grained analysis pertaining to the three basic abilities ( 5.4).</p>
<p> MPA can be used as a data augmentation approach to improve the performance of LLMs ( 6).</p>
<p>The contributions of this paper are summarized as follows:</p>
<p> A psychometrics-inspired dynamic evaluation Protocol.MPA is general and flexible to mitigate data contamination and facilitate multifaceted analysis.</p>
<p> Comprehensive analysis of the basic abilities of LLMs.</p>
<p>Our modular design allows for the dynamic combination of the three basic cognitive abilities, providing systematic evaluation and analysis for future research.</p>
<p>Related Work</p>
<p>LLMs Evaluation &amp; Data Contamination.Various benchmarks have been introduced to assess LLMs (Hendrycks et al., 2021;Li et al., 2023a;Zhong et al., 2023;Hugging-Face, 2023;Chang et al., 2023), including: (1) Humancentric evaluation, typified by AdaVision (Gao et al., 2022) and AdaTest (Ribeiro &amp; Lundberg, 2022) that emphasize human-driven feedback.</p>
<p>(2) Crowd-sourcing, e.g., Dyn-aBench (Kiela et al., 2021) and DynaBoard (Ma et al., 2021), which prioritizes diverse and comprehensive evaluations through crowd-sourced tests.(3) More challenging tasks such as HELM (Liang et al., 2023), DeepTest (Tian et al., 2018) and CheckList (Ribeiro et al., 2020), create custom tests, while platforms such as Big-Bench (bench authors, 2023) aim to challenge LLMs with specialized tasks.</p>
<p>Recent research has highlighted the significant issue of data contamination in LLMs (Lovin, 2023;Bender et al., 2021;Koco et al., 2023;Li, 2023;Zhou et al., 2023).In particular, the reports of GPT-4 (OpenAI, 2023b), LLama (Touvron et al., 2023), and Skywork LLM (Wei et al., 2023) have acknowledged this phenomenon.Several researchers (Golchin &amp; Surdeanu, 2023a;b;Oren et al., 2023;Yang et al., 2023;Oren et al., 2023) developed methods to detect data contamination.Zhu et al. (2023a); Lei et al. (2023); Fan et al. (2023) Language understanding Problem solving Domain knowledge</p>
<p>Original benchmark</p>
<p>Probing benchmark</p>
<p>An astronomer observes that a planet rotates faster after a meteorite impact.Which is the most likely effect of this increase in rotation?</p>
<p>A: Planetary density will decrease.B: Planetary years will become longer.C: Planetary days will become shorter.(Correct answer) D: Planetary gravity will become stronger.</p>
<p>Basic cognitive ability</p>
<p>Probing principles In a distant solar system, astronomers detect a planet similar to Earth in terms of mass and composition.Following a significant event where the planet was struck by a rogue meteorite, which was noted to have a sizeable mass relative to the planet, the celestial body is now observed to have a quicker spin on its axis.</p>
<p>Considering the laws of conservation of angular momentum, what is the probable consequence of this accelerated rotational speed for the planet?</p>
<p>A: The duration of a single rotation on its axis will be reduced.(Correctanswer) B: The planet's mass will be distributed more widely.C: It will take longer for the planet to complete an orbit around its star.D: The planet will emit more heat due to increased rotational energy.E: The force exerted by the planet's mass will intensify.LLMs as Autonomous Agents.The adoption of LLMs as autonomous agents for task completion has recently gained popularity, such as AutoGPT (Significant-Gravitas, 2023) and MetaGPT (Hong et al., 2023), which have advanced our understanding of collaborative and planning abilities in LLMs.Another growing trend is the use of LLMs as judges (Dubois et al., 2023;Li et al., 2023b;Fernandes et al., 2023;Bai et al., 2023;Wang et al., 2024), where LLMs assess the output of other LLMs.Furthermore, there is a burgeoning interest in leveraging LLMs to enhance training datasets (Wang et al., 2022;Yu et al., 2023;Yuan et al., 2024;Liu &amp; Yao, 2024) and aligning LLMs' outputs with specific goals or values (Burns et al., 2023).Our work differs from them in designing psychometrically inspired principles for sample generation and support for multifaceted analysis.</p>
<p>Principle</p>
<p>Method</p>
<p>Overview</p>
<p>There are two critical challenges in designing a dynamic evaluation protocol.First, there is no general principle to guide the evaluation sample generation process for diverse tasks such as knowledge, language understanding, reasoning, and mathematics.Existing literature like Dy-Val (Zhu et al., 2023a) and NPHardEval (Fan et al., 2023) adopted manually designed principles to generate samples for specific tasks and cannot easily extend to other scenarios.Second, the principle of generating evaluation samples should be fine-grained yet atomic enough to analyze the multifaceted capabilities of LLMs.Our hope is that the evaluation should reflect the primitive abilities, and more fine-grained analysis of their correlations can be conducted.</p>
<p>In this work, we take inspiration from psychometrics (Raykov &amp; Marcoulides, 2011;Burnell et al., 2023) to generate evaluation samples using LLMs as agents.Specifically, instead of relying on specific rules like DyVal (Zhu et al., 2023a), we employ LLMs as agents to automatically generate new problems based on the given evaluation samples.This agent-based evaluation design can potentially fit most tasks.More importantly, psychometric theory categorizes cognitive abilities into three basic ones: language understanding to comprehend and generate texts, problemsolving to deal with complex problems, and domain knowledge.Therefore, as shown in Figure 2(a), we follow these criterion to design principles that not only change the problems but also assist in multifaceted analysis.</p>
<p>Our approach, termed as meta probing agents (MPA), is illustrated in Figure 2(b).Given an original evaluation sample from existing benchmarks, MPA aims to evaluate the ability of LLMs by creating new problems following psychometric theory.We create a set of principles that involve two agents: the probing agent and the judge agent.The probing agent aims to investigate the knowledge of a given question Q and return a new one according to a principle p i , and the judge agent is responsible for the validity and consistency check based on the original question.Their interaction features a feedback mechanism: if the judge agent determines that the new question lacks consistency ('No'), the probing agent is prompted again to generate another version of the question.Conversely, if the answer is 'Yes', the question is deemed to have passed the consistency check.</p>
<p>Judge</p>
<p>Your task is to analyze both the original question and the revised question and determine if they are effectively assessing the same concept or knowledge area.</p>
<p>Paraphrasing Choices</p>
<p>Language Understanding Probing I plan to paraphrase the choices: each choice should be paraphrased to reflect the same concept or idea as the original.The essence and meaning must be preserved.If a choice cannot be paraphrased without changing its meaning, it should be kept unchanged.</p>
<p>Judge</p>
<p>Your task is to analyze the paraphrased choices in the context of the question and determine if the paraphrased choices (A, B, C, D) still reflect the original meaning of their respective original choices.</p>
<p>Permuting</p>
<p>Judge</p>
<p>Your task is to analyze both the original question and the revised question and determine if they are effectively assessing the same concept or knowledge area.</p>
<p>Adding A New Choice Domain Knowledge Probing I plan to keep the choices A,B,C,D unchanged, and introduce an additional relevant choice E that is related to the topic but doesn't provide another correct answer.This choice should be plausible but clearly not correct in the context of the question.</p>
<p>Judge</p>
<p>Your task is to analyze the paraphrased choices in the context of the question and determine whether the new choice (E) is relevant to the question but does not provide an alternative correct answer.</p>
<p>The dynamic nature of MPA lies in two aspects: the dynamic generation of evaluation samples and the dynamic combination of principles.Such a dynamic combination allows for multifaceted analysis of LLMs' abilities, thus providing more insight for future research.It can also be seen as granting problems with flexible complexities (Zhu et al., 2023a) for a comprehensive evaluation.Now, we present the details of the agents and the psychometric principles.</p>
<p>Probing Agent</p>
<p>The probing agent aims to transform a given question into a new one to assess LLMs' ability required by a question Q.The probing is guided by principles inspired by psychometrics, which are encapsulated within a carefully crafted prompt.Unlike generating training samples, one of the most important criteria in probing agents is to ensure that the generated questions maintain the core essence of the original while presenting a different perspective, thus maintaining its correctness.An example is shown in Figure 2(c), where a sample of ARC-C is transformed into a new one by applying different principles.The prompts in our experiments incorporate directives that guide the agent towards creating semantically similar but structurally different questions.</p>
<p>Judge Agent</p>
<p>Although we have explicitly restricted the probing agent to maintain the integrity of the original question, there are cases where probing agents unintentionally change the meaning.Thus, the judge agent is designed to provide a clear and unambiguous assessment of whether the generated question maintains the integrity of the original intent and informational content.Its prompt is designed to direct LLMs to compare the original with the rephrased questions, ensuring the preservation of the essence and factual accuracy.Specifically, unlike traditional evaluation methods that may use a variety of metrics, the judge agent operates in an adversarial manner via a binary response system through prompts.It simply returns a 'Yes' or 'No' verdict, indicating whether the new question maintains consistency with the original.In this prompt, the judge agent is required to analyze the essence of both the original and rephrased questions.Its goal is not merely to identify superficial similarities or differences in wording, but to delve deeper into whether both versions of the question are aligned in terms of the concept or knowledge area they are assessing.</p>
<p>Human Verification For each ability (language understanding, problem solving, and domain knowledge), we randomly selected 500 samples from the MMLU dataset and 100 samples from the ARC-c dataset, totaling 1, 800 questions.30 human experts (with bachelor or higher degree) are divided into 3 groups, each with 10 person.They were asked to judge the following two questions: (1) whether the original and paraphrased questions were equivalent; (2) if the answers to the probing questions remained correct.The evaluation required a simple 'Yes' or 'No' answer.</p>
<p>The positive results shown in Appendix Table 9 from our human evaluation with an overall accuracy rate of 94% and 97% for each question, underscoring the effectiveness of our methodology.The following table provides a detailed breakdown of the evaluation outcomes, showcasing the high level of confidence in the equivalence and correctness of our probing questions across the three abilities.</p>
<p>Psychometric principles</p>
<p>Psychometric principles guide how we probe the understanding of a question.Inspired by psychometric theory (Raykov &amp; Marcoulides, 2011), we aim to evaluate three basic abilities of LLMs: language understanding, problem solving, and domain knowledge.We have identified five key principles that correspond to these categories, as shown in Table 1.These principles incorporate both a probing agent and a judge agent, as previously discussed, with their functions differing according to the specific principle applied.</p>
<p>LANGUAGE UNDERSTANDING</p>
<p>Language understanding assesses the ability to process, interpret, and generate texts.To evaluate this ability, we focus on how well LLMs can grasp the underlying meaning of various linguistic expressions and maintain its integrity when presented in different forms.We design three principles to evaluate this ability:</p>
<p> Principle 1: Paraphrasing Questions.It focuses on altering the phrasing of a question while retaining its core concept.This is achieved via a prompt that guides the probing agent to restructure the question without changing its underlying meaning.The new questions challenge LLMs in understanding to ensure that they grasp the essence of the question beyond surface-level recognition.</p>
<p> Principle 2: Paraphrasing Choices.It is similar to the first one, but applies to choices in a multiple-choice format.It involves rephrasing the options provided in a way that maintains their original intent and meaning. Principle 5: Adding A New Choice.It focuses on supplementing existing choices with an additional one.The new choice is relevant to this question, but is not a correct answer, which relies on domain knowledge to exclude.</p>
<p>Remark: MPA is not limited to these five principles and more can be added easily through our framework.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Tasks and Datasets.We selected four popular datasets for evaluation: MMLU (Hendrycks et al., 2021), ARC-Challenge (ARC-C) (Clark et al., 2018), GSM8K (Cobbe et al., 2021), and BigBench-Hard (BBH) (Suzgun et al., 2022;Srivastava et al., 2022), encompassing a broad spectrum of computational challenges ranging from knowledgeintensive understanding to complex mathematical and logical reasoning tasks.We adopted only three hard tasks from BBH: Formal Fallacies, Object Counting, and Temporal Sequences.We used their test sets to generate new evaluation samples.The detailed introduction is given in Appendix A.</p>
<p>Evaluated LLMs.We evaluated three proprietary LLMs: GPT-4-Turbo (OpenAI, 2023b), GPT-3.5-Turbo(OpenAI, 2023a), and Gemini-Pro (GeminiTeam, 2023), and three strong open-sourced models: Llama2-70b-chat (Touvron et al., 2023), Yi-34b-chat (01-ai, 2024), and Mixtral-8x7b-Instruct (MistralAITeam, 2023).To ensure a standardized comparison, we set the generation temperature to 0 for all models, with the generation length as 1000 tokens.2</p>
<p>Agent LLMs in MPA.We utilized GPT-4-Turbo as probing and judging agents, with temperatures of 0.7 and 0, respectively.The maximum token generation for each agent is set as 1000.While GPT-4-Turbo serves as the main agents, we also explored the potential to integrate other LLMs such as GPT-3.5-Turboand Gemini-Pro as agents in later experiments in Section 4.6.The evaluation prompts are detailed in the Appendix B. Our primary evaluation metric is accuracy.</p>
<p>A bitter reality is that currently, only GPT-4 are capable to generate questions and judge the quality of generated questions.We believe as the field progresses, more cheaper models (such as Claude 3 and Gemini) will become capable of fulfilling both probing and judging roles, thereby reducing dependency on any single model's API and enhancing the scalability.</p>
<p>Main Results</p>
<p>In this part, we applied all five principles to generate new evaluation samples for MMLU and ARC-C.For GSM8K and BBH that do not have multiple choices, we restricted our probing to Principle 1 and 4. Appendix D shows some examples generated by MPA.We repeated the evaluation three times to reduce randomness.Table 2 presented the test accuracy of different LLMs on the original and our MPA benchmarks.Standard deviation are mostly around 1 (Table 5), indicating the robustness of the benchmark.</p>
<p>As can be seen, all LLMs exhibited performance degradation on our probing benchmarks.Although GPT-4-Turbo demonstrated the strongest data contamination problem, it remains the strongest model.For MMLU, GPT-4-Turbo performed 15.7% worse than the original benchmark.Furthermore, a notable performance decline is evident in the case of MMLU and ARC-C, which is significantly more pronounced than that observed in GSM8K and BBH.This suggests that LLMs may encounter the memorization of knowledge-based benchmarks, resulting in substantial performance degradation for evaluation on our benchmarks.</p>
<p>We also presented a confusion matrix for analysis as shown in Figure 3.The matrix categorizes the responses into four distinct segments, with four categories to evaluate the model responses: OT (Original True), PT (Probing True), OF (Original False), and PF (Probing False).A notable observation is the high frequency of 'OT/PF' instances, which suggests a potential data contamination to specific dataset characteristics.Furthermore, the frequency of open-source models is markedly higher than that of proprietary models like GPT-4.This discrepancy indicates that open-source models might be more susceptible to data contamination.</p>
<p>Effect of Different Probing Principles</p>
<p>We further studied the effects of each modular probing principle.To this end, we established a baseline where the combined effect of all principles leads to a decrease in performance, normalized to a value of "1".This approach enables us to compare the relative effectiveness of each  principle in isolation.The performance decrement for each principle, when applied independently, was evaluated and compared with this baseline.The Relative effectiveness (RE) is computed as: RE = Accp i Acc Accp all Acc , where Acc pi is the accuracy when only apply principle p i to MPA, Acc pall denotes the accuracy of MPA when all principles are applied.Acc is the accuracy on the original benchmark.</p>
<p>The results on MMLU and ARC-C datasets are detailed in Figure 4 and Appendix C.2.It can be observed that, principle 1, 2 and 5 are the most effective principles.While principle 3, which randomly permute choices, are less effective.For GPT-4-Turbo and GPT-3.5-Turbo on ARC-C dataset, it can intriguingly increase the performance.</p>
<p>Ablation Study on Prompt Engineering</p>
<p>We explored the efficacy of prompt engineering techniques, specifically Chain-of-Thought (CoT) (Wei et al., 2022) and In-Context Learning (ICL) (Brown et al., 2020).As can be observed in Table 3, neither CoT nor ICL can effectively boost the performance of LLMs on our probing benchmarks.Performance enhancements are varied across models and datasets when CoT and ICL are applied.For instance, GPT-4-Turbo showed a modest increase in accuracy on the GSM8K dataset with ICL, improving from 88.50 to 89.39.In contrast, GPT-3.5-Turbo'sperformance slightly decreased under the same conditions.Note that the significant performance gain observed for Gemini-Pro on the GSM8K dataset when using ICL is attributed to its initial lower effectiveness in a zero-shot context.</p>
<p>Albation Study on Data Contamination</p>
<p>We collected 30 novel reasoning questions from the experts (the same experts in the human evaluation in the general response), and examined the performance of the original questions and the probing questions.The GPT-4's accuracies of the original questions and the probing questions are both 60%.The OT/PF ratio is 3%, which is much lower than those in common public benchmarks, indicating that the newly generated benchmarks are not likely to be memorized compared to the public benchmarks.</p>
<p>Weak LLMs as Probing and Judge Agents</p>
<p>In this section, we assess the feasibility of using less advanced LLMs to reduce costs of MPA evaluation.We initially configured GPT-3.5-Turbo and Gemini-Pro as judging agents, while maintaining GPT-4-Turbo as the probing agent for GSM8K dataset.Through meticulous manual examination of the questions transformed by this setup, we observed a significant shortfall in the judging agents' ability to discern and exclude transformed examples whose meanings deviated from the original questions.highlight- LU+PS+DK represent probing benchmarks that applied language understanding principles, both language understanding principles and problem solving principles, and all principles, respectively.</p>
<p>ing the need for robust and capable LLMs to effectively sieve out appropriately probing questions.Subsequently, we extended our inquiry to assess the potential of employing less capable LLMs like GPT-3.5-Turbo and Gemini-Pro as probing agents.Using GPT-4-Turbo as the judging agent, our experiments revealed that when weaker LLMs served as probing agents, they often altered the essence or subtleties of the original questions, leading to misrepresentations or the introduction of unintended elements.And GPT-4-Turbo struggled to consistently detect these nuanced alterations.This suggests that the sophistication of the probing agent is crucial, as even advanced judging agents like GPT-4-Turbo cannot always offset the limitations of the probing agents.</p>
<p>Multifaceted Analysis of the Basic Abilities</p>
<p>One advantage of MPA is the support for multifaceted analysis of abilities, which is discussed in this section.</p>
<p>Analysis on Benchmark Complexity</p>
<p>We first explore the impact of benchmark complexity on the MMLU and ARC-C datasets, as illustrated in Figure 5.We construct probing benchmarks with different levels of complexity using (1) language understanding principle 1, (2) language understanding principle 1 and problem-solving principle 4, and (3) language understanding principle 1, problem-solving principle 4, and domain knowledge principle 5.In particular, as the complexity of the benchmarks increases, the performance of all LLMs decreases and GPT-4-Turbo consistently reaches the highest precision.This result shows that there is still much room to improve the abilities of LLMs in complex benchmarks.</p>
<p>Relationship of the Basic Abilities</p>
<p>To gain a deep understanding of the relationship between the abilities of language understanding (LU), problem solving (PS), and domain knowledge (DK), we constructed three probing benchmarks using the principles that belong to a certain ability and then evaluated LLMs on these benchmarks.</p>
<p>The results are presented in Table 8.After obtaining the performance of different LLMs on each probing benchmark, we then calculated the Pearson correlation coefficients (Pearson, 1920), the Spearman correlation coefficients (Spearman, 1904), and the Kendall correlation coefficients (Kendall, 1938), the results are presented in Table 4.It is evident that all abilities are highly correlated, which aligns with the findings of Burnell et al. (2023).Furthermore, it is observed that language understanding and problem solving are more relevant compared to other pairs of abilities.This suggests that the two abilities can predict each other, which has great potential to train and improve LLMs in the future.In the future, further detailed analysis can be performed to gain a deeper insight into the basic abilities of LLMs by constructing MPA evaluation samples based on other existing benchmarks such as HELM (Liang et al., 2023).</p>
<p>Analysis on Model Size</p>
<p>We studied the influence of model size on basic abilities.First, Figure 6(a) shows the correlation with variants of Llama2: 7b, 13b, and 70b.It can be observed that each ability positively correlates with the model size with nearly the same slope, indicating that when the size of the model increases, all abilities are equally enhanced to improve overall performance.Second, we explored the relationship between model size and correlations between different abilities.We roughly divided the models into three sizes:</p>
<p>(1) small: Llama2-7b-chat, Llama2-13b-chat; (2) mid: Yi-34b-chat, Mixtral-8x7b-Instruct, Llama2-70b-chat; and (3) Gemini-Pro, GPT-3.5-Turbo,GPT-4-Turbo.The results in Figure 6(b) implies an implicit "Matthew Effect" (Merton, 1968): larger (often stronger) models tend to have stronger correlations between basic abilities.This aligns well with existing psychological theory about the g factor of general intelligence (Spearman, 1961).This finding can potentially help explain the emergent abilities of LLMs (Biderman et al., 2023;Schaeffer et al., 2023) and provide insight into the evolution of LLMs.</p>
<p>Error Analysis</p>
<p>We conducted an in-depth analysis of LLMs' failure modes in the three basic abilities.We meticulously selected 50 instances where GPT-4-Turbo correctly answered the original questions but failed in the transformed questions in the GSM8K dataset.These error modes are shown below.</p>
<p> Language understanding.</p>
<p>(1) Question Understanding Error: GPT-4-Turbo calculates the correct answer but misinterprets the intent of the question, leading to an incorrect response.This error indicates a gap in comprehension of the question.</p>
<p>(2) Instruction Following Error:</p>
<p>In this mode, GPT-4-Turbo arrives at the correct answer but fails to present it in the required format specified in the prompt, indicating a lack of follow-up instructions.</p>
<p> Problem solving.Here, GPT-4-Turbo understands the question correctly but errs during the calculation process, resulting in a wrong final answer.</p>
<p> Domain knowledge.We investigated the distribution of topic error among 57 tasks of MMLU in Figure 9. Notably, the professional law domain has the highest error rate, followed by moral scenarios and professional psychology, suggesting challenges predominantly in professional and ethical tasks.Furthermore, we observed two possible reasons for performance degradation: ambiguity in the original questions and inconsistency between the probing and original questions.Some questions in the original dataset were found to be ambiguous (see Appendix C.5).Despite this, GPT-4-Turbo often provided plausible answers, suggesting potential issues with data memorization.However, certain transformed questions deviated in meaning from their original versions, leading to inconsistencies in responses.</p>
<p>MPA as Data Augmentation for LLMs</p>
<p>Although the main purpose of MPA is to evaluate LLMs, its generated samples can also be used as augmented data for fine-tuning.In this section, we conducted a pilot study using the OpenAI API to fine-tune GPT-3.5-Turbo on the data generated by MPA.Specifically, we used samples from the training split of MMLU and ARC-C, which are fed to MPA for data generation.We then evaluated the fine-tuned  models on the original test split and our probing benchmarks.The fine-tuning data includes two parts, the first part is the probing questions generated by 5 principles separately, and the second part is the original training set.</p>
<p>The results in Figure 8 show that the data generated by MPA can improve the performance of LLMs, with an average of 2% improvements on both MMLU and ARC-C.</p>
<p>The fine-tuning results demonstrated that MPA is not only an evaluation protocol, but a general data augmentation approach to improve the performance of LLMs, creating a huge advantage for training stronger LLMs in the future.</p>
<p>The improved results also demonstrate the correctness of the generated data, indicating that our MPA is effective.</p>
<p>Conclusion and Discussion</p>
<p>This paper introduced MPA, a dynamic evaluation protocol to address data contamination and provide an in-depth analysis of the three key cognitive abilities of LLMs inspired by psychometric theory.Our experimental findings revealed several notable insights.Crucially, MPA-generated samples can not only function as evaluation tools, but also improve LLMs training as a data augmentation method.We believe that the psychometric-inspired adoption of LLMs as agents represents a promising direction.</p>
<p>Our work has several limitations.(1) Tasks and datasets: Our focus was limited to four datasets, encompassing a specific range of topics.Incorporating a broader spectrum of datasets and tasks could yield more comprehensive insights into LLMs capabilities.</p>
<p>(2) The validity of probing benchmarks: While MPA employs a judge agent to assess the consistency and accuracy of probing benchmarks, we observed discrepancies in some questions, deviating from their original intent.This highlights the potential to further enhance MPA's robustness and effectiveness.</p>
<p>Impact Statement</p>
<p>Evaluating the general abilities of LLMs is essential to ensure responsible AI for the society.This work proposed a new evaluation protocol of LLMs to ensure that their true capabilities can be measured, which will help foster a better understanding of the models.We carefully controlled the generative models (agents) in the paper to ensure that they will not generate harmful content.</p>
<p>A. Datasets</p>
<p>The MMLU dataset contains 13, 985 test samples across 57 tasks, encompassing diverse areas such as humanities and social sciences, offering a comprehensive assessment of language understanding capabilities.The ARC-C dataset collected 1, 172 grade-school level science questions, presenting a unique blend of natural language understanding and scientific reasoning.</p>
<p>In the GSM8K dataset, the focus is on mathematical problem-solving, featuring 1, 319 problems that require a combination of numerical understanding and logical reasoning.For the Formal Fallacies, Object Counting, and Temporal Sequences tasks in BBH dataset, each contains 250 test samples.These subsets were chosen for their relevance and representativeness, as they challenge LLMs to understand nuanced logical fallacies, accurately count objects in complex settings, and understand sequences of events over time.</p>
<p>B. Evaluation Prompts</p>
<p>In the following, we show the evaluation prompts while adopting different datasets.</p>
<p>MMLU</p>
<p>Here is a question about {task}: {question} {choices}</p>
<p>Choose the correct answer and explain why.Please include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if you choose A, please write &lt;&lt;<A>&gt;&gt;.</p>
<p>GSM8K</p>
<p>Here is a math problem:{question} Please solve this math problem and include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if your answer is 1, please write &lt;&lt;&lt;1&gt;&gt;&gt;.</p>
<p>ARC-C</p>
<p>Here is a multiple-choice science problem:</p>
<h3>Question: {question} ### Choices: {choices} Please solve this problem and include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if your choose A, please write &lt;&lt;<A>&gt;&gt;.</h3>
<p>BBH (formal fallaices)</p>
<p>Here is a question about formal fallacies (given a context involving a set of statements, determine whether an argument can be logically deduced from the provided context):</p>
<h3>Question: {question} ### Choices:{choices}</h3>
<p>Please answer this question and include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if your answer is valid, please write &lt;&lt;<valid>&gt;&gt;.</p>
<p>BBH (object counting)</p>
<p>Here is a question about object counting (given a collection of possessions that a person has along with their quantities, determine the number of a certain object/item class.):</p>
<p>{question}</p>
<p>C.5. Examples of wrong/ambiguous evaluation samples</p>
<p>In this section, we presented several wrong/ambiguous evaluation samples in GSM8K dataset.</p>
<p> Question:</p>
<p>Lee used to be able to run the 400-meter hurdles two seconds faster than Gerald would run the 400-meter hurdles.But Gerald changed his diet, which improved his speed by 10%.If Lee runs the 400-meter hurdles in 38 seconds, how fast can Gerald, with his improved diet, run the 400-meter hurdles, in seconds?</p>
<p> Answer: 36</p>
<p> Analysis: The correct answer is 36.3636.</p>
<p> Question:</p>
<p>Mandy owes Benedict $100.They agreed to have monthly interest of 2%.If Mandy was able to pay it after 3 months, how much should she give to Benedict?</p>
<p> Answer: 106</p>
<p> Analysis: The financial arrangement between Mandy and Benedict involves a principal loan of $100 with an agreed monthly interest rate of 2%.The ambiguity in the original question arises from the lack of specificity regarding the interest calculation method: simple or compound.The provided answer ($106) initially suggests a simple interest calculation.However, considering the possibility of compound interest sheds light on a different approach to determining the final amount owed.</p>
<p>eggs to be sold at the bustling local farmers' market.Her eggs are quite popular, and she sells them for $2 each.Given her daily routine, how much money does Janet make from selling her eggs at the farmers' market each day?</p>
<p>Answer: 18 ARC-C:</p>
<p> Original Question: An astronomer observes that a planet rotates faster after a meteorite impact.Which is the most likely effect of this increase in rotation?</p>
<p>A: Planetary density will decrease.</p>
<p>B: Planetary years will become longer.</p>
<p>C: Planetary days will become shorter.</p>
<p>D: Planetary gravity will become stronger.</p>
<p>Answer: C</p>
<p> Probing Question:</p>
<p>In the vast expanse of the solar system, where celestial bodies are constantly in motion, a planet's day-to-day existence can be altered by events such as collisions with other objects.Imagine a scenario where astronomers witness a planet whose day has been significantly shortened due to the impact of a meteorite.This incident has resulted in the planet having a quicker rotational period around its axis.Given this situation, what is the probable consequence of this accelerated spin on the planet's environment or physical state?</p>
<p>A: The time it takes for the planet to orbit the sun will increase.B: The duration of a single rotation of the planet on its axis will be less.</p>
<p>C: The planet's atmosphere will become significantly thicker due to increased centrifugal force.D: The force with which the planet pulls objects towards itself will intensify.E: The mass per unit volume of the planet will be reduced.</p>
<p>Answer: B</p>
<p>BBH (formal fallacies):</p>
<p> Original Question:</p>
<p>Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla.In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra.</p>
<p>Is the argument, given the explicitly stated premises, deductively valid or invalid?Options:</p>
<p>-valid -invalid Answer: invalid</p>
<p> Probing</p>
<p>Question:</p>
<p>At Ridgemont High, a peculiar rule is established by the student council: anyone who is a classmate of Sondra cannot concurrently be a half-sibling of Pricilla.This rule came about after a complex dispute over club memberships and family connections within the school.Now, consider a debate that erupted in the philosophy club during a discussion on logical reasoning.One of the members presented what seemed to be a sound argument based on the student council's rule: if it is true that no classmate of Sondra can be a half-sibling of Pricilla, then it logically follows that anyone who is not a half-sibling of Pricilla must be a classmate of Sondra.The club is now pondering whether this argument is logically coherent and deductively sound, based on the premises provided by the student council's peculiar rule.</p>
<p>Options:</p>
<p>-valid -invalid Answer: invalid</p>
<p>41 st International Conference on Machine Learning, Vienna, Austria.PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>Figure 1 .
1
Figure1.Performance of different LLMs on vanilla MMLU testset and our probing benchmarks based on the MMLU.LU, PS, and DK denote the evaluation sets to evaluate language understanding, problem solving, and domain knowledge ability, respectively.</p>
<p> 1 :
1
Paraphrasing questions  2 : Paraphrasing choices  3 : Permuting choices  4 : Adding extra context into questions  5 : Adding a new choice</p>
<p>Figure 3 .Figure 4 .
34
Figure 3.The confusion matrix of original benchmarks and probing benchmarks on ARC-C dataset.</p>
<p>For ICL, we selected five examples from the corresponding training set as few-shot examples.We also applied MPA to these five examples, creating a transformed version of few-shot examples.We refer to the use of ICL with the original training examples as ICL o and the use of ICL with the</p>
<p>Figure 5 .
5
Figure5.The accuracy of different LLMs on ARC-C and MMLU on different levels of probing benchmarks.LU, LU+PS, and LU+PS+DK represent probing benchmarks that applied language understanding principles, both language understanding principles and problem solving principles, and all principles, respectively.</p>
<p>Figure 6 .
6
Figure 6.(a) The correlation between the performance and model size.(b) Cross-ability correlation with model size.</p>
<p>Figure 7 .Figure 8 .
78
Figure 7.The bar chart of top 20 error topics and their corresponding frequencies of GPT-4-Turbo on MMLU dataset.</p>
<p>Figure 9 .
9
Figure 9.The bar chart of top 20 topics and their corresponding frequencies of GPT-4-Turbo on MMLU dataset.</p>
<p>a) From psychometrics theory to probing principles (b) Meta Probing Agents (c) A working example</p>
<p>Judge Agent Multi-round probing and judge Probing Agent Principle   Judge agent Probing agent Multi-round probing and judge Dynamic configuration (Figure 2. Inspired by psychometric theory on the three basic cognitive abilities, our Meta Probing Agent (MPA) designs corresponding Original Transformed sampleprinciples that transforms original benchmarks into a new one. These principles can be flexibly combined to create various probingbenchmarks for multifaceted analysis. Subfigure (c) shows how MPA generates the new sample given an existing sample from ARC-C.introduced dynamic evaluation strategies via different al-gorithms to reduce data contamination. MPA significantlydiffers from them in the sample generation mechanism andthe support for multifaceted analysis.</p>
<p>Table 1 .
1
The prompts for different principles based on the three cognitive abilities.I plan to paraphrase the question to present the same concept in a different way.Please assist me in paraphrasing the question.
PrincipleAbilityAgent TypePromptParaphrasingLanguageProbingQuestionUnderstanding</p>
<p>add non-essential context to the question: introducing context or details to the question that are relevant but not directly helpful in answering it.The context can be put at the beginning, middle, or end of the question.
ChoicesLanguage Understanding-(This principle does not require agents and can be implemented directly in the code)I plan toProbingAdding Extra ContextProbleminto QuestionSolving</p>
<p>It aims to introduce additional, non-essential context to the question, which is relevant to the topic, but does not directly aid in answering the question.The prompt guides the probing agent to seamlessly integrate extra context into the original question.The new questions assess whether LLMs can filter out extraneous information and focus on the key elements to solve the problem.3.4.3.DOMAIN KNOWLEDGEDomain knowledge refers to the depth and accuracy of the model's knowledge in specific areas.It is crucial not only to have a broad understanding of general concepts, but also to possess detailed, nuanced knowledge.It tests the model's expertise in various domains, its ability to differentiate between closely related concepts, and to apply this knowledge appropriately in context-specific scenarios.
3.4.2. PROBLEM SOLVINGProblem solving refers to the ability to analyze, deduce, andderive answers. It involves critical thinking, distinguish-ing relevant from irrelevant data, and applying knowledgeto new situations. Principles under this category test themodel's ability in navigating complex, often nuanced sce-narios, and its proficiency in delivering solutions. Note thatwe cannot create completely new problems by the agent,since their correctness cannot be guaranteed. Therefore, wedesign one general principle:
(Zong et al., 2023)uting Choices(Zong et al., 2023).This principle simply involves rearranging the order of the choices in a multiple-choice question.It determines if the model's understanding is influenced by the position of the correct answer.As it can be achieved through coding, specific prompts are not required for this principle.Principle 4: Adding Extra Context into Questions.</p>
<p>Table 2 .
2
The performance of different LLMs on vanilla benchmarks and our probing benchmarks.
ModelGPT-4-TurboGPT-3.5-TurboGemini ProYi-34bMixtral-8x7bLlama2-70b-chatDatasetVanilla OursVanilla OursVanilla OursVanilla OursVanilla OursVanilla Ours</p>
<p>Table 3 .
3
Results of different prompt engineering techniques for GSM8K and ARC-C dataset, with the highest and second-highest accuracies highlighted in bold and underlined, respectively.
DatasetModelOriginal CoTICL o ICL tGPT-4-Turbo88.5089.31 89.39 88.78GSM8KGPT-3.5-Turbo71.5470.58 65.73 64.90Gemini-Pro20.3924.49 75.28 73.01GPT-4-Turbo84.6782.85 85.32 85.67ARC-CGPT-3.5-Turbo74.6075.94 74.32 74.49Gemini-Pro75.9176.02 76.71 79.10transformed examples as ICL t .</p>
<p>Table 4 .
4
The correlation efficient of the three basic abilities.
Pearson Spearman KendallLU &amp; PS0.9940.9860.939LU &amp; DK0.9860.9720.909PS &amp; DK0.9860.9790.909Accuracy40 607b-chat13b-chat70b-chat</p>
<p>Table 8 .
8
Results of different LLMs on MPA based on ARC-C and MMLU datasets.
DatasetARC-CMMLUModelLanguage understanding Problem solving Domain knowledge Language understanding Problem solving Domain knowledgeGPT-4-Turbo90.2794.2893.6975.1881.4881.42GPT-3.5-Turbo79.1884.6482.2561.0267.3964.61Gemini-Pro80.4684.8181.8359.5366.2762.77Yi-34b-chat79.4485.6783.1960.0166.1064.50Mixtral-8x7b-Instruct78.1682.1778.5861.1866.0161.64Llama2-70b-chat70.1475.0068.9454.5457.6054.23respect to the ethics and psychology domains.
Most benchmarks adopt the QA style. For non-QA benchmarks such as GSM8K, the choice-related principles do not apply.
For Gemini-Pro on MMLU dataset, we set the temperature to 0.7 and omitted some evaluation samples (around 20 samples) to avoid response failures due to Google's safety constraints.
https://github.com/microsoft/promptbench.Please answer this question and include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if your answer is 1, please write &lt;&lt;&lt;1&gt;&gt;&gt;.BBH (temporal sequences)Here is a question about temporal sequences (given a series of events and activities a person has completed in the course of a day, determine what time, during the day, they might have been free to perform another activity.):### Question: {question} ### Choices: {choices} Please answer this question and include your answer into &lt;&lt;&lt;&gt;&gt;&gt;.For example, if your answer is (A), please write &lt;&lt;&lt;(A)&gt;&gt;&gt;.C. Detailed ResultsC.1. Standard Deviation of Main ResultsThe dynamic evaluation protocol introduces randomness into the evaluation results.Therefore, we run all experiments three times to get the average results and the standard error.As shown in Table5, the standard deviations for all models in all data sets are small, thus ensuring the fairness of our evaluation.C.2. Results of Different Modular PrinciplesWe show the results on different principles in Table6 and Table 7.Note that we only adopted partial samples from BBH.C.3. Results of Relationship of the Basic AbilitiesTable8shows the results on different abilities.C.4. Top topics of MMLUFigure9shows the top 20 MMLU topics where GPT-4-Turbo made the most errors.It can be observed that GPT-4-Turbo made more mistakes in "profession law", "moral scenarios", and "security studies", potentially due to insufficient training data and ambiguious groundtruth in these domains.For example, questions from "moral scenarios" are often difficult to answer.This trend underscores potential limitations in GPT-4-Turbo's current understanding or processing capabilities withD. Examples generated by MPAFinally, we show some examples generated by MPA on the basis of different benchmarks.MMLU: Original Question: This question refers to the following information.Read the the following quotation to answer questions.The various modes of worship which prevailed in the Roman world were all considered by the people as equally true; by the philosopher as equally false; and by the magistrate as equally useful.Edward Gibbon, The Decline and Fall of the Roman Empire, 1776{178Gibbon's interpretation of the state of religious worship in ancient Rome could be summarized as A: In ancient Rome, religious worship was decentralized and tended to vary with one's social position.B: In ancient Rome, religious worship was the source of much social tension and turmoil.C: In ancient Rome, religious worship was homogeneous and highly centralized.D: In ancient Rome, religious worship was revolutionized by the introduction of Christianity.Answer: A Probing Question:In his seminal work ''The Decline and Fall of the Roman Empire,'' Edward Gibbon explores the vast expanse of Roman history, from its zenith to its eventual collapse.Amid his detailed examination, Gibbon provides insights into the cultural and religious diversity that characterized the Roman Empire.How does he describe the differing perspectives of the general public, philosophers, and government officials regarding this religious diversity, particularly in terms of their acceptance and the impact on Roman society?A: Religious worship in ancient Rome was uniform and controlled by a central authority.B: The multiple forms of religious worship in ancient Rome often led to social conflicts and disturbances.C: The Roman state endorsed all forms of worship equally in an attempt to appease the gods and ensure the empire's prosperity.D: The arrival of Christianity in ancient Rome was a transformative force that completely changed the nature of religious worship.E: Religious practices in ancient Rome were not centralized, and they varied according to the social status of an individual.Answer: E GSM8K: Original Question:Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?Answer: 18 Probing Question: Janet has a small farm where she raises a variety of animals, but her ducks are the most productive when it comes to laying eggs.Each day, without fail, her flock of ducks provides her with 16 fresh eggs.Janet has a particular routine where she enjoys a hearty breakfast that always includes three scrambled eggs.After breakfast, she dedicates some time to baking, preparing four delicious muffins that she shares with her friends.These muffins are special because they each require one egg.After using the eggs for her breakfast and baking, Janet packages the remaining BBH (object counting): Original Question:I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet.How many musical instruments do I have?Answer: 8 ProbingQuestion:As a passionate collector and music enthusiast, I've dedicated a significant portion of my living space to housing various items that reflect my interests and hobbies.Over the years, I've amassed a collection that includes both musical instruments and household items.Among my cherished possessions are one flute, one piano, one trombone, one violin, one accordion, one clarinet, one drum, and one trumpet.In addition to these, my practical side has led me to acquire four stoves and two lamps to meet my daily needs.Given this eclectic mix of items, can you carefully count and tell me how many items from my collection are musical instruments?Answer: 8 BBH (temporal sequences): Original Question:  Probing Question: On which occasion during her busy schedule could Susan have potentially squeezed in a visit to the local coffee shop?Susan's day kicked off at the crack of dawn, at 7am. Between the early hours of 7am and the late morning at 11am, Linda witnessed Susan making her way to the refreshing water park, where she was set to enjoy the slides and pools.During the late morning hour, from 11am to noon, John caught a glimpse of Susan amidst the bustling shoppers at the mall, where she was selecting some fashionable clothing items.As the clock struck noon and the day progressed to 1pm, Jessica was with Susan, snapping pictures against the backdrop of the iconic Eiffel Tower at a well-visited tourist spot.Following her tourist escapades, from 1pm to 2pm, Steven saw Susan at the cozy deli downtown, where she was deciding on her midday meal from a variety of savory options.Later in the afternoon, from 2pm to 6pm, Thomas noticed Susan deeply engrossed in literature at the quiet library, a place where she often finds solace in the pages of her favorite books.It's also important to note that the coffee shop in question shuts down its espresso machines and locks its doors to customers promptly at 9pm.Given Susan's known whereabouts throughout the day, deduce the time interval where she could have enjoyed a coffee shop visit.
-Ai Yi, A series of large language models. 2024</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Y Bai, J Ying, Y Cao, X Lv, Y He, X Wang, J Yu, K Zeng, Y Xiao, H Lyu, arXiv:2306.04181Transactions on Machine Learning Research. 2835-88562023. 2023arXiv preprintBenchmarking foundation models with language-model-as-an-examiner</p>
<p>On the dangers of stochastic parrots: Can language models be too big? FAccT 2021. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 2021Association for Computing MachineryNew York, NY, USAISBN 9781450383097</p>
<p>The reversal curse: Llms trained on "a is b" fail to learn "b is a. L Berglund, M Tong, M Kaufmann, M Balesni, A C Stickland, T Korbak, O Evans, arXiv:2309.122882023arXiv preprint</p>
<p>S Biderman, U S Prashanth, L Sutawika, H Schoelkopf, Q Anthony, S Purohit, Raf , E , arXiv:2304.11158Emergent and predictable memorization in large language models. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>R Burnell, H Hao, A R Conway, J H Orallo, arXiv:2306.10062Revealing the structure of language model capabilities. 2023arXiv preprint</p>
<p>Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. C Burns, P Izmailov, J H Kirchner, B Baker, L Gao, L Aschenbrenner, Y Chen, A Ecoffet, M Joglekar, J Leike, arXiv:2312.093902023arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.031092023arXiv preprint</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Y Dubois, X Li, R Taori, T Zhang, I Gulrajani, J Ba, C Guestrin, P Liang, T B Hashimoto, arXiv:2305.143872023arXiv preprint</p>
<p>L Fan, W Hua, L Li, H Ling, Y Zhang, L Hemphill, arXiv:2312.14890Dynamic benchmark on reasoning ability of large language models via complexity classes. 2023arXiv preprint</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. P Fernandes, D Deutsch, M Finkelstein, P Riley, A F Martins, G Neubig, A Garg, J H Clark, M Freitag, O Firat, arXiv:2308.072862023arXiv preprint</p>
<p>I Gao, G Ilharco, S Lundberg, M T Ribeiro, arXiv:2212.02774arXiv:2312.11805GeminiTeam. Gemini: a family of highly capable multimodal models. 2022. 2023arXiv preprintAdaptive testing of computer vision models</p>
<p>Data contamination quiz: A tool to detect and estimate contamination in large language models. S Golchin, M Surdeanu, arXiv:2311.062332023aarXiv preprint</p>
<p>S Golchin, M Surdeanu, arXiv:2308.08493Time travel in llms: Tracing data contamination in large language models. 2023barXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>S Hong, X Zheng, J Chen, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Open-source large language models leaderboard. Huggingface, 2023</p>
<p>A new measure of rank correlation. M G Kendall, Biometrika. 301/21938</p>
<p>Dynabench: Rethinking benchmarking in NLP. D Kiela, M Bartolo, Y Nie, D Kaushik, A Geiger, Z Wu, B Vidgen, G Prasad, A Singh, P Ringshia, Z Ma, T Thrush, S Riedel, Z Waseem, P Stenetorp, R Jia, M Bansal, C Potts, A Williams, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language TechnologiesJune 2021</p>
<p>. J Koco, I Cichecki, O Kaszyca, M Kochanek, D Szydo, J Baran, J Bielaniewicz, M Gruza, A Janz, K Kanclerz, 2023Jack of all trades, master of none. Information Fusion101861</p>
<p>S3eval: A synthetic, scalable, systematic evaluation suite for large language models. F Lei, Q Liu, Y Huang, S He, J Zhao, K Liu, arXiv:2310.151472023arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, 2023a</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, 2023b</p>
<p>An open source data contamination report for llama series models. Y Li, arXiv:2310.175892023arXiv preprint</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, B Newman, B Yuan, B Yan, C Zhang, C A Cosgrove, C D Manning, C Re, D Acosta-Navas, D A Hudson, E Zelikman, E Durmus, F Ladhak, F Rong, H Ren, H Yao, J Wang, K Santhanam, L Orr, L Zheng, M Yuksekgonul, M Suzgun, N Kim, N Guha, N S Chatterji, O Khattab, P Henderson, Q Huang, R A Chi, S M Xie, S Santurkar, S Ganguli, T Hashimoto, T Icard, T Zhang, V Chaudhary, W Wang, X Li, Y Mai, Y Zhang, Y Koreeda, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Augmenting math word problems via iterative question composing. H Liu, A C Yao, -C , arXiv:2401.090032024arXiv preprint</p>
<p>Gpt-4 performs significantly worse on coding problems not in its training data. B Lovin, 2023</p>
<p>Dynaboard: An evaluation-as-a-service platform for holistic nextgeneration benchmarking. Z Ma, K Ethayarajh, T Thrush, S Jain, L Wu, R Jia, C Potts, A Williams, D Kiela, Advances in Neural Information Processing Systems. 202134</p>
<p>The matthew effect in science: The reward and communication systems of science are considered. R K Merton, Science. 15938101968</p>
<p>MistralAITeam. Mixtral-8x7b-v0.1OpenAI. Gpt-4 technical report. 2023a. 2023b</p>
<p>Y Oren, N Meister, N Chatterji, F Ladhak, T B Hashimoto, arXiv:2310.17623Proving test set contamination in black box language models. 2023arXiv preprint</p>
<p>The history and theory of correlation. K Pearson, Biometrika Office. 1920</p>
<p>Introduction to psychometric theory. T Raykov, G A Marcoulides, 2011Routledge</p>
<p>Adaptive testing and debugging of nlp models. M T Ribeiro, S Lundberg, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. M T Ribeiro, T Wu, C Guestrin, S Singh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Are emergent abilities of large language models a mirage? In NeurIPS. R Schaeffer, B Miranda, S Koyejo, 2023</p>
<p>. Significant-Gravitas, Autogpt, 2023</p>
<p>The proof and measurement of association between two things. C Spearman, The American Journal of Psychology. 1511904</p>
<p>general intelligence" objectively determined and measured. C Spearman, 1961</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schrli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, arXiv:2210.092612022arXiv preprint</p>
<p>Automated testing of deep-neural-network-driven autonomous cars. Y Tian, K Pei, S Jana, B Ray, Deeptest, Proceedings of the 40th international conference on software engineering. the 40th international conference on software engineering2018</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>B Wang, W Chen, H Pei, C Xie, M Kang, C Zhang, C Xu, Z Xiong, R Dutta, R Schaeffer, arXiv:2306.11698Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 2023arXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Y Wang, Z Yu, Z Zeng, L Yang, C Wang, H Chen, C Jiang, R Xie, J Wang, X Xie, W Ye, S Zhang, Y Zhang, 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>T Wei, L Zhao, L Zhang, B Zhu, L Wang, H Yang, B Li, C Cheng, W L, R Hu, arXiv:2310.19341A more open bilingual foundation model. 2023arXiv preprint</p>
<p>Glue-x: Evaluating natural language understanding models from an out-ofdistribution generalization perspective. L Yang, S Zhang, L Qin, Y Li, Y Wang, H Liu, J Wang, X Xie, Y Zhang, arXiv:2211.080732022arXiv preprint</p>
<p>Rethinking benchmark and contamination for language models with rephrased samples. S Yang, W.-L Chiang, L Zheng, J E Gonzalez, I Stoica, arXiv:2311.048502023arXiv preprint</p>
<p>L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>W Yuan, R Y Pang, K Cho, S Sukhbaatar, J Xu, J Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Don't make your llm an evaluation benchmark cheater. K Zhou, Y Zhu, Z Chen, W Chen, W X Zhao, X Chen, Y Lin, J.-R Wen, J Han, arXiv:2311.019642023arXiv preprint</p>
<p>K Zhu, J Chen, J Wang, N Z Gong, D Yang, X Xie, arXiv:2309.17167Graph-informed dynamic evaluation of large language models. 2023aarXiv preprint</p>
<p>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. K Zhu, J Wang, J Zhou, Z Wang, H Chen, Y Wang, L Yang, W Ye, N Z Gong, Y Zhang, arXiv:2306.045282023barXiv preprint</p>
<p>Fool your (vision and) language model with embarrassingly simple permutations. Y Zong, T Yu, B Zhao, R Chavhan, T Hospedales, arXiv:2310.016512023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
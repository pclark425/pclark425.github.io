<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3293 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3293</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3293</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-57100e39d0413ee585b381ba9ab366e8a6cf2866</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/57100e39d0413ee585b381ba9ab366e8a6cf2866" target="_blank">Solving Math Word Problems by Combining Language Models With Symbolic Solvers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations.</p>
                <p><strong>Paper Abstract:</strong> Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when interfacing with an external tool for solving complex math word problems. Our data and prompts are publicly available at https://github.com/joyheyueya/declarative-math-word-problem.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3293.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3293.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits explicit intermediate reasoning steps from LLMs by providing few-shot demonstrations of step-by-step solutions to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized variant of OpenAI's large autoregressive transformer family evaluated in this paper; used with top-1 decoding, temperature 0, and max_tokens=600 for all prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'step-by-step natural language reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT uses few-shot examples that include explicit intermediate natural-language reasoning steps; the model is prompted to produce a chain of reasoning leading to the final answer. Implemented via 8-shot (original) and a 3-shot prompt designed by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (step-by-step natural-language procedural reasoning). The paper treats CoT as a procedural/stepwise style and compares it to programmatic (PAL) and declarative formalization methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and Algebra</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K: grade-school math word problems (test set 1319 examples). Algebra: new dataset (222 word problems) curated from open-access Algebra textbooks, covering topics up to systems of equations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CoT_8-shot (original): GSM8K = 62.5 ± 0.16% ; Algebra = 45.3 ± 0.56%. CoT_3-shot (this paper's 3-shot): GSM8K = 58.9 ± 0.16% ; Algebra = 47.9 ± 1.18%. (All runs used Codex as the LLM.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT is compared directly to PAL and DECLARATIVE variants on both GSM8K and Algebra; it performs worse than PAL on GSM8K and substantially worse than the best DECLARATIVE+SymPy variant on Algebra.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT yields solid performance on GSM8K but degrades on the harder Algebra dataset; procedural natural-language step-by-step reasoning is less effective than declarative formalization plus a symbolic solver for some algebra problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT's accuracy drops from GSM8K to Algebra (62.5% → 45.3%), demonstrating negative results on harder, more declarative problems compared to declarative methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3293.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3293.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language model (PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that has the LLM generate programs (Python) as intermediate reasoning steps and executes them in an interpreter to perform arithmetic and algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Codex model used to generate Python programs in the PAL prompting approach; evaluated with the paper's 8-shot and 3-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['programmatic procedural reasoning (generate Python programs)', 'external execution via Python interpreter']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>PAL frames reasoning as program synthesis: the model emits Python code that expresses the procedural steps to compute the answer; a Python interpreter executes the code to avoid arithmetic errors. The paper runs PAL with original 8-shot and a 3-shot prompt designed by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (procedural/programmatic). PAL is biased toward procedural solutions and is contrasted with declarative formalization approaches in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and Algebra</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same GSM8K and Algebra datasets as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>PAL_8-shot (original): GSM8K = 70.2 ± 0.25% ; Algebra = 51.7 ± 0.21%. PAL_3-shot (this paper's 3-shot): GSM8K = 73.3 ± 0.13% ; Algebra = 56.2 ± 0.21%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PAL outperforms CoT on GSM8K and often outperforms DECLARATIVE variants on GSM8K, but on the harder Algebra dataset the DECLARATIVE_3-shot+principles+SymPy variant strongly outperforms PAL by ~20 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Programmatic procedural reasoning (PAL) is effective on grade-school problems (GSM8K) and benefits from program execution to avoid arithmetic errors, but it is less suitable for problems that require declarative reasoning (Algebra dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PAL's procedural bias makes it less effective on algebra problems that are more naturally expressed declaratively; while PAL is best on GSM8K, it underperforms compared to DECLARATIVE+SymPy on Algebra.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3293.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3293.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECLARATIVE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeClarative prompting (incremental declarative formalization) + SymPy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method introduced in this paper that elicits incremental, declarative formalizations (variables and equations) from an LLM and uses an external symbolic solver (SymPy) to solve the equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex is used to generate interleaved natural-language and formal variable/equation declarations in a few-shot DeClarative prompt; variants include adding explicit 'principles' and incremental vs one-step formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['declarative formalization (variables + equations)', 'incremental stepwise formalization', 'external symbolic solving (SymPy)', 'one-step formalization (ablation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The DeClarative prompt asks the LLM to produce sentences that either introduce variables or state equations (declarative statements) following explicit principles; equations are then passed to SymPy for algebraic solving. Variants: (a) 3-shot prompt, (b) prepend list of principles, (c) ask LLM to compute final answer directly (no SymPy), (d) ONE-STEP non-incremental formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse: contrasts declarative formalization plus external symbolic solving against procedural programmatic reasoning (PAL) and natural-language CoT. The paper implements multiple variants (incremental vs one-step, with/without principles, with/without SymPy) to probe the impact of representation and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and Algebra</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same GSM8K and Algebra datasets; Algebra is a harder, more declarative dataset assembled by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DECLARATIVE_8-shot+SymPy: GSM8K = 64.7% ; DECLARATIVE_3-shot+SymPy: GSM8K = 66.0 ± 0.33% ; DECLARATIVE_3-shot + principles + SymPy: GSM8K = 69.4 ± 0.65% , Algebra = 76.3 ± 0.93% ; DECLARATIVE_3-shot + principles (LLM solves equations, no SymPy): GSM8K = 22.4 ± 0.27% ; ONE-STEP DECLARATIVE_3-shot+SymPy: GSM8K = 57.5 ± 0.06%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Ablations show (1) using SymPy vs having the LLM compute answers yields a dramatic improvement (22.4% vs 69.4% on GSM8K), (2) adding explicit declarative 'principles' to the prompt helps, (3) incremental formalization outperforms one-step formalization, and (4) DECLARATIVE_3-shot+principles+SymPy outperforms PAL by ~20 percentage points on Algebra while being competitive with PAL on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Declarative, incremental representations combined with a symbolic solver enable better performance on harder algebra problems where procedural solutions are unnatural; external symbolic solving (SymPy) is crucial to avoid arithmetic/solving mistakes by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When the LLM is asked to solve the equations itself (no SymPy), accuracy drops dramatically (GSM8K 69.4% → 22.4%), and the one-step (non-incremental) formalization underperforms the incremental variant (57.5% vs 69.4% on GSM8K). Also, on GSM8K PAL still outperforms the DECLARATIVE method in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3293.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3293.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymPy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymPy (symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Python library for symbolic mathematics used as an external solver to algebraically solve systems of equations generated by the LLM's declarative formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SymPy (external symbolic computation library)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic algebra library invoked by the pipeline to solve equation systems extracted from the DeClarative prompt outputs; cited as Meurer et al. (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['symbolic algebraic solving (exact equation solving)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SymPy receives variables and equations parsed from the LLM's output and performs algebraic solution (e.g., solve systems for the goal variable), replacing the need for LLM-driven numeric calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>tool-use complementing declarative reasoning: allows separation of declarative specification (by LLM) and precise symbolic computation (by SymPy).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and Algebra (as post-processing solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to solve the systems of equations defined by the DeClarative prompting outputs for problems in GSM8K and the Algebra dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Enabling SymPy dramatically improves performance when paired with DECLARATIVE prompting: e.g., DECLARATIVE_3-shot+principles+SymPy GSM8K = 69.4 ± 0.65% vs DECLARATIVE_3-shot+principles (no SymPy) GSM8K = 22.4 ± 0.27%. Also yields Algebra = 76.3 ± 0.93% for the best DECLARATIVE variant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Using SymPy vs relying on the LLM to perform calculations is a core ablation showing large gains; SymPy + declarative spec outperforms purely LLM-driven procedural or arithmetic approaches on harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External symbolic solving is essential to avoid arithmetic and algebraic errors by LLMs; it enables declarative formalizations to translate into correct final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No negative results for SymPy itself were reported; rather, the negative result was when SymPy was removed (LLM-only solving).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3293.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3293.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model cited in related work (scaling language models with Pathways) and referenced in the context of prior approaches to prompting and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as prior work in large-scale language modeling; the paper cites it in discussing related work on calculators and tool-augmented LLMs but does not evaluate PaLM experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['mention in context of tool-augmented / scaled LLMs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>PaLM is mentioned in the related work to situate research on LLM capabilities and tool-usage; no implementation details or experimental results are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>not applicable (mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PaLM is referenced but not compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>N/A in this paper (PaLM only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3293.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3293.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The concrete LLM used for all experiments in this paper; a code-focused variant of OpenAI's transformer-based LLM family used to generate CoT, PAL programs, and DeClarative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized large autoregressive transformer evaluated by the authors; used across all prompting methods (CoT, PAL, DeClarative variants) with top-1 decoding and temperature 0 as the single model backbone for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'program generation (PAL)', 'declarative formalization generation (DeClarative variants)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Codex was prompted in different styles: CoT natural-language chains, programmatic code for PAL, and interleaved natural language plus variable/equation declarations for DeClarative; the same model and decoding hyperparameters were used to enable controlled comparisons of reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single model evaluated under multiple reasoning-method prompts (diverse prompting styles applied to the same LLM to compare reasoning styles).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and Algebra</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Codex's outputs under different prompting paradigms were measured on these two math word-problem datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>See method-specific entries (CoT, PAL, DECLARATIVE) — Codex is the shared backbone for those reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Using the same model (Codex) with distinct prompting styles isolates the impact of reasoning-style (procedural/programmatic vs declarative vs natural-language chains) and external tool use (Python interpreter for PAL, SymPy for DECLARATIVE).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The choice of reasoning style and external tool has a major effect even with the same underlying LLM: declarative+SymPy beats procedural PAL on harder Algebra problems, while PAL remains competitive on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None beyond the method-specific negative results (e.g., LLM-only solving without SymPy performs poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Math Word Problems by Combining Language Models With Symbolic Solvers', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
                <li>PaLM: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
                <li>Sympy: symbolic computing in python <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3293",
    "paper_id": "paper-57100e39d0413ee585b381ba9ab366e8a6cf2866",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that elicits explicit intermediate reasoning steps from LLMs by providing few-shot demonstrations of step-by-step solutions to improve multi-step reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Code-specialized variant of OpenAI's large autoregressive transformer family evaluated in this paper; used with top-1 decoding, temperature 0, and max_tokens=600 for all prompting methods.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "step-by-step natural language reasoning"
            ],
            "reasoning_methods_description": "CoT uses few-shot examples that include explicit intermediate natural-language reasoning steps; the model is prompted to produce a chain of reasoning leading to the final answer. Implemented via 8-shot (original) and a 3-shot prompt designed by the authors.",
            "diversity_of_methods": "similar style (step-by-step natural-language procedural reasoning). The paper treats CoT as a procedural/stepwise style and compares it to programmatic (PAL) and declarative formalization methods.",
            "reasoning_task_name": "GSM8K and Algebra",
            "reasoning_task_description": "GSM8K: grade-school math word problems (test set 1319 examples). Algebra: new dataset (222 word problems) curated from open-access Algebra textbooks, covering topics up to systems of equations.",
            "performance_by_method": "CoT_8-shot (original): GSM8K = 62.5 ± 0.16% ; Algebra = 45.3 ± 0.56%. CoT_3-shot (this paper's 3-shot): GSM8K = 58.9 ± 0.16% ; Algebra = 47.9 ± 1.18%. (All runs used Codex as the LLM.)",
            "comparison_of_methods": "CoT is compared directly to PAL and DECLARATIVE variants on both GSM8K and Algebra; it performs worse than PAL on GSM8K and substantially worse than the best DECLARATIVE+SymPy variant on Algebra.",
            "key_findings": "CoT yields solid performance on GSM8K but degrades on the harder Algebra dataset; procedural natural-language step-by-step reasoning is less effective than declarative formalization plus a symbolic solver for some algebra problems.",
            "counter_examples_or_negative_results": "CoT's accuracy drops from GSM8K to Algebra (62.5% → 45.3%), demonstrating negative results on harder, more declarative problems compared to declarative methods.",
            "uuid": "e3293.0",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PAL",
            "name_full": "Program-Aided Language model (PAL)",
            "brief_description": "A method that has the LLM generate programs (Python) as intermediate reasoning steps and executes them in an interpreter to perform arithmetic and algorithmic reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Same Codex model used to generate Python programs in the PAL prompting approach; evaluated with the paper's 8-shot and 3-shot prompts.",
            "model_size": null,
            "reasoning_methods": [
                "programmatic procedural reasoning (generate Python programs)",
                "external execution via Python interpreter"
            ],
            "reasoning_methods_description": "PAL frames reasoning as program synthesis: the model emits Python code that expresses the procedural steps to compute the answer; a Python interpreter executes the code to avoid arithmetic errors. The paper runs PAL with original 8-shot and a 3-shot prompt designed by the authors.",
            "diversity_of_methods": "similar style (procedural/programmatic). PAL is biased toward procedural solutions and is contrasted with declarative formalization approaches in the paper.",
            "reasoning_task_name": "GSM8K and Algebra",
            "reasoning_task_description": "Same GSM8K and Algebra datasets as above.",
            "performance_by_method": "PAL_8-shot (original): GSM8K = 70.2 ± 0.25% ; Algebra = 51.7 ± 0.21%. PAL_3-shot (this paper's 3-shot): GSM8K = 73.3 ± 0.13% ; Algebra = 56.2 ± 0.21%.",
            "comparison_of_methods": "PAL outperforms CoT on GSM8K and often outperforms DECLARATIVE variants on GSM8K, but on the harder Algebra dataset the DECLARATIVE_3-shot+principles+SymPy variant strongly outperforms PAL by ~20 percentage points.",
            "key_findings": "Programmatic procedural reasoning (PAL) is effective on grade-school problems (GSM8K) and benefits from program execution to avoid arithmetic errors, but it is less suitable for problems that require declarative reasoning (Algebra dataset).",
            "counter_examples_or_negative_results": "PAL's procedural bias makes it less effective on algebra problems that are more naturally expressed declaratively; while PAL is best on GSM8K, it underperforms compared to DECLARATIVE+SymPy on Algebra.",
            "uuid": "e3293.1",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "DECLARATIVE",
            "name_full": "DeClarative prompting (incremental declarative formalization) + SymPy",
            "brief_description": "A prompting method introduced in this paper that elicits incremental, declarative formalizations (variables and equations) from an LLM and uses an external symbolic solver (SymPy) to solve the equations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Codex is used to generate interleaved natural-language and formal variable/equation declarations in a few-shot DeClarative prompt; variants include adding explicit 'principles' and incremental vs one-step formalization.",
            "model_size": null,
            "reasoning_methods": [
                "declarative formalization (variables + equations)",
                "incremental stepwise formalization",
                "external symbolic solving (SymPy)",
                "one-step formalization (ablation)"
            ],
            "reasoning_methods_description": "The DeClarative prompt asks the LLM to produce sentences that either introduce variables or state equations (declarative statements) following explicit principles; equations are then passed to SymPy for algebraic solving. Variants: (a) 3-shot prompt, (b) prepend list of principles, (c) ask LLM to compute final answer directly (no SymPy), (d) ONE-STEP non-incremental formalization.",
            "diversity_of_methods": "diverse: contrasts declarative formalization plus external symbolic solving against procedural programmatic reasoning (PAL) and natural-language CoT. The paper implements multiple variants (incremental vs one-step, with/without principles, with/without SymPy) to probe the impact of representation and tool use.",
            "reasoning_task_name": "GSM8K and Algebra",
            "reasoning_task_description": "Same GSM8K and Algebra datasets; Algebra is a harder, more declarative dataset assembled by the authors.",
            "performance_by_method": "DECLARATIVE_8-shot+SymPy: GSM8K = 64.7% ; DECLARATIVE_3-shot+SymPy: GSM8K = 66.0 ± 0.33% ; DECLARATIVE_3-shot + principles + SymPy: GSM8K = 69.4 ± 0.65% , Algebra = 76.3 ± 0.93% ; DECLARATIVE_3-shot + principles (LLM solves equations, no SymPy): GSM8K = 22.4 ± 0.27% ; ONE-STEP DECLARATIVE_3-shot+SymPy: GSM8K = 57.5 ± 0.06%.",
            "comparison_of_methods": "Ablations show (1) using SymPy vs having the LLM compute answers yields a dramatic improvement (22.4% vs 69.4% on GSM8K), (2) adding explicit declarative 'principles' to the prompt helps, (3) incremental formalization outperforms one-step formalization, and (4) DECLARATIVE_3-shot+principles+SymPy outperforms PAL by ~20 percentage points on Algebra while being competitive with PAL on GSM8K.",
            "key_findings": "Declarative, incremental representations combined with a symbolic solver enable better performance on harder algebra problems where procedural solutions are unnatural; external symbolic solving (SymPy) is crucial to avoid arithmetic/solving mistakes by the LLM.",
            "counter_examples_or_negative_results": "When the LLM is asked to solve the equations itself (no SymPy), accuracy drops dramatically (GSM8K 69.4% → 22.4%), and the one-step (non-incremental) formalization underperforms the incremental variant (57.5% vs 69.4% on GSM8K). Also, on GSM8K PAL still outperforms the DECLARATIVE method in some settings.",
            "uuid": "e3293.2",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "SymPy",
            "name_full": "SymPy (symbolic solver)",
            "brief_description": "An open-source Python library for symbolic mathematics used as an external solver to algebraically solve systems of equations generated by the LLM's declarative formalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SymPy (external symbolic computation library)",
            "model_description": "Symbolic algebra library invoked by the pipeline to solve equation systems extracted from the DeClarative prompt outputs; cited as Meurer et al. (2017).",
            "model_size": null,
            "reasoning_methods": [
                "symbolic algebraic solving (exact equation solving)"
            ],
            "reasoning_methods_description": "SymPy receives variables and equations parsed from the LLM's output and performs algebraic solution (e.g., solve systems for the goal variable), replacing the need for LLM-driven numeric calculation.",
            "diversity_of_methods": "tool-use complementing declarative reasoning: allows separation of declarative specification (by LLM) and precise symbolic computation (by SymPy).",
            "reasoning_task_name": "GSM8K and Algebra (as post-processing solver)",
            "reasoning_task_description": "Used to solve the systems of equations defined by the DeClarative prompting outputs for problems in GSM8K and the Algebra dataset.",
            "performance_by_method": "Enabling SymPy dramatically improves performance when paired with DECLARATIVE prompting: e.g., DECLARATIVE_3-shot+principles+SymPy GSM8K = 69.4 ± 0.65% vs DECLARATIVE_3-shot+principles (no SymPy) GSM8K = 22.4 ± 0.27%. Also yields Algebra = 76.3 ± 0.93% for the best DECLARATIVE variant.",
            "comparison_of_methods": "Using SymPy vs relying on the LLM to perform calculations is a core ablation showing large gains; SymPy + declarative spec outperforms purely LLM-driven procedural or arithmetic approaches on harder tasks.",
            "key_findings": "External symbolic solving is essential to avoid arithmetic and algebraic errors by LLMs; it enables declarative formalizations to translate into correct final answers.",
            "counter_examples_or_negative_results": "No negative results for SymPy itself were reported; rather, the negative result was when SymPy was removed (LLM-only solving).",
            "uuid": "e3293.3",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PaLM",
            "name_full": "PaLM",
            "brief_description": "A large language model cited in related work (scaling language models with Pathways) and referenced in the context of prior approaches to prompting and tool use.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PaLM",
            "model_description": "Referenced as prior work in large-scale language modeling; the paper cites it in discussing related work on calculators and tool-augmented LLMs but does not evaluate PaLM experimentally.",
            "model_size": null,
            "reasoning_methods": [
                "mention in context of tool-augmented / scaled LLMs"
            ],
            "reasoning_methods_description": "PaLM is mentioned in the related work to situate research on LLM capabilities and tool-usage; no implementation details or experimental results are provided in this paper.",
            "diversity_of_methods": "not applicable (mention only)",
            "reasoning_task_name": "",
            "reasoning_task_description": "",
            "performance_by_method": "",
            "comparison_of_methods": "PaLM is referenced but not compared experimentally in this paper.",
            "key_findings": "N/A in this paper (PaLM only cited in related work).",
            "counter_examples_or_negative_results": "N/A",
            "uuid": "e3293.4",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Codex",
            "name_full": "Codex (code-davinci-002)",
            "brief_description": "The concrete LLM used for all experiments in this paper; a code-focused variant of OpenAI's transformer-based LLM family used to generate CoT, PAL programs, and DeClarative outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Code-specialized large autoregressive transformer evaluated by the authors; used across all prompting methods (CoT, PAL, DeClarative variants) with top-1 decoding and temperature 0 as the single model backbone for comparisons.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought prompting",
                "program generation (PAL)",
                "declarative formalization generation (DeClarative variants)"
            ],
            "reasoning_methods_description": "Codex was prompted in different styles: CoT natural-language chains, programmatic code for PAL, and interleaved natural language plus variable/equation declarations for DeClarative; the same model and decoding hyperparameters were used to enable controlled comparisons of reasoning styles.",
            "diversity_of_methods": "single model evaluated under multiple reasoning-method prompts (diverse prompting styles applied to the same LLM to compare reasoning styles).",
            "reasoning_task_name": "GSM8K and Algebra",
            "reasoning_task_description": "Codex's outputs under different prompting paradigms were measured on these two math word-problem datasets.",
            "performance_by_method": "See method-specific entries (CoT, PAL, DECLARATIVE) — Codex is the shared backbone for those reported results.",
            "comparison_of_methods": "Using the same model (Codex) with distinct prompting styles isolates the impact of reasoning-style (procedural/programmatic vs declarative vs natural-language chains) and external tool use (Python interpreter for PAL, SymPy for DECLARATIVE).",
            "key_findings": "The choice of reasoning style and external tool has a major effect even with the same underlying LLM: declarative+SymPy beats procedural PAL on harder Algebra problems, while PAL remains competitive on GSM8K.",
            "counter_examples_or_negative_results": "None beyond the method-specific negative results (e.g., LLM-only solving without SymPy performs poorly).",
            "uuid": "e3293.5",
            "source_info": {
                "paper_title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1
        },
        {
            "paper_title": "PaLM: Scaling language modeling with pathways",
            "rating": 1
        },
        {
            "paper_title": "Sympy: symbolic computing in python",
            "rating": 1
        }
    ],
    "cost": 0.01256825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving Math Word Problems by Combining Language Models With Symbolic Solvers</h1>
<p>Joy He-Yueya<br>Stanford University<br>heyueya@stanford.edu<br>Gabriel Poesia<br>Stanford University<br>poesia@cs.stanford.edu<br>Rose E. Wang<br>Stanford University<br>rewang@stanford.edu<br>Noah D. Goodman<br>Stanford University<br>ngoodman@stanford.edu</p>
<h4>Abstract</h4>
<p>Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute $20 \%$ on Algebra, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when interfacing with an external tool for solving complex math word problems. Our data and prompts are publicly available at https://github.com/joyheyueya/declarative-math-word-problem.</p>
<h2>1 Introduction</h2>
<p>Learning to solve mathematical word problems (see an example in Figure 1) is an important skill but can be challenging for students. [5, 13]. A tool that can automatically generate step-by-step solutions to such problems has the potential to provide personalized support for students working through word problems [14, 6] and help educators with curriculum development [12].
Using few-shot prompting over large language models (LLMs) has recently emerged as a promising approach for solving math word problems [15, 17, 7]. The chain-of-thought (CoT) [15] prompting method presents explicit intermediate reasoning steps to the LLM to further enhance its reasoning capability. However, LLMs often struggle with performing arithmetic operations [8, 9, 15]. To address this, [15] uses an external calculator to evaluate the arithmetic operations in the generated reasoning steps. Program-Aided Language model (PAL) [7] extends this idea by generating Python programs as reasoning steps, offloading all calculations to a Python interpreter. Although programs offer a direct representation of procedures, they require special devices to represent more abstract mathematical declarations. For example, a statement like $a=b+1$ can be directly interpreted as a variable assignment in Python if $b$ is known, but not if $b$ is unknown. Nonetheless, the equation remains a valid mathematical expression even when $b$ is unknown, suggesting that we instead want to</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a math word problem and its solution from the DECLARATIVE prompt. Variables and equations are in red.
allow models to perform mathematical declarations beyond those that yield a procedure (for a full example, see the problem in Figure 4).</p>
<p>In this work, we present an approach that combines an LLM, which can incrementally formalize word problems as a set of variables and equations, with an external symbolic solver that can solve the equations. Our approach achieves comparable performance to the original PAL on the GSM8K [4] benchmark of math word problems. To evaluate current approaches on more challenging word problems, we introduce Algebra, a dataset of 222 word problems collected from open access Algebra textbooks. We show that our approach outperforms PAL by an absolute $20 \%$ on Algebra. Our work highlights the effectiveness of incrementally generating declarative formalizations when interfacing with an external tool for solving complex math word problems.</p>
<h1>2 Related work</h1>
<p>Recent studies have explored the use of few-shot prompting over LLMs for solving math word problems [15, 17, 7]. The chain-of-thought [15] prompting method presents explicit intermediate reasoning steps to the LLM to improve its reasoning capability. Since LLMs often make arithmetic errors [8, 9, 15], several prior works [15, 3] have experimented with using an external calculator to carry out the operations generated by LLMs. This generally improves final performance by less than $5 \%$ on GSM8K. Program-Aided Language model [7] extends to more complex arithmetic by generating Python programs as reasoning steps and using a Python interpreter to perform the calculations. However, generating Python programs carries a strong bias toward procedural calculations and does not work well for word problems that do not have a straightforward procedural solution.</p>
<h2>3 Our Approach: Equipping an LLM With an External Symbolic Solver</h2>
<p>Our approach for solving a math word problem consists of two steps: (1) declarative and incremental formalization using an LLM and (2) solving equations using a symbolic solver.</p>
<h3>3.1 Declarative and incremental formalization using an LLM</h3>
<p>To solve a math word problem, we first use an LLM to formalize the problem as a set of variables and equations. Recently, using few-shot prompting over LLMs has emerged as an effective approach for natural language understanding and decomposition.</p>
<h1>3.1.1 Few-shot prompting</h1>
<p>Few-shot prompting is a technique that uses LLMs to solve a task by providing the LLMs with a few demonstrations of the task as part of the input at inference time [1]. In this technique, the demonstrations (i.e., examples of input-output pairs) are concatenated into a prompt, which is passed to the model along with the new input to generate an output. Formally, a set of $k$ input-output examples $\left{\left(x_{i}, y_{i}\right)\right}<em 1="1">{i=1}^{k}$ are concatenated in a prompt $p \equiv\left(x</em>$.}, y_{2}\right) |\left(x_{1}, y_{2}\right)\left|... |\left(x_{k}, y_{k}\right)\right.$ where $|$ denotes the concatenation of examples. At inference time, $p | x_{\text {test }}$ is passed to the model where $x_{\text {test }}$ denotes a new input instance, and the model attempts to complete $p | x_{\text {test }}$ by generating the output $y_{\text {test }</p>
<h3>3.1.2 Crafting the DeClarative prompt</h3>
<p>To formalize word problems using few-shot prompting, we introduce the DeClarative prompt $p \equiv\left(x_{1}, y_{2}\right) |\left(x_{1}, y_{2}\right)\left|... |\left(x_{k}, y_{k}\right)\right.$ where $x_{i}$ is the word problem in natural language, and $y_{i}$ is the step-by-step solution to $x_{i}$. In the DECLARATIVE prompt, $y_{i}$ consists of interleaved natural language statements and formal variable or equation declarations in double-square brackets. Our approach aims to generate solutions that formalize word problems based on a set of principles listed in Table 1. Figure 1 shows an example used in the DeClarative prompt that we created according to these principles. The full prompt is publicly available at https://github.com/joyheyueya/declarative-math-word-problem. To solve a new word problem, $x_{\text {test }}$, we append it to $p$ and pass $p | x_{\text {test }}$ to an LLM, which generates $y_{\text {test }}$ as the solution for $x_{\text {test }}$.</p>
<h2>Principles for solutions</h2>
<ol>
<li>Each sentence in the solution either introduces a new variable or states a new equation.</li>
<li>The last sentence gives the goal: which variable will contain the answer to the problem.</li>
<li>Each equation only uses previously introduced variables.</li>
<li>Each quantity is only named by one variable.</li>
<li>The solution uses all the numbers in the question.</li>
</ol>
<p>Table 1: A list of principles we would like the solutions to satisfy.</p>
<h3>3.2 Solving equations using a symbolic solver</h3>
<p>The step-by-step solution generated by the LLM using the DeClarative prompt includes the list of variables and equations that describe the word problem but does not provide the final answer (see Figure 1). Instead of relying on the LLM to solve the equations directly, we pass the equations to an external symbolic solver to do the calculation. In this work, we use SymPy [11], a Python library for symbolic computation, to algebraically solve a system of equations extracted from the solution generated by the LLM.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate our approach on two math word problem datasets: GSM8K [4] and a new dataset called AlGEBRA ${ }^{1}$. We use the GSM8K test set, which contains 1319 math word problems at grade-school level. To evaluate our approach on more challenging problems, we curated AlGEBRA, which consists of 222 word problems from two open-access Algebra textbooks: Basic Algebra with Applications ([16]; released under the Creative Commons Attribution-ShareAlike license) and Elementary Algebra 2e ([10]; released under the Creative Commons Attribution license). The resulting dataset includes word problems covering all topics leading up to System of Equations, with the exception of problems related to geometry, graphing, or inequalities.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Adding principles to the beginning of the DECLARATIVE prompt.</p>
<p>Let's solve mathematical word problems in a careful, formal manner. The solution will follow the Peano format:
1- Each sentence in the solution either introduces a new variable or states a new equation.
2- The last sentence gives the goal: which variable will contain the answer to the problem.
3- Each equation only uses previously introduced variables.
4- Each quantity is only named by one variable.
5- Use all the numbers in the question.
Q: Mario and Luigi together had 10 years of experience in soccer. Luigi had 3 more than Mario. How many did Mario have?</p>
<p>Peano solution:
Let a be the number of years Mario had [[var a]].
Let $b$ be the number of years Luigi had [[var b]]. We have [[eq $a+b=10]]$. We also have [[eq $b=a+3]]$.
The answer is the value of a [[answer a]].
(b) Adding principles to the beginning of the DECLARATIVE prompt and calculating the final answer. The final answer is in red.</p>
<p>Figure 2: The difference between "DECLARATIVE ${ }<em 3-_text="3-\text" _="{" shot="shot">{3-\text { shot }}+$ principles + SymPy" and "DECLARATIVE ${ }</em>}}+$ principles" is that "DECLARATIVE ${ <em 3-_text="3-\text" _="{" shot="shot">{3-\text { shot }}+$ principles + SymPy" passes the equations to SymPy to solve, but "DECLARATIVE ${ }</em>+$ principles" asks the LLM to solve the equations directly.}</p>
<p>Q: Mario and Luigi together had 10 years of experience in soccer. Luigi had 3 more than Mario. How many did Mario have?</p>
<p>A:
$[[e q a=(10-3) / 2]]$
[[answer a]]
Figure 3: An example of formalizing a math word problem in a single equation.</p>
<h1>4.2 Baselines and variants of the DeClarative prompting</h1>
<p>We consider three methods: chain-of-thought (CoT) prompting [15], Program-Aided Language model (PAL) [7], and our DeClarative prompting combined with SymPy (DeClarative + SymPy). We created two different prompts for each prompting method. The first prompt (8-shot) uses the same set of eight examples used in prior work [15]. The second prompt (3-shot) uses three examples that we designed to help illustrate step-by-step and declarative thinking and the formalization format we expect.</p>
<p>For our DeClarative prompting method, we experimented with three variants.</p>
<ol>
<li>DeClarative $_{3-\text { shot }}+$ principles + SymPy: adding the list of principles in Table 1 at the beginning of the prompt (see an example in Figure 2a).</li>
<li>DeClarative $_{3-\text { shot }}+$ principles: using the LLM to directly calculate the value of the goal variable (see an example in Figure 2b).</li>
<li>ONE-STEP DECLARATIVE ${ }_{3-\text { shot }}+$ SymPy: formalizing the word problem in a single step instead of incrementally (see an example in Figure 3).</li>
</ol>
<p>All the prompts used in this work are publicly available at https://github.com/joyheyueya/declarative-math-word-problem.</p>
<p>We use Codex (code-davinci-002) [2] as the LLM for all methods. We use top-1 decoding and a temperature of 0 . We set max_tokens to be 600 .</p>
<h1>5 Results</h1>
<h3>5.1 Results on GSM8K and Algebra</h3>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">GSM8K</th>
<th style="text-align: left;">Algebra</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{CoT}_{\text {8-shot (original) }}$</td>
<td style="text-align: left;">$62.5 \pm 0.16$</td>
<td style="text-align: left;">$45.3 \pm 0.56$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{CoT}_{\text {3-shot (ours) }}$</td>
<td style="text-align: left;">$58.9 \pm 0.16$</td>
<td style="text-align: left;">$47.9 \pm 1.18$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{PAL}_{\text {8-shot (original) }}$</td>
<td style="text-align: left;">$70.2 \pm 0.25$</td>
<td style="text-align: left;">$51.7 \pm 0.21$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{PAL}_{\text {3-shot (ours) }}$</td>
<td style="text-align: left;">$\mathbf{7 3 . 3} \pm 0.13$</td>
<td style="text-align: left;">$56.2 \pm 0.21$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{DECLARATIVE}_{\text {8-shot }}+\mathrm{SymPy}$</td>
<td style="text-align: left;">64.7</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{DECLARATIVE}_{\text {3-shot }}+\mathrm{SymPy}$</td>
<td style="text-align: left;">$66.0 \pm 0.33$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{DECLARATIVE}_{\text {3-shot }}+$ principles + SymPy</td>
<td style="text-align: left;">$69.4 \pm 0.65$</td>
<td style="text-align: left;">$\mathbf{7 6 . 3} \pm 0.93$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{DECLARATIVE}_{\text {3-shot }}+$ principles</td>
<td style="text-align: left;">$22.4 \pm 0.27$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{ONE}-\mathrm{STEP} \mathrm{DECLARATIVE}_{\text {3-shot }}+\mathrm{SymPy}$</td>
<td style="text-align: left;">$57.5 \pm 0.06$</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Problem solve rate (\%) on GSM8K and Algebra. We report the average and standard deviation across three runs. The highest number on each dataset is in bold. For CoT and PAL, we ran both the 8 -shot prompt used in the original papers and the 3 -shot prompt we created.</p>
<p>Q: Bob says to Alice: if you give me 3 apples and then take half of my apples away, then I will be left with 13 apples. How many apples do I have now?
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Declarative solutions are typically more intuitive to write than procedural solutions for challenging algebra word problems. PAL and COT try to generate procedural solutions that describe a set of plans for achieving the goal, which are incorrect in this case. The DECLARATIVE prompting generates a correct solution that describes the properties of the goal, which is generally more appropriate for hard problems with no obvious procedural solutions.</p>
<p>On GSM8K (Table 2), our 3-shot prompt leads to a better performance than the original 8-shot prompt for PAL and DECLARATIVE. PAL outperforms DECLARATIVE across both sets of comparable examples, but using our DECLARATIVE prompting method with the 3-shot prompt (DECLARATIVE ${ }<em 8="8" _-shot="{-shot" _original_="(original)" _text="\text">{3 \text {-shot }}+$ principles + SymPy) gives a performance equivalent to the original PAL (PAL $</em>$ ).
Interestingly, prepending the list of principles to the DECLARATIVE prompt (DECLARATIVE ${ }}<em _-shot="{-shot" _text="\text">{3 \text {-shot }}+$ principles + SymPy) leads to a better performance on GSM8K than DECLARATIVE $3</em>+$ SymPy. Asking the LLM to solve the equations directly leads to a dramatic drop in accuracy (from $69.4 \%$}</p>
<p>to $22.4 \%$ ), which highlights the benefit of using an external solver. Additionally, our DECLARATIVE prompting benefits from incremental formalization, as shown by the performance gap between the incremental version (DECLARATIVE $3_{\text {-shot }}+$ principles + SymPy) and the non-incremental variant (ONE-STEP DECLARATIVE $3_{\text {-shot }}+$ SymPy).
On Algebra (Table 2), our approach (DeClarative $3_{\text {-shot }}+$ principles + SymPy) achieves the highest accuracy among all methods, outperforming PAL by an absolute $20 \%$. The accuracy of the original CoT drops from $62.5 \%$ on GSM8K to $45.3 \%$ on Algebra, which demonstrates that problems in Algebra are generally harder than those in GSM8K. The main reason that the DeClarative prompting method works better than CoT and PAL on Algebra is that it is less intuitive to generate procedural solutions to Algebra problems that require declarative reasoning (see an example in Figure 4). Although our 3-shot prompt improves the performance of CoT and PAL on Algebra compared to the original 8-shot prompt, our Declarative method is still much more effective than CoT and PAL.</p>
<h1>6 Conclusion</h1>
<p>We present an approach for automatically generating step-by-step solutions to math word problems by equipping an LLM with an external symbolic solver. Our approach uses an LLM to incrementally formalize word problems as variables and equations and avoids arithmetic errors by using an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on GSM8K and improves over PAL by an absolute $20 \%$ on a new dataset consisting of harder word problems from Algebra textbooks.
We demonstrate the effectiveness of using declarative formalization when interfacing with an external tool for solving complex math word problems. Additionally, encouraging incremental formalization is beneficial, especially when using declarative representations. Our approach is particularly useful for math education since many advanced math problems can be divided into separate conceptual pieces, with one piece being declarative and the other involving procedural knowledge.</p>
<h2>References</h2>
<p>[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[4] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[5] D. D. Cummins. Children's interpretations of arithmetic word problems. Cognition and instruction, 8(3):261-289, 1991.
[6] J. del Olmo-Muñoz, J. A. González-Calero, P. D. Diago, D. Arnau, and M. Arevalillo-Herráez. Intelligent tutoring systems for word problem solving in covid-19 days: could they have been (part of) the solution? ZDM-Mathematics Education, pages 1-14, 2022.
[7] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Programaided language models. arXiv preprint arXiv:2211.10435, 2022.
[8] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[9] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>[10] L. Marecek, M. Anthony-Smith, and A. H. Mathis. Elementary Algebra 2E. OpenStax, 2020.
[11] A. Meurer, C. P. Smith, M. Paprocki, O. Čertík, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017.
[12] O. Polozov, E. O’Rourke, A. M. Smith, L. Zettlemoyer, S. Gulwani, and Z. Popović. Personalized mathematical word problem generation. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.
[13] N. Pongsakdi, A. Kajamies, K. Veermans, K. Lertola, M. Vauras, and E. Lehtinen. What makes mathematical word problem solving challenging? exploring the roles of word problem characteristics, text comprehension, and arithmetic skills. ZDM, 52:33-44, 2020.
[14] S. Ritter, J. R. Anderson, K. R. Koedinger, and A. Corbett. Cognitive tutor: Applied research in mathematics education. Psychonomic bulletin \&amp; review, 14:249-255, 2007.
[15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[16] I. G. Zaigralin. Basic Algebra with Applications. Ivan G. Zaigralin, 6 edition, 2018.
[17] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The Algebra dataset is publically available at https://github.com/joyheyueya/declarative-math-wordproblem.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
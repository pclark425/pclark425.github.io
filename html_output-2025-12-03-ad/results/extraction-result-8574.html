<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8574 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8574</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8574</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278165164</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.18572v1.pdf" target="_blank">BELL: Benchmarking the Explainability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models have demonstrated remarkable capabilities in natural language processing, yet their decision-making processes often lack transparency. This opaqueness raises significant concerns regarding trust, bias, and model performance. To address these issues, understanding and evaluating the interpretability of LLMs is crucial. This paper introduces a standardised benchmarking technique, Benchmarking the Explainability of Large Language Models, designed to evaluate the explainability of large language models.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8574.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8574.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source foundation language model evaluated in this paper; reported as the best-performing model on the paper's reasoning benchmark, with strong sequential multi-step reasoning and low hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model used as a high-performing reference; described in the paper as demonstrating superior sequential reasoning and low hallucination on the OpenOrca mathematical problem-solving subset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A subset of the OpenOrca dataset focused on mathematical problem-solving drawn from the FLAN collection; requires multi-step, sequential logical and arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated using thought-eliciting prompting techniques including Chain-of-Thought (CoT), Thread-of-Thought (ThoT), ReRead variations (ReRead CoT/ReRead ThoT), and Chain-of-Verification (CoVe); metrics used include coherence, uncertainty, cosine similarity to baseline responses, and a hallucination score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Quantitative highlights reported: CoT score 85.28; ThoT score 92.39; other ReRead/verification metrics described as leading. Reported hallucination score: 19.42 (lowest among evaluated models). Overall described as consistently outperforming other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against several smaller/open models (Gemma-2 9B, Mistral 7B, Llama variants, Llava, Nemotron-mini). GPT-4 consistently outperforms these models across CoT, ThoT, ReRead techniques and exhibits substantially lower hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No detailed failure cases provided for GPT-4 beyond general subject-area caveats; paper notes that even with strong performance GPT-4 can hallucinate (though less than others) and that different elicitation techniques vary in effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large model scale and capacity strongly correlate with better multi-step logical reasoning and lower hallucination; combining read/re-read and verification prompting further improves output faithfulness and explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8574.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2 9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 (9B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 9B parameter model evaluated in the benchmark; reported to follow GPT-4 in several categories but with minor gaps in reasoning coherence and hallucination control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2 9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open/available 9 billion parameter foundation model evaluated on the OpenOrca math subset using the paper's thought-eliciting techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step mathematical reasoning problems from the OpenOrca dataset requiring stepwise decomposition and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with Chain-of-Thought, Thread-of-Thought, ReRead variants and Chain-of-Verification prompts; scored with coherence, uncertainty, cosine similarity to baselines and hallucination metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported qualitatively as close behind GPT-4 in several categories; the paper states Gemma-2 and Mistral 'followed closely' though with some gaps in hallucination control. Exact numeric scores for Gemma-2 are not unambiguously reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to GPT-4 and smaller Llama/Llava/Nemotron models; Gemma-2 trails GPT-4 but surpasses smaller constrained models on many elicitation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Minor gaps in reasoning and hallucination control relative to GPT-4; specific failure cases are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mid-to-large scale open models can approach top closed-model performance with thought-eliciting prompting, but still lag on hallucination and verification tasks compared to the largest models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8574.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter model evaluated in the benchmark; showed competitive performance among open models but with observable gaps versus GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7 billion parameter foundation model evaluated on the OpenOrca math problems using the suite of thought-eliciting techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Problems requiring multi-step mathematical/logical reasoning; evaluation focused on faithfulness and stepwise explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Assessed with CoT, ThoT, ReRead variants and CoVe; metrics: coherence, uncertainty, cosine similarity, hallucination score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Described as following Gemma-2 closely in several categories; numeric details are not clearly and unambiguously provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4 and other open models; outperforms smaller Llama variants but underperforms GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Displayed minor gaps in reasoning and hallucination control relative to the best model (GPT-4); specific failure examples not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates that 7B-scale models can perform competitively with careful elicitation, but scale and architecture remain important for top-tier logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8574.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2 3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.2 (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter variant of the Llama-3.2 family evaluated in the benchmark; reported moderate performance with limitations in verification and coherence on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2 3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 3-billion parameter Llama-3.2 variant included in the evaluation set, tested with multiple thought-eliciting prompting strategies on the OpenOrca math subset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Structured math reasoning requiring decomposition into intermediate steps; used to evaluate coherence and verification ability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with Chain-of-Thought, Thread-of-Thought, ReRead variants and Chain-of-Verification, scored by coherence, uncertainty, cosine similarity and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as moderate: performs reasonably on ThoT and ReRead techniques but shows limitations on coherence and verification compared to larger models; exact numeric values are not unambiguously reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms larger models (GPT-4, Gemma-2, Mistral) and shows worse verification/hallucination behavior relative to them.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant limitations in coherence and verification for more complex prompts; higher hallucination and lower reliability on multi-step deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller parameter budgets limit reliable multi-step logical reasoning; combining elicitation techniques helps but does not fully close the gap to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8574.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2 1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.2 (1B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1B-parameter Llama-3.2 model included in the evaluation; reported to have notable limitations in coherence, verification and hallucination relative to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2 1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 1-billion parameter variant of Llama-3.2 used in the paper's experiments; assessed across the same suite of thought-eliciting techniques on the OpenOrca math subset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step mathematical problems requiring logical decomposition and verification of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated using CoT, ThoT, ReRead CoT/ThoT, CoVe; metrics: coherence, uncertainty, cosine similarity to baselines, hallucination score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as noticeably weaker than larger models; the paper highlights marked limitations in CoVe and hallucination metrics and overall lower reliability. No unambiguous numeric aggregate reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against larger variants and GPT-4; substantially lower on verification and hallucination control.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles with coherence and verification; high hallucination relative to larger models; limited capability on tasks needing robust multi-step formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Illustrates how small parameter-count models can be inadequate for strict multi-step logical reasoning tasks despite using elicitation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8574.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llava-1.6 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llava (1.6/7B?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal/instruction-tuned model (listed as Llava-1.6 7B) evaluated and reported to have moderate performance but struggles with coherence and verification compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llava-1.6 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B parameter model (Llava-1.6) included in the benchmark; evaluated on the OpenOrca mathematical problems using thought-eliciting prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Mathematical multi-step reasoning from OpenOrca, to test explainability and stepwise deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Tested with Chain-of-Thought, Thread-of-Thought, ReRead variants, CoVe and hallucination scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Moderate performance reported; the paper notes struggles with coherence and verification despite some moderate ThoT and ReRead results. Precise numeric scores are not clearly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than top models (GPT-4, Gemma-2) and similar or slightly better than smaller Llama variants depending on the elicitation technique.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles particularly on coherence and verification tasks; higher hallucination than the largest models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction-tuned 7B models can benefit from elicitation techniques but still have measurable gaps in strict logical/verification tasks relative to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8574.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8574.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nemotron-mini-4B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nemotron-mini-4B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4B-parameter instruct-tuned model evaluated in the benchmark; reported to struggle with coherence and verification on complex reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Nemotron-mini-4B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 4 billion parameter instruct-tuned model included in the study; evaluated on the OpenOrca mathematical subset with multiple thought-eliciting techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>4B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OpenOrca — mathematical problem-solving (FLAN-derived subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step math problems that require stepwise logical reasoning and verification of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with CoT, ThoT, ReRead, CoVe; metrics include coherence, uncertainty, cosine similarity and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to have constrained reasoning capabilities with struggles in coherence and verification compared to larger models; no unambiguous numeric aggregate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than larger models and is comparable to other small models (Llama 1B/3B) on many metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Constrained reasoning capacity on multi-step problems; higher hallucination and lower coherence in verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller instruct-tuned models show limited gains from elicitation techniques and require more targeted improvements to be reliable on strict logical reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BELL: Benchmarking the Explainability of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Thread of Thought Unraveling Chaotic Contexts <em>(Rating: 2)</em></li>
                <li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models <em>(Rating: 2)</em></li>
                <li>Re-Reading Improves Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Verification Reduces Hallucination in Large Language Models <em>(Rating: 2)</em></li>
                <li>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Orca: Progressive Learning from Complex Explanation Traces of GPT-4 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8574",
    "paper_id": "paper-278165164",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large closed-source foundation language model evaluated in this paper; reported as the best-performing model on the paper's reasoning benchmark, with strong sequential multi-step reasoning and low hallucination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source large language model used as a high-performing reference; described in the paper as demonstrating superior sequential reasoning and low hallucination on the OpenOrca mathematical problem-solving subset.",
            "model_size": null,
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "A subset of the OpenOrca dataset focused on mathematical problem-solving drawn from the FLAN collection; requires multi-step, sequential logical and arithmetic reasoning.",
            "method_or_approach": "Evaluated using thought-eliciting prompting techniques including Chain-of-Thought (CoT), Thread-of-Thought (ThoT), ReRead variations (ReRead CoT/ReRead ThoT), and Chain-of-Verification (CoVe); metrics used include coherence, uncertainty, cosine similarity to baseline responses, and a hallucination score.",
            "performance": "Quantitative highlights reported: CoT score 85.28; ThoT score 92.39; other ReRead/verification metrics described as leading. Reported hallucination score: 19.42 (lowest among evaluated models). Overall described as consistently outperforming other evaluated models.",
            "baseline_comparison": "Compared against several smaller/open models (Gemma-2 9B, Mistral 7B, Llama variants, Llava, Nemotron-mini). GPT-4 consistently outperforms these models across CoT, ThoT, ReRead techniques and exhibits substantially lower hallucination.",
            "limitations_or_failures": "No detailed failure cases provided for GPT-4 beyond general subject-area caveats; paper notes that even with strong performance GPT-4 can hallucinate (though less than others) and that different elicitation techniques vary in effectiveness.",
            "insights_or_conclusions": "Large model scale and capacity strongly correlate with better multi-step logical reasoning and lower hallucination; combining read/re-read and verification prompting further improves output faithfulness and explainability.",
            "uuid": "e8574.0",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Gemma-2 9B",
            "name_full": "Gemma-2 (9B)",
            "brief_description": "A 9B parameter model evaluated in the benchmark; reported to follow GPT-4 in several categories but with minor gaps in reasoning coherence and hallucination control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2 9B",
            "model_description": "An open/available 9 billion parameter foundation model evaluated on the OpenOrca math subset using the paper's thought-eliciting techniques.",
            "model_size": "9B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Multi-step mathematical reasoning problems from the OpenOrca dataset requiring stepwise decomposition and verification.",
            "method_or_approach": "Evaluated with Chain-of-Thought, Thread-of-Thought, ReRead variants and Chain-of-Verification prompts; scored with coherence, uncertainty, cosine similarity to baselines and hallucination metric.",
            "performance": "Reported qualitatively as close behind GPT-4 in several categories; the paper states Gemma-2 and Mistral 'followed closely' though with some gaps in hallucination control. Exact numeric scores for Gemma-2 are not unambiguously reported in the text.",
            "baseline_comparison": "Compared directly to GPT-4 and smaller Llama/Llava/Nemotron models; Gemma-2 trails GPT-4 but surpasses smaller constrained models on many elicitation techniques.",
            "limitations_or_failures": "Minor gaps in reasoning and hallucination control relative to GPT-4; specific failure cases are not detailed.",
            "insights_or_conclusions": "Mid-to-large scale open models can approach top closed-model performance with thought-eliciting prompting, but still lag on hallucination and verification tasks compared to the largest models.",
            "uuid": "e8574.1",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Mistral 7B",
            "name_full": "Mistral (7B)",
            "brief_description": "A 7B-parameter model evaluated in the benchmark; showed competitive performance among open models but with observable gaps versus GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B",
            "model_description": "A 7 billion parameter foundation model evaluated on the OpenOrca math problems using the suite of thought-eliciting techniques.",
            "model_size": "7B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Problems requiring multi-step mathematical/logical reasoning; evaluation focused on faithfulness and stepwise explanations.",
            "method_or_approach": "Assessed with CoT, ThoT, ReRead variants and CoVe; metrics: coherence, uncertainty, cosine similarity, hallucination score.",
            "performance": "Described as following Gemma-2 closely in several categories; numeric details are not clearly and unambiguously provided in the paper text.",
            "baseline_comparison": "Compared to GPT-4 and other open models; outperforms smaller Llama variants but underperforms GPT-4.",
            "limitations_or_failures": "Displayed minor gaps in reasoning and hallucination control relative to the best model (GPT-4); specific failure examples not provided.",
            "insights_or_conclusions": "Demonstrates that 7B-scale models can perform competitively with careful elicitation, but scale and architecture remain important for top-tier logical reasoning.",
            "uuid": "e8574.2",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3.2 3B",
            "name_full": "Llama-3.2 (3B)",
            "brief_description": "A 3B-parameter variant of the Llama-3.2 family evaluated in the benchmark; reported moderate performance with limitations in verification and coherence on complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.2 3B",
            "model_description": "A 3-billion parameter Llama-3.2 variant included in the evaluation set, tested with multiple thought-eliciting prompting strategies on the OpenOrca math subset.",
            "model_size": "3B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Structured math reasoning requiring decomposition into intermediate steps; used to evaluate coherence and verification ability.",
            "method_or_approach": "Evaluated with Chain-of-Thought, Thread-of-Thought, ReRead variants and Chain-of-Verification, scored by coherence, uncertainty, cosine similarity and hallucination.",
            "performance": "Reported as moderate: performs reasonably on ThoT and ReRead techniques but shows limitations on coherence and verification compared to larger models; exact numeric values are not unambiguously reported.",
            "baseline_comparison": "Underperforms larger models (GPT-4, Gemma-2, Mistral) and shows worse verification/hallucination behavior relative to them.",
            "limitations_or_failures": "Significant limitations in coherence and verification for more complex prompts; higher hallucination and lower reliability on multi-step deductions.",
            "insights_or_conclusions": "Smaller parameter budgets limit reliable multi-step logical reasoning; combining elicitation techniques helps but does not fully close the gap to larger models.",
            "uuid": "e8574.3",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3.2 1B",
            "name_full": "Llama-3.2 (1B)",
            "brief_description": "A 1B-parameter Llama-3.2 model included in the evaluation; reported to have notable limitations in coherence, verification and hallucination relative to larger models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.2 1B",
            "model_description": "A 1-billion parameter variant of Llama-3.2 used in the paper's experiments; assessed across the same suite of thought-eliciting techniques on the OpenOrca math subset.",
            "model_size": "1B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Multi-step mathematical problems requiring logical decomposition and verification of intermediate steps.",
            "method_or_approach": "Evaluated using CoT, ThoT, ReRead CoT/ThoT, CoVe; metrics: coherence, uncertainty, cosine similarity to baselines, hallucination score.",
            "performance": "Reported as noticeably weaker than larger models; the paper highlights marked limitations in CoVe and hallucination metrics and overall lower reliability. No unambiguous numeric aggregate reported.",
            "baseline_comparison": "Compared against larger variants and GPT-4; substantially lower on verification and hallucination control.",
            "limitations_or_failures": "Struggles with coherence and verification; high hallucination relative to larger models; limited capability on tasks needing robust multi-step formal reasoning.",
            "insights_or_conclusions": "Illustrates how small parameter-count models can be inadequate for strict multi-step logical reasoning tasks despite using elicitation techniques.",
            "uuid": "e8574.4",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llava-1.6 7B",
            "name_full": "Llava (1.6/7B?)",
            "brief_description": "A multimodal/instruction-tuned model (listed as Llava-1.6 7B) evaluated and reported to have moderate performance but struggles with coherence and verification compared to larger models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llava-1.6 7B",
            "model_description": "A 7B parameter model (Llava-1.6) included in the benchmark; evaluated on the OpenOrca mathematical problems using thought-eliciting prompts.",
            "model_size": "7B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Mathematical multi-step reasoning from OpenOrca, to test explainability and stepwise deduction.",
            "method_or_approach": "Tested with Chain-of-Thought, Thread-of-Thought, ReRead variants, CoVe and hallucination scoring.",
            "performance": "Moderate performance reported; the paper notes struggles with coherence and verification despite some moderate ThoT and ReRead results. Precise numeric scores are not clearly reported.",
            "baseline_comparison": "Performs worse than top models (GPT-4, Gemma-2) and similar or slightly better than smaller Llama variants depending on the elicitation technique.",
            "limitations_or_failures": "Struggles particularly on coherence and verification tasks; higher hallucination than the largest models.",
            "insights_or_conclusions": "Instruction-tuned 7B models can benefit from elicitation techniques but still have measurable gaps in strict logical/verification tasks relative to larger models.",
            "uuid": "e8574.5",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Nemotron-mini-4B-instruct",
            "name_full": "Nemotron-mini-4B-instruct",
            "brief_description": "A 4B-parameter instruct-tuned model evaluated in the benchmark; reported to struggle with coherence and verification on complex reasoning prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Nemotron-mini-4B-instruct",
            "model_description": "A 4 billion parameter instruct-tuned model included in the study; evaluated on the OpenOrca mathematical subset with multiple thought-eliciting techniques.",
            "model_size": "4B",
            "reasoning_task_name": "OpenOrca — mathematical problem-solving (FLAN-derived subset)",
            "reasoning_task_description": "Multi-step math problems that require stepwise logical reasoning and verification of intermediate steps.",
            "method_or_approach": "Evaluated with CoT, ThoT, ReRead, CoVe; metrics include coherence, uncertainty, cosine similarity and hallucination.",
            "performance": "Reported to have constrained reasoning capabilities with struggles in coherence and verification compared to larger models; no unambiguous numeric aggregate provided.",
            "baseline_comparison": "Performs worse than larger models and is comparable to other small models (Llama 1B/3B) on many metrics.",
            "limitations_or_failures": "Constrained reasoning capacity on multi-step problems; higher hallucination and lower coherence in verification tasks.",
            "insights_or_conclusions": "Smaller instruct-tuned models show limited gains from elicitation techniques and require more targeted improvements to be reliable on strict logical reasoning benchmarks.",
            "uuid": "e8574.6",
            "source_info": {
                "paper_title": "BELL: Benchmarking the Explainability of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Thread of Thought Unraveling Chaotic Contexts",
            "rating": 2,
            "sanitized_title": "thread_of_thought_unraveling_chaotic_contexts"
        },
        {
            "paper_title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Re-Reading Improves Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "rereading_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "logicofthought_injecting_logic_into_contexts_for_full_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
            "rating": 2,
            "sanitized_title": "orca_progressive_learning_from_complex_explanation_traces_of_gpt4"
        }
    ],
    "cost": 0.0123015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BELL: Benchmarking the Explainability of Large Language Models</p>
<p>Ahmed Syed 
Vokkaliga Bharathi 
Jagadish Ganesh 
Babu 
Karthick Selvaraj </p>
<p>Infosys Responsible AI Office</p>
<p>ReddySiva Naga Parvathi Devi Sravya Kappala</p>
<p>Responsible AI Office Infosys Limited
BangaloreIndia</p>
<p>Infosys/Infosys-Responsible-AI-Toolkit</p>
<p>BELL: Benchmarking the Explainability of Large Language Models
D41F239512A2244773CFD14C5063C1AC
Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing (NLP), yet their decision-making processes often lack transparency.This opaqueness raises significant concerns regarding trust, bias, and model performance.To address these issues, understanding and evaluating the interpretability of LLMs is crucial.This paper introduces a standardized benchmarking technique, BELL (Benchmarking the Explainability of Large Language Models), designed to evaluate the explainability of large language models.To assess the reasoning capabilities of LLMs, a variety of thought-eliciting techniques are utilized, including Chain-of-Thought (CoT), Thread-of-Thought (ThoT), ReRead CoT, ReRead ThoT, Chain-of-Verifications (CoVe), Hallucination, Graph-of-Thought (GoT), and Logic-of-Thought (LoT).These evaluations are conducted using open-orca dataset, with the quality of explanations measured through a comprehensive set of metrics.These metrics encompass factors such as coherence, uncertainty, hallucination and cosine similarity relative to the model's internal reasoning.By systematically evaluating the performance of various LLMs against this benchmark, the goal is to identify models that offer more transparent and reliable explanations.This benchmarking technique serves as a valuable tool for researchers and practitioners in selecting appropriate LLMs for tasks that demand interpretability and accountability.To foster reproducibility and facilitate wider adoption, we have implemented each of the proposed and utilized explainability techniques as open-source software, with the code and necessary resources available at https://github.</p>
<p>Introduction</p>
<p>Large language models have revolutionized natural language processing and generative Artificial Intelligence (AI), as shown by numerous foundational studies [1].These models' exceptional capabilities have attracted significant attention, enabling a wide range of applications.LLMs are utilized for tasks such as translation [2], content generation, content summarization, article writing [3], as well as enhancing search functions (Bing Chat [4]) etc., The impact of LLMs extends to fields like software development, with models like Code Llama [5] aiding engineers.Their applications also span finance sector [6], scientific research [7] [8], including areas such as arts [9], education [10], oceanography [11], law [12], political science [13], medicine [14] [15], showcasing their broad and diverse influence.</p>
<p>However, the exponential rise in use of LLMs also brings challenges related to their explainability and interpretability.First, the complexity of LLMs, driven by their diverse outputs and generative capabilities, makes it difficult to understand how they arrive at specific responses.This lack of transparency can lead to unpredictable [16] and potentially misleading outputs, complicating efforts to ensure trust in their performance.Second, biases [17] in training datasets can affect the fairness and interpretability of LLM outputs.For example, a model might favor certain perspectives due to data biases, making it difficult to explain or justify certain outputs.Third, the inclusion of sensitive data in training sets can compromise the interpretability of LLMs in ensuring privacy [18].Additionally, high user expectations for accurate and human-aligned responses highlight the need for LLMs to be interpretable in ways that align with human values, as misalignment can lead to ethical concerns and reduce trust in their applications.</p>
<p>To tackle these challenges, it is essential to address the core issue of benchmarking the explainability and interpretability of LLMs.How can the transparency of their decision-making processes be systematically evaluated?Establishing robust benchmarking like BELL enables researchers to critically evaluate and compare LLMs, fostering the development of models that prioritize transparency and accountability.This approach not only addresses current concerns but also sets the foundation for future advancements in trustworthy AI systems.Key contributions of this paper include:</p>
<p>• Standardized Benchmarking: The paper introduces standardized benchmarking, BELL, employing diverse thought-eliciting techniques such as CoT, ThoT, GoT, ReRead, LoT, CoVe, and Hallucination to systematically evaluate the reasoning capabilities of LLMs.These methods provide a structured approach to assess the interpretability and transparency of model decision-making.• Metrics: This paper describes a set of evaluation metrics, including coherence, uncertainty, and cosine similarity, to quantitatively measure the quality and transparency of LLM explanations.</p>
<p>A key contribution of this work is the public release of our implementation of explainability techniques and respective evaluation metrics enabling the community to readily adopt and extend our findings.The codebase is accessible at Infosys-Responsible-AI-Toolkit/responsible-ai-llm-explain at master • Infosys/Infosys-Responsible-AI-Toolkit</p>
<p>The structure of the paper is as follows: Section 2 presents the background and related work, while Section 3 introduces proposed benchmarking designed to evaluate the explainability of LLMs.Section 4 details the datasets and evaluation metrics, and Section 5 outlines the findings of the study.Section 6, which is the last section, concludes with insights and directions for future research.</p>
<p>Background</p>
<p>Interpretability</p>
<p>The rise of highly effective closed-source LLMs, such as ChatGPT, has sparked significant interest in these models.At the same time, open-source alternatives like Granite [19], Llama [20] have gained notable popularity.The growing adoption of LLMs has also captured the attention of researchers focused on model interpretability and explainability.Understanding how LLM capabilities evolve during training is critical to analyzing their formation and functionality in downstream tasks.Additionally, investigating inference processes and the influence of context is vital for uncovering insights into model behavior.However, this analysis is challenging due to the vast number of parameters and complex non-linear structures inherent to LLMs.</p>
<p>Most research on explainability and interpretability focuses on LLMs at a macroscopic level, analyzing their behaviors qualitatively.Many studies in this area utilize prompts to explore these aspects [21] [22].</p>
<p>Nonetheless, a smaller subset of works explores the inner workings of LLMs using probing methods such as prompt tuning [23] or activation analysis.In contrast, mechanical interpretability aims to develop theories based on the fundamental behaviors of transformers.These approaches focus on examining the core components that drive LLM creation and analyze their internal representations by injecting or training specific modules.However, applying probing or mechanistic interpretability techniques to closed foundation models presents significant challenges, as their internal architectures and parameters are often not accessible.In such cases, exploring thought-eliciting techniques offers an alternative, providing a means to validate the reasoning capabilities of models, regardless of whether they are open-source or closed-source.</p>
<p>Reasoning</p>
<p>Reasoning in large language models (LLMs) encompasses their ability to logically analyze information, address problems, and formulate conclusions based on the provided context.This capability enables LLMs to perform tasks that demand critical thinking, such as drawing inferences, identifying relationships, and making decisions.Techniques like Chain-of-Thought (CoT) prompting, Thread-of-Thought (ThoT) methodologies, Graph-of-Thoughts (GoT) and other frameworks [24][25][26][27][28][29][30] are widely employed to enhance reasoning capacities.These approaches promote the decomposition of complex problems into smaller, intermediate steps, thereby making the reasoning process more structured and efficient.</p>
<p>The integration of reasoning significantly enhances the explainability of LLMs by clarifying their decision-making processes.By explicitly detailing the steps leading to a conclusion, reasoning ensures that the model's output becomes more transparent and comprehensible.This clarity enables users to understand the logic behind the results, facilitating a more thorough evaluation of the model's reliability and accuracy.Moreover, the structured reasoning process bolsters confidence in the system by demonstrating how outcomes are derived.</p>
<p>Furthermore, reasoning fosters seamless collaboration between humans and AI.By aligning computational problem-solving strategies with human thought processes, LLMs become more intuitive and easier to interact with.This alignment enhances user trust and reliance, thereby improving the practical applicability and effectiveness of AI systems in diverse real-world scenarios.</p>
<p>The advancements in reasoning frameworks such as CoT, ThoT, and GoT demonstrate significant strides in enhancing LLMs logical reasoning capabilities.However, despite these innovations, a critical gap remains in the benchmarking of LLMs for explainability.Current benchmarks often emphasize performance metrics while overlooking the need for consistent, interpretable reasoning processes.This limitation becomes particularly concerning in tasks where trust and transparency are essential, as unfaithful reasoning can undermine the reliability of LLM outputs.Addressing this drawback, our work focuses on benchmarking LLMs for explainability, aiming to evaluate and improve their ability to provide coherent and transparent reasoning aligned with their predictions.</p>
<p>Benchmarking Explainability of Large Language Models (BELL)</p>
<p>Thought Eliciting Techniques in Large Language Models are strategies used to organize and direct the model's reasoning process, helping it produce responses that are logical, coherent, and focused.These methods emulate human problem-solving and critical thinking by simplifying complex questions into smaller steps, examining different possibilities, and improving clarity.</p>
<p>In this research, we investigate how employing thought-eliciting strategies enhances the ability of large language models to handle complex reasoning tasks effectively.We demonstrate that such reasoning capabilities naturally emerge in large models using straightforward prompt engineering techniques, including Chain-of-Thought, Thread-of-Thought, Graph-of-Thought, Logic-of-Thought, ReRead, and Chain-of-Verifications.</p>
<p>Fig- 1 illustrates a comprehensive architecture for leveraging LLMs to enhance reasoning and evaluation processes.The process begins with input, comprising dataset and an LLM which serves as the foundation for subsequent stages.The "Technique" &amp; "Thought-Elicitation" phase encompasses various reasoning techniques (discussed from section 3.1 to 3.2), including CoT, ThoT, ReRead CoT, ReRead Thot, CoVe, Hallucination.These components are guided by a prompt template to generate structured explanations.The outputs of this stage undergo evaluation based on metrics (discussed in section 4.2) such as cosine similarity, uncertainty quantification, coherence and hallucination.These evaluations collectively determine the final score, which represents the system's output, ensuring a robust and methodical approach to reasoning and decision-making tasks.This architecture highlights the integration of diverse reasoning methods with quantitative evaluation to assess the reasoning capabilities of LLMs in complex problem-solving.</p>
<p>Chain of Thought (CoT):</p>
<p>Chain of Thought (CoT) reasoning, introduced by Wei et al. [31], is a process where each step logically follows the previous one, forming a structured path to a solution.This method helps LLMs tackle complex problems by breaking them down into smaller steps, similar to how humans connect ideas in a step-by-step manner.For example, when solving a math problem, the model first defines the variables, then applies the relevant equations, and finally computes the result step by step.The CoT approach has shown to significantly improve model accuracy, particularly when the reasoning process involves multiple stages, as it allows the model to verify and refine its thought process at each step.This approach has proven valuable in increasing the capabilities of large models like GPT-3 in reasoning-intensive tasks, highlighting how such models can be prompted to generate detailed, step-by-step explanations.</p>
<p>Thread of Thoughts (ThoT):</p>
<p>Thread of Thought (ThoT) by Zhou et al. [32], is a novel reasoning framework designed to improve the logical coherence and consistency of large language models in complex tasks involving multiple interconnected pieces of information.For example, when analyzing a lengthy, disorganized document, the model is prompted to break down the text into manageable parts, examine each segment step by step -summarizing and analyzing as it proceeds -and then synthesize the insights to form a coherent understanding.This technique is particularly beneficial for tasks that require long-term reasoning or involve ambiguous or chaotic contexts, as it enables the model to adapt its thinking process as new information is introduced.ThoT reduces the risk of logical errors or inconsistencies that may arise when reasoning steps are treated in isolation, making the model's responses more reliable and interpretable.</p>
<p>ReRead</p>
<p>ReRead by Xu et al. [35], improves reasoning in large language models by incorporating a "re-reading" process, where the model revisits its prompt.This technique aims to enhance the model's ability to correct errors and adjust its reasoning based on a deeper understanding of the context.By prompting the model to re-examine its prompt, ReRead allows it to resolve ambiguities and inconsistencies.For instance, when faced with a complex problem, the model first reads the question to grasp the general context and then re-reads it to capture finer details and nuances.This dual-pass approach allows the model to refine its comprehension, leading to more accurate and coherent responses.Xu [35] demonstrate that this iterative approach significantly improves the model's performance on complex tasks, particularly those requiring multi-step reasoning or clarification of ambiguous queries.Further analyses show that with its adaptability, it can be easily integrated with any other thought eliciting technique to get better result.We have combined this with CoT &amp; Thot to get different improved explanation and benchmarked the same.</p>
<p>Chain of Verification (CoVe):</p>
<p>Chain-of-Verification (CoVe) by Dhuliawala et al. [36], addresses the issue of hallucination in large language models, where models generate plausible but incorrect information.CoVe introduces a multistep process where the model first drafts an initial response and then generates verification questions to fact-check its output.The model independently answers these questions to avoid bias from the initial response, followed by the generation of a final verified answer.This method helps correct errors by allowing the model to deliberate on its reasoning and ensure its accuracy.</p>
<p>Graph of Thought (GoT):</p>
<p>Graph-of-Thought (GoT) by Besta et al. [33], enhances large language models reasoning by organizing the thought process into a graph structure.In this framework, each reasoning step is represented as a node, and the relationships between steps are captured by edges, allowing for clear tracking of dependencies and logical flow.For instance, when planning a trip, GoT can explore different date options while also considering how each date impacts potential accommodation and activity choices, creating a network of interconnected decisions.This allows for more flexible and context-aware problem-solving compared to a simple chain of reasoning.GoT improves the model's ability to reason coherently and generate accurate responses.It also enhances interpretability, as the graph structure provides a transparent view of the model's reasoning path.</p>
<p>Logic of Thought (LoT):</p>
<p>Logic of Thought (LoT) by Tongxuan et al. [34], integrates logical reasoning directly into the contextual understanding of large language models.This approach focuses on explicitly infusing logical principles, such as deductive and inductive reasoning, into the model's decision-making process, allowing it to perform more structured and accurate reasoning.By embedding logic within the context of the problem, LoT enhances the model's ability to draw reliable conclusions, particularly in tasks requiring formal reasoning or complex problem-solving.The method also enhances interpretability by making the reasoning steps transparent and logically grounded.</p>
<p>Experimental Setup</p>
<p>The experiments were conducted on a 64-bit Windows 10 operating system utilizing an Nvidia NC16as_T4_v3 GPU configuration with 16 GB of dedicated GPU memory.The experimental environment was configured with Python, along with the requisite libraries and dependencies, to support the implementation and execution of transformer-based models.</p>
<p>Dataset</p>
<p>OpenOrca dataset [37] is an augmented version of the FLAN Collection, consisting of approximately 1 million completions from GPT-4 and 3.2 million completions from GPT-3.5.Each entry includes a question from the FLAN collection, submitted to either GPT-4 or GPT-3.5, with the corresponding response recorded.This dataset is suitable for tasks such as language modeling, text generation, and text augmentation, making it a valuable resource for developing and evaluating generative AI models for reasoning.</p>
<p>The dataset encompasses a diverse range of categories such as, mathematical problem-solving, sentiment analysis, etc.For the purpose of this research, the evaluation was specifically conducted on the mathematical problem-solving category.</p>
<p>Quality Evaluation Metrics</p>
<p>• Cosine Similarity: Cosine Similarity is a method used to evaluate how closely a generated response aligns with a base or reference response.Both generated and reference responses are transformed into vector representations in the model's embedding space, and cosine similarity is calculated to measures the degree of semantic similarity between them.This technique helps assess whether the model's output captures the intended meaning or context of the reference, making it valuable for evaluating performance in tasks like text generation, summarization, and question answering using eq.( 1).
𝐶𝑜𝑠𝑖𝑛𝑒 𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 = 𝐴.𝐵 ||𝐴|| ||𝐵||(1)
Here,  and  are the embeddings of the generated and reference responses, . is their dot product, |||| and |||| are their magnitudes calculated as shown below.
||𝐴|| = √∑ 𝐴 𝑖 2 𝑛 𝑖=1 , ||𝐵|| = √∑ 𝐵 𝑖 2 𝑛 𝑖=1
where,   and   are the components of the vectors  and , respectively, and  is the dimensionality of the vectors.</p>
<p>• Uncertainty: Uncertainty indicates how confident the model is in its predictions or generated responses.It highlights situations where the model might lack clarity or face ambiguity in understanding an input.For instance, uncertainty is higher with incomplete or vague queries.</p>
<p>By assessing uncertainty, it becomes easier to identify responses that may need validation or refinement.This metric is crucial for improving model reliability and performance.It also helps in tasks like filtering low-confidence outputs or integrating human-in-the-loop systems for better decision-making.• Coherence: Coherence refers to how logically consistent and contextually aligned a generated response is with the input or surrounding text.It measures the flow of ideas and the internal consistency of the response.A coherent response stays on topic and maintains a clear, structured narrative throughout.Coherence ensures that the model's output makes sense and remains relevant to the input, improving the quality of text generation, summarization, and dialogue.High coherence is essential for generating human-like and contextually appropriate responses.</p>
<p>• Hallucination: Hallucination in Large Language Models refers to the phenomenon where the model generates outputs that are factually incorrect, nonsensical, or fabricated, even though they may sound plausible or grammatically correct.With the help of G-Eval metrics and cosine similarity of orca dataset, hallucination score can be detected from the LLM response.</p>
<p>Hallucination Score = 1 -(0.8 * Average of Evaluation Metrics) -(0.2 * Average Similarity Score)</p>
<p>Findings</p>
<p>The performance evaluation of language model's explainability is conducted through a detailed assessment process.This involves calculating key metrics such as coherence and uncertainty for each generated response to measure the logical consistency and confidence in the outputs.Additionally, the cosine similarity between each response and a predefined baseline response is computed to evaluate the alignment with expected outcomes.For a comprehensive evaluation, these individual metrics are averaged to derive a combined score for each response.Finally, an overall explainability final score for the model is obtained by taking the average across all responses in the dataset.This approach provides a systematic and quantitative measure to assess the explainability of language models effectively.The evaluations results are illustrated from Graph 1 -Graph 7.</p>
<p>• (D) = The dataset of responses.</p>
<p>• (R_i) = The (i)-th response in the dataset (D).</p>
<p>• (E_i) = The generated explanation for the (i)-th response (R_i)</p>
<p>• (B_i) = The predefined baseline response for the (i)-th response (R_i).</p>
<p>• ({Coherence}(E_i)) = The coherence score of the explanation related to response (R_i).</p>
<p>• ({Uncertainty}(E_i)) = The uncertainty score of the explanation related to response (R_i).</p>
<p>• ({CosSim}(E_i, B_i)) = The cosine similarity score between the explanation (E_i) and the baseline response (B_i).• (n) = The total number of responses in the dataset (D).</p>
<p>• ({OverallScore}) = The final overall explainability score for the given eliciting technique.</p>
<p>• {OverallScore} = frac{1}{n} \sum_{i=1}^{n} \left( \frac{{Coherence}(E_i) + {Uncertainty}(E_i) + {CosSim}(E_i, B_i)}{3} \right) • ({Hallucination}(E_i)) = The hallucination score of the explanation related to response (R_i).Overall, these findings emphasize the superiority of larger models in reasoning tasks and the need for significant improvements in smaller architectures.</p>
<p>Conclusion and Future Work</p>
<p>The findings presented herein underscore the multifaceted nature of explainability.There is no single "best" XAI technique; rather, the optimal choice depends heavily on the specific application domain, the characteristics of the AI model being explained, and the intended audience of the explanations.This study evaluated the performance of several reasoning techniques across various models, revealing that larger models, such as GPT-4, consistently outperformed smaller ones.This is demonstrated using reasoning techniques such as, Chain of Thought (CoT), Thread of Thought (ThoT), ReRead techniques, and Chain of Verification (CoVe).GPT-4 demonstrated superior sequential reasoning and minimal hallucination, establishing its dominance in reasoning tasks.Conversely, smaller models, including Llama-3.2 1B and Llava-1.6 7B, displayed significant limitations in coherence and verification, underscoring the need for targeted improvements in their reasoning capabilities.The results emphasize that while smaller model's performance is moderate, larger models remain more reliable for complex reasoning tasks.</p>
<p>Future research explores a broader range of models, particularly with latest and emerging architectures that offer enhancements in reasoning capabilities.Additionally, we plan to assess the model's reasoning capabilities with latest eliciting techniques such as Graph of Thought, Logic of Thought using diverse datasets from fields like healthcare, law, and scientific research to understand how well models generalize across different domains.</p>
<p>As researchers, our aim in this white paper has been to lay a foundational framework for the rigorous evaluation and comparison of Explainable AI (XAI) techniques.We have explored a range of methodologies, metrics, and considerations crucial for understanding the strengths, weaknesses, and applicability of various XAI approaches.Our benchmarking efforts, while providing initial insights, serve as a steppingstone in the ongoing journey towards building truly transparent and trustworthy artificial intelligence systems.We strongly encourage readers -fellow researchers, practitioners, and policymakers -to leverage the insights and methodologies presented in this paper in their own work.Specifically, we urge you to leverage our code that is available here at Infosys-Responsible-AI-Toolkit/responsible-ai-llm-explain at master • Infosys/Infosys-Responsible-AI-Toolkit and extend the</p>
<p>Fig 1 :
1
Fig 1: Proposed architecture</p>
<p>, we have assessed reasoning techniques, including Chain of Thought, Thread of Thought, ReRead CoT, ReRead ThoT, Chain of Verification, and Hallucination, across multiple models using cosine similarity, uncertainty and coherence.Among the models evaluated, GPT-4 consistently outperformed others, demonstrating robust sequential reasoning with a CoT score of 85.28 and leading performance in ThoT (92.39) and ReRead techniques.Gemma-2 9B and Mistral 7B followed closely in several categories, though they displayed minor gaps in reasoning and hallucination control compared to GPT-4.Smaller models such as Llama-3.2 1B exhibited notable limitations, particularly in CoVe and hallucination scores, where their reliability was significantly lower than the larger counterparts that are evaluated.Despite moderate performance in ThoT and ReRead techniques, models like Llava-1.6 7B and Nemotron-mini-4B-instruct struggled with coherence and verification tasks, reflecting their constrained reasoning capabilities.Hallucination scores further highlighted discrepancies, with GPT-4 maintaining the lowest score of 19.42, while smaller models faced challenges exceeding acceptable thresholds.
In this studyGPT-4Gemma-2 9B80 10085.392.491.9191.3785.1CoT ThoT100 8072.991.790.4191.883.1CoT ThoTScores40 60 2019.42ReRead CoT ReRead ThoT CoVeScores40 60 2025.07ReRead CoT ReRead ThoT CoVe0Hallucination0HallucinationThought Elicitation TechniquesThought Elicitation TechniquesGraph 1: Evaluation of GPT-4Graph 2: Evaluation of Gemma-2 9BMistral 7BLlama-3.2 3B10079.990.379.9489.7188.3CoT1007788.788.7686.882.2CoT80ThoT80ThoTScores40 6026.4ReRead CoT ReRead ThoTScores40 6031.96ReRead CoT ReRead ThoT20CoVe20CoVe0Hallucination0HallucinationThought Elicitation TechniquesThought Elicitation TechniquesGraph 3: Evaluation of Mistral 7BGraph 4: Evaluation of Llama-3.2 3BLlava-1.6 7BNemotron-Mini 4B Instruct80 10058.886.985.9387.9582.1CoT ThoT100 8079.875.474.9478.3778.5CoT ThoTScores40 6025.1ReRead CoT ReRead ThoTScores40 6026.4ReRead CoT ReRead ThoT20CoVe20CoVeThought Elicitation Techniques • ({Model_Score}) = Avg ({OverallScore}) -({Hallucination}(E_i)) Thought Elicitation Techniques 0 Hallucination 0 HallucinationModel Graph 5: Evaluation of Llava-1.6 7B CoT ThoT ReRead CoT Graph 6: Evaluation of Nemotron-Mini-4B-Instruct Reread ThoT CoVe Hallucination Score ModelGPT-485.2892.3991.91 Llama-3.2 1B 91.3785.1419.4287.78Gemma-2 9B Mistral 7B Llama-3.2 3B Llava-1.6 7B Nemotron-mini-4B-instruct72.91 79.93 76.95 58.81 79.79Scores91.73 90.34 88.7 86.87 75.7 100 80 75.44 40 60 2083.290.41 79.94 88.76 85.93 84.03 83.19 74.9467.591.8 89.71 86.8 87.95 CoT ThoT 78.37 34.33 ReRead CoT 83.08 88.33 82.21 82.13 78.46 ReRead ThoT CoVe25.07 26.4 31.96 25.1 26.484.15 83.64 81.91 79.43 76.95Llama-3.2 1B75.783.19 Thought Elicitation Techniques 84.03 83.19 0 Hallucination 67.5134.3376.55Graph 7: Evaluation of Llama-3.2 1B
work presented here by developing new explainability techniques &amp; evaluation metrics, curating diverse and challenging benchmark datasets, and evaluating a wider range of emerging XAI techniques.Abbreviations:• BELL -Benchmarking the Explainability of Large language model
Natural language processing: State of the art, current trends and challenges. Multimedia tools and applications. Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh, 2023</p>
<p>Multilingual machine translation with large language models: Empirical results and analysis. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li, 2023</p>
<p>Wordcraft: story writing with large language models. Ann Yuan, Andy Coenen, Emily Reif, Daphne Ippolito, Intelligent User Interfaces. 2022</p>
<p>Enhancing search using large language models. 2023</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, Bloomberggpt: A large language model for finance. 2023</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023</p>
<p>Pllama: An open-source large language model for plant science. Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson, 2024</p>
<p>Artgpt-4: Artistic vision-language understanding with adapter-enhanced minigpt-4. Zhengqing Yuan, Huiwen Xue, Xinyi Wang, Yongming Liu, Zhuanzhe Zhao, Kun Wang, 2023</p>
<p>Jingsi Yu, Junhui Zhu, Yujie Wang, Yang Liu, Hongxiang Chang, Jinran Nie, Cunliang Kong, Ruining Chong, Jiyuan Xinliu, Luming An, Lu, Mingwei Fang, and Lin Zhu. Taoli llama. 2023</p>
<p>. Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him, Wong Tim, Sai-Kit Yeung, 2023Marinegpt: Unlocking secrets of "ocean" to the public</p>
<p>Disc-lawllm: Fine-tuning large language models for intelligent legal services. Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, Zhongyu Wei, 2023</p>
<p>Large language models and political science. Mitchell Linegar, Rafal Kocielnik, Michael Alvarez, Frontiers in Political Science. 512570922023</p>
<p>Alpacare:instruction-tuned large language models for medical application. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold, 2023</p>
<p>Biomedgpt: A unified and generalist biomedical generative pretrained transformer for vision, language, and multimodal tasks. Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, Lifang He, Brian D Davison, Quanzheng Li, Yong Chen, Hongfang Liu, Lichao Sun, 2023</p>
<p>Factuality challenges in the era of large language models. Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee Diresta, Emilio Ferrara, Scott Hale, Alon Halevy, 2023</p>
<p>Unraveling the link between translations and gender bias in llms. Appen , 2023</p>
<p>Alexander Wei, Nika Haghtalab, Jacob Steinhardt, Jailbroken, arXiv:2307.02483How does llm safety training fail?. 2023arXiv preprint</p>
<p>Granite Foundation Models. 2024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023arXiv preprint</p>
<p>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, Empirical Methods in Natural Language Processing. 2022</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS2022</p>
<p>Soft-prompt Tuning for Large Language Models to Evaluate Bias. Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyedkalantari, Faiza Khan, Khattak , arXiv:2306.047352023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, CoRR, abs/2305.106012023a</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, CoRR, abs/2308.096872023</p>
<p>Beyond chain-of-thought, effective graph-ofthought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, CoRR, abs/2305.165822023b</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Chain of Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, E H Chi, F Xia, Q Le, D Zhou, ArXiv, abs/2201.119032022</p>
<p>Thread of Thought Unraveling Chaotic Contexts. Y Zhou, X Geng, T Shen, C Tao, G Long, J Lou, J Shen, ArXiv, abs/2311.087342023</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej &amp; Besta, Nils &amp; Blach, Kubicek, Robert &amp; Gerstenberger, Michal &amp; Podstawski, Lukas &amp; Gianinazzi, Joanna &amp; Gajda, Tomasz &amp; Lehmann, Hubert &amp; Niewiadomski, Piotr &amp; Nyczyk, Torsten Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models. Liu Tongxuan, Wenjiang &amp; Xu, Weizhe &amp; Huang, Xingyu &amp; Wang, Jiaxing &amp; Wang, Hailong Yang, Jing Li, 10.48550/arXiv.2409.175392024</p>
<p>Re-Reading Improves Reasoning in Large Language Models. X Xu, C Tao, T Shen, C Xu, H Xu, G Long, J Lou, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Chain-of-Verification Reduces Hallucination in Large Language Models. Shehzaad &amp; Dhuliawala, Komeili, Mojtaba, Jing &amp; Xu, Roberta &amp; Raileanu, Xian &amp; Li, Asli, Jason Celikyilmaz &amp; Weston, 3563-3578.10.18653/v1/2024.findings-acl.2122024</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Hassan, Awadallah , Orca: Progressive Learning from Complex Explanation Traces of GPT-4. </p>            </div>
        </div>

    </div>
</body>
</html>
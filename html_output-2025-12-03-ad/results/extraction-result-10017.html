<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-10017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-10017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-10017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-170.html">extraction-schema-170</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <p><strong>Paper ID:</strong> paper-276969720</p>
                <p><strong>Paper Title:</strong> AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion</p>
                <p><strong>Paper Abstract:</strong> Early prediction of Alzheimer's disease (AD) is crucial to improving patient quality of life and treatment outcomes. However, current predictive methods face challenges such as insufficient multimodal information integration and the high cost of PET image acquisition, which limit their effectiveness in practical applications. To address these issues, this paper proposes an innovative model, AD-Diff. This model significantly improves AD prediction accuracy by integrating PET images generated through a diffusion process with cognitive scale data and other modalities. Specifically, the AD-Diff model consists of two core components: the ADdiffusion module and the multimodal Mamba Classifier. The ADdiffusion module uses a 3D diffusion process to generate high-quality PET images, which are then fused with MRI images and tabular data to provide input for the Multimodal Mamba Classifier. Experimental results on the OASIS and ADNI datasets demonstrate that the AD-Diff model performs exceptionally well in both long-term and short-term AD prediction tasks, significantly improving prediction accuracy and reliability. These results highlight the significant advantages of the AD-Diff model in handling complex medical image data and multimodal information, providing an effective tool for the early diagnosis and personalized treatment of Alzheimer's disease.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e10017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e10017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Amyloid-beta / amyloid plaques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amyloid-beta accumulation (amyloid plaques)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extracellular aggregation of amyloid-beta peptides forming plaques in brain regions (e.g., hippocampus) that are widely implicated as a molecular hallmark and proposed causal factor in Alzheimer's disease (AD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>molecular</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Accumulation of amyloid-beta (Aβ) peptides into extracellular plaques, including hippocampal amyloid accumulation associated with cognitive decline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The paper cites studies reporting that PET imaging found amyloid accumulation in hippocampus closely related to the speed of cognitive decline and that biomarker studies (CSF, PET) implicate amyloid pathology as correlated with AD development (references: Shi et al., 2017; Loddo et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No direct refuting empirical results are provided in this paper; limitations mentioned include that imaging/biomarker approaches are costly and not universally deployable, and the paper does not provide negative clinical trial or longitudinal contradictory evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>neuroimaging / biomarker</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Amyloid PET imaging (detects amyloid deposits in brain); CSF measures (amyloid-beta levels and tau/amyloid ratios) used as molecular biomarkers.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>The paper reports descriptive statements only (e.g., 'high sensitivity in marker detection' for biomarkers and that hippocampal amyloid accumulation relates to cognitive decline) but does not provide numeric sensitivity/specificity for amyloid PET or CSF amyloid in the cited studies within the text.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>mention of PET and CSF biomarker studies (imaging and biomarker observational studies cited)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Not specified in this paper for the cited amyloid findings (references to prior PET/biomarker studies).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Practical limitations: PET is expensive, requires radioactive tracers and specialized infrastructure limiting large-scale screening; biomarker assays (CSF) are costly/complex and have limited universality. The paper notes these operational limitations but does not present mechanistic controversies (e.g., failed amyloid-targeting trials) explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e10017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tau/β-amyloid (CSF ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cerebrospinal fluid tau to β-amyloid (Aβ) ratio</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ratio of tau protein to β-amyloid measured in cerebrospinal fluid (CSF), used as a molecular biomarker correlated with AD pathology and proposed as an early diagnostic indicator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>molecular</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Elevated tau relative to β-amyloid in CSF reflects neurofibrillary degeneration and amyloid pathology associated with AD progression.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The paper cites a 2022 study reporting that abnormal tau/β-amyloid ratios in CSF are highly correlated with AD development and provide a molecular basis for early diagnosis (Loddo et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No counter-evidence presented in this paper; the authors note general challenges with biomarker approaches (cost, technical demand, need for validation) rather than biological refutation.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>biomarker (CSF assay)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Laboratory measurement of CSF tau and Aβ concentrations and their ratio to infer AD-related pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Paper states these markers are 'highly correlated' with AD and 'provide a reliable molecular basis for early diagnosis' but does not report sensitivity/specificity or numeric performance metrics in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>reference to prior biomarker study (observational/biomarker validation)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Not specified in this paper for the cited CSF study.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Operational limitations highlighted: assays are costly, complex, and not easily scalable for large-screening; need further clinical validation for broad adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e10017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSE / S100 blood markers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuron-specific enolase (NSE) and S100 protein (blood biomarkers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Blood-based biomarkers (NSE, S100) proposed as economical alternatives to CSF biomarkers for detecting neuronal injury/AD pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>blood biomarker</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Elevated levels of NSE and S100 proteins in blood are associated with neuronal damage and were proposed to detect AD-related pathology comparably to CSF measures in preliminary studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The paper cites a blood-sample based study (Bi et al., 2020) proposing that NSE and S100 detection provided preliminary accuracy comparable to CSF analysis, suggesting potential for broader screening.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>The paper notes that further research is required for large-scale clinical validation; no explicit contradictory studies are described here.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>biomarker (blood assay)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Measurement of NSE and S100 proteins in blood samples as surrogate markers of neuronal injury and possible AD pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Described as 'accuracy comparable to traditional cerebrospinal fluid analysis in preliminary studies' but no numeric sensitivity/specificity or stage-specific performance reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>mention of preliminary biomarker study (observational / exploratory)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Not specified in this paper (referenced as preliminary blood-sample study).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations: Preliminary evidence only; requires larger-scale validation; potential variability and lower specificity compared with CSF or PET not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e10017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic mutations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specific gene mutations associated with early AD (in vitro gene-editing study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identification of specific gene mutations linked to early development of AD using gene-editing in vitro models, suggesting genetic causal contributors and targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>genetic</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Specific gene mutations discovered via gene-editing experiments in vitro were reported as closely associated with early AD development and proposed as targets for future therapies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Paper references a 'recent breakthrough study' using gene-editing in vitro that identified specific mutations closely associated with early AD development, indicating candidate genetic contributors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No refuting evidence provided in the paper; the gene-editing result is presented as a preliminary experimental finding requiring future validation and translational work.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>genetic screening / molecular</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Genetic analysis / gene-editing identification in vitro to find mutations associated with AD susceptibility or early pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>No diagnostic performance metrics provided in this paper; the referenced work is reported as an identification/discovery study rather than a clinical diagnostic test.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>in vitro experimental study (gene-editing model) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>In vitro model; not a human cohort; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations: in vitro findings may not translate directly to human disease; need validation in human cohorts and functional studies; the paper does not provide replication or population-level evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e10017.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRI (structural)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Magnetic Resonance Imaging (MRI) structural imaging</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-resolution anatomical brain imaging used to detect structural changes such as brain atrophy and morphological alterations associated with AD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>detection_method</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a causal factor; MRI detects structural brain changes (e.g., atrophy) that correlate with AD progression.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The paper states MRI provides high-resolution images of brain structure and helps identify brain atrophy and morphological changes associated with Alzheimer's disease (Zhao et al., 2021; Yildirim and Cinar, 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>MRI structural changes may not fully capture subtle early cognitive decline and are less specific to molecular pathology (authors note that imaging may not comprehensively reflect functional/cognitive impairment).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>neuroimaging (structural MRI)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>T1-weighted structural MRI identifying brain atrophy, volumetric measures (e.g., whole brain volume, hippocampal atrophy) and morphological features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Paper does not provide explicit sensitivity/specificity values for MRI-alone detection; authors report that MRI features are essential components in multimodal fusion and that removing image data from their model substantially reduced performance (ablation: removal of image data decreased precision/recall/F1/accuracy/MCC across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>observational imaging studies and multimodal machine learning experiments (used in current paper's experiments on ADNI and OASIS datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Used in this paper on ADNI (~2,354 MRI-PET pairs; 1,854 train/500 validation) and OASIS (416 subjects, 1,098 MRI-PET pairs; 798 train/300 validation).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations: MRI may miss subtle early functional changes; image-only models have reduced predictive power relative to multimodal models; differences across scanners/sites lead to heterogeneity; the paper emphasizes multimodal fusion to overcome MRI-only limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e10017.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PET (amyloid / FDG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positron Emission Tomography (PET) imaging (amyloid/FDG-PET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional imaging that measures brain metabolic activity and can detect abnormal protein deposits (amyloid) and glucose metabolism deficits associated with AD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>detection_method</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause; PET detects metabolic dysfunction and amyloid/tau deposition that are markers of AD pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Paper emphasizes PET's unique value for early detection of AD and MCI by revealing metabolic activity and abnormal protein deposits; cites studies showing amyloid accumulation measured with PET is associated with cognitive decline (Shi et al., 2017).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No direct biological refutation provided; operational limitations (cost, radioactive tracers, time-consuming, safety and technical requirements) limit widespread clinical adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>neuroimaging (molecular PET)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>PET imaging (e.g., amyloid PET, FDG-PET) measuring radiotracer uptake to visualize amyloid deposition or regional glucose metabolism deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Paper states PET has 'unique value' and 'high sensitivity in marker detection' but does not report numeric sensitivity/specificity in-text; in this work PET (including generated PET) contributes strongly to prediction performance — ablation removing PET reference decreased precision/recall/accuracy/MCC markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>imaging studies cited and used within multimodal machine learning experiments on ADNI and OASIS in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>PET data used from ADNI (~2,354 MRI-PET pairs) and OASIS (1,098 pairs) in model training/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Operational limitations: PET is expensive, requires radioactive tracers and specialized facilities; limited feasibility for mass screening. The paper also raises concerns about trust/clinical acceptance of synthetic PET images generated by diffusion models and the need for comparative validation against real PET.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e10017.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cognitive scales (MMSE, CDR, FAQ, GDS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive and clinical assessment scales: MMSE, CDR, FAQ, GDS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standardized neuropsychological and functional scales used to assess global cognition (MMSE), dementia severity (CDR), daily function (FAQ), and depression (GDS), contributing clinical information for AD diagnosis/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>detection_method</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not causal; these scales quantify cognitive and functional impairment that reflect disease stage and progression.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Paper uses MMSE and CDR as features in the multimodal model; cites literature that these scales provide critical information for early detection and quantification of dementia severity (Arevalo-Rodriguez et al., 2015; Delor et al., 2013). The FAQ is cited as improving MCI detection when combined with MMSE and age (Suárez-Araujo et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Cognitive tests can miss subtle early-stage changes and are influenced by subjectivity, education, and other confounders; the paper notes traditional clinical assessments often fail to capture subtle early cognitive changes and are lengthy and subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>cognitive test / clinical assessment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>MMSE (global cognitive screening), CDR (dementia staging), FAQ (functional activities), GDS (depression), used singly or combined as tabular features in predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>The paper cites an ANN study (Suárez-Araujo et al., 2021) that combined MMSE+FAQ+GDS+demographics achieving AUC 95.2%, sensitivity 90.0%, specificity 84.78%; it also shows ablation in this paper that removing tabular data (scales) substantially reduced AD-Diff model performance (e.g., reductions in precision/recall/accuracy/MCC up to ~24 percentage points for recall in some ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>referenced ANN diagnostic study; used as tabular features in present machine learning experiments (retrospective dataset analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Referenced ANN study population details not specified here; in this paper, cognitive scales were used from ADNI and OASIS cohorts (ADNI: ~2,354 pairs; OASIS: 416 subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations: scales are subjective, can vary across datasets, and require preprocessing/standardization; they may not detect the earliest molecular changes but are valuable when combined with imaging/biomarkers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e10017.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e10017.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AD-Diff multimodal model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AD-Diff (AD-Di) multimodal diffusion + Mamba classifier model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's proposed AI pipeline that generates PET images from MRI using a 3D diffusion model (ADdiffusion) and fuses generated PET, MRI, and tabular cognitive data via a Mamba classifier with Pixel-Level Bi-Cross Attention to predict AD progression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>detection_method</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a causal hypothesis; a diagnostic/predictive method that leverages MRI-to-PET synthesis and multimodal fusion to detect/predict conversion from MCI to AD.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>N/A (model is a detection method). Evidence for effectiveness comes from experiments on ADNI and OASIS where AD-Diff outperformed multiple baseline ML methods across precision/recall/F1/accuracy/MCC metrics and ablation studies demonstrated contributions of ADdiffusion, PET reference, image and table data, and Mamba modules.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>N/A as causal. Limitations discussed include need for validation on noisy/heterogeneous clinical data, clinician trust in synthetic PET, and computational cost; no external clinical trial validation presented.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>multimodal AI classifier (synthetic PET + MRI + cognitive/tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Generates PET from MRI via 3D diffusion (ADdiffusion), fuses generated PET, real MRI and tabular cognitive data via Mamba classifier and pixel-level bi-cross attention to predict AD conversion within specified time intervals (180/365/730 days and 1/3 year evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Reported classification/prediction metrics on public datasets: ADNI 3-year — precision 93.30%, recall 93.21%, F1 88.47%, accuracy 90.78%, MCC 86.95%; ADNI 1-year — precision 96.88%, recall 96.79%, F1 92.05%, accuracy 94.36%, MCC 90.53%; OASIS 3-year — precision 91.29%, recall 91.20%, F1 86.46%, accuracy 88.77%, MCC 84.94%; OASIS 1-year — precision 93.92%, recall 93.83%, F1 92.09%, accuracy 94.40%, MCC 90.57%. Ablation studies quantify drops when modules/data are removed (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>retrospective machine-learning evaluation / benchmarking on existing cohort datasets</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Training/validation on ADNI (~2,354 MRI-PET pairs; 1,854 train / 500 validation) and OASIS (416 subjects, 1,098 MRI-PET pairs; 798 train / 300 validation). Predictions targeted MCI→AD conversion at intervals (180, 365, 730 days and 1-/3-year evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations explicitly listed: need validation on noisy/heterogeneous real-world clinical data; clinician trustworthiness of synthetic PET images; high computational cost of diffusion generation; potential generalization issues across scanners/sites/populations; interpretability concerns of complex deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Loddo et al., 2022 <em>(Rating: 2)</em></li>
                <li>Shi et al., 2017 <em>(Rating: 2)</em></li>
                <li>Bi et al., 2020 <em>(Rating: 2)</em></li>
                <li>Suárez-Araujo et al., 2021 <em>(Rating: 2)</em></li>
                <li>Tu et al., 2024 <em>(Rating: 1)</em></li>
                <li>Chételat, 2018 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-10017",
    "paper_id": "paper-276969720",
    "extraction_schema_id": "extraction-schema-170",
    "extracted_data": [
        {
            "name_short": "Amyloid-beta / amyloid plaques",
            "name_full": "Amyloid-beta accumulation (amyloid plaques)",
            "brief_description": "Extracellular aggregation of amyloid-beta peptides forming plaques in brain regions (e.g., hippocampus) that are widely implicated as a molecular hallmark and proposed causal factor in Alzheimer's disease (AD).",
            "citation_title": "",
            "mention_or_use": "mention",
            "cause_type": "molecular",
            "cause_description": "Accumulation of amyloid-beta (Aβ) peptides into extracellular plaques, including hippocampal amyloid accumulation associated with cognitive decline.",
            "evidence_for_cause": "The paper cites studies reporting that PET imaging found amyloid accumulation in hippocampus closely related to the speed of cognitive decline and that biomarker studies (CSF, PET) implicate amyloid pathology as correlated with AD development (references: Shi et al., 2017; Loddo et al., 2022).",
            "evidence_against_cause": "No direct refuting empirical results are provided in this paper; limitations mentioned include that imaging/biomarker approaches are costly and not universally deployable, and the paper does not provide negative clinical trial or longitudinal contradictory evidence.",
            "detection_method_type": "neuroimaging / biomarker",
            "detection_method_description": "Amyloid PET imaging (detects amyloid deposits in brain); CSF measures (amyloid-beta levels and tau/amyloid ratios) used as molecular biomarkers.",
            "detection_performance": "The paper reports descriptive statements only (e.g., 'high sensitivity in marker detection' for biomarkers and that hippocampal amyloid accumulation relates to cognitive decline) but does not provide numeric sensitivity/specificity for amyloid PET or CSF amyloid in the cited studies within the text.",
            "study_type": "mention of PET and CSF biomarker studies (imaging and biomarker observational studies cited)",
            "study_population": "Not specified in this paper for the cited amyloid findings (references to prior PET/biomarker studies).",
            "controversies_or_limitations": "Practical limitations: PET is expensive, requires radioactive tracers and specialized infrastructure limiting large-scale screening; biomarker assays (CSF) are costly/complex and have limited universality. The paper notes these operational limitations but does not present mechanistic controversies (e.g., failed amyloid-targeting trials) explicitly.",
            "uuid": "e10017.0",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Tau/β-amyloid (CSF ratio)",
            "name_full": "Cerebrospinal fluid tau to β-amyloid (Aβ) ratio",
            "brief_description": "Ratio of tau protein to β-amyloid measured in cerebrospinal fluid (CSF), used as a molecular biomarker correlated with AD pathology and proposed as an early diagnostic indicator.",
            "citation_title": "",
            "mention_or_use": "mention",
            "cause_type": "molecular",
            "cause_description": "Elevated tau relative to β-amyloid in CSF reflects neurofibrillary degeneration and amyloid pathology associated with AD progression.",
            "evidence_for_cause": "The paper cites a 2022 study reporting that abnormal tau/β-amyloid ratios in CSF are highly correlated with AD development and provide a molecular basis for early diagnosis (Loddo et al., 2022).",
            "evidence_against_cause": "No counter-evidence presented in this paper; the authors note general challenges with biomarker approaches (cost, technical demand, need for validation) rather than biological refutation.",
            "detection_method_type": "biomarker (CSF assay)",
            "detection_method_description": "Laboratory measurement of CSF tau and Aβ concentrations and their ratio to infer AD-related pathology.",
            "detection_performance": "Paper states these markers are 'highly correlated' with AD and 'provide a reliable molecular basis for early diagnosis' but does not report sensitivity/specificity or numeric performance metrics in the text.",
            "study_type": "reference to prior biomarker study (observational/biomarker validation)",
            "study_population": "Not specified in this paper for the cited CSF study.",
            "controversies_or_limitations": "Operational limitations highlighted: assays are costly, complex, and not easily scalable for large-screening; need further clinical validation for broad adoption.",
            "uuid": "e10017.1",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "NSE / S100 blood markers",
            "name_full": "Neuron-specific enolase (NSE) and S100 protein (blood biomarkers)",
            "brief_description": "Blood-based biomarkers (NSE, S100) proposed as economical alternatives to CSF biomarkers for detecting neuronal injury/AD pathology.",
            "citation_title": "",
            "mention_or_use": "mention",
            "cause_type": "blood biomarker",
            "cause_description": "Elevated levels of NSE and S100 proteins in blood are associated with neuronal damage and were proposed to detect AD-related pathology comparably to CSF measures in preliminary studies.",
            "evidence_for_cause": "The paper cites a blood-sample based study (Bi et al., 2020) proposing that NSE and S100 detection provided preliminary accuracy comparable to CSF analysis, suggesting potential for broader screening.",
            "evidence_against_cause": "The paper notes that further research is required for large-scale clinical validation; no explicit contradictory studies are described here.",
            "detection_method_type": "biomarker (blood assay)",
            "detection_method_description": "Measurement of NSE and S100 proteins in blood samples as surrogate markers of neuronal injury and possible AD pathology.",
            "detection_performance": "Described as 'accuracy comparable to traditional cerebrospinal fluid analysis in preliminary studies' but no numeric sensitivity/specificity or stage-specific performance reported in this paper.",
            "study_type": "mention of preliminary biomarker study (observational / exploratory)",
            "study_population": "Not specified in this paper (referenced as preliminary blood-sample study).",
            "controversies_or_limitations": "Limitations: Preliminary evidence only; requires larger-scale validation; potential variability and lower specificity compared with CSF or PET not quantified here.",
            "uuid": "e10017.2",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Genetic mutations",
            "name_full": "Specific gene mutations associated with early AD (in vitro gene-editing study)",
            "brief_description": "Identification of specific gene mutations linked to early development of AD using gene-editing in vitro models, suggesting genetic causal contributors and targets.",
            "citation_title": "",
            "mention_or_use": "mention",
            "cause_type": "genetic",
            "cause_description": "Specific gene mutations discovered via gene-editing experiments in vitro were reported as closely associated with early AD development and proposed as targets for future therapies.",
            "evidence_for_cause": "Paper references a 'recent breakthrough study' using gene-editing in vitro that identified specific mutations closely associated with early AD development, indicating candidate genetic contributors.",
            "evidence_against_cause": "No refuting evidence provided in the paper; the gene-editing result is presented as a preliminary experimental finding requiring future validation and translational work.",
            "detection_method_type": "genetic screening / molecular",
            "detection_method_description": "Genetic analysis / gene-editing identification in vitro to find mutations associated with AD susceptibility or early pathology.",
            "detection_performance": "No diagnostic performance metrics provided in this paper; the referenced work is reported as an identification/discovery study rather than a clinical diagnostic test.",
            "study_type": "in vitro experimental study (gene-editing model) referenced",
            "study_population": "In vitro model; not a human cohort; details not provided in this paper.",
            "controversies_or_limitations": "Limitations: in vitro findings may not translate directly to human disease; need validation in human cohorts and functional studies; the paper does not provide replication or population-level evidence.",
            "uuid": "e10017.3",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MRI (structural)",
            "name_full": "Magnetic Resonance Imaging (MRI) structural imaging",
            "brief_description": "High-resolution anatomical brain imaging used to detect structural changes such as brain atrophy and morphological alterations associated with AD.",
            "citation_title": "",
            "mention_or_use": "use",
            "cause_type": "detection_method",
            "cause_description": "Not a causal factor; MRI detects structural brain changes (e.g., atrophy) that correlate with AD progression.",
            "evidence_for_cause": "The paper states MRI provides high-resolution images of brain structure and helps identify brain atrophy and morphological changes associated with Alzheimer's disease (Zhao et al., 2021; Yildirim and Cinar, 2020).",
            "evidence_against_cause": "MRI structural changes may not fully capture subtle early cognitive decline and are less specific to molecular pathology (authors note that imaging may not comprehensively reflect functional/cognitive impairment).",
            "detection_method_type": "neuroimaging (structural MRI)",
            "detection_method_description": "T1-weighted structural MRI identifying brain atrophy, volumetric measures (e.g., whole brain volume, hippocampal atrophy) and morphological features.",
            "detection_performance": "Paper does not provide explicit sensitivity/specificity values for MRI-alone detection; authors report that MRI features are essential components in multimodal fusion and that removing image data from their model substantially reduced performance (ablation: removal of image data decreased precision/recall/F1/accuracy/MCC across datasets).",
            "study_type": "observational imaging studies and multimodal machine learning experiments (used in current paper's experiments on ADNI and OASIS datasets)",
            "study_population": "Used in this paper on ADNI (~2,354 MRI-PET pairs; 1,854 train/500 validation) and OASIS (416 subjects, 1,098 MRI-PET pairs; 798 train/300 validation).",
            "controversies_or_limitations": "Limitations: MRI may miss subtle early functional changes; image-only models have reduced predictive power relative to multimodal models; differences across scanners/sites lead to heterogeneity; the paper emphasizes multimodal fusion to overcome MRI-only limitations.",
            "uuid": "e10017.4",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PET (amyloid / FDG)",
            "name_full": "Positron Emission Tomography (PET) imaging (amyloid/FDG-PET)",
            "brief_description": "Functional imaging that measures brain metabolic activity and can detect abnormal protein deposits (amyloid) and glucose metabolism deficits associated with AD.",
            "citation_title": "",
            "mention_or_use": "use",
            "cause_type": "detection_method",
            "cause_description": "Not a cause; PET detects metabolic dysfunction and amyloid/tau deposition that are markers of AD pathology.",
            "evidence_for_cause": "Paper emphasizes PET's unique value for early detection of AD and MCI by revealing metabolic activity and abnormal protein deposits; cites studies showing amyloid accumulation measured with PET is associated with cognitive decline (Shi et al., 2017).",
            "evidence_against_cause": "No direct biological refutation provided; operational limitations (cost, radioactive tracers, time-consuming, safety and technical requirements) limit widespread clinical adoption.",
            "detection_method_type": "neuroimaging (molecular PET)",
            "detection_method_description": "PET imaging (e.g., amyloid PET, FDG-PET) measuring radiotracer uptake to visualize amyloid deposition or regional glucose metabolism deficits.",
            "detection_performance": "Paper states PET has 'unique value' and 'high sensitivity in marker detection' but does not report numeric sensitivity/specificity in-text; in this work PET (including generated PET) contributes strongly to prediction performance — ablation removing PET reference decreased precision/recall/accuracy/MCC markedly.",
            "study_type": "imaging studies cited and used within multimodal machine learning experiments on ADNI and OASIS in this paper",
            "study_population": "PET data used from ADNI (~2,354 MRI-PET pairs) and OASIS (1,098 pairs) in model training/validation.",
            "controversies_or_limitations": "Operational limitations: PET is expensive, requires radioactive tracers and specialized facilities; limited feasibility for mass screening. The paper also raises concerns about trust/clinical acceptance of synthetic PET images generated by diffusion models and the need for comparative validation against real PET.",
            "uuid": "e10017.5",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Cognitive scales (MMSE, CDR, FAQ, GDS)",
            "name_full": "Cognitive and clinical assessment scales: MMSE, CDR, FAQ, GDS",
            "brief_description": "Standardized neuropsychological and functional scales used to assess global cognition (MMSE), dementia severity (CDR), daily function (FAQ), and depression (GDS), contributing clinical information for AD diagnosis/prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "cause_type": "detection_method",
            "cause_description": "Not causal; these scales quantify cognitive and functional impairment that reflect disease stage and progression.",
            "evidence_for_cause": "Paper uses MMSE and CDR as features in the multimodal model; cites literature that these scales provide critical information for early detection and quantification of dementia severity (Arevalo-Rodriguez et al., 2015; Delor et al., 2013). The FAQ is cited as improving MCI detection when combined with MMSE and age (Suárez-Araujo et al., 2021).",
            "evidence_against_cause": "Cognitive tests can miss subtle early-stage changes and are influenced by subjectivity, education, and other confounders; the paper notes traditional clinical assessments often fail to capture subtle early cognitive changes and are lengthy and subjective.",
            "detection_method_type": "cognitive test / clinical assessment",
            "detection_method_description": "MMSE (global cognitive screening), CDR (dementia staging), FAQ (functional activities), GDS (depression), used singly or combined as tabular features in predictive models.",
            "detection_performance": "The paper cites an ANN study (Suárez-Araujo et al., 2021) that combined MMSE+FAQ+GDS+demographics achieving AUC 95.2%, sensitivity 90.0%, specificity 84.78%; it also shows ablation in this paper that removing tabular data (scales) substantially reduced AD-Diff model performance (e.g., reductions in precision/recall/accuracy/MCC up to ~24 percentage points for recall in some ablations).",
            "study_type": "referenced ANN diagnostic study; used as tabular features in present machine learning experiments (retrospective dataset analysis)",
            "study_population": "Referenced ANN study population details not specified here; in this paper, cognitive scales were used from ADNI and OASIS cohorts (ADNI: ~2,354 pairs; OASIS: 416 subjects).",
            "controversies_or_limitations": "Limitations: scales are subjective, can vary across datasets, and require preprocessing/standardization; they may not detect the earliest molecular changes but are valuable when combined with imaging/biomarkers.",
            "uuid": "e10017.6",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AD-Diff multimodal model",
            "name_full": "AD-Diff (AD-Di) multimodal diffusion + Mamba classifier model",
            "brief_description": "This paper's proposed AI pipeline that generates PET images from MRI using a 3D diffusion model (ADdiffusion) and fuses generated PET, MRI, and tabular cognitive data via a Mamba classifier with Pixel-Level Bi-Cross Attention to predict AD progression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "cause_type": "detection_method",
            "cause_description": "Not a causal hypothesis; a diagnostic/predictive method that leverages MRI-to-PET synthesis and multimodal fusion to detect/predict conversion from MCI to AD.",
            "evidence_for_cause": "N/A (model is a detection method). Evidence for effectiveness comes from experiments on ADNI and OASIS where AD-Diff outperformed multiple baseline ML methods across precision/recall/F1/accuracy/MCC metrics and ablation studies demonstrated contributions of ADdiffusion, PET reference, image and table data, and Mamba modules.",
            "evidence_against_cause": "N/A as causal. Limitations discussed include need for validation on noisy/heterogeneous clinical data, clinician trust in synthetic PET, and computational cost; no external clinical trial validation presented.",
            "detection_method_type": "multimodal AI classifier (synthetic PET + MRI + cognitive/tabular data)",
            "detection_method_description": "Generates PET from MRI via 3D diffusion (ADdiffusion), fuses generated PET, real MRI and tabular cognitive data via Mamba classifier and pixel-level bi-cross attention to predict AD conversion within specified time intervals (180/365/730 days and 1/3 year evaluations).",
            "detection_performance": "Reported classification/prediction metrics on public datasets: ADNI 3-year — precision 93.30%, recall 93.21%, F1 88.47%, accuracy 90.78%, MCC 86.95%; ADNI 1-year — precision 96.88%, recall 96.79%, F1 92.05%, accuracy 94.36%, MCC 90.53%; OASIS 3-year — precision 91.29%, recall 91.20%, F1 86.46%, accuracy 88.77%, MCC 84.94%; OASIS 1-year — precision 93.92%, recall 93.83%, F1 92.09%, accuracy 94.40%, MCC 90.57%. Ablation studies quantify drops when modules/data are removed (see paper).",
            "study_type": "retrospective machine-learning evaluation / benchmarking on existing cohort datasets",
            "study_population": "Training/validation on ADNI (~2,354 MRI-PET pairs; 1,854 train / 500 validation) and OASIS (416 subjects, 1,098 MRI-PET pairs; 798 train / 300 validation). Predictions targeted MCI→AD conversion at intervals (180, 365, 730 days and 1-/3-year evaluations).",
            "controversies_or_limitations": "Limitations explicitly listed: need validation on noisy/heterogeneous real-world clinical data; clinician trustworthiness of synthetic PET images; high computational cost of diffusion generation; potential generalization issues across scanners/sites/populations; interpretability concerns of complex deep models.",
            "uuid": "e10017.7",
            "source_info": {
                "paper_title": "AD-Diff: enhancing Alzheimer's disease prediction accuracy through multimodal fusion",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Loddo et al., 2022",
            "rating": 2,
            "sanitized_title": "loddo_et_al_2022"
        },
        {
            "paper_title": "Shi et al., 2017",
            "rating": 2,
            "sanitized_title": "shi_et_al_2017"
        },
        {
            "paper_title": "Bi et al., 2020",
            "rating": 2,
            "sanitized_title": "bi_et_al_2020"
        },
        {
            "paper_title": "Suárez-Araujo et al., 2021",
            "rating": 2,
            "sanitized_title": "suárezaraujo_et_al_2021"
        },
        {
            "paper_title": "Tu et al., 2024",
            "rating": 1,
            "sanitized_title": "tu_et_al_2024"
        },
        {
            "paper_title": "Chételat, 2018",
            "rating": 1,
            "sanitized_title": "chételat_2018"
        }
    ],
    "cost": 0.0125642,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Han L ( ) AD-Di : enhancing Alzheimer's disease prediction accuracy through multimodal fusion</p>
<p>Lei Lu 
Kumar Chandan 
Behera 
Lei Han hanlei@com </p>
<p>King's College London
United Kingdom</p>
<p>University of Texas Health Science Center at Houston
United States</p>
<p>Carmen Paz Suárez-Araujo</p>
<p>Universidad de las Palmas de Gran Canarias
Spain</p>
<p>School of Clinical Sciences
Faculty of Health and Environmental Sciences
Auckland University of Technology
AucklandNew Zealand</p>
<p>Han L ( ) AD-Di : enhancing Alzheimer's disease prediction accuracy through multimodal fusion
0F51A5AB11A97C560DFA9A33EAEB260BRECEIVED August ACCEPTED February PUBLISHEDAlzheimer's diseasemultimodal fusiondi usionMambamachine learning
Early prediction of Alzheimer's disease (AD) is crucial to improving patient quality of life and treatment outcomes.However, current predictive methods face challenges such as insu cient multimodal information integration and the high cost of PET image acquisition, which limit their e ectiveness in practical applications.To address these issues, this paper proposes an innovative model, AD-Di .This model significantly improves AD prediction accuracy by integrating PET images generated through a di usion process with cognitive scale data and other modalities.Specifically, the AD-Di model consists of two core components: the ADdi usion module and the multimodal Mamba Classifier.The ADdi usion module uses a D di usion process to generate high-quality PET images, which are then fused with MRI images and tabular data to provide input for the Multimodal Mamba Classifier.Experimental results on the OASIS and ADNI datasets demonstrate that the AD-Di model performs exceptionally well in both long-term and short-term AD prediction tasks, significantly improving prediction accuracy and reliability.These results highlight the significant advantages of the AD-Di model in handling complex medical image data and multimodal information, providing an e ective tool for the early diagnosis and personalized treatment of Alzheimer's disease.</p>
<p>Introduction</p>
<p>Alzheimer's disease (AD) is a progressively worsening neurodegenerative disorder that primarily affects the elderly, manifesting as declines in memory, cognitive function, and behavioral abilities (Ebrahimighahnavieh et al., 2020;Lee et al., 2019;Khojaste-Sarakhsi et al., 2022).According to the World Health Organization (WHO), ∼50 million people worldwide suffer from Alzheimer's disease or other forms of dementia, a number expected to rise to 150 million by 2050.As the global aging population issue intensifies, Alzheimer's disease not only imposes significant burdens on patients and their families but also poses major challenges to the healthcare systems.Therefore, enhancing research on early diagnosis and intervention for Alzheimer's disease is crucial not only to improve the quality of life for patients but also to alleviate socio-economic pressures (Helaly et al., 2022;Venugopalan et al., 2021).Traditionally, the diagnosis of Alzheimer's disease has relied primarily on clinical assessments, including detailed medical history collection, neuropsychological testing, and brain imaging studies.Although these methods can provide relatively reliable diagnostic criteria in the later stages of the disease, they often fail to capture subtle cognitive changes in the early stages (Spasov et al., 2019).Moreover, these assessment methods usually require a lengthy process and depend on the subjective judgment of professionals, which to some extent limits the efficiency and universality of diagnosis.With the development of biomarkers and molecular imaging technologies, researchers have begun to explore the biological mechanisms of the disease at the molecular level, but these technologies are often costly and complex to operate, making them unsuitable for large-scale screening (Saleem et al., 2022).</p>
<p>The rise of artificial intelligence has provided unprecedented opportunities for the diagnosis and research of AD.In this field, the application of AI can be divided into three core directions: medical imaging analysis, behavioral data analysis, and genetic and molecular biology data analysis (Dwivedi et al., 2022).These technologies not only greatly enhance the accuracy of diagnoses, but also offer new possibilities for early detection and treatment of the disease.In medical imaging, deep learning technologies such as convolutional neural networks (CNNs) and generative adversarial networks (GANs) have become powerful tools to revolutionize traditional diagnostic methods (Liu et al., 2018).CNNs can automatically extract key features from brain MRI or PET scans, identifying early pathological changes, while GANs are used to generate higher-quality medical images, assisting in more accurate diagnostic analysis.Furthermore, the application of transfer learning allows researchers to accelerate the analysis of Alzheimer's disease images using models already trained in other medical imaging tasks, which is particularly valuable in cases of limited sample sizes (Ahmed et al., 2017).In the analysis of behavioral data, machine learning techniques such as support vector machines (SVMs) (Sharma et al., 2021) and decision trees (Saputra et al., 2020) have been applied to parse patients' daily activity data and cognitive test results to detect signs of potential cognitive decline.For more complex time series data, deep learning models such as long-short-term memory networks (LSTMs) (Dua et al., 2020) can effectively analyze patients' language expressions and behavioral patterns, providing support for early diagnosis and condition monitoring.In the analysis of genetic and molecular biology data, deep learning methods such as deep belief networks (DBNs) (Zhou et al., 2021) are used to study genetic markers related to Alzheimer's disease, revealing the molecular mechanisms of the disease, which are crucial for the development of future drugs and the formulation of personalized treatment strategies.</p>
<p>However, these methods are mainly based on unimodal information, such as the use of only neuroimaging data or individual cognitive evaluation data (Lee et al., 2019;Qiu et al., 2022;Zhang et al., 2019;Young et al., 2013).This type of unimodal analysis may not fully capture the complexity of Alzheimer's disease, as a single data source often provides only a partial view of the disease.For example, while neuroimaging can reveal changes in brain structure and function, it may not comprehensively reflect the actual decline in cognitive functions (Ritter et al., 2015).Similarly, cognitive test results might not fully capture subtle physiological changes in the brain.Therefore, to overcome these limitations, modern research tends to employ multimodal AI techniques, integrating data types such as neuroimaging, cognitive test results, and biomarkers (El-Sappagh et al., 2021).This cross-modal analysis method can analyze and understand more comprehensively the pathological characteristics and cognitive performance of Alzheimer's patients, providing more accurate disease prediction and diagnosis (Cabrera-León et al., 2024a).In particular in the critical phase of transitioning from Mild Cognitive Impairment (MCI) (Sikka et al., 2018) to Alzheimer's disease, multimodal analysis has become a key technology, helping to identify and intervene in the disease process earlier and more precisely.</p>
<p>Furthermore, in the field of neuroimaging, Magnetic Resonance Imaging (MRI) (Zhao et al., 2021) and Positron Emission Tomography (PET) (Lu et al., 2018) are key tools for diagnosing Alzheimer's disease.MRI provides high-resolution images of brain structures, helping to identify brain atrophy and morphological changes associated with Alzheimer's disease (Yildirim and Cinar, 2020).PET imaging, on the other hand, detects brain metabolic activity and abnormal protein deposits, such as amyloid plaques, which are crucial for the early detection of Alzheimer's disease and its preliminary stage of Mild Cognitive Impairment (MCI).</p>
<p>Although PET imaging has unique value for diagnosis, it also has significant limitations: the process is complex and timeconsuming, involves the use of radioactive tracers, and requires high technical and safety standards; furthermore, the high costs restrict its widespread use in routine clinical practices and largescale screenings, especially in resource-limited settings (Zhang et al., 2011).In recent years, synthetic data has demonstrated practical value in areas such as medical image enhancement and data augmentation, offering researchers more possibilities (Frid-Adar et al., 2018;Qi et al., 2020;Niemeijer et al., 2024).</p>
<p>Based on the shortcomings discussed above, we have developed a new artificial intelligence model named AD-Diff, specifically designed for the classification and prediction of Alzheimer's disease.The AD-Diff model integrates neuroimaging data and cognitive assessment information to enhance the efficiency of data utilization.Specifically, the model employs a 3D diffusion process that reconstructs PET images from MRI scans through a series of denoising steps, significantly reducing the high costs and technical complexities associated with traditional PET imaging.Additionally, the AD-Diff model incorporates a mamba block backbone network that optimizes the feature extraction process and achieves precise classification and prediction through a pixellevel BiCross Attention mechanism.This attention mechanism enhances the model's ability to recognize key features in complex brain images, thereby improving the accuracy and efficiency of diagnosis.With the integration of these technologies, the AD-Diff model provides an efficient and economical new tool for the early diagnosis and treatment of Alzheimer's disease, with potential for widespread application in clinical and research fields.</p>
<p>• The AD-Diff model uses its ADdiffusion module to generate high-quality PET images through a 3D diffusion process.These images are then fused with MRI images and tabular data, effectively addressing the high cost and accessibility issues associated with acquiring PET images.• The Multimodal Mamba Classifier within the model integrates information from PET images, MRI images, and cognitive scale data, significantly enhancing the accuracy and reliability of AD predictions.• Through experimental results on the OASIS and ADNI datasets, the AD-Diff model demonstrates excellent performance in both long-term and short-term AD prediction tasks, confirming its significant advantages in handling complex medical image data and multimodal information.This provides an effective tool for early diagnosis and personalized treatment of Alzheimer's disease.</p>
<p>The structure of this paper is organized as follows: The second section reviews related work, discussing both traditional and deep learning-based methods for studying Alzheimer's Disease (AD).The third section elaborates on the core concept of the AD-Diff model and its key components, including the ADdiffusion module and the Mamba Classifier.The fourth section covers the experimental part, detailing the datasets used, comparative experiments, and ablation studies.The final section concludes the paper, discussing the limitations of the model and directions for future research.</p>
<p>Related work . Biomedical methods in predicting AD</p>
<p>In traditional methods for predicting AD, recent studies have made significant advances, particularly in the discovery of biomarkers (Arya et al., 2023).For example, a 2022 study that analyzed the ratio of tau protein to β-amyloid in cerebrospinal fluid found that abnormal levels of these markers are highly correlated with the development of AD, providing a reliable molecular basis for early diagnosis (Loddo et al., 2022).Subsequently, another study using PET scan technology found that amyloid accumulation in the hippocampus region is closely related to the speed of cognitive decline, further validating the value of imaging biomarkers in monitoring disease progression (Shi et al., 2017).However, despite their high sensitivity in marker detection, the high costs and reliance on specialized equipment limit their feasibility in widespread clinical use.Furthermore, a study based on blood samples analyzed neuron-specific enolase (NSE) and the S100 protein, proposing a more economical method of biomarker detection that demonstrated accuracy comparable to traditional cerebrospinal fluid analysis in preliminary studies (Bi et al., 2020).This method's development offers new possibilities for broad screening, although further research is needed for large-scale clinical validation.Lastly, a recent breakthrough study employed gene-editing technology in an in vitro model to successfully identify specific gene mutations closely associated with the early development of AD, providing new targets for future genetic therapies.Although these studies have achieved notable success in the discovery and application of biomarkers, they still face challenges such as high costs, stringent technical requirements, and limited universality, which restrict their global adoption and application (Shi et al., 2019).</p>
<p>In this study, we propose the AD-Diff model, which enhances traditional biomarker-based methods by integrating multimodal data, including PET and MRI images, along with cognitive assessment information.Unlike previous approaches that rely solely on direct PET imaging, AD-Diff employs a 3D diffusion process to reconstruct PET images from MRI scans, significantly reducing dependency on expensive PET scans while maintaining diagnostic accuracy.Additionally, the model incorporates a Mamba block backbone for more efficient feature extraction and a BiCross Attention mechanism to optimize multimodal data fusion, enabling more precise classification and prediction of Alzheimer's disease.These innovations make AD-Diff a cost-effective and practical solution for both clinical and research applications.</p>
<p>. Machine learning in predicting AD</p>
<p>In Alzheimer's disease (AD) prediction research, machine learning techniques have become indispensable tools.Recent studies emphasize their unique advantages in handling and analyzing large volumes of data.A study using Support Vector Machine (SVM) models (Sharma et al., 2021) analyzed the relationship between cognitive assessment scores and brain imaging data, revealing that cognitive scores are closely linked to brain atrophy, thus improving the accuracy of early AD diagnosis.Furthermore, another study employed Random Forest (RF) algorithms (Bi et al., 2020) to integrate genetic and lifestyle data, identifying new biomarkers associated with AD risk, helping in the identification of high-risk groups.Additionally, another study demonstrated the use of an Artificial Neural Network (ANN) model (Suárez-Araujo et al., 2021), where the authors adopted a hybrid approach combining multiple neuropsychological assessments to improve the accuracy of Mild Cognitive Impairment (MCI) diagnosis.This ANN model integrates cognitive tests like the Mini-Mental State Examination (MMSE), functional assessments such as the Functional Activities Questionnaire (FAQ) and the Geriatric Depression Scale (GDS), along with demographic factors like age and years of education.By utilizing these diverse input features, the ANN model is able to capture both the cognitive and functional dimensions of MCI, which are crucial for an accurate diagnosis.The model demonstrated excellent diagnostic performance, achieving an AUC of 95.2%, sensitivity of 90.0%, and specificity of 84.78%.These results highlight the potential of the ANN system as a comprehensive diagnostic tool that can assist clinicians in evaluating cognitive and functional impairments in primary care settings, thus providing more comprehensive diagnostic support for MCI.Finally, another study utilized the Modular Hybrid Growing Neural Gas (MyGNG) (Cabrera-León et al., 2024b) system, which achieved excellent results in classifying MCI and AD, with an AUC of 0.96 and a sensitivity of 0.91.The system demonstrated similar effectiveness to deep learning methods while performing better in handling non-neuroimaging data.This study highlights the potential of the MyGNG model in MCI-AD classification, offering new insights for early diagnosis.Despite the outstanding performance of machine learning models in data analysis, they face certain limitations.For instance, these models typically require large amounts of training data to achieve optimal accuracy, and high-quality data can be difficult to obtain in the medical field (Gao and Lima, 2022).Moreover, while deep learning models such as Convolutional Neural Networks (CNN) (Ebrahimi et al., 2021) excel in image analysis, their complexity often makes them hard to interpret, which can raise credibility issues in medical applications.</p>
<p>In this study, AD-Diff leverages a series of innovative approaches, including a diffusion model and the Mamba classifier, to fully utilize multimodal data integration.The diffusion model generates high-quality PET images, while the Mamba classifier combines PET images, MRI images, and cognitive scale data to further enhance the model's performance in predicting Alzheimer's disease.Compared to traditional machine learning models, AD-Diff not only captures patient characteristics more comprehensively but also provides more accurate and reliable predictions when dealing with complex medical data.</p>
<p>. Multimodal fusion in predicting AD</p>
<p>In the field of AD research, multimodal deep learning methods have demonstrated significant contributions to enhancing disease diagnosis and prediction capabilities (Young et al., 2013).Firstly, methods integrating CNN and Recurrent Neural Networks (RNN) can simultaneously process static neuroimaging data and dynamic cognitive scores to provide a comprehensive assessment of disease progression.For example, CNNs are utilized to analyze MRI or PET scans to identify pathological features, while RNNs track temporal changes in cognitive test scores, offering ongoing insight into disease progression (Ritter et al., 2015).Secondly, Graph Convolutional Networks (GCN) have been applied to analyze patients' genetic data and social networks, revealing how genetic factors and social interactions jointly influence AD development.In addition, ensemble learning methods such as Random Forests have been used to integrate data from PET and MRI scans, as well as blood biomarkers, enhancing diagnostic precision through the powerful combination of multiple data sources (El-Sappagh et al., 2021).Researchers have also employed multikernel learning strategies to integrate various types of brain scan data, optimizing the ability to extract useful features from multimodal data.Lastly, Deep Belief Networks (DBN) combine clinical assessment data, neuroimaging, and molecular biomarkers to predict AD, showcasing the efficiency of deep learning in handling multimodal datasets (Chételat, 2018).Despite these technological advances, the application of these methods still faces challenges such as the complexity of data integration, inconsistencies between different data sources, and model interpretability.Addressing these issues requires ongoing attention and innovation in future research (Shi et al., 2019).</p>
<p>The AD-Diff model we propose, compared to the aforementioned methods, integrates more multimodal data, including PET images, MRI images, cognitive scales, and other information, enabling a more comprehensive capture of the multidimensional characteristics of AD patients.This integration of data enhances the expressive power and accuracy of AD-Diff in predicting Alzheimer's disease.</p>
<p>Methods</p>
<p>This paper proposes a model specifically designed for the classification and prediction of Alzheimer's disease, AD-Diff.The model generates Positron Emission Tomography (PET) images through a 3D diffusion process and integrates multimodal information to achieve efficient disease prediction.By incorporating the ADdiffusion process, Mamba classifier, and Pixel-Level Bi-Cross Attention (PL-Bi-Cross Attention) mechanism, the model ensures that the generated PET images possess a high degree of authenticity and structural consistency.</p>
<p>To substantiate this, we evaluated the authenticity of the generated PET images using the Structural Similarity Index (SSIM) metric, comparing them against ground-truth images.These quantitative results demonstrated a close alignment with actual PET scans, confirming the model's ability to retain essential anatomical details.Additionally, the model effectively predicts the onset of AD by leveraging these high-fidelity PET images, further validating the robustness of the image generation process.</p>
<p>As shown in Figure 1, the process begins with the generation of PET images using the ADdiffusion model.This model employs a 3D diffusion process, starting from the initial noise, and gradually reduces the noise while applying the diffusion equations to reconstruct high-quality PET images that are structurally consistent with the input MRI images.This process not only ensures that the generated PET images have excellent visual quality but also maintains consistency with the MRI data.Subsequently, the generated PET images, along with the real PET images, are input into the Mamba classifier.The Mamba classifier utilizes the PL-Bi-Cross Attention mechanism to integrate multimodal information, including assessment scales and imaging data.Its primary task is to distinguish whether the generated PET images are real, thereby further enhancing the accuracy and reliability of the model.Finally, the network performs a comprehensive prediction using integrated multimodal information, which not only improves the accuracy of AD prediction, but also provides higher diagnostic reliability for clinicians.</p>
<p>. ADdi usion for PET synthesis</p>
<p>In this paper, we propose ADdiffusion.We explore the application of the 3D diffusion process, particularly focusing on adapting a pretrained Text-to-Video (T2V) diffusion (Wu et al., 2023) model for generating Positron Emission Tomography (PET) images (Tu et al., 2024) from Magnetic Resonance Imaging (MRI) data.Despite the pre-trained T2V model's proven success in video generation, it initially struggles with synthesizing PET images from MRI data.This limitation arises primarily because the model was trained on diverse datasets that do not emphasize the detailed nuances and specific contrasts required for medical imaging, especially between the distinct modalities of MRI and PET scans.To address this issue, we aim to leverage the generalizability of the T2V model while introducing necessary domain-specific adjustments to customize it for PET image generation.This requires adjustments and optimizations of the model to enable it to process and transform unique image features inherent in the medical imaging field more accurately, thus improving the precision and effectiveness of MRI-to-PET image conversion.</p>
<p>The application of image prompting techniques has significantly enhanced the generative capabilities of diffusion models.In this study, we further strengthened the adaptability of the model by combining image prompting with text prompting, without making any structural modifications to the original diffusion model.To improve the model's ability to generate medical images without compromising its overall performance, we drew inspiration from the design of the IP-Adapter (Ye et al., 2023;Guo et al., 2024), focusing modifications on the cross-attention layers within the video generation model, while leaving the temporal attention layers unchanged.This ensures that the model's ability to generate consistent time sequences remains intact.To achieve this, we designed and introduced a lightweight adapter module, initialized based on the original diffusion model.Although we utilized the IP-Adapter weights, which were pretrained on nonmedical image data, to initialize the projection weights W K i and W V i within the adapter, we fine-tuned these weights to better suit the specific requirements of medical image generation.By further training on medical imaging data, we ensured that the model could more effectively capture subtle structures and contrast differences inherent in medical images.This approach not only rapidly enhanced the model's responsiveness to image prompts but also significantly reduced the complexity and cost of training, making it more suitable for the generation and processing of medical imaging data.Through these improvements, our model is able to generate PET images more accurately from MRI data, providing a more effective tool for the early diagnosis and treatment of Alzheimer's disease.</p>
<p>Equation 1 demonstrates the attention mechanism that combines images and text prompts.This mechanism integrates attention outputs from both the temporal and image dimensions, balancing their influence through the weight parameter λ.
Z = Attention(Q, K t , V t ) + λ • Attention(Q, K i , V i ) (1)
where Q represents the query vector, K t and V t are the key and value vectors from the temporal attention mechanism, and K i and V i are the key and value vectors from the image attention mechanism.The parameter λ controls the relative contribution of the image attention mechanism to the final output Z. Equation 2 describes the initialization process of the lightweight adapter module, where the projection weights W K i and W V i are initialized using the weights from the IP-Adapter to enhance the model's response to image prompts.
W K i = W K IP-Adapter , W V i = W V IP-Adapter (2)
where W K i and W V i are the projection weights in the lightweight adapter module, initialized using the weights W K IP-Adapter and W V IP-Adapter from the IP-Adapter, respectively.This initialization enhances the model's responsiveness to image prompts.Similarity Index (SSIM) between the generated PET images and the ground-truth PET images.
Z PET = arg max Z PET SSIM(Z PET , Z GT ) (3)
where Z GT represents the ground-truth PET images, and the SSIM function measures the structural similarity between the reconstructed Z PET and Z GT , ensuring that the model retains essential anatomical details during the reconstruction process.</p>
<p>. Multimodal Mamba classifier</p>
<p>.</p>
<p>. Temporal interval extraction for MCI progression</p>
<p>In the multimodal Mamba prediction model, the extraction of time steps is a crucial first step in predicting the progression from MCI to AD.MCI is an early stage of cognitive decline, with some patients gradually transition to Alzheimer's disease over time.Therefore, determining the prediction time interval is vital for the accuracy and practicality of the model.</p>
<p>In this study, our goal is to predict whether MCI patients will develop AD within a specific time frame using multimodal data.To achieve this, it is essential to first establish an appropriate time interval.This interval should reflect the natural progression from MCI to AD while also considering the need for practical clinical application.As a result, we selected several time intervals for prediction, specifically focusing on whether patients with MCI will progress to AD after 180, 365, and 730 days (Koponen et al., 2017;Hamina et al., 2017;Langballe et al., 2014).The choice of these intervals is based on the existing medical literature and an analysis of the disease course in patients with MCI, with the aim of providing a sufficient observation window for potential progression trends without excessively extending the prediction period.To support this research, we extracted relevant data from two large publicly available datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Open Access Series of Imaging Studies (OASIS).These datasets contain not only detailed clinical records of patients, but also extensive imaging data, genetic information, and cognitive test results.From these datasets, we recorded the actual time intervals between MCI and AD progression for each patient, providing a reliable foundation for model training and validation.Specifically, we recorded the time when each MCI patient was first diagnosed with MCI and the time when they were diagnosed with AD during follow-up.The difference between these time points represents the actual time interval used in our model.This recording of time intervals provides the prediction model with a true progression pathway and helps the model learn the varying speeds of progression from MCI to AD during training, ultimately providing more accurate predictions for clinical applications.</p>
<p>. . Preprocessing of assessment scales</p>
<p>In this study, we used assessment scales from multimodal data as part of the prediction model.The main reason for selecting these scales is that they provide critical information about the cognitive and functional status of patients, which plays an important role in accurately predicting the progression of MCI to AD.Specifically, the Mini-Mental State Examination (MMSE) (Arevalo-Rodriguez et al., 2015;Ding et al., 2009) is used for a quick assessment of the patient's overall cognitive function, especially for early detection of cognitive impairment, while the Clinical Dementia Rating (CDR) (Delor et al., 2013;Williams et al., 2013) scale quantifies dementia progression by evaluating the patient's performance in daily life and the severity of cognitive impairment.These scales not only offer quantitative measurements but also address the limitations of relying solely on imaging and genetic data, helping to achieve a more comprehensive and accurate prediction of disease progression.</p>
<p>However, the scale information in the datasets presents variations, such as differences in scale formats, scoring methods, and inconsistencies in data entry.These differences can introduce bias and, if not addressed, may affect the model's performance.Therefore, it is essential to preprocess the data from these assessment scales to ensure consistency and reliability.</p>
<p>In the multimodal data fusion process, the preprocessing of categorical and numerical variables is a crucial step.This process first involves the linear transformation of numerical variables x num i to generate standardized numerical feature representations Tnum i , which can be expressed as:
Tnum i = W num x num i + b num ,(4)
where W num is the weight matrix for the linear transformation, and b num is the bias term.The goal of this process is to standardize the numerical variables and convert them into a form compatible with other features, thereby facilitating better fusion of multimodal data in subsequent model processing.</p>
<p>Next, the numerical, categorical and image features processed are combined to form a unified multimodal representation.This process can be expressed as:
z = concat(x cat , Tnum , f img ) ∈ R (p+q+r)×d ,(5)
where x cat represents the categorical variables after embedding, Tnum is the transformed numerical features, and f img represents the image features.By concatenating these features, we obtain a multimodal representation z, with dimensions (p + q + r) × d, where p is the number of categorical features, q is the number of numerical features, r is the number of image features, and d is the feature dimension.</p>
<p>.</p>
<p>. Mamba classifier</p>
<p>The Mamba Classifier is a key component in our approach to predicting the progression from MCI to AD.This classifier is designed to efficiently fuse and process multimodal data, including MRI and PET imaging data as well as tabular data such as cognitive assessment scores.The implementation of the Mamba Classifier (Gurung et al., 2024) involves several key steps, each of which is crucial to the accuracy and robustness of the model.The network architecture of Mamba is depicted in Figure 2.</p>
<p>At the core of the Mamba Classifier is the Mamba block, a modular unit specifically designed to handle the complexity
z = SiLU(Conv1D(W in x + b in )) (6) z out = SSM(z) • SiLU(z)(7)
where, W in and b in are the weights and biases of the input linear layer, and Conv1D applies a one-dimensional convolution to extract relevant features.The Selective Scan Model (SSM) further refines these features and combines them with the element-wise multiplication of the SiLU activation to generate the processed output z out .</p>
<p>RMSNorm is applied after each Mamba block to stabilize the feature distribution and ensure consistent scaling:
x norm = RMSNorm(z out )(8)
The Mamba block can be repeated multiple times in the model to increase the network's depth, allowing for more complex feature extraction.The output of each Mamba block is used as the input to the next block, and this process is repeated n times:
x (i+1) mamba = MambaBlock(x (i) mamba ) (9)
where i denotes the current iteration of the Mamba block.The final output x (n) mamba is a deeply processed feature representation, ready to be fed into the next stage of processing.</p>
<p>The classifier combines image features from MRI and PET with tabular data during forward propagation.However, it does not effectively utilize pixel-level information from these images.While the attention mechanism helps the model focus on important global features, pixel-level details may contain critical local information related to disease progression.The insufficient use of these details may limit the potential for improving the model's accuracy.Therefore, further optimization may require processing pixellevel information more finely to capture complex features in medical images comprehensively, thereby improving the model's predictive capability.In this paper, we propose an improved method by combining a pixel-level bi-cross attention mechanism (as shown in Figure 3) with the output of the Mamba module, enabling the model to focus on both global and local information simultaneously.In this way, the model can better utilize pixel-level details from the images, improving the accuracy of predicting MCI progression to AD.The formula is:
Attention MRI = softmax Q mamba K T MRI d k V MRI (10) Attention PET = softmax Q mamba K T PET d k V PET (11)
Frontiers in Computational Neuroscience frontiersin.orgwhere, Q mamba is the query vector from the Mamba module, and K MRI and V MRI are the key and value matrices from the MRI data, with PET data processed similarly.The outputs from these attention mechanisms are combined and processed through a feedforward neural network to generate the final feature representation:
x final = FFN (Attention MRI + λ • Attention PET ) (12)
where λ is a balancing factor that adjusts the contribution of PET features relative to MRI features.The final representation of characteristics x final is processed through a linear classifier to generate the prediction of whether a patient with MCI will progress to AD.The classifier is trained by minimizing a cross-entropy loss function to improve prediction accuracy:
ŷ = softmax(W out x final + b out ) (13)
where W out and b out are the weights and biases of the output layer.Through this design, the Mamba Classifier effectively integrates and processes multimodal data, utilizing sophisticated feature extraction and attention mechanisms to provide a robust framework for predicting MCI progression to AD.This model not only improves prediction accuracy but also offers a reliable tool for practical clinical applications.</p>
<p>Experiments . Datasets</p>
<p>In this study, we used two significant Alzheimer's disease research datasets, OASIS (LaMontagne et al., 2019) and ADNI (Huckvale et al., 2021).These data sets provided us with a wealth of MRI and PET imaging data, essential for evaluating the performance of the AD-Diff model in the classification and prediction of Alzheimer's disease.</p>
<p>. . OASIS dataset</p>
<p>The OASIS dataset is a publicly available neuroimaging resource widely used in the research of Alzheimer's disease and other neurodegenerative disorders.This dataset includes information from 416 subjects aged between 18 and 96 years, collected using a 1.5 T scanner.Among the 416 entries, 20 nondemented subjects underwent additional follow-up visits after their initial visit, serving as a control group to ensure the reliability of the provided data and analysis.All subjects are right-handed.The clinical condition of the patients was determined using the CDR scale, and the dataset also provides MMSE scores, other clinically relevant information, and demographic data such as gender, age, years of education, and socioeconomic status.In addition, the dataset includes measurements of brain anatomical features such as estimated total intracranial volume (eTIV), normalized whole brain volume (nWBV), and atlas scaling factor (ASF).All this information was standardized to ensure data quality and consistency.Moreover, the OASIS dataset offers 1,098 pairs of MRI and PET images, which were uniformly preprocessed to ensure registration and consistency in image quality between MRI and PET scans.Of these, 798 image pairs were used for the training set and 300 for the validation set.Each data pair was labeled according to the clinical diagnosis, with confirmed Alzheimer's cases marked as 1 and non-diagnosed cases as 0, aiding the model in learning to differentiate between Alzheimer's patients and healthy controls during training.</p>
<p>. . ADNI dataset</p>
<p>The ADNI dataset (Huckvale et al., 2021) is one of the most widely used resources in Alzheimer's research, designed to advance the early diagnosis and study of Alzheimer's disease by collecting and sharing various forms of data.This includes neuroimaging data such as MRI and PET images, as well as genetic information, cognitive tests, cerebrospinal fluid (CSF), and blood biomarkers, all used as predictive indicators of the disease.In our experiments, we utilized ∼2,354 pairs of MRI and PET images, with 1,854 pairs allocated to the training set and 500 pairs to the validation set.These multimodal data provide a comprehensive foundation for evaluating the AD-Diff model's performance in predicting Alzheimer's disease progression.</p>
<p>. Implementation details . . Experimental environment</p>
<p>In this study, we used a high performance computing environment to train and validate the AD-Diff model.In terms of hardware configuration, we employed an Intel Xeon Gold 6226R @ 2.90GHz CPU with 32 cores (64 threads) to handle tasks such as data preprocessing and model deployment.Furthermore, the system was equipped with 2 NVIDIA Tesla V100 32GB GPUs, which provided powerful parallel computing capabilities, significantly accelerating the deep learning model training process, especially when dealing with large-scale MRI and PET imaging data.The system was also configured with 512GB of DDR4 RAM to ensure sufficient memory capacity for large datasets and model training.Data storage was supported by a 10TB NVMe SSD, enabling fast data read and write operations and reducing I/O bottlenecks.On the software side, we used the Ubuntu 20.04 LTS operating system, which provides a stable environment that is well suited for deep learning tasks.The model was constructed and trained using PyTorch 1.10.0 as a deep learning framework, combined with CUDA 11.3 and cuDNN 8.2 to fully leverage the computational power of the NVIDIA Tesla V100 GPUs.The experimental code and data processing scripts were written in Python 3.8.10.We utilized Numpy 1.21.2 for numerical computations, Scikit-learn 0.24.2 for dataset splitting and machine learning tasks, Matplotlib 3.4.3for results visualization, and Pandas 1.3.3 for data management and processing.The specific settings of the experimental environment are detailed in Table 1.</p>
<p>. . Data preprocessing</p>
<p>In this study, we performed data augmentation to enhance the performance of our model.First, we focused on tabular data from the OASIS and ADNI datasets, filtering all patients diagnosed</p>
<p>. . Parameter settings</p>
<p>In this study, we carefully configured the parameters of the ADdiffusion model network structure to optimize its performance in generating PET images from MRI data.Specifically, the diffusion steps (T) were set to 1,000 to ensure a gradual refinement and denoising process, resulting in high-quality PET images.The latent dimension was set to 256 to balance the capacity to represent the features of the model while controlling its complexity.We incorporated multi-head self-attention mechanisms to enhance the model's ability to extract features when processing multimodal data and employed residual connections and skip connections to improve the model's stability and the efficiency of information transfer.The optimizer used was Adam, with an initial learning rate set at 0.0001, complemented by a weight-decay parameter of 0.01 to prevent overfitting.A cosine annealing scheduler was utilized to gradually reduce the learning rate during training, helping the model to find the optimal solution as it approached convergence.to ensure the model fully learned the patterns and features in the data.These parameter settings ensured that the ADdiffusion model could effectively capture key features when processing complex multimodal medical imaging data and deliver high-quality results in the MRI-to-PET image conversion process.</p>
<p>. . Evaluation metrics</p>
<p>In this study, we used several evaluation metrics to measure the performance of the ADdiffusion model in the Alzheimer's disease classification task.Precision, recall, F1 score (Yacouby and Axman, 2020), accuracy, and Matthews correlation coefficient (MCC) (Chicco and Jurman, 2020).These metrics provide a comprehensive assessment of the model's performance, ensuring the accuracy and reliability of the classification results.F1 score was chosen because it balances precision and recall, making it especially useful for imbalanced datasets, where false negatives and false positives have different impacts.MCC is used because it takes into account all elements of the confusion matrix, providing a more balanced and robust measure of model performance, even when the class distribution is uneven.</p>
<p>. Results</p>
<p>. . ADNI dataset comparison</p>
<p>As shown in Tables 2, 3, we validated the superiority of the AD-Diff model in Alzheimer's disease classification and prediction by comparing it with several popular machine learning methods (including ResNet50, ResNet101, TabTransformer, XGBoost, GBDT, Adaboost, and 3D CNN) on the 1-year and 3-year ADNI datasets.These models represent the current mainstream methods in medical image analysis and tabular data processing.ResNet50 and ResNet101 are widely used for image classification tasks, particularly in the field of medical imaging.TabTransformer excels in handling tabular data, while ensemble learning methods such as XGBoost, GBDT, and Adaboost perform well on small-scale datasets.3D CNN is suitable for processing three-dimensional medical images.The experimental results demonstrate that the AD-Diff model outperforms other methods across all evaluation metrics, especially in Precision, Recall, F1-score, Accuracy, and Matthews Correlation Coefficient (MCC).In the 3-year dataset experiments, the AD-Diff model achieved 93.30% precision, 93.21% recall, 88.47% F1-score, 90.78% accuracy, and 86.95% MCC.These results indicate that AD-Diff has strong robustness and classification capability over a long time scale.In contrast, other methods, particularly 3D CNN and ResNet50, performed relatively worse on these metrics, especially in recall and MCC, suggesting potential limitations in capturing complex multimodal features.</p>
<p>In the 1-year dataset experiments, the AD-Diff model further demonstrated its effectiveness on a short time scale, with precision reaching 96.88%, recall at 96.79%, F1-score at 92.05%, accuracy at 94.36%, and MCC at 90.53%.Compared to other methods, AD-Diff showed significant improvements in all metrics, with particularly superior performance in short-term predictions.Conversely, the performance of 3D CNN and ResNet50 was relatively lower, especially in recall and MCC, which may be due to limitations in short-term feature extraction and fusion.The experimental results of AD-Diff on the 1-and 3year ADNI datasets indicate that the model not only exhibits strong stability and accuracy in long-term predictions but also demonstrates exceptional performance in short-term predictions.This is attributed to AD-Diff 's innovative design in multimodal data fusion, feature extraction, and complex relationship modeling, giving it a significant advantage in Alzheimer's disease classification and prediction tasks.</p>
<p>. . OASIS dataset comparison</p>
<p>As shown in Tables 4, 5, we compared the AD-Diff model with several other popular machine learning methods on the OASIS 1-year and 3-year datasets.Specifically, on the 3-year OASIS dataset, AD-Diff achieved a precision of 91.29%, which is 3.40 percentage points higher than Adaboost's 87.89% and 4.83 percentage points higher than XGBoost's 86.46%.The recall reached 91.20%, surpassing Adaboost's 87.71% by 3.49 percentage points and XGBoost's 86.41% by 4.79 percentage points.Although the F1 score of 86.46% is slightly lower than TabTransformer's 91.46%, the precision increased to 88. 77%, which is 22.54 percentage points higher than ResNet50's 66.23% and 18.63 percentage points higher than 3D CNN's 70.14%.In terms of Matthews Correlation Coefficient (MCC), AD-Diff achieved an MCC of 84.94%, which is 39.69 percentage points higher than 3D CNN's 45.25%, indicating greater predictive stability.</p>
<p>On the 1-year OASIS dataset, AD-Diff also performed exceptionally well.Its precision of 93.92% is 0.40 percentage points higher than Adaboost's 93.52% and 1.83 percentage points higher than XGBoost's 92.09%.The recall of 93.83% is 0.49 percentage points higher than Adaboost's 93.34% and 1.79 percentage points higher than XGBoost's 92.04%.The F1 score of 92.09%, although lower than TabTransformer's 97.09%, resulted in an accuracy of 94.40%, which is 22.54 percentage points higher than ResNet50's 71.86% and 18.63 percentage points higher than 3D CNN's 75.77%.AD-Diff also achieved the highest MCC of 90.57%, 39.69 percentage points higher than 3D CNN's 50.88%.These improvements demonstrate that AD-Diff provides higher accuracy and stability in Alzheimer's disease classification tasks, showcasing its superior performance on both long-term and short-term data.</p>
<p>. Significance study of the di erent parts or processes for the AD diagnosis and prediction</p>
<p>As shown in Table 6, the ablation experiments on the 3year ADNI dataset reveal the significant impact and profound implications of each component on the performance of the AD-Diff model.Removing the ADdiffusion module resulted in a decrease of 5.30 percentage points in precision, 9.45 percentage points in recall, 3.75 percentage points in F1-score, 7.14 percentage points in accuracy, and 9.92 percentage points in MCC.This indicates that the ADdiffusion module plays a critical role in enhancing model precision and stability, and its removal significantly weakened the model's ability to handle complex data features and prediction   and 9.92 percentage points in MCC.This significant drop indicates that the ADdiffusion module has a decisive impact on the model's precision and recall, and its removal severely weakens the overall performance of the model, particularly in handling short-term data.Removing the PET image reference led to a decrease of 11.01 percentage points in precision, 13.37 percentage points in recall, 4.94 percentage points in F1-score, 8.82 percentage points in accuracy, and 9.13 percentage points in MCC.This demonstrates the important role of the PET image reference in processing short-term data.Its removal caused a comprehensive decline in performance, especially in terms of precision and MCC metrics.Removing the Mamaba module resulted in a decrease of 12.08 percentage points in precision, an increase of 8.37 percentage points in recall, a decrease of 8.57 percentage points in F1-score, a decrease of 3.76 percentage points in accuracy, and a decrease of 4.82 percentage points in MCC.This suggests that the Mamaba module has a significant impact on the model's precision and overall performance.Although its removal improved recall, the overall performance decline reflects its indispensable role in shortterm data processing.Removing image data caused a decrease of 13.21 percentage points in precision, 13.83 percentage points in recall, 10.12 percentage points in F1-score, 7.02 percentage points in accuracy, and 10.41 percentage points in MCC.This indicates that image data is crucial for the model's overall performance, with its removal significantly weakening the model's ability to handle visual features and causing substantial declines in multiple performance metrics.Finally, removing table data led to a decrease of 16.66 percentage points in precision, 24.36 percentage points in recall, 11.37 percentage points in F1-score, 13.88 percentage points in accuracy, and 18.68 percentage points in MCC.This result highlights the core role of table data in short-term data processing.Its removal caused a significant decline in multiple metrics, reflecting the key role of table data in enhancing model performance.</p>
<p>. Discussion</p>
<p>The AD-Diff model introduced in this study presents a novel approach to Alzheimer's disease (AD) classification and prediction, offering a more refined method for generating PET images.By utilizing a diffusion process that begins with random noise, the model gradually refines this noise through iterative diffusion equations, ultimately reconstructing PET images that align with the structural details from corresponding MRI images.This innovative mechanism is further strengthened by the integration of the ADdiffusion module, the Mamba classifier, and the Pixel-Level Bi-Cross Attention (PL-Bi-Cross Attention) mechanism, which together enhance the model's ability to process and analyze multimodal data.</p>
<p>Experiments conducted on the ADNI and OASIS datasets evaluated the performance of the AD-Diff model in predicting Alzheimer's disease (AD) over 3-year and 1-year time spans.For the 3-year ADNI dataset, the results demonstrate that the AD-Diff model performs exceptionally well, showing stable and efficient performance in long-term prediction tasks.Key metrics such as accuracy, recall, and F1-score significantly improve, indicating that the generated PET images effectively capture features related to long-term AD development, thus enhancing prediction accuracy and reliability.Similarly, on the 1-year OASIS dataset, the AD-Diff model also performs excellently, accurately predicting the occurrence of AD.The results show that the model's performance in short-term prediction is comparable to its performance in longterm prediction, with improvements in most metrics.Specifically, the model is capable of quickly identifying early symptoms of AD when handling short-term data, providing accurate prediction results.This further validates the model's broad adaptability and strong predictive capability, offering effective early detection and prediction support for AD, regardless of the time span.Figure 4 displays the confusion matrices for the ADNI and OASIS datasets, demonstrating the applicability and accuracy of the AD-Diff model across different time spans.This provides a solid foundation for optimizing early diagnosis and treatment strategies for Alzheimer's disease, highlighting the potential of multimodal fusion methods in early disease detection.</p>
<p>The exceptional performance of our model is mainly attributed to the effective integration of multimodal information, including MRI, PET images, assessment scales, and tabular data.By incorporating these diverse data sources, AD-Diff can leverage the unique information provided by each type of data, resulting in more comprehensive and accurate predictions.Specifically, MRI and PET images provide rich structural and functional information, assessment scales offer quantitative evaluations of clinical diagnosis and pathological progression, and tabular data enhances the understanding of patient history and other relevant factors.This multi-faceted data fusion enables the model to excel in capturing early symptoms and development trends of the disease.</p>
<p>Figure 5 demonstrates the fusion effect of MRI and PET images generated by AD-Diff through ADdiffusion.In the first part (2D Latent MRI), the generated MRI images accurately depict the anatomical structures of the brain, with good detail retention, ensuring that the morphological features of various brain regions are clearly visible.The second part (2D Latent PET) presents the generated PET images, which effectively reflect the brain's metabolic and functional areas, revealing functional changes related to Alzheimer's disease.Finally, the third part shows the fusion effect of 2D Latent MRI and 2D Latent PET images.The fused images are highly consistent in anatomical structure and metabolic function.The PET images not only visually resemble real images but also accurately reflect the anatomical details from the MRI images.Through this fusion, the generated PET images faithfully reproduce the brain's structural features while preserving functional information related to the disease, thereby significantly improving the overall image quality and clinical application value.</p>
<p>Figure 6 shows the prediction performance of the model on multiple Alzheimer's disease (AD) datasets.As seen in the figure, the model successfully predicted all cases labeled as AD correctly, with a confidence level of 100%.The labels below each image display the actual AD status, the model's predicted AD status, and  the corresponding confidence level.This result indicates that the model's classification performance in this task is highly reliable, accurately identifying AD patients from MRI images.</p>
<p>.</p>
<p>Limitations and future work</p>
<p>Although the AD-Diff model has demonstrated excellent performance in Alzheimer's disease classification and prediction tasks, its effectiveness on noisy and heterogeneous clinical data requires further investigation.Actual clinical data often come from different medical environments, involve various scanning devices, and contain diverse patient populations, which are prone to noise and other interfering factors.Therefore, future work should focus on validating the model on more diverse and representative datasets, including those from different regions, hospitals, and longitudinal studies with longer time spans.This would not only enhance the model's generalization ability but also ensure its robustness and applicability across different clinical settings, providing a more universally applicable tool for medical practice.</p>
<p>Furthermore, the AD-Diff model relies on a diffusion process to generate high-quality PET images.While this approach reduces the high cost of obtaining real PET images to some extent, the trust that clinicians and patients place in synthetic data remains a challenge.Whether the synthetic images can accurately reflect real pathological features and whether they are reliable enough for clinical decision-making are key concerns.Future research could improve the credibility of synthetic images by conducting more comparative studies with real clinical data, ensuring consistency in structure and diagnostic information between synthetic and real data.Additionally, incorporating feedback from clinicians could help validate the practical utility of these synthetic images in actual diagnoses, thus increasing trust in this technology.</p>
<p>On the other hand, the AD-Diff model demands significant computational resources.The complexity of the diffusion process results in high computational costs, particularly when generating high-quality PET images, which could limit its application in resource-constrained environments.One future direction is to reduce the computational complexity of the model.This could be achieved through model compression, lightweight design, and hardware acceleration, ultimately reducing the computational requirements and enabling broader clinical adoption.In addition, future research could consider incorporating additional assessment scales into the model to further enrich functional evaluation and enhance diagnostic comprehensiveness.For example, the Functional Activities Questionnaire (FAQ) has been shown to play a significant role in assessing patients' daily living abilities.Existing studies have demonstrated that FAQ exhibits high effectiveness in detecting mild cognitive impairment (MCI) and can be combined with MMSE and age to significantly improve diagnostic accuracy (Suárez-Araujo et al., 2021).Research has proposed a hybrid artificial neural network (ANN)-based clinical decision support system, which has demonstrated excellent performance in MCI diagnosis, achieving an AUC of 95.2% and a sensitivity of 90.0%.The results indicate that FAQ, as a key input variable, can effectively enhance MCI diagnostic sensitivity and the clinical utility index (CUI) when combined with MMSE.These findings further support the value of FAQ in cognitive assessment and provide useful insights for future research directions.</p>
<p>Conclusion</p>
<p>This paper introduces the AD-Diff model, an innovative approach for Alzheimer's disease classification and prediction.By combining a diffusion process to generate high-quality PET images, and utilizing the Mamba classifier and Pixel-Level Bi-Cross Attention mechanism, the model effectively integrates multimodal data such as MRI images and clinical assessments, enhancing prediction accuracy.The AD-Diff model reduces the cost of obtaining PET images, effectively merges multimodal data, and improves the performance of Alzheimer's classification and prediction.Its effectiveness has been validated through comparative and ablation experiments.Future work will focus on further optimizing the model and validating its potential for real-world clinical applications.</p>
<p>FIGURE</p>
<p>FIGUREOverview of the AD-Di network structure.The process is divided into two steps: first, MRI images are converted into PET images using the ADdi usion model; second, D latent MRI, D latent PET, and tabular data are fused in a multimodal manner, with predictions made through the Mamba model.</p>
<p>FIGURE</p>
<p>FIGUREThe architecture diagram of the Mamba network: (A) Mamba classifier, where each unit consists of a Mamba module and an RMSNorm layer; (B) Mamba module, whose internal structure includes one-dimensional convolution, Sigmoid activation, SSM module, and linear transformation.</p>
<p>FIGURE Pixel-level bi-cross attention network diagram: (A) Shows the network structure of pixel-level bi-cross attention, including MRI attention mechanism, PET attention, and feedforward operations.(B) Illustrates the specific calculation method of MRI/PET attention.</p>
<p>FIGURE</p>
<p>FIGURE Confusion matrices of AD-Di on ADNI and OASIS datasets.(A) Represents the results on ADNI -Year, (B) represents the results on ADNI -Year, (C) represents the results on OASIS -Year, and (D) represents the results on OASIS -Year.</p>
<p>FIGURE</p>
<p>FIGUREFusion e ects of MRI and PET images in ADdi usion.</p>
<p>FIGURE</p>
<p>FIGUREAD-Di prediction results.</p>
<p>Equation 3 demonstrates the reconstruction process from MRI images to PET images, where the model processes MRI images to generate the intermediate representation Z MRI , and the final PET image Z PET is reconstructed by maximizing the Structural
Frontiers in Computational Neurosciencefrontiersin.org</p>
<p>TABLE Experimental environment settings.
ComponentSpecificationCPUIntel Xeon Gold 6226R @ 2.90GHz, 32 cores (64threads)GPU2x NVIDIA Tesla V100 32GBMemory512GB DDR4 RAMStorage10TB NVMe SSDOperating systemUbuntu 20.04 LTSDeep learning frameworkPyTorch 1.10.0CUDA version11.3cuDNN version8.2Programming languagePython 3.8.10Numerical computationNumpy 1.21.2Machine learningScikit-learn 0.24.2VisualizationMatplotlib 3.4.3Data managementPandas 1.3.3Simultaneously, we removed unnecessary information from thedataset, such as nondiagnostic indicators and redundant data, toensure that the data were concise and relevant. Finally, to betteranalyze the progression of patients with MCI to Alzheimer's Disease(AD), we divided the time intervals into two main ranges: oneranging from 150 days to 365 days and the other from 365 to 1,095days. This partitioning helps the model capture the progression ofthe disease more accurately over different time periods. Throughthis series of data augmentation steps, we provided richer and moretargeted feature data for subsequent model training.
with MCI and organizing the relevant information to accurately identify the corresponding MRI images.Next, we added a new feature to the dataset that represents the time interval between diagnoses, named "tadpole t."This feature is used to capture the time difference from the initial diagnosis to the follow-up diagnosis.</p>
<p>The batch size was set to 32 to balance the training speed and model convergence.The number of training epochs was set to 100TABLE Comparison of AD-Di with other methods on the -year ADNI dataset.
Han./fncom..MethodPrecisionRecallF -scoreAccuracyMCCResnet50 (Fulton et al., 2019)76.06%73.47%63.69%68.24%50.89%Resnet101 (Buvaneswari and Gayathri, 2021)75.61%71.80%71.80%71.80%51.80%TabTransformer (Aguayo et al., 2023)81.33%61.80%93.47%74.47%65.11%XGBoost (Pang et al., 2019)88.47%88.42%88.22%88.05%78.00%GBDT (Huang et al., 2021)80.82%83.09%71.80%77.04%62.64%Adaboost (Morra et al., 2009)89.90%89.72%89.72%89.72%80.97%3D CNN (Khagi and Kwon, 2020)69.06%59.34%91.80%72.15%47.26%AD-Diff (Ours)93.30%93.21%88.47%90.78%86.95%Bold indicates the best results.Frontiers in Computational Neurosciencefrontiersin.org</p>
<p>TABLE Comparison of AD-Di with other methods on the -year ADNI dataset.
MethodPrecisionRecallF -scoreAccuracyMCCResnet50 (Fulton et al., 2019)79.64%77.05%67.27%71.82%54.47%Resnet101 (Buvaneswari and Gayathri, 2021)79.19%75.38%75.38%75.38%55.38%TabTransformer (Aguayo et al., 2023)84.91%65.38%97.05%78.05%68.69%XGBoost (Pang et al., 2019)92.05%92.00%91.80%91.63%81.58%GBDT (Huang et al., 2021)84.40%86.67%75.38%80.62%66.22%Adaboost (Morra et al., 2009)93.48%93.30%93.30%93.30%84.55%3D CNN (Khagi and Kwon, 2020)72.64%62.92%95.38%75.73%50.84%AD-Diff (Ours)96.88%96.79%92.05%94.36%90.53%Bold indicates the best results.</p>
<p>TABLE Comparison of AD-Di with other methods on the -year OASIS dataset.
Han./fncom..MethodPrecisionRecallF -scoreAccuracyMCCResnet50 (Fulton et al., 2019)74.05%71.46%61.68%66.23%48.88%Resnet101 (Buvaneswari and Gayathri, 2021)73.60%69.79%69.79%69.79%49.79%TabTransformer (Aguayo et al., 2023)79.32%59.79%91.46%72.46%63.10%XGBoost (Pang et al., 2019)86.46%86.41%86.21%86.04%76.99%GBDT (Huang et al., 2021)78.81%81.08%69.79%75.03%60.63%Adaboost (Morra et al., 2009)87.89%87.71%87.71%87.71%78.96%3d CNN (Khagi and Kwon, 2020)67.05%57.33%89.79%70.14%45.25%AD-Diff (Ours)91.29%91.20%86.46%88.77%84.94%Bold indicates the best results.Frontiers in Computational Neurosciencefrontiersin.org</p>
<p>TABLE Comparison of AD-Di with other methods on the -year OASIS dataset.
MethodPrecisionRecallF -scoreAccuracyMCCResnet50 (Fulton et al., 2019)79.68%77.09%67.31%71.86%54.51%Resnet101 (Buvaneswari and Gayathri, 2021)79.23%75.42%75.42%75.42%55.42%TabTransformer (Aguayo et al., 2023)84.95%65.42%97.09%78.09%68.73%XGBoost (Pang et al., 2019)92.09%92.04%91.84%91.67%82.62%GBDT (Huang et al., 2021)84.44%86.71%75.42%80.66%66.26%Adaboost (Morra et al., 2009)93.52%93.34%93.34%93.34%84.59%3d CNN (Khagi and Kwon, 2020)72.68%62.96%95.42%75.77%50.88%AD-Diff (Ours)93.92%93.83%92.09%94.40%90.57%Bold indicates the best results.</p>
<p>TABLE Ablation experiments on AD-Di in -Year ADNI dataset.The final row represents the method proposed in this paper, while the remaining rows retain other key components.Ablation experiments have been conducted to remove ADdiffusion, PET Image Reference, PL-Bi-Cross Attention, Image Data, and Table Data, respectively.Bold indicates the best results.
Han./fncom..MethodPrecisionRecallF -scoreAccuracyMCCw/o ADdiffusion85.99%81.75%82.71%81.63%75.02%w/o PET image reference81.28%88.42%81.52%89.95%83.81%w/o PL-Bi-cross attention89.80%83.42%87.89%85.60%80.56%w/o Image data88.08%77.37%86.66%81.75%74.53%w/o Table data84.63%66.84%85.09%74.89%66.26%AD-Diff (Ours)91.29%91.20%86.46%88.77%84.94%Frontiers in Computational Neurosciencefrontiersin.org</p>
<p>TABLE Ablation experiments on AD-Di in -year ADNI dataset.The final row represents the method proposed in this paper, while the remaining rows retain other key components.Ablation experiments have been conducted to remove ADdiffusion, PET Image Reference, PL-Bi-Cross Attention, Image Data, and Table Data, respectively.Bold indicates the best results.This emphasizes the core role of table data in enhancing the comprehensive performance of the model.As shown in Table 7, we further conducted ablation experiments on the 1-year ADNI dataset:Removing the ADdiffusion module resulted in a decrease of 13.89 percentage points in precision, 15.04 percentage points in recall, 9.34 percentage points in F1-score, 12.73 percentage points in accuracy,
MethodPrecisionRecallF -scoreAccuracyMCCw/o ADdiffusion82.99%81.75%82.71%81.63%80.61%w/o PET image reference85.87%83.42%87.11%85.54%81.40%w/o PL-Bi-cross attention84.80%88.42%83.48%90.60%85.71%w/o Image data83.67%82.96%81.93%87.34%80.12%w/o Table data80.22%72.43%80.68%80.48%71.85%AD-Diff (Ours)96.88%96.79%92.05%94.36%90.53%performance. Removing the PET image reference led to a 7.67percentage point increase in recall, but precision decreased by 10.01percentage points, F1-score decreased by 4.94 percentage points,accuracy saw only a slight improvement of 1.18 percentage points,and MCC decreased by 1.13 percentage points. This suggests thatthe PET image reference plays an important role in improvingmodel precision and overall performance. Although its removalimproved recall, it also caused a significant decline in precisionand F1-score, highlighting its critical role in feature extractionand model optimization. Removing the Mamaba module led to adecrease of 1.49 percentage points in precision, 7.78 percentagepoints in recall, a slight increase of 1.43 percentage points in F1-score, a decrease of 3.17 percentage points in accuracy, and adecrease of 4.38 percentage points in MCC. This indicates thatthe Mamaba module has a certain impact on improving recalland overall stability, with a slight improvement in F1-score butan overall decrease in performance. Removing image data resultedin a decrease of 3.21 percentage points in precision, a significantdecrease of 13.83 percentage points in recall, a slight increase inF1-score, a decrease of 7.02 percentage points in accuracy, anda decrease of 10.41 percentage points in MCC. This shows thatimage data is crucial for improving the model's recall and overallaccuracy, and its removal severely weakened the model's abilityto handle complex visual features. Finally, removing table datacaused a decrease of 6.66 percentage points in precision, 24.36percentage points in recall, 1.37 percentage points in F1-score,13.88 percentage points in accuracy, and 18.68 percentage pointsin MCC.
Frontiers in Computational Neuroscience frontiersin.org
Data availability statementThe original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author.FundingThe author(s) declare that no financial support was received for the research and/or publication of this article.Author contributionsLH: Data curation, Methodology, Writing -original draft.Conflict of interestThe author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher's noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.
Machine learning for predicting neurodegenerative diseases in the general older population: a cohort study. G A Aguayo, L Zhang, M Vaillant, M Ngari, M Perquin, V Moran, 10.1186/s12874-023-01837-4BMC Med. Res. Methodol. 2382023</p>
<p>Recognition of Alzheimer's disease and mild cognitive impairment with multimodal image-derived biomarkers and multiple kernel learning. O B Ahmed, J Benois-Pineau, M Allard, G Catheline, C B Amar, A D N Initiative, 10.1016/j.neucom.2016.08.041Neurocomputing. 2202017</p>
<p>Mini-mental state examination (MMSE) for the detection of Alzheimer's disease and other dementias in people with mild cognitive impairment (MCI). I Arevalo-Rodriguez, N Smailagic, M Roqué I Figuls, A Ciapponi, E Sanchez-Perez, A Giannakou, 10.1002/14651858.CD010783.pub2Cochrane Database Syst. Rev. D0107832015. 2015</p>
<p>A systematic review on machine learning and deep learning techniques in the effective diagnosis of Alzheimer's disease. A D Arya, S S Verma, P Chakarabarti, T Chakrabarti, A A Elngar, A.-M Kamali, 10.1186/s40708-023-00195-7Brain Inform. 10172023</p>
<p>Multimodal data analysis of Alzheimer's disease based on clustering evolutionary random forest. X Bi, -A, X Hu, H Wu, Y Wang, 10.1109/JBHI.2020.2973324IEEE J. Biomed. Health Inform. 242020</p>
<p>Neural computation-based methods for the early diagnosis and prognosis of Alzheimer's disease not using neuroimaging biomarkers: a systematic review. P Buvaneswari, R Gayathri, Y Cabrera-León, P G Báez, P Fernández-López, C P Suárez-Araujo, 10.3233/JAD-231271doi: 10.3233/JAD-231271J. Alzheimers Dis. 462021. 2024aJ. Sci. Eng.</p>
<p>Toward an intelligent computing system for the early diagnosis of Alzheimer's disease based on the modular hybrid growing neural gas. Y Cabrera-León, P Fernández-López, P García Báez, K Kluwak, J L Navarro-Mesa, C P Suárez-Araujo, 10.1177/20552076241284349Digit. Health. 10205520762412843492024b</p>
<p>Multimodal neuroimaging in Alzheimer's disease: early diagnosis, physiopathological mechanisms, and impact of lifestyle. G Chételat, 10.3233/JAD-179920J. Alzheimers Dis. 642018</p>
<p>The advantages of the matthews correlation coefficient (MCC) over f1 score and accuracy in binary classification evaluation. D Chicco, G Jurman, 10.1186/s12864-019-6413-7BMC Genom. 212020</p>
<p>Modeling Alzheimer's disease progression using disease onset time and disease trajectory concepts applied to cdr-sob scores from ADNI. I Delor, J.-E Charoin, R Gieschke, S Retout, P Jacqmin, 10.1038/psp.2013.54CPT: Pharmacometrics Syst. Pharmacol. 22013</p>
<p>Correlation of iron in the hippocampus with mmse in patients with Alzheimer's disease. B Ding, K.-M Chen, H.-W Ling, F Sun, X Li, T Wan, 10.1002/jmri.21730J. Magn. Reson. Imaging. 292009</p>
<p>A CNN-RNN-LSTM based amalgamation for Alzheimer's disease detection. M Dua, D Makhija, P Manasa, P Mishra, 10.1007/s40846-020-00556-1J. Med. Biol. Eng. 402020</p>
<p>Multimodal fusion-based deep learning network for effective diagnosis of Alzheimer's disease. S Dwivedi, T Goel, M Tanveer, R Murugan, R Sharma, 10.1109/MMUL.2022.3156471IEEE MultiMedia. 292022</p>
<p>Alzheimer's Disease Neuroimaging Initiative. A Ebrahimi, S Luo, 2021</p>
<p>Convolutional neural networks for Alzheimer's disease detection on mri images. 10.1117/1.JMI.8.2.024503J. Med. Imaging. 824503</p>
<p>Deep learning to detect Alzheimer's disease from neuroimaging: a systematic literature review. M A Ebrahimighahnavieh, S Luo, R Chiong, 10.1016/j.cmpb.2019.105242Comput. Methods Programs Biomed. 1871052422020</p>
<p>A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer's disease. S El-Sappagh, J M Alonso, S R Islam, A M Sultan, K S Kwak, M Adar, I Diamant, E Klang, M Amitai, J Goldberger, H Greenspan, 10.1016/j.neucom.2018.09.013doi: 10.1016/j.neucom.2018.09.013Neurocomputing. 112021. 2018Sci. Rep.</p>
<p>Classification of Alzheimer's disease with and without imagery using gradient boosted machines and resnet-50. L V Fulton, D Dolezel, J Harrop, Y Yan, C P Fulton, 10.3390/brainsci9090212Brain Sci. 92122019</p>
<p>A review of the application of deep learning in the detection of Alzheimer's disease. S Gao, D Lima, 10.1016/j.ijcce.2021.12.002Int. J. Cogn. Comput. Eng. 32022</p>
<p>I2v-adapter: a general image-to-video adapter for diffusion models. X Guo, M Zheng, L Hou, Y Gao, Y Deng, P Wan, 10.1145/3641519.3657407ACM SIGGRAPH 2024 Conference Papers. New York, NYACM2024</p>
<p>Long-term use of opioids for nonmalignant pain among community-dwelling persons with and without Alzheimer disease in Finland: a nationwide register-based study. A Hamina, H Taipale, A Tanskanen, A.-M Tolppanen, N Karttunen, L Pylkkänen, 10.1097/j.pain.0000000000000752Pain. 1582017</p>
<p>Deep learning approach for early detection of Alzheimer's disease. H A Helaly, M Badawy, A Y Haikal, 10.1007/s12559-021-09946-2Cognit. Comput. 142022</p>
<p>Pairwise correlation analysis of the Alzheimer's disease neuroimaging initiative (adni) dataset reveals significant feature correlation. Y Huang, X Sun, H Jiang, S Yu, C Robins, M J Armstrong, 10.3390/genes12111661doi: 10.3390/genes12111661Nat. Commun. 1216612021. 2021Genes</p>
<p>3D CNN design for the classification of Alzheimer's disease using brain mri and pet. B Khagi, G.-R Kwon, 10.1109/ACCESS.2020.3040486IEEE Access. 82020</p>
<p>Deep learning for Alzheimer's disease diagnosis: a survey. M Khojaste-Sarakhsi, S S Haghighi, S F Ghomi, E Marchiori, 10.1016/j.artmed.2022.102332Artif. Intell. Med. 1301023322022</p>
<p>Risk of mortality associated with antipsychotic monotherapy and polypharmacy among community-dwelling persons with Alzheimer's disease. M Koponen, H Taipale, P Lavikainen, A Tanskanen, J Tiihonen, A.-M Tolppanen, 10.3233/JAD-160671J. Alzheimers Dis. 562017</p>
<p>Oasis-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer disease. P J Lamontagne, T L Benzinger, J C Morris, S Keefe, R Hornbeck, C Xiong, 10.1101/2019.12.13.19014902medRxiv. 2019</p>
<p>Short-and long-term mortality risk associated with the use of antipsychotics among 26,940 dementia outpatients: a population-based study. E M Langballe, B Engdahl, H Nordeng, C Ballard, D Aarsland, G Selbaek, 10.1016/j.jagp.2013.06.007Am. J. Geriatr. Psychiatry. 222014</p>
<p>Use of multimodality imaging and artificial intelligence for diagnosis and prognosis of early stages of Alzheimer's disease. G Lee, K Nho, B Kang, K.-A Sohn, D Kim, X Liu, K Chen, T Wu, D Weidman, F Lure, J Li, 10.1016/j.trsl.2018.01.001doi: 10.1016/j.trsl.2018.01.001Transl. Res. 92019. 2018Sci. Rep.</p>
<p>Deep learning based pipelines for Alzheimer's disease diagnosis: a comparative study and a novel deep-ensemble method. A Loddo, S Buttau, C Di Ruberto, 10.1016/j.compbiomed.2021.105032Comput. Biol. Med. 1411050322022</p>
<p>Multimodal and multiscale deep neural networks for the early diagnosis of Alzheimer's disease using structural MR and FDG-PET images. D Lu, K Popuri, G W Ding, R Balachandar, M F Beg, J H Morra, Z Tu, L G Apostolova, A E Green, A W Toga, 10.1038/s41598-018-22871-zSci. Rep. 856972018</p>
<p>Comparison of adaboost and support vector machines for detecting Alzheimer's disease through automated hippocampal segmentation. M , 10.1109/TMI.2009.2021941IEEE Trans. Med. Imaging. 292009</p>
<p>Vision mamba: cuttingedge classification of Alzheimer's disease with 3D MRI scans. K A Muthukumar, A Gurung, P Ranjan, 10.48550/arXiv.2406.05757arXiv:2406.057572024Preprint</p>
<p>TSYND: targeted synthetic data generation for enhanced medical image classification: leveraging epistemic uncertainty to improve model performance. J Niemeijer, J Ehrhardt, H Uzunova, H Handels, 10.1007/978-3-031-73281-2_7International Workshop on Simulation and Synthesis in Medical Imaging. ChamSpringer2024</p>
<p>A novel protein subcellular localization method with CNN-xgboost model for Alzheimer's disease. L Pang, J Wang, L Zhao, C Wang, H Zhan, 10.3389/fgene.2018.00751Front. Genet. 97512019</p>
<p>SAG-GAN: semi-supervised attention-guided gans for data augmentation on medical images. C Qi, J Chen, G Xu, Z Xu, T Lukasiewicz, Y Liu, 10.48550/arXiv.2011.07534arXiv:2011.075342020Preprint</p>
<p>Multimodal deep learning for Alzheimer's disease dementia assessment. S Qiu, M I Miller, P S Joshi, J C Lee, C Xue, Y Ni, 10.1038/s41467-022-31037-5Nat. Commun. 1334042022</p>
<p>Multimodal prediction of conversion to Alzheimer's disease based on incomplete biomarkers. K Ritter, J Schumacher, M Weygandt, R Buchert, C Allefeld, J.-D Haynes, 10.1016/j.dadm.2015.01.006Alzheimers Dement. 12015</p>
<p>. Frontiers in Computational Neuroscience frontiersin.org. </p>
<p>. Han, 10.3389/fncom.2025.1484540</p>
<p>Deep learning-based diagnosis of Alzheimer's disease. T J Saleem, S R Zahra, F Wu, A Alwakeel, M Alwakeel, F Jeribi, 10.3390/jpm12050815J. Pers. Med. 128152022</p>
<p>Detecting Alzheimer's disease by the decision tree methods based on particle swarm optimization. R Saputra, C Agustina, D Puspitasari, R Ramanda, D Pribadi, K Indriani, 10.1088/1742-6596/1641/1/012025J. Phys. Conf. Ser. 1641120252020</p>
<p>. A Sharma, S Kaur, N Memon, A J Fathima, S Ray, M W Bhatt, 2021</p>
<p>Alzheimer's patients detection using support vector machine (SVM) with quantitative analysis. 10.1016/j.neuri.2021.100012Neurosci. Inform. 1100012</p>
<p>Multimodal neuroimaging feature learning with multimodal stacked deep polynomial networks for diagnosis of Alzheimer's disease. J Shi, X Zheng, Y Li, Q Zhang, S Ying, 10.1109/JBHI.2017.2655720IEEE J. Biomed. Health Inform. 222017</p>
<p>Leveraging coupled interaction for multimodal Alzheimer's disease diagnosis. Y Shi, H.-I Suk, Y Gao, S.-W Lee, D Shen, 10.1109/TNNLS.2019.2900077IEEE Trans. Neural Netw. Learn. Syst. 312019</p>
<p>A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer's disease. A Sikka, S V Peri, D R Bathula, S Spasov, L Passamonti, A Duggento, P Lio, N Toschi, A D N Initiative, 10.1016/j.neuroimage.2019.01.031doi: 10.1016/j.neuroimage.2019.01.031Simulation and Synthesis in Medical Imaging: Third International Workshop, SASHIMI 2018, Held in Conjunction with MICCAI 2018. Granada, Spain; ChamSpringer2018. September 16. 2018. 20193Neuroimage</p>
<p>A real-time clinical decision support system, for mild cognitive impairment detection, based on a hybrid neural architecture. C P Suárez-Araujo, P García Báez, Y Cabrera-León, A Prochazka, N Rodríguez Espinosa, Fernández, C Viadero, 10.1155/2021/5545297Comput. Math. Methods Med. 55452972021. 2021</p>
<p>Multimodal fusion diagnosis of Alzheimer's disease based on fdg-pet generation. Y Tu, S Lin, J Qiao, Y Zhuang, Z Wang, D Wang, 10.1016/j.bspc.2023.105709Biomed. Signal Process. Control. 891057092024</p>
<p>. J Venugopalan, L Tong, H R Hassanzadeh, M D Wang, 2021</p>
<p>Progression of Alzheimer's disease as measured by clinical dementia rating sum of boxes scores. M M Williams, M Storandt, C M Roe, J C Morris, 10.1016/j.jalz.2012.01.005doi: 10.1016/j.jalz.2012.01.005Alzheimers Dement. 112013Sci. Rep.</p>
<p>Tune-avideo: one-shot tuning of image diffusion models for text-to-video generation. J Z Wu, Y Ge, X Wang, S W Lei, Y Gu, Y Shi, 10.1109/ICCV51070.2023.00701Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionParisIEEE2023</p>
<p>Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification models. R Yacouby, D Axman, 10.18653/v1/2020.eval4nlp-1.9Proceedings of the first workshop on evaluation and comparison of NLP systems. the first workshop on evaluation and comparison of NLP systemsAssociation for Computational Linguistics2020</p>
<p>H Ye, J Zhang, S Liu, X Han, W Yang, 10.48550/arXiv.2308.06721arXiv:2308.06721Ip-adapter: text compatible image prompt adapter for text-to-image diffusion models. 2023Preprint</p>
<p>Classification of Alzheimer's disease mri images with CNN based hybrid method. M Yildirim, A Cinar, 10.18280/isi.250402Ing. Syst. Inf. 252020</p>
<p>Accurate multimodal probabilistic prediction of conversion to Alzheimer's disease in patients with mild cognitive impairment. J Young, M Modat, M J Cardoso, A Mendelson, D Cash, S Ourselin, 10.1016/j.nicl.2013.05.004Neuroimage Clin. 22013</p>
<p>. D Zhang, Y Wang, L Zhou, H Yuan, D Shen, A D Initiative, </p>
<p>Multimodal classification of Alzheimer's disease and mild cognitive impairment. N , 10.1016/j.neuroimage.2011.01.008Neuroimage. 552011</p>
<p>Multi-modal deep learning model for auxiliary diagnosis of Alzheimer's disease. F Zhang, Z Li, B Zhang, H Du, B Wang, X Zhang, 10.1016/j.neucom.2019.04.093Neurocomputing. 3612019</p>
<p>. X Zhao, C K E Ang, U R Acharya, K H Cheong, 2021</p>
<p>Application of artificial intelligence techniques for the detection of Alzheimer's disease using structural MRI images. 10.1016/j.bbe.2021.02.006Biocybernet. Biomed. Eng. 41</p>
<p>Use of a sparse-response deep belief network and extreme learning machine to discriminate Alzheimer's disease, mild cognitive impairment, and normal controls based on amyloid PET/MRI images. P Zhou, S Jiang, L Yu, Y Feng, C Chen, F Li, 10.3389/fmed.2020.621204Frontiers in Computational Neuroscience frontiersin.org. 76212042021Front. Med.</p>            </div>
        </div>

    </div>
</body>
</html>
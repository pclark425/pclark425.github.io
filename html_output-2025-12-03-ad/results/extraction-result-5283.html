<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-43ed30245390d183cd8c6ec629def096c4b14479</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/43ed30245390d183cd8c6ec629def096c4b14479" target="_blank">A Transformer-based Generative Model for De Novo Molecular Design</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A Transformer-based deep model for de novo target-speciﬁc molecular design is proposed and allows the generation of SMILES strings to be conditional on the speci ﬁed target.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>conditional Transformer for target-specific de novo molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer trained autoregressively on SMILES (unsupervised pre-training on MOSES) and fine-tuned with target-specific embeddings (fed as keys/values in multi-head attention) to generate target-conditioned novel drug-like molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cTransformer (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only Transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-training: MOSES dataset (1.76M drug-like SMILES from ZINC clean lead collection; 1,584,664 train / 176,075 test). Fine-tuning: target-specific compound sets for EGFR, HTR1A, S1PR1 (from public sources / dataset in [55]); QSAR training sets from ExCAPE-DB.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo, target-specific small-molecule generation (EGFR, HTR1A, S1PR1)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Unsupervised next-token autoregressive pre-training on SMILES, then conditional fine-tuning where target-specific embeddings are provided as the keys and values of the decoder's second multi-head attention so sampling is conditioned on target; stochastic sampling to generate novel SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (RDKit), Uniqueness (Unique@1k, Unique@10k), Novelty (not in fine-tune train set), Fragment similarity (BRICS), Similarity to nearest neighbor (SNN, Tanimoto on fingerprints), property distributions (MW, LogP, SA, QED, with Wasserstein-1 distance to MOSES test), QSAR-predicted activity distributions (pXC50 via LightGBM QSAR models; Pearson R and RMSE reported), UMAP visualization of chemical-space overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES (pre-training & benchmarking), ExCAPE-DB (QSAR labels), target-specific sets (EGFR, HTR1A, S1PR1) from referenced dataset [55], RDKit for validity/descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Base cTransformer achieved state-of-the-art MOSES-style metrics (e.g., Valid ~0.988 on base model; strong Frag and SNN). Conditional cTransformer (fine-tuned per-target) generated 30k samples per target with validity 0.885–0.926, Unique@10k 0.838–0.940, and novelty 0.684–0.898 (depending on target). QSAR evaluation showed generated compounds' predicted activity distributions were superior to a conditional RNN baseline; QSAR models had Pearson R > 0.75, and UMAP projections showed generated molecules occupy the same subchemical space as real target actives while including many novel structures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to a conditional RNN (cRNN) baseline: cTransformer produced comparable or higher uniqueness and much higher novelty for most targets and produced distributions of predicted activity (via QSAR) superior to cRNN. Base cTransformer was compared to a range of generative baselines (HMM, NGram, Combinatorial, CharRNN, VAE, AAE, JTN-VAE, LatentGAN) on MOSES metrics and achieved state-of-the-art results on validity, uniqueness and scaffold/fragment similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model size/compute requirements not specified; reliance on SMILES representation (permutation and grammar sensitivity). Conditional performance evaluated only with computational QSAR—no experimental (wet-lab) validation of binding or synthesis. For some targets cTransformer had slightly lower validity than cRNN (e.g., EGFR: cTransformer validity 0.885 vs cRNN 0.921). Conditional conditioning mechanism requires labeled <compound,target> pairs for fine-tuning; generalization to novel/unseen targets not demonstrated. QSAR-based activity evaluation may propagate QSAR model errors and biases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>conditional Recurrent Neural Network baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional RNN baseline (char-level RNN) trained by pre-training on MOSES and fine-tuning on target-specific data to generate SMILES conditional on target labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cRNN (this work, baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (char-level autoregressive RNN / LSTM-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-training on MOSES (same as cTransformer) and fine-tuning on the same target-specific datasets (EGFR, HTR1A, S1PR1).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — target-conditioned SMILES generation (baseline comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pre-train autoregressive RNN on SMILES, then fine-tune with target annotations to condition generation; sampling from the RNN to produce SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as cTransformer: validity, uniqueness@10k, novelty, QSAR predicted activities, and distribution comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MOSES, ExCAPE-DB, target-specific datasets</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Produced valid molecules with validity ~0.921–0.926 across targets; uniqueness and novelty generally lower than cTransformer (novelty: EGFR 0.662 vs cTransformer 0.898; HTR1A 0.498 vs 0.787; S1PR1 0.514 vs 0.684). Predicted activity distributions (QSAR) were worse than cTransformer's for the top-ranked generated compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as direct RNN-based baseline; cTransformer outperformed cRNN on novelty, uniqueness (most targets) and QSAR activity distributions, though cRNN had slightly higher validity on some targets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RNNs struggle with long-range dependencies and parallelization compared to Transformer; produced less novel and less target-active compounds in these experiments; same limitation of only computational activity evaluation without experimental validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT (GPT / GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer family (GPT, GPT-2) referenced as inspiration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive Transformer language models (GPT, GPT-2) trained by next-token prediction on large unlabeled text corpora; cited as motivating the use of unsupervised pre-training/autoregressive modeling for SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving language understanding by generative pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT (Radford et al.) / GPT-2 (Radford et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive Transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale unlabeled natural text corpora (massive web text for GPT-2); not chemical data in the referenced works but cited as conceptual inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Natural language generation (cited as methodological inspiration for SMILES generation in drug discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Unsupervised next-token prediction pre-training, conditional generation by prompt/conditioning in downstream tasks (in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Text (in original works); here cited as inspiration for SMILES autoregressive generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated on chemistry in this paper; cited papers evaluate language metrics and downstream task transfer; in this work they motivate next-token SMILES modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Large web text corpora (as in original GPT/GPT-2 papers); not used directly for molecules in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as evidence that autoregressive Transformer pre-training can learn rich generative structure and motivate applying similar unsupervised pre-training to SMILES; no chemical generation experiments with GPT/GPT-2 reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used conceptually to justify decoder-only autoregressive Transformer design; cTransformer is an application-specific Transformer trained on chemical SMILES rather than natural text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Original GPT/GPT-2 are trained on text, so direct application to SMILES requires domain-specific adaptation; paper does not report applying off-the-shelf GPT/GPT-2 weights to chemistry.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5283.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5283.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (Attention is All You Need)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer architecture (self-attention based models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The self-attention Transformer architecture is the backbone for cTransformer; used both as conceptual and architectural basis for SMILES autoregressive generation and conditioning via attention keys/values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (Vaswani et al.) as implemented in cTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multi-head self-attention based Transformer (decoder-only variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>See cTransformer (MOSES pre-training, target fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Used for SMILES sequence modeling and conditional molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Decoder-only Transformer autoregressive next-token prediction; conditioning by injecting target embeddings as keys/values to attention</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>See cTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>See cTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Transformer-based implementation (cTransformer) outperformed several other generative model baselines on MOSES metrics and produced more novel and predicted-active target-specific molecules than the RNN baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Transformer (decoder-only) outperformed RNN baseline and matched/beat other generative models on multiple MOSES metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper does not report ablation on Transformer hyperparameters or model size; conditioning via keys/values is a proposed method but its limits (e.g., scaling to many targets or unseen targets) are not fully explored.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Deep learning enables rapid identification of potent ddr1 kinase inhibitors <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>A de novo molecular generation method using latent vector based generative adversarial network <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5283",
    "paper_id": "paper-43ed30245390d183cd8c6ec629def096c4b14479",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "cTransformer",
            "name_full": "conditional Transformer for target-specific de novo molecular generation",
            "brief_description": "A decoder-only Transformer trained autoregressively on SMILES (unsupervised pre-training on MOSES) and fine-tuned with target-specific embeddings (fed as keys/values in multi-head attention) to generate target-conditioned novel drug-like molecules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "cTransformer (this work)",
            "model_type": "decoder-only Transformer (autoregressive)",
            "model_size": null,
            "training_data": "Pre-training: MOSES dataset (1.76M drug-like SMILES from ZINC clean lead collection; 1,584,664 train / 176,075 test). Fine-tuning: target-specific compound sets for EGFR, HTR1A, S1PR1 (from public sources / dataset in [55]); QSAR training sets from ExCAPE-DB.",
            "application_domain": "Drug discovery — de novo, target-specific small-molecule generation (EGFR, HTR1A, S1PR1)",
            "generation_method": "Unsupervised next-token autoregressive pre-training on SMILES, then conditional fine-tuning where target-specific embeddings are provided as the keys and values of the decoder's second multi-head attention so sampling is conditioned on target; stochastic sampling to generate novel SMILES.",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "Validity (RDKit), Uniqueness (Unique@1k, Unique@10k), Novelty (not in fine-tune train set), Fragment similarity (BRICS), Similarity to nearest neighbor (SNN, Tanimoto on fingerprints), property distributions (MW, LogP, SA, QED, with Wasserstein-1 distance to MOSES test), QSAR-predicted activity distributions (pXC50 via LightGBM QSAR models; Pearson R and RMSE reported), UMAP visualization of chemical-space overlap.",
            "benchmarks_or_datasets": "MOSES (pre-training & benchmarking), ExCAPE-DB (QSAR labels), target-specific sets (EGFR, HTR1A, S1PR1) from referenced dataset [55], RDKit for validity/descriptors",
            "results_summary": "Base cTransformer achieved state-of-the-art MOSES-style metrics (e.g., Valid ~0.988 on base model; strong Frag and SNN). Conditional cTransformer (fine-tuned per-target) generated 30k samples per target with validity 0.885–0.926, Unique@10k 0.838–0.940, and novelty 0.684–0.898 (depending on target). QSAR evaluation showed generated compounds' predicted activity distributions were superior to a conditional RNN baseline; QSAR models had Pearson R &gt; 0.75, and UMAP projections showed generated molecules occupy the same subchemical space as real target actives while including many novel structures.",
            "comparison_to_other_methods": "Compared directly to a conditional RNN (cRNN) baseline: cTransformer produced comparable or higher uniqueness and much higher novelty for most targets and produced distributions of predicted activity (via QSAR) superior to cRNN. Base cTransformer was compared to a range of generative baselines (HMM, NGram, Combinatorial, CharRNN, VAE, AAE, JTN-VAE, LatentGAN) on MOSES metrics and achieved state-of-the-art results on validity, uniqueness and scaffold/fragment similarity.",
            "limitations_or_challenges": "Model size/compute requirements not specified; reliance on SMILES representation (permutation and grammar sensitivity). Conditional performance evaluated only with computational QSAR—no experimental (wet-lab) validation of binding or synthesis. For some targets cTransformer had slightly lower validity than cRNN (e.g., EGFR: cTransformer validity 0.885 vs cRNN 0.921). Conditional conditioning mechanism requires labeled &lt;compound,target&gt; pairs for fine-tuning; generalization to novel/unseen targets not demonstrated. QSAR-based activity evaluation may propagate QSAR model errors and biases.",
            "uuid": "e5283.0"
        },
        {
            "name_short": "cRNN",
            "name_full": "conditional Recurrent Neural Network baseline",
            "brief_description": "A conditional RNN baseline (char-level RNN) trained by pre-training on MOSES and fine-tuning on target-specific data to generate SMILES conditional on target labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "cRNN (this work, baseline)",
            "model_type": "recurrent neural network (char-level autoregressive RNN / LSTM-style)",
            "model_size": null,
            "training_data": "Pre-training on MOSES (same as cTransformer) and fine-tuning on the same target-specific datasets (EGFR, HTR1A, S1PR1).",
            "application_domain": "Drug discovery — target-conditioned SMILES generation (baseline comparator)",
            "generation_method": "Pre-train autoregressive RNN on SMILES, then fine-tune with target annotations to condition generation; sampling from the RNN to produce SMILES.",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "Same as cTransformer: validity, uniqueness@10k, novelty, QSAR predicted activities, and distribution comparisons.",
            "benchmarks_or_datasets": "MOSES, ExCAPE-DB, target-specific datasets",
            "results_summary": "Produced valid molecules with validity ~0.921–0.926 across targets; uniqueness and novelty generally lower than cTransformer (novelty: EGFR 0.662 vs cTransformer 0.898; HTR1A 0.498 vs 0.787; S1PR1 0.514 vs 0.684). Predicted activity distributions (QSAR) were worse than cTransformer's for the top-ranked generated compounds.",
            "comparison_to_other_methods": "Serves as direct RNN-based baseline; cTransformer outperformed cRNN on novelty, uniqueness (most targets) and QSAR activity distributions, though cRNN had slightly higher validity on some targets.",
            "limitations_or_challenges": "RNNs struggle with long-range dependencies and parallelization compared to Transformer; produced less novel and less target-active compounds in these experiments; same limitation of only computational activity evaluation without experimental validation.",
            "uuid": "e5283.1"
        },
        {
            "name_short": "GPT (GPT / GPT-2)",
            "name_full": "Generative Pre-trained Transformer family (GPT, GPT-2) referenced as inspiration",
            "brief_description": "Autoregressive Transformer language models (GPT, GPT-2) trained by next-token prediction on large unlabeled text corpora; cited as motivating the use of unsupervised pre-training/autoregressive modeling for SMILES generation.",
            "citation_title": "Improving language understanding by generative pre-training.",
            "mention_or_use": "mention",
            "model_name": "GPT (Radford et al.) / GPT-2 (Radford et al.)",
            "model_type": "autoregressive Transformer language models",
            "model_size": null,
            "training_data": "Large-scale unlabeled natural text corpora (massive web text for GPT-2); not chemical data in the referenced works but cited as conceptual inspiration.",
            "application_domain": "Natural language generation (cited as methodological inspiration for SMILES generation in drug discovery)",
            "generation_method": "Unsupervised next-token prediction pre-training, conditional generation by prompt/conditioning in downstream tasks (in cited works).",
            "output_representation": "Text (in original works); here cited as inspiration for SMILES autoregressive generation",
            "evaluation_metrics": "Not evaluated on chemistry in this paper; cited papers evaluate language metrics and downstream task transfer; in this work they motivate next-token SMILES modeling.",
            "benchmarks_or_datasets": "Large web text corpora (as in original GPT/GPT-2 papers); not used directly for molecules in this paper.",
            "results_summary": "Mentioned as evidence that autoregressive Transformer pre-training can learn rich generative structure and motivate applying similar unsupervised pre-training to SMILES; no chemical generation experiments with GPT/GPT-2 reported in this paper.",
            "comparison_to_other_methods": "Used conceptually to justify decoder-only autoregressive Transformer design; cTransformer is an application-specific Transformer trained on chemical SMILES rather than natural text.",
            "limitations_or_challenges": "Original GPT/GPT-2 are trained on text, so direct application to SMILES requires domain-specific adaptation; paper does not report applying off-the-shelf GPT/GPT-2 weights to chemistry.",
            "uuid": "e5283.2"
        },
        {
            "name_short": "Transformer (Attention is All You Need)",
            "name_full": "Transformer architecture (self-attention based models)",
            "brief_description": "The self-attention Transformer architecture is the backbone for cTransformer; used both as conceptual and architectural basis for SMILES autoregressive generation and conditioning via attention keys/values.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "model_name": "Transformer (Vaswani et al.) as implemented in cTransformer",
            "model_type": "multi-head self-attention based Transformer (decoder-only variant used)",
            "model_size": null,
            "training_data": "See cTransformer (MOSES pre-training, target fine-tuning)",
            "application_domain": "Used for SMILES sequence modeling and conditional molecular generation",
            "generation_method": "Decoder-only Transformer autoregressive next-token prediction; conditioning by injecting target embeddings as keys/values to attention",
            "output_representation": "SMILES",
            "evaluation_metrics": "See cTransformer",
            "benchmarks_or_datasets": "See cTransformer",
            "results_summary": "Transformer-based implementation (cTransformer) outperformed several other generative model baselines on MOSES metrics and produced more novel and predicted-active target-specific molecules than the RNN baseline.",
            "comparison_to_other_methods": "Transformer (decoder-only) outperformed RNN baseline and matched/beat other generative models on multiple MOSES metrics.",
            "limitations_or_challenges": "Paper does not report ablation on Transformer hyperparameters or model size; conditioning via keys/values is a proposed method but its limits (e.g., scaling to many targets or unseen targets) are not fully explored.",
            "uuid": "e5283.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Deep learning enables rapid identification of potent ddr1 kinase inhibitors",
            "rating": 2
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "A de novo molecular generation method using latent vector based generative adversarial network",
            "rating": 2
        }
    ],
    "cost": 0.01178925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Pre-trained Conditional Transformer for Target-specific De Novo Molecular Generation</h1>
<p>Wenlu Wang ${ }^{\S}$<br>Texas A\&amp;M University-Corpus Christi Corpus Christi, TX wenlu.wang@tamucc.edu<br>Honggang Zhao<br>Cornell University<br>Ithaca, NY<br>hz269@cornell.edu</p>
<p>Ye Wang ${ }^{\S}$<br>Biogen<br>Cambridge, MA<br>ye.wang@biogen.com<br>Simone Sciabola<br>Biogen<br>Cambridge, MA<br>simone.sciabola@biogen.com</p>
<h4>Abstract</h4>
<p>In the scope of drug discovery, the molecular design aims to identify novel compounds from the chemical space where the potential drug-like molecules are estimated to be in the order of $10^{60}-10^{100}$. Since this search task is computationally intractable due to the unbounded search space, deep learning draws a lot of attention as a new way of generating unseen molecules. As we seek compounds with specific target proteins, we propose a Transformer-based deep model for de novo targetspecific molecular design. The proposed method is capable of generating both drug-like compounds (without specified targets) and target-specific compounds. The latter are generated by enforcing different keys and values of the multi-head attention for each target. In this way, we allow the generation of SMILES strings to be conditional on the specified target. Experimental results demonstrate that our method is capable of generating both valid drug-like compounds and target-specific compounds. Moreover, the sampled compounds from conditional model largely occupy the real target-specific molecules' chemical space and also cover a significant fraction of novel compounds.</p>
<p>Index Terms-knowledge discovery, molecular design, generative models</p>
<h2>I. INTRODUCTION</h2>
<p>Small molecule drug design aims to identify novel compounds with desired chemical properties. From the computational perspective, we consider this task an optimization problem, where we search for the compounds that will maximize our quantitative goals in chemical space. However, this optimization task is computationally intractable because of the unbounded search space. It has been estimated that the range of potential drug-like molecules estimated to be in the order of $10^{60}$ to $10^{100}$ [1], but only about $10^{8}$ molecules have ever been synthesized [2]. It is impossible to test all the possible virtual molecules due to the combinatorial explosion: the number of different ways the elements are linked is enormous. Numerous computational methods, such as virtual screening, combinatorial libraries, and evolutionary algorithms, have been developed to search the chemical space in silico and in vitro. Recent works have demonstrated that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Deep generative model-based drug discovery workflow.
machine learning, especially deep learning methods, could produce new small molecules [3]-[5] with biological activity. We aim to incorporate as much chemical domain knowledge as possible for directed navigation toward a desired location in the search space.</p>
<p>As shown in Figure 1, deep generative models can generate millions of novel compounds, which can be further refined with the help of virtual screening tools using statistical and physics-based techniques, and a final set of compounds can be synthesized in the laboratory. This paper works on the first step Molecular Generation.</p>
<p>Since the chemical structures can be represented as SMILES (Simplified Molecular Input Line Entry System) strings (will be introduced in Section II-A), the small molecule drug design problem can be transformed to a text generation problem in Natural Language Processing (NLP). A number of deep learning techniques have been successfully applied to text generation. For instance, the GPT (Generative Pre-trained Transformer) series [6]-[8] uses an autoregressive language model to produce human-like text by training from massive unlabeled human-written text. The text generated from the GPT model is of high quality and hard to distinguish from humanwritten content. Similarly, deep models are likely to learn drug-like structures from massive discovered compounds.</p>
<p>Many NLP tasks, such as language translation and text summarization, will be prompted with input (e.g., a sentence and a document), and the problem can be formed as end-toend learning. Unlike traditional language tasks, drug discovery</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The workflow of our <em>cTransformer</em> design. The base model of <em>cTransformer</em> is trained with Table I-(a) data in an unsupervised manner (target-specific embeddings set to <em>zeros</em>). The pre-trained <em>cTransformer</em> will be further fine-tuned with Table I-(b) data with target-specific embeddings.</p>
<p>is a pure generator confined by drug-likeness. The "drug-like" structure generally means meeting the a set of criteria, such as good solubility, potency and molecular weight.</p>
<p>In the scope of small molecule drug design, it is essential to enable the generations to be guided by predefined conditions, such as the target protein. Here we formulate the molecular design problem as a conditional sequential generation given the target protein and propose a conditional Transformer architecture <em>cTransformer</em> that auto-regressively generates target-specific compounds. As shown in Figure 2, we first pre-train a transformer on the MOSES [9] dataset without target information (denoted as the base model), and after pre-training, the base model can generate drug-like structures. Then, the conditional transformer is fine-tuned on three target-specific datasets (EGFR, HTR1A, and S1PR1 targets). Our experiment results show that the transformer is capable of generating compounds similar to the ones in the training set but are novel structures. We believe the conditional transformer to be a valuable tool for de novo molecule design. Note that our conditional transformer design is not limited to target proteins and can be generalized to a broader scope of targets.</p>
<p>We define our molecular design task as generating a valid SMILES string, which is not covered in training data and has desired potency and physicochemical properties. In order to address our task using deep learning, we summarize the challenges as follows: (1) the generated sequence is a valid drug-like structure in compliance with the SMILES grammar; (2) the generated chemical space covers enough variations that are likely to be new drugs; and (3) the generations can be guided by predefined conditions (incorporating the specified targets).</p>
<p>We propose to use a Transformer-based auto-regressive decoder due to the following reasons: the Transformer structure performs well in sequential modeling, especially for long sequences (challenge (1)), we are able to introduce randomness in the sampling process to generate more variations and makes the trained model more "creative" (challenge (2)), and we can prompt the Transformer paradigm with different embeddings as <em>keys</em> and <em>values</em> so that the generative process is conditioned on the specified targets (challenge (3)). The problem setting of molecular generation can be formed as a text generation problem in natural language processing (NLP) where a generated molecule is a SMILES string, and we enforce conditional generation to enable target-specific generations.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We propose a conditional generative Transformer <em>cTransformer</em> by enforcing different <em>keys</em> and <em>values</em> of the multi-head attention for each target. We enable the generation of SMILES strings to be conditional on the specified target.</li>
<li>The proposed method is capable of generating both drug-like compounds (without specified targets) and target-specific compounds. We explore the generated chemical space by sampling and visualizing a large number of novel compounds. The sampled compounds largely occupy the real target-specific data's chemical space and also cover a significant fraction of novel compounds.</li>
<li>Our conditional generation aims to generate active molecules correspond to EGFR, S1PR1, and HTR1A target protein. We test the target-specific activity of the generated compounds using a computational model (QSAR) for revealing relationships between chemical compounds and biological activities.</li>
</ul>
<h2>II. PRELIMINARY</h2>
<h3>A. SMILES</h3>
<p>SMILES (Simplified Molecular Input Line Entry System) is a chemical notation that represents a chemical structure</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
by short ASCII strings. One chemical structure might map to different SMILES. There are many softwares that are able to validate a SMILES string and convert SMILES string to two-dimensional drawings. Figure 3 shows an example of a molecule with its chemical structure and two valid SMILES strings.</p>
<h2>B. Data examples</h2>
<p>Drug discovery aims to find novel compounds with specific chemical properties for the treatment of diseases. In addition to generating valid SMILES strings, we need to ensure the corresponding compound is bind to target proteins. We consider the target protein information as an annotated label of each compound. We propose to pre-train on SMILES-only dataset
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 3. Ring structures are written by breaking each ring at an arbitrary point to make an acyclic structure and adding numerical ring closure labels to show connectivity between non-adjacent atoms. By choosing different starting points, a molecule might generate different SMILES strings.
and fine-tune on <compound, target protein> pairs with the compound represented as SMILES string.
(a) Pre-training data: As shown in Table I-(a), we are able to learn the patterns of a valid drug-like SMILES sequence from the pre-training data.
(b) Fine-tune data: As shown in Table I-(b), the chemical structures are manually annotated with different target proteins.</p>
<h2>C. Problem Definition</h2>
<p>Drug Discovery. The overall idea of drug discovery is to discover new candidate medications [10]. In the small molecule drug discovery process [11], the first step is discovering molecular compounds that potentially have beneficial effects against any diseases. One of the common practices is to screen largesize compound libraries against isolated biological targets [12]. An intuitive idea is to search for novel molecule candidates in silico in the whole drug-like chemical space. Such an idea is not feasible before deep learning due to the unbounded search space of synthetically feasible chemicals. The success of AI changes this line of work since deep generative models are able to learn the distribution of desirable molecules from training data and generate novel drug-like compounds [13].</p>
<p>AI in Molecular Design. Given a set of chemical structures of drugs $\mathcal{D}$, we aim to learn from $\mathcal{D}$ and generate a set of novel chemical structures $\mathcal{\mathcal { D }}$. $\mathcal{D}$ has to be valid drug-like structures</p>
<p>and perform well for certain types of metrics in quantitatively measuring the quality of the novel chemical structures.</p>
<p>Conditional Molecular Design. Appropriate tuning of binding selectivity is a primary objective in the molecular design, and we aim to support conditional generation of novel molecules to be active on the target protein. In the scope of this paper, we consider the target protein of the generated molecule and embed the target protein as a condition. Given a condition (e.g., target) $c$, we aim to generate a set of compounds $\hat{\mathcal{D}}_{c}$ that are more likely to be active on the target protein.</p>
<h3>II-D Transformer</h3>
<p>RNN-based methods such as seq2seq with attention have achieved great performance in sequential modeling (e.g., machine translation), but the recurring nature of RNN hinders its parallelization, thus making it hard to model long sequences effectively. The Transformer [14] is proposed to address sequential modeling by attention, which is suitable for parallelization and performs well for long input sequences [15], [16]. In addition to sequence-to-sequence modeling, Transformer also works well for decoder-only sequence transduction [17].</p>
<p>Many neural sequence transduction models consist of an encoder and a decoder. The encoder first takes a sequence of tokens $(x_{1},...,x_{m})$ and transforms them to a sequence of latent representations $\mathbf{z}=\left(z_{1},...,z_{m}\right)$ (e.g., memories). The decoder will generate an output sequence $\left(t_{1},...,t_{n}\right)$ one by one conditioning on $\mathbf{z}$. An intuitive way of sequential generation is in an auto-regressive manner [18], which means consuming all the previously generated tokens while generating the next one. While the traditional Transformer is in an encoder-decoder manner, we define our de novo SMILES generation task as a conditional generator and we use a decoder-only design. Nevertheless, we introduce the complete design of the Transformer to make this paper self-contained.</p>
<p>An attention mechanism mimics the process of querying a set of key-value pairs, where the output is a weighted sum over the values and each weight is based on the matching of the key and query. The multi-head attention projects the keys, values, and queries $h$ times and performs attention in parallel. The formal definition of Multi-head Attention is as follows</p>
<p>Multi-head Attention (MHA) We first define some annotations: query matrices $Q_{i}=Q W_{i}^{Q}$, key matrices $K_{i}=K W_{i}^{K}$, and value matrices $V_{i}=V W_{i}^{V}$ $(i=1,...,h)$.</p>
<p>$$
\begin{aligned}
O_{i} &amp; =\operatorname{Attention}\left(Q_{i}, K_{i}, V_{i}\right) \
&amp; =\operatorname{softmax}\left(\frac{Q_{i} K_{i}^{T}}{\sqrt{d_{k}}}\right) V_{i}
\end{aligned}
$$</p>
<p>$\operatorname{MultiHeadAttention}(Q, K, V)=\operatorname{CONCAT}\left(O_{1}, \ldots, O_{h}\right) W^{O}$
$W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }} \times d_{k}} W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{v}}$ are learnable parameters.</p>
<p>Encoder The encoder has a stack of identical layers. Each has two sub-layers: a multi-head attention component, followed by a feed-forward network; a residual connection is deployed around each of the two sub-layers, followed by layer normalization.</p>
<p>Decoder The decoder also has a stack of identical layers. Each has three sub-layers (two of them are the same as the encoder) including an extra sub-layer performing multi-head attention over the output (e.g., latent representations $\mathbf{z}$ ) of the encoder.</p>
<h2>III. Method</h2>
<p>We propose to use a decoder-only Transformer to generate molecules in SMILES format. The token-wise generation is in an auto-regressive manner; at each step, the decoder consumes the previously generated tokens as input while generating the next. The proposed model is pre-trained with a large-scale SMILES dataset to learn a parametric probabilistic distribution over the SMILES vocabulary space and ensure the generation is in compliance with the SMILES grammar (e.g., atom type, bond type, and size of molecules) (Section III-A). Then the conditional generation is enforced by feeding target-specific embeddings to the multi-head attention component of the Transformer (Section III-B).</p>
<h2>A. Unsupervised Pre-training</h2>
<p>Many deep learning tasks rely on supervised learning and human-labeled dataset. For instance, the sequence-tosequence [19] (or seq2seq) model has been enjoying massive success in many natural language processing applications, and the seq2seq models are usually trained end-to-end with a large number of training pairs (e.g., article-summary pairs for text summarization).</p>
<p>However, in the chemical space, given a large number of unlabeled data but limited labeled data, we form an unsupervised learning task for drug discovery to overcome the challenge of expensive and hard-to-manage human labeling. Instead of a seq2seq encoder-decoder model, we consider a decoder-only generative model and form our task as predicting the next token given previously generated tokens.</p>
<p>GPT [6] and GPT-2 [7] have gained great success in language generation. Specifically, GPT-2 learns natural language by predicting next word given the previous words. Inspired by the success of unsupervised Transformer on NLP (e.g., GPT [6] and GPT2 [7]), we propose to use the Transformerbased model for drug discovery. Since a Transformer-based model works well for natural language applications such as writing assistants and dialogue systems, we are optimistic about its capabilities of generating drug-like SMILES sequences.</p>
<p>We form our drug discovery task by taking advantage of SMILES sequential structure and transforming it as a sequential generation. As shown in Figure 4-(a), we formalize our task as predicting the next token given the previous tokens. The sequential dependencies are trained to mimic structures observed in the training set and follow SMILES grammar.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 4. (a) Pre-training and (b) Fine-tuning.</p>
<p>During auto-regressive generation, the sampling should be able to produce more variations that are not previously observed.</p>
<h2>B. Conditional Transformer</h2>
<p>Our decoder-only design is able to memorize drug-like structure from pre-training. By conditioning on pre-defined conditions, we are able to further confine the search space and sample drug-like structures with desired properties.</p>
<p>Our decoder learns a parametric probabilistic distribution over the SMILES vocabulary space conditioning on the conditions $c$ (we denote target as $c$ ) and previously generated tokens. At $i$-th step, our decoder produces a probabilistic distribution of the new token attentively on previously generated token embeddings $\left[e_{1}^{t}, \ldots, e_{i-1}^{t}\right]$ and the target-specific embedding $e^{c}$.</p>
<p>$$
\begin{aligned}
z^{(0)} &amp; =\left[e_{1}^{t} ; e_{2}^{t} ; \ldots ; e_{i-1}^{t}\right] \
\bar{z}^{(l-1)} &amp; =L N\left(z^{(l-1)}+M H A\left(z^{(l-1)}, z^{(l-1)}, z^{(l-1)}\right)\right) \
\bar{z}^{(l)} &amp; =L N\left(\bar{z}^{(l-1)}+M H A\left(\bar{z}^{(l-1)}, e^{c}, e^{c}\right)\right) \
z^{(l)} &amp; =L N\left(\bar{z}^{(l)}+F F N\left(\bar{z}^{(l)}\right)\right)
\end{aligned}
$$</p>
<p>$L N$ is layerNorm, $M H A$ is multi head attention, and $F F N$ is Feed-Forward Networks. Note that we use mask by multiplying masked positions with negative infinity to avoid attending on the masked positions. With attending on previous generated tokens, we maintain the structural consistency with the SMILES grammar; In other words, we make sure the generated sequence is drug-like based on the memorization in training.
Incorporating target information One of the major challenges in our task is to generate target-specific SMILE sequences. The model needs to not only memorize a valid druglike structure, but also be able to memorize and generate</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5. Target-specific conditional molecular generation.</p>
<p>target-specific information. The challenge lies in capturing target-specific information and further target-specific generation. We propose taking advantage of the Multi-head Attention in the Transformer decoder and imposing the target-specific embeddings to the keys and values of the attention operations. We denote our Transformer model with imposed conditional embeddings (e.g., target-specific embeddings) as cTransformer.</p>
<p>We draw inspiration from a Transformer-based encoder-decoder structure for sequence-to-sequence translation. Taking machine translation as an example, the Transformer encoder-decoder model performs very well for this task. Specifically, the encoder memorizes the input sentence and stores them in "memories", and the decoder first attends to the previously generated tokens (first mha ^{1}) and then performs multi-head attention over the output "memories" of the encoder (second mha). When attending to the "memories" of the encoder using second multi-head attention, the queries are from the first multi-head attention and values and keys from the "memories" of the encoder. The intuition is to enable the decoder to attend over the input sequence [14].</p>
<p>We enable the SMILES sequence generation to condition on the specific target by feeding target-specific embeddings (denoted as e^{c}) to a decoder-only Transformer (shown in Figure 4-(b)). We use target-specific embeddings as keys and values of second mha, which allows every position of the decoder to attend to the target-specific embeddings and ensures the subsequent token generations are conditioned on the target-specific embeddings. It is worth noticing that our target-specific design is orthogonal to the decoder and can be easily removed by setting the condition embeddings as zero embeddings.</p>
<p>As shown in Figure 4, instead of fetching memory from an encoder, our decoder-only design initializes the memory based on condition embeddings. For base model pre-training, e^{c} is initialized with zero embeddings; when targets are involved, e^{c} is initialized with target-specific embeddings.</p>
<h3>C. Workflow</h3>
<p>As in Figure 4, the training process of our task can be summarized as follows:</p>
<p>1) We first pre-train the base model of cTransformer by setting the target-specific embeddings as zero embeddings (following Figure 4-(a) without feeding target-specific information) using data from Table I-(a). We do not have any target constraint on the sequential generation and solely focus on learning the drug-like structure from the data.</p>
<p>2) To feed target-specific information, we fine-tune cTransformer with <compound, target> pairs and enforce conditions of the corresponding target by feeding target-specific embeddings to the attention layer as "memories" (as shown in Figure 4-(b)). We use data from Table I-(b) where each SMILES sequence is manually tagged with a target (e.g., target proteins) indicating the specific physicochemical property of the small molecule.</p>
<p>3) We can generate drug-like structure by auto-regressively sampling tokens from the trained decoder (following Figure 5). Optionally, we can enforce the desired target by feeding a target-specific embedding. The new generation will condition the target-specific information and likely has the desired property.</p>
<h3>D. Likelihood of molecular sequential generation</h3>
<p>An intuitive idea of likelihood estimation of SMILES string sampling is using Negative Log-Likelihood (NLL) loss. We propose a conditional design by enforcing values and keys of multi-head attention to be the generative condition (denoted as c). Overall, our conditional NLL is as follows with the initial state set to c.</p>
<p>$$NLL(S|c) = -\left\lfloor \ln P(t_1|c) + \sum_{i=2}^{N} \ln P(t_i | t_{1:i-1}, c) \right\rfloor$$</p>
<p>Where c represents the generative condition (e.g., target protein), S is a SMILES sequence with length N, and t_{i} is the i-th token.</p>
<h2>IV. RELATED WORK</h2>
<h3>A. Deep Learning-based Molecular Design</h3>
<p>Computational chemistry has reduced the experimental efforts of molecular design and overcome the experimental limitations [20]–[23]. With the emerging applications of deep</p>
<p>learning, deep learning leads to a promising direction for drug discovery.</p>
<p>SMILES string is a popular representation of chemical structure in deep learning-based drug discovery. Recurrent Neural Network (RNN)-based generative models have been extensively tested in molecular design [24]–[27]. [28] samples an increased number of chemical structures by learning from a limited set of SMILES strings using Recurrent Neural Network (RNN). [27] further fine-tunes an RNN with a small set of SMILES strings, which are active against a specific biological target. Reinforcement learning can also further [29], [30] confine the chemical space for specific properties.</p>
<p>Autoencoder is also applied in molecule generation [9], [31]. A variational autoencoder can further generate molecules with specific properties by taking concatenated SMILES strings with the property of interests [32]. Our generation does not involve an encoder as we sample directly from the molecular structure memorized by the sequential decoder. RNNs can be further modified by setting the interval states of RNN cells (e.g., LSTM) to produce SMILES string with specific target properties [33]. [34] proposes to modify an existing SMILES string based on the chemists’ intention by prepending the desired changes to the original SMILES string. They test on both Transformer [14] and RNN seq2seq structure by inputting a SMILES string along with the changed properties and outputting a SMILES string with the potentially desired properties. In addition to variational autoencoder [32] and conditional recurrent neural networks [33], conditional graph generative models [35] also generate molecular graphs instead of SMILES with specified conditions, and the conditional representation is added as an additional term to the hidden state of each layer.</p>
<h3>III-B Linguistic Models</h3>
<p>Recurrent Neural Networks (RNNs) such as Gated Recurrent Unit (GRU) [36] and Long Short-Term Memory (LSTM) [37] have successfully addressed a number of sequential modeling tasks and especially in language models. The idea of capturing temporal dependencies using RNN is by imposing connections to hidden units with a time delay to retain information about the past. Language models also benefit from RNN since we can treat a sentence of $m$ words as a sequence of $m$ time steps. In our sequential generation task, we need to memorize the previously generated tokens, also called context, to produce meaningful subsequent tokens. To be specific, the prediction of the token at time step $t$ depends on the previous tokens generated during $t^{\prime}&lt;t$. RNN has an internal state for “memorizing” the context, which is updated by backpropagation every time step to reflect the context change. One of the drawbacks of recurrent structure is the difficulties in parallelization, which becomes more critical with longer input sequences. The idea of RNN also suffers from practical difficulties since the training of deep learning benefits from gradient descent, which becomes inefficient when the learning spans a long sequence of time steps [38]. In the task of generating SMILES sequences, the output sequence can expand to long sequences (e.g., the SMILES representation of Remdesivir has 92 characters). RNN also suffers from the vanishing, and exploding gradient problems [38], [39] during training as the long-term gradients via back-propagation will tend to zero or infinity. Long short-term memory (LSTM) [37] is proposed to address the gradient problem but to some extend. In language models, the usage of RNN restricted the prediction to a short range while Transformer performs well in long-range predictions. Existing literature also experimentally ascertains that Transformer-based language models outperform LSTM in many tasks [6], [17].</p>
<p>Sequence-to-sequence generative model The sequence-tosequence [19] model (or seq2seq as it is commonly referred to) takes input in the form of a sequence of tokens and produces output also as a sequence. Such a model has been enjoying massive success in many natural language processing applications. The seq2seq modeling has been applied to many language tasks, such as text summarization [40] and neural machine translation [41], [42], and the seq2seq models are trained end-to-end with a large number of training pairs (e.g., article-summary pairs for text summarization).</p>
<p>Notable techniques in sequential modeling include attention mechanism [43] and pointer network [44], which are designed for handling long sequences and rare tokens. Many successful applications can be seen in language modeling [45], text summarization [46], text understanding [47], and neural computing [48]. Specifically, Transformer performs better than the standard LSTM encoder-decoder with attention in generating abstractive sections [17].</p>
<h3>III-C Generative Pre-trained Transformer</h3>
<p>A Generative Pre-trained Transformer (GPT) [6] is proposed for natural language processing (NLP) using Transformer and unsupervised learning. The intuitive idea is to train a model with a very large amount of data for natural language modeling in an unsupervised way and then fine-tune the pre-trained model with relatively small-sized supervised data for specific downstream tasks. The unsupervised pre-training is able to prime the model with drug-like knowledge and enforce valid SMILES strings.</p>
<p>GPT-2 [7], a successor of GPT [6], is a Transformer-based model that learns natural language by predicting next word given the previous words. GPT-2 is trained from a large text corpus using unsupervised learning. The well-trained GPT-2 is able to generate synthetic text excerpts conditioning on an arbitrary input, specifically the model generates a continuation of the input in the form of natural language. The synthetic text samples are conditioned on the input and adaptive to the style and content of the input. The benefits of the GPT-2 design are the capability of generating text (1) coherent with the input and (2) similar to human writing. GPT-2 has shown great capabilities of generating reasonable reviews by training on Amazon Reviews dataset conditioning on ratings and categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Frag $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SNN $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Valid $\uparrow$</td>
<td style="text-align: center;">Unique@ $1 \mathrm{k} \uparrow$</td>
<td style="text-align: center;">Unique@10k $\uparrow$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">TestSF</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">TestSF</td>
</tr>
<tr>
<td style="text-align: center;">HMM</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: center;">NGram</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Combinatorial</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.439</td>
</tr>
<tr>
<td style="text-align: center;">CharRNN</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.565</td>
</tr>
<tr>
<td style="text-align: center;">AAE</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.568</td>
</tr>
<tr>
<td style="text-align: center;">VAE</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.578</td>
</tr>
<tr>
<td style="text-align: center;">JTN-VAE</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.519</td>
</tr>
<tr>
<td style="text-align: center;">LatentGAN</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.514</td>
</tr>
<tr>
<td style="text-align: center;">cTransformer ${ }^{\text {a }}$</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.578</td>
</tr>
</tbody>
</table>
<p>TABLE II
Performance Metrics for baseline models: Fraction of VALID MOLECULES, Fraction of unique MOLECULES from 1,000 and 10,000 MOLECULES. Fragment SIMILARITY (FRAG) AND SIMILARITY TO A NEAREST NEIGHBOR (SNN) - RESULTS FOR RANDOM TEST SET (TEST) AND SCAFFOLD SPLIT TEST SET (TESTSF). ${ }^{\text {a }}$ MEANS THE BASE MODEL OF cTransformer.</p>
<h2>V. EXPERIMENT</h2>
<p>We evaluate the performance of base generative model in Section V-A and the performance of conditional generations in Section III-B.</p>
<h2>A. Base generative model</h2>
<p>1) Dataset: We use the MOSES molecular dataset from Polykovskiy [9] to perform unsupervised pre-training on our cTransformer with the target-specific embedding initialized as zero. It contains 1,760,739 drug molecules extracted from ZINC clean Lead Collection [49], including 1,584,664 training molecules and 176,075 testing molecules. We follow the same train/test split as in [9].
2) Evaluation: In this section, we propose to use a set of metrics to quantitatively measure our generative model. We evaluate the generated compounds in various aspects of molecule generation proposed in [9]. Besides basic metrics such as chemical validity and diversity, we compare the distribution of drug-likeness properties between generated and real compounds.</p>
<p>The performance is reported on 30,000 molecules generated from each generative model. We compute all metrics (except for validity) only for valid molecules from the generated set. We generate 30,000 molecules and use valid molecules from this set. We use the following metrics:</p>
<ul>
<li>Fraction of valid (Valid) and unique molecules report valid and unique SMILES strings. The validity is checked using a molecular structure parser (RDKit).</li>
<li>Unique@1K and Unique@10K for the first 1000 and 10000 valid molecules in the generated set. In general, validity measures whether the model captures enough chemical constraints (e.g., valence); Uniqueness measures whether the model overlaps with trained molecules.</li>
<li>Fragment similarity (Frag) compares distributions of BRICS fragments [50] in generated and reference sets. This metric measures how similar are the scaffolds present in generated and reference datasets.</li>
<li>Similarity to a nearest neighbor (SNN) calculates the average Tanimoto similarity (also known as the Jaccard
index) between fingerprints of a molecule from the generated set and its nearest molecule in the reference dataset based on [51] [52].</li>
</ul>
<p>We compare our method with the following baselines:</p>
<ul>
<li>Hidden Markov Model (HMM) uses Baum-Welch algorithm to learn a probabilistic distribution over the SMILES strings, then samples the next token and state from learned probabilities.</li>
<li>N-gram generative model (NGram) [9] collects statistics of n-grams frequencies in the training set and uses such a distribution to sample strings.</li>
<li>Combinatorial generator (Combinatorial) [9] splits molecules into fragments and generates new molecules by random connections. The sampling of fragments is based on training data frequencies.</li>
<li>Character-level recurrent neural network (CharRNN) [27] learns the probability distribution of the next token over the vocabulary space conditioning on previously generated tokens using RNN.</li>
<li>Variational autoencoder (VAE) [53] encodes SMILES strings to the latent space, then reconstructs the strings from the latent codes with a decoder. New SMILES strings can be sampled from the latent space.</li>
<li>Adversarial Autoencoder (AAE) [54] replaces the objective of VAE with an adversarial objective, which enables relaxed assumptions of the prior distribution.</li>
<li>Junction Tree VAE (JTN-VAE) [4] generates molecules by first generating a tree-structured object and further assembled into a molecule.</li>
<li>Latent Vector Based Generative Adversarial Network (LatentGAN) [55] pre-trains an autoencoder to map SMILES into the latent space and mapped back to SMILES, then uses a generative adversarial network to produce the latent code.</li>
</ul>
<p>The performance of the different approaches is summarized in Table II. Our method (base model of cTransformer) achieves state-of-the-art results in the Fraction of valid (Valid), Unique@1k, Unique@10k, Fragment similarity, and Similarity to the nearest neighbor.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 6. Distribution of chemical properties for MOSES dataset and sets of generated molecules. Wasserstein-1 distance to MOSES test set is denoted in parenthesis. We cover Molecular Weight, LogP, Synthetic Accessibility, and QED.</p>
<p>We also compare the distribution of chemical properties for MOSES dataset and sets of generated molecules using the following chemical properties:</p>
<ul>
<li><strong>Molecular weight (MW)</strong> is the sum of atomic weights in a molecule.</li>
<li><strong>LogP</strong> is the octanol-water partition coefficient computed by RDKit's Crippen [56] estimation.</li>
<li><strong>Synthetic Accessibility Score (SA)</strong> estimates the ease of synthesis (synthetic accessibility) of drug-like molecules based on molecular complexity and fragment contributions [57].</li>
<li><strong>Quantitative Estimation of Drug-likeness (QED)</strong> measures the drug-likeness based on desirability in a value between zero (all properties unfavorable) to one (all properties favorable) [58].</li>
</ul>
<p>The resultant distributions of four molecular properties in generated and test datasets are shown in Figure 6. Our model closely matches the real data distribution. This validates that our method is capable of generating drug-like molecules.</p>
<h2><em>B. Target-specific molecular generation</em></h2>
<p><em>1) Dataset:</em> We download the target-specific molecular dataset from [55] for training our target-specific <em>cTransformer</em>, which contains 1,381,795, and 3,485 active molecules corresponding to EGFR, S1PR1, and HTR1A target protein, respectively.</p>
<p><em>2) ML-based QSAR model for active scoring:</em> To evaluate the capability of generating active target-specific compounds, we build a regression-based QSAR (Quantitative structure-activity relationship) model for each target. The molecular set with activity data for each target is acquired from ExCAPE-DB [59], which includes 5,181, 6,332, and 1,400 molecules corresponding to EGFR, HTR1A, and S1PR1 target proteins, respectively. We train a LightGBM model to predict activity on 2,533 molecular features, including a 2048-length FCFP6 fingerprint, 166-length MACCSkeys, and 319 RDKit Molecular Descriptors. We report Pearson correlation and RMSE on</p>
<table>
<thead>
<tr>
<th>Target</th>
<th># of active mols</th>
<th># of mols</th>
<th>Activity</th>
<th>QSAR</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>R</td>
</tr>
<tr>
<td>EGFR</td>
<td>1,381</td>
<td>5,181</td>
<td>6.29 ± 1.39</td>
<td>0.843</td>
</tr>
<tr>
<td>HTR1A</td>
<td>3,485</td>
<td>6,332</td>
<td>7.33 ± 1.23</td>
<td>0.763</td>
</tr>
<tr>
<td>S1PR1</td>
<td>795</td>
<td>1,400</td>
<td>7.25 ± 1.58</td>
<td>0.825</td>
</tr>
</tbody>
</table>
<p>TABLE III</p>
<p><strong>TARGET DATA SETS AND THE PERFORMANCE OF THE QSAR MODELS.</strong> THE ACTIVE COMPOUNDS ARE USED FOR TRAINING TARGET-SPECIFIC GENERATIVE MODELS. QSAR MODELS ARE TRAINED ON BOTH ACTIVE AND NON-ACTIVE COMPOUNDS FOR EACH TARGET. R STANDS FOR PEARSON CORRELATION. RMSE STANDS FOR ROOT MEAN SQUARE ERROR.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 7. Distribution of predicted activity (pXC50) of the top 1000 (left) / 2000(middle) / 5000(right) compounds from cTransformer(orange) and cRNN(blue) for EGFR(top), HTR1A(middle) and S1PR1 (bottom) targets.</p>
<table>
<thead>
<tr>
<th>Target</th>
<th>Model</th>
<th>Valid†</th>
<th>Unique@10k†</th>
<th>Novel †</th>
</tr>
</thead>
<tbody>
<tr>
<td>EGFR</td>
<td>cRNN</td>
<td>0.921</td>
<td>0.861</td>
<td>0.662</td>
</tr>
<tr>
<td></td>
<td>cTransformer</td>
<td>0.885</td>
<td>0.940</td>
<td>0.898</td>
</tr>
<tr>
<td>HTR1A</td>
<td>cRNN</td>
<td>0.922</td>
<td>0.844</td>
<td>0.498</td>
</tr>
<tr>
<td></td>
<td>cTransformer</td>
<td>0.905</td>
<td>0.896</td>
<td>0.787</td>
</tr>
<tr>
<td>S1PR1</td>
<td>cRNN</td>
<td>0.926</td>
<td>0.861</td>
<td>0.514</td>
</tr>
<tr>
<td></td>
<td>cTransformer</td>
<td>0.926</td>
<td>0.838</td>
<td>0.684</td>
</tr>
</tbody>
</table>
<p>TABLE IV</p>
<p>EVALUATION METRICS: FRACTION OF VALID MOLECULES, FRACTION OF UNIQUE MOLECULES FROM 10,000 MOLECULES, AND NOVELTY (FRACTION OF MOLECULES NOT PRESENT IN THE TRAINING SET)</p>
<p>test dataset in table III. All the testing Pearson correlations of QSAR models are higher than 0.75, demonstrating that our QSAR models are capable of benchmarking the activity of the generated compounds.</p>
<p>3) Evaluation: To benchmark the performance of the targeted models, we built a conditional RNN model (denoted as cRNN) by first training a base RNN model on the same MOSES set and fine-tuning it on the target set.</p>
<p>We sampled 30,000 compounds from the cTransformer and cRNN model and reported the metric in the table IV. Results demonstrate that the validity of all the cases was above 88%, and the uniqueness of 10k valid compounds was 94%, 90%, and 83% for EGFR, HTR1A, and S1PR1, respectively, and are higher than the values of cRNN except for S1PR1. Moreover, in terms of novelty, the values are 90%, 78%, and 68% for EGFR, HTR1A, and S1PR1 respectively, which significantly outperforms the cRNN model. This shows that cTransformer is not only able to generate valid compounds but also design novel molecules, which is critical for de novo drug design. Furthermore, we use QSAR models to predict the activity of all the generated valid compounds. We rank the compounds</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 8. UMAP of the generated target-specific molecules (dark color) by the cTransformer and the ground-true target-specific molecules (light color).</p>
<p>Based on predicted value and plot the activity distribution of the top 1000/2000/5000 most active ones in Figure 7. Results show that the distributions of predicted activity of compounds from cTransformer are significantly better than those from cRNN for all three targets. This highlights that cTransformer is more capable of generating active compounds than cRNN. Additionally, we show the top 24 most active molecules in Figure 9.</p>
<p>Moreover, we evaluate the capability of generating target-specific compounds by visualizing the chemical space. The hypothesis is that the compounds that can potentially interact with the same protein target would populate the same subchemical space. To evaluate the overlapping of chemical space, we select top predicted active 5000 compounds for each target, then the 1024-bit FCFP6 fingerprint vectors are calculated for the generated compounds and the real compounds from the training dataset (the training dataset here refers to the fine-tuning dataset in Table I-(b)). We use the Uniform Manifold Approximation and Projection (UMAP) to construct 2D projections. These projections are illustrated in Figure 8. Each point corresponds to a molecule and is colored according to its target label. The dark and light colors represent the generated compounds and training set compounds, respectively. The visualization of chemical space in Figure 8 demonstrates the generated target-specific molecules (dark color) and real target-specific molecules (light color) occupy the same subchemical space. These results show that our cTransformer can generate compounds that are similar to the ones in the training set but are still novel structures.</p>
<h1>VI. CONCLUSION AND FUTURE WORK</h1>
<p>In this study, we first present a Transformer-based random molecular generator and compare it with a number of baseline models using standard metrics. We demonstrate that the Transformer-based molecular generation achieves state-of-the-art performances in generating drug-like structures. To incorporate the protein information, we present a target-specific molecular generator by feeding the target-specific embeddings to a Transformer decoder. We apply the method on three target-biased datasets (EGFR, HTR1A, and S1PR1) to evaluate the capability of the cTransformer to generate target-specific compounds and compare it with conditional RNN. Our results demonstrate that the sampled compounds from the model are predicted to be more active than cRNN in all three targets. Additionally, we visualize the chemical space, and the generated novel target-specific compounds largely populate the original sub-chemical space. In summary, these experimental results demonstrate that our cTransformer can be a valuable tool for de novo drug design.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>HTR1A
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>S1PR1
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 9. The top 24 most active generated compounds for each target protein.</p>
<h2>REFERENCES</h2>
<p>[1] Gisbert Schneider and Uli Fechner. Computer-based de novo design of drug-like molecules. Nature Reviews Drug Discovery, 4(8):649-663, 2005.
[2] Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, et al. Pubchem substance and compound databases. Nucleic acids research, 44(D1):D1202-D1213, 2016.
[3] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.
[4] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 2323-2332. PMLR, 2018.
[5] Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy, Anastasiya V Aladinskaya, Victor A Terentiev, Daniil A Polykovskiy, Maksim D Kuznetsov, Arip Asadulaev, et al. Deep learning enables rapid identification of potent ddr1 kinase inhibitors. Nature biotechnology, 37(9):1038-1040, 2019.
[6] Alec Radford, Kurthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[7] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[9] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11:565644, 2020.
[10] Drug discovery. https://en.wikipedia.org/wiki/Drug_discovery, September 2022.
[11] The drug development process. https://www.fda.gov/patients/ learn-about-drug-and-device-approvals/drug-development-process, September 2022.
[12] Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.
[13] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3d generative model for structure-based drug design. Advances in Neural Information Processing Systems, 34:6229-6239, 2021.
[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[15] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, YuXiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32, 2019.
[16] Chen Jiang, Jingjing Li, Wenlu Wang, and Wei-Shinn Ku. Modeling real estate dynamics using temporal encoding. In Proceedings of the 29th International Conference on Advances in Geographic Information Systems, pages 516-525, 2021.
[17] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
[18] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[19] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112, 2014.
[20] Jean-Louis Reymond, Ruud Van Deursen, Lorenz C Blum, and Lars Ruddigkeit. Chemical space as a source for new drugs. MedChemComm, $1(1): 30-38,2010$.
[21] Tiejun Cheng, Qingliang Li, Zhigang Zhou, Yanli Wang, and Stephen H Bryant. Structure-based virtual screening for drug discovery: a problemcentric review. The AAPS journal, 14(1):133-141, 2012.
[22] Thomas Scior, Andreas Bender, Gary Tresadern, José L Medina-Franco, Karina Martínez-Mayorga, Thierry Langer, Karina Cuanalo-Contreras, and Dimitris K Agrafiotis. Recognizing pitfalls in virtual screening: a critical review. Journal of chemical information and modeling, 52(4):867-881, 2012.
[23] Brian K Shoichet. Virtual screening of chemical libraries. Nature, 432(7019):862-865, 2004.
[24] William Yuan, Dadi Jiang, Dhanya K Nambiar, Lydia P Liew, Michael P Hay, Joshua Bloomstein, Peter Lu, Brandon Turner, Quynh-Thu Le, Robert Tibshirani, et al. Chemical space mimicry for drug discovery. Journal of chemical information and modeling, 57(4):875-882, 2017.
[25] Esben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks (rnns). arXiv preprint arXiv:1705.04612, 2017.
[26] Anvita Gupta, Alex T Müller, Berend JH Huisman, Jens A Fuchs, Petra Schneider, and Gisbert Schneider. Generative recurrent networks for de novo drug design. Molecular informatics, 37(1-2):1700111, 2018.
[27] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120-131, 2018.
[28] Josep Arús-Pous, Thomas Blaschke, Silas Ulander, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. Exploring the gdb-13 chemical space using deep generative models. Journal of cheminformatics, 11(1):1-14, 2019.
[29] Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018.
[30] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):1-14, 2017.
[31] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096-1108, 2019.
[32] Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based on conditional variational autoencoder for de novo molecular design. Journal of cheminformatics, 10(1):1-9, 2018.
[33] Panagiotis-Christos Kotsias, Josep Arús-Pous, Hongming Chen, Ola Engkvist, Christian Tyrchan, and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nature Machine Intelligence, 2(5):254-265, 2020.
[34] Jiazhen He, Huifang You, Emil Sandström, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist's intuition using deep neural networks. Journal of cheminformatics, 13(1):1-17, 2021.
[35] Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of cheminformatics, 10(1):1-24, 2018.
[36] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
[37] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.
[38] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
[39] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. PMLR, 2013.
[40] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Lluís Márquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 1721, 2015, pages 379-389. The Association for Computational Linguistics, 2015.
[41] Drmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 79, 2015, Conference Track Proceedings, 2015.</p>
<p>[42] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[43] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.
[44] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pages 2692-2700, 2015.
[45] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In ICLR, 2017.
[46] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of $A C L$, volume 1, pages 1631-1640, 2016.
[47] Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić, Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. In Proceedings of EACL, volume 1, pages 438-449, 2017.
[48] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.
[49] Teague Sterling and John J Irwin. Zinc 15-ligand discovery for everyone. Journal of chemical information and modeling, 55(11):23242337, 2015.
[50] Jörg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey. On the art of compiling and using'drug-like'chemical frag-
ment spaces. ChemMedChem: Chemistry Enabling Drug Discovery, 3(10):1503-1507, 2008.
[51] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754, 2010.
[52] Rdkit: Open-source cheminformatics software. http://www.rdkit.org/, September 2022.
[53] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[54] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
[55] Oleksii Prykhodko, Simon Viet Johansson, Panagiotis-Christos Kotsias, Josep Arús-Pous, Esben Jannik Bjerrum, Ola Engkvist, and Hongming Chen. A de novo molecular generation method using latent vector based generative adversarial network. Journal of Cheminformatics, 11(1):1-13, 2019.
[56] Scott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic contributions. Journal of chemical information and computer sciences, 39(5):868-873, 1999.
[57] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):1-11, 2009.
[58] G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90-98, 2012.
[59] Jiangming Sun, Nina Jeliazkova, Vladimir Chupakhin, Jose-Felipe Golib-Dzib, Ola Engkvist, Lars Carlsson, Jörg Wegner, Hugo Ceulemans, Ivan Georgiev, Vedrin Jeliazkov, et al. Excape-db: an integrated large scale dataset facilitating big data analysis in chemogenomics. Journal of cheminformatics, 9(1):1-9, 2017.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\S}$ Equal contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
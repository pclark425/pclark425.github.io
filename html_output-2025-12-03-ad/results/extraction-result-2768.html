<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2768 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2768</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2768</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff" target="_blank">Neuro-Symbolic Reinforcement Learning with First-Order Logic</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2768.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2768.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOL-LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First-Order Logic - Logical Neural Network (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic RL agent that converts textual observations and observation history into first-order logical facts, and trains an interpretable Logical Neural Network (AND-OR LNN) policy using those facts and rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FOL-LNN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pipeline: (1) FOL converter that parses current observation and observation history into propositional and first-order logical facts (uses ConceptNet to find word classes), (2) Logical Neural Network (LNN) with conjunction (AND) nodes feeding a disjunction (OR) node representing actions; weights are trained end-to-end with a DQN-style replay and reward signal. Not based on a large pre-trained language model (uses semantic parser + ConceptNet).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A text-based navigation/collection game where the agent receives room descriptions and must navigate connected rooms to find and take a coin; actions are two-word commands (verb, noun) like go north or take coin. Difficulty levels differ by presence and number of distractor (dead-end) rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / external knowledge / experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Multiple memory components: (a) a sequential observation history used to produce boolean FOL predicates (visited flags, initial direction), (b) an experience replay buffer storing tuples (o_t, a_t, r_t, o_{t+1}), and (c) ConceptNet as an external semantic graph for word classes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past observations (used to compute visited/unvisited and initial direction flags), action history (stored in replay buffer tuples), rewards, and ConceptNet word-class relations (e.g., direction vs. money types).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Replay buffer capacity explicitly 500,000 (prioritized replay); observation-history capacity not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>History and current observation are queried by the FOL converter to extract logical predicates; ConceptNet queried to get word classes. Experience replay samples mini-batches (batch size = 4) from prioritized replay for training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>At every time step the agent appends (o_t, a_t, r_t, o_{t+1}) to the prioritized replay buffer; the observation-history used to compute visited/initial flags is updated each step as the agent visits rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To convert history into symbolic facts (e.g., visited(x), initial(x)) feeding into LNN to form decision rules (avoid revisiting, return from dead-ends), to support learning via replay (stabilize RL updates), and to provide semantic typing of words via ConceptNet for selecting appropriate LNN predicate sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>FOL-LNN (using history-derived FOL facts + ConceptNet + replay) achieved strong generalization on 50 unseen TextWorld coin-collector games: example metrics (moving average N=100, 5 seeds): Easy: reward 0.95 / 19.0 steps (epoch 100) -> 1.00 / 15.0 (epoch 1000); Medium: reward 0.94 / 32.7 steps (epoch 100) -> 1.00 / 28.6 (epoch 2000); Hard: reward 0.95 / 44.8 steps (epoch 100) -> 1.00 / 42.0 (epoch 2000). (See Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper attributes faster convergence and better generalization to using logical facts extracted from observation history (visited/unvisited, initial) together with ConceptNet typing: these history-derived predicates allow the LNN to learn compact, human-readable rules such as 'if find direction x and not visited(x) and not initial(x) -> go x' and a rule for returning from dead-ends, which improve decision-making and reduce unnecessary steps. They also report that methods using logical inputs (which include history-derived predicates) outperform agents trained on raw textual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit limitations of the memory components are reported (observation-history capacity and potential clutter are not discussed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2768.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2768.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-based DQN++ (baseline neuro-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural baseline that encodes observations with an LSTM and uses DQN-style action scoring; used as a neuro-only baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for textbased games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN++</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neuro-only agent: LSTM encoder over textual observations combined with a DQN action scorer (as in Narasimhan et al., 2015). The LSTM provides a recurrent hidden-state memory over time.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same Coin-Collector TextWorld tasks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden-state (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential recurrent hidden state held in LSTM cells</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Implicit compressed representation of recent observations in LSTM hidden state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Hidden state updated each time step by LSTM recurrence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To provide temporal context for decision-making from sequences of textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline performance (moving average): Easy at epoch100: 0.07 reward / 93.4 steps; epoch1000: 0.12 / 88.6; Medium and Hard difficulties show near-zero rewards and very high step counts over epochs (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>While LSTM encoders are used in baseline agents, the paper reports that agents using logical inputs (including history-derived predicates) outperform neuro-only agents with LSTM encoders; this suggests that LSTM hidden-state memory alone did not achieve the same generalization in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>The paper does not analyze internal LSTM memory failure modes in detail; it only reports inferior empirical performance compared to FOL-LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2768.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2768.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-Deep Recurrent Q-Network (memory-augmented action scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant that augments the action scorer with memory units (DRQN-style recurrence) to handle partial observability; cited as prior work that adds memory in the action scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counting to explore and generalize in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned prior agent that extends DQN with recurrent/memory units in the action scorer (Yuan et al., 2018); not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector) [context of mention]</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based games; DRQN variants are used to help with partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent memory (DRQN-style)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Recurrent hidden-state within the action scorer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Compressed temporal context from past observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Recurrent update each time step</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Cited as an approach that adds memory to the action scorer to better handle partial observability; paper does not evaluate it directly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Only mentioned in related work as an example of adding memory units; no experimental or analytic findings about its effectiveness are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2768.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2768.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Replay buffer (prioritized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Experience Replay Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experience memory storing recent (o_t, a_t, r_t, o_{t+1}) tuples used for off-policy DQN-style updates; prioritized sampling used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Prioritized Replay Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard prioritized replay memory used by the RL training loop: capacity 500,000, priority fraction 0.25, mini-batch updates every 4 steps, prioritized sampling for training.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Used as the experience memory for training agents on the Coin-Collector task.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay (off-policy episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Buffer storing tuples (o_t, a_t, r_t, o_{t+1}); prioritized sampling mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past transitions, actions, rewards, next observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>500,000 transitions (explicitly stated)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Prioritized sampling with priority fraction 0.25 for minibatch selection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Append each transition; sampling for training every 4 steps; prioritized replay priorities updated per usual mechanisms (paper references standard prioritized replay but does not detail exact update rule).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Stabilize learning by replaying past experiences for gradient updates (DQN-style training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Replay buffer is part of standard training; the paper does not ablate replay vs no-replay, but uses it across all methods, so no direct conclusions specific to replay memory are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2768.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2768.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet (used as external knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet - commonsense knowledge network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external semantic network used to classify words (e.g., 'north' as direction, 'coin' as money) so that the FOL converter can create typed predicates and select appropriate LNN modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptnet - a practical commonsense reasoning tool-kit</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ConceptNet (external semantic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-structured commonsense knowledge resource queried by the FOL converter to get class/type of words present in observations; not a trainable model in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Used to determine word classes (direction vs money) to convert propositional logics to typed FOL predicates for the LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic/external knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph database of word relations and classes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Word classes/types and commonsense relations (used to type nouns into categories such as W_direction, W_money)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Lookup by word to retrieve class/type from ConceptNet</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Static external resource (not updated during training in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To assign types to observed nouns so the FOL converter can generate appropriate predicates and select corresponding LNN predicate sets (enables generalization via type-based rules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Using ConceptNet to type nouns supports compact FOL rules (e.g., quantification over W_direction or W_money) that generalize across vocabulary and enable interpretable rules; the paper credits semantic typing as a component of the FOL conversion beneficial for generalization, but does not present a direct ablation isolating ConceptNet's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No limitations discussed for ConceptNet usage in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
                <li>Language understanding for textbased games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Neural logic machines <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 1)</em></li>
                <li>LeDeepChef deep reinforcement learning agent for families of text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2768",
    "paper_id": "paper-0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "FOL-LNN",
            "name_full": "First-Order Logic - Logical Neural Network (proposed)",
            "brief_description": "A neuro-symbolic RL agent that converts textual observations and observation history into first-order logical facts, and trains an interpretable Logical Neural Network (AND-OR LNN) policy using those facts and rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FOL-LNN",
            "agent_description": "Pipeline: (1) FOL converter that parses current observation and observation history into propositional and first-order logical facts (uses ConceptNet to find word classes), (2) Logical Neural Network (LNN) with conjunction (AND) nodes feeding a disjunction (OR) node representing actions; weights are trained end-to-end with a DQN-style replay and reward signal. Not based on a large pre-trained language model (uses semantic parser + ConceptNet).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector)",
            "game_description": "A text-based navigation/collection game where the agent receives room descriptions and must navigate connected rooms to find and take a coin; actions are two-word commands (verb, noun) like go north or take coin. Difficulty levels differ by presence and number of distractor (dead-end) rooms.",
            "uses_memory": true,
            "memory_type": "episodic / external knowledge / experience replay",
            "memory_structure": "Multiple memory components: (a) a sequential observation history used to produce boolean FOL predicates (visited flags, initial direction), (b) an experience replay buffer storing tuples (o_t, a_t, r_t, o_{t+1}), and (c) ConceptNet as an external semantic graph for word classes.",
            "memory_content": "Past observations (used to compute visited/unvisited and initial direction flags), action history (stored in replay buffer tuples), rewards, and ConceptNet word-class relations (e.g., direction vs. money types).",
            "memory_capacity": "Replay buffer capacity explicitly 500,000 (prioritized replay); observation-history capacity not specified.",
            "memory_retrieval_strategy": "History and current observation are queried by the FOL converter to extract logical predicates; ConceptNet queried to get word classes. Experience replay samples mini-batches (batch size = 4) from prioritized replay for training.",
            "memory_update_strategy": "At every time step the agent appends (o_t, a_t, r_t, o_{t+1}) to the prioritized replay buffer; the observation-history used to compute visited/initial flags is updated each step as the agent visits rooms.",
            "memory_usage_purpose": "To convert history into symbolic facts (e.g., visited(x), initial(x)) feeding into LNN to form decision rules (avoid revisiting, return from dead-ends), to support learning via replay (stabilize RL updates), and to provide semantic typing of words via ConceptNet for selecting appropriate LNN predicate sets.",
            "performance_with_memory": "FOL-LNN (using history-derived FOL facts + ConceptNet + replay) achieved strong generalization on 50 unseen TextWorld coin-collector games: example metrics (moving average N=100, 5 seeds): Easy: reward 0.95 / 19.0 steps (epoch 100) -&gt; 1.00 / 15.0 (epoch 1000); Medium: reward 0.94 / 32.7 steps (epoch 100) -&gt; 1.00 / 28.6 (epoch 2000); Hard: reward 0.95 / 44.8 steps (epoch 100) -&gt; 1.00 / 42.0 (epoch 2000). (See Table 1.)",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper attributes faster convergence and better generalization to using logical facts extracted from observation history (visited/unvisited, initial) together with ConceptNet typing: these history-derived predicates allow the LNN to learn compact, human-readable rules such as 'if find direction x and not visited(x) and not initial(x) -&gt; go x' and a rule for returning from dead-ends, which improve decision-making and reduce unnecessary steps. They also report that methods using logical inputs (which include history-derived predicates) outperform agents trained on raw textual inputs.",
            "memory_limitations": "No explicit limitations of the memory components are reported (observation-history capacity and potential clutter are not discussed).",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2768.0",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "LSTM-DQN++",
            "name_full": "LSTM-based DQN++ (baseline neuro-only)",
            "brief_description": "A neural baseline that encodes observations with an LSTM and uses DQN-style action scoring; used as a neuro-only baseline in experiments.",
            "citation_title": "Language understanding for textbased games using deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN++",
            "agent_description": "Neuro-only agent: LSTM encoder over textual observations combined with a DQN action scorer (as in Narasimhan et al., 2015). The LSTM provides a recurrent hidden-state memory over time.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector)",
            "game_description": "Same Coin-Collector TextWorld tasks used in the paper.",
            "uses_memory": true,
            "memory_type": "recurrent hidden-state (LSTM)",
            "memory_structure": "Sequential recurrent hidden state held in LSTM cells",
            "memory_content": "Implicit compressed representation of recent observations in LSTM hidden state",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "Hidden state updated each time step by LSTM recurrence",
            "memory_usage_purpose": "To provide temporal context for decision-making from sequences of textual observations.",
            "performance_with_memory": "Reported baseline performance (moving average): Easy at epoch100: 0.07 reward / 93.4 steps; epoch1000: 0.12 / 88.6; Medium and Hard difficulties show near-zero rewards and very high step counts over epochs (see Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "While LSTM encoders are used in baseline agents, the paper reports that agents using logical inputs (including history-derived predicates) outperform neuro-only agents with LSTM encoders; this suggests that LSTM hidden-state memory alone did not achieve the same generalization in this setup.",
            "memory_limitations": "The paper does not analyze internal LSTM memory failure modes in detail; it only reports inferior empirical performance compared to FOL-LNN.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2768.1",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "LSTM-DRQN",
            "name_full": "LSTM-Deep Recurrent Q-Network (memory-augmented action scorer)",
            "brief_description": "A variant that augments the action scorer with memory units (DRQN-style recurrence) to handle partial observability; cited as prior work that adds memory in the action scorer.",
            "citation_title": "Counting to explore and generalize in text-based games",
            "mention_or_use": "mention",
            "agent_name": "LSTM-DRQN",
            "agent_description": "Mentioned prior agent that extends DQN with recurrent/memory units in the action scorer (Yuan et al., 2018); not used in experiments in this paper.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector) [context of mention]",
            "game_description": "Text-based games; DRQN variants are used to help with partial observability.",
            "uses_memory": true,
            "memory_type": "recurrent memory (DRQN-style)",
            "memory_structure": "Recurrent hidden-state within the action scorer",
            "memory_content": "Compressed temporal context from past observations",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "Recurrent update each time step",
            "memory_usage_purpose": "Cited as an approach that adds memory to the action scorer to better handle partial observability; paper does not evaluate it directly.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Only mentioned in related work as an example of adding memory units; no experimental or analytic findings about its effectiveness are given in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2768.2",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Replay buffer (prioritized)",
            "name_full": "Prioritized Experience Replay Buffer",
            "brief_description": "An experience memory storing recent (o_t, a_t, r_t, o_{t+1}) tuples used for off-policy DQN-style updates; prioritized sampling used.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Prioritized Replay Buffer",
            "agent_description": "Standard prioritized replay memory used by the RL training loop: capacity 500,000, priority fraction 0.25, mini-batch updates every 4 steps, prioritized sampling for training.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector)",
            "game_description": "Used as the experience memory for training agents on the Coin-Collector task.",
            "uses_memory": true,
            "memory_type": "experience replay (off-policy episodic memory)",
            "memory_structure": "Buffer storing tuples (o_t, a_t, r_t, o_{t+1}); prioritized sampling mechanism",
            "memory_content": "Past transitions, actions, rewards, next observations",
            "memory_capacity": "500,000 transitions (explicitly stated)",
            "memory_retrieval_strategy": "Prioritized sampling with priority fraction 0.25 for minibatch selection",
            "memory_update_strategy": "Append each transition; sampling for training every 4 steps; prioritized replay priorities updated per usual mechanisms (paper references standard prioritized replay but does not detail exact update rule).",
            "memory_usage_purpose": "Stabilize learning by replaying past experiences for gradient updates (DQN-style training).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Replay buffer is part of standard training; the paper does not ablate replay vs no-replay, but uses it across all methods, so no direct conclusions specific to replay memory are reported.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2768.3",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "ConceptNet (used as external knowledge)",
            "name_full": "ConceptNet - commonsense knowledge network",
            "brief_description": "An external semantic network used to classify words (e.g., 'north' as direction, 'coin' as money) so that the FOL converter can create typed predicates and select appropriate LNN modules.",
            "citation_title": "Conceptnet - a practical commonsense reasoning tool-kit",
            "mention_or_use": "use",
            "agent_name": "ConceptNet (external semantic memory)",
            "agent_description": "Graph-structured commonsense knowledge resource queried by the FOL converter to get class/type of words present in observations; not a trainable model in this work.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector)",
            "game_description": "Used to determine word classes (direction vs money) to convert propositional logics to typed FOL predicates for the LNN.",
            "uses_memory": true,
            "memory_type": "semantic/external knowledge graph",
            "memory_structure": "Graph database of word relations and classes",
            "memory_content": "Word classes/types and commonsense relations (used to type nouns into categories such as W_direction, W_money)",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Lookup by word to retrieve class/type from ConceptNet",
            "memory_update_strategy": "Static external resource (not updated during training in this work)",
            "memory_usage_purpose": "To assign types to observed nouns so the FOL converter can generate appropriate predicates and select corresponding LNN predicate sets (enables generalization via type-based rules).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Using ConceptNet to type nouns supports compact FOL rules (e.g., quantification over W_direction or W_money) that generalize across vocabulary and enable interpretable rules; the paper credits semantic typing as a component of the FOL conversion beneficial for generalization, but does not present a direct ablation isolating ConceptNet's contribution.",
            "memory_limitations": "No limitations discussed for ConceptNet usage in this work.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2768.4",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for textbased games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Neural logic machines",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 1
        },
        {
            "paper_title": "LeDeepChef deep reinforcement learning agent for families of text-based games",
            "rating": 1
        }
    ],
    "cost": 0.012900499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neuro-Symbolic Reinforcement Learning with First-Order Logic</h1>
<p>Daiki Kimura Masaki Ono Subhajit Chaudhury Ryosuke Kohita Akifumi Wachi Don Joven Agravante Michiaki Tatsubori Asim Munawar Alexander Gray<br>IBM Research<br>{daiki, moono, subhajit}@jp.ibm.com, {kohi, akifumi.wachi}@ibm.com<br>don.joven.r.agravante@ibm.com, mich@jp.ibm.com, {asim, alexander.gray}@ibm.com</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neurosymbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning (RL) has been successfully applied to many applications, such as computer games, text-based games, and robot control applications (Mnih et al., 2015; Narasimhan et al., 2015; Kimura, 2018; Yuan et al., 2018; Kimura et al., 2018). However, these methods require many training trials for converging to the optimal action policy, and the trained action policy is not understandable for human operators. This is because, although the training results are sufficient, the policy is stored in a black-box deep neural network. These issues become critical problems when the human operator wants to solve a real-world problem and verify the trained rules. If the trained rules are understandable and modifiable, the human operator can control them and design an action restriction. While using a symbolic (logical) format as representation for stored rules is suitable for achieving interpretability and quick training, it is difficult to train the logical rules with a traditional training approach.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed method. The agent takes a text observation from the environment, and the first-order logical facts are extracted from an FOL converter that uses a semantic parser, ConceptNet, and history. The weights (shown by line thickness in this figure) of the network are updated by these extracted predicate logics. Solid lines show one trained rule; when the agent finds a direction $x$ and the direction $x$ has not been visited, the agent takes a "Go $x$ " action. Dashed lines show the initial connections before training.</p>
<p>In order to train logical rules, a recent neurosymbolic framework called the Logical Neural Network (LNN) (Riegel et al., 2020) has been proposed to simultaneously provide key properties of both the neural network (learning) and the symbolic logic (reasoning). The LNN can train the symbolic rules with logical functions in the neural networks by having an end-to-end differentiable network minimizes a contradiction loss. Every neuron in the LNN has a component for a formula of weighted real-valued logics from a unique logical conjunction, disjunction, or negation nodes, and</p>
<p>then it can calculate the probability and logical contradiction loss during the inference and training. At the same time, the trained LNN can extract obtained logical rules by selecting high weighted connections that represent the important rules for an action policy.</p>
<p>In this paper, we propose an action knowledge acquisition method featuring a neuro-symbolic LNN framework for the RL algorithm. Through experiments, we demonstrate the advantages of the proposed method for real-world problems which is not logically grounded games such as Blocks World. Since natural language observation is easier to convert into logical information than visual or audio, we tackle text-based interaction games for verifying the proposed method.</p>
<p>Figure 1 shows an overview of our method. The observation text is input to a semantic parser to extract the logical values of each propositional logic. In this case, the semantic parser finds there are two exits (north and south). The method then converts first-order logical (predicates) facts from the propositional logics and categories of each word, such as $\exists x \in{$ south, north $},\langle$ find $x\rangle=$ True and $\exists x \in{$ east, west $},\langle$ find $x\rangle=$ False. These extracted predicated logics are fed into LNN which has some conjunction gates and one disjunction gate. The LNN trains the weights for these connections by the reward value to obtain the action policy. The contributions of this paper are as follows.</p>
<ul>
<li>The paper describes design and implementation of a novel neuro-symbolic RL for a textbased interaction games.</li>
<li>The paper explains an algorithm to extract first-order logical facts from given textual observation by using the agent history and ConceptNet as an external knowledge.</li>
<li>We observed our proposed method has advantages for faster convergence and interpretability than state-of-the-art methods and baselines by ablation study on the text-based games.</li>
</ul>
<h2>2 Related work</h2>
<p>Various prior works have examined RL for textbased games. LSTM-DQN (Narasimhan et al., 2015) is an early study on an LSTM-based encoder for feature extraction from observation and Q-learning for action policy. LSTM-DQN++ (Yuan et al., 2018) extended the exploration and LSTMDRQN (Yuan et al., 2018) was proposed for adding
memory units in the action scorer. KG-DQN (Ammanabrolu and Riedl, 2019) and GATA (Adhikari et al., 2020) extended the language understanding. LeDeepChef (Adolphs and Hofmann, 2020) used recurrent feature extraction along with the A2C (Mnih et al., 2016). CREST (Chaudhury et al., 2020) was proposed for pruning observation information. TWC (Murugesan et al., 2021) was proposed for utilizing common sense reasoning. However, none of these studies used the neuro-symbolic approach.</p>
<p>For recent neuro-symbolic RL work, the Neural Logic Machine (NLM) (Dong et al., 2018) was proposed as a method for combination of deep neural network and symbolic logic reasoning. It uses a sequence of multi-layer perceptron layers to deduct symbolic logics. Rules are combined or separated during forward propagation, and an output of the entire architecture represents complicated rules. In this paper, we compare our method with this NLM.</p>
<h2>3 Proposed method</h2>
<h3>3.1 Problem formulation</h3>
<p>As text-based games are sequential decisionmaking problems, they can naturally be applied to RL. These games are partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998), where the observation text does not include the entire information of the environment. Formally, the game is a discrete-time POMDP defined by $\langle S, A, T, R, \omega, O, \gamma\rangle$, where $S$ is a set of states $\left(s_{t} \in S\right), A$ is a set of actions, $T$ is a set of transition probabilities, $R$ is a reward function, $\omega$ is a set of observations $\left(o_{t} \in \omega\right), O$ is a set of conditional observation probabilities, and $\gamma$ is a discount factor. Although the state $s_{t}$ contains the complete internal information, the observation $o_{t}$ does not. In this paper, we follow following two assumptions: one, the word in each command is taken from a fixed vocabulary $V$, and two, each action command consists of two words (verb and object). The objective for the agent is to maximize the expected discounted reward $E\left[\sum_{t} \gamma^{t} r_{t}\right]$.</p>
<h3>3.2 Method</h3>
<p>The proposed method consists of two processes: converting text into first-order logic (FOL), and training the action policy in LNN.</p>
<h3>3.2.1 FOL converter</h3>
<p>The FOL converter converts a given natural observation text $o_{t}$ and observation history $\left(o_{t-1}, o_{t-2}, \ldots\right)$ into first-order logic facts. The method first converts text into propositional logics $l_{i, t}$ by a semantic parser from $o_{t}$, such as, the agent understands an opened direction from the current room. The agent then retrieves the class type $c$ of the word meaning in propositional logic $l_{i, t}$ by using ConceptNet (Liu and Singh, 2004) or the network of another word's definition. For example, "east" and "west" are classified as a direction-type, and "coin" is as a money-type. The class is used for selecting the appropriate LNN for FOL training and inference.</p>
<h3>3.2.2 LNN training</h3>
<p>The LNN training component is for obtaining an action policy from the given FOL logics. LNN (Riegel et al., 2020) has logical conjunction (AND), logical disjunction (OR), and negation (NOT) nodes directly in its neural network. In our method, we prepare an AND-OR network for training arbitrary rules from given inputs. As shown in Fig. 1, we prepare all logical facts at the first layer, several AND gates (as many as the network is required) at the second layer, and one OR gate connected to all previous AND gates. During the training, the reward value is used for adding a new AND gate, and for updating the weight value for each connection. More specifically, the method is storing the replay buffer which has current observation $o_{t}$, action $a_{t}$, reward $r_{t}$, and next observation $o_{t+1}$ value. For each training step, the method selects some replies, and it extracts firstorder logical facts from current observation $o_{t}$ and action $a_{t}$. The LNN trains by this fact inputs and reward; that means it forwards from input facts through LNN, calculates a loss values from the reward value, and optimizes weights in LNN. The whole training mechanism is similar to DQN (Mnih et al., 2013), the difference from these is the network. To aid the interpretability of node values, we define a threshold $\alpha \in\left[\frac{1}{2}, 1\right]$ such that a continuous value is considered True if the value is in $[\alpha, 1]$, and False if it is in $[0,1-\alpha]$.</p>
<p>Algorithm 1 describes the whole algorithm for the proposed method.</p>
<p>Algorithm 1 RL by FOL-LNN
procedure REINFORCEMENT LEARNING
for $t=1,2,3, \ldots$ do
$o_{t} \leftarrow$ Observe observation
$l_{t, i} \leftarrow$ Extract logic from $o_{t}, o_{t-1}, \ldots$
for $i=1,2,3, \ldots$ do
$c \leftarrow$ Find class from ConceptNet
$\theta^{c} \leftarrow$ Select $L N N$
$l_{t, i}^{c} \leftarrow$ Convert into FOL logic
$a_{t, i} \leftarrow \theta^{c}\left(l_{t, i}^{c}\right)$
end for
$a_{t} \leftarrow \arg \max a_{t, i}$
$r_{t}, o_{t+1} \leftarrow$ Get reward and next obs
Store reply $\left(o_{t}, a_{t}, r_{t}, o_{t+1}\right)$
$\nabla \theta \leftarrow$ Update LNN from reply
end for
end procedure</p>
<h2>4 Experiments</h2>
<p>We evaluated the proposed method on a coincollector game in TextWorld (Ct et al., 2018) with three different difficulties (easy, medium, and hard). The objective of the game is to find and collect a coin which is placed in a room within connected rooms. Since we tackle a real-world game problem rather than a symbolic games, we need to extract logical facts from given natural texts for neuro-symbolic methods. We prepare the following propositional logics as extracting logical facts: which object is found in the observation, which direction has already been visited, and which direction the agent comes from initially. These logical values are easily calculated from visited room history and word definitions. In this experiment, we prepared 26 logical values ${ }^{1}$, and all the following neuro-symbolic methods used these value as input. For the evaluation metric, we focused on (1) the test reward value on the unseen (test) games and (2) the number of steps to achieve the goal on unseen games. Since we focus on the performance of generalization, we only use 50 small-size (level $=5$ ) games for training, 50 unseen games from 5 different size (level $=5,10,15,20,25$ ) games for test ${ }^{2}$, and mini-batch in training (batch size $=4$ ). The other parameters for the game and agent follow LSTM-DQN++ (Narasimhan et al., 2015).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Average reward and number of steps (reward: higher is better / number of steps: lower is better) for each epoch on 50 unseen games with three difficulty levels. These results are from moving average $(N=100)$ and 5 random seeds. Training is done on only small-size games. Although neuro-only method cannot solve unseen test games, our proposed method (FOL-LNN) can solve and converge extremely faster than other SOTAs and baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">Easy game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Medium game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Hard game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Epoch</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">2000</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">2000</td>
</tr>
<tr>
<td style="text-align: right;">LSTM-DQN++ *</td>
<td style="text-align: right;">0.07 / 93.4</td>
<td style="text-align: right;">0.10 / 90.9</td>
<td style="text-align: right;">0.12 / 88.6</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.00 / 99.6</td>
<td style="text-align: right;">0.03 / 97.3</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.04 / 96.6</td>
</tr>
<tr>
<td style="text-align: right;">NLM-DQN **</td>
<td style="text-align: right;">0.87 / 26.4</td>
<td style="text-align: right;">0.93 / 20.8</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.27 / 81.1</td>
<td style="text-align: right;">0.48 / 65.9</td>
<td style="text-align: right;">1.00 / 29.7</td>
<td style="text-align: right;">0.01 / 99.7</td>
<td style="text-align: right;">0.10 / 94.8</td>
<td style="text-align: right;">0.66 / 64.0</td>
</tr>
<tr>
<td style="text-align: right;">NN-DQN</td>
<td style="text-align: right;">0.91 / 22.8</td>
<td style="text-align: right;">0.95 / 19.0</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.48 / 65.6</td>
<td style="text-align: right;">0.65 / 54.3</td>
<td style="text-align: right;">1.00 / 29.5</td>
<td style="text-align: right;">0.19 / 89.0</td>
<td style="text-align: right;">0.28 / 84.0</td>
<td style="text-align: right;">0.97 / 46.3</td>
</tr>
<tr>
<td style="text-align: right;">LNN-NN-DQN</td>
<td style="text-align: right;">0.88 / 24.8</td>
<td style="text-align: right;">0.94 / 20.2</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.49 / 65.8</td>
<td style="text-align: right;">0.61 / 57.0</td>
<td style="text-align: right;">1.00 / 29.6</td>
<td style="text-align: right;">0.24 / 86.9</td>
<td style="text-align: right;">0.27 / 84.9</td>
<td style="text-align: right;">0.97 / 47.4</td>
</tr>
<tr>
<td style="text-align: right;">FOL-LNN (Ours)</td>
<td style="text-align: right;">0.95 / 19.0</td>
<td style="text-align: right;">0.98 / 17.1</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.94 / 32.7</td>
<td style="text-align: right;">0.97 / 30.7</td>
<td style="text-align: right;">1.00 / 28.6</td>
<td style="text-align: right;">0.95 / 44.8</td>
<td style="text-align: right;">0.98 / 43.5</td>
<td style="text-align: right;">1.00 / 42.0</td>
</tr>
</tbody>
</table>
<ul>
<li>State-of-the-art neuro-only method with a simple DQN action scorer (Narasimhan et al., 2015)
** State-of-the-art neuro-symbolic method has same input as ours and other neuro-symbolic methods (Dong et al., 2018)</li>
</ul>
<p>We prepared five methods for an evaluation of the proposed method:</p>
<ul>
<li>LSTM-DQN++ (Narasimhan et al., 2015): State-of-the-art neuro-only method with a simple DQN action scorer. We use this method as a baseline method for the neuro-only agent, and LSTM receives extracted embedding vector from natural text information.</li>
<li>NLM-DQN (Dong et al., 2018): State-of-the-art neuro-symbolic method. The input is propositional logical values that is also used in following baselines and proposed method. The original NLM uses the REINFORCE (Williams, 1992) algorithm, but in order to handle text-based games with the same setting as the other methods, we applied the DQN algorithm. In short, the method uses an NLM layer instead of an LSTM (Hochreiter and Schmidhuber, 1997) for the encoder of the LSTM-DQN++ method. We tuned the hyper-parameters from the same search space as the original paper.</li>
<li>NN-DQN: Nave neuro-symbolic baseline method. The input of the network is propositional logical values, and it uses a multi layer perceptron as the encoder of the LSTMDQN++.</li>
<li>LNN-NN-DQN: Neuro-symbolic baseline method. The method first gets propositional logical values, it converts by LNN into some conjunction values for all combinations of given logical values, and then it inputs them
into a multi layer perceptron. It differs from NN-DQN in that LNN-NN-DQN has prepared conjunction nodes, which should lead to faster training in beginning of the training, and better interpretabiliity after the training.</li>
<li>FOL-LNN: Our neuro-symbolic method.</li>
</ul>
<p>Table 1 shows the test reward and test step values on unseen games, and Fig. 2 shows curves. First, all the RL results with logical input were better than those with textual input. Second, our proposed method could converge much faster than the other neuro-symbolic state-of-the-art and baseline methods. Third, only our method could extract the trained rules by checking the weight value of the LNN. We attached the extracted rules from the medium level games here:</p>
<p>$$
\begin{aligned}
&amp; \exists x \in W_{\text {money }} \
&amp; \quad(\text { find } x) \rightarrow \llbracket \text { take } x \rrbracket \
&amp; \exists x \in W_{\text {direction }} \
&amp; \quad((\text { find } x) \wedge \neg(\text { visited } x) \wedge \neg(\text { initial } x)) \vee \
&amp; \quad((\text { find } x) \wedge(\text { all are visited }) \wedge(\text { initial } x)) \rightarrow \llbracket \text { go } x \rrbracket
\end{aligned}
$$</p>
<p>where $W_{\text {direction }}$ is a set of words in a type of "direction" in ConceptNet. The rule for "take"-action is for taking a coin. The first conjunction rule for "go"-action is for visiting an un-visited room, and the second rule is for returning to the initial room from a dead-end. With our proposed method, we can see that these trained rules will be helpful for operating the neural agent in real use cases.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Curves for reward and number of steps for 50 unseen games. Moving average is applied.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed a novel neuro-symbolic method for RL on text-based games. According to the evaluation on the natural language text-based game with several difficulties, our method can converge extremely faster than other state-of-theart neuro-only and neuro-symbolic methods, and extract trained logical rules for improving interpretability of the model.</p>
<h2>Discussion about ethics</h2>
<p>Our model is not using any sensitive contexts such as legal or health-care settings. The data set used in our experiment does not contain any sensitive information. Since our proposed neuro-symbolic RL method can extract the trained rules for interpretability of the model, the method can analyze a reason behind taken action. We are sure that if the model returns biased results, this functionality is helpful for clearing the reason for these data bias issues.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Leonard Adolphs and Thomas Hofmann. 2020. Ledeepchef deep reinforcement learning agent for families of text-based games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7342-7349.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3002-3008.</p>
<p>Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In Computer Games - 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers, pages 41-75.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2018. Neural logic machines. In International Conference on Learning Representations.</p>
<p>Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1):99-134.</p>
<p>Daiki Kimura. 2018. Daqn: Deep auto-encoder and q-network. arXiv preprint arXiv:1806.00630.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, and Sakyasingha Dasgupta. 2018. Internal model from observations for reward shaping.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
H. Liu and P. Singh. 2004. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal, 22(4):211-226.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937. PMLR.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature, 518:529-533.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021. Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence.</p>
<p>Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. 2015. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages $1-11$.</p>
<p>Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, and Santosh Srivastava. 2020. Logical neural networks.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Xingdi Yuan, Marc-Alexandre Ct, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525.</p>
<h2>Appendix A: Environment Setting</h2>
<p>In this section we describe Coin-Collector (Yuan et al., 2018), a text-based game used in our experiments. Then, we describe the hyper-parameter setting.</p>
<h2>Appendix A.1: Coin-Collector</h2>
<p>Coin-Collector is a kind of text-based games, and we have to move an agent through rooms to get a coin placed in a room. An agent receives an observation text that describes the structure of a current room from a game. The goal of Coin-Collector is to analyze textual data and understand the structure of given rooms for training an agent.</p>
<p>A game has hyper-parameters such as level and difficulty. A game level indicates the minimum number of steps to a room in which a coin is placed. Rooms are randomly connected and their structure depends on difficulty. An easy game has no distractor rooms (dead ends) along the path. On a medium game, each room along the optimal trajectory has one distractor room randomly connected to it. A hard game, each room has two distractor rooms which means each room has one for optimal trajectory, one for the previous room, and two for distractor rooms.</p>
<p>An agent can use two types of verbs ( ${$ take, go}) and five types of nouns ( ${$ coin, east, west, south, north}). Since an action consists of a verb and a noun, there are ten different actions that an agent can take. For the settings of LSTM-DQN++ (Narasimhan et al., 2015), the agent gets the positive reward when the agent goes in a new room. The agent also gets positive reward when the agent successfully returns the initial coming direction for medium setting. If an agent takes an invalid action such as "go coin", or "go north" at no north room, the agent does not receive a negative reward.</p>
<h2>Appendix A.2: Hyper-parameters</h2>
<p>For the all experiments, we used the same hyperparameters with the previous work for CoinCollector as follows.</p>
<ul>
<li>We used a prioritized replay memory with capacity of 500,000 and the priority fraction is 0.25 .</li>
<li>A mini-batch gradient update is performed every 4 steps in the game play.</li>
<li>The discount factor for Q-learning $\gamma$ is 0.9 .</li>
<li>We used an episodic discovery bonus that encourages an agent to discover unseen states and the coefficient $\beta$ is 1.0 .</li>
<li>We anneal the $\epsilon$ for the $\epsilon$-greedy strategy from 1 to 0.2 over 1000 epochs. After 1000 episodes, the $\epsilon$ is 0.2 .</li>
<li>We used the Adam algorithm (Kingma and Ba, 2014) for the optimization and the learning rate is $1 e^{-3}$.</li>
</ul>
<h2>Appendix B: Experiment details</h2>
<p>The training and validation times until 3,000 epochs for each method are as follows.</p>
<ul>
<li>LSTM-DQN++ (Narasimhan et al., 2015): Around 2 hours for easy difficulty, and around 4 hours for medium difficulty.</li>
<li>NLM-DQN (Dong et al., 2018): Around 40 minutes for easy difficulty, and around 2.5 hours for medium difficulty.</li>
<li>NN-DQN: Around 30 minutes for easy difficulty, and around 1.5 hours for medium difficulty.</li>
<li>LNN-NN-DQN: Around 30 minutes for easy difficulty, and around 1.5 hours for medium difficulty.</li>
<li>FOL-LNN: Around 35 minutes for easy difficulty, and around 2 hours for medium difficulty.</li>
</ul>
<p>These results are calculated on Intel Core i7-6700K CPU (4.00GHz) and NVIDIA Titan X. From these results, our proposed method is not computationally expensive than other methods.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1} 26=(5$ (object) +4 (visited) +4 (initial $) \times 2$ (negation)
${ }^{2}$ The agent needs to generalize the game level size&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
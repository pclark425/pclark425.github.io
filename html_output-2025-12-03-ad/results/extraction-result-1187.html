<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1187 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1187</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1187</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-272599855</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.07932v2.pdf" target="_blank">Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies</a></p>
                <p><strong>Paper Abstract:</strong> Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods. Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings. An area of particular interest is search in social networks due to its numerous applications. Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network. We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity. Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines. Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1187.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1187.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Facebook ego graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Facebook social-network ego graphs (SNAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Real-world social-network ego graphs from the SNAP repository used as navigation environments; nodes represent users with binary anonymized attributes and edges represent friendships, used to evaluate decentralized graph path search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Facebook ego graphs (SNAP)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-world social-network ego graphs (Facebook friendships) with binary anonymized node attributes collected via surveys; used to study decentralized message-routing/navigation across social communities.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Social-network topology with community structure, homophily, heterogeneous degree distribution (hubs and many low-degree nodes), and weak ties bridging communities; topology is not fully described numerically in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Five largest connected-component ego graphs selected; each between ~100 and ~600 nodes (exact per-graph sizes reported in Appendix Table 4 but not enumerated in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GARDEN (Graph Attention-guided Routing for DEcentralised Networks); baselines: DistanceWalker, GreedyWalker, ConnectionWalker, RandomWalker, MLPA2C, MLPA2CWD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GARDEN: decentralized routing policy trained centrally (CTDE) using Advantage Actor-Critic with node embeddings produced by a Graph Attention Network (GAT); policies are stochastic, memory-less (reactive) and choose a neighbor to forward the message based on learned embeddings and SoftMax over neighbor scores. Baselines include stochastic heuristics based on attribute distance (DistanceWalker), node degree (ConnectionWalker), greedy attribute distance, and simple random walk, plus MLP-based RL variants (MLPA2C / MLPA2CWD).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Mean Oracle Ratio (episode length / shortest path), Truncation Rate (% episodes truncated at Tmax), Win Rate (fraction of episodes obtaining the relative shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reactive, stochastic, memory-less policy using learned GAT embeddings (decentralized execution) — GARDEN performed best on real-world social graphs in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper reports that homophily and community structure enable decentralized navigation: if node attributes reflect a hidden metric and communities are present, learned embeddings can approximate that metric and allow short decentralized routes. Heterogeneous degree distribution (hubs/weak ties) provides shortcuts; relying only on degree or homophily can fail if bridging nodes lack degree or attribute similarity. In real Facebook ego graphs GARDEN leverages both attributes and local topology to outperform attribute-only or degree-only stochastic policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>While the Facebook graphs themselves are not exhaustively compared to one another in the main text, the paper reports that GARDEN consistently outperforms baselines across the five real-world ego graphs tested; the authors attribute success to latent structure (homophily and community connectivity) present in these graphs and to GARDEN's ability to recover a latent navigation-friendly metric.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Memory-less (history-free) stochastic policies augmented with task-trained GNN embeddings suffice to find short paths in these social graphs when homophily and community structure are present; stochasticity helps avoid deterministic loops. The paper notes that adding memory (e.g., RNNs encoding visited history) could improve performance where necessary, but was not used in their main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1187.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1187.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic homophilic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic attributed graphs generated via spatial/homophily model (Kaiser & Hilgetag generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic attributed graphs with homophily where node attributes are sampled uniformly in a unit box and edges are formed stochastically with probability decreasing with attribute distance; used to study the effect of graph density and attribute-driven edge formation on decentralized navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spatial growth of real-world networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic homophilic attributed graphs (Kaiser & Hilgetag generator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Graphs with node attributes sampled uniformly in [0,1]^d and edges sampled with probability p((u,u')) = max(1, β e^{-α ||x_u - x_u'||^2}) (parameters α, β control edge formation and density), explicitly used to create controlled sparsity and homophily for navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Topology controlled by β (density) and α (attribute-distance decay); can range from sparse to dense; exhibits homophily (edges more likely between attribute-similar nodes) and community-like structure induced by attribute proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Typical experimental setting: n = 200 nodes; experiments vary β over {0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.75,1.0} (10 graph topologies per β) for sensitivity analysis; one ablation used n=200, α=30, β=5 (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GARDEN (primary), MLPA2C, MLPA2CWD, DistanceWalker, GreedyWalker, ConnectionWalker, RandomWalker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GARDEN: centralized training (A2C) with decentralized execution; uses GAT to compute node embeddings from 1-hop ego graphs, then an MLP policy producing stochastic SoftMax over neighbors; other agents include MLP-only RL variants and stochastic heuristic walkers (DistanceWalker uses SoftMax over attribute distance; ConnectionWalker uses SoftMax over neighbor degrees).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Mean Oracle Ratio (episode length / shortest path), Truncation Rate (% episodes exceeding Tmax), Win Rate (fraction achieving relative shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>For ablation on synthetic graphs (n=200, α=30, β=5): GARDEN Mean Oracle Ratio = 1.95 ± 0.09 (lower is better).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>For the same ablation setting: Win Rate (R_win) = 34.44% (reported as 'R win' in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Stochastic, memory-less reactive policy with learned GAT embeddings (GARDEN) — found to be optimal/best-performing in most synthetic settings tested.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper reports that graph density (controlled by β) strongly affects exploration difficulty: lower density reduces available paths, increasing exploration difficulty and truncation rates; GARDEN remains robust across β values for Mean Oracle Ratio and Win Rate, but DistanceWalker (which uses ground-truth attributes) can outperform GARDEN in truncation rate at high β where many edges exist and true attributes directly indicate connectivity. Homophily and heterogeneity (community structure and weak ties) enable decentralized search when the embedding recovers an approximate hidden metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Sensitivity analysis across β (density) shows GARDEN generally matches or surpasses baselines for Mean Oracle Ratio and Win Rate across all β; DistanceWalker beats GARDEN on Truncation Rate at high β because it exploits ground-truth attribute information used to generate edges. Lower β (sparser graphs) increase truncation and make exploration harder for all agents.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that combine learned local structural features and attributes (GAT embeddings) with stochastic action selection perform best across topologies. Memory-less reactive policies suffice under strong homophily and identifiable community structure, but the paper notes limitations of memory-less policies (they cannot adapt based on the history of visited nodes) and suggests that adding history (e.g., RNNs) would be needed where backtracking or forbidding revisits is important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The small-world phenomenon: An algorithmic perspective <em>(Rating: 2)</em></li>
                <li>Decentralized search in networks using homophily and degree disparity <em>(Rating: 2)</em></li>
                <li>Navigability of complex networks <em>(Rating: 2)</em></li>
                <li>Learning graph search heuristics <em>(Rating: 2)</em></li>
                <li>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning <em>(Rating: 2)</em></li>
                <li>M-walk: Learning to walk over graphs using Monte Carlo Tree Search <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1187",
    "paper_id": "paper-272599855",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Facebook ego graphs",
            "name_full": "Facebook social-network ego graphs (SNAP)",
            "brief_description": "Real-world social-network ego graphs from the SNAP repository used as navigation environments; nodes represent users with binary anonymized attributes and edges represent friendships, used to evaluate decentralized graph path search.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "Facebook ego graphs (SNAP)",
            "environment_description": "Real-world social-network ego graphs (Facebook friendships) with binary anonymized node attributes collected via surveys; used to study decentralized message-routing/navigation across social communities.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Social-network topology with community structure, homophily, heterogeneous degree distribution (hubs and many low-degree nodes), and weak ties bridging communities; topology is not fully described numerically in the text.",
            "environment_size": "Five largest connected-component ego graphs selected; each between ~100 and ~600 nodes (exact per-graph sizes reported in Appendix Table 4 but not enumerated in the main text).",
            "agent_name": "GARDEN (Graph Attention-guided Routing for DEcentralised Networks); baselines: DistanceWalker, GreedyWalker, ConnectionWalker, RandomWalker, MLPA2C, MLPA2CWD",
            "agent_description": "GARDEN: decentralized routing policy trained centrally (CTDE) using Advantage Actor-Critic with node embeddings produced by a Graph Attention Network (GAT); policies are stochastic, memory-less (reactive) and choose a neighbor to forward the message based on learned embeddings and SoftMax over neighbor scores. Baselines include stochastic heuristics based on attribute distance (DistanceWalker), node degree (ConnectionWalker), greedy attribute distance, and simple random walk, plus MLP-based RL variants (MLPA2C / MLPA2CWD).",
            "exploration_efficiency_metric": "Mean Oracle Ratio (episode length / shortest path), Truncation Rate (% episodes truncated at Tmax), Win Rate (fraction of episodes obtaining the relative shortest path).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Reactive, stochastic, memory-less policy using learned GAT embeddings (decentralized execution) — GARDEN performed best on real-world social graphs in the paper's experiments.",
            "topology_performance_relationship": "Paper reports that homophily and community structure enable decentralized navigation: if node attributes reflect a hidden metric and communities are present, learned embeddings can approximate that metric and allow short decentralized routes. Heterogeneous degree distribution (hubs/weak ties) provides shortcuts; relying only on degree or homophily can fail if bridging nodes lack degree or attribute similarity. In real Facebook ego graphs GARDEN leverages both attributes and local topology to outperform attribute-only or degree-only stochastic policies.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "While the Facebook graphs themselves are not exhaustively compared to one another in the main text, the paper reports that GARDEN consistently outperforms baselines across the five real-world ego graphs tested; the authors attribute success to latent structure (homophily and community connectivity) present in these graphs and to GARDEN's ability to recover a latent navigation-friendly metric.",
            "policy_structure_findings": "Memory-less (history-free) stochastic policies augmented with task-trained GNN embeddings suffice to find short paths in these social graphs when homophily and community structure are present; stochasticity helps avoid deterministic loops. The paper notes that adding memory (e.g., RNNs encoding visited history) could improve performance where necessary, but was not used in their main experiments.",
            "uuid": "e1187.0",
            "source_info": {
                "paper_title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Synthetic homophilic graphs",
            "name_full": "Synthetic attributed graphs generated via spatial/homophily model (Kaiser & Hilgetag generator)",
            "brief_description": "Synthetic attributed graphs with homophily where node attributes are sampled uniformly in a unit box and edges are formed stochastically with probability decreasing with attribute distance; used to study the effect of graph density and attribute-driven edge formation on decentralized navigation.",
            "citation_title": "Spatial growth of real-world networks",
            "mention_or_use": "use",
            "environment_name": "Synthetic homophilic attributed graphs (Kaiser & Hilgetag generator)",
            "environment_description": "Graphs with node attributes sampled uniformly in [0,1]^d and edges sampled with probability p((u,u')) = max(1, β e^{-α ||x_u - x_u'||^2}) (parameters α, β control edge formation and density), explicitly used to create controlled sparsity and homophily for navigation experiments.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Topology controlled by β (density) and α (attribute-distance decay); can range from sparse to dense; exhibits homophily (edges more likely between attribute-similar nodes) and community-like structure induced by attribute proximity.",
            "environment_size": "Typical experimental setting: n = 200 nodes; experiments vary β over {0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.75,1.0} (10 graph topologies per β) for sensitivity analysis; one ablation used n=200, α=30, β=5 (reported).",
            "agent_name": "GARDEN (primary), MLPA2C, MLPA2CWD, DistanceWalker, GreedyWalker, ConnectionWalker, RandomWalker",
            "agent_description": "GARDEN: centralized training (A2C) with decentralized execution; uses GAT to compute node embeddings from 1-hop ego graphs, then an MLP policy producing stochastic SoftMax over neighbors; other agents include MLP-only RL variants and stochastic heuristic walkers (DistanceWalker uses SoftMax over attribute distance; ConnectionWalker uses SoftMax over neighbor degrees).",
            "exploration_efficiency_metric": "Mean Oracle Ratio (episode length / shortest path), Truncation Rate (% episodes exceeding Tmax), Win Rate (fraction achieving relative shortest path).",
            "exploration_efficiency_value": "For ablation on synthetic graphs (n=200, α=30, β=5): GARDEN Mean Oracle Ratio = 1.95 ± 0.09 (lower is better).",
            "success_rate": "For the same ablation setting: Win Rate (R_win) = 34.44% (reported as 'R win' in Table 2).",
            "optimal_policy_type": "Stochastic, memory-less reactive policy with learned GAT embeddings (GARDEN) — found to be optimal/best-performing in most synthetic settings tested.",
            "topology_performance_relationship": "Paper reports that graph density (controlled by β) strongly affects exploration difficulty: lower density reduces available paths, increasing exploration difficulty and truncation rates; GARDEN remains robust across β values for Mean Oracle Ratio and Win Rate, but DistanceWalker (which uses ground-truth attributes) can outperform GARDEN in truncation rate at high β where many edges exist and true attributes directly indicate connectivity. Homophily and heterogeneity (community structure and weak ties) enable decentralized search when the embedding recovers an approximate hidden metric.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Sensitivity analysis across β (density) shows GARDEN generally matches or surpasses baselines for Mean Oracle Ratio and Win Rate across all β; DistanceWalker beats GARDEN on Truncation Rate at high β because it exploits ground-truth attribute information used to generate edges. Lower β (sparser graphs) increase truncation and make exploration harder for all agents.",
            "policy_structure_findings": "Policies that combine learned local structural features and attributes (GAT embeddings) with stochastic action selection perform best across topologies. Memory-less reactive policies suffice under strong homophily and identifiable community structure, but the paper notes limitations of memory-less policies (they cannot adapt based on the history of visited nodes) and suggests that adding history (e.g., RNNs) would be needed where backtracking or forbidding revisits is important.",
            "uuid": "e1187.1",
            "source_info": {
                "paper_title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The small-world phenomenon: An algorithmic perspective",
            "rating": 2,
            "sanitized_title": "the_smallworld_phenomenon_an_algorithmic_perspective"
        },
        {
            "paper_title": "Decentralized search in networks using homophily and degree disparity",
            "rating": 2,
            "sanitized_title": "decentralized_search_in_networks_using_homophily_and_degree_disparity"
        },
        {
            "paper_title": "Navigability of complex networks",
            "rating": 2,
            "sanitized_title": "navigability_of_complex_networks"
        },
        {
            "paper_title": "Learning graph search heuristics",
            "rating": 2,
            "sanitized_title": "learning_graph_search_heuristics"
        },
        {
            "paper_title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning",
            "rating": 2,
            "sanitized_title": "go_for_a_walk_and_arrive_at_the_answer_reasoning_over_paths_in_knowledge_bases_using_reinforcement_learning"
        },
        {
            "paper_title": "M-walk: Learning to walk over graphs using Monte Carlo Tree Search",
            "rating": 2,
            "sanitized_title": "mwalk_learning_to_walk_over_graphs_using_monte_carlo_tree_search"
        }
    ],
    "cost": 0.012843,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies
26 Nov 2024</p>
<p>Alexei Pisacane pisacane.alexei@gmail.com 
University College London</p>
<p>Victor-Alexandru Darvariu 
University of Oxford</p>
<p>Mirco Musolesi 
University College London</p>
<p>University of Bologna</p>
<p>Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies
26 Nov 2024AC5656427B88DB50B8CEDC6ADA111850arXiv:2409.07932v2[cs.LG]
Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods.Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings.An area of particular interest is search in social networks due to its numerous applications.Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network.We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity.Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines.Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning.</p>
<p>Introduction</p>
<p>Graph path search is a fundamental task in Computer Science, pivotal in various domains such as knowledge bases [1], robotics [2], and social networks [3].Given a start node and end node, the goal is to find a path from a source to a destination in the graph that connects them and optimizes desiderata such as minimizing path length.We refer to search strategies that achieve this as efficient.The problem is generally framed from a centralized perspective with a global view of the network, which is impractical or infeasible for several applications.In peer-to-peer networks [4], where privacy is a primary concern, a centralized agent poses significant risks [5][6][7].Large graphs may also induce scalability bottlenecks as the storage requirements of a centralized directory strain memory limitations [8].Moreover, in dynamic networks, maintaining a consistent global view of the topology may be impossible [9].Graph path search is of particular interest in social networks given the inherent commercial applications and potential for new insights from a social sciences perspective [10,11].</p>
<p>In this paper, we will study the problem of decentralized path graph search using local information.We will consider social networks and we will discuss how the proposed method can be directly applied to any networks for which topological and node attribute information is available.Indeed, prior experiments in human social networks, such as Stanley Milgram's renowned "small world" experiment [12] 2 reveals the existence of short paths in social networks that are discoverable solely through local graph topology and high-level node attributes, e.g., characteristics of the individuals, such as their occupation, their high-school, and so on.Many social networks exhibit two key properties that make decentralized search with partial information possible and efficient.The first, homophily [15], reflects the tendency of individuals to connect with others who share similar attributes.The second is the heterogeneity of local structure in many networks, in which nodes are often organized into highly connected communities [16], with a smaller number of weak ties [17,18] or central connectors [19] bridging these node clusters and acting as shortcuts.</p>
<p>Reinforcement Learning (RL) has recently been employed with a centralized perspective for discovering learned heuristics for graph search [20,21] and reasoning over paths in knowledge graphs [22,23], in a way that complements or outperforms classic algorithms.Motivated by the promise of RL and the goal to attain decentralized graph path search, in this paper, we propose a multi-agent RL formulation in the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework.Agents have only local visibility of graph topology and neighbor attributes, and cooperate towards finding paths to the target node.We propose a method for learning in this Dec-POMDP that, in accordance with the Centralized Training and Decentralized Execution (CTDE) paradigm [24], trains an actor-critic model with node representations learned by a Graph Attention Network [25] with shared parameters.These embeddings are computed via a message-passing procedure starting from the raw node attributes and the graph topology.For this reason we name the resulting method GARDEN: Graph Attention-guided Routing for DEcentralised Networks.At execution time, the policy is used in a decentralized fashion by all agents.</p>
<p>We conduct experiments on synthetic and real-world social network graphs with up to 600 nodes to evaluate our model design.Our findings highlight the superior ability of our method to utilize both homophily and local connectivity better when compared to learned and handcrafted baseline models.Moreover, we find that the learned embeddings are meaningful representations for navigation in highdimensional feature space.Our results show that the dynamics observed in Milgram's experiment can emerge using reward-driven learning.RL is able to construct a latent feature space that effectively incorporates both node attribute information and network structure for use in graph path search.Therefore, this work supports the notion that decentralized graph path search can succeed given appropriate representations, and shows a possible mechanism for how representations similar to those inherently used by individuals may be constructed.</p>
<p>Related Work</p>
<p>Network Science</p>
<p>Search is a common operation in network applications.Various classic algorithms [26] ensure path discovery between two nodes under specific conditions.They require maintaining global knowledge of the graph structure, which, as we have argued, is impractical in certain cases due to considerations of privacy, scalability, and dynamicity.We therefore focus our attention on graph path search using only local information.</p>
<p>As previously discussed, our inspiration for studying this problem is Milgram's "small world" experiment [12].The findings, later validated on a larger scale [27], support this hypothesis in social networks, which are characterized by short mean path lengths.Subsequent research [28] highlighted the discovery of effective routing strategies, emphasising the concept of homophily [15], which states that individuals seek connections to others that are similar to themselves.</p>
<p>In addition to homophily, many networks are characterized by a power-law degree distribution [29] and exhibit heterogeneity in node degree.In such networks, a few "hub" nodes with numerous connections coexist with many nodes having a relatively small degree.Highly connected nodes therefore offer potential shortcuts in search trajectories by bridging sparsely connected communities.</p>
<p>For effective search, finding the bridging node between two communities is often required.Relying solely on homophily or node degree may be ineffective, as the bridging node might lack a large degree or significant attribute similarity with the target node.In networks with large clusters, an agent may spend considerable time navigating the current cluster before reaching the desired community.Identifying weak ties [17] between communities is challenging using only node attributes or degrees; therefore, an effective search for weak ties requires awareness of candidate nodes' neighborhoods.</p>
<p>A useful lens for viewing this problem is through a "hidden metric space" [30] of node features.Assuming node features are representative of their position within this space, the probability of edge-sharing increases with decreasing pairwise attribute distance.Empirical evidence supports the efficiency of navigation using this underlying metric.If an approximation to the hidden metric using only local graph structure is feasible, a decentralized strategy could involve moving toward nodes minimizing the approximate metric [30,31].</p>
<p>The approach of Simsek and Jensen [32] is most closely related to ours as it also treats the problem of search by leveraging both homophily and the node degree disparity.The algorithm uses an estimate of the statistical relationship between the attribute similarity and connection probability whose computation requires knowledge of the attributes of all the nodes in the network.In contrast, our method does not require the availability of this global information.</p>
<p>Reinforcement Learning for Graph Routing and Search</p>
<p>Reinforcement Learning methods have been applied for a variety of graph optimization problems in recent years as a mechanism for discovering, by trial-and-error, algorithms that can outperform classic heuristics and metaheuristics [33].Their appeal stems from the flexibility of the RL framework to address a wide variety of problems, requiring merely that they can be formulated as MDPs.The most relevant works in this area treat routing and search problems over graphs.</p>
<p>Early work on RL for routing demonstrated the potential of the MDP formalism [34][35][36], but suffered from the main pitfall of tabular RL methods: poor scalability.Interest has been reignited recently by several works that employ function approximation for scaling to larger problems.In this line of work, Valadarsky et al. [37] considered learning routing policies on a dataset of historical traces of pairwise demands and applying them in new traffic conditions.The MDP is framed as learning a set of edge weights from which the routing strategy is determined.Hope and Yoneki [38] expanded on this work by introducing a Graph Neural Network (GNN) [39] technique for function approximation, showing its advantages over using simple feedforward neural networks.More recent work by Almasan et al. [40] leveraged a GNN representation trained using a policy gradient algorithm.They frame actions as the choice of a middle-point for a flow given start and target nodes, with previous action choices becoming part of the state.Other recent works on RL for routing considered optimizing a weighted combination of delay and throughput [41] and deciding how to re-route the most important flows (i.e., those with the most traffic) given an initial routing scheme [42].</p>
<p>Another important line of work studies how to perform search on graphs.In contrast to routing, for search tasks there is no notion of a link load associated with traversing a particular node or edge in the graph.A notable contribution in this direction is work by Pándy et al. [21], where RL agents are tasked with learning a heuristic function for augmentation of A* search.Patankar et al. [43] considered the task of validating the way in which humans perform graph navigation, adopting two theories relating topological graph properties to minimizing gaps in knowledge or compressing existing knowledge.Their DQN-based agent parameterized by a GNN was validated successfully using human graph navigation trajectories.</p>
<p>The problem of knowledge graph completion may also be viewed as graph traversal in instances with heterogeneous edge types [44] and with a target node that is not specified a priori.Das et al. [22] proposed an MDP formulation of this task, in which an agent chooses the next relationship to traverse given the current node.A proportion of the true relationships in the knowledge graph is masked and used to provide the reward signal for training the agent via REINFORCE.The M-Walk method [23] builds further in this direction by leveraging the determinism of the transition dynamics.Therefore, training with trajectories from a Monte Carlo Tree Search (MCTS) policy [45] can overcome the reward sparsity associated with the random exploration of model-free methods.Zhang et al. [46] proposed a hierarchical method that features a high-level agent for choosing a cluster in which the target may be located, and a lower-level agent that navigates within the cluster.</p>
<p>Lastly, we note that, while the works reviewed in this section share features of our MDP and model design, none are directly applicable to the problem formulation.Chiefly, we consider a decentralized graph path search scenario in which each agent has only partial visibility of the network.</p>
<p>Methods</p>
<p>In this section, we first introduce our decentralized mathematical formulation of the graph search problem.Next, we describe the proposed multi-agent reinforcement learning algorithm, which leverages learnable graph embeddings.</p>
<p>MDP Formulation</p>
<p>We frame the search problem as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) [47] taking place over an attributed, undirected, and unweighted graph structure G = (V, E) with n nodes and m edges.An agent is placed on each node u i ∈ V in the graph, while the edges E indicate direct bidirectional communication links between agents.An attribute vector x ui ∈ R d is associated with each agent.The aim is to find a path starting from an initial node u src to a designated target node u tgt by passing a single global message.</p>
<p>States.The global state S t at time t is a tuple ⟨S
(1) t , S (2) t , . . . , S (n) t ⟩ composed of the states S (i) t = (M (i) t , u tgt ) of the individual agents. Here, M (i)
t is an indicator variable that denotes the presence or absence of the message at a given node u i at time t.While specifying the target node u tgt is required for MDP stationarity, its identity is not provided to the agent in the observation.</p>
<p>Actions.The joint action space A = × i A (i) is the product of the agent-wise action spaces.At each timestep t of an MDP episode, a node u i receives the message m ∈ R d specifying the attributes x utgt of the target node (but not its identity).It chooses as its action A (i) t one of its neighbors, denoted N (u i ), to pass the message on to.We denote this action of node u i passing the message to node u j by a ui→uj .All other agents take a no-op action at this step, which has no effect.Hence,
A (i) t = {a ui→uj |u j ∈ N (u i )} if M (i) t = 1,
and {no-op} otherwise.</p>
<p>Observations. The environment emits a global observation
O t = ⟨O (1) t , O (2) t , . . . , O(n)
t ⟩ at each time step t, from which each node u i only observes its own component O (i) t .In accordance with our motivations, we provide agents with only local observations of the graph topology.Concretely, we equip each agent u i with observations of 1-hop ego subgraphs G uj centered on its neighboring nodes, including visibility of pairwise edges between 1-hop neighbors.The observation will also contain information on the target node: if the agent possesses the message, it symmetrically can observe an ego graph centered on u tgt .Formally, the observation O (i)
t is defined as (m, {G uj |u j ∈ N (u i ), G utgt }) if M (i) t = 1, and {G uj |u j ∈ N (u i )} otherwise. The ego graph G uj = (V uj , E uj ) is defined such that V uj = N (u j ) ∪ {u j } and E uj = {(u k , u l ) ∈ E|u k ∈ V uj ∧ u l ∈ V uj }.
Transitions.The message moves deterministically to the selected node, updating the indicator
M (i) t+1 accordingly. Concretely, M (i) t+1 = 1 if M (j) t = 1 ∧ A (j)
t = a uj →ui , and 0 otherwise.Rewards.The episode ends when the message reaches the target node, yielding a collective reward of +1 for the agents and terminating the episode.Formally, R
(k) t+1 = 1 ∀k if ∃i . M (i) t = 1 ∧ A (i) t =
a ui→utgt , and 0 otherwise.To prevent agents from entering action cycles, we introduce a truncation criterion: the episode can also end after T max interaction steps with the environment.</p>
<p>Learning Architecture</p>
<p>In our design, we employ the common multi-agent Reinforcement Learning paradigm of Centralized Training with Decentralized Execution (CTDE) [48].We consider a fully collaborative setting in which the agents are all rewarded if messages are successfully delivered to the target node.The collaborative objective is formulated such that each agent selecting the optimal next action results in an optimal trajectory through the graph.Therefore, the optimal trajectory can be constructed in a decentralized manner.</p>
<p>As it is common in the CTDE paradigm, we utilize parameter-sharing across agent networks.In the training scheme, a centralized agent receives localized observations from individual agents at each step, and is tasked with selecting optimal actions in the search path.The optimal decision is first learnt, and then replicated and distributed to individual nodes at execution time.</p>
<p>At each training step, a central agent is given incomplete observations and receives sparse and delayed rewards from the environment.Given these specifications, we propose the use of a variant of the Advantage Actor-Critic (A2C) algorithm [49] to promote adequate exploration with acceptable sample efficiency.The A2C value network is also learned in a centralized fashion to guide the training of the policy network.Learning a stochastic policy (rather than a deterministic one) is important for the problem under consideration given that a short path to the target may not be available via a particular neighbor despite a high level of attribute similarity.Lastly, we incorporate entropy regularization to ensure the policy maintains a high degree of randomness while still aiming to maximize the expected discounted return.</p>
<p>The goal is to learn, for each agent, a policy π ui that maps observations to actions.We formulate the choice of neighbor to which the message should be transmitted based on values output by an MLPparameterized policy network f Θ1 π .The policy network is applied for each neighbor u j ∈ N (u i ) of the node u i that is currently in possession of the message at time t, and the SoftMax function is used to derive a probability distribution.Concretely, for node u i in posession of the message m, the policy is defined as:
π ui (a ui→uj |O (i) ) = exp(f Θ1 π ([x uj ||x utgt ])) u k ∈N (ui) exp(f Θ1 π ([x u k ||x utgt ])) ,(1)
where [•||•] denotes concatenation.Similarly, to estimate the value function, we pass the current node u i and the target node u tgt attributes through an MLP-parameterized value network f Θ2 v :
v(O (i) ) = f Θ2 v ([x ui ||x utgt ]),(2)
Where u i is the node in possession of the message m at time t.It is interesting to note that the node features that are used as input to the policy and value networks will impact the effectiveness of the learned policies.The simplest choice is to use the raw node features x ui , and we denote the resulting algorithm as MLPA2C.We also consider the simplest extension to this model that minimally incorporates local graph topology by augmenting node attributes with node degrees, i.e.,</p>
<p>x WD ui = [x ui ||deg(u i )].We refer to this as MLPA2CWD.</p>
<p>GARDEN</p>
<p>Recall the "hidden metric" hypothesis discussed in Section 2.1, which posits that a viable policy can be motivated by moving through the graph to reduce node distance, provided a good approximation of the underlying metric is obtained.Instead of prescribing that the raw node attributes should be used to approximate this metric, we propose that relevant node features, which capture the potentially complex interplay between attributes and topologies, can be learned.To do so, we suggest replacing raw node attributes with learned embeddings x GAT ui obtained from a Graph Attention Network (GAT) [25], denoted f Θ3 rep .These embeddings are computed via a message-passing procedure starting from the raw node attributes and the graph topology.</p>
<p>The method, which we refer to as Graph Attention-guided Routing for Decentralized Networks (GARDEN), is shown using pseudocode in Algorithm 1.The full set of model parameters Θ = {Θ i } 3 i=1 is trained implicitly as we take gradient descent steps over the combined episodic loss
t L (π) t + L (v)
t .The node embeddings are recalculated at the start of each episode.The notation [[•]] denotes the partial stopping of gradients, and H(p) denotes the entropy of a discrete distribution, given by i p i log(p i ).</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>Real-world Graphs.To assess the performance of decentralized graph strategies on real-world data, we consider several ego graphs from the Facebook social network [50] present in the SNAP [51] repository.These graphs depict individuals and their Facebook friendships.Each node is equipped Algorithm 1 Graph Attention-guided Routing for DEcentralized Networks (GARDEN).for w ∈ V do
Â = r + [[f Θ2 v ([x GAT u ′ ||m])]] − f Θ2 v ([x GAT u ||m]) 22: L (π) = − Â log f Θ1 π ([x GAT u ′ ||m]) − λH(π(u|m))
23:
L (v) = Â2 24: L = L + L (π) + L (v)
25:</p>
<p>end for</p>
<p>26:</p>
<p>Take gradient descent step on L w.r.t.Θ 27: end for with binary, anonymized attributes collected through surveys.Due to computational budget constraints, we select the largest connected components of 5 graphs such that they have between 100 and 600 nodes.High-level descriptive statistics for these graphs are presented in Table 4 in the Appendix.Synthetic Graphs.We additionally consider synthetically generated graphs that are both attributed and display homophily.This allows for the creation of a diverse range of graphs with varying degrees of sparsity, enabling evaluations under different synthetic conditions.We follow the generative graph construction procedure proposed by Kaiser and Hilgetag [52], which samples node attributes uniformly from a unit box [0, 1] d and creates edges stochastically according to the rule p((u, u ′ ) ∈ E) = max(1, βe −α∥xu−x u ′ ∥2 ), where α and β are scaling coefficients.</p>
<p>Baselines</p>
<p>The learned baselines we use are the MLPA2C and MLPA2CWD techniques as introduced in Section 3.2.Furthermore, we consider a suite of heuristic baselines that utilize homophily and graph structure for graph path search.The simplest baseline, GreedyWalker, selects the next node greedily based on the smallest Euclidean attribute distance:
π(u) = argmin u ′ ∈N (u) ∥x u ′ − x utgt ∥ 2 .
Given that deterministic policies may result in action loops, we generalize this to a stochastic agent (DistanceWalker) that acts via a SoftMax policy over attribute distances with a tuned temperature parameter:
π (τ ) (u ′ |u) = exp(−∥x u ′ −xu∥2/τ ) u ′′ ∈N (u) exp(−∥x u ′′ −xu∥2/τ )
. Similarly, we consider the stochastic ConnectionWalker agent, which uses a SoftMax policy over node degree:
π (τ ) (u ′ |u) = exp(deg(u ′ )/τ )
u ′′ ∈N (u) exp(deg(u ′′ )/τ ) .Lastly, the RandomWalker baseline selects uniformly at random between nodes from the current neighbourhood:
π(u ′ |u) = 1 deg(u) .
The stochastic agents use a temperature parameter τ to control greediness.To perform a fair comparison with our learned models, we individually tune the temperature for the DistanceWalker and ConnectionWalker models using a validation set for each graph.</p>
<p>Model Evaluation &amp; Selection</p>
<p>For evaluating models, we consider the following metrics:</p>
<ol>
<li>
<p>Mean Oracle Ratio Roracle : the ratio between episode length and the shortest path length averaged over all source-destination pairs;</p>
</li>
<li>
<p>Truncation Rate R trunc : % of episodes exceeding the truncation length T max ;</p>
</li>
<li>
<p>Win Rate R win : % of episodes where a given agent obtains the relative shortest path length, with ties broken randomly.</p>
</li>
</ol>
<p>To mitigate potential memorization of routes during training, especially when nodes are uniquely identifiable based on attributes, we partition the node set V into three disjoint groups: V train , V val , and V test at ratios of 80%/10%/10%.The source node u src is sampled uniformly at random from V , while the target node u tgt depends on whether training, validation, or testing is performed.This ensures that the agent cannot memorize the path to a target node since, by construction, it is not encountered during training.We always sample a "fresh" source-target pair during training, while for validation and evaluation the source-target pairs are serialized and stored (such that the performance evaluated over them is consistent).The Mean Oracle Ratio Roracle is used as the primary metric for model validation and evaluation.</p>
<p>Sensitivity Analysis of Graph Density Parameter</p>
<p>Given a constant graph size, reduced graph density diminishes available paths to a target [53].This intensifies exploration challenges and heightens the risk of truncated episodes, yielding sparser reward signals in training.Motivated by this rationale, we assess GARDEN across a set of generated graphs with diverse sparsity levels.We randomly generate 10 graph topologies for β ∈ {0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0} with number of nodes n = 200 and α = 30.We train GARDEN separately for each value of β and gauge its performance against the baselines.</p>
<p>Ablation of Node Representation</p>
<p>We assess our GNN-based model against alternative designs through an ablation study on synthetic graphs.Using five random seeds and fixed graph parameters (n = 200, α = 30, β = 5), we conduct experiments on our three model designs: the MLPA2C model using only the raw node attributes x, the MLPA2C variant incorporating both node attributes and degrees x WD , and the proposed GNN-based GARDEN method, which employs learned graph embeddings x GAT .</p>
<p>5 Experimental Results</p>
<p>Facebook Graphs</p>
<p>As shown in Table 1, we find that GARDEN significantly outperforms baselines across all the real-world datasets and metrics we have tested on.Given the variety of attribute dimensions and densities, as displayed in Table 4 in the Appendix, we may argue that in graphs with high amounts of latent structure, our model is robust to these factors.</p>
<p>In Figure 3 in the Appendix, we visualize the value function learned by GARDEN on these social network ego graphs.This highlights that the values obtained by GARDEN serve as a reliable proxy for graph distance, assigning highest values to nodes in the target's cluster or clusters with strong connectivity to the target's community.Furthermore, it demonstrates the interpretability of the proposed technique for graph path search.</p>
<p>In Table 5 in the Appendix, we also include a runtime analysis to quantify the scalability of the proposed method.These results show that GARDEN maintains sub-millisecond per-action inference times even with CPU-only execution as the graph size increases, thanks to its decentralized nature.We therefore expect the method to maintain fast inference times even in substantially larger topologies.In Figure 1, we show the validation performance as a function of the Soft-Max temperature τ of the stochastic DistanceWalker and Connection-Walker baselines.For both methods, a middle-ground temperature value yields shorter path lengths.Furthermore, performance is more sensitive to τ for the DistanceWalker method.</p>
<p>The sensitivity analysis for the synthetic graph density parameter β is shown in Figure 2. GARDEN consistently matches or surpasses baseline performance for all β values for both Mean Oracle Ratio and Win Rate metrics.However, DistanceWalker outperforms our model for higher β in Truncation Rate.In this setting, DistanceWalker benefits from knowledge of the "true" node attributes determining link generation and high β values leading to most connections being realized.This is in contrast with the gap on real-world datasets, for which this metric is not available: indeed, GARDEN may be seen as recovering an underlying "hidden metric".</p>
<p>Ablation of Node Representation</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we have considered the problem of decentralized search in graphs, which is motivated by privacy, scalability, and dynamicity requirements of many network modeling scenarios.Despite the lack of a central view of the network, the homophily and community structure observed in many networks can allow for decentralized agents to find short paths to a given target, as famously demonstrated by the Milgram experiment [12].</p>
<p>We have proposed the GARDEN method to address this problem, which trains agents in a centralized fashion and allows for decentralized policy execution.Our approach is based on message routing policies that are learned using Reinforcement Learning, paired with node features learned by a Graph Neural Network specifically for the task.Our results show that our method can outperform stochastic routing policies based on attribute or structural information alone.It is possible to observe significant improvements when searching on real-world social network graphs with non-trivial latent structures and high-dimensional node attributes.</p>
<p>For simplicity, we have considered a memory-less search procedure that is akin to a biased random walk.This means that the agents cannot react to the unsuccessful exploration of a given region of the graph before arriving at a previously visited node, and the same distribution over actions will apply independently of the historical trajectory.The problem formulation can be extended by including the history of visited nodes in the message m and forbidding using already-visited nodes as actions.</p>
<p>RNNs may be used to encode the history as input to the learned models, as performed in other learning-based graph search works [21,23].</p>
<p>We believe that our results provide evidence for a sort of "hidden metric" hypothesis, showing how a latent feature space amenable for graph navigation can be recovered by reward-driven learning.An interesting aspect that can be considered by future work is to compare these emergent representations with the means in which individuals take decisions for routing messages in experiments conducted over real social networks.</p>
<p>while u ̸ = u tgt and t &lt; T max do 13:Sample action a u→u ′ ∼ π u (• |m) 14: Move message to node u ′ , observe reward r 15: Store transition (u, u ′ , r) in B episode loss L = 0 20: for (u, u ′ , r) in B do 21:</p>
<p>Figure 1 :
1
Figure 1: Mean Oracle Ratio obtained by the stochastic baselines on the validation dataset as a function of the temperature τ for varying values of β.</p>
<p>Figure 2 :
2
Figure 2: Metric values obtained by the methods as a function of the synthetic graph density parameter β.GARDEN generally performs best, but it is notably surpassed by DistanceWalker in the truncation rate for high values of β.</p>
<p>Figure 3 :
3
Figure 3: Visualization of the learned value function v(u, u tgt ) learned by GARDEN for each node u (left) and preferability score −∥x u − x utgt ∥ 2 /τ of the DistanceWalker baseline (right) for the social network graphs.The black arrows indicate the target node, while the color intensities of the other nodes are proportional to the value function learned by GARDEN (left) or baseline score (right).Concretely, dark red nodes indicate high proximity to the target, while dark blue nodes reflect low proximity.GARDEN recovers meaningful values for performing graph navigation, effectively leveraging proximity in both node attributes and topological structure.</p>
<p>1 :
1
Input: Policy Network f Θ1 π , Value Network f Θ2 v , Graph Representation Network f Θ3 rep , Ego Graphs {G u |u ∈ V },entropy regularization coefficient λ, discount factor γ. 2: Output: Learned policy, value and representation networks f Θ1 π , f Θ2 v , f Θ3 rep .3: Randomly initialize model parameters Θ = {Θ i } 3 i=1 .4: for i = 1 to N episodes do
5:Initialize episode buffer B6:Sample starting node u, target node u tgt7:</p>
<p>Table 1 :
1
Metrics obtained by the methods on the 5 social network ego graphs.GARDEN consistently yields the best performance, followed by the DistanceWalker method.
MetricGreedyWalker DistanceWalker ConnectionWalker RandomWalker GARDEN (Ours)Roracle (↓)27.17 ± 1.0915.98 ± 0.7933.39 ± 1.1533.17 ± 1.1610.95 ± 0.7631.78 ± 1.2213.01 ± 0.9034.62 ± 1.1734.27 ± 1.189.89 ± 0.7827.35 ± 1.199.99 ± 0.7838.84 ± 1.4238.88 ± 1.438.74 ± 0.7625.69 ± 0.8814.33 ± 0.6528.25 ± 1.0028.01 ± 1.0112.08 ± 0.7128.29 ± 0.6622.71 ± 0.7028.11 ± 0.6928.22 ± 0.6716.97 ± 0.74R trunc (↓)79.0039.7076.9076.7014.9078.0019.5068.5064.1012.6070.0013.9069.8067.2013.5088.0044.0081.5079.4040.1096.0072.7089.5090.0048.20R win (↑)7.9026.202.702.5060.7010.9029.904.303.9051.0011.9033.801.502.0050.806.1032.105.308.2048.302.2024.907.6010.5054.80
5.2 Sensitivity Analysis of Graph Density and Temperature Parameters</p>
<p>Table 2 :
2
Ablation results obtained by pairing different node representations with the proposed problem formulation and Reinforcement Learning algorithm.
AgentRoracle (↓) R trunc (↓) R win (↑)GARDEN1.95±0.09 1.6834.44MLPA2CWD 2.23±0.13 2.9430.78MLPA2C2.31±0.12 4.3234.78
A AppendixA.1 Implementation Details Our implementation is publicly available at https://github.com/flxclxc/rl-graph-search.Please see the README.mdfile for instructions on how to set up the dependencies, download the publicly available data, and run the code.We train our models using the Adam optimizer[54]for 200, 000 episodes, evaluating performance every 100 episodes on the serialized validation set.Early stopping is applied based on the validation loss Roracle .Unless otherwise stated, we train and evaluate models over 10 random seeds, reporting confidence intervals where appropriate.Table3presents the hyperparameter configuration shared across the three model designs.We fix γ = 0.99 and the maximum episode length T max = 100.Lastly, when providing node input features to the GAT, we append a binary indicator variable that signals that a particular node u is the center of the ego graph to the raw attributes defined as [x w ||I[w = u]].This is used to distinguish the node from which the message must be sent.We note that, while the GAT-based model contains 3 layers, we only use the ego graphs centered around the nodes to compute the embeddings, hence ensuring that only 1-hop visibility is provided.The first layer is fed the raw node attributes, while the second and third layers use the "latent" embeddings constructed by the previous layer.The message-passing therefore occurs over the same 1-hop subgraph in all the layers.A.2 Additional Tables and FiguresSummary statistics.Statistics about the considered real-world graphs are shown in Table4.Runtime analysis.We carry out a runtime analysis to examine the scalability and computational cost of GARDEN.In terms of methodology, we calculate the mean action time, i.e., the elapsed wall clock time measured in milliseconds from the arrival of a message at a node until an action is chosen.The measurements are averaged over 100 target nodes and are carried out using an Intel i7-11800H CPU.We note that the execution of GARDEN also involves a time overhead for creating the local ego graph embeddings from f Θ3 rep , which is reported separately.The results are shown in Table5.They demonstrate that both the embedding overhead time and action time increase slightly as the number of neighbors grows, but stays reasonably bounded.As it is expected, a clear hierarchy is present in which the neural network-based models require more inference time, followed by the heuristic attribute-based baselines, and finally the simple random walk baseline.This analysis highlights the fact that GARDEN maintains sub-millisecond inference    times even with CPU-only execution as the graph size increases, thanks to its decentralized nature.Notably, the total inference time is lower on the largest 532-node graph compared to the smallest 148-node graph, owing to differences in density.We therefore expect the method to maintain fast inference times even in substantially larger topologies.Interpretability of the learned value function.In Figure3, we plot GARDEN's learned value function f Θ2 v across the social network ego graphs.Brighter colors indicate a higher estimated value function relative to the target node, which is indicated with a black arrow and chosen randomly from the respective test sets.For comparison, we also plot the implicit preferability score −∥x u − x utgt ∥ 2 /τ generated by the best-performing baseline, DistanceWalker, for the same source-target pairs.DistanceWalker struggles with Euclidean pairwise attribute distance due to high dimensionality and sparsity of node attributes.Conversely, values obtained by GARDEN serve as a reliable proxy for graph distance, assigning highest values to nodes in the target's cluster or clusters with strong connectivity to the target's community.
Yago: a core of semantic knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, WWW'07. 2007</p>
<p>Planning Algorithms. M Steven, Lavalle, 2006Cambridge University Press</p>
<p>Identity and search in social networks. Duncan J Watts, Peter Sheridan Dodds, M E J Newman, Science. 29655712002</p>
<p>Peer-to-Peer: Harnessing the Benefits of a Disruptive Technology. Andrew Oram, 2001O'Reilly Media Inc</p>
<p>Error and attack tolerance of complex networks. Réka Albert, Hawoong Jeong, Albert-László Barabási, Nature. 40667942000</p>
<p>Fault-tolerant routing in peer-to-peer systems. James Aspnes, Zoë Diamadi, Gauri Shah, PODC'02. 2002</p>
<p>Network robustness and fragility: Percolation on random graphs. Mark Ej Duncan S Callaway, Steven H Newman, Duncan J Strogatz, Watts, Physical Review Letters. 852554682000</p>
<p>Making Gnutella-like P2P systems scalable. Yatin Chawathe, Sylvia Ratnasamy, Lee Breslau, Nick Lanham, Scott Shenker, SIGCOMM'03. 2003</p>
<p>Mobile ad hoc networks. Handbook of Wireless Networks and Mobile Computing. Silvia Giordano, 2002</p>
<p>David Easley, Jon Kleinberg, Networks, Crowds, and Markets: Reasoning about a Highly Connected World. Cambridge University Press2010</p>
<p>Stanley Wasserman, Katherine Faust, Social Network Analysis: Methods and Applications. Cambridge University Press1994</p>
<p>An experimental study of the small world problem. Jeffrey Travers, Stanley Milgram, Sociometry. 32491969</p>
<p>Six degrees of separation. John Guare, The Contemporary Monologue: Men. Routledge2016</p>
<p>Six Degrees: The Science of a Connected Age. J Duncan, Watts, 2004WW Norton &amp; Company</p>
<p>Birds of a feather: Homophily in social networks. Lynn Miller Mcpherson, James M Smith-Lovin, Cook, Annual Review of Sociology. 2712001</p>
<p>Community detection in graphs. Santo Fortunato, Physics Reports. 4863-52010</p>
<p>The strength of weak ties. Mark S Granovetter, American Journal of Sociology. 7861973</p>
<p>The future of weak ties. Sinan Aral, American Journal of Sociology. 12162016</p>
<p>The structure and function of complex networks. E J Mark, Newman, SIAM Review. 4522003</p>
<p>Learning heuristic search via imitation. Mohak Bhardwaj, Sanjiban Choudhury, Sebastian Scherer, CoRL'17. 2017</p>
<p>Learning graph search heuristics. Michal Pándy, Weikang Qiu, Gabriele Corso, Petar Veličković, Zhitao Ying, Jure Leskovec, Pietro Liò, LoG'22. 202229</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew Mccallum, ICLR'18201823</p>
<p>M-walk: Learning to walk over graphs using Monte Carlo Tree Search. Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, Jianfeng Gao, Advances in Neural Information Processing Systems. 2018319</p>
<p>Counterfactual multi-agent policy gradients. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson, AAAI'18. 2018</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, ICLR'18. 201825</p>
<p>Introduction to Algorithms. H Thomas, Charles E Cormen, Ronald L Leiserson, Clifford Rivest, Stein, 2022MIT Press</p>
<p>An experimental study of search in global social networks. Roby Peter Sheridan Dodds, Duncan J Muhamad, Watts, Science. 30156342003</p>
<p>The reversal small-world experiment. D Peter, H Killworth, Bernard Russell, Social networks. 121978</p>
<p>Emergence of scaling in random networks. Albert-László Barabási, Réka Albert, Science. 28654391999</p>
<p>Navigability of complex networks. Marián Boguñá, Dmitri Krioukov, Kimberly C Claffy, Nature Physics. 512009</p>
<p>The small-world phenomenon: An algorithmic perspective. Jon Kleinberg, STOC'00. 2000</p>
<p>Decentralized search in networks using homophily and degree disparity. Özgür Simsek, David Jensen, IJCAI'05. 2005</p>
<p>Graph reinforcement learning for combinatorial optimization: A survey and unifying perspective. Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi, Transactions on Machine Learning Research (TMLR). 32024</p>
<p>Packet routing in dynamically changing networks: A reinforcement learning approach. Justin Boyan, Michael Littman, Advances in Neural Information Processing Systems. 631993</p>
<p>Predictive q-routing: A memory-based reinforcement learning approach to adaptive traffic control. Samuel Choi, Dit-Yan Yeung, Advances in Neural Information Processing Systems. 81995</p>
<p>Reinforcement learning for adaptive routing. Leonid Peshkin, Virginia Savova, IJCNN'022002</p>
<p>Learning to route. Asaf Valadarsky, Michael Schapira, Dafna Shahaf, Aviv Tamar, HotNets'17. 2017</p>
<p>GDDR: GNN-based Data-Driven Routing. Oliver Hope, Eiko Yoneki, ICDCS'21. 2021</p>
<p>Graph Representation Learning. William L Hamilton, 2020Morgan &amp; Claypool Publishers</p>
<p>Pere Barlet-Ros, and Albert Cabellos-Aparicio. Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case. Paul Almasan, José Suárez-Varela, Krzysztof Rusek, Computer Communications. 19632022</p>
<p>Experience-driven networking: A deep reinforcement learning based approach. Zhiyuan Xu, Jian Tang, Jingsong Meng, Weiyi Zhang, Yanzhi Wang, Chi Harold Liu, Dejun Yang, INFOCOM'18. 2018</p>
<p>CFR-RL: Traffic engineering with reinforcement learning in SDN. Junjie Zhang, Minghao Ye, Zehua Guo, Chen-Yu Yen, H Jonathan Chao, IEEE Journal on Selected Areas in Communications. 38102020</p>
<p>Intrinsically motivated graph exploration using network theories of human curiosity. P Shubhankar, Mathieu Patankar, Juan Ouellet, Alejandro Cervino, Kieran A Ribeiro, Dani S Murphy, Bassett, 202323</p>
<p>Traversing knowledge graphs in vector space. Kelvin Guu, John Miller, Percy Liang, EMNLP'15. 2015</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in Games. 412012</p>
<p>Learning to walk with dual agents for knowledge graph reasoning. Denghui Zhang, Zixuan Yuan, Hao Liu, Hui Xiong, AAAI'22. 2022</p>
<p>A Concise Introduction to Decentralized POMDPs. Frans Oliehoek, Christopher Amato, 2016Springer</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, Advances in Neural Information Processing Systems. 2017OpenAI Pieter Abbeel, and Igor Mordatch</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, ICML'16. 2016</p>
<p>Learning to discover social circles in ego networks. Jure Leskovec, Julian Mcauley, Advances in Neural Information Processing Systems. 2012</p>
<p>Jure Leskovec, Andrej Krevl, SNAP Datasets: Stanford large network dataset collection. June 2014. 5</p>
<p>Spatial growth of real-world networks. Marcus Kaiser, Claus C Hilgetag, Physical Review E. 693361032004</p>
<p>Random walks on graphs. László Lovász, Combinatorics, Paul Erdős is Eighty. 19932</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR'15. 201512</p>            </div>
        </div>

    </div>
</body>
</html>
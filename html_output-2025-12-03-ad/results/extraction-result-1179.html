<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1179 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1179</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1179</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-253244045</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.00177v1.pdf" target="_blank">Learning to Navigate Wikipedia by Taking Random Walks</a></p>
                <p><strong>Paper Abstract:</strong> A fundamental ability of an intelligent web-based agent is seeking out and acquiring new information. Internet search engines reliably find the correct vicinity but the top results may be a few links away from the desired target. A complementary approach is navigation via hyperlinks, employing a policy that comprehends local content and selects a link that moves it closer to the target. In this paper, we show that behavioral cloning of randomly sampled trajectories is sufficient to learn an effective link selection policy. We demonstrate the approach on a graph version of Wikipedia with 38M nodes and 387M edges. The model is able to efficiently navigate between nodes 5 and 20 steps apart 96% and 92% of the time, respectively. We then use the resulting embeddings and policy in downstream fact verification and question answering tasks where, in combination with basic TF-IDF search and ranking methods, they are competitive results to the state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1179.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1179.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wiki-Paragraph Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikipedia paragraph-level hyperlink graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directed text-world formed by splitting Wikipedia articles into paragraph-sized nodes and connecting them via organic hyperlinks, entity links, and next/previous paragraph edges; used as the navigation environment for all experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikipedia paragraph graph (2017 / 2018 snapshots)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Web-domain text world where each node is a paragraph-sized block (≈100 words) from Wikipedia; edges are directed and include organic hyperlinks, additional entity-link edges, and next/previous paragraph links. Domain: encyclopedic web navigation / information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Directed hyperlinks (one-way links) and next/previous links; no explicit locks/keys but directedness creates asymmetric accessibility and nodes with no incoming edges ('dead-ends') can act like traps for reverse-walks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Well-connected giant component with power-law degree distribution (few hub nodes); median shortest-path length ≈ 15; connectivity sufficient to reach most nodes in tens of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>2017: ~36.3M nodes, ~359M edges; 2018: ~38.5M nodes, ~387M edges (paragraph-level nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>median shortest-path length; distribution of shortest path lengths; success rate within bounded budget B; mean shortest-path from BM25 start to target (downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>median path length = 15 (unit: hops); BM25 mean shortest-path to target: FEVER 4.3 hops, NQ 5.1 hops.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Directedness and presence of 'dead-ends' affect sampled trajectory diversity and hence learning; power-law hubs and short median path length make local navigation feasible (most targets reachable within ~15 hops).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Paper compares behavior when sampling trajectories forward vs reverse vs shortest-path and contrasts small (200k) vs full (∼38M) graphs: (1) reverse (backward) trajectory sampling suffers due to dead-ends and produces low-diversity training data and poor generalization to forward navigation; (2) random-forward trajectories generalize better and are cheap to compute; (3) shortest-path trajectories give best performance but are computationally intractable at full graph scale.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Not applicable for the environment entry (see policy-specific entries).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1179.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1179.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Forward Behavioral Cloning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-conditioned navigation training method that behavioral-clones trajectories sampled by forward random walks (start uniformly, walk T steps) to learn page embeddings and a link-selection policy for text-world navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikipedia paragraph graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same paragraph-level Wikipedia graph; RFBC trains on randomly sampled forward walk trajectories to produce a policy p_theta(n_{t+1} | n_t, n_g) that selects outgoing hyperlink actions conditioned on current and goal text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Directed hyperlinks (one-way).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse web-graph with power-law degree distribution; average out-degree (E_out) small (~O(10^1)); giant connected component present.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Experiments run on small subsampled graph (200k nodes) and full graphs (2017/2018 with ~36M–38M nodes and ~359M–387M edges).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RFBC (behavioral cloning policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Goal-conditioned policy that embeds current node, candidate outgoing link texts, and goal text via a Transformer; the action distribution is proportional to exp(e_tg · a_i) where e_tg is combined state-goal embedding. Architectures used: 1-layer feed-forward policy and a 4-layer Transformer policy that optionally includes trajectory history.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>success rate to reach target within time budget B (steps to goal / success@B); downstream recall/F1 for retrieved evidence from visited nodes; steps-to-goal T in T-step navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported success rates: on full graph (Embed-trained + Feed-forward) navigation success = 96.1% @5 hops, 94.1% @10 hops, 89.8% @20 hops; sentence-search success (find node given one sentence) = 96.3% @5, 92.8% @10, 90.2% @20. On small 200k graph: RFBC navigation = 85.3% @5, 76.4% @10, 67.5% @20.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>See exploration_efficiency_value (e.g., up to 96.1% for 5-hop tasks; ≈92% for 20-hop tasks in best config).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reactive feed-forward policy works best for short/medium horizons; Transformer-based policy (memory of trajectory) helps for longer trajectories (20+ steps).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>RFBC trained with forward walks performs robustly across the web-graph topology; performance degrades with increasing shortest-path distance (longer T) but remains high when trained at scale. Training on shortest-path trajectories yields superior performance but is infeasible at full graph scale; reverse/backward trajectories degrade when many nodes have no incoming edges (dead-ends).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Paper's empirical comparisons show RFBC (random-forward sampling) generalizes better across forward-evaluation than reverse-sampled training; shortest-path samples provide best accuracy (especially at 20-step tasks) but cannot be computed at scale; larger full-graph training improved performance relative to the small graph (more data reduces overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Feed-forward (stateless) policies perform best for most tasks and shorter trajectories; adding trajectory history via a Transformer improves performance on longer-horizon navigation. Policies must exploit semantic embeddings (replacing them with random features collapses performance nearly to chance).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1179.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1179.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trajectory Distributions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Forward, Reverse (Backward), and Random Shortest-Path trajectory sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three methods for generating training trajectories for behavioral cloning: (1) Random forward walks (start uniform, walk forward T steps); (2) Reverse trajectories (sample target uniform, walk backwards on transposed graph); (3) Random shortest-paths (sample start/target pairs at exact shortest path length T and take the shortest path).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikipedia paragraph graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sampling procedures used to produce (n_0,...,n_T) expert trajectories on the paragraph-level Wikipedia graph for behavioral cloning pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Directed links create asymmetries which make reverse-walk sampling sensitive to nodes with no incoming edges.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Same web-graph characteristics as environment entry; reverse sampling is sensitive to nodes with zero in-degree.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Applies at both 200k subsampled graphs and full ~38M node graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Used to generate BC training data (not an agent itself)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sampling distributions that define the 'expert' trajectories which the behavioral-cloning loss attempts to imitate.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>generalization of learned policy to forward and reverse evaluation; success rate on T-step navigation tasks when trained with each sampling scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Empirical points: (small graph) Forward-training/generalization: Random-forward training -> 5/10/20 step forward accuracy 85.3 / 76.4 / 67.5; Reverse-training achieves high reverse-eval accuracy (91.3/87.9/87.3 on reverse sampling) but very poor forward generalization (2.3/0.5/0.2). Shortest-path training yields highest forward and reverse accuracies (e.g., ~86.7/85.0/84.6 forward on 5/10/20).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Training with shortest-path trajectories yields policies closest to optimal for the sampled T but impractical at scale; random-forward trajectory sampling is a practical and effective surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Reverse sampling overfits in presence of many nodes with no incoming edges (dead-ends) because backward random walks get stuck; shortest-path sampling aligns the training objective with optimal paths and improves long-horizon performance; random-forward sampling provides a tractable compromise with high practical performance and good generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Direct comparison of the three trajectory schemes on the same graph shows: shortest-path > random-forward > reverse (for forward-evaluation). Reverse training performs well only on reverse-evaluation because of skewed/more constrained target distributions induced by dead-ends.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies trained with shortest-path trajectories implicitly learn to follow paths aligned with graph shortest paths; policies trained with random-forward walks learn more broadly useful local heuristics. Reverse-trained policies can exploit artifacts of dead-ends and thus generalize poorly to forward-navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1179.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1179.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy architectures (FF vs Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feed-forward (reactive) vs Transformer (history-aware) navigation policies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two policy implementations evaluated: a single-layer feed-forward network that combines current and goal embeddings to score outgoing actions, and a 4-layer Transformer that processes the trajectory history plus goal to produce action distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikipedia paragraph graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Architectures for p_theta(n_{t+1} | n_t, n_g) used during evaluation on T-step navigation and sentence-search tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Directed hyperlinks; policies must select among variable-size outgoing action sets.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>As above: sparse web-graph with hubs and median path ~15.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Models evaluated on both 200k and full ~38M node graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Feed-forward policy; Transformer policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Feed-forward: concatenates φ(s_t) and φ(s_g), passes through 1-layer MLP to get e_tg, scores candidate link embeddings via dot-product. Transformer: a 4-layer transformer ingests embeddings of the trajectory [φ(s_0),...,φ(s_t),φ(s_g)] and outputs e_tg used to score actions; effectively provides memory of visited nodes and context.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>navigation success rate at varying horizon (5/10/20 steps); sentence-search success.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Empirical: on full graph with trained embeddings: Feed-forward: navigation 96.1/94.1/89.8 (@5/10/20), sentence-search 96.3/92.8/90.2. Transformer: navigation 93.5/90.6/92.2, sentence-search 93.9/86.3/79.7. Transformer outperforms feed-forward on some longer-horizon tasks (notably 20-step navigation in some configs) while feed-forward is superior on sentence-search.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Feed-forward (reactive) optimal for shorter/medium horizons and sentence-search; Transformer (history-aware) advantageous for longer-horizon navigation where trajectory context and memory help.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Longer effective graph distances (higher T) favor policies that incorporate trajectory history (Transformer); shorter distances and tasks requiring fine-grained semantic matching (sentence search) favor lightweight feed-forward scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparison concerns trajectory length (task horizon) rather than distinct graph topologies: feed-forward best for shorter horizons while Transformer helps for longer horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policy memory (history) becomes more important as required navigation distance increases; stateless/reactive scoring suffices in many practical web-navigation cases because semantic embeddings carry strong local signal.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Webnav: A new large-scale task for natural language based sequential decision making <em>(Rating: 2)</em></li>
                <li>Wikispeedia: An online game for inferring semantic distances between concepts <em>(Rating: 2)</em></li>
                <li>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning <em>(Rating: 1)</em></li>
                <li>M-walk: Learning to walk over graphs using monte carlo tree search <em>(Rating: 1)</em></li>
                <li>Learning to retrieve reasoning paths over wikipedia graph for question answering <em>(Rating: 1)</em></li>
                <li>Deeppath: A reinforcement learning method for knowledge graph reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1179",
    "paper_id": "paper-253244045",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Wiki-Paragraph Graph",
            "name_full": "Wikipedia paragraph-level hyperlink graph",
            "brief_description": "A directed text-world formed by splitting Wikipedia articles into paragraph-sized nodes and connecting them via organic hyperlinks, entity links, and next/previous paragraph edges; used as the navigation environment for all experiments in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Wikipedia paragraph graph (2017 / 2018 snapshots)",
            "environment_description": "Web-domain text world where each node is a paragraph-sized block (≈100 words) from Wikipedia; edges are directed and include organic hyperlinks, additional entity-link edges, and next/previous paragraph links. Domain: encyclopedic web navigation / information retrieval.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Directed hyperlinks (one-way links) and next/previous links; no explicit locks/keys but directedness creates asymmetric accessibility and nodes with no incoming edges ('dead-ends') can act like traps for reverse-walks.",
            "graph_connectivity": "Well-connected giant component with power-law degree distribution (few hub nodes); median shortest-path length ≈ 15; connectivity sufficient to reach most nodes in tens of steps.",
            "environment_size": "2017: ~36.3M nodes, ~359M edges; 2018: ~38.5M nodes, ~387M edges (paragraph-level nodes).",
            "agent_name": null,
            "agent_description": null,
            "exploration_efficiency_metric": "median shortest-path length; distribution of shortest path lengths; success rate within bounded budget B; mean shortest-path from BM25 start to target (downstream)",
            "exploration_efficiency_value": "median path length = 15 (unit: hops); BM25 mean shortest-path to target: FEVER 4.3 hops, NQ 5.1 hops.",
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Directedness and presence of 'dead-ends' affect sampled trajectory diversity and hence learning; power-law hubs and short median path length make local navigation feasible (most targets reachable within ~15 hops).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Paper compares behavior when sampling trajectories forward vs reverse vs shortest-path and contrasts small (200k) vs full (∼38M) graphs: (1) reverse (backward) trajectory sampling suffers due to dead-ends and produces low-diversity training data and poor generalization to forward navigation; (2) random-forward trajectories generalize better and are cheap to compute; (3) shortest-path trajectories give best performance but are computationally intractable at full graph scale.",
            "policy_structure_findings": "Not applicable for the environment entry (see policy-specific entries).",
            "uuid": "e1179.0"
        },
        {
            "name_short": "RFBC",
            "name_full": "Random Forward Behavioral Cloning",
            "brief_description": "A goal-conditioned navigation training method that behavioral-clones trajectories sampled by forward random walks (start uniformly, walk T steps) to learn page embeddings and a link-selection policy for text-world navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Wikipedia paragraph graph",
            "environment_description": "Same paragraph-level Wikipedia graph; RFBC trains on randomly sampled forward walk trajectories to produce a policy p_theta(n_{t+1} | n_t, n_g) that selects outgoing hyperlink actions conditioned on current and goal text embeddings.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Directed hyperlinks (one-way).",
            "graph_connectivity": "Sparse web-graph with power-law degree distribution; average out-degree (E_out) small (~O(10^1)); giant connected component present.",
            "environment_size": "Experiments run on small subsampled graph (200k nodes) and full graphs (2017/2018 with ~36M–38M nodes and ~359M–387M edges).",
            "agent_name": "RFBC (behavioral cloning policy)",
            "agent_description": "Goal-conditioned policy that embeds current node, candidate outgoing link texts, and goal text via a Transformer; the action distribution is proportional to exp(e_tg · a_i) where e_tg is combined state-goal embedding. Architectures used: 1-layer feed-forward policy and a 4-layer Transformer policy that optionally includes trajectory history.",
            "exploration_efficiency_metric": "success rate to reach target within time budget B (steps to goal / success@B); downstream recall/F1 for retrieved evidence from visited nodes; steps-to-goal T in T-step navigation tasks.",
            "exploration_efficiency_value": "Reported success rates: on full graph (Embed-trained + Feed-forward) navigation success = 96.1% @5 hops, 94.1% @10 hops, 89.8% @20 hops; sentence-search success (find node given one sentence) = 96.3% @5, 92.8% @10, 90.2% @20. On small 200k graph: RFBC navigation = 85.3% @5, 76.4% @10, 67.5% @20.",
            "success_rate": "See exploration_efficiency_value (e.g., up to 96.1% for 5-hop tasks; ≈92% for 20-hop tasks in best config).",
            "optimal_policy_type": "Reactive feed-forward policy works best for short/medium horizons; Transformer-based policy (memory of trajectory) helps for longer trajectories (20+ steps).",
            "topology_performance_relationship": "RFBC trained with forward walks performs robustly across the web-graph topology; performance degrades with increasing shortest-path distance (longer T) but remains high when trained at scale. Training on shortest-path trajectories yields superior performance but is infeasible at full graph scale; reverse/backward trajectories degrade when many nodes have no incoming edges (dead-ends).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Paper's empirical comparisons show RFBC (random-forward sampling) generalizes better across forward-evaluation than reverse-sampled training; shortest-path samples provide best accuracy (especially at 20-step tasks) but cannot be computed at scale; larger full-graph training improved performance relative to the small graph (more data reduces overfitting).",
            "policy_structure_findings": "Feed-forward (stateless) policies perform best for most tasks and shorter trajectories; adding trajectory history via a Transformer improves performance on longer-horizon navigation. Policies must exploit semantic embeddings (replacing them with random features collapses performance nearly to chance).",
            "uuid": "e1179.1"
        },
        {
            "name_short": "Trajectory Distributions",
            "name_full": "Random Forward, Reverse (Backward), and Random Shortest-Path trajectory sampling",
            "brief_description": "Three methods for generating training trajectories for behavioral cloning: (1) Random forward walks (start uniform, walk forward T steps); (2) Reverse trajectories (sample target uniform, walk backwards on transposed graph); (3) Random shortest-paths (sample start/target pairs at exact shortest path length T and take the shortest path).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Wikipedia paragraph graph",
            "environment_description": "Sampling procedures used to produce (n_0,...,n_T) expert trajectories on the paragraph-level Wikipedia graph for behavioral cloning pretraining.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Directed links create asymmetries which make reverse-walk sampling sensitive to nodes with no incoming edges.",
            "graph_connectivity": "Same web-graph characteristics as environment entry; reverse sampling is sensitive to nodes with zero in-degree.",
            "environment_size": "Applies at both 200k subsampled graphs and full ~38M node graphs.",
            "agent_name": "Used to generate BC training data (not an agent itself)",
            "agent_description": "Sampling distributions that define the 'expert' trajectories which the behavioral-cloning loss attempts to imitate.",
            "exploration_efficiency_metric": "generalization of learned policy to forward and reverse evaluation; success rate on T-step navigation tasks when trained with each sampling scheme.",
            "exploration_efficiency_value": "Empirical points: (small graph) Forward-training/generalization: Random-forward training -&gt; 5/10/20 step forward accuracy 85.3 / 76.4 / 67.5; Reverse-training achieves high reverse-eval accuracy (91.3/87.9/87.3 on reverse sampling) but very poor forward generalization (2.3/0.5/0.2). Shortest-path training yields highest forward and reverse accuracies (e.g., ~86.7/85.0/84.6 forward on 5/10/20).",
            "success_rate": null,
            "optimal_policy_type": "Training with shortest-path trajectories yields policies closest to optimal for the sampled T but impractical at scale; random-forward trajectory sampling is a practical and effective surrogate.",
            "topology_performance_relationship": "Reverse sampling overfits in presence of many nodes with no incoming edges (dead-ends) because backward random walks get stuck; shortest-path sampling aligns the training objective with optimal paths and improves long-horizon performance; random-forward sampling provides a tractable compromise with high practical performance and good generalization.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Direct comparison of the three trajectory schemes on the same graph shows: shortest-path &gt; random-forward &gt; reverse (for forward-evaluation). Reverse training performs well only on reverse-evaluation because of skewed/more constrained target distributions induced by dead-ends.",
            "policy_structure_findings": "Policies trained with shortest-path trajectories implicitly learn to follow paths aligned with graph shortest paths; policies trained with random-forward walks learn more broadly useful local heuristics. Reverse-trained policies can exploit artifacts of dead-ends and thus generalize poorly to forward-navigation.",
            "uuid": "e1179.2"
        },
        {
            "name_short": "Policy architectures (FF vs Transformer)",
            "name_full": "Feed-forward (reactive) vs Transformer (history-aware) navigation policies",
            "brief_description": "Two policy implementations evaluated: a single-layer feed-forward network that combines current and goal embeddings to score outgoing actions, and a 4-layer Transformer that processes the trajectory history plus goal to produce action distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Wikipedia paragraph graph",
            "environment_description": "Architectures for p_theta(n_{t+1} | n_t, n_g) used during evaluation on T-step navigation and sentence-search tasks.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Directed hyperlinks; policies must select among variable-size outgoing action sets.",
            "graph_connectivity": "As above: sparse web-graph with hubs and median path ~15.",
            "environment_size": "Models evaluated on both 200k and full ~38M node graphs.",
            "agent_name": "Feed-forward policy; Transformer policy",
            "agent_description": "Feed-forward: concatenates φ(s_t) and φ(s_g), passes through 1-layer MLP to get e_tg, scores candidate link embeddings via dot-product. Transformer: a 4-layer transformer ingests embeddings of the trajectory [φ(s_0),...,φ(s_t),φ(s_g)] and outputs e_tg used to score actions; effectively provides memory of visited nodes and context.",
            "exploration_efficiency_metric": "navigation success rate at varying horizon (5/10/20 steps); sentence-search success.",
            "exploration_efficiency_value": "Empirical: on full graph with trained embeddings: Feed-forward: navigation 96.1/94.1/89.8 (@5/10/20), sentence-search 96.3/92.8/90.2. Transformer: navigation 93.5/90.6/92.2, sentence-search 93.9/86.3/79.7. Transformer outperforms feed-forward on some longer-horizon tasks (notably 20-step navigation in some configs) while feed-forward is superior on sentence-search.",
            "success_rate": null,
            "optimal_policy_type": "Feed-forward (reactive) optimal for shorter/medium horizons and sentence-search; Transformer (history-aware) advantageous for longer-horizon navigation where trajectory context and memory help.",
            "topology_performance_relationship": "Longer effective graph distances (higher T) favor policies that incorporate trajectory history (Transformer); shorter distances and tasks requiring fine-grained semantic matching (sentence search) favor lightweight feed-forward scoring.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "Comparison concerns trajectory length (task horizon) rather than distinct graph topologies: feed-forward best for shorter horizons while Transformer helps for longer horizons.",
            "policy_structure_findings": "Policy memory (history) becomes more important as required navigation distance increases; stateless/reactive scoring suffices in many practical web-navigation cases because semantic embeddings carry strong local signal.",
            "uuid": "e1179.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Webnav: A new large-scale task for natural language based sequential decision making",
            "rating": 2,
            "sanitized_title": "webnav_a_new_largescale_task_for_natural_language_based_sequential_decision_making"
        },
        {
            "paper_title": "Wikispeedia: An online game for inferring semantic distances between concepts",
            "rating": 2,
            "sanitized_title": "wikispeedia_an_online_game_for_inferring_semantic_distances_between_concepts"
        },
        {
            "paper_title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning",
            "rating": 1,
            "sanitized_title": "go_for_a_walk_and_arrive_at_the_answer_reasoning_over_paths_in_knowledge_bases_using_reinforcement_learning"
        },
        {
            "paper_title": "M-walk: Learning to walk over graphs using monte carlo tree search",
            "rating": 1,
            "sanitized_title": "mwalk_learning_to_walk_over_graphs_using_monte_carlo_tree_search"
        },
        {
            "paper_title": "Learning to retrieve reasoning paths over wikipedia graph for question answering",
            "rating": 1,
            "sanitized_title": "learning_to_retrieve_reasoning_paths_over_wikipedia_graph_for_question_answering"
        },
        {
            "paper_title": "Deeppath: A reinforcement learning method for knowledge graph reasoning",
            "rating": 1,
            "sanitized_title": "deeppath_a_reinforcement_learning_method_for_knowledge_graph_reasoning"
        }
    ],
    "cost": 0.014495999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Navigate Wikipedia by Taking Random Walks</p>
<p>Manzil Zaheer manzilzaheer@google.com 
Kenneth Marino kmarino@google.com 
Will Grathwohl wgrathwohl@google.com 
John Schultz jhtschultz@google.com 
Wendy Shang wendyshang@google.com 
Sheila Babayan sbabayan@google.com 
Arun Ahuja arahuja@google.com 
Ishita Dasgupta 
Christine Kaeser-Chen christinech@google.com 
Rob Fergus Deepmind 
New York 
Learning to Navigate Wikipedia by Taking Random Walks</p>
<p>A fundamental ability of an intelligent web-based agent is seeking out and acquiring new information. Internet search engines reliably find the correct vicinity but the top results may be a few links away from the desired target. A complementary approach is navigation via hyperlinks, employing a policy that comprehends local content and selects a link that moves it closer to the target. In this paper, we show that behavioral cloning of randomly sampled trajectories is sufficient to learn an effective link selection policy. We demonstrate the approach on a graph version of Wikipedia with 38M nodes and 387M edges. The model is able to efficiently navigate between nodes 5 and 20 steps apart 96% and 92% of the time, respectively. We then use the resulting embeddings and policy in downstream fact verification and question answering tasks where, in combination with basic TF-IDF search and ranking methods, they are competitive results to the state-of-the-art methods. 1 A demo of our agent navigating is shown here https://www.youtube.com/watch?v=LVqOaPKpC2c 2 Exact comparisons are infeasible given idiosyncrasies in graph construction and other differences in setup.</p>
<p>Introduction</p>
<p>The ability to gather new knowledge about the world is a fundamental aspect of intelligence. Based only on a few words, a fact, question, or even vague idea, humans have the ability to use the internet to find extremely specific information about the world. From the important (how to administer CPR) to the trivial (the LA Raiders won the 1983 SuperBowl), virtually any knowledge is a click away.</p>
<p>In this work, we focus in one particular aspect of this ability: web navigation. In general, navigation is a key component of an embodied agent: the ability to move efficiently toward a target. In known environments, where map information is available, shortest-path algorithms provide a viable solution. However, in novel settings, the agent lacks such global information and must instead navigate to the target, using its understanding of the local environment to select actions.</p>
<p>Other approaches to web agents have focused mostly on retrieval or search engines. These, however, only provide a partial solution, typically getting close to a desired target but often not exactly to the right page. In this work, we present an approach for navigation on graph-structured web data that uses hyperlinks within articles to navigate toward a target. It complements search engines, using them to provide a sensible starting point for a local search for specific information.</p>
<p>To be able to navigate on the web effectively, we must read the text of the page, see how concepts in the current page are related to a possible next page, and use our understanding to learn a policy to make the right navigation decision. Consider the scenario shown in Figure 1. The agent is trying to navigate to the target node NORTH AMERICA while currently at the node PRESIDENTS OF THE UNITED STATES containing two hyperlinks: BARACK OBAMA and USA. Our approach learns an embedding from the text of the current page and the text for each hyperlink node and passes these 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Node 2 ϕ ϕ ϕ θ θ Figure 1: Our agent navigates through hyperlinks on a Wikipedia graph towards a target node. It first embeds the text at each link node using Transformer φ(.) and then evaluates different actions/links a 1 , a 2 with a policy θ, conditioned on the current and target embeddings s t and s g , respectively.</p>
<p>to a goal-conditioned policy that selects the best possible action. To make the correct choice in this example, the agent should associate USA with NORTH AMERICA.</p>
<p>The particular web environment we consider here is Wikipedia, converted into a graph form. Each of the 38M paragraphs is represented by a node; edges are links within and between articles. We introduce an approach for navigation on this graph based on behavioral cloning of automatically generated trajectories. This allows for unsupervised pre-training of both web page embeddings and navigation policy. We explore different trajectory distributions for training the model, the best practical choice being equivalent to random walks on the graph.</p>
<p>Contributions: Our work presents the first viable solution to the problem of web navigation 1 , as demonstrated by reliable (&gt;90% success) navigation on a graph of size 10 7 . Previous attempts such as Nogueira and Cho [2016] show a far inferior performance, well below that needed for practical utility. Furthermore, as there is nothing Wikipedia-specific about our approach and it scales gracefully, the method is applicable to general web settings. We also demonstrate the relevance of the pre-train + fine-tune paradigm to the web domain, showing that pre-training for navigation is a suitable objective to aid downstream tasks with limited data such as Q&amp;A. We demonstrate this by applying our navigation approach on two challenging tasks: fact verification (FEVER; Thorne et al. [2018]) and Q&amp;A (Natural Questions; Lee et al. [2019]) for evidence gathering. In both settings, we show a jump in performance over existing search-based approaches that rely on retrieval followed by re-ranking, matching state-of-the-art methods despite using far simpler retrieval and re-ranking mechanisms.</p>
<p>Background</p>
<p>Wikipedia as a navigation environment: Human behavior on Wikipedia [West and Leskovec, 2012a] was collected via the Wikispeedia game [West et al., 2009], which asked the players to move between a pair of arbitrarily chosen articles in the fewest steps. This rule allows the emergence of a semantic distance between concepts that facilitates retrieval. West and Leskovec [2012b] conducted initial studies on this dataset for navigation, using supervised learning and reinforcement learning methods on a small subset of articles (4.6K) with simple TF-IDF features.</p>
<p>Most relevantly, Nogueira and Cho [2016] explore the same problem of navigation on a large Wikipedia graph (12M nodes). They train LSTM agents to perform navigation on the graph, using Bag-of-Word features for each article. The trained agent is then applied to a Jeopardy Q&amp;A task. Our approach revisits this framing but with a number of important differences: (i) training on a significantly larger graph (38M vs 12M nodes, 51.5M vs 380M edges); (ii) using Transformer-based architectures for paragraph encodings and for the navigation policy; (iii) they use a single fixed start node, whereas ours is able to navigate from any start node; (iv) removal of beam search, or other complex search mechanisms at evaluation time. Collectively, our approach boosts the navigation success rate from 12.5% in Nogueira and Cho [2016] to over 90%, 2 despite a much larger graph.</p>
<p>Fact verification and Q&amp;A using Wikipedia: Wikipedia is a rich resource for evaluating language understanding through Q&amp;A tasks. Chen et al. [2017] combine TF-IDF retrieval with an LSTM document reader to find answers to free-form questions within Wikipedia documents. Contemporary Q&amp;A systems [Semnani andPandey, 2020, Qu et al., 2020] extend this general approach to more complex pipelines, combining traditional and neural methods with inverted index search/retrieval schemes, followed by document readers that perform re-ranking. We demonstrate our navigation approach in conjunction with very basic retrieval and re-ranking approaches, so as to clearly demonstrate the value of navigation. In common with our approach,  also use a graph version of Wikipedia, but with the goal of aligning it to a knowledge base, rather than navigation. Stammbach and Neumann [2019] propose a scheme for finding evidence for the FEVER [Thorne et al., 2018] benchmark that explores all links one step away from an initial retrieval set. This can be viewed as a 1-step navigation operation, in contrast to our scheme that permits an arbitrary number of steps.</p>
<p>Navigation in knowledge bases: Graph navigation is a key element in utilizing knowledge bases (KB), where facts are represented as tuples of entity nodes connected by a labelled edge corresponding to relation between the two nodes. For example, Das et al. [2017] answer complex queries by navigating the graph, inferring missing edges along the way, and finding a valid path to the answer node. In our case, the graph is fixed but consists of natural language text and is partially observed. This makes the task much harder on two fronts. First, the policy must comprehend the natural language actions, without access to crisply labelled relation actions.  [Xiong et al., 2017] and action dropout [Lin et al., 2018]. We adopted the action dropout where some outgoing edges are masked during training to enable more effective path exploration. Pushing more on exploration, inspired by AlphaGo, Shen et al. [2018], He et al. [2022] combine the policy with Monte-Carlo tree search to navigate KB. We find that no search method is necessary for our approach at evaluation time, with good performance obtained from the policy alone.</p>
<p>Internet powered Q&amp;A: Several recent works have attempted complex question answering using the internet. Talmor and Berant [2018] retrieve snippets of information from the web with a search engine to answer a structured decomposition of the question with a simpler Q&amp;A model. The recent WebGPT [Nakano et al., 2021] system takes a broadly similar approach to long-form question answering. WebGPT accures information from the Bing search API as well as some navigation, and passes it to an LM to generate the answer text. The complexity of the system necessitates the use of human demonstrations for training. Related work from Lazaridou et al. [2022] utilizes a few-shot supervision of an LM to perform Google queries, which are then used by the LM to generate an answer. They show that by incorporating information from Google queries, the answers are more factual than those generated directly from LMs. Another line of work, has leveraged link structure of webpages found on the internet to improve Q&amp;A. In LinkBERT [Yasunaga et al., 2022] is pretrained to capture dependencies between documents and results in improvement for for multi-hop reasoning and few-shot reading comprehension. Asai et al. [2020] use the link structure to enhance open domain Q&amp;A by learning to retrieve according to reasoning paths. In contrast to these works, we focus on mastering navigation rather than any combination with search/retrieval, albeit in more limited domain (Wikipedia vs the internet). However, our approach could potentially act as a core component in a general web navigation, being more versatile on account of not requiring supervision.</p>
<p>Approach</p>
<p>We consider navigation in a web environment W = {N, L}, where N := {n i } N i=1 is the set of nodes, each n i representing a web page with content s i . The links between pages are represented by L :
= {l i } N i=1 , where l i = {l 1 i , . . . , l j i , .
. . , l Mi i } are the set of M i (directed) out-links from node i. We define NH(n i ) as the set of nodes reached by following all links l i from node n i , where the jth neighbor is given by l j i (n i ). We assume some target node n g and some starting node n 0 are given, and wish to train an agent that can navigate from n 0 to n g by walking along the edges of the graph within T steps. To accomplish this, we construct a parametric policy p θ (n t+1 |n t , n g ) which, at time step t, parameterizes a distribution over n t+1 ∈ NH(n t ). We train this model via behavioral cloning to maximize
L(θ) = E E(n0,...,n T ) T −1 t=0 log p θ (n t+1 |n t , n g )(1)
where E(n 0 , . . . , n T ) is our trajectory distribution which generates trajectories (n 0 , . . . , n T ) that we would like our model to replicate (i.e. setting n g = n T ).</p>
<p>Trajectory distribution</p>
<p>This framework opens up many choices for E. Clearly some will result in ineffective navigation strategies while others may enable us to train a policy which is capable of generalizing to new graphs and downstream applications. We elaborate on some choices below.</p>
<p>Reverse trajectories:</p>
<p>We define E(n T ) = Uniform(N) (the uniform distribution over nodes in the graph). Now we can define
E(n t |n t+1 ) = 1 |{n i ; n i ∈ NH(n t )}| if n t+1 ∈ NH(n t ), else 0 (2)
or, a distribution which uniformly walks the reverse graph (with transposed adjacency matrix). We begin with some uniformly chosen target node and walk backward randomly to n 0 in such a way that we can reach the target by walking forward from n 0 .</p>
<p>Intuitively, training with the reverse trajectories defined above should allow our model to be able to reach any node in the graph from any node it is connected to. Unfortunately, in realistic web graph data, there exist many "dead-ends" or nodes with no in-going edges. The trajectories get stuck at these nodes leading to low-diversity training data.</p>
<p>Random forward trajectories: In our real-world knowledge graph data, there are far fewer nodes which have no out-going edges. Thus we propose an alternative method to generate target nodes based on randomly walking forward. We define E(n 0 ) to be the Uniform(N) and E(n t+1 |n t ) = 1 |NH(n t )| if n t+1 ∈ NH(n t ), else 0.</p>
<p>or, a distribution which uniformly picks a starting node and randomly walks the forward graph for T steps. Training in this way should encourage our model to be able to reach any node which is reachable from some uniformly chosen start. We note that the target node distribution E(n T ) will not be uniform and instead be a function of the graph structure. We also note that while this generates a T -step trajectory, because it is generated by random walk, the shortest path distance between start and target nodes may be smaller.</p>
<p>Random shortest paths:</p>
<p>We define E(n T ) = Uniform(N) and let E(n 0 |n T ) be the uniform distribution of nodes whose shortest path length to n T is T steps. We then define E(n t |n 0 , n T ) to be the delta distribution that n t is the t-th node of this shortest path. While it can be challenging to compute the probabilities of this distribution efficiently we can draw samples from it using a shortest path finding algorithm such as Dijkstra's algorithm.</p>
<p>Model parameterization</p>
<p>From above, we can see the core of our model is the p θ (n t+1 |n t , n T ) distribution. Given a sampled trajectory that begins at n 0 , this is the distribution which attempts to choose the correct action to take at timestep t + 1 starting at node n t in order to get to target node n T . We parameterize this distribution as a function of the text available at node n t .</p>
<p>p θ (n t+1 |n t , n g ) ∝ exp(f θ (s t+1 , s t , s g , t))</p>
<p>where f θ embeds the text s g , s t and s t+1 of the target, current and destination nodes, respectively. Further details on how we apply this general model to Wikipedia are given in Section 4.2 and an alternative interpretation of this approach based on variational inference in a latent-variable model is presented in Appendix D.</p>
<p>4 Application to Wikipedia 4.1 Navigation and sentence search tasks Data processing: We convert a snapshot of English Wikipedia into a graph but split each article (μ = 1000 words) into paragraph-sized blocks (μ = 100 words), which form the nodes. Edges in the graph are the union of (i) organic hyperlinks, (ii) additional entity linking and (iii) next/previous paragraph links. We consider two snapshots from year 2017 and 2018, which produces large full graphs with ∼ 37M nodes and ∼ 370M edges. For initial experiments, we also use a smaller subsampled graph, for which we sub-sample disjoint sets of 200k train / 200k evaluation nodes from the 2018 graph. All the obtained graphs are well connected with median path length of 15. Further details have been relegated to the Appendix (full graphs in Appendix A.1, the 200k graph in Appendix A.2, and graph statistics in Appendix A.3).</p>
<p>T-step navigation: The most basic task is T -step navigation. At the start of the episode, we randomly sample a start node and generate a T -step random walk trajectory. We then take the last node of this trajectory as the target n g . The agent is then given a time budget B (in most experiments, we fix max steps to B = 100), and succeeds if it reaches the target node within B steps. In our experiments, we show results on various graphs and splits with T = {5, 10, 20}-step navigation.</p>
<p>1-T Multistep navigation:</p>
<p>The Multistep task is much like the previous setting, except that instead of generating fixed T -step trajectories, we sample from Uniform(1, T ), T = 20.</p>
<p>T-step sentence search: This is the same setup as T -step navigation, except that rather than provide the full text for the target node n g , only one sentence within that text is selected at random to compute the target embeddings from. Thus agents have the more challenging task of locating the correct node, given only a small snippet of its text (see Section 4.2 for details on target representation). We include this task because one of the difficulties of web navigation is establishing which page should be the target in the first place. It also has direct relevance to our downstream fact verification task.</p>
<p>Model architecture</p>
<p>State representation: For each node n i we embed its text s i using a Transformer φ(.) to produce φ(s i ). By basing the state representation on semantic content, rather than i.e. node index, the agent is able to learn about the relationships between entities in a manner that allows for generalization to new graphs at evaluation time, instead of memorizing the structure of the training graph. Following Karpukhin et al. [2020], we append to the node text the title of the Wikipedia article to which it belongs. This often provides context to the node that might otherwise be missing -in many articles, the subject of the article will be referred to indirectly (e.g. it might state that "she" was born in 1945 instead of repeating the subject's name).</p>
<p>For those experiments where the node embeddings are kept fixed, φ(.) is a pre-trained RoBERTA [Liu et al., 2019] model that encodes the node text &amp; title. However, in our large-scale experiments, we pre-train the Transformer φ(.) directly using the text &amp; title (see Appendix B.1). In both cases, we apply the Transformer to the tokenized text, take the mean over the input tokens, and apply a tanh nonlinearity to produce φ(s i ).</p>
<p>Target representation: Similarly, we use the article text s g of the target node as input to the Transformer model φ to compute its representation φ(s g ). For some tasks, such as sentence search or fact verification, the target representation instead relies on a single sentence from each node or other text specific to the task and a separate transformer model φ target is trained to embed the target.</p>
<p>Navigation policy network: The network operates on the target node n g and the current node n t , and outputs a distribution over the possible actions that can be taken at n t . As the nodes in the graph have a variable number of outgoing edges, the policy must produce a distribution with a variable number of outcomes. To do this, we first concatenate the embeddings of s g and s t , and pass them through a 1-layer feed-forward network to produce a combined embedding e tg = F F [φ(s t ), φ(s g )].</p>
<p>Next we embed each possible action a i . When we use a fixed φ, we directly use a i = φ(s i ) for n i ∈ NH(n t ). However this is not feasible when pre-training our embedding model due to memory concerns. Instead, we compute
a i = φ(s t [l i t ])
where s[l i ] selects the words in s which belong to hyperlink i -see Appendix B.1 for more details. We then define the probability of moving to n i from n t with target n g as p(n i |n t , n g ) ∝ exp (e tg · a i ) which is normalized to produce a distribution over the |NH(n t )| actions.</p>
<p>In our larger-scale model, we also include the trajectory leading up to the current node (n 0 , . . . , n t ). We pass their embeddings, combined with the target embedding, [φ(s 0 ), . . . , φ(s t ), φ(s g )], through a 4-layer transformer and use the output as e tg when computing the transition probabilities above. We explore these two policy architecture choices, feed-forward and Transformer in Table 4.</p>
<p>Downstream tasks</p>
<p>Finally, we use our navigation approach to gather information for the tasks of fact verification on the FEVER benchmark [Thorne et al., 2018] and question answering on Natural Questions (NQ) [Lee et al., 2019] which also use Wikipedia as a knowledge base. One important difference from our earlier navigation task is that the target node n g is not specified at evaluation time. Instead, we are given a claim whose veracity must be established or a question that must be answered by finding information in a particular node in the graph. To determine which node this is, we use a target encoder which maps the claim or question to an embedding vector that can be used in place of φ(s g ) in Section 4.2.</p>
<p>Due to the limited size of the downstream training sets, we pre-train our graph embeddings, navigation policy and target encoder on the similar sentence search task. We then freeze the embeddings and policy and fine-tune the target encoder on (n g , s g ) pairs from the downstream task's training set. To avoid over-fitting, we add an auxiliary loss which enforces the similarity between our target embedding and the embedding of the ground truth node containing the sentence needed to resolve the claim or question. Training and evaluation on FEVER and NQ required alignment between our version of Wikipedia and the one used to compile the benchmark and details of this can be found in Appendix C.1 and C.2.</p>
<p>Next, our navigation method requires a starting node. To put our agent close to the articles we need in the downstream tasks, we run a popular variant of TF-IDF, called BM25 [Robertson and Zaragoza, 2009]. We first create an index over all of the nodes in the graph, then take the top-5 BM25 matches as the starting nodes, and run navigation for 20 steps on each. Note that BM25 often does not find the exact text (i.e. the hits@1 is not high) but matches are in the right vicinity, usually the same or similar article. Thus the agent only has to navigate a few steps to reach the target: in FEVER and NQ datasets, the mean length of shortest path between start and target node is 4.3 and 5.1 steps respectively.</p>
<p>The final component is a re-ranker that takes all the sentences from all nodes visited by the agent and assigns a match score between each one and the claim. The ranked list of sentences can then be used for benchmark evaluation. We explore two types of ranker: (a) basic TF-IDF and (b) a transformer based model like BERT or BigBird, fine tuned using hard negatives mined from the TF-IDF ranker. See Appendix C.1 and C.2 for more details.</p>
<p>Evaluation of our approach thus consists of: (i) given a claim/question, use BM-25 to return a list of promising start locations; (ii) from the top N = 10 of these, run our navigation model for 20 steps, using the target encoder to embed the claim/question, (iii) use the re-ranker to score sentences visited against the claim/question and (iv) select the top 5 for computing the recall, precision, or F1 score, for comparison on the benchmarks.</p>
<p>Experiments</p>
<p>Investigation of training trajectory distribution</p>
<p>On the smaller 200k node graph, we investigate different choices for the trajectory policy distributions E described in Section 3: (i) Random Forward Trajectories, where we randomly sample start nodes and do a random walk to get a target, (ii) Reverse Trajectories, where we randomly sample a target node and and do a random walk on the reverse graph to get a start node, and (iii) Random Shortest Paths where we randomly sample a start node and take a random walk and then compute the shortest path between the start and target nodes. For each of these we trained policies on 5, 10, and 20 step navigation tasks. We evaluate these in two different ways: (i) forward sampling of source and target in the disjoint evaluation graph, (ii) reverse sampling of source and target. The results of this experiment are shown in Table 1. Unsurprisingly, we can see that forward and reverse trained policies both do better when evaluated in the way that matches training. However, forward trajectory policies generalize much better to reverse navigation. When we analyze the traces of these experiments, we see that in the reverse sampling, many target nodes are in "dead-ends" where many target nodes do not have long random trajectories to sample. This results in easier navigation (as we can see from higher accuracy in reverse trajectory trained policies on reverse navigation) and more extreme overfitting, leading to poor performance of reverse trajectory on forward navigation.</p>
<p>Not surprisingly, we also see that training on a shortest path trajectories leads to better performance both in the forward and reverse sampling. This advantage is especially seen in the 20-step navigation task. Ideally, we would always train on shortest paths. However, the time complexity of pre-computing all shortest paths in a graph using Dijkstra's algorithm is O(V (E + V logV )) for the number of nodes V and number of edges E in the graph. On the full 38M Wikipedia graph, this computation would be completely intractable. Luckily, the random forward trajectories perform almost as well and can be computed in constant time with respect to the size of the graph and only linear time with respect to the trajectory length. For all subsequent experiments, we use the Random Forward Trajectories for our trajectory policy and evaluate on the forward version of the navigation tasks, where we call our approach Random Forward Behavioral Cloning (RFBC).</p>
<p>Navigation</p>
<p>We next do a more thorough set of experiments on the smaller 200k Wikipedia graph. Because the graph is much smaller than the full Wikipedia and thus prone to overfitting, for all of our methods and baselines we use the fixed RoBERTA embeddings φ(s i ), the simplest 1-layer feed-forward network, and MiniBERT [Turc et al., 2019] for target sentence embedding φ(s g ).</p>
<p>Alongside our approach, we employ the following baselines/ablations:</p>
<p>• RFBC: Random-forward behavioral cloning (our approach).</p>
<p>• RFBC + RF: RFBC but with random features sampled from the unit sphere instead of φ • RL + RF: RL with random features.</p>
<p>• Random: policy that chooses random action (out-link) at each timestep.</p>
<p>• Greedy: selects the action embedding which has the smallest cosine distance with the target. • Random DFS: DFS of depth equal to number of steps, choosing actions at random. • Greedy DFS: DFS but select action with smallest cosine distance to the target.</p>
<p>More training details are in Appendix B.2 and computational cost of each in Appendix E.</p>
<p>The results evaluated on the held-out graph are shown in columns 1-4 of Table 2. Our approach (RFBC) performs well, achieving 77% in the multistep case. Performance also drops as the distance to the target increases. In contrast, the RL agent performs poorly (around 40% on navigation tasks). Because we generate virtually infinite expert trajectories with RFBC, RL performs worse as it is trained with essentially the same amount of exploration, but with a more sparse signal than behavioral cloning. Using random features in place of φ(.) caused the performance to drop to near chance, showing that the models are utilizing the semantics at each node and are not relying on a general search strategy of some kind. Random methods also do quite poorly, even when adding a DFS. Greedy methods do a bit better than random, especially with DFS, but still well below our method.</p>
<p>We also consider the more challenging sentence search task described in Section 4.1. This task requires not only finding a known target node, but learning to find a target node given only a single random sentence from that node, more closely matching our downstream task and the way humans tend to approach finding information on the web. Columns 5-7 of Table 2 show performance on this task for our approach and RL. Our method has reduced performance on the task due to the extra difficulty of learning the target embedding, but still performs reasonably. RL, however, degrades to near chance performance.  Our method scales naturally to much larger graphs, which we demonstrate by training on the entire Wikipedia 2017 graph. We evaluate on the entire Wikipedia 2018 graph. Each graph is ∼ 37M nodes and ∼ 370M edges (see Table 3 for details). We would like to point out there is significant evolution (difference) between the 2017 and 2018 graph as analysed in Appendix A.3 along with further statistics.</p>
<p>We explore both architectures described in Section 4.2 for the policy network: (i) the single feedforward layer (as used in the smaller graph) and (ii) the 4-layer Transformer model. We also compare using a fixed RoBERTA model for φ versus pre-training a text transformer directly on the navigation task (see Appendix B.3). DistillBERT [Sanh et al., 2019] is used for φ(n g ). Table 4 shows the results on the full graph, evaluating on {5, 10, 20} step tasks, for both navigation and sentence search. Figure 2 shows example trajectories of the trained agent. The first thing to note is that the overall navigation and sentence search performance of our methods is high. The 2018 Wikipedia graph we use for evaluation contains over 5 million articles, over 38 million nodes and 387 million edges. On this graph (which we generalize to from the previous year's graph) our best method can find an article 20 steps away 92.2% percent of the time and can locate it given just a single sentence of that article 90.2% of the time. This compares favorably with a similar evaluation performed in Nogueira and Cho [2016], where their WebNav approach achieved a 12.5% success rate for 16 step tasks on a 12M node graph. Compared to the performance of our method on the smaller 200k graph, we see that performance has greatly improved, most likely due to the addition of orders of magnitude more training data. In particular we see that sentence search performance on 20-step improves greatly from 34.6% to a best performance of 90.2%.</p>
<p>Additionally, in all cases, our learned embeddings perform better than using the fixed features, derived from large language models. In general, feed-forward policies perform best, except for 20 step navigation tasks. For longer trajectories, the Transformer policy network achieves slightly better performance, while on sentence search, the feed-forward model is consistently superior. </p>
<p>Application to fact verification</p>
<p>We now evaluate the ability of our navigation approach to select evidence on the FEVER development set [Thorne et al., 2018] with results shown in Table 5. The primary metric used is F1@top5 but we also give precision and recall. Paired with a basic retrieval (BM25) and re-ranker (TF-IDF), the approach obtains a respectable F1 of 0.46. If we remove our navigation component then this drops substantially to 0.36, demonstrating its utility. Swapping to a more powerful re-ranker (BigBird) boosts the F1 to 0.731, which is competitive with the state-of-the-art (FEVER leaderboard [2022] rank #6; rank #1 F1 is 0.799). We also compare to a leading approach [Stammbach, 2021] that similarly relies on the BigBird re-ranker, with our approach having a significantly better F1 score. A possible explanation is that their approach overwhelms the re-ranker with all possible hyperlinks whereas, navigation being inherently sparse, our model presents a more refined set for re-ranking. We note that our approach: (a) is significantly simpler than many top ranked approaches and (b) selects evidence over a much larger set than the curated version of Wikipedia used in FEVER (38M vs 5M). Examples of successful navigation traces are shown in Figure 3.</p>
<p>Application to question answering</p>
<p>Finally, we present results on another downstream task where navigation helps. In particular, we consider the task of finding correct evidence passage for open domain question answering. We use the Natural Question (NQ) open-domain dataset presented in Lee et al. [2019]. Since we target navigating to the exact evidence passage required to answer the question, we use recall@{1,2,3,4,5} for finding the gold evidence passage as our metric. More commonly the metric marks a retrieved passage to be correct if it contains the answer string, but this causes a lot of false positives, e.g. the answer string appears in a totally irrelevant context. The results are tabulated in Table 6. Paired with a basic retrieval (BM25) and re-ranker (BigBird), the approach obtains a respectable Recall@1 of 31.6. If we remove our navigation component then this drops substantially to 22.2, demonstrating its utility. As a reference, we also ran a state-of-the-art system RocketQA [Qu et al., 2021] on our setup and evaluated it using our harder metric. It is worthwhile to note that our approach (a) is significantly simpler than state-of-the-art approaches like RocketQA which employ 4 stages of dual-encoder and cross-attention models, and (b) selects evidence over a larger set than the commonly used preprocess Wikipedia passages (38M vs 21M).</p>
<p>Discussion</p>
<p>We have presented a simple and effective scheme for navigating a large Wikipedia graph that is applicable to more general web navigation problems. We show that behavioral cloning of random trajectories is a viable approach to learning both entity embeddings and a navigation policy. When applied to the fact verification task on FEVER dataset and the open-domain question answering task on NQ dataset, they offer highly competitive performance, whilst being complementary to existing approaches. Another advantage worth highlighting is that the navigating agent provides a provenance on how it arrived at the relevant evidence, which many other methods (like dense passage retrieval) do not provide. One limitation of our approach when we move from Wikipedia to the wider Internet is that our scheme relies on a good target encoder. For navigation and our downstream tasks, there was a clear ground-truth target node available training, but in other settings this might not be the case. Another limitation is that we require a re-ranker to decide on the final retrieved sentence, but ideally the agent would decide for itself when it has reached the correct node.</p>
<p>A Data / Environment details A.1 Wikipedia data processing</p>
<p>We start by downloading English Wikipedia snapshot (the pages-articles.xml.bz2 file) from Wikimedia 3 or Internet Archive. 4 We extract text from English Wikipedia for a given snapshot using Gensim's WikiCorpus class. 5 For each page, this tool extracts the plain text and hyperlinks, and strips out all structured data such as lists and figures. To avoid pages without sufficient textual content, we filter out categories, listical, disambiguation pages and any other page with less than 200 characters. Each article (µ = 1000 words) is split into paragraph-sized blocks (µ = 100 words), which form the nodes in our Wikipedia Graph. This was done both for conceptual reasons (humans don't read an entire article at once but look at bits and pieces of it at one go) and for modeling reasons (most language models have an upper input token length that is smaller than the average Wikipedia page).</p>
<p>The edges in the graph are formed in three ways:</p>
<ol>
<li>
<p>for nodes in the same article, we add "previous node" and "next node" links to connect them in a chain;</p>
</li>
<li>
<p>organic hyperlinks -the internal links connecting Wikipedia pages to each other; and 3. we additionally run entity linking 6 because repeat mentions of an entity in Wikipedia lack hyperlinks, which is problematic when articles are split up.</p>
</li>
</ol>
<p>This constitutes our Wikipedia graph construction, where the text blocks act as vertices and the organic hyperlinks along with entity links are the edges. We store the graph as an adjacency list with metadata, using a memory mapped key-value data-structure to enable fast random access during navigation.</p>
<p>We run the above graph generation pipeline for two different snapshots of Wikipedia:</p>
<ol>
<li>June 01 2017 7 : This corresponds to FEVER fact verification dataset.</li>
</ol>
<p>December 2018 8 : This corresponds to Natural Questions open dataset.</p>
<p>A.2 Smaller 200k node graph construction</p>
<p>We construct smaller train and eval graphs with 200k nodes each for the following reasons: 1. Checking strict generalization by making train and eval graphs to be totally disjoint, i.e. no common nodes or edges. The full 2017 and 2018 are sufficiently different but not entirely disjoint. 2. Running experiments on full graphs are expensive, so for more thorough evaluation across multiple baselines we constructed the smaller graphs.</p>
<p>To construct these two smaller graphs, we start with the full 2018 graph. We sort the nodes by their in-degrees. We mark nodes with odd ranks to be in train and even ranks to be in eval, which ensures complete separation between the two. For the train graph we start with rank 1 node (and for eval with rank 2 node) and then select all its neighbors which have odd rank (even rank). Then all the odd (even) nodes connected to the selected ones are picked. This process is continued until the desired number of nodes are selected.</p>
<p>A.3 Graph statistics</p>
<p>We construct two versions of the full Wikipedia graph following the method outlined in Appendix A.1. The main summary statistics (number of articles, nodes, edges, words per node, median path length) of the resulting graphs are listed in Table 3.</p>
<p>A more detailed picture of the graph can be obtained by looking at its degree distribution, which is presented in Figure 4. It shows the graph has tell-tale sign of web-graph: it has a power-law distribution with a few hub nodes. We try to estimate how far are two nodes in the graph. Calculating the entire distribution of shortest path length is prohibitively expensive for such large graph. We follow Ye et al. [2010] method to get an approximation. We start by selecting 250k nodes having highest in-degree, which is a good approximation of the giant component in a web graph as required by Ye et al. [2010]. We then perform a single source to all shortest paths on each of these 250k nodes using Dijkstra's algorithm for sparse graphs. This resulted in computing approximately 1 trillion shortest paths based on which the estimated distribution of shortest path length is shown in Figure 5.  Finally, we would like to point out that there is significant evolution (difference) between the 2017 and 2018 graph. Some important statistics reflecting the changes across the two graphs are:</p>
<p>• Further in Figure 6, we show histogram of changes in edges among the common nodes between the two graphs. Apart from addition/deletion of organic hyperlinks, just modifications of text would yield to differences as the chunking will be different. This shows significant generalization is needed to successfully navigate across the two graphs, simply memorization will not work. log p θ (n t |n t , n g )</p>
<p>where we set n g = n T . We parameterize this distribution as p θ (n t+1 |n t , n g ) ∝ exp(f θ (s φ (n g ), s φ (n t ), a φ (n t , n t+1 )))</p>
<p>where s φ (·) and a φ (·, ·) are functions which extract embeddings for an entire node and a node-action combination, respectively and f θ (·, ·, ·) is a function which combines these embeddings to produce action probabilities. At their core, s φ and a φ are based on the same transformer model. We tokenize the text at node n to produce L tokens and pass these tokens through the transformer to produce L embeddings.</p>
<p>The state representation s φ (n) is produced by taking the mean of these embeddings and passing this vector through a tanh nonlinearity. The node-action embedding a φ (n, n ) embeds the action of moving from node n to its neighbor n . Let us assume the text at node n is "Barack Obama was the President of the United States during..." where the substrings "Barack Obama" and "President of the United States" correspond to hyperlinks to other neighboring articles n and n . Then we construct the node-action embedding a φ (n, n ) by passing the tokenized text of node n through our transformer, again producing L transformed token embeddings. Then we take the transformed tokens which correspond to the text "Barack Obama" and take their mean and pass this through the tanh nonlinearity. If we instead wanted to produce a φ (n, n ) we would take the transformed tokens which correspond to the text "President of the United States" instead, take the mean and apply the nonlinearity. This is visualized in Figure 7.</p>
<p>The function f θ which combines these embeddings first concatenates them and passes the combined input through a simple neural network with a single hidden layer and 1 output neuron. Training details We train these models to optimize Equation 5. The objective is optimized with the Adam [Kingma and Ba, 2014] optimizer using an initial learning rate of 10 −5 with a one-cycle cosine decay schedule and linear warm up of 10K steps. The training batch size is 512 and the model is trained over 3M iterations. Node text is clipped (or padded) to 200 tokens. The weights of the transformer are initialized from a pretrained RoBERTa [Liu et al., 2019] masked language model.</p>
<p>B.2 Small graph experiments</p>
<p>Model details In the small graph experiments, we use the simple policy network F F as described in Section 4.2. As discussed, we use a 1-layer MLP (meaning a single learned linear layer with no activation function). We use a hidden size of 768 to match the size of φ. Because the final logits are the result of an inner product between each state and action embedding of size 768, to aid network training, we add a normalization layer (jax.nn.normalize) before the inner product.</p>
<p>For the action embedding a i = φ(s i ) we also concatenate a 1-hot vector that represents the action type (e.g. whether it was a link action, or a next action, or a prev action). We also concatenate another bit that indicates whether the state s i of the particular action has been visited before.</p>
<p>For the purposes of the RL baseline, we can view graph navigation as a goal-conditioned MDP. 9 Our observation is the same as the inputs to the RFBC models. The reward function R for current node n and goal node n g we can write simply as:
R = 1 n = n g 0 otherwise(7)
For the RL baseline, we used IMPALA [Espeholt et al., 2018], which is a kind of policy gradient approach, and thus has a policy network π and a "baseline" or value function network. Since the policy network has the same output space as our RFBC models, we use an identical architecture for this. For the value network, we use a very similar architecture. We again use φ as our encoding of the current node and the target node, again concatenate them together. We then feed this through a 1-layer MLP to compute the value.</p>
<p>Training details The procedure for generating the trajectories we use for the BC loss is described in Section 3.1. Except for our experiment in Section 5.1, we always use the random forward trajectories. Following Lin et al. [2018], in the navigation experiments, we randomly drop out edges in the training graph with probability 0.5 to reduce over-fitting. As stated in Section 4.1, we set max steps to B = 100. For RFBC training on the small graph, we use RMSProp with a learning rate of 0.01, a decay of 0.9 an epsilon of 10 −10 .</p>
<p>For the sentence search tasks, our models train a separate φ for the target embedding. As we stated, we use MiniBERT for this embedding φ target and train it with the same optimization settings, except that we reduce the learning rate for these weights to 10 −4 . We use a batch size of 512 and we train for 50, 000 network update steps.</p>
<p>For RL training, we use VTRACE [Espeholt et al., 2018] loss, using the reward above. We again use RMSProp with a learning rate of 0.01, a decay of 0.9 an epsilon of 10 −10 . We set the baseline cost to 0.5, trajectory length (the number of timesteps the RL loss backprops through) to 100, batch size again to 512 and max update steps to 50, 000. We did a parameter sweep for entropy costs of 0.1, 0.01, 0.001 and gamma of 0.8, 0.9. To give RL the best chance possible, we choose the maximum over this sweep for each experiment (but still none of these match RFBC performance).</p>
<p>B.3 Full Wikipedia navigation experiments</p>
<p>On the full Wikipedia training experiments in Section 5.3, we evaluate on navigation and sentence search using either our feed-forward or transformer model and using either RoBERTA fixed embeddings, or our trained embeddings described in Appendix B.1.</p>
<p>The feed-forward model is identical to the one described in Appendix B.2. The transformer model is the standard transformer model from Vaswani et al. [2017] with 4 layers, an attention size of 64, 12 heads, and mlp hidden size of 3072 and dropout rate of 0. DistillBERT [Sanh et al., 2019] is used for φ(n g ) for the sentence search experiments.</p>
<p>The learning parameters and all other relevant training parameters for all models are identical to those in Appendix B.2 except that the learning rate for the transformer model is lower (10 −4 ) as the earlier learning rate was too high for transformer models.</p>
<p>B.4 RFBC + RL</p>
<p>We additionally try finetuning an RL policy starting from our RFBC-trained navigation policies. We use the same network and learning settings as we did in the other RL experiments. We train for an additional 10,000 network updates (512M environment steps). We add the final numbers in Table 2 and show the training curve in Figure 8. We can see from the training curve that RL training accuracy fluctuates around the point where RFBC training left off. From the results in Table 2 we see little statistically significant difference (about 1 point in either direction). Data processing To make an aligned navigation graph for FEVER, we start with the June 01 2017 snapshot of Wikipedia and build the graph using the procedure described in Section A.1. Because of differences in text preprocessing, FEVER's version of Wikipedia does not precisely align to our own. We then try to match FEVER's Wikipedia articles to our graph, relying on Wikipedia URL redirections for handling disambiguations. This resulted in 99.5% matching of articles between our navigation graph and the FEVER version of Wikipedia. Second, we align the evidence sentences in FEVER to sentences from our text blocks using a fuzzy string match. Specifically, we use the token set ratio to score the similarity between an evidence sentence and the text in a graph node. If the score between a sentence and a node is ≥ 80, the node is added as an evidence node for the given claim. This threshold is sufficiently high to minimize the chances of a false positive, while also matching a high percentage of evidence sentences. Some sentences are difficult to match, particularly those that are split between two text blocks. Ultimately, 93.1% of all evidence sentences were matched to a node. This gives us an augmented FEVER dataset, where for each claim we have a corresponding set of evidence nodes in the graph. These nodes are then used as navigation targets for fine-tuning during the training phase. We generate trajectories for BC by running BM25 over all nodes in the graph, taking the top 10 matches as starting nodes, and finding shortest paths to evidence nodes.</p>
<p>Data statistics</p>
<p>We compute the shortest-path distance between top-1 retrieval of BM25 and target node in this graph in Figure 9. As can be seen that most path lengths are relatively short, which implies that BM25 lands us in the right vicinity and by a small amount of navigation around we will be able to find the right evidence passage.</p>
<p>Training details For the FEVER benchmark, our model uses learned embeddings for φ with a single feed-forward layer for the policy network. The model is pretrained on the 5-step sentence search task, and finetuned on trajectories generated from the augmented FEVER dataset using the the following loss function:
L(θ) = L BC (θ) + 0.1 φ target (claim) − φ(s g ) 2(8)
where L BC is the normal BC loss we use in our other experiments.</p>
<p>Only the φ target weights are finetuned for this task. We use AdamW with a constant learning rate of 10 −6 , β 1 = 0.9, β 2 = 0.999, and = 10 −6 . The model is finetuned for 100, 000 update steps with a batch size of 512. No edge dropout is used during finetuning.</p>
<p>Evaluation details For each claim in the FEVER development dataset, we first run BM25 over all nodes in the graph and take the top 5 matches as starting nodes. From each node, the finetuned model then navigates for 20 steps. We collect all sentences in all nodes visited by the agent over the 100 total navigation steps, and match them to FEVER evidence sentences using the WRatio function in the fuzzywuzzy package. 10 Two sentence ranking methods are explored: Gensim's TF-IDF implementation, 11 and the BigBird re-ranker of Stammbach [2021], using their open source model and weights. 12 Finally, the top-5 evidence sentences are then submitted to the official FEVER scorer 13 to compute the accuracy, recall, and F1 scores. Data processing Most prior works in literature utilize the processed collection of 21M passages provided by Karpukhin et al. [2020], but unfortunately we cannot use it as it has no hyperlink information which is crucial for our graph building. So we re-align the questions in NQ to our 2018 graph with 38M passages (i.e. nodes) which has different text blocks than Karpukhin et al. [2020]. As the article title corresponding to the passage was given as part of the dataset, the alignment task was local to the document. We could find all the article titles, i.e. 100% match in locating the document in our dump. As before, to align the evidence passage in NQ to our text blocks, we first used a fuzzy string match. Then in the ranked list of fuzzy string matches, we searched for exact answer string. The highest ranked node with answer string was labelled as the ground truth. Some answer strings were difficult to match, particularly those that are split between two text blocks. Ultimately, 99.3% of all evidence passages were matched to a node. This gives the re-aligned NQ dataset, where for each question we have a corresponding set of evidence nodes in our graph. These nodes are then used as navigation targets for fine-tuning during the training phase. We generate trajectories for BC by running BM25 over all nodes in the graph, taking the top 10 matches as starting nodes, and finding shortest paths to evidence nodes.</p>
<p>C.2 NQ experiments</p>
<p>Data statistics</p>
<p>We compute the shortest-path distance between top-1 retrieval of BM25 and target node in this graph in Figure 10. As can be seen that most path lengths are relatively short, which implies that BM25 lands us in the right vicinity and by a small amount of navigation around we will be able to find the right evidence passage.</p>
<p>Training details For the NQ benchmark, our model uses learned embeddings for φ with a single feed-forward layer for the policy network. The model is pretrained on the 5-step sentence search task, and finetuned on trajectories generated from the NQ dataset using the the following loss function:
L(θ) = L BC (θ) + 0.1 φ target (question) − φ(s g ) 2(9)
where L BC is the normal BC loss we use in our other experiments.</p>
<p>Only the φ target weights are finetuned for this task. We use AdamW with a constant learning rate of 10 −6 , β 1 = 0.9, β 2 = 0.999, and = 10 −6 . The model is finetuned for 100, 000 update steps with a batch size of 512. No edge dropout is used during finetuning. Evaluation details For each question, we first run BM25 over all nodes in the graph and take the top-5 matches as starting nodes. From each starting node, the agent then navigates for 20 steps. All the 100 nodes visited by the agent is then ranked by a simple cross-attention model. For the cross-attention model, we follow Hofstätter et al.</p>
<p>[2020] to train 6-layer BigBird [Zaheer et al., 2020].</p>
<p>Since we target navigating to the exact evidence passage required to answer the question, we use recall@{1,2,3,4,5} for finding the gold evidence passage as our metric. More commonly the metric marks a retrieved passage to be correct if it contains the answer string, but this causes a lot of false positives, e.g. the answer string appears in a totally irrelevant context.</p>
<p>D Variational Interpretation of our Approach</p>
<p>We present an alternative motivation of our method based on variational inference in a latent variable model of states. Assume we are given some goal node n g . We would like to parameterize a model p θ (n T |n g ) which will generate a trajectory of states n 0 , n 1 , . . . , n T such that n T = n g . We define our model as a latent variable model p θ (n T |n g ) = n0,n1,...,n T −1 p θ (n T , n T −1 , . . . , n 1 , n 0 |n g ) = n0,n1,...,n T −1 p(n 0 ) T t=1 p θ (n t |n t−1 , n g )</p>
<p>i.e an autoregressive model which samples an initial state from n 0 ∼ p(n 0 ) and then samples subsequent states n t ∼ p θ (n t |n t−1 , n g ). The transition distribution samples the next node n t from the neighbors of n t−1 in our graph. We would like the probability that n T = n g to be large, thus we will train our model to maximize log p θ (n T = n g |n g ).</p>
<p>For latent-variable models such as this, we can rewrite the marginal likelihood as log p θ (n T |n g ) = E p θ (n0,...,n T −1 |n T ,ng) p(n 0 ) T t=1 p θ (n t |n t−1 , n g ) − log p θ (n 0 , . . . , n T −1 |n T , n g )</p>
<p>and can obtain a lower-bound on this quantity by replacing the intractable posterior p θ (n 0 , . . . , n T −1 |n T , n g ) with a variational approximation q(n 0 , . . . , n T −1 |n T , n g ), i.e L θ (n T , n g ; q) := log p θ (n T |n g ) ≥ E q(n0,...,n T −1 |n T ,ng) p(n 0 ) T t=1 p θ (n t |n t−1 , n g ) − log q(n 0 , . . . , n T −1 |n T , n g ) .</p>
<p>The above bound becomes tight when q(n 0 , . . . , n T −1 |n T , n g ) = p θ (n 0 , . . . , n T −1 |n T , n g ). Thus, we can optimize our model parameters θ to maximize L θ (n T , n g ; q). This approach has had a long history of successfully training latent-variable generative models Welling, 2013, Ho et al., 2020].</p>
<p>In the context of this work, we can view q(n 0 , . . . , n T −1 |n T , n g ) as a distribution over trajectories of nodes which end at our goal node. We can interpret the various trajectory generation methods introduced in Section 3.1 as different variational approximations q. Ideally, the option which most closely approximates the true posterior p θ (n 0 , . . . , n T −1 |n T , n g ) would be the most desirable but in general this distribution is intractable.</p>
<p>We find that using simple random trajectories provides a good-enough approximation to the posterior to enable us to train a model which reliably finds the goal state. This result is not completely surprising in context of prior work [Dai et al., 2020] which successfully trains latent-variable models using random trajectories as an inference model.</p>
<p>E Efficiency Analysis</p>
<p>In this section we analyze the asmpytotic runtime of our method and baselines. In Table 7 we show the asymptotic runtime and whether or not it is scalable for each method. We also show the 5-step accuracy from Table 2 for quick reference.</p>
<p>T is the trajectory length (either the length of the trajectory for BC methods or the maximum allowed trajectory for other methods. For BC, T is guaranteed to be shorter than the max, but this is a constant factor difference. E out is the average number of outgoing edges (i.e. actions) at a node. (small -10 1 ) E is the total edges in the graph (very large -10 8 ). V is the number of nodes in the graph (large -10 7 ). term is the runtime of finding a shortest path (Dijkstra's algorithm with Fibonacci heap). As the graph gets larger, this term because infeasible (we could not even run this on the full Wikipedia graph). DFS might seem like it should be larger than O(E out T ) or O(T ), but because we restrict the maximum allowable steps, it is only proportional to T and (for greedy) E out .</p>
<p>From this Table, we see that all methods (except shortest path) have scalable runtimes. The dominant factor is E out , which theoretically grows slowly with graph size. Moreover, in practice, there is a natural upper bound on the number of links one can have in a paragraph of text, and the runtime difference in our experiments going from 200k nodes to 38M nodes is only 1.5x despite the 200-fold increase in graph size.</p>
<p>Figure 4 :
4Degree Distribution</p>
<p>Figure 6 :
6Number changes in edges among common nodes between 2017 and 2018</p>
<p>Figure 5 :
5Distribution of Short Path Length estimated by computing shortest path between 1 trillion pairs of random nodes in the 2018 graph. details our embedding pretraining procedure, as introduced in Section 3.Model detailsThe transformer is fine-tuned from a pre-trained RoBERTa[Liu et al., 2019] model. As in Equation 1 our model is trained to optimize L(θ) = E E(n0,...,n T )</p>
<p>Figure 7 :
7Description of how embeddings are extracted for navigation pretraining.</p>
<p>Figure 8 :Figure 9 :
89Training curve for RFBC with RL finetuning. C Downstream task details C.1 FEVER experiments FEVER [Thorne et al., 2018] is a dataset containing 185, 445 claims labeled as SUPPORTED, RE-FUTED, or NOTENOUGHINFO. Verifiable claims (i.e. SUPPORTED or REFUTED) are annotated with Distribution of Short Path Length from starting node computed by BM25 to target node on FEVER dataset on 2017 graph. evidence sentences supporting this classification, which are drawn from a preprocessed version of the June 2017 Wikipedia snapshot. Notably, only the article introductions are retained, and claims are generated from a curated set of approximately 50, 000 popular pages.</p>
<p>Figure 10 :
10Distribution of Short Path Length from starting node computed by BM25 to target node on Natural Questions dataset on 2018 graph.</p>
<p>Second, the action space is open and varying in size, unlike in a KB which typically has a fixed schema, i.e. relations/actions come from a fixed size set. Perhaps the closest work involving natural language actions, Fu et al. [2019] augment a KB with textual nodes. However, it still operates under the fixed KB schema. Further improvements on navigation in KB have been shown by incorporating tricks like reward-shaping</p>
<p>Table 1 :
1Different trajectory distributions on small graph (200k nodes) Navigation (Forward) Navigation (Reverse)Trajectory Policy 
5 
10 
20 
5 
10 
20 </p>
<p>Forward 
85.3 76.4 
67.5 
31.1 32.6 
19.9 
Reverse 
2.3 
0.5 
0.2 
91.3 87.9 
87.3 
Shortest path 
86.7 85.0 
84.6 
74.3 31.6 
28.4 </p>
<p>Table 2 :
2Small graph navigation (200k nodes) -Success rate (%) 
Navigation 
Sentence Search </p>
<p>Method 
5 
10 
20 
multistep 
5 
10 
20 </p>
<p>RFBC (ours) 
85.3 76.4 67.5 
77.4 
60.7 47.6 34.6 
RL 
41.4 40.2 41.6 
43.6 
14.0 
1.5 
8.9 
RFBC + RL (ours) 85.1 77.6 68.1 
78.3 
-
-
-
RFBC + RF 
17.4 16.3 16.6 
17.1 
-
-
-
RL + RF 
6.0 
7.5 
7.0 
6.7 
-
-
-
Random 
12.3 14.9 12.3 
14.0 
-
-
-
Greedy 
19.7 16.7 21.7 
23.4 
-
-
-
Random DFS 
10.0 
9.5 
8.3 
10.0 
-
-
-
Greedy DFS 
31.1 23.8 22.7 
51.8 
-
-
-</p>
<p>Table 3 :
3Wikipedia graph statistics Year # Articles # Nodes # Edges # Words / Node Median Path Length2017 
4.92M 
36.3M 
359M 
110 
15 
2018 
5.27M 
38.5M 
387M 
109 
15 </p>
<p>5.3 Navigation on full 38M node Wikipedia graph: </p>
<p>Table 4 :
4Full graph navigation (38M nodes) -Success rate (%) 
Navigation 
Sentence Search </p>
<p>Embedding + Policy 
5 
10 
20 
5 
10 
20 </p>
<p>RoBERTA + Feed-forward 
85.5 80.1 71.5 91.2 88.9 77.5 
RoBERTA + Transformer 
85.3 88.3 87.9 81.6 75.4 70.9 
Embed train (ours) + Feed-forward 96.1 94.1 89.8 96.3 92.8 90.2 
Embed train (ours) + Transformer 
93.5 90.6 92.2 93.9 86.3 79.7 </p>
<p>Table 5 :
5Results on the FEVER benchmark (evidence retrieval only). The first two rows show the effect of adding our navigation scheme to the simple BM25 retrieval and TF-IDF re-ranker combination. Example navigation trajectories in the 38M node Wikipedia graph. The start and target nodes are shown in the first two rows. Parentheses indicate paragraph block (zero-indexed) within article. Note that in cases where the agent fails to find the target node (cols 1 &amp; 3), it visits ones that are very close by.Switching to a more powerful re-ranker (BigBird) results in a significant boost in F1 score, surpassing 
Stammbach [2021] who also use BigBird [Zaheer et al., 2020]. </p>
<p>Method 
Precision@5 Recall@5 F1@5 </p>
<p>BM25 + TF-IDF [Thorne et al., 2018] 
0.33 
0.40 
0.36 
BM25 + RFBC (Ours) + TF-IDF 
0.38 
0.55 
0.46 
BM25 + RFBC (Ours) + BigBird 
0.71 
0.75 
0.73 
Stammbach [2021] 
0.26 
0.94 
0.41 </p>
<p>Figure 2: </p>
<p>Table 6 :
6Results on the Natural Questions open domain benchmark (evidence retrieval only). Third and fourth rows show the effect of adding our navigation scheme to the simple BM25 retrieval and BigBird re-ranker. First row is a reference re-training of state-of-the-art method on our setup.Recall@1 Recall@2 Recall@3 Recall@4 Recall@5 </p>
<p>RocketQA [Qu et al., 2021] 
32.7 
43.8 
51.7 
58.3 
62.5 </p>
<p>BM25 
11.1 
17.3 
21.8 
24.9 
27.3 
BM25 + BigBird ReRank 
22.2 
30.2 
35.1 
38.7 
41.2 </p>
<p>BM25 + RFBC (Ours) + BigBird ReRank 
31.6 
41.3 
46.6 
49.8 
51.6 
Improvement 
+42.3% 
+36.7% 
+32.7% 
+28.7% 
+25.2% </p>
<p>Natural Questions (NQ) is a dataset collected from real users asking questions on the Google search engine which are answerable using Wikipedia. We use the Natural Question open-domain subset presented inLee et al. [2019] which has been aligned to Wikipedia dump of December 20, 2018 byKarpukhin et al. [2020]. In this split it has 58,880 questions for training and another 8,757 questions as development set for evaluation. (The test set is not aligned to Wikipedia passages so we do not evaluate on it.)</p>
<p>Table 7 :
7Asmptotic runtime analysis of RFBC and baselines. Shortest path BC O(E log V + E out T ) No nodes in the graph [large -10 7 ] E total edges in the graph [very large -10 8 − 10 9 ] E out average out degree of node (# of actions) [small -10 1 ] T length of trajectory, i.e. the maximum steps agents can run. [small -10 1 ] For most methods, the runtime is O(E out T ) because the method does some constant amount of work for each possible actions E o ut for O(T ) steps. For the random methods it is O(T ) because the work is constant per step (choose randomly). Shortest path distance has the worst runtime. The O(ElogV )Method 
Cost (Big-O) 
Scalable Accuracy (5-step 200k graph) </p>
<p>RFBC (ours) 
O(E out T ) 
Yes 
85.3 
Backwards BC 
O(E out T ) 
Yes 
2.3 
86.7 
RL 
O(E out T ) 
Yes 
41.4 
Random 
O(T ) 
Yes 
12.3 
Greedy 
O(E out T ) 
Yes 
19.7 
Random DFS 
O(T ) 
Yes 
10.0 
Greedy DFS 
O(E out T ) 
Yes 
31.1 
V </p>
<p>https://dumps.wikimedia.org/enwiki/ 4 https://archive.org/search.php?query=Wikimedia%20database%20dump%20of%20the% 20English%20Wikipedia 5 https://github.com/RaRe-Technologies/gensim/blob/master/gensim/corpora/ wikicorpus.py 6 We use a standard linker from https://cloud.google.com/natural-language/docs/ analyzing-entities 7 https://archive.org/download/enwiki-20170601/enwiki-20170601-pages-articles.xml. bz2 8 https://archive.org/download/enwiki-20181220/enwiki-20181220-pages-articles.xml. bz2
On a known fixed graph, we can formulate this an an MDP. However, in the case where the graph changes or in a setting where there are multiple possible graphs it is trivial to reformulate this as a POMDP where we only can see the current node and neighboring nodes
https://pypi.org/project/fuzzywuzzy/ 11 https://radimrehurek.com/gensim/models/tfidfmodel.html 12 https://github.com/dominiksinsaarland/document-level-FEVER 13 https://github.com/sheffieldnlp/fever-scorer/blob/9d9ed27637adddf73bc2f8e38c436bdc032c9f1f/ src/fever/scorer.py</p>
<p>Learning to retrieve reasoning paths over wikipedia graph for question answering. A Asai, K Hashimoto, H Hajishirzi, R Socher, C Xiong, International Conference on Learning Representations. A. Asai, K. Hashimoto, H. Hajishirzi, R. Socher, and C. Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgVHkrYDH.</p>
<p>Reading wikipedia to answer open-domain questions. D Chen, A Fisch, J Weston, A Bordes, abs/1704.00051CoRRD. Chen, A. Fisch, J. Weston, and A. Bordes. Reading wikipedia to answer open-domain questions. CoRR, abs/1704.00051, 2017.</p>
<p>Learning discrete energy-based models via auxiliary-variable local exploration. H Dai, R Singh, B Dai, C Sutton, D Schuurmans, Advances in Neural Information Processing Systems. H. Dai, R. Singh, B. Dai, C. Sutton, and D. Schuurmans. Learning discrete energy-based models via auxiliary-variable local exploration. Advances in Neural Information Processing Systems, 2020.</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. R Das, S Dhuliawala, M Zaheer, L Vilnis, I Durugkar, A Krishnamurthy, A J Smola, A Mccallum, abs/1711.05851CoRRR. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy, A. J. Smola, and A. McCallum. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. CoRR, abs/1711.05851, 2017.</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, S Legg, K Kavukcuoglu, abs/1802.01561ArXiv. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. ArXiv, abs/1802.01561, 2018.</p>
<p>Collaborative policy learning for open knowledge graph reasoning. C Fu, T Chen, M Qu, W Jin, X Ren, abs/1909.00230CoRRC. Fu, T. Chen, M. Qu, W. Jin, and X. Ren. Collaborative policy learning for open knowledge graph reasoning. CoRR, abs/1909.00230, 2019.</p>
<p>Neurally-guided semantic navigation in knowledge graph. L He, B Shao, Y Xiao, Y Li, T.-Y Liu, E Chen, H Xia, IEEE Transactions on Big Data. 83L. He, B. Shao, Y. Xiao, Y. Li, T.-Y. Liu, E. Chen, and H. Xia. Neurally-guided semantic navigation in knowledge graph. IEEE Transactions on Big Data, 8(3):607-615, 2022.</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems. 33J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.</p>
<p>Improving efficient neural ranking models with cross-architecture knowledge distillation. CoRR, abs. S Hofstätter, S Althammer, M Schröder, M Sertkan, A Hanbury, S. Hofstätter, S. Althammer, M. Schröder, M. Sertkan, and A. Hanbury. Improving efficient neural ranking models with cross-architecture knowledge distillation. CoRR, abs/2010.02666, 2020.</p>
<p>Dense passage retrieval for open-domain question answering. CoRR, abs. V Karpukhin, B Oguz, S Min, L Wu, S Edunov, D Chen, W Yih, V. Karpukhin, B. Oguz, S. Min, L. Wu, S. Edunov, D. Chen, and W. Yih. Dense passage retrieval for open-domain question answering. CoRR, abs/2004.04906, 2020.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, abs/1412.6980CoRRD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, abs/1312.6114CoRRD. P. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.</p>
<p>Internet-augmented language models through few-shot prompting for open-domain question answering. A Lazaridou, E Gribovskaya, W Stokowiec, N Grigorev, A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering, 2022.</p>
<p>FEVER development set leaderboard. F Leaderboard, 2022F. leaderboard. FEVER development set leaderboard. https://competitions.codalab.org/ competitions/18814#results, 2022.</p>
<p>Latent retrieval for weakly supervised open domain question answering. K Lee, M.-W Chang, K Toutanova, 10.18653/v1/P19-1612Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics012019K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, 01 2019. doi: 10.18653/v1/P19-1612.</p>
<p>Multi-hop knowledge graph reasoning with reward shaping. X V Lin, R Socher, C Xiong, abs/1808.10568CoRRX. V. Lin, R. Socher, and C. Xiong. Multi-hop knowledge graph reasoning with reward shaping. CoRR, abs/1808.10568, 2018.</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, abs/1907.11692A robustly optimized BERT pretraining approach. CoRR. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, X Jiang, K Cobbe, T Eloundou, G Krueger, K Button, M Knight, B Chess, J Schulman, abs/2112.09332CoRRR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021.</p>
<p>Webnav: A new large-scale task for natural language based sequential decision making. CoRR. R Nogueira, K Cho, abs/1602.02261R. Nogueira and K. Cho. Webnav: A new large-scale task for natural language based sequential decision making. CoRR, abs/1602.02261, 2016.</p>
<p>Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. Y Qu, Y Ding, J Liu, K Liu, R Ren, X Zhao, D Dong, H Wu, H Wang, abs/2010.08191CoRRY. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. CoRR, abs/2010.08191, 2020.</p>
<p>Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. Y Qu, Y Ding, J Liu, K Liu, R Ren, W X Zhao, D Dong, H Wu, H Wang, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesY. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, 2021.</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval. S Robertson, H Zaragoza, 10.1561/15000000193S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Foun- dations and Trends® in Information Retrieval, 3(4):333-389, 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL http://dx.doi.org/10.1561/1500000019.</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, abs/1910.01108ArXiv. V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019.</p>
<p>Revisiting the open-domain question answering pipeline. CoRR, abs. S J Semnani, M Pandey, S. J. Semnani and M. Pandey. Revisiting the open-domain question answering pipeline. CoRR, abs/2009.00914, 2020.</p>
<p>M-walk: Learning to walk over graphs using monte carlo tree search. Y Shen, J Chen, P.-S Huang, Y Guo, J Gao, Advances in Neural Information Processing Systems. 31Y. Shen, J. Chen, P.-S. Huang, Y. Guo, and J. Gao. M-walk: Learning to walk over graphs using monte carlo tree search. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Evidence selection as a token-level prediction task. D Stammbach, 10.18653/v1/2021.fever-1.2Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER). the Fourth Workshop on Fact Extraction and VERification (FEVER)Dominican RepublicAssociation for Computational LinguisticsD. Stammbach. Evidence selection as a token-level prediction task. In Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER), pages 14-20, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.fever-1.2. URL https://aclanthology.org/2021.fever-1.2.</p>
<p>Team DOMLIN: Exploiting evidence enhancement for the FEVER shared task. D Stammbach, G Neumann, 10.18653/v1/D19-6616Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER). the Second Workshop on Fact Extraction and VERification (FEVER)Hong Kong, ChinaAssociation for Computational LinguisticsD. Stammbach and G. Neumann. Team DOMLIN: Exploiting evidence enhancement for the FEVER shared task. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 105-109, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6616. URL https://aclanthology.org/D19-6616.</p>
<p>The web as a knowledge-base for answering complex questions. A Talmor, J Berant, abs/1803.06643CoRRA. Talmor and J. Berant. The web as a knowledge-base for answering complex questions. CoRR, abs/1803.06643, 2018.</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, Association for Computational Linguistics. dataset released under Apache license.J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Association for Computational Linguistics; dataset released under Apache license., June 2018.</p>
<p>Well-read students learn better: The impact of student initialization on knowledge distillation. I Turc, M Chang, K Lee, K Toutanova, abs/1908.08962I. Turc, M. Chang, K. Lee, and K. Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962, 2019. URL http: //arxiv.org/abs/1908.08962.</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Wikigraphs: A wikipedia text -knowledge graph paired dataset. L Wang, Y Li, Ö Aslan, O Vinyals, abs/2107.09556CoRRL. Wang, Y. Li, Ö. Aslan, and O. Vinyals. Wikigraphs: A wikipedia text -knowledge graph paired dataset. CoRR, abs/2107.09556, 2021.</p>
<p>Human wayfinding in information networks. R West, J Leskovec, WWW. R. West and J. Leskovec. Human wayfinding in information networks. In WWW, page 619-628, 2012a.</p>
<p>Automatic versus human navigation in information networks. R West, J Leskovec, ICWSM. R. West and J. Leskovec. Automatic versus human navigation in information networks. In ICWSM, 2012b.</p>
<p>Wikispeedia: An online game for inferring semantic distances between concepts. R West, J Pineau, D Precup, Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI). the 21st International Joint Conference on Artificial Intelligence (IJCAI)R. West, J. Pineau, and D. Precup. Wikispeedia: An online game for inferring semantic distances be- tween concepts. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), 2009.</p>
<p>Deeppath: A reinforcement learning method for knowledge graph reasoning. W Xiong, T Hoang, W Y Wang, abs/1707.06690CoRRW. Xiong, T. Hoang, and W. Y. Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. CoRR, abs/1707.06690, 2017.</p>
<p>Linkbert: Pretraining language models with document links. M Yasunaga, J Leskovec, P Liang, abs/2203.15827CoRRM. Yasunaga, J. Leskovec, and P. Liang. Linkbert: Pretraining language models with document links. CoRR, abs/2203.15827, 2022.</p>
<p>Distance distribution and average shortest path length estimation in real-world networks. Q Ye, B Wu, B Wang, International Conference on Advanced Data Mining and Applications. SpringerQ. Ye, B. Wu, and B. Wang. Distance distribution and average shortest path length estimation in real-world networks. In International Conference on Advanced Data Mining and Applications, pages 322-333. Springer, 2010.</p>
<p>Big bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, Advances in Neural Information Processing Systems. 33M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283-17297, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-225062392</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.12142v1.pdf" target="_blank">Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Sample efficiency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overfitting to training trajectories, and thus model-based value estimation and policy search will be pone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efficiency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIRD world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BrIdging Reality and Dream (BIRD) latent world model (RSSM-based, PlaNet/Dreamer style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses a latent-space, VAE-style world model (representation, transition, observation, reward) based on RSSM (deterministic+stochastic latent dynamics) to generate multi-step imaginary trajectories for analytic-gradient model-based RL; the model is trained with a VAE-like objective and augmented in BIRD by a mutual-information objective tying imagined and real trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM latent world model (as used in BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space generative model with four modules: representation model p(s_t | s_{t-1}, a_{t-1}, o_t) (encoder), transition model p(s_t | s_{t-1}, a_{t-1}) implemented as RSSM (combining deterministic recurrent unit and stochastic latent variables), observation decoder p(o_t | s_t), and reward predictor p(r_t | s_t). Trained with a VAE-like objective (reconstruction terms + KL regularizer) and multi-step prediction; used to unroll latent imaginary trajectories of horizon H for stochastic value gradients and policy optimization. CNN encoder for images, GRU/deterministic unit size 300, stochastic latent size 40, imagination horizon H=15.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic + deterministic RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual continuous control (DeepMind Control Suite from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE reconstruction log-likelihood and KL term during model training; additional evaluation metric: latent-state discrepancy (distance between predicted latent states and encoder-derived posterior latents) used as model error for evaluation (because pixel reconstruction dominated by background); also qualitative prediction accuracy on key actions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: BIRD reports substantially lower latent-state prediction error than Dreamer (figure comparisons) and more accurate predictions of key action outcomes; no absolute numerical fidelity values provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural latent model; not designed for explicit interpretability. The paper notes that learned knowledge/policy are 'not friendly to humans' and interpretable explanations are not provided. Interpretability is limited to visualizing predicted latent trajectories and decoded reconstructions to inspect key-action forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted trajectories and decoded observations for key actions (qualitative comparisons); no explicit disentanglement, attention, or symbolic extraction methods used; future work mentions possible causal-inference integration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Authors state same computational complexity as Dreamer; model uses CNN + GRU, deterministic unit size 300, stochastic unit 40. Training reported on a single NVIDIA 2080Ti + single CPU: ~8 hours per 1 million environment samples under their setup. Hyperparameters: batch size 50, sequence length 50, buffer 100k, imagination horizon 15. No total parameter count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample-efficiency: BIRD is reported to be significantly more sample-efficient than PlaNet and Dreamer (for some tasks uses roughly half the samples to reach same score); analytic-gradient planning (used here) is described as more computationally efficient than MPC/shooting approaches in pixel domains. Compute-per-step comparable to Dreamer (authors used same architectures and report same computational complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On DeepMind Control Suite (visual tasks) BIRD achieves state-of-the-art sample efficiency relative to Dreamer and PlaNet and achieves comparable or higher converged returns; specific claims include reaching same Hopper task scores with ~50% of samples compared to PlaNet/Dreamer and matching model-free baselines trained with orders-of-magnitude more data (D4PG/A3C). No numeric reward scores are tabulated in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The latent world model's fidelity (especially on 'key actions') correlates with better policy generalization to the real environment: reducing latent-state prediction error and increasing mutual information between imagined and real trajectories improved real-world returns. The paper argues that minimizing standard model prediction error alone is insufficient — tying model learning to mutual information with real trajectories yields imaginations that are more useful for policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>The paper discusses trade-offs between (1) model error minimization and (2) policy search/entropy: increasing policy entropy without addressing model-reality discrepancy (Soft-BIRD ablation) can hurt final performance; mutual-information regularization couples model learning and policy to prioritize imaginations that match reality. Higher exploration (entropy) increases candidate policies but requires confidence-weighting to avoid learning from low-confidence imaginations. No explicit compute vs fidelity quantitative tradeoff is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices: use of RSSM (deterministic+stochastic latents) for transition dynamics; VAE-style objective with KL regularizer β; multi-step prediction (as in Dreamer), removal of latent overshooting from PlaNet; latent sizes (deterministic 300, stochastic 40); CNN encoder + GRU; imagination horizon H=15; mutual-information term decomposed into model error minimization (optimize ψ) and policy entropy + confidence-aware weighting (optimize θ); confidence weight computed from log-likelihood log P(τ_img_roll | τ_img) and normalized per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to Dreamer and PlaNet (same architecture family) and to model-free baselines (D4PG, A3C): BIRD shows superior sample efficiency and lower latent-model error vs Dreamer; compared qualitatively to MPC-based planners (PlaNet-style alternatives) which are more computationally expensive due to many rollouts. Ablation (Soft-BIRD) shows that entropy augmentation alone does not match mutual-information approach.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends balancing model-error minimization with policy entropy and confidence-aware reweighting (mutual-information maximization) to get imaginations that transfer to reality; hyperparameters used include mutual-information coefficient 1e-8, β (KL) = 1e-1 (text shows 1 × 10^-1? — paper cites β but main config lists β = 1), H=15, deterministic/stochastic sizes as above. No formal optimality proof; guidance is empirical: jointly optimize model and policy via mutual information rather than only improving model reconstruction or policy entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent imagination model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dreamer is an analytic-gradient model-based RL method that learns a latent world model (RSSM-like) to generate imagined trajectories in latent space and applies stochastic value gradients to optimize policies end-to-end; it is used here as the primary baseline and implementation reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer world model (RSSM-based latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent dynamics model using RSSM (deterministic recurrent units + stochastic latents), encoder/decoder (VAE-like) and reward predictor; generates latent rollouts for policy/value optimization via analytic gradients and multi-step value expansion. Uses CNN encoder for pixels and GRU for dynamics; multi-step prediction training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual continuous control (DeepMind Control Suite) and other pixel-based control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE reconstruction + KL training objective; authors of this paper evaluate model error as discrepancy between model-predicted latents and posterior-encoded latents to compare fidelity (Dreamer exhibits larger latent-state discrepancy than BIRD in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper Dreamer serves as baseline and is reported to have higher latent-state prediction error and worse predictions on some key actions compared to BIRD; no absolute numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Dreamer's model is a neural latent model and treated as a black box; interpretability not emphasized in Dreamer or in this paper beyond visualizing decoded predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoded image predictions from latent rollouts is used for qualitative assessment; no explicit interpretability tools used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported to have similar computational complexity to BIRD in this paper; analytic-gradient approach is more compute-efficient than MPC in pixel domains. Original Dreamer computational costs not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>In this work BIRD claims equal computational complexity to Dreamer but better sample efficiency and lower latent-model error; analytic-gradient Dreamer family generally cheaper than MPC/shooting methods on pixel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Baseline method; in this paper Dreamer is outperformed by BIRD in sample efficiency and final returns on several visual control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreamer's imaginations alone can lead to effective policies, but without mechanisms to align imaginations with real trajectories (mutual information), Dreamer may overfit imperfect model rollouts leading to poorer generalization in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Dreamer relies on multi-step latent predictions which can accumulate error; this paper argues that analytic-gradient policy updates through such imperfect models can get stuck in poor local optima unless augmented (as BIRD does) to account for reality-imagination mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>RSSM latent dynamics, VAE training, multi-step value expansion with value function bootstrapping, CNN encoder and GRU dynamics; choices intended to balance expressivity and tractability of latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against PlaNet, BIRD (this paper) and model-free baselines; Dreamer is more compute-efficient than MPC and typically more sample-efficient than naive model-free baselines but here is outperformed by BIRD in several metrics due to reality-aware regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail in this paper beyond noting that BIRD used the same architecture and hyperparameters as Dreamer; BIRD suggests that augmenting Dreamer-style models with mutual-information and confidence-aware weighting improves transfer to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet / RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (Learning latent dynamics for planning from pixels) / RSSM transition model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PlaNet introduced the RSSM (Recurrent State Space Model) combining deterministic recurrent and stochastic latent components for latent dynamics learning from pixels and used planning in latent space; BIRD uses RSSM for its transition module and references PlaNet as a foundational model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (Recurrent State Space Model) as in PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transition model combining deterministic recurrent state (e.g., GRU) and stochastic latent variables to model uncertainty and multi-step dynamics in latent space; used together with encoder/decoder and reward predictor in a VAE-like training framework enabling multi-step latent prediction and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (hybrid deterministic+stochastic RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control / planning from pixels (DeepMind Control Suite, similar benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE losses (observation reconstruction, reward prediction), KL regularizer; PlaNet also uses latent overshooting (PlaNet chooses multi-step objectives), though BIRD uses PlaNet's RSSM but removes latent overshooting and uses Dreamer-style multi-step prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not directly quantified in this paper beyond being a referenced baseline; BIRD reports outperforming PlaNet in sample efficiency on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>RSSM is a neural latent model with limited interpretability; the model captures compact latent dynamics but latent dimensions are not mapped to human-interpretable variables in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not used in BIRD; PlaNet/RSSM assessment typically relies on rollout visualizations and predictive accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>PlaNet-style models involve planning (MPC) which can be computationally expensive due to many rollouts; BIRD contrasts analytic-gradient approaches (used with RSSM here) as more computationally efficient in pixel domains. Exact PlaNet compute numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PlaNet (planning) is described as more computationally expensive than analytic-gradient methods; BIRD and Dreamer (analytic gradient) are positioned as more efficient for deep networks on pixel inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>PlaNet is a strong model-based baseline on visual control; BIRD reports higher sample efficiency and similar-or-better final performance on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM provides an expressive latent model that is useful for planning and imagination; however, fidelity drift over multi-step rollouts remains a challenge that can harm policy generalization unless addressed (e.g., via BIRD's mutual-information objective).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Planning-based PlaNet can achieve powerful performance but at higher computational cost (many rollouts); RSSM's deterministic+stochastic design trades representational power against potential latent-rollout accumulation errors that must be mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>PlaNet's RSSM mixes deterministic recurrent units with stochastic latents and uses multi-step prediction losses; BIRD inherits RSSM but removes latent overshooting and pairs it with mutual-information and confidence-aware policy weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet (MPC planning) vs Dreamer/BIRD (analytic gradients): PlaNet's planning is more expensive; Dreamer and BIRD are more computationally efficient on pixel tasks though require careful handling of model-reality discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper indicates RSSM is a good compromise for pixel-based control (deterministic+stochastic latents) and that combining such models with mutual-information objectives and confidence-aware optimization improves real-world generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero: planning with a learned model and MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MuZero is cited as a related model-based approach that learns a model for planning (value/policy/reward prediction) and uses Monte Carlo Tree Search (MCTS) to plan; it is contrasted with analytic-gradient methods for computational considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero learned planning model (value/policy/reward predictor + dynamics for MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model that predicts policy, value and reward in a learned latent space and is used in conjunction with MCTS for planning; not used in experiments of this paper, only referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned planning model used with search (hybrid model + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games and Atari-style discrete control (games where MCTS is applicable); not the focus of the current visual continuous-control experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not discussed in this paper; MuZero evaluates planning/model quality via game-playing performance and MCTS rollouts in its own work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not applicable in this paper (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here; MuZero is primarily a learned black-box planning model with MCTS external interpretability via search trees.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper contrasts MuZero's MCTS-based planning as computationally heavier than analytic-gradient approaches for pixel-control domains; no numerical costs provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MuZero (MCTS) achieves strong performance on discrete-game benchmarks but requires expensive search; analytic-gradient latent-rollout methods (Dreamer/BIRD) are described as more efficient for high-dimensional pixel-control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero-style planning is highly effective in domains amenable to deep search but is less suitable for pixel-based continuous action domains where many rollouts are costly; this motivates analytic-gradient latent rollout approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MCTS/planning yields high sample efficiency or strong final performance in some domains but at significant computational cost, motivating alternative analytic-gradient approaches used in BIRD.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not applicable here beyond being mentioned as an alternative planning approach that learns a model and uses search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper positions MuZero-style search as complementary but more computationally expensive relative to analytic-gradient latent-rollout methods (Dreamer/BIRD) for the visual control tasks considered.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper for MuZero; merely cited as an example of learned-model planning using MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 1)</em></li>
                <li>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1228",
    "paper_id": "paper-225062392",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "BIRD world model (RSSM)",
            "name_full": "BrIdging Reality and Dream (BIRD) latent world model (RSSM-based, PlaNet/Dreamer style)",
            "brief_description": "The paper uses a latent-space, VAE-style world model (representation, transition, observation, reward) based on RSSM (deterministic+stochastic latent dynamics) to generate multi-step imaginary trajectories for analytic-gradient model-based RL; the model is trained with a VAE-like objective and augmented in BIRD by a mutual-information objective tying imagined and real trajectories.",
            "citation_title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "RSSM latent world model (as used in BIRD)",
            "model_description": "Latent-space generative model with four modules: representation model p(s_t | s_{t-1}, a_{t-1}, o_t) (encoder), transition model p(s_t | s_{t-1}, a_{t-1}) implemented as RSSM (combining deterministic recurrent unit and stochastic latent variables), observation decoder p(o_t | s_t), and reward predictor p(r_t | s_t). Trained with a VAE-like objective (reconstruction terms + KL regularizer) and multi-step prediction; used to unroll latent imaginary trajectories of horizon H for stochastic value gradients and policy optimization. CNN encoder for images, GRU/deterministic unit size 300, stochastic latent size 40, imagination horizon H=15.",
            "model_type": "latent world model (stochastic + deterministic RSSM)",
            "task_domain": "Visual continuous control (DeepMind Control Suite from pixels)",
            "fidelity_metric": "VAE reconstruction log-likelihood and KL term during model training; additional evaluation metric: latent-state discrepancy (distance between predicted latent states and encoder-derived posterior latents) used as model error for evaluation (because pixel reconstruction dominated by background); also qualitative prediction accuracy on key actions.",
            "fidelity_performance": "Qualitative: BIRD reports substantially lower latent-state prediction error than Dreamer (figure comparisons) and more accurate predictions of key action outcomes; no absolute numerical fidelity values provided in the paper.",
            "interpretability_assessment": "Primarily a black-box neural latent model; not designed for explicit interpretability. The paper notes that learned knowledge/policy are 'not friendly to humans' and interpretable explanations are not provided. Interpretability is limited to visualizing predicted latent trajectories and decoded reconstructions to inspect key-action forecasts.",
            "interpretability_method": "Visualization of predicted trajectories and decoded observations for key actions (qualitative comparisons); no explicit disentanglement, attention, or symbolic extraction methods used; future work mentions possible causal-inference integration.",
            "computational_cost": "Authors state same computational complexity as Dreamer; model uses CNN + GRU, deterministic unit size 300, stochastic unit 40. Training reported on a single NVIDIA 2080Ti + single CPU: ~8 hours per 1 million environment samples under their setup. Hyperparameters: batch size 50, sequence length 50, buffer 100k, imagination horizon 15. No total parameter count reported.",
            "efficiency_comparison": "Sample-efficiency: BIRD is reported to be significantly more sample-efficient than PlaNet and Dreamer (for some tasks uses roughly half the samples to reach same score); analytic-gradient planning (used here) is described as more computationally efficient than MPC/shooting approaches in pixel domains. Compute-per-step comparable to Dreamer (authors used same architectures and report same computational complexity).",
            "task_performance": "On DeepMind Control Suite (visual tasks) BIRD achieves state-of-the-art sample efficiency relative to Dreamer and PlaNet and achieves comparable or higher converged returns; specific claims include reaching same Hopper task scores with ~50% of samples compared to PlaNet/Dreamer and matching model-free baselines trained with orders-of-magnitude more data (D4PG/A3C). No numeric reward scores are tabulated in the paper text.",
            "task_utility_analysis": "The latent world model's fidelity (especially on 'key actions') correlates with better policy generalization to the real environment: reducing latent-state prediction error and increasing mutual information between imagined and real trajectories improved real-world returns. The paper argues that minimizing standard model prediction error alone is insufficient — tying model learning to mutual information with real trajectories yields imaginations that are more useful for policy improvement.",
            "tradeoffs_observed": "The paper discusses trade-offs between (1) model error minimization and (2) policy search/entropy: increasing policy entropy without addressing model-reality discrepancy (Soft-BIRD ablation) can hurt final performance; mutual-information regularization couples model learning and policy to prioritize imaginations that match reality. Higher exploration (entropy) increases candidate policies but requires confidence-weighting to avoid learning from low-confidence imaginations. No explicit compute vs fidelity quantitative tradeoff is reported.",
            "design_choices": "Key design choices: use of RSSM (deterministic+stochastic latents) for transition dynamics; VAE-style objective with KL regularizer β; multi-step prediction (as in Dreamer), removal of latent overshooting from PlaNet; latent sizes (deterministic 300, stochastic 40); CNN encoder + GRU; imagination horizon H=15; mutual-information term decomposed into model error minimization (optimize ψ) and policy entropy + confidence-aware weighting (optimize θ); confidence weight computed from log-likelihood log P(τ_img_roll | τ_img) and normalized per batch.",
            "comparison_to_alternatives": "Compared experimentally to Dreamer and PlaNet (same architecture family) and to model-free baselines (D4PG, A3C): BIRD shows superior sample efficiency and lower latent-model error vs Dreamer; compared qualitatively to MPC-based planners (PlaNet-style alternatives) which are more computationally expensive due to many rollouts. Ablation (Soft-BIRD) shows that entropy augmentation alone does not match mutual-information approach.",
            "optimal_configuration": "Paper recommends balancing model-error minimization with policy entropy and confidence-aware reweighting (mutual-information maximization) to get imaginations that transfer to reality; hyperparameters used include mutual-information coefficient 1e-8, β (KL) = 1e-1 (text shows 1 × 10^-1? — paper cites β but main config lists β = 1), H=15, deterministic/stochastic sizes as above. No formal optimality proof; guidance is empirical: jointly optimize model and policy via mutual information rather than only improving model reconstruction or policy entropy.",
            "uuid": "e1228.0",
            "source_info": {
                "paper_title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (latent imagination model-based RL)",
            "brief_description": "Dreamer is an analytic-gradient model-based RL method that learns a latent world model (RSSM-like) to generate imagined trajectories in latent space and applies stochastic value gradients to optimize policies end-to-end; it is used here as the primary baseline and implementation reference.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "Dreamer world model (RSSM-based latent model)",
            "model_description": "Latent dynamics model using RSSM (deterministic recurrent units + stochastic latents), encoder/decoder (VAE-like) and reward predictor; generates latent rollouts for policy/value optimization via analytic gradients and multi-step value expansion. Uses CNN encoder for pixels and GRU for dynamics; multi-step prediction training.",
            "model_type": "latent world model (RSSM)",
            "task_domain": "Visual continuous control (DeepMind Control Suite) and other pixel-based control tasks",
            "fidelity_metric": "VAE reconstruction + KL training objective; authors of this paper evaluate model error as discrepancy between model-predicted latents and posterior-encoded latents to compare fidelity (Dreamer exhibits larger latent-state discrepancy than BIRD in this paper).",
            "fidelity_performance": "In this paper Dreamer serves as baseline and is reported to have higher latent-state prediction error and worse predictions on some key actions compared to BIRD; no absolute numbers given in this paper.",
            "interpretability_assessment": "Dreamer's model is a neural latent model and treated as a black box; interpretability not emphasized in Dreamer or in this paper beyond visualizing decoded predictions.",
            "interpretability_method": "Visualization of decoded image predictions from latent rollouts is used for qualitative assessment; no explicit interpretability tools used.",
            "computational_cost": "Reported to have similar computational complexity to BIRD in this paper; analytic-gradient approach is more compute-efficient than MPC in pixel domains. Original Dreamer computational costs not enumerated here.",
            "efficiency_comparison": "In this work BIRD claims equal computational complexity to Dreamer but better sample efficiency and lower latent-model error; analytic-gradient Dreamer family generally cheaper than MPC/shooting methods on pixel tasks.",
            "task_performance": "Baseline method; in this paper Dreamer is outperformed by BIRD in sample efficiency and final returns on several visual control tasks.",
            "task_utility_analysis": "Dreamer's imaginations alone can lead to effective policies, but without mechanisms to align imaginations with real trajectories (mutual information), Dreamer may overfit imperfect model rollouts leading to poorer generalization in some tasks.",
            "tradeoffs_observed": "Dreamer relies on multi-step latent predictions which can accumulate error; this paper argues that analytic-gradient policy updates through such imperfect models can get stuck in poor local optima unless augmented (as BIRD does) to account for reality-imagination mismatch.",
            "design_choices": "RSSM latent dynamics, VAE training, multi-step value expansion with value function bootstrapping, CNN encoder and GRU dynamics; choices intended to balance expressivity and tractability of latent rollouts.",
            "comparison_to_alternatives": "Compared against PlaNet, BIRD (this paper) and model-free baselines; Dreamer is more compute-efficient than MPC and typically more sample-efficient than naive model-free baselines but here is outperformed by BIRD in several metrics due to reality-aware regularization.",
            "optimal_configuration": "Not discussed in detail in this paper beyond noting that BIRD used the same architecture and hyperparameters as Dreamer; BIRD suggests that augmenting Dreamer-style models with mutual-information and confidence-aware weighting improves transfer to reality.",
            "uuid": "e1228.1",
            "source_info": {
                "paper_title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PlaNet / RSSM",
            "name_full": "PlaNet (Learning latent dynamics for planning from pixels) / RSSM transition model",
            "brief_description": "PlaNet introduced the RSSM (Recurrent State Space Model) combining deterministic recurrent and stochastic latent components for latent dynamics learning from pixels and used planning in latent space; BIRD uses RSSM for its transition module and references PlaNet as a foundational model.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "RSSM (Recurrent State Space Model) as in PlaNet",
            "model_description": "Transition model combining deterministic recurrent state (e.g., GRU) and stochastic latent variables to model uncertainty and multi-step dynamics in latent space; used together with encoder/decoder and reward predictor in a VAE-like training framework enabling multi-step latent prediction and planning.",
            "model_type": "latent world model (hybrid deterministic+stochastic RSSM)",
            "task_domain": "Visual control / planning from pixels (DeepMind Control Suite, similar benchmarks)",
            "fidelity_metric": "VAE losses (observation reconstruction, reward prediction), KL regularizer; PlaNet also uses latent overshooting (PlaNet chooses multi-step objectives), though BIRD uses PlaNet's RSSM but removes latent overshooting and uses Dreamer-style multi-step prediction.",
            "fidelity_performance": "Not directly quantified in this paper beyond being a referenced baseline; BIRD reports outperforming PlaNet in sample efficiency on several tasks.",
            "interpretability_assessment": "RSSM is a neural latent model with limited interpretability; the model captures compact latent dynamics but latent dimensions are not mapped to human-interpretable variables in the paper.",
            "interpretability_method": "Not used in BIRD; PlaNet/RSSM assessment typically relies on rollout visualizations and predictive accuracy metrics.",
            "computational_cost": "PlaNet-style models involve planning (MPC) which can be computationally expensive due to many rollouts; BIRD contrasts analytic-gradient approaches (used with RSSM here) as more computationally efficient in pixel domains. Exact PlaNet compute numbers not provided in this paper.",
            "efficiency_comparison": "PlaNet (planning) is described as more computationally expensive than analytic-gradient methods; BIRD and Dreamer (analytic gradient) are positioned as more efficient for deep networks on pixel inputs.",
            "task_performance": "PlaNet is a strong model-based baseline on visual control; BIRD reports higher sample efficiency and similar-or-better final performance on evaluated tasks.",
            "task_utility_analysis": "RSSM provides an expressive latent model that is useful for planning and imagination; however, fidelity drift over multi-step rollouts remains a challenge that can harm policy generalization unless addressed (e.g., via BIRD's mutual-information objective).",
            "tradeoffs_observed": "Planning-based PlaNet can achieve powerful performance but at higher computational cost (many rollouts); RSSM's deterministic+stochastic design trades representational power against potential latent-rollout accumulation errors that must be mitigated.",
            "design_choices": "PlaNet's RSSM mixes deterministic recurrent units with stochastic latents and uses multi-step prediction losses; BIRD inherits RSSM but removes latent overshooting and pairs it with mutual-information and confidence-aware policy weighting.",
            "comparison_to_alternatives": "PlaNet (MPC planning) vs Dreamer/BIRD (analytic gradients): PlaNet's planning is more expensive; Dreamer and BIRD are more computationally efficient on pixel tasks though require careful handling of model-reality discrepancy.",
            "optimal_configuration": "Paper indicates RSSM is a good compromise for pixel-based control (deterministic+stochastic latents) and that combining such models with mutual-information objectives and confidence-aware optimization improves real-world generalization.",
            "uuid": "e1228.2",
            "source_info": {
                "paper_title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MuZero (mentioned)",
            "name_full": "MuZero: planning with a learned model and MCTS",
            "brief_description": "MuZero is cited as a related model-based approach that learns a model for planning (value/policy/reward prediction) and uses Monte Carlo Tree Search (MCTS) to plan; it is contrasted with analytic-gradient methods for computational considerations.",
            "citation_title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
            "mention_or_use": "mention",
            "model_name": "MuZero learned planning model (value/policy/reward predictor + dynamics for MCTS)",
            "model_description": "Model that predicts policy, value and reward in a learned latent space and is used in conjunction with MCTS for planning; not used in experiments of this paper, only referenced as related work.",
            "model_type": "learned planning model used with search (hybrid model + MCTS)",
            "task_domain": "Board games and Atari-style discrete control (games where MCTS is applicable); not the focus of the current visual continuous-control experiments.",
            "fidelity_metric": "Not discussed in this paper; MuZero evaluates planning/model quality via game-playing performance and MCTS rollouts in its own work.",
            "fidelity_performance": "Not applicable in this paper (only referenced).",
            "interpretability_assessment": "Not discussed here; MuZero is primarily a learned black-box planning model with MCTS external interpretability via search trees.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Paper contrasts MuZero's MCTS-based planning as computationally heavier than analytic-gradient approaches for pixel-control domains; no numerical costs provided here.",
            "efficiency_comparison": "MuZero (MCTS) achieves strong performance on discrete-game benchmarks but requires expensive search; analytic-gradient latent-rollout methods (Dreamer/BIRD) are described as more efficient for high-dimensional pixel-control tasks.",
            "task_performance": "Not evaluated in this paper.",
            "task_utility_analysis": "MuZero-style planning is highly effective in domains amenable to deep search but is less suitable for pixel-based continuous action domains where many rollouts are costly; this motivates analytic-gradient latent rollout approaches.",
            "tradeoffs_observed": "MCTS/planning yields high sample efficiency or strong final performance in some domains but at significant computational cost, motivating alternative analytic-gradient approaches used in BIRD.",
            "design_choices": "Not applicable here beyond being mentioned as an alternative planning approach that learns a model and uses search.",
            "comparison_to_alternatives": "Paper positions MuZero-style search as complementary but more computationally expensive relative to analytic-gradient latent-rollout methods (Dreamer/BIRD) for the visual control tasks considered.",
            "optimal_configuration": "Not discussed in this paper for MuZero; merely cited as an example of learned-model planning using MCTS.",
            "uuid": "e1228.3",
            "source_info": {
                "paper_title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "World models",
            "rating": 1,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
            "rating": 1,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        }
    ],
    "cost": 0.016275249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</p>
<p>Guangxiang Zhu guangxiangzhu@outlook.com 
School of Software
IIIS Tsinghua University
Tsinghua University
EECS University of Michigan
IIIS Tsinghua University</p>
<p>Minghao Zhang mehoozhang@gmail.com 
School of Software
IIIS Tsinghua University
Tsinghua University
EECS University of Michigan
IIIS Tsinghua University</p>
<p>Honglak Lee honglak@eecs.umich.edu 
School of Software
IIIS Tsinghua University
Tsinghua University
EECS University of Michigan
IIIS Tsinghua University</p>
<p>Chongjie Zhang chongjie@tsinghua.edu.cn 
School of Software
IIIS Tsinghua University
Tsinghua University
EECS University of Michigan
IIIS Tsinghua University</p>
<p>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</p>
<p>Sample efficiency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overfitting to training trajectories, and thus model-based value estimation and policy search will be prone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efficiency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) is proposed as a general-purpose learning framework for artificial intelligence problems, and has led to tremendous progress in a variety of domains [1,2,3,4]. Modelfree RL adopts a trail-and-error paradigm, which directly learns a mapping function from observations to values or actions through interactions with environments. It has achieved remarkable performance in certain video games and continuous control tasks because of its simplicity and minimal assumptions about environments. However, model-free approaches are not yet sample efficient and require several orders of magnitude more training samples than human learning, which limits its applications on real-world tasks [5].</p>
<p>A promising direction for improving sample efficiency is to explore model-based RL, which first builds an action-conditioned world model and then performs planning or policy search based on the learned model. The world model needs to encode the representations and dynamics of an environment is then used as a "dreamer" to do multi-step lookaheads for planning or policy search. Recently, world models based on deep neural networks were developed to handle dynamics in complex highdimensional environments, which offers opportunities for learning model-based polices with visual observations [6,7,8,9,10,11,12,13].</p>
<p>Model-based frameworks can be roughly grouped into four categories. First, Dyna-style algorithms alternate between building the world model from interactions with environments and performing policy optimization on simulated data generated by the learned model [14,15,16,17,11]. Second, model predictive control (MPC) and shooting algorithms alternate model learning, planning and action execution [18,19,20]. Third, model-augmented value expansion algorithms use model-based rollouts to improve targets for model-free temporal difference (TD) updates or policy gradients [21,9,6,10]. Fourth, analytic-gradient algorithms leverage the gradients of the model-based imaginary returns with respect to the policy and directly propagate such gradients through a differentiable world model to the policy network [22,23,24,25,26,27,13]. Compared to conventional planning algorithms that generate numerous rollouts to select the highest performing action sequence, analyticgradient algorithm is more computationally efficient, especially in complex domains with deep neural networks. Dreamer [13] as a landmark of analytic-gradient model-based RL, achieves state-of-the-art performance on visual control tasks.</p>
<p>However, most existing breakthroughs on analytic gradients focus on optimizing the policy on imaginary trajectories and leave the discrepancy between imagination and reality largely unstudied, which often bottlenecks their performance on real trajectories. In practice, a learning-based world model is not perfect, especially in complex environments. Unrolling with an imperfect model for multiple steps generates a large accumulative error, leaving a gap between the generated trajectories and reality. If we directly optimize policy based on the analytic gradients through the imaginary trajectories, the policy will tend to deviate from reality and get sucked in an inferior local solution.</p>
<p>Evidence from humans' cognition and learning in the physical world suggests that humans naturally have the capacity of self-reflection and introspection. In everyday life, we track and review our past thoughts and imaginations, introspect to further understand our internal states and interactions with the external world, and change our values and behavior patterns accordingly [28,29]. Inspired by this insight, our basic idea is to leverage information from real trajectories to endow policy improvement on imaginations with awareness of discrepancy between imagination and reality. We propose a novel reality-aware model-based framework, called BrIdging Reality and Dream (BIRD), which performs differentiable planning on imaginary trajectories, as well as enables adaptive generalization to reality for learned policy by optimizing mutual information between imaginary and real trajectories. Our model-based policy optimization framework naturally unifies confidence-aware analytic gradients, entropy regularization maximization, and model learning. We conduct experiments on challenging visual control benchmarks (DeepMind Control Suite with image inputs [30]) and the results demonstrate that BIRD achieves state-of-the-art performance in terms of sample efficiency. Our ablation study further verifies the superiority of BIRD benefits from mutual information maximization rather than from the increase of policy entropy.</p>
<p>Related Work</p>
<p>Model-Based Reinforcement Learning</p>
<p>Model-based RL exhibits high sample efficiency and has been widely used in several real-world control tasks, such as robotics [31,32,7]. Dyna-style algorithms [14,15,16,17,11] optimize policies with samples generated from a learned world model. Model predictive control (MPC) and shooting methods [18,19,20] leverage planning to select actions, but suffer from expensive computation. In model-augmented value expansion approaches, MVE [21], VPN [6] and STEVE [9] use model-based rollouts to improve targets for model-free TD updates. MuZero [10] further incorporates Monte-Carlo tree search (MCTS) and achieves remarkable performance on Atari and board games. To manage visual control tasks, VisualMPC [33] introduces a visual prediction model to keep track of entities through occlusion by temporal skip connections. PlaNet [12] improves the model learning by combining deterministic and stochastic latent dynamics models. [34] presents a summary of model-based approaches and benchmarks popular algorithms for comparisons and extensions.</p>
<p>Analytic Value Gradients If a differentiable world model is available, analytic value gradients are proposed to directly update the policy by gradients that flow through the world model. PILCO [24] and iLQR [25] compute an analytic gradient by assuming Gaussian processes and linear functions for the dynamics model, respectively. Guided policy search (GPS) [26,35,36,37,38] uses deep neural networks to distill behaviors from the iLQR controller. Value Gradients (VG) [22] and Stochastic Value Gradients (SVG) [23] provide a new direction to calculate analytic value gradients through a generic differentiable world model. Dreamer [13] and IVG [27] further extend SVG by generating imaginary rollouts in the latent space. However, these works focus on improving the policy in imaginations, leaving the discrepancy between imagination and reality largely unstudied. Our approach enables policy generalization to real-world interactions by maximizing mutual information between imagination and real trajectories, while optimizing the policy on imaginary trajectories. In addition, alternative end-to-end planning methods [39,40] leverage analytic gradients, but they either focus on online planning in simple tasks [39] or require goal images and distance metrics for the reward function [40].</p>
<p>Information-Based Optimization</p>
<p>In addition to maximizing the expected return objective, a reliable RL agent may exhibit more characteristics, like meaningful representations, strong generalization, and efficient exploration. Deep information-based methods [41,42,43,44] recently show progress towards this direction. [45,46,47] are proposed to learn more efficient representations. Maximum entropy RL maximizes the entropy regularized return to obtain a robust policy [48,49] and [50,51] further connect policy optimization under such regularization with value based RL.</p>
<p>[52] learns a goal-conditioned policy with information bottleneck to identify decision states. IDS [53] estimates the information gain for a sampling-based exploration strategy. These algorithms mainly focus on facilitating policy learning in the model-free setting, while BIRD aims at bridging imagination and reality by mutual information maximization in the context of model-based RL.</p>
<p>Preliminaries</p>
<p>Reinforcement Learning</p>
<p>A reinforcement learning agent aims at learning a policy to maximize the cumulative rewards by exploring in a Markov Decision Processes (MDP) [54]. Normally, we use denote time step as t and introduce state s t ∈ S, action a t ∈ A, reward function r(s t , a t ), a policy π θ (s), and a transition probability p(s t+1 |s t , a t ) to characterize the process of interacting with the environment. The goal of the agent is to find a policy parameter θ that maximizes the long-horizon summed rewards represented by a value function v φ (s t )</p>
<p>.
= E t+H i=t γ i−t r i parameterized with φ.
In model-based RL, the agent builds a world model p ψ parameterized by ψ for environmental dynamics p and reward function r, and then performs planning or policy search based on this model.</p>
<p>World Model</p>
<p>Considering that several complex tasks (e.g., visual control tasks [30]) are partially observable Markov decision process (POMDP), this paper adopts a similar world model with PlaNet [12] and Dreamer [13], which learns latent states from the history of visual observations and models the latent dynamics by LSTM-like recurrent networks. Specifically, the world model consists of the following modules:
Representation model : s t ∼ p ψ (s t |s t−1 , a t−1 , o t ) Transition model : s t ∼ p ψ (s t |s t−1 , a t−1 ) Observation model : o t ∼ p ψ (o t |s t ) Reward model : r t ∼ p ψ (r t |s t ).(1)
The representation model encodes the image input into a compact latent space and the long-horizon dynamics on latent states are captured by a latent transition model. We use RSSM [12] as our transition model, which combines deterministic and stochastic transition model in order to learn dynamics more accurately and efficiently. For each latent state on the predicted trajectories, observation model learns to reconstruct its visual observations, and the reward model predicts the immediate reward. The entire world model J Model ψ is optimized by a VAE-like objective [55]:
J Model ψ (τ img , τ real ) = (at−1,ot,rt)∼τ real ln(p ψ (o t |s t )) + ln(p ψ (r t |s t )) − βD KL (p ψ (s t |s t−1 , a t−1 , o t )||p ψ (s t |s t−1 , a t−1 )) .(2)</p>
<p>Stochastic Value Gradients</p>
<p>Given a differentiable world model, stochastic value gradients (SVG) [22,23] can be applied to directly compute policy gradient on the whole imaginary trajectory, which is a recursive composition of policy, transition, reward, and value function. According to the stochastic Bellman Equation, we have:
v(s) = E ρ(η) r(s, π θ (s, η)) + γE ρ(ξ) (v(p(s, π θ (s, η), ξ))) ,(3)
where η ∼ ρ(η) and ξ ∼ ρ(ξ) are noises from a fixed noise distribution for re-parameterization. So the gradients through trajectories can be iteratively computed as:
∂v ∂s = E ρ(η) ∂r ∂s + ∂r ∂a ∂π ∂s + γE ρ(ξ) ∂v ∂s ∂p ∂s + ∂p ∂a ∂π ∂s ∂v ∂θ = E ρ(η) ∂r ∂a ∂π ∂θ + γE ρ(ξ) ∂v ∂s ∂p ∂a ∂π ∂θ + ∂v ∂θ ,(4)
where s denotes the next state given by the transition function. Intuitively, policy can be improved by propagating analytic gradients with respect to the policy network through the imaginary trajectories. </p>
<p>Reality-Aware Model-Based Policy Improvement</p>
<p>In this section, we present a novel model-based RL framework, called BrIdging Reality and Dream (BIRD), as shown in Figure 1. The agent represents its policy function with a policy network ( ). To estimate the future effects of its policy and enable potential policy improvement, it unrolls trajectories based on its world model ( ) using the current policy and optimizes the accumulative rewards on the imaginary trajectories. The policy network and differentiable world model connect to one another forming a larger trainable network, which supports differentiable planning and allows the analytic gradients of accumulative rewards with respect to the policy flow through the world model. In the meantime, the agent also interacts with the real world ( ) and generates real trajectories. BIRD maximizes the mutual information between real and imaginary trajectories to endow both the policy network and the world model with adaptive generalization to real-world interactions. In summary, BIRD maximizes the total objective function:
J BIRD = J SVG θ (τ img_roll ) − L TD φ (τ img_roll ) + wI θ,ψ (τ img , τ real ),(5)
where τ real and τ img indicate the real trajectories and the corresponding imaginary trajectories under the same policy, and τ img_roll indicate the rolled out imaginary trajectories during the optimization of policy improvement. θ, φ, ψ, are parameters of policy network π θ , value network p φ , and world model p ψ , respectively. The first two terms J SVG θ (τ img_roll ) − J TD φ (τ img_roll ) account for policy improvement on imaginations, the last term I θ,ψ (τ img , τ real ) optimizes the mutual information, and w is a weighting factor between them.</p>
<p>In conventional model-based RL approaches, real-world trajectories are normally used to optimize model prediction error, which is quite different from BIRD. In complex domains, optimizing model prediction error cannot guarantee a perfect predictive model. Unrolling with such an imperfect model for multiple steps will generate a large accumulative error, leaving a large gap between the generated trajectories and real ones. Thus, policy optimized by such a model may overfit undesirable imaginations and have a low generalization ability to reality, which is also shown in our experiments ( Figure 3). This problem is further exacerbated in analytic-gradient RL that performs differentiable planning by gradient-based local search. This is because even a small gradient step along the imperfect model can easily reach a non-generalizable neighbourhood and lead to a direction of incorrect policy improvement. To address this problem, our method optimizes mutual information with respect to both the model and the policy, which makes policy improvement aware of the discrepancy between real and imaginary trajectories. Intuitively, BIRD optimizes the world model to be more real and reinforces the actions whose resulting imaginations not only have large accumulative rewards, but also resemble real trajectories. As a result, BIRD learns a policy from imaginations with easier generalization to the real-world environment.</p>
<p>Policy Improvement on Imaginations</p>
<p>As a model-based RL algorithm, BIRD improves the policy by maximizing the accumulative rewards of the imaginary trajectories unrolled by the world model. Conventional model-based approaches [18,7,11] perform policy improvement by selecting the optimal action sequence that maximizes the expected planning reward, that is max a t:t+H E sx∼p ψ t+H x=t r(s x , a x ). If the world model is differentiable, we use stochastic value gradients (SVG) to directly leverage the gradients through the world model for policy improvement. Similar with Dreamer [13], our objective of maximizing the model-based value expansion within horizon H is given by:
J SVG θ (τ img ) = max θ t+H x=t V λ (s x ), V λ (s x ) = E ai∼π θ ,si∼p ψ (si|si−1,ai−1) H k=1 λ k h−1 i=t γ i−t r i + γ h−t v φ (s h ) ,(6)
where r i represents the immediate reward at timestep i predicted by the world model ψ. For each expand length k, we expand the expected value from current timestep x to timestep h − 1 (h = min(x + k, t + H)) and use learned value function v φ (s h ) to estimate returns beyond h steps,
i.e., v φ (s h ) = E H i=h γ i−h r i .
Here, we use the exponentially-weighted average of the estimates for different values of k to balance bias and variance, and the exponential weighting factor is indicated by λ k . As shown in the Equation 6, we alternate the policy network π θ and the differentiable world model p ψ , connect them to one another to form a large end-to-end trainable network, and then back-propagate the gradients of expected values with respect to policy parameters θ though this large network. Intuitively, a gradient step of the policy network encourages the world model to obtain a gradient step of new states, and in turn affect the future value. As a result, the states and policy will be optimized sequentially based on the feedback on future values. To optimize the value network, we use TD updates as actor-critic algorithms [54,56,21], instead of Monte Carlo estimation:
L TD φ (τ img ) = t+H x=t v φ (s x ) − V λ (s x ) 2 ,(7)</p>
<p>Bridge Imagination and Reality by Mutual Information Maximization</p>
<p>To ensure the policy improvement based on the learned world model is equally effective in the real world, we introduce an information-theoretic objective, that optimizes mutual information between real and imaginary trajectories with respect to the policy network and the world model:
I θ,ψ (τ img , τ real ) = H(τ real ) − H(τ real τ img ) = H(τ real ) − u P (u)H(τ real τ img = u) = H(τ real ) + u P (u) v P (v|u) log(P (τ real = v|u)) = H(τ real ) + u,v P (u, v) log(P (v|u)).(8)
To reduce computational complexity, we alternately optimize the total mutual information with respect to world model and policy network. First, we fix the policy parameters θ and only optimize the parameters of world model ψ to maximize the total mutual information I θ,ψ (τ img , τ real ). Since the first term H(τ real ) measures the entropy of real trajectories generated by policy π θ on real MDP, it is not related to parameters of the world models ψ and we can remove this term. As for the second term u,v P (u, v) log(P (v|u)), we consider the fact that our world model in conjunction with the policy network, can be regarded as a predictor for real trajectories and the second term serves as a log likelihood of a real trajectory of given imagined one. Thus, optimizing this term is equivalent to minimize the prediction error on training pairs of imagined and real trajectories (u, v). When the policy is fixed, P (u, v) is tractable and we can directly approximate it by sampling the data from replay buffer B (i.e., a collection of experienced trajectories). Thus, the second term becomes u,v∼B log(P (v|u; ψ)), which is equivalent to the conventional model prediction error −L Model ψ . In summary, we can get the gradient,
∇ ψ I θ,ψ (τ img , τ real ) = −∇ ψ L Model ψ (τ img , τ real ),(9)
Second, we fix the model parameters ψ and only optimize the parameters of policy network θ to maximize the total mutual information I θ,ψ (τ img , τ real ). The first term of mutual information becomes maximizing the entropy of the current policy. In some sense, this term encourages exploration and also learns a robust policy. We use a Gaussian distribution N (m θ (s t ), v θ (s t )) to model the stochastic policy π θ , and thus can analytically compute its entropy on real data as E st∼τ real 1 2 log 2πev 2 θ (s t ). Then we consider how to optimize the second term, u,v P (u, v) log(P (v|u)). The joint distribution of real and imagined trajectories P (u, v) is determined by the policy π θ . When the updates of the world model are stopped, the log likelihood of a real trajectory of given imagined one log(P (v|u)) is fixed and can be regarded as the weight for optimizing distribution P (u, v) by policy. Thus, the essential objective of maximizing u,v P (u, v) log(P (v|u)) with respect to policy parameters θ is to guide policy to the space with high confidence of model prediction (i.e., high log likelihood log(P (v|u))). Specifically, we implement it by a confidence-aware policy optimization, which reweights the degree of learning by prediction confidence log(P (τ img_roll |τ img )) during the policy improvement process. The new objective of reweighted policy improvement is written as log(P (τ img_roll |τ img ))J SVG θ (τ img_roll ). In addition, we normalize the confidence weight for each batch to make training stable. In summary, the gradient of policy optimization is rewritten as:
∇ θ I θ,ψ (τ img , τ real ) + J SVG θ (τ img )) =∇ θ E st∼τ real 1 2 log 2πev 2 θ (s t ) + log(P (τ img_roll |τ img ))J SVG θ (τ img_roll ) .(10)
From Equation 9 and 10, we can see there are three terms, model error minimization, policy entropy maximization, and confidence-aware policy optimization, derivated by our total objective of optimizing mutual information between real and imaginary trajectories. We have the same model error loss as Dreamer, and thus the main difference from Dreamer is the policy entropy maximization and confidence-aware policy optimization. Intuitively, entropy maximization term aims at increasing the search space of SVG-based policy search like Dreamer and thus can explore more possibilities. Then the confidence-aware optimization term reweighs the search results by confidence, which contributes to improve the search quality and make sure the additional search results from large entropy are reliable enough. This approach has strong connections to distributional shift refinement in offline RL setting and may be beneficial to the community of batch RL [57]. In addition, considering that τ real , τ img and τ img_roll are trajectories under current policy, we use a first-in-first-out replay buffer with limited capacity to mimic a approximately on-policy data stream.</p>
<p>Algorithm 1 summarizes our entire algorithm of optimizing mutual information and policy.</p>
<p>Algorithm 1 BIRD Algorithm</p>
<p>Initialize buffer B with random agent. Initialize parameters θ, φ, ψ randomly. Set hyper-parameters: imagination horizon H, learning step C, interacting step T , batch size B, batch length L. while not converged do
for i = 1 . . . C do Draw B data sequences {(o t , a t , r t )} t+L t from B. Compute latent states s t ∼ p ψ (s t |s t−1 , a t−1 , o t ) and imaginary trajectories {(s x , a x )} t+H x=t
For each s x , predict rewards p ψ (r x |s x ) and values v φ (s x ) Calculate imaginary returns Update θ, φ, ψ using Equation 5 Optimize policy and mutual information end for Reset o 1 in real world.
for t = 1 . . . T do Compute latent state s t ∼ p ψ (s t |s t−1 , a t−1 , o t ).
Compute a t ∼ π θ (a t |s t ) using policy network and add exploration noise. Take action a t and get r t , o t+1 from real world. Interact with real world end for Add experience {(o t , a t , r t ) T t=1 } to B. end while</p>
<p>Policy Optimization with Entropy Maximization</p>
<p>In the context of model-free RL, maximum entropy deep RL [49,58] contributes to learning robust policies with estimation errors, generating a question: if we simply add a maximization objective for policy entropy in the context of model-based RL with stochastic value gradients, can we also obtain policies from imaginations that generalize well to real environment? Thus, we design an ablation version of BIRD, Soft-BIRD, which just adds a entropy augmented objective to the return objective:
π * θ = arg max θ t E (r t + αH(π(·|s t ))) ,(11)
where α is a hyper-parameter. We use a soft Bellman Equation for value function v φ (s t ) like SAC [49] and rewrite the objective of policy improvement J SVG θ as:</p>
<p>v φ (s t ) = E r t + αH(π θ (·|s t )) + γv φ (s t+1 ) ,
J SVG θ (τ img ) = E ai∼π θ ,si∼p ψ (si|si−1,ai−1) H k=1 λ k h−1 i=t γ i−t (r i + αH(π θ (·|s i ))) + γ h−t v φ (s h ) .
(12) Compared to BIRD, soft-BIRD only maximizes the entropy of the policy instead of optimizing the mutual information between real and imaginary trajectories generated from the policy, which will provide further insights on the contribution of BIRD.</p>
<p>Experiments</p>
<p>We evaluate BIRD on DeepMind Control Suite (https://github.com/deepmind/dm_control) [30], a standard benchmark for continuous control. In Section 5.2, we compare BIRD with both model-free and model-based RL methods. For model-free baselines, we compare with D4PG [59], a distributed extension of DDPG [2], and A3C [56], the distributed actor-critic approach. We include the scores for D4PG with pixel inputs and A3C with state inputs, which are also used as baselines in Dreamer. For model-based baselines, we use PlaNet [12] and Dreamer [13], two state-of-the-art model-based RL. Some popular model-based RL papers [60,61,62,63] are not inlcuded in our experiments since they use MPC for sampling-based planning and do not show effectiveness on RL tasks with image inputs. Compared to the MPC-based approaches that generate many rollouts to select the highest performing action sequence, our paper builds upon analytic value gradients that can directly propagate gradients through a differentiable world model and is more computationally efficient on domains that require learning from pixels. Our paper focuses on visual control tasks, and thus we only compare with state-of-the-art algorithms of these tasks (i.e., PlaNet and Dreamer).</p>
<p>In addition, we conduct an ablation experiment in Section 5.3 to illustrate the contribution of mutual information maximization. In Section 5.4, we further study cases and visualize BIRD's generalization to real-world information.</p>
<p>Experiment Setting</p>
<p>We mainly follow the experiment settings of Dreamer. Among all environments, observations are 64 × 64 × 3 images, rewards are scaled to 0 to 1, and the dimensions of action space vary from 1 to 12 . Action repeat is fixed at 2 for all tasks. We implement Dreamer by its released codes (https://github.com/google-research/dreamer) and all hyper-parameters remain the same as reported. Since our model loss term in Equation 9 has the same form as Dreamer, we directly use the same model learning component as Dreamer that adopts multi-step prediction and removes latent overshooting used in PlaNet. We also use the same architecture for neural networks thus we have the same computational complexity as Dreamer. Specifically, CNN layers are employed to compress observations into latent state space and GRU [64] is used for learning latent dynamics. Policy network, reward network, and value network are all implemented with multi-layer perceptrons (MLP) and they respectively trained with Adam optimizer [65]. For all experiments, we select a discount factor of 0.99 and a mutual information coefficient of 1e-8. Buffersize is 100k. We train BIRD with a single Nvidia 2080Ti and a single CPU, and it takes 8 hours to run 1 million samples. </p>
<p>Results on DeepMind Control Suite</p>
<p>Learning policy from raw visual observation has always been a challenging problem for RL algorithms. We significantly improve the state-of-the-art visual control approach on the visual control tasks from DeepMind Control Suite, which provides a promising avenue for model-based policy learning from pixels. Figure 5 shows the training curves on 6 tasks and additional results are placed in supplementary materials. Comparison results demonstrate that BIRD significantly outperforms baselines in terms of sample efficiency. We observe that BIRD can use half training samples to obtain the same score with PlaNet and Dreamer in Hopper Stand and Hopper Hop. Among all tasks, BIRD achieves comparable performance to D4PG and A3C, which are trained with 1,000 times more samples. In addition, BIRD achieves higher or similar convergence scores in all tasks than baselines. Here, we provide insights into the superiority of BIRD. As the mutual information between real and imaginary trajectories increases, the behaviors that BIRD learns using the world model can be adapted to the real environment more appropriately and faster, while other model-based methods require a slower adaptation process. Besides, although world model usually tend to overfit poor policies in the early stage, BIRD will not be tempted by greedy policy optimization on the poor trajectories generated by such an imperfect model. Because the entropy maximization term in Equation 10 endows the agent a stronger exploration ability, and the confidence-aware policy optimization term encourages it re-estimate all the gathered trajectories and focus on optimizing high-confidence ones.</p>
<p>Ablation Study</p>
<p>In order to verify the outperformance of BIRD is not simply due to simply increasing the entropy of policy, we conduct an ablation study that compares BIRD with Soft-BIRD (4.3). Figure 5 shows the best performance of Soft-BIRD, but there is still a big gap from BIRD. As shown in Walker Run of Figure 5, we find that the score of Soft-BIRD first rises for a while, but eventually falls. The failure of Soft-BIRD suggests that policy improvement in model-based RL with analytic gradients is bottlenecked by the discrepancy of reality and imagination, thus only improving the entropy of policy will not help.  Figure 3: Prediction comparison between BIRD and Dreamer. BIRD has better predictions on key actions that will have long-term impacts, which implies that the policy generalizes to real environments well.</p>
<p>Case Study: Predictions on Key Actions</p>
<p>Our algorithm learns a world model with better generalization to real trajectories, especially on key actions which matters for long-horizon behavior learning. We visualize some predictions on key actions, such as the explosive force for standing up and jumping in Hopper Stand and Hopper Hop, stomping with front leg to prevent tumble in Walker Run, and throwing pole up to keep stable in Cartpole Swingup. As shown in Figure 3, BIRD makes more accurate predictions compared to Dreamer. For example, in Hopper Hop, Dreamer wrongly predicts the takeoff moment to fall down while BIRD has an accurate foresight that the agent will leap from the ground. Precise forecast of the key actions implicitly suggests that our imaginary trajectories generated by the learned policy indeed possess more real-world information.</p>
<p>Conclusion</p>
<p>Generalization from imagination to reality is a crucial yet challenging problem in the context of model-based RL. In this paper, we propose a novel model-based framework, called BrIdging Reality and Dream (BIRD), which not only performs differentiable planning on imaginary trajectories, but also encourages adaptive generalization to reality by optimizing mutual information between imaginary and real trajectories. Results on challenging visual control tasks demonstrate that our algorithm achieves state-of-the-art performance in terms of sample efficiency. Our ablation study further shows that the superiority is attributed to maximizing mutual information rather than simply increasing the entropy of the policy. In the future, we will explore directions to further improve the generalization of imaginations, such as generalizable representations and reusable skill discovery.</p>
<p>Broader Impact</p>
<p>Model-free RL requires a large amount of samples, thus limits its applications to real-world tasks. For example, the trial-and-error training process of a robot requires substantial manpower and financial resources, and certain harmful actions can greatly reduce the life of the robot. Building a world model and learning behaviors by imaginations provides a boarder prospect for real-world applications. This paper is situated in model-based RL and further improves sample efficiency over existing work, which will accelerate the development of real-world applications on automatic control, such as robotics and autonomous driving. In addition, this paper tackles a valuable problem about generalization, from imagination to reality, thus it is also of great interest to researchers in generalizable machine learning.</p>
<p>In the long run, this paper will improve the efficiency of factory operations, avoid artificial repetition of difficult or dangerous work, save costs, and reduce risks in the industrial and agricultural industry. For daily life, it will create a more intelligent lifestyle and improve the quality of life.</p>
<p>Our algorithm is a generic framework that does not leverages biases in data. We evaluated our model in a popular benchmark of visual control tasks. However, similar to a majority of deep learning approaches, our algorithm has a common disadvantage. The learned knowledge and policy is not friendly to humans and it is hard for us to know why the agent learns to act so well. Interpretability has always been a challenging open question and in the future we are interested in incorporating recent deep learning progresses on causal inference into RL.</p>
<p>Experiment Details</p>
<p>We use the convolutional neural network and GRU [64] architecture for the dynamics model, with the deterministic unit size of 300 and stochastic unit size of 40. All other neural networks consist of three linear layers with ELU activations. Action network separately outputs a scaled tanh mean and a softplus standard deviation for the Normal distribution. The distribution will be transformed with a tanh function before sampling the action vector. The learning rates for dynamics model, action network and value estimation are 6 × 10 −4 , 8 × 10 −5 , 8 × 10 −5 . We use Adam [65] to optimize the networks and clip the gradients not to exceed 100. The mutual information regularizer α and the KL regularizer β are 1 × 10 −8 and 1. γ and λ in value target are set as 0.99 and 0.95. The training data for dynamics model are in batch size of 50 and each of them is in length 50. The horizon steps H for imagination is 15. When interacting with the environment, we sample a random variable from a normal distribution N (0, 0.3) as exploration noise. The maximum of episode length is 1000. The networks are updated for 100 steps once an episode is done. The buffer is prefilled with 5 episodes using random agent and the total buffersize is 100k. The action repeat is fixed to 2 for all tasks. Figure 5: Comparison of model error. Since image reconstruction error will be dominated by image background and cannot reflect the prediction error on latent state, we calculate the model error as the discrepancy between latent states that predicted by model and encoded from posterior image observations. BIRD that significantly outperforms Dreamer in terms of returns has a much lower model error.</p>
<p>Figure 1 :
1Overall framework of BIRD.</p>
<p>Figure 2 :
2Results on DeepMind Control Suite. The shaded areas show the standard deviation across 10 seeds. BIRD achieves considerable performance in several challenging tasks and requires less samples than baselines.
Additional ResultsFigure 4: Results on DeepMind Control Suite. The shaded areas show the standard deviation across 3 seeds.
Acknowledgments and Disclosure of FundingThe authors received no specific funding for this work.
Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540529Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</p>
<p>P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, Continuous control with deep reinforcement learning. International Conference on Learning Representations. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. International Conference on Learning Representations, 2016.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Human learning in atari. Pedro Tsividis, Thomas Pouncy, Jaqueline L Xu, Joshua B Tenenbaum, Samuel J Gershman, 2017 AAAI Spring Symposia. Palo Alto, California, USAAAAI PressStanford UniversityPedro Tsividis, Thomas Pouncy, Jaqueline L. Xu, Joshua B. Tenenbaum, and Samuel J. Gershman. Human learning in atari. In 2017 AAAI Spring Symposia, Stanford University, Palo Alto, California, USA, March 27-29, 2017. AAAI Press, 2017.</p>
<p>Value prediction network. Junhyuk Oh, Satinder Singh, Honglak Lee, Advances in Neural Information Processing Systems. Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118-6128, 2017.</p>
<p>Deep visual foresight for planning robot motion. Chelsea Finn, Sergey Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEEChelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.</p>
<p>. David Ha, Jurgen Schmidhuber, abs/1803.10122David Ha and Jurgen Schmidhuber. World models. CoRR, abs/1803.10122, 2018.</p>
<p>Sample-efficient reinforcement learning with stochastic ensemble value expansion. Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, Honglak Lee, ; Hanna, M Wallach, Hugo Larochelle, Kristen Grauman, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Nicolò Cesa-Bianchi, and Roman GarnettNeurIPS; Montréal, CanadaSamy Bengio,Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efficient reinforcement learning with stochastic ensemble value expansion. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 8234-8244, 2018.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, arXiv:1911.08265Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. arXiv e-prints. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Sil- ver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. arXiv e-prints, page arXiv:1911.08265, November 2019.</p>
<p>Model-based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, H Roy, Konrad Campbell, Dumitru Czechowski, Chelsea Erhan, Piotr Finn, Sergey Kozakowski, Levine, arXiv:1903.00374arXiv preprintLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based rein- forcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy P Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USAPMLR97Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2555-2565. PMLR, 2019.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy P Lillicrap, Jimmy Ba, Mohammad Norouzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.</p>
<p>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. S Richard, Sutton, Machine learning proceedings. ElsevierRichard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine learning proceedings 1990, pages 216-224. Elsevier, 1990.</p>
<p>Model-ensemble trust-region policy optimization. Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, Pieter Abbeel, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netThanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.</p>
<p>Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, Tengyu Ma, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAOpenReview.netYuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. In 7th Interna- tional Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.</p>
<p>Modelbased reinforcement learning via meta-policy optimization. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel, 2nd Annual Conference on Robot Learning. Zürich, SwitzerlandPMLR87ProceedingsIgnasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model- based reinforcement learning via meta-policy optimization. In 2nd Annual Conference on Robot Learning, CoRL 2018, Zürich, Switzerland, 29-31 October 2018, Proceedings, volume 87 of Proceedings of Machine Learning Research, pages 617-629. PMLR, 2018.</p>
<p>Model predictive control. F Eduardo, Carlos Camacho, Alba Bordons, Springer Science &amp; Business MediaEduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer Science &amp; Business Media, 2013.</p>
<p>A survey of numerical methods for optimal control. V Anil, Rao, Advances in the Astronautical Sciences. 1351Anil V Rao. A survey of numerical methods for optimal control. Advances in the Astronautical Sciences, 135(1):497-528, 2009.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in Neural Information Processing Systems. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754-4765, 2018.</p>
<p>Modelbased value estimation for efficient model-free reinforcement learning. Vladimir Feinberg, Alvin Wan, Ion Stoica, I Michael, Joseph E Jordan, Sergey Gonzalez, Levine, arXiv:1803.00101arXiv preprintVladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model- based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.</p>
<p>Reinforcement learning by value gradients. CoRR, abs/0803. Michael Fairbank, 3539Michael Fairbank. Reinforcement learning by value gradients. CoRR, abs/0803.3539, 2008.</p>
<p>Learning continuous control policies by stochastic value gradients. Nicolas Heess, Gregory Wayne, David Silver, Timothy P Lillicrap, Tom Erez, Yuval Tassa, Corinna Cortes, Neil D. Lawrence, Daniel DNicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Corinna Cortes, Neil D. Lawrence, Daniel D.</p>
<p>Masashi Lee, Roman Sugiyama, Garnett, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaLee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2944-2952, 2015.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472, 2011.</p>
<p>Synthesis and stabilization of complex behaviors through online trajectory optimization. Yuval Tassa, Tom Erez, Emanuel Todorov, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEYuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4906-4913. IEEE, 2012.</p>
<p>Guided policy search. Sergey Levine, Vladlen Koltun, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningAtlanta, GA, USA28Workshop and Conference ProceedingsSergey Levine and Vladlen Koltun. Guided policy search. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 1-9. JMLR.org, 2013.</p>
<p>Imagined value gradients: Modelbased policy optimization with transferable latent dynamics models. Arunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland Hafner, Michael Neunert, Thomas Lampe, Noah Siegel, Nicolas Heess, Martin Riedmiller, arXiv:1910.04142arXiv preprintArunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland Hafner, Michael Neunert, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Imagined value gradients: Model- based policy optimization with transferable latent dynamics models. arXiv preprint arXiv:1910.04142, 2019.</p>
<p>Neural correlates of self-reflection. C Sterling, Leslie C Johnson, Lana S Baxter, James G Wilder, Joseph E Pipe, George P Heiserman, Prigatano, Brain. 8Sterling C. Johnson, Leslie C. Baxter, Lana S. Wilder, James G. Pipe, Joseph E. Heiserman, and George P. Prigatano. Neural correlates of self-reflection. Brain, (8):8, 2002.</p>
<p>A history of introspection. E G Boring, Psychological bulletin. 503EG BORING. A history of introspection. Psychological bulletin, 50(3):169-189, May 1953.</p>
<p>. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Abbas Budden, Josh Abdolmaleki, Andrew Merel, Timothy P Lefrancq, Martin A Lillicrap, Riedmiller, Deepmind control suite. CoRR, abs/1801.00690Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018.</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, Sergey Levine, 2018 IEEE International Conference on Robotics and Automation. Brisbane, AustraliaIEEEAnusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pages 7559-7566. IEEE, 2018.</p>
<p>Deep dynamics models for learning dexterous manipulation. Anusha Nagabandi, Kurt Konolige, Sergey Levine, Vikash Kumar, 3rd Annual Conference on Robot Learning. Leslie Pack Kaelbling, Danica Kragic, and Komei SugiuraOsaka, JapanPMLR100Anusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics models for learning dexterous manipulation. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 -November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 1101-1112. PMLR, 2019.</p>
<p>Self-supervised visual planning with temporal skip connections. Frederik Ebert, Chelsea Finn, Alex X Lee, Sergey Levine, 1st Annual Conference on Robot Learning. Mountain View, California, USAPMLR78ProceedingsFrederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In 1st Annual Conference on Robot Learning, CoRL 2017, Mountain View, California, USA, November 13-15, 2017, Proceedings, volume 78 of Proceedings of Machine Learning Research, pages 344-356. PMLR, 2017.</p>
<p>Benchmarking model-based reinforcement learning. Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba, abs/1907.02057CoRRTingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. CoRR, abs/1907.02057, 2019.</p>
<p>Learning neural network policies with guided policy search under unknown dynamics. Sergey Levine, Pieter Abbeel, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. WeinbergerMontreal, Quebec, CanadaSergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 1071-1079, 2014.</p>
<p>Learning contact-rich manipulation skills with guided policy search. Sergey Levine, Nolan Wagener, Pieter Abbeel, IEEE International Conference on Robotics and Automation, ICRA 2015. Seattle, WA, USAIEEESergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with guided policy search. In IEEE International Conference on Robotics and Automation, ICRA 2015, Seattle, WA, USA, 26-30 May, 2015, pages 156-163. IEEE, 2015.</p>
<p>Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search. Tianhao Zhang, Gregory Kahn, Sergey Levine, Pieter Abbeel, 2016 IEEE International Conference on Robotics and Automation, ICRA 2016. Danica Kragic, Antonio Bicchi, and Alessandro De LucaStockholm, SwedenIEEETianhao Zhang, Gregory Kahn, Sergey Levine, and Pieter Abbeel. Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search. In Danica Kragic, Antonio Bicchi, and Alessandro De Luca, editors, 2016 IEEE International Conference on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages 528-535. IEEE, 2016.</p>
<p>Path integral guided policy search. Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, Sergey Levine, 2017 IEEE International Conference on Robotics and Automation. Singapore, SingaporeIEEEYevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and Sergey Levine. Path integral guided policy search. In 2017 IEEE International Conference on Robotics and Automation, ICRA 2017, Singapore, Singapore, May 29 -June 3, 2017, pages 3381-3388. IEEE, 2017.</p>
<p>Model-based planning with discrete and continuous actions. Mikael Henaff, F William, Yann Whitney, Lecun, arXiv:1705.07177arXiv preprintMikael Henaff, William F Whitney, and Yann LeCun. Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177, 2017.</p>
<p>Universal planning networks. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn, arXiv:1804.00645arXiv preprintAravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.</p>
<p>Deep variational information bottleneck. Alexander A Alemi, Ian Fischer, Joshua V Dillon, Kevin Murphy, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netAlexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.</p>
<p>On mutual information maximization for representation learning. Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, Mario Lucic, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.</p>
<p>Learning deep representations by mutual information estimation and maximization. R , Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, Yoshua Bengio, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAOpenReview.netR. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.</p>
<p>Entropy and mutual information in models of deep neural networks. Marylou Gabrié, Andre Manoel, Clément Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, Lenka Zdeborová, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman GarnettNeurIPS; Montréal, CanadaMarylou Gabrié, Andre Manoel, Clément Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, and Lenka Zdeborová. Entropy and mutual information in models of deep neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 1826-1836, 2018.</p>
<p>Learning representations in reinforcement learning: An information bottleneck approach. Yingjun Pei, Xinwen Hou, abs/1911.05695CoRRYingjun Pei and Xinwen Hou. Learning representations in reinforcement learning: An information bottleneck approach. CoRR, abs/1911.05695, 2019.</p>
<p>Representation learning with contrastive predictive coding. Aäron Van Den Oord, Yazhe Li, Oriol Vinyals, abs/1807.03748CoRRAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.</p>
<p>Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A Pires, abs/1811.06407Toby Pohlen, and Rémi Munos. Neural predictive belief representations. CoRR. Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A. Pires, Toby Pohlen, and Rémi Munos. Neural predictive belief representations. CoRR, abs/1811.06407, 2018.</p>
<p>Reinforcement learning with deep energy-based policies. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. Doina Precup and Yee Whye Tehthe 34th International Conference on Machine LearningSydney, NSW, AustraliaPMLR70Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1352-1361. PMLR, 2017.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, Proceedings of the 35th International Conference on Machine Learning. Jennifer G. Dy and Andreas Krausethe 35th International Conference on Machine LearningStockholmsmässan, Stockholm, SwedenPMLR80Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 1856-1865. PMLR, 2018.</p>
<p>Combining policy gradient and q-learning. Rémi Brendan O&apos;donoghue, Koray Munos, Volodymyr Kavukcuoglu, Mnih, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netBrendan O'Donoghue, Rémi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient and q-learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.</p>
<p>Bridging the gap between value and policy based reinforcement learning. Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USAOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 2775-2785, 2017.</p>
<p>Infobot: Transfer and exploration via the information bottleneck. Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick, Yoshua Bengio, Sergey Levine, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAOpenReview.netAnirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick, Yoshua Bengio, and Sergey Levine. Infobot: Transfer and exploration via the information bottleneck. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.</p>
<p>Information-directed exploration for deep reinforcement learning. Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, Andreas Krause, 7th International Conference on Learning Representations. New Orleans, LA, USAOpenReview.netNikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-directed exploration for deep reinforcement learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT press Cambridge1Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of the 33nd International Conference on Machine Learning. Maria-Florina Balcan and Kilian Q. Weinbergerthe 33nd International Conference on Machine LearningNew York City, NY, USA48JMLR Workshop and Conference Proceedings. JMLR.orgVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1928-1937. JMLR.org, 2016.</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. 2020.</p>
<p>S2{vg}: Soft stochastic value gradient method. Xiaoyu Tan, Chao Qu, Junwu Xiong, James Zhang, Xiaoyu Tan, Chao Qu, Junwu Xiong, and James Zhang. S2{vg}: Soft stochastic value gradient method, 2020.</p>
<p>Distributed distributional deterministic policy gradients. Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, T B Dhruva, Alistair Muldal, Nicolas Heess, Timothy P Lillicrap, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netGabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy P. Lillicrap. Distributed distributional deterministic policy gradients. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in Neural Information Processing Systems. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754-4765, 2018.</p>
<p>Regularizing trajectory optimization with denoising autoencoders. Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola, Advances in Neural Information Processing Systems. Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, and Harri Valpola. Regularizing trajectory optimization with denoising autoencoders. In Advances in Neural Information Processing Systems, pages 2859-2869, 2019.</p>
<p>Regularizing model-based planning with energy-based models. Rinu Boney, Juho Kannala, Alexander Ilin, Conference on Robot Learning. PMLRRinu Boney, Juho Kannala, and Alexander Ilin. Regularizing model-based planning with energy-based models. In Conference on Robot Learning, pages 182-191. PMLR, 2020.</p>
<p>Model-based reinforcement learning: A survey. Joost Thomas M Moerland, Catholijn M Jonker Broekens, arXiv:2006.16712arXiv preprintThomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learning: A survey. arXiv preprint arXiv:2006.16712, 2020.</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, Yoshua Bengio, abs/1412.3555CoRRJunyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8291 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8291</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8291</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-278165383</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.18839v2.pdf" target="_blank">Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) are transforming numerous applications, their susceptibility to conversational breakdowns remains a critical challenge undermining user trust. This paper introduces a"Detect, Explain, Escalate"framework to manage dialogue breakdowns in LLM-powered agents, emphasizing low-carbon operation. Our approach integrates two key strategies: (1) We fine-tune a compact 8B-parameter model, augmented with teacher-generated reasoning traces, which serves as an efficient real-time breakdown 'detector' and 'explainer'. This model demonstrates robust classification and calibration on English and Japanese dialogues, and generalizes well to the BETOLD dataset, improving accuracy by 7% over its baseline. (2) We systematically evaluate frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity breakdown assessment. These are integrated into an 'escalation' architecture where our efficient detector defers to larger models only when necessary, substantially reducing operational costs and energy consumption. Our fine-tuned model and prompting strategies establish new state-of-the-art results on dialogue breakdown detection benchmarks, outperforming specialized classifiers and significantly narrowing the performance gap to larger proprietary models. The proposed monitor-escalate pipeline reduces inference costs by 54%, offering a scalable, efficient, and more interpretable solution for robust conversational AI in high-impact domains. Code and models will be publicly released.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8291.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8291.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DisruptionMonitor-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dialogue Disruption Monitor (fine-tuned Llama-3.1 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact Llama-3.1 8B model fine-tuned with teacher-generated reasoning traces to detect, justify, and calibrate per-utterance dialogue breakdown labels in real time; intended as a low-cost monitor that escalates to larger LLMs when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1 8B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3.1 8B instruct model fine-tuned with LoRA (rank=16) on DBDC5 English/Japanese and teacher-generated chain-of-thought traces from Llama-3.3 70B; optimized for low-latency inference on a single A100 40GB.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['teacher-guided chain-of-thought distillation', 'explicit justification generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>During supervised fine-tuning the student (8B) was supplied with teacher-generated synthetic reasoning traces (chains-of-thought) produced by a larger Llama-3.3 70B; training objective combined binary breakdown classification (cross-entropy) with generation of textual justifications matching the teacher traces.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single distilled method with explanation generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>The student model was trained with augmented dataset D' containing teacher chains; its performance was compared to the un-finetuned base 8B and to various prompted large LLMs (ZS/FS/CoT/AR/CL+AR) in downstream evaluation (DBDC5, BETOLD); no multi-method fusion within the student was reported beyond the single distilled CoT traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese (utterance-level breakdown detection), BETOLD (conversation-level task-oriented dialogues)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: accuracy 81.5% (fine-tuned monitor); DBDC5 Japanese: accuracy 67.9%; BETOLD (zero-shot baseline 8B = 60.2%) fine-tuned monitor = 67.2% (≈ +7.0 pp over baseline). Calibration (MSE) on DBDC5 English: 4.9.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Distilling teacher reasoning into a smaller model improved accuracy and produced interpretable justifications; the monitor generalized to BETOLD despite training on DBDC5. It yields stable calibration though slightly worse than best large models; efficient for real-time per-turn monitoring.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>A compact student LLM fine-tuned with teacher-generated reasoning traces can approach larger-model performance for breakdown detection, produce useful justifications, and enable cost-effective escalation architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8291.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CL+AR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curricular Learning with Analogical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks the model to generate a sequence of analogical examples of increasing difficulty (an easy-to-hard curriculum) before deciding on the label for the target dialogue, intended to scaffold multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompting technique applied to multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategy rather than a model; implemented by instructing an LLM to produce A^(1)_i (easy analogies) ... A^(m)_i (progressively harder analogies) and finally classify the target (H_i, s_i).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['analogical reasoning (self-generated exemplars)', 'curriculum learning (easy→hard sequencing)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The prompt first instructs the model to self-generate an easy analogous conversation, then harder analogies, forming a curriculum of examples; the model then makes a decision based on both the analogies and the original input. This increases prompt-output length and token use.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (combines analogical reasoning with curriculum ordering and multi-step inference)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared CL+AR against ZS, FS (2S-Easy, 2S-Hard, 4S), CoT, and AR across models; run on DBDC5 English/Japanese and a subset (10%) of datasets for AR/CL+AR due to token costs. Results reported per model and prompt variant; ablation noted token/truncation effects and a two-pass vs single-pass AR comparison (two-pass comparable to single-pass).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese (utterance-level breakdown detection), BETOLD (subset experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: CL+AR achieved up to 85.5% accuracy (tied for best, e.g., Llama-3.3 70B with CL+AR = 85.5%); DBDC5 Japanese: Claude-3.5 Sonnet v2 with CL+AR = 89.0% (best). On BETOLD, CL+AR performance degraded and was consistently worse than on natural-dialogue datasets (no numeric best given; results highlighted as lower in Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CL+AR improved classification on natural dialogue datasets (DBDC5), but effectiveness declines on structured or abstract representations (BETOLD) due to mismatch between generated natural-language analogies and abstract intent/entity dialogue format; increases token usage and sometimes hits context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CL+AR can materially improve classification accuracy on natural dialogue benchmarks by scaffolding reasoning, but it is sensitive to dataset format and token-budget constraints; benefits are inconsistent on structured task-oriented data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8291.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5 Sonnet v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-3.5 Sonnet v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing closed-source LLM from Anthropic used for breakdown detection, showing strong gains with analogical and curriculum-style prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5 Sonnet v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's 3.5-series (Sonnet v2) closed-source model (large parameter family, cited as strong classification consistency and high cost); used via API with temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['analogical reasoning (AR)', 'curricular learning + analogical reasoning (CL+AR)', 'few-shot prompting (2S-Easy, 2S-Hard, 4S)', 'chain-of-thought (CoT)', 'zero-shot (ZS)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated under ZS, FS (easy/hard/4S), CoT, AR, and CL+AR prompts. AR prompts instruct the model to self-generate analogous conversations and use them for classification; CL+AR sequences analogies from easy to hard. CoT uses 'Let's think step by step.' Few-shot uses labeled exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons across prompting families were performed: AR/CL+AR vs. ZS/FS/CoT; reported accuracy and F1 per prompt type on DBDC5 and BETOLD. Observed sensitivity to exemplar selection (2S-Hard vs 2S-Easy) and prompt ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese; BETOLD</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: up to 85.5% accuracy using AR prompt; DBDC5 Japanese: up to 89.0% accuracy with CL+AR; on BETOLD reported high accuracies in some FS settings (e.g., 2S-Hard/4S) though exact BETOLD top figure for Sonnet v2 not isolated. Calibration: Sonnet v2 combinations yield strong classification consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sonnet v2 benefits substantially from AR and CL+AR prompts on natural dialogue tasks and retains strong calibration; however, prompt length / token costs are high and effectiveness decreases on structured BETOLD inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Large closed-source models like Claude-3.5 Sonnet v2 achieve state-of-the-art accuracy on DBDC5 when provided with analogical or curriculum-style prompts; prompting strategy choice materially impacts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8291.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier OpenAI model evaluated under multiple prompting strategies; shows robust performance and sensitivity to prompt style, with modest benefits from few-shot and CoT in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 family variant (frontier closed-source model) evaluated at temperature 0 across prompting strategies (ZS, FS, CoT, AR, CL+AR).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot prompting (2S-Hard, 4S)', 'chain-of-thought (CoT)', 'analogical reasoning (AR)', 'zero-shot (ZS)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompts included few-shot exemplars (including 'hard' ambiguous examples), CoT ('think step-by-step'), AR/CL+AR self-generated analogies, and ZS instructions. For CoT, asking for justification before decision improved deliberation and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared across ZS, FS (2S-Easy/Hard, 4S), CoT, and AR/CL+AR; reported per-prompt performance. Noted that 2S-Hard improved DBDC5 English to 83.5% for GPT-4o and CoT slightly improved F1(B) from 85.9%→86.5% in one case.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese; BETOLD</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: up to 83.5% accuracy (2S-Hard); BETOLD: up to 77.7% (4S prompting); CoT produced marginal F1(B) gains (example: +0.6 pp on DBDC5 English).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4o is robust but sensitive to prompt design; CoT yields small improvements on nuanced cases while AR/CL+AR can improve natural-dialogue benchmarks but are token-expensive; ordering of justification before decision improves calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Providing challenging few-shot exemplars (2S-Hard) and requesting justifications before decisions refines accuracy and calibration; CoT helps borderline case identification but gains vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8291.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source Llama model used both as a teacher for reasoning traces and as an evaluated classifier; shows top-tier performance when combined with CL+AR prompting and serves as a source for distilled chains-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 70B-parameter Llama model; used to (1) generate synthetic reasoning traces to fine-tune the 8B student and (2) evaluated with multiple prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['teacher-generated chain-of-thought (for distillation)', 'few-shot prompting (2S-Hard)', 'CL+AR and AR', 'CoT']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>As teacher, it produced step-by-step reasoning traces r_i for training examples; as an evaluated model, it was prompted with FS/CoT/AR/CL+AR similarly to other LLMs. Best calibration achieved with 2S-Hard few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (used single-method distillation for student and diverse prompting for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used both as backbone for distillation experiments (teacher→student) and was itself evaluated across prompting families. Compared CL+AR/AR/CoT/FS/ZS effects on DBDC5/ BETOLD and measured calibration (MSE).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese; used to generate teacher traces for DBDC5 training; evaluated on BETOLD subset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: CL+AR 85.5% accuracy (tied top). Best calibration MSE on DBDC5 English: 4.0 using 2S-Hard. General strong performance on DBDC5; variable on BETOLD.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Large open-source Llama-3.3 70B is effective both as a teacher for reasoning distillation and as a high-performing classifier when paired with CL+AR; it achieves excellent calibration with hard few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Distilling reasoning from a 70B teacher into a smaller student yields an efficient monitor; Llama-3.3 70B with CL+AR matches top closed-source accuracy on DBDC5 while providing good calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8291.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source model trained with reinforcement techniques to incentivize reasoning; evaluated here across prompting strategies and showing competitive few-shot performance on DBDC5 and BETOLD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek's R1 model (reported large parameter family) that emphasizes reasoning ability via RL-based training; evaluated via ZS, FS, AR, CL+AR and CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot prompting (4S improved performance)', 'analogical reasoning (AR)', 'chain-of-thought (CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated with ZS/FS/CoT/AR/CL+AR. Few-shot 4S improved DBDC5 English performance; AR/CL+AR also provided gains on natural dialogues. On BETOLD, few-shot 2S-Easy yielded strong transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared ZS vs 2S-Easy/Hard vs 4S vs CoT vs AR vs CL+AR on DBDC5 and BETOLD; AR/CL+AR applied to 10% subsets due to token cost. Reported both accuracy and F1 per prompt type.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese; BETOLD</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DBDC5 English: ZS 81.1% → 4S 83.0% (example improvement); BETOLD: up to 75.8% accuracy with 2S-Easy. CL+AR/AR further improved DBDC5 accuracy in some runs (e.g., DeepSeek-R1 4S = 83.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DeepSeek-R1 benefits from few-shot exemplars and analogical prompting on natural dialogue tasks; however, gains vary by dataset and token budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Models trained to encourage internal reasoning (e.g., DeepSeek-R1) respond well to few-shot and analogical prompts, achieving competitive accuracy with proper prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8291.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI model evaluated across many prompting strategies; exhibited variable and generally weaker performance on BETOLD and sensitivity to prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5 Turbo model; lower-parameter family relative to GPT-4 and other frontier models, evaluated with ZS/FS/CoT/AR/CL+AR.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot prompting (2S-Easy/Hard/4S)', 'chain-of-thought (CoT)', 'analogical reasoning (AR)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompted similarly to other LLMs; evaluated under few-shot settings and CoT. Observed sensitivity to prompt presentation and skewed confidence in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple prompting strategies evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons across ZS, FS, CoT, AR, CL+AR. Performance reported as ranges across datasets; noted substantial underperformance on BETOLD relative to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BETOLD; DBDC5 English & Japanese</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BETOLD accuracy ranged from 41.2% to 64.5% depending on prompt; DBDC5 English accuracy ranged ~66.5%–76.6% depending on prompt style. Notably poor on BETOLD compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-3.5 Turbo misclassifies borderline near-breakdown utterances and displays prompt-format sensitivity leading to skewed confidences; few-shot/CoT do not reliably rescue performance on structured BETOLD inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Smaller or older frontier models can struggle with structured task-oriented benchmarks and are highly sensitive to prompt construction; prompting alone may not bridge capability gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8291.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8291.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paper-wide Prompting Suite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies evaluated (ZS, FS, CoT, AR, CL+AR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The collection of prompting/elicitation methods tested in the paper to induce different reasoning behaviours in LLMs: zero-shot, few-shot (easy/hard/4-shot), chain-of-thought, analogical reasoning, and curricular analogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to multiple LLMs (OpenAI, Anthropic, Meta, Mistral, DeepSeek, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt families used consistently across models: ZS (task description only), FS (2S-Easy, 2S-Hard, 4S), CoT ('Let's think step-by-step'), AR (self-generate analogous examples), CL+AR (generate analogies from easy→hard).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot reasoning', 'few-shot exemplar reasoning (including hard/easy exemplar selection)', 'chain-of-thought (CoT)', 'analogical reasoning (AR)', 'curriculum learning combined with analogical reasoning (CL+AR)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ZS provides only instructions; FS inserts 2–4 labeled exemplars (easy vs hard chosen by annotator agreement); CoT elicits explicit intermediate steps; AR asks model to self-generate analogies then classify; CL+AR sequences analogies easy→hard to scaffold reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (the study systematically applied multiple, diverse prompting methods and contrasted them individually)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Systematic evaluation and ablations comparing ZS vs FS (2S-Easy/Hard/4S) vs CoT vs AR vs CL+AR across many LLMs and datasets (DBDC5 English/Japanese, BETOLD). Additional ablations: ordering justification before decision (improves calibration), two-pass AR vs single-pass (two-pass comparable), and token-budgeted subset experiments for AR/CL+AR (10% subset).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>DBDC5 English & Japanese (utterance-level), BETOLD (task-oriented conversation-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Few-shot prompting (FS) consistently outperformed ZS and often CoT. Example cross-model highlights: DeepSeek-R1: ZS 81.1% → 4S 83.0% (DBDC5 EN); GPT-4o: ZS→2S-Hard increased to 83.5% (DBDC5 EN); Claude-3.5 Sonnet v2: AR/CL+AR yields up to 85.5% (DBDC5 EN) and 89.0% (DBDC5 JP). CoT produced mixed results: slight F1(B) gains in some cases (e.g., GPT-4o +0.6 pp) but often degraded performance on structured BETOLD.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Few-shot exemplars, especially 'hard' ambiguous ones (2S-Hard), improve calibration and accuracy; CoT helps borderline inferences but can worsen performance on highly structured/abstract datasets; AR and CL+AR produce the biggest gains on natural dialogues but are token-expensive and fragile on abstract representations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Prompting diversity matters: few-shot (particularly hard exemplars) and analogical/curricular prompting can improve accuracy and calibration on natural-language dialogue breakdown tasks, but benefits are dataset-dependent and traded off against token costs and instruction-following reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models as analogical reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Curriculum learning for natural language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8291",
    "paper_id": "paper-278165383",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "DisruptionMonitor-8B",
            "name_full": "Dialogue Disruption Monitor (fine-tuned Llama-3.1 8B)",
            "brief_description": "A compact Llama-3.1 8B model fine-tuned with teacher-generated reasoning traces to detect, justify, and calibrate per-utterance dialogue breakdown labels in real time; intended as a low-cost monitor that escalates to larger LLMs when needed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1 8B (fine-tuned)",
            "model_description": "Llama-3.1 8B instruct model fine-tuned with LoRA (rank=16) on DBDC5 English/Japanese and teacher-generated chain-of-thought traces from Llama-3.3 70B; optimized for low-latency inference on a single A100 40GB.",
            "reasoning_methods": [
                "teacher-guided chain-of-thought distillation",
                "explicit justification generation"
            ],
            "reasoning_methods_description": "During supervised fine-tuning the student (8B) was supplied with teacher-generated synthetic reasoning traces (chains-of-thought) produced by a larger Llama-3.3 70B; training objective combined binary breakdown classification (cross-entropy) with generation of textual justifications matching the teacher traces.",
            "reasoning_diversity": "similar (single distilled method with explanation generation)",
            "reasoning_diversity_experimental_setup": "The student model was trained with augmented dataset D' containing teacher chains; its performance was compared to the un-finetuned base 8B and to various prompted large LLMs (ZS/FS/CoT/AR/CL+AR) in downstream evaluation (DBDC5, BETOLD); no multi-method fusion within the student was reported beyond the single distilled CoT traces.",
            "task_or_benchmark": "DBDC5 English & Japanese (utterance-level breakdown detection), BETOLD (conversation-level task-oriented dialogues)",
            "performance_results": "DBDC5 English: accuracy 81.5% (fine-tuned monitor); DBDC5 Japanese: accuracy 67.9%; BETOLD (zero-shot baseline 8B = 60.2%) fine-tuned monitor = 67.2% (≈ +7.0 pp over baseline). Calibration (MSE) on DBDC5 English: 4.9.",
            "qualitative_findings": "Distilling teacher reasoning into a smaller model improved accuracy and produced interpretable justifications; the monitor generalized to BETOLD despite training on DBDC5. It yields stable calibration though slightly worse than best large models; efficient for real-time per-turn monitoring.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "A compact student LLM fine-tuned with teacher-generated reasoning traces can approach larger-model performance for breakdown detection, produce useful justifications, and enable cost-effective escalation architectures.",
            "uuid": "e8291.0",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CL+AR",
            "name_full": "Curricular Learning with Analogical Reasoning",
            "brief_description": "A prompting technique that asks the model to generate a sequence of analogical examples of increasing difficulty (an easy-to-hard curriculum) before deciding on the label for the target dialogue, intended to scaffold multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prompting technique applied to multiple LLMs",
            "model_description": "Prompting strategy rather than a model; implemented by instructing an LLM to produce A^(1)_i (easy analogies) ... A^(m)_i (progressively harder analogies) and finally classify the target (H_i, s_i).",
            "reasoning_methods": [
                "analogical reasoning (self-generated exemplars)",
                "curriculum learning (easy→hard sequencing)"
            ],
            "reasoning_methods_description": "The prompt first instructs the model to self-generate an easy analogous conversation, then harder analogies, forming a curriculum of examples; the model then makes a decision based on both the analogies and the original input. This increases prompt-output length and token use.",
            "reasoning_diversity": "diverse (combines analogical reasoning with curriculum ordering and multi-step inference)",
            "reasoning_diversity_experimental_setup": "Compared CL+AR against ZS, FS (2S-Easy, 2S-Hard, 4S), CoT, and AR across models; run on DBDC5 English/Japanese and a subset (10%) of datasets for AR/CL+AR due to token costs. Results reported per model and prompt variant; ablation noted token/truncation effects and a two-pass vs single-pass AR comparison (two-pass comparable to single-pass).",
            "task_or_benchmark": "DBDC5 English & Japanese (utterance-level breakdown detection), BETOLD (subset experiments)",
            "performance_results": "DBDC5 English: CL+AR achieved up to 85.5% accuracy (tied for best, e.g., Llama-3.3 70B with CL+AR = 85.5%); DBDC5 Japanese: Claude-3.5 Sonnet v2 with CL+AR = 89.0% (best). On BETOLD, CL+AR performance degraded and was consistently worse than on natural-dialogue datasets (no numeric best given; results highlighted as lower in Table II).",
            "qualitative_findings": "CL+AR improved classification on natural dialogue datasets (DBDC5), but effectiveness declines on structured or abstract representations (BETOLD) due to mismatch between generated natural-language analogies and abstract intent/entity dialogue format; increases token usage and sometimes hits context limits.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CL+AR can materially improve classification accuracy on natural dialogue benchmarks by scaffolding reasoning, but it is sensitive to dataset format and token-budget constraints; benefits are inconsistent on structured task-oriented data.",
            "uuid": "e8291.1",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Claude-3.5 Sonnet v2",
            "name_full": "Anthropic Claude-3.5 Sonnet v2",
            "brief_description": "A high-performing closed-source LLM from Anthropic used for breakdown detection, showing strong gains with analogical and curriculum-style prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5 Sonnet v2",
            "model_description": "Anthropic's 3.5-series (Sonnet v2) closed-source model (large parameter family, cited as strong classification consistency and high cost); used via API with temperature 0.",
            "reasoning_methods": [
                "analogical reasoning (AR)",
                "curricular learning + analogical reasoning (CL+AR)",
                "few-shot prompting (2S-Easy, 2S-Hard, 4S)",
                "chain-of-thought (CoT)",
                "zero-shot (ZS)"
            ],
            "reasoning_methods_description": "Evaluated under ZS, FS (easy/hard/4S), CoT, AR, and CL+AR prompts. AR prompts instruct the model to self-generate analogous conversations and use them for classification; CL+AR sequences analogies from easy to hard. CoT uses 'Let's think step by step.' Few-shot uses labeled exemplars.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Direct comparisons across prompting families were performed: AR/CL+AR vs. ZS/FS/CoT; reported accuracy and F1 per prompt type on DBDC5 and BETOLD. Observed sensitivity to exemplar selection (2S-Hard vs 2S-Easy) and prompt ordering.",
            "task_or_benchmark": "DBDC5 English & Japanese; BETOLD",
            "performance_results": "DBDC5 English: up to 85.5% accuracy using AR prompt; DBDC5 Japanese: up to 89.0% accuracy with CL+AR; on BETOLD reported high accuracies in some FS settings (e.g., 2S-Hard/4S) though exact BETOLD top figure for Sonnet v2 not isolated. Calibration: Sonnet v2 combinations yield strong classification consistency.",
            "qualitative_findings": "Sonnet v2 benefits substantially from AR and CL+AR prompts on natural dialogue tasks and retains strong calibration; however, prompt length / token costs are high and effectiveness decreases on structured BETOLD inputs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Large closed-source models like Claude-3.5 Sonnet v2 achieve state-of-the-art accuracy on DBDC5 when provided with analogical or curriculum-style prompts; prompting strategy choice materially impacts performance.",
            "uuid": "e8291.2",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A frontier OpenAI model evaluated under multiple prompting strategies; shows robust performance and sensitivity to prompt style, with modest benefits from few-shot and CoT in some settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI's GPT-4 family variant (frontier closed-source model) evaluated at temperature 0 across prompting strategies (ZS, FS, CoT, AR, CL+AR).",
            "reasoning_methods": [
                "few-shot prompting (2S-Hard, 4S)",
                "chain-of-thought (CoT)",
                "analogical reasoning (AR)",
                "zero-shot (ZS)"
            ],
            "reasoning_methods_description": "Prompts included few-shot exemplars (including 'hard' ambiguous examples), CoT ('think step-by-step'), AR/CL+AR self-generated analogies, and ZS instructions. For CoT, asking for justification before decision improved deliberation and calibration.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared across ZS, FS (2S-Easy/Hard, 4S), CoT, and AR/CL+AR; reported per-prompt performance. Noted that 2S-Hard improved DBDC5 English to 83.5% for GPT-4o and CoT slightly improved F1(B) from 85.9%→86.5% in one case.",
            "task_or_benchmark": "DBDC5 English & Japanese; BETOLD",
            "performance_results": "DBDC5 English: up to 83.5% accuracy (2S-Hard); BETOLD: up to 77.7% (4S prompting); CoT produced marginal F1(B) gains (example: +0.6 pp on DBDC5 English).",
            "qualitative_findings": "GPT-4o is robust but sensitive to prompt design; CoT yields small improvements on nuanced cases while AR/CL+AR can improve natural-dialogue benchmarks but are token-expensive; ordering of justification before decision improves calibration.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Providing challenging few-shot exemplars (2S-Hard) and requesting justifications before decisions refines accuracy and calibration; CoT helps borderline case identification but gains vary by dataset.",
            "uuid": "e8291.3",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3.3-70B",
            "name_full": "Llama-3.3 70B",
            "brief_description": "A large open-source Llama model used both as a teacher for reasoning traces and as an evaluated classifier; shows top-tier performance when combined with CL+AR prompting and serves as a source for distilled chains-of-thought.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.3 70B",
            "model_description": "Open-source 70B-parameter Llama model; used to (1) generate synthetic reasoning traces to fine-tune the 8B student and (2) evaluated with multiple prompting strategies.",
            "reasoning_methods": [
                "teacher-generated chain-of-thought (for distillation)",
                "few-shot prompting (2S-Hard)",
                "CL+AR and AR",
                "CoT"
            ],
            "reasoning_methods_description": "As teacher, it produced step-by-step reasoning traces r_i for training examples; as an evaluated model, it was prompted with FS/CoT/AR/CL+AR similarly to other LLMs. Best calibration achieved with 2S-Hard few-shot prompting.",
            "reasoning_diversity": "both (used single-method distillation for student and diverse prompting for evaluation)",
            "reasoning_diversity_experimental_setup": "Used both as backbone for distillation experiments (teacher→student) and was itself evaluated across prompting families. Compared CL+AR/AR/CoT/FS/ZS effects on DBDC5/ BETOLD and measured calibration (MSE).",
            "task_or_benchmark": "DBDC5 English & Japanese; used to generate teacher traces for DBDC5 training; evaluated on BETOLD subset",
            "performance_results": "DBDC5 English: CL+AR 85.5% accuracy (tied top). Best calibration MSE on DBDC5 English: 4.0 using 2S-Hard. General strong performance on DBDC5; variable on BETOLD.",
            "qualitative_findings": "Large open-source Llama-3.3 70B is effective both as a teacher for reasoning distillation and as a high-performing classifier when paired with CL+AR; it achieves excellent calibration with hard few-shot exemplars.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Distilling reasoning from a 70B teacher into a smaller student yields an efficient monitor; Llama-3.3 70B with CL+AR matches top closed-source accuracy on DBDC5 while providing good calibration.",
            "uuid": "e8291.4",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "An open-source model trained with reinforcement techniques to incentivize reasoning; evaluated here across prompting strategies and showing competitive few-shot performance on DBDC5 and BETOLD.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "DeepSeek's R1 model (reported large parameter family) that emphasizes reasoning ability via RL-based training; evaluated via ZS, FS, AR, CL+AR and CoT prompts.",
            "reasoning_methods": [
                "few-shot prompting (4S improved performance)",
                "analogical reasoning (AR)",
                "chain-of-thought (CoT)"
            ],
            "reasoning_methods_description": "Evaluated with ZS/FS/CoT/AR/CL+AR. Few-shot 4S improved DBDC5 English performance; AR/CL+AR also provided gains on natural dialogues. On BETOLD, few-shot 2S-Easy yielded strong transfer.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared ZS vs 2S-Easy/Hard vs 4S vs CoT vs AR vs CL+AR on DBDC5 and BETOLD; AR/CL+AR applied to 10% subsets due to token cost. Reported both accuracy and F1 per prompt type.",
            "task_or_benchmark": "DBDC5 English & Japanese; BETOLD",
            "performance_results": "DBDC5 English: ZS 81.1% → 4S 83.0% (example improvement); BETOLD: up to 75.8% accuracy with 2S-Easy. CL+AR/AR further improved DBDC5 accuracy in some runs (e.g., DeepSeek-R1 4S = 83.0%).",
            "qualitative_findings": "DeepSeek-R1 benefits from few-shot exemplars and analogical prompting on natural dialogue tasks; however, gains vary by dataset and token budget constraints.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Models trained to encourage internal reasoning (e.g., DeepSeek-R1) respond well to few-shot and analogical prompts, achieving competitive accuracy with proper prompting.",
            "uuid": "e8291.5",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "OpenAI GPT-3.5 Turbo",
            "brief_description": "An earlier OpenAI model evaluated across many prompting strategies; exhibited variable and generally weaker performance on BETOLD and sensitivity to prompt format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "OpenAI's GPT-3.5 Turbo model; lower-parameter family relative to GPT-4 and other frontier models, evaluated with ZS/FS/CoT/AR/CL+AR.",
            "reasoning_methods": [
                "few-shot prompting (2S-Easy/Hard/4S)",
                "chain-of-thought (CoT)",
                "analogical reasoning (AR)"
            ],
            "reasoning_methods_description": "Prompted similarly to other LLMs; evaluated under few-shot settings and CoT. Observed sensitivity to prompt presentation and skewed confidence in some setups.",
            "reasoning_diversity": "diverse (multiple prompting strategies evaluated)",
            "reasoning_diversity_experimental_setup": "Direct comparisons across ZS, FS, CoT, AR, CL+AR. Performance reported as ranges across datasets; noted substantial underperformance on BETOLD relative to other LLMs.",
            "task_or_benchmark": "BETOLD; DBDC5 English & Japanese",
            "performance_results": "BETOLD accuracy ranged from 41.2% to 64.5% depending on prompt; DBDC5 English accuracy ranged ~66.5%–76.6% depending on prompt style. Notably poor on BETOLD compared to larger models.",
            "qualitative_findings": "GPT-3.5 Turbo misclassifies borderline near-breakdown utterances and displays prompt-format sensitivity leading to skewed confidences; few-shot/CoT do not reliably rescue performance on structured BETOLD inputs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Smaller or older frontier models can struggle with structured task-oriented benchmarks and are highly sensitive to prompt construction; prompting alone may not bridge capability gaps.",
            "uuid": "e8291.6",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Paper-wide Prompting Suite",
            "name_full": "Prompting strategies evaluated (ZS, FS, CoT, AR, CL+AR)",
            "brief_description": "The collection of prompting/elicitation methods tested in the paper to induce different reasoning behaviours in LLMs: zero-shot, few-shot (easy/hard/4-shot), chain-of-thought, analogical reasoning, and curricular analogical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to multiple LLMs (OpenAI, Anthropic, Meta, Mistral, DeepSeek, etc.)",
            "model_description": "Prompt families used consistently across models: ZS (task description only), FS (2S-Easy, 2S-Hard, 4S), CoT ('Let's think step-by-step'), AR (self-generate analogous examples), CL+AR (generate analogies from easy→hard).",
            "reasoning_methods": [
                "zero-shot reasoning",
                "few-shot exemplar reasoning (including hard/easy exemplar selection)",
                "chain-of-thought (CoT)",
                "analogical reasoning (AR)",
                "curriculum learning combined with analogical reasoning (CL+AR)"
            ],
            "reasoning_methods_description": "ZS provides only instructions; FS inserts 2–4 labeled exemplars (easy vs hard chosen by annotator agreement); CoT elicits explicit intermediate steps; AR asks model to self-generate analogies then classify; CL+AR sequences analogies easy→hard to scaffold reasoning.",
            "reasoning_diversity": "both (the study systematically applied multiple, diverse prompting methods and contrasted them individually)",
            "reasoning_diversity_experimental_setup": "Systematic evaluation and ablations comparing ZS vs FS (2S-Easy/Hard/4S) vs CoT vs AR vs CL+AR across many LLMs and datasets (DBDC5 English/Japanese, BETOLD). Additional ablations: ordering justification before decision (improves calibration), two-pass AR vs single-pass (two-pass comparable), and token-budgeted subset experiments for AR/CL+AR (10% subset).",
            "task_or_benchmark": "DBDC5 English & Japanese (utterance-level), BETOLD (task-oriented conversation-level)",
            "performance_results": "Few-shot prompting (FS) consistently outperformed ZS and often CoT. Example cross-model highlights: DeepSeek-R1: ZS 81.1% → 4S 83.0% (DBDC5 EN); GPT-4o: ZS→2S-Hard increased to 83.5% (DBDC5 EN); Claude-3.5 Sonnet v2: AR/CL+AR yields up to 85.5% (DBDC5 EN) and 89.0% (DBDC5 JP). CoT produced mixed results: slight F1(B) gains in some cases (e.g., GPT-4o +0.6 pp) but often degraded performance on structured BETOLD.",
            "qualitative_findings": "Few-shot exemplars, especially 'hard' ambiguous ones (2S-Hard), improve calibration and accuracy; CoT helps borderline inferences but can worsen performance on highly structured/abstract datasets; AR and CL+AR produce the biggest gains on natural dialogues but are token-expensive and fragile on abstract representations.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Prompting diversity matters: few-shot (particularly hard exemplars) and analogical/curricular prompting can improve accuracy and calibration on natural-language dialogue breakdown tasks, but benefits are dataset-dependent and traded off against token costs and instruction-following reliability.",
            "uuid": "e8291.7",
            "source_info": {
                "paper_title": "Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models as analogical reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_as_analogical_reasoners"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Curriculum learning for natural language understanding",
            "rating": 1,
            "sanitized_title": "curriculum_learning_for_natural_language_understanding"
        }
    ],
    "cost": 0.01868,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents
5 Jun 2025</p>
<p>Abdellah Ghassel 
Xianzhi Li 
Member, IEEEXiaodan Zhu 
Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents
5 Jun 2025914105775835E661CF5DFEC00E7B39A3arXiv:2504.18839v2[cs.CL]large language modelsconversational artificial intelligencehuman-computer interactiondialogue systemsmodel calibration
While Large Language Models (LLMs) are transforming numerous applications, their susceptibility to conversational breakdowns remains a critical challenge undermining user trust.This paper introduces a "Detect, Explain, Escalate" framework to manage dialogue breakdowns in LLM-powered agents, emphasizing low-carbon operation.Our approach integrates two key strategies: (1) We fine-tune a compact 8B-parameter model, augmented with teacher-generated reasoning traces, which serves as an efficient real-time breakdown 'detector' and 'explainer.'This model demonstrates robust classification and calibration on English and Japanese dialogues, and generalizes well to the BETOLD dataset, improving accuracy by 7% over its baseline.(2) We systematically evaluate frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity breakdown assessment.These are integrated into an 'escalation' architecture where our efficient detector defers to larger models only when necessary, substantially reducing operational costs and energy consumption.Our fine-tuned model and prompting strategies establish new state-of-the-art results on dialogue breakdown detection benchmarks, outperforming specialized classifiers and significantly narrowing the performance gap to larger proprietary models.The proposed monitorescalate pipeline reduces inference costs by 54%, offering a scalable, efficient, and more interpretable solution for robust conversational AI in high-impact domains.Code and models will be publicly released.</p>
<p>I. INTRODUCTION</p>
<p>Conversational artificial intelligence (AI) has experienced exponential growth driven by the rapid advancement and widespread adoption of large language models (LLMs) such as ChatGPT [1], Claude [2], and Llama [3].The unprecedented success of ChatGPT, which attracted one million users in five days and exceeded 100 million in two months [4], [5], highlights both the unique capabilities and the immense potential of LLMs.These generative models have rapidly permeated numerous sectors, including customer service, content creation, marketing, and education, fundamentally reshaping human-AI interactions [6]- [8].</p>
<p>Despite these advancements, the rapid integration of LLMs into critical domains has surfaced significant challenges, notably dialogue breakdowns, which can severely disrupt user trust and conversational effectiveness [9]- [11].Dialogue breakdowns typically manifest as lapses in conversational coherence, leading to irrelevant, contradictory, or incoherent This paper was submitted on May X, 2025.Abdellah Ghassel, Xianzhi Li and Xiaodan Zhu are associated with the Ingenuity Labs Research Institute and the Department of Electrical and Computer Engineering at Queen's University, Kingston, ON, Canada (email: abdellah.ghassel@queensu.ca,li.xianzhi@queensu.ca,xiaodan.zhu@queensu.ca).exchanges that negatively impact human-AI interactions [12], [13].Table I presents an example where the assistant's utterances are contradictory, illustrating a common breakdown scenario.</p>
<p>Addressing dialogue breakdowns becomes increasingly critical with the increasing adoption of LLMs in high-stakes environments [6]- [8].Moreover, the tendency of LLMs to produce overly confident yet potentially inaccurate or hallucinated responses further complicates their reliable deployment [14], [15].Consequently, there is a pressing need for robust methodologies to detect and mitigate dialogue breakdowns, thereby preserving conversational quality and user trust.</p>
<p>Previous research in dialogue breakdown detection has predominantly focused on specialized classifiers trained on labeled datasets like the Dialogue Breakdown Detection Challenge [12], [16].However, the generalization of these models to diverse, real-world contexts remains limited.Recent literature also indicates that generalist LLMs, despite their flexibility, still lag behind human-level performance in nuanced conversational tasks, indicating a persistent research gap [17], [18].</p>
<p>Addressing these challenges, we introduce approaches that leverage the reasoning capabilities of generalist LLMs through supervised fine-tuning and structured prompting.Specifically, we fine-tune the efficient and powerful Llama-3.1 8B model [3] on both English and Japanese tracks from the DBDC5 dataset, incorporating synthetic reasoning trajectories generated by a larger, more capable Llama-3.3 70B model.These distilled reasoning tracks, incorporated during finetuning, aim to improve the student model's decision-making process and enhance the interpretability of its predictions.We rigorously evaluate its generalization capability using the challenging BETOLD dataset [19], which explicitly focuses on task-oriented dialogue breakdowns across diverse conversational contexts.</p>
<p>Additionally, we provide a comprehensive comparative analysis of both closed-source frontier models (OpenAI [1] and Anthropic [2]) and open-source alternatives (Meta [3], Mistral AI [20], and DeepSeek [21]), specifically targeting their dialogue breakdown detection capabilities.We explore advanced prompting methodologies including few-shot learning [22], [23], zero-shot chain-of-thought prompting [24], and propose analogical reasoning enhanced by curricular learning strategies [25]- [27].These strategies encourage systematic reasoning and adaptability, often reducing the need for extensive labeled datasets.</p>
<p>Through these novel techniques, we establish state-of-theart benchmarks for the Dialogue Breakdown Detection Challenge.Furthermore, we examine calibration metrics alongside traditional classification metrics, gaining insights into model overconfidence and reliability.Our proposed architecture introduces a cost-effective, real-time dialogue breakdown monitoring system that utilizes a fine-tuned Llama-3.1 8B model, selectively invoking resource-intensive models such as GPT-4, DeepSeek-R1, Sonnet-3.5, or Llama-3.1 405B when necessary.Our architecture optimizes resource utilization by strategically employing high-capacity models, which typically incur significant operational costs and energy consumption, only when necessary.This approach achieves substantial cost reductions and promotes sustainability.</p>
<p>Our contributions are summarized as follows:</p>
<p>• Conducted the first comprehensive comparative analysis across a wide range of open-source and frontier closedsource models, establishing new benchmarks in dialogue breakdown detection.• Extensively evaluated model calibration alongside accuracy, revealing critical insights into reliability and overconfidence across diverse conversational scenarios.• Proposed an efficient, real-time deployment architecture that significantly reduces operational costs and promotes sustainability by selectively invoking large-scale models.</p>
<p>II. RELATED WORK Ensuring robustness in conversational AI systems, particularly in dialogue breakdown detection and mitigation, is a vital area of research.Dialogue breakdowns refer to situations where conversational coherence or relevance is disrupted, hindering the conversation's smooth progression and diminishing user satisfaction [12], [13].Effective conversational agents must not only detect these breakdowns but also address them to maintain user engagement and trust [28], [29].</p>
<p>A. Specialized Models for Dialogue Breakdown Detection</p>
<p>Research into dialogue breakdown detection has yielded specialized classifiers that push the performance on datasets like DBDC5 (see Section V-A for more information) [12].The top-performing approaches leverage pre-trained transformer [30] encoders fine-tuned for breakdown classification.For example, the best model on the DBDC5 English track was an augmented BERT-based classifier, BERT+SSMBA [31], [32].BERT+SSMBA incorporates unlabeled dialogue data via two complementary techniques: extended pre-training on dialogue-rich datasets such as Reddit and Self-Supervised Manifold-Based Data Augmentation (SSMBA) [33].This approach exploits unlabeled data to improve the classifier's robustness in detecting dialogue breakdowns.</p>
<p>Further advances in semi-supervised learning have also significantly impacted the domain.The S2T2 model, for example, introduced a dual-teacher training paradigm to leverage both labeled and unlabeled dialogue data [34].S2T2 employs two teacher models: one trained on high-quality labeled data and another trained on masked dialogues variations, to guide a student model collaboratively.This hybrid training strategy has achieved new state-of-the-art results on the DBDC5 dataset, surpassing earlier methods.Specifically, S2T2 utilizes RoBERTa-large for the English track and XLM-R-large (a multilingual variant of RoBERTa) combined with a contextmatching mechanism for the Japanese track [34]- [36].</p>
<p>Recent studies have also explored the potential of generalpurpose large language models (LLMs) in dialogue breakdown detection.Finch et al. [17], [18] evaluated ChatGPT's performance in identifying dialogue behaviour across nine distinct categories defined in the ABC-Eval dataset.Their findings reveal that while ChatGPT shows promise, even outperforming specialized models in identifying empathetic behaviour, it still falls short of human performance in other dialogue tasks.This suggests that current large models still face challenges in reliably identifying breakdown instances despite their sophisticated conversational abilities and extensive world knowledge.Nonetheless, using LLMs for dialogue evaluation is a promising direction since they bring broad world knowledge and understanding of conversational dynamics.To our knowledge, no other work has evaluated LLMs for dialogue breakdown detection and remediation.</p>
<p>B. Current State of Conversational Agents</p>
<p>Beyond detecting breakdowns, state-of-the-art conversational agents incorporate strategies to mitigate breakdowns.For example, if a breakdown is detected, a system might employ a recovery strategy (as studied in DBDC5's recovery track) to ask for clarification or provide corrections [12].Leading chatbots like ChatGPT and Claude are also trained via alignment techniques such as reinforcement learning from human feedback (RLHF) to minimize toxic, incoherent, or nonsensical outputs that could derail a dialogue [1], [2].Models like Claude-2 even engage in internal debates during training to identify and eliminate reasoning flaws, aiming to reduce the likelihood of breakdowns due to factual errors or contradictions [37].The net result is that modern conversational AI systems are progressively improving at maintaining coherent dialogues despite ambiguous user inputs.</p>
<p>C. Techniques in Conversational AI</p>
<p>To further improve the reliability of LLM conversational agents, researchers are exploring a variety of techniques: Analogical Reasoning.This technique guides a language model to draw on relevant past examples or scenarios by analogy when confronting a new problem.Instead of relying solely on provided examples, the model generates analogous examples as context.Inspired by human problem-solving via analogy, analogical prompting has been shown to improve reasoning accuracy [25], [38].It "prompts language models to self-generate relevant exemplars or knowledge in context, before proceeding to solve the given problem," thereby removing the need for hand-crafted exemplars and tailoring the reasoning to the specific query [25].In practice, a dialogue agent using analogical reasoning might recall a similar conversation or scenario it has encountered and use that information to form a better response to a user query.Chain-of-Thought Reasoning.Chain-of-thought (CoT) is a reasoning technique in which the model is encouraged to produce explicit intermediate reasoning steps ("thinking aloud") before giving a final answer.This has effectively improved logical consistency and arithmetic or commonsense reasoning in LLMs [24], [39], [40].For dialogue agents, chain-ofthought can be used internally (the model can reason through the user's query, context, and knowledge base step-by-step) to formulate a correct and context-appropriate response.This reduces mistakes and non-sequiturs.CoT reasoning is now a common technique to bolster LLM robustness on tasks requiring multi-step inference or clarification, as it significantly "enhances their ability to reason" about the conversation [24], [39], [40].Zero-Shot and Few-Shot Learning.Unlike traditional dialogue systems requiring extensive task-specific training, LLMbased agents excel at in-context learning as they can adapt to new instructions or domains given few examples or none at all.In few-shot learning, a handful of example dialogues or question-answer pairs are provided in the prompt, and the model generalizes the pattern to the new query.The seminal GPT-3 work showed that large models can perform new language tasks by observing just a few demonstrations, essentially treating "language models as few-shot learners" [22].This allows a conversational agent to be quickly customized to a new domain by giving it a few sample conversations as guidance rather than retraining the model.LLMs can often do this for dialogue tasks.For instance, a well-designed prompt can instruct the model to play the role of a customer service agent and will generate reasonable dialogue without any fine-tuning.In-context learning empowers LLM-based agents to handle various conversational scenarios with minimal additional training.</p>
<p>III. PROBLEM DEFINITION Dialogue breakdown, defined as the deterioration of coherence, relevance, or conversational fluidity between a user and a conversational agent, remains a significant challenge in conversational AI [12], [29].Such breakdowns may manifest as irrelevant responses, misunderstandings, contradictions, or incoherent interactions that hinder the natural progression of dialogue and consequently degrade user trust and satisfaction [10], [11].As LLMs from the OpenAI [1], Claude [2], and Llama [3] families are rapidly adopted for diverse conversational tasks, addressing these dialogue disruptions has become paramount.Moreover, dialogue breakdown detection becomes even more critical due to LLMs' propensity for confident but potentially incorrect or hallucinated outputs, resulting in increased user frustration [14], [15].</p>
<p>We consider a multi-turn dialogue sequence D between a user (U) and a conversational agent (A).The dialogue proceeds in pairs of utterances, where at turn i, the user produces an utterance u i , and the agent replies with s i .Formally, we may represent the dialogue as follows:
D = u 1 , s 1 , u 2 , s 2 , . . . , u n , s n .
We aim to detect, at each agent utterance s i , whether the conversation has experienced a breakdown in coherence, relevance, or consistency.</p>
<p>A. Utterance-Level Breakdown Detection</p>
<p>Let H i denote the contextual history available just before the agent produces its i-th response:
H i = u 1 , s 1 , . . . , u i−1 , s i−1 , u i .
We define a classification function f that, given H i and the agent's latest utterance s i :
f : H i , s i → ( bi , ĉi , ĵi ),
where:
• bi ∈ {0, 1} is a binary classification indicating dialogue breakdown (1) or non-breakdown (0). • ĉi ∈ [0, 1]
is a confidence score representing the model's certainty about the predicted label.</p>
<p>• ĵi is a textual justification explaining the model's reasoning process.For a complete dialogue D of length n system turns, the output O of the detection system is thus:
O(D) = {(b i , c i , j i )} n i=1</p>
<p>B. Consolidation of Three-Class Annotations</p>
<p>Previous datasets like DBDC [12] use three-level labels: Breakdown (B), Possible Breakdown (PB), and Non-Breakdown (NB).Given the subjectivity of human annotations in dialogue breakdown detection, we consolidate 'Possible Breakdown' PB into the 'Breakdown' (B) class.When multiple human annotators supply these labels, let p i be the fraction who labeled s i as B or PB.We can convert each utterance into a binary label b i by thresholding:
b i = 1, if p(b i | s i ) ≥ 0.5, 0, otherwise.
where p(b i | s i ) is the fraction of human annotators labeling s i as a breakdown.Throughout this paper, the binary classification of "breakdown versus non-breakdown" refers to this mapping.</p>
<p>C. Conversation-Level Labeling</p>
<p>In task-oriented datasets such as BETOLD [19], the focus is on the entire conversation outcome.Each dialogue D is labeled a failure if, for instance, the user hangs up or requests escalation to a human agent.We denote this conversation-level label as
O(D) =
1, if the conversation leads to breakdown, 0, otherwise.</p>
<p>In this case, the model seeks to predict after observing all (u i , s i ) pairs in D. This differs from utterance-level detection since any local breakdown event might cause the entire conversation to fail.</p>
<p>D. Confidence Calibration</p>
<p>An effective breakdown detector should classify accurately and calibrate its confidence well.Let p i be the true probability of a breakdown for the i-th utterance (estimated from multiple annotators) and ĉi be the model's predicted probability.We measure calibration quality via mean-squared error (MSE):
MSE = 1 N N i=1 ĉi − p i 2 ,
where N is the total number of utterances in the test set.Lower MSE indicates that the model's self-reported confidence aligns more closely with actual annotator distributions.</p>
<p>IV. METHODOLOGY Our approach combines specialized fine-tuning and advanced prompting to achieve robust dialogue breakdown detection in both open-domain and task-oriented conversations.We focus on three key components: a compact fine-tuned model to detect dialogue breakdowns efficiently, leveraging advanced prompting strategies for generalist LLMs, and a proposed multi-tier inference architecture that calls upon frontier models only when necessary for corrections, optimizing both cost and accuracy.</p>
<p>A. Supervised Fine-tuning with Reasoning Augmentation</p>
<p>To deploy an efficient alternative for real-time monitoring, we fine-tune a smaller model on labeled breakdown data using supervised fine-tuning (SFT).We choose Llama-3.1 8B [3] as it offers a good balance between accuracy and computational cost.The model is fine-tuned using the DBDC5 English and Japanese tracks [12] for per-utterance breakdown labels.Let:
• D = {(H i , s i , b i )} N
i=1 be the training data, where each sample has context H i , agent utterance s i , and a binary label b i ∈ {0, 1}.</p>
<p>• T be a larger "teacher" LLM, such as Llama-3.3</p>
<p>70B that can generate synthetic reasoning traces r i (i.e., a chain-of-thought explanation) for each sample (H i , s i , b i ).We augment the original training input with this synthetic reasoning, effectively creating a more informative training set D ′ .We fine-tune our student model S on the augmented dataset D ′ .The model is trained to predict the binary label b i (minimizing cross-entropy loss L CE ) and to generate a textual justification ĵi .For the latter, the teacher-generated reasoning traces r i serve as training targets, enabling the student model to distill these structured reasoning patterns for improved classification and explanation.The cross-entropy loss [41] is defined as: where, y i denotes the true label for utterance i, and ŷi denotes the predicted probability of breakdown.This training phase allows the student model to observe both the original dialogue context (H i , s i ) and the teacher's structured reasoning patterns r i during training, learning to distill these insights.
L CE = − N i=1 [y i log ŷi + (1 − y i ) log(1 − ŷi )],</p>
<p>B. Advanced Prompting Strategies</p>
<p>While a fine-tuned compact model is efficient for real-time monitoring, powerful but costly LLMs can still be employed through strategic prompt engineering.Let G denote a generalist LLM, for instance GPT-4 or DeepSeek-R1.Given a conversation snippet (H i , s i ), we form a prompt Π(H i , s i ; α) under a chosen strategy α ∈ {ZS, FS, CoT, AR, CL + AR, . . .}.We then parse g(Π(H i , s i ; α)) to obtain a breakdown label and confidence.The main prompting strategies we explore are: Zero-Shot (ZS) Prompting.We provide only a task description and the current example (H i , s i ).The model must infer the classification criterion from the instructions alone.A sample zero-shot prompt for DBDC5 is shown in Figure 1.Few-Shot (FS) Prompting.We supply k labeled examples of breakdown vs. non-breakdown before the new sample.Formally,
Π FS (H i , s i ; k) = (H ′ 1 , s ′ 1 , b ′ 1 ), . . . , (H ′ k , s ′ k , b ′ k ) ∪ (H i , s i ).
Within FS, we explore:</p>
<p>• 2-Shot Easy (2S-Easy): Two clear dialogues (one breakdown and one non-breakdown) where human annotators were very confident in the labels (&gt;80% agreement).For example, an easy non-breakdown case might be a smooth dialogue that successfully completes, and an easy breakdown case might feature an obvious user hang-up after a system error.This helps the model anchor on unambiguous prototypes.• 2-Shot Hard (2S-Hard): We instead use two challenging samples where the correct outcome is subtle (annotator agreement 60-70%).One example might be a conversation with some confusion that eventually recovers (almost a breakdown, but not quite), and another might show a user hesitant or mildly frustrated (not a clear-cut hangup, but dialogue quality is low).We hypothesize that exposing the model to ambiguous scenarios can improve its performance under uncertainty and lead to more calibrated confidence scores in similar situations.• 4-Shot (4S): We combine the Easy and Hard exemplars, two from 2S-Easy and two from 2S-Hard, thus exposing the model to a broader range of scenarios.While 4S can yield higher accuracy, prompt length grows, which increases token consumption and may approach context limits for some models, especially with longer dialogues.</p>
<p>In datasets like BETOLD where per-utterance breakdown rationales from annotators are unavailable, we select short (15-20 turns) vs. long (21-30 turns) dialogues as "easier" vs. "harder" exemplars, then mimic the above FS setups accordingly.In addition, since the annotators did not provide their reasoning, we used Llama-3.370B to generate the step-by-step reasoning field given each exemplar, the annotators' probability distribution (treated as a confidence score) and the decision label.</p>
<p>C. Self-Guided Reasoning Techniques</p>
<p>To further enhance the model's reasoning depth and generalizability, we used and proposed advanced prompting techniques designed to encourage structured, self-generated reasoning: Chain-of-Thought (CoT).We encourage multi-step reasoning by instructing g to "think step-by-step" before deciding:
Π CoT (H i , s i ) = H i , s i , "Let's think step by step." .
This often yields more coherent predictions on complex queries, albeit at higher token usage.Analogical Reasoning (AR).Instead of providing examples directly, we ask G to generate hypothetical analogous conversations A i from its internal knowledge, then classify the current conversation.Formally:
A i = G Π AR (H i , s i ) ,
where Π AR instructs the model to "recall relevant past dialogues" similar to (H i , s i ).The final prediction is then based on both H i , s i and the newly generated analogies A i .This self-guided technique removes the need for hand-crafted exemplars, offering more tailored guidance for each test case.</p>
<p>D. Our Proposed Reasoning Technique</p>
<p>Curricular Learning with Analogical Reasoning (CL+AR).Curriculum learning involves training or prompting the model with an order of tasks that progresses from easy to hard, mimicking the way humans learn.The model builds a foundation to tackle more difficult ones by mastering simpler dialogue tasks.We propose applying this idea to LLM reasoning: the model is first steered to solve easy "proxy" queries related to the target query, then gradually given harder versions of the problem.The easy queries and solutions serve as stepping stones, forming a curriculum for the model's chain of thought.In a dialogue context, we first ask the LLM to generate A (1) i (an easy analogous example), then A (close to the given input's complexity).This gradually "walks" the model from simpler to harder analogies.We then request the final decision for (H i , s i ).Let A</p>
<p>(1) i , . . ., A (m) i be the generated set of analogies.The overall prompt is:
Π CL+AR (H i , s i ) = {A (1) i , A (2) i , . . . , A (m) i , (H i , s i )}.
We find that this method improves classification on challenging dialogues, although it may increase output token length.</p>
<p>E. Deployment Architecture</p>
<p>While a powerful LLM can reliably detect breakdowns, frequent usage may be cost-prohibitive.We address this via a real-time, hierarchical system (Figure 2) with three main modules to ensure dialogue robustness and maintain user trust:</p>
<p>1) AI Assistant: Generates a candidate response s i given the user's input u i and context H i :
s i = G assistant H i , u i .
Here, G assistant can be a moderately large language model such as Llama-3.3 70B.2) Dialogue Disruption Monitor: Before presenting the assistant-generated response s i to the user, our finetuned model rapidly assesses the response for potential breakdowns or unsafe interactions, outputting:
( bi , ĉi , ĵi ) = G monitor H i , s i ,
where bi ∈ {0, 1} is the predicted breakdown label, ĉi ∈ [0, 1] is the confidence, and ĵi (optional) is a justification.If bi = 0 or ĉi &lt; T , s i is deemed acceptable.Otherwise, we escalate to a larger "superior" LLM.The sub-second detection latency supports seamless user experiences.3) Superior Model: Upon intervention, this model revises the response to prevent breakdown and ensure user safety:
s * i = G superior H i , u i .
The monitor then re-checks H i , s * i .This process can iterate up to k times or until the breakdown is resolved.This multi-tiered strategy significantly reduces computational demands and ensures resource-intensive models are only utilized in critical scenarios, thereby enhancing efficiency and user experience in conversational AI deployments.</p>
<p>V. EXPERIMENTS</p>
<p>We evaluate our proposed approach using three dialogue breakdown detection datasets: English and Japanese tracks from DBDC5 and the BETOLD task-oriented dialogue dataset.Breakdown Expectation for Task-Oriented Long Dialogues (BETOLD).Introduced in 2022, BETOLD identifies breakdowns specifically in task-oriented service dialogues, such as customer support calls [19].This dataset comprises 13,524 real human-agent phone dialogues, annotated based on whether the interaction ended in a "late user-initiated hang-up or forward" (LUHF).LUHF instances indicate user frustration, leading to either terminating the call prematurely or requesting human intervention.Approximately 33% of the dataset represents breakdown cases.BETOLD dialogues utilize abstract representations of intents and entities derived from NLU/NLG annotations instead of raw textual data to maintain user privacy.We follow official train-test splits provided by dataset creators, treating BETOLD's automatic labels as ground truth for evaluation purposes.</p>
<p>B. Data Preprocessing and Experimental Setup</p>
<p>Following prior works [9], [34], we preprocess dialogue data from DBDC5 and BETOLD datasets by converting annotations into binary labels (Breakdown/Non-Breakdown). Ambiguous annotations ('Possible Breakdown') are consolidated into the Breakdown class.To accommodate privacy constraints, the BETOLD dataset utilizes intent and entity abstractions rather than raw utterances, potentially limiting the efficacy of specific prompting techniques.We parse these structured dialogues and present them in a text form like "System: Intent: X -Entities: Y" for each turn, thereby preserving the sequence and dialogue flow in anonymized form.Basic cleaning (removal of extraneous symbols, ensuring consistent turn indexing) is applied for both datasets.No additional data augmentation is used beyond what is provided in the datasets.</p>
<p>Our evaluation requires the LLM to provide three fields per response: justification, decision, and confidence score.Experimentally, the ordering of these requests influenced the model's performance significantly.When the model was first prompted for a decision, subsequent justifications often appeared overly confident and less reflective.Conversely, requesting justifications first yielded more nuanced and deliberative decisions, enhancing overall output quality.</p>
<p>For advanced prompting methods (AR and CL+AR), we conducted experiments on a random subset representing 10% of each dataset due to higher token demands.Ablation studies indicated that a two-pass analogical reasoning approach (initial analogy generation followed by integration) performed comparably to single-pass prompting, aligning with findings in existing literature [25].</p>
<p>C. Fine-tuning Parameters</p>
<p>We fine-tune a Llama-3.1 8B instruct model using Low-Rank Adaptation (LoRA, rank=16) [42], employing an AdamW 8-bit optimizer [43] with a learning rate of 2 × 10 −4 , linear decay, batch size of 8, and weight decay of 0.01 across 3 epochs.Training utilized a single NVIDIA A100 40GB GPU.We hypothesize that a finely tuned smaller LLM can balance competitive accuracy with efficiency, making it viable for real-time dialogue monitoring.Our trained dialogue disruption monitor is openly available on HuggingFace 1 , facilitating community engagement.Emphasizing sustainability, our fine-tuning approach, which leverages LoRA's partial weight updates, significantly reduces computational load and energy consumption compared to traditional full-scale training.</p>
<p>D. Evaluation Metrics</p>
<p>We use accuracy and F1 score as our primary evaluation metrics for breakdown detection.Accuracy measures the overall correctness of breakdown vs non-breakdown predictions.F1 score (the harmonic mean of precision and recall) is especially important because the breakdown class is under-represented; it evaluates how well models detect breakdowns without neglecting the non-breakdown class.We therefore report the F1 score for both classes along with overall accuracy, in line with prior work on DBDC5 [32], [34].This mirrors the official challenge metrics and provides continuity with previous state-of-the-art results.Metrics are computed on held-out test sets (DBDC5 evaluation sets and reserved BE-TOLD splits).We parse the model's JSON-formatted output to extract the decision label.In cases where model outputs deviate from the required JSON format, instead of discarding these responses, we employ Llama-3.3 70B as an LLMbased judge to interpret the model's textual output and extract the intended classification decision when the JSON format is not strictly adhered to.</p>
<p>On the DBDC English and Japanese tracks, we investigated the model's overconfidence using the MSE between the annotators' probability distribution for a dialogue utterance and the LLM's verbalized confidence in its corresponding decision.</p>
<p>E. Inference and Cost Considerations</p>
<p>For a comprehensive study, we examine a diverse range of LLMs, as listed in Table II.The models are ordered beginning with proprietary models from organizations such as OpenAI (GPT-3.5 and GPT-4) [1], Anthropic (Claude-3.5Haiku and Claude-3.5Sonnet v2) [2].In contrast, open-source models from Mistral AI (Mixtral 8x7B and Mixtral 8x22B) [20], DeepSeek (DeepSeek-R1) [21] and Meta (Llama-3.1 8B, Llama-3.3 70B and Llama-3.1 405B) [3] offer greater transparency regarding model details, increased flexibility for customization, and are often more cost-effective.For inference, we route queries through the OpenRouter API 2 , which provides a unified interface to multiple LLM endpoints.This approach allowed us to evaluate different models under a consistent framework and logging.However, using these hosted models incurs costs per token.In practice, Claude 3.5 Sonnet v2 is the most expensive, roughly on the order of $5/M input tokens and $15/M output tokens.We carefully designed prompts to be concise (especially for zeroshot and few-shot setups) to control token usage.Techniques like AR and CL+AR produce longer interactions since the model generates examples as part of its answer, which we note as a trade-off.They may improve reasoning but use more tokens, impacting cost and latency.All models use a temperature of 0. While larger models support longer contexts, 2 https://openrouter.aia uniform maximum token limit of 2048 was applied across all models to ensure consistent comparison, particularly reflecting the constraints of some smaller models and API limitations for longer AR/CL+AR generations.</p>
<p>VI. RESULTS AND DISCUSSION</p>
<p>This section presents detailed performance analyses across three datasets: BETOLD (task-oriented dialogues) and DBDC5 (English and Japanese tracks).We evaluate multiple prompting strategies, proprietary and open-source models, and our Dialogue Disruption Monitor.Table II summarizes accuracy, F1 scores for Breakdown (B) and Non-Breakdown (NB) classes, and calibration performance (MSE) where applicable.</p>
<p>A. Main Findings</p>
<p>State-of-the-Art Results.Our results confirm significant progress beyond earlier benchmarks.On the DBDC5 English track, multiple closed and open-source modern LLMs surpass the prior best (77.9% accuracy from S2T2).For instance, Claude-3.5Sonnet v2 attains up to 85.5% accuracy (AR prompt), matching Llama-3.3 70B with CL+AR (85.5%).This tie represents the new top performance on DBDC5 English.On the DBDC5 Japanese track, the best outcome of 89.0% accuracy comes from Claude-3.5 Sonnet v2 with CL+AR, slightly above the leading opensource DeepSeek-R1 model at 87.0%.Thus, for Japanese, the Anthropic model retains a slight edge.Nonetheless, the top open-source systems now perform competitively, typically within 1-3 points of the best closed-source counterpart.Closed-Source Frontier Models.Claude-3.5 (Haiku, Sonnet v2) demonstrate strong classification consistency across both DBDC5 tracks, ranging from 74% to 89% on Japanese and 78% to 85% on English.In particular, Sonnet v2 combined with AR or CL+AR prompts yields top accuracies, for instance, 85.5% on English and 89.0% on Japanese.GPT-4o likewise competes closely, achieving up to 77.7% on BETOLD via 4S prompting and 83.5% on DBDC5 English using a more challenging 2S-Hard strategy.While marginally behind Claude-3.5Sonnet v2 on Japanese, GPT-4o's performance remains robust, although it exhibits greater sensitivity to variations in prompt style.</p>
<p>In contrast, GPT-3.5 Turbo underperforms substantially on BETOLD, with accuracy ranging from 41% to 64.5%.It tends to misclassify borderline "near-breakdown" utterances or produce imbalanced predictions.A plausible explanation is that GPT-3.5 Turbo is more sensitive to how examples are presented; certain prompt structures lead to skewed confidence or confusion in distinguishing near-breakdown from nonbreakdown scenarios.</p>
<p>Open-Source</p>
<p>Models.Larger open-source models (Llama-3.370B, Llama-3.1 405B, Mixtral 8x22B, DeepSeek-R1) match or exceed closed-source baselines on DBDC5 English (80%-85%) and demonstrate competitive performance on DBDC5 Japanese.However, on BETOLD, performance variability is pronounced, ranging from 68.3% (Llama-3.1 8B, 2S-Hard) to 75.8% (DeepSeek-R1, 2S-Easy), reflecting difficulties generalizing Limitations of Chain-of-Thought.CoT yields mixed results, slightly improving borderline-case identification.For instance, GPT-4o on DBDC5 English F1(B) improves from 85.9% to 86.5%.However, this occasionally degrades performance on structured datasets like BETOLD, indicating that an optimal reasoning complexity is dataset-dependent.This highlights the importance of tailoring reasoning complexity to the task: dialogues with short turns or highly structured, domain-specific content may be more effectively processed using concise prompts rather than elaborate 'think step-by-step' sequences.</p>
<p>Challenges with AR and CL+AR on BETOLD.While AR and CL+AR techniques generally improved performance on natural dialogue datasets (such as the DBDC5 tracks), their effectiveness significantly declined when applied to BE-TOLD.Results for BETOLD (highlighted in grey in Table II) consistently showed lower performance.We hypothesize this reduction is primarily due to the nature of BETOLD itself, which represents dialogues through structured intents and entities rather than complete natural language utterances.This mismatch likely hinders the models' generalization capabilities since they were predominantly trained on conventional dialogue.Through comprehensive error analysis, we observed that models frequently did not fully adhere to the provided instructions.Common issues included either failing to generate any analogous examples, opting instead to solve the original dialogue directly, or producing overly brief analogous summaries that poorly aligned with the dialogue under evaluation.As demonstrated in Figure 4, even frontier models like GPT-4o exhibited challenges in accurately following instructions.This problem is exacerbated by the relatively longer dialogues in BETOLD (averaging 20-30 turns) compared to the shorter dialogues in DBDC5 English and Japanese tracks (typically 10-20 turns).Providing three analogous examples plus restating the original conversation often exhausted the available token limit, as depicted in Figure 3.Moreover, smaller model variants such as Llama-3.1 8B, Mixtral 8x7B, and Claude-3.5Haiku struggled to generate relevant analogies compared to their larger counterparts.</p>
<p>C. Calibration and Confidence Analysis</p>
<p>Superior calibration (low MSE) indicates alignment between model confidence and human correctness.Llama-3.3 70B reaches the best DBDC5 English calibration (4.0, 2S-Hard), while Llama-3.1 405B achieves 4.6 on Japanese (2S-Hard).Among closed-source, Claude-3.5Haiku can hit 4.1 MSE on DBDC5 English (2S-Hard).Meanwhile, our fine-tuned 8B Dialogue Disruption Monitor yields an MSE of 4.9.While slightly higher, this result still demonstrates that moderate-scale, task-specific training can produce stable confidence alignment.</p>
<p>D. Practical Costs and Sustainability</p>
<p>Resource efficiency is a crucial factor influencing the practical deployment of language models.Advanced models such as Claude-3.5Sonnet v2 (400B parameters), DeepSeek-R1 (671B parameters), and GPT-4 (estimated 1.7 trillion parameters [45]) demand substantial monetary costs, experience network-induced latency, and have a higher carbon footprint per query due to their considerable size and reliance on data-center computation.For example, based on cloud-provider pricing from Amazon Web Services (AWS), querying Llama-3.1 70B is approximately 3.3 times more expensive than its 8B-parameter counterpart, while Llama-3.1 405B incurs a cost approximately 10.3 times higher [46].Additionally, using longer prompts (such as AR or CL+AR) significantly escalates costs due to the increased token usage per query.</p>
<p>In contrast, our fine-tuned Llama-3.1 8B model operates efficiently on a single A100 GPU, resulting in minimal incremental cost per query and low inference latency (each dialogue inference completes in under half a second).This efficiency is advantageous for deployments requiring real-time detection of dialogue breakdowns for every utterance.Assuming only 10% of utterances lead to dialogue breakdown, our practical deployment architecture is estimated to reduce costs by approximately 54% compared to employing Llama-3.1 405B for each turn (see Appendix A for detailed cost calculations).Such selective invocation lowers operational costs and aligns with sustainability objectives.Enterprise-scale LLM deployments like ChatGPT consume around 1,058.5 GWh annually, equivalent to the yearly electricity consumption of approximately 100,810 U.S. households [47].By limiting resource-intensive model invocations to instances of suspected critical dialogue breakdowns or high uncertainty, our approach effectively balances performance, cost-efficiency, and sustainability.</p>
<p>VII. CONCLUSION</p>
<p>Research and development in conversational AI robustness is rapidly advancing on multiple fronts.Larger and bettertrained LLMs provide a stronger base, specialized benchmarks like DBDC5 and BETOLD drive progress in error detection, and techniques such as analogical reasoning, curriculum learning, and chain-of-thought help models use their internal knowledge more effectively.By combining high-performing models with these strategies, the latest conversational agents are increasingly adept at sustaining error-free dialogues; however, evaluations show that a gap remains between current systems and human-level communication when handling the full complexity of conversation.While advanced prompting (CoT, AR) can improve performance for some high-parameter models, these gains are inconsistent in specialized domains.Short, curated exemplars typically offer a good trade-off between performance and token usage.The cost and energy savings are considerable, especially in frequent-turn applications.Eliciting numeric confidence and justifications can mitigate overconfidence and improve trustworthiness.Our experiments reveal that carefully structured prompts, where the model justifies and then decides, yield lower calibration error.For high-stakes, high-volume systems, coupling a fast, fine-tuned breakdown detector with on-demand escalation to a large frontier LLM can ensure both reliability and sustainability.While closed-source models still lead in absolute accuracy, the performance gap is closing, especially once small open-source models are carefully tuned.Future work should focus on systematically combining these approaches to achieve robust, interpretable breakdown detection at scale.</p>
<p>VIII. LIMITATIONS</p>
<p>Although our approach performs competitively on both English and Japanese DBDC5 benchmarks, the smaller Llama-3.1 8B model shows limited Japanese coverage, suggesting stronger multilingual pretraining is needed.Moreover, real-world dialogues (involving code-switching, adversarial inputs, or varied domains) may demand specialized adaptations beyond our well-defined benchmarks, while prompt engineering remains model-and dataset-specific, increasing resource demands.The opaque nature of LLMs, even under chain-of-thought or analogical prompting, leaves internal reasoning partially hidden; consequently, escalating errors to more costly, higher-capacity models adds latency, energy use, and expense.Finally, simpler binary breakdown labels may miss borderline cases, pointing to a need for finer-grained schemes and more resource-aware architectures for robust deployment in diverse, high-stakes environments.Nonetheless, these constraints do not undermine the contributions of this work; we leave their resolution to future research.</p>
<p>APPENDIX</p>
<p>This section quantifies the benefit of selective model escalation.We consider a 15-turn dialogue that normally runs on Llama-3.1 70B.When a real-time monitor detects a potential breakdown, the request is re-issued to the larger Llama-3.1 405B.Empirically, such escalations are required on roughly 10% of turns.</p>
<p>Table III lists on-demand prices3 for Meta's Llama-3.1 family.Bedrock bills input and output tokens separately, so the effective cost per 1k tokens is the sum of the two columns.</p>
<p>D. Adding a Lightweight Monitor</p>
<p>The monitor itself runs on Llama-3.1 8B and processes every turn:</p>
<p>9 600 tokens × $0.00044 = $0.00422.</p>
<p>Putting it all together: $0.00422 (monitor) + $0.01705 (dialogue) = $0.02127.</p>
<p>Selective escalation with monitoring cuts cost by 1 − 0.02127 0.0461 ≈ 54% relative to running the entire conversation on Llama-3.1 405B, yet preserves the option to leverage the larger model when necessary.</p>
<p>Fig. 1 .
1
Fig. 1.Sample Zero-Shot Prompt for DBDC5</p>
<p>Fig. 2 .
2
Fig. 2. Real-time Response Correction Architecture.The dialogue disruption monitor intercepts potentially unsafe assistant responses, triggering a correction from a superior model before presenting the response to the user.</p>
<p>•</p>
<p>Always 405B 9.6 × $0.00480 = $0.0461• Always 70B 9.6 × $0.00144 = $0.0138C. Selective Escalation (70B + 10% 405B) 70B: 0.9 × 9 600 = 8 640 tokens ⇒ 8.64 × $0.00144 = $0.01244,405B: 0.1 × 9 600 = 960 tokens ⇒ 0.96 × $0.00480 = $0.00461,Total ≈ $0.01705.</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. BETOLD: CL+AR Prompt</p>
<p>TABLE I SAMPLE
I
DIALOGUE ILLUSTRATING CONTRADICTORY TURNS.1.Assistant: It's nice to go shopping alone.2. User: I agree.That's nice.3. Assistant: Shopping takes time.4. User: Window shopping is also fun.Determine if the next utterance causes a breakdown: 5. Assistant: It's fun to go shopping with somebody.</p>
<p>TABLE II RESULTS
II
OF PROPRIETARY AND OPEN-SOURCE MODELS ON DIALOGUE BREAKDOWN DATASETS.BOLD = BEST, UNDERLINED = SECOND-BEST.
from natural dialogues to abstract intent representations.Llama-3.1 405B, despite its size, does not consistentlyFamily surpass its 70B counterpart. Its best English-track accuracy Model Prompt BETOLDDBDC5 EnglishDBDC5 JapaneseAccuracy F1(B) F1(NB) Accuracy F1(B) F1(NB) MSE Accuracy F1(B) F1(NB) MSE hovers near 79%-81%, indicating marginal variances toPrev. SOTA S2T2 prompting techniques.----77.9 82.4--76.7 75.4--ZS Dialogue Disruption Monitor. Despite having fewer pa-74.4 55.0 CoT 73.9 52.6 rameters, our fine-tuned Llama-3.1 8B model achieves82.1 82.080.4 86.3 82.0 86.965.3 5.9 71.2 6.874.6 76.0 77.1 76.573.0 7.3 77.6 8.2Claude-3.5 Haiku competitive performance. On the DBDC5 English dataset, it 2S (Easy) 74.0 64.3 2S (Hard) 74.1 66.9 4S 74.1 67.0 attains an accuracy of 81.5%, surpassing several larger models.79.5 78.8 78.782.5 87.5 82.5 87.4 82.9 87.670.9 5.5 70.9 4.1 72.5 4.468.9 73.3 66.3 72.0 67.0 72.362.9 10.6 57.7 8.4 59.2 10.2Anthropic For the DBDC5 Japanese dataset, the model achieves a mod-AR 76.3 68.6 CL+AR 77.0 64.4 ZS 75.1 54.9 erate yet stable accuracy of 67.9%, with balanced F1 scores CoT 76.6 56.7 across classes. Our DBDC5-fine-tuned monitor, when evalu-81.0 83.1 82.8 84.078.5 85.8 78.0 85.4 82.7 87.5 82.5 87.355.7 55.1 71.9 8.1 --71.6 7.882.0 88.5 78.0 85.9 81.3 81.2 78.8 78.759.1 50.0 81.4 8.2 --78.9 8.3Claude-3.5 Sonnet ated on the BETOLD dataset (on which it was not trained), 2S (Easy) 75.8 60.4 2S (Hard) 76.9 62.9 4S 76.7 64.4 achieved 67.2% accuracy. This represents a 7% absolute82.5 83.3 82.783.5 88.2 81.2 86.9 84.0 88.772.3 7.1 66.7 7.5 72.7 6.574.0 76.7 71.2 75.3 71.4 75.470.5 10.8 65.6 10.3 65.9 10.7AR CL+AR improvement over the base Llama-3.1 8B model's zero-73.3 55.0 76.3 63.6 ZS 41.2 51.4 shot performance (60.2%) on BETOLD, indicating effective CoT 43.1 51.8 transfer of breakdown detection capabilities to a new dataset81.1 82.4 25.8 30.685.5 89.8 83.5 88.5 68.7 80.5 67.9 77.974.8 70.8 21.2 16.8 --41.9 17.888.0 91.7 89.0 92.4 50.8 64.4 55.1 58.978.6 80.0 20.4 25.2 --50.5 22.0GPT-3.5 Turbo and format. On DBDC5 English, the model's performance is 2S (Easy) 64.5 62.3 2S (Hard) 57.3 52.9 4S 47.5 55.0 comparable to that of significantly larger Llama variants while66.5 60.9 37.167.2 76.6 67.0 74.7 70.0 77.144.9 14.7 52.8 8.6 56.4 12.856.5 61.8 51.1 64.6 57.1 57.749.6 21.5 21.0 16.0 56.4 20.5OpenAI demonstrating better calibration scores. However, performance AR 43.0 52.2 CL+AR 42.2 51.9 on the Japanese dataset remains limited due to insufficient29.4 27.871.5 81.9 68.0 79.232.9 30.4--71.0 82.2 72.0 81.821.6 39.1--ZS CoT Japanese training data. Future research could benefit from 74.1 47.8 73.2 43.682.7 82.581.4 85.9 82.3 86.572.5 9.2 74.3 9.179.2 76.7 79.5 77.681.2 9.8 81.2 9.82S (Easy) utilizing models such as Llama-3.1 Swallow 8B, which 75.6 53.383.582.7 86.775.2 7.279.3 78.979.6 8.7GPT-4o has undergone additional training on 200 billion tokens derived 2S (Hard) 77.4 63.0 4S 77.7 62.883.7 84.183.5 87.3 82.0 86.176.3 5.1 74.6 5.880.2 77.7 79.8 79.282.1 6.4 80.4 7.5AR from the extensive Japanese web corpus (Swallow Corpus 70.4 42.980.080.5 86.366.1-87.0 91.076.4-CL+AR Version 2), as well as Japanese and English Wikipedia [44]. 70.4 44.479.881.5 86.470.9-85.0 89.474.6-ZS60.2 59.960.673.2 80.457.8 9.165.0 66.962.9 12.3CoT56.3 56.356.473.4 81.254.6 12.160.6 66.053.2 12.5Llama-3.1 8B B. Impact of Prompting Strategies2S (Easy) 2S (Hard)69.2 44.0 68.3 53.478.7 76.075.6 82.6 73.7 81.959.6 8.9 52.5 8.359.7 68.2 59.7 66.445.1 16.7 49.8 12.64S AR Few-Shot Prompting. Few-shot prompting consistently out-71.6 50.9 65.9 60.3 CL+AR 60.0 60.4 performs ZS and CoT approaches across datasets. For instance,80.0 70.1 67.176.4 83.3 64.0 75.0 66.5 78.360.1 6.9 35.7 -26.4 -59.9 66.7 69.0 78.9 67.0 79.249.7 11.7 41.5 -19.5 -ZS DeepSeek-R1 rises from 81.1% (ZS) to 83.0% (4S). The 72.7 38.582.483.0 87.474.2 6.277.9 76.878.9 7.8Meta "Hard" exemplars comprised of borderline dialogues also Llama-3.3 70B CoT 74.0 49.7 2S (Easy) 73.1 36.4 2S (Hard) 74.1 43.0 yield stronger calibration (lower MSE). 2S-Hard achieves82.5 82.9 83.381.7 86.2 81.7 85.5 82.6 86.273.0 6.0 75.3 6.2 76.4 4.076.3 76.0 76.9 77.9 77.8 77.176.7 7.7 75.8 8.7 78.4 5.84S AR substantial calibration improvements, with Llama-3.3 70B 73.2 40.4 70.4 28.6 CL+AR 72.6 43.1 reaching an MSE of 4.0 on DBDC5 English, the lowest82.7 81.3 82.081.9 85.5 84.5 88.0 85.5 89.575.8 4.7 78.0 -76.8 -78.7 78.4 84.0 88.1 77.0 83.179.0 6.7 75.8 -67.6 -ZS among all models. Similarly, GPT-4o's accuracy improves 71.2 30.181.881.5 86.371.4 6.278.7 77.080.2 6.1Llama-3.1 405B from 81.4% (ZS) to 83.5% (2S-Hard), affirming the effec-CoT 72.1 35.1 2S (Easy) 72.6 45.5 2S (Hard) 74.0 45.7 tiveness of providing challenging examples to refine model82.3 81.7 82.980.7 85.5 81.0 84.6 79.4 82.971.2 6.3 75.3 6.4 74.1 4.379.5 77.0 80.7 78.7 79.4 75.581.6 6.1 82.4 7.1 82.2 4.64S AR uncertainty estimations more effectively than simpler or even 75.2 51.0 65.2 29.9 CL+AR 58.5 22.2 more examples.83.4 76.8 71.479.6 82.9 77.0 82.8 79.0 84.674.8 5.6 65.2 -67.2 -81.0 78.4 72.0 78.1 82.0 87.183.1 5.8 61.1 -70.0 -Disruption Monitor 8B -67.2 59.772.381.5 86.272.0 4.967.9 68.866.9 8.8ZS70.5 42.480.258.4 56.160.5 11.962.5 37.473.2 13.5CoT67.0 21.579.157.9 54.660.8 13.264.4 40.474.6 13.92S (Easy)69.7 30.080.764.9 65.664.2 10.367.4 54.374.7 11.9Mixtral 8x7B2S (Hard)71.5 50.879.968.0 69.566.4 6.366.8 52.274.5 10.14S71.2 43.080.868.1 69.966.0 8.368.6 55.075.9 11.4AR65.2 43.675.158.5 66.944.3-60.0 69.741.2-MistralCL+AR65.9 44.479.862.5 70.149.7-62.0 68.951.3-ZS70.7 28.581.681.8 85.974.7 8.974.3 64.879.8 14.3CoT70.8 33.481.380.9 85.472.5 9.073.7 64.079.3 14.72S (Easy)73.8 55.881.483.5 87.476.0 6.778.9 75.881.3 9.9Mixtral 8x22B2S (Hard)73.2 61.779.481.1 84.974.5 4.275.5 69.979.4 6.64S75.2 60.482.081.9 85.775.5 5.976.2 71.079.8 9.5AR68.1 31.779.281.0 86.866.1-79.0 84.268.7-CL+AR63.7 47.372.375.5 84.047.3-83.0 88.369.1-ZS73.8 55.981.381.1 86.469.1 6.774.5 76.771.8 8.8CoT74.9 57.582.280.4 86.067.6 6.676.6 78.174.9 7.62S (Easy)75.8 57.883.082.3 87.072.5 6.472.7 75.669.0 10.1DeepSeekDeepSeek-R12S (Hard)76.4 65.482.182.0 86.971.4 4.472.5 76.167.7 6.94S75.8 61.782.383.0 87.274.5 4.574.7 77.171.8 7.7AR71.1 60.677.280.0 86.263.6-85.0 90.268.1-CL+AR75.6 66.780.780.0 86.561.5-87.0 91.473.5-</p>
<p>TABLE III REPRESENTATIVE
III
AWS BEDROCK PRICES FOR LLAMA-3.1 (MAY 2025).Each turn contributes ≈ 40 tokens in the user prompt and another 40 tokens in the reply, for 80 new tokens per turn.Because the entire history is sent at every step, turn i carries 80 i tokens.Over 15 turns:
ModelInput (per 1k) Output (per 1k)Llama-3.1 8B$0.00022$0.00022Llama-3.1 70B$0.00072$0.00072Llama-3.1 405B$0.00240$0.00240A. Token Budget for 15 TurnsTotal tokens = 80
https://huggingface.co/aghassel/dialogue disruption monitor
https://aws.amazon.com/bedrock/pricing/</p>
<p>Openai, 10.48550/arXiv.2303.08774abs/2303.08774GPT-4 technical report. 2023</p>
<p>Anthropic, The Claude 3 Model Family: Opus, Sonnet, Haiku. Claude 3 Model Card</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, 10.48550/arXiv.2302.13971abs/2302.13971CoRR. 2023</p>
<p>Chatgpt, a friend or a foe?. M A Yatoo, F Habib, MRS Bulletin. 2023</p>
<p>Chatbots and chatgpt: A bibliometric analysis and systematic review of publications in web of science and scopus databases. H Khosravi, M Shafie, M Hajiabadi, A S Raihan, I Ahmed, Int. J. Data Min. Model. Manag. 162023</p>
<p>A conversation with chatgpt. G Currie, The Journal of Nuclear Medicine Technology. 512023</p>
<p>The use of chatbot and its impact on academic achievement. A Gabriella, A Gui, R C Chanda, 2024 IEEE Symposium on Industrial Electronics &amp; Applications (ISIEA). 2024</p>
<p>Chatgpt for marketing communications: Friend or foe. H Arviani, R Tutiasri, L A Fauzan, A Kusuma, Kanal: Jurnal Ilmu Komunikasi. 2023</p>
<p>Are large language models general-purpose solvers for dialogue breakdown detection? an empirical investigation. A Ghassel, X Zhu, S W Thomas, IEEE Canadian Conference on Electrical and Computer Engineering. 2024. 2024</p>
<p>Building multi-turn query interpreters for e-commercial chatbots with sparse-to-dense attentive modeling. Y Fan, C Wang, P He, Y Hu, Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. the Fifteenth ACM International Conference on Web Search and Data Mining2022</p>
<p>Recovering from Dialogue Failures Using Multiple Agents in Wealth Management Advice. H Candello, C Pinhanez, 10.1007/978-3-319-95579-7_7Studies in Conversational UX Design. ChamSpringer International Publishing2018</p>
<p>The dialogue breakdown detection challenge: Task description, datasets, and evaluation metrics. R Higashinaka, K Funakoshi, Y Kobayashi, M Inaba, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož, SloveniaEuropean Language Resources Association (ELRA)may 2016</p>
<p>Resilient chatbots: Repair strategy preferences for conversational breakdowns. Z Ashktorab, M Jain, Q V Liao, J D Weisz, 10.1145/3290605.3300484Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI '19. the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI '19New York, NY, USAAssociation for Computing Machinery2019</p>
<p>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Larger and more instructable language models become less reliable. L Zhou, W Schellaert, F Martínez-Plumed, Y Moros-Daval, C Ferri, J Hernández-Orallo, Nature. 6348032oct 2024Nature Publishing Group</p>
<p>Evaluating dialogue breakdown detection in chat-oriented dialogue systems. Y Tsunomori, R Higashinaka, T Takahashi, M Inaba, Proceedings of the 22nd Workshop on the Semantics and Pragmatics of Dialogue -Full Papers. the 22nd Workshop on the Semantics and Pragmatics of Dialogue -Full PapersAix-en-Provence, FranceSEMDIALnov 2018</p>
<p>Leveraging large language models for automated dialogue analysis. S E Finch, E S Paek, J D Choi, Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 24th Annual Meeting of the Special Interest Group on Discourse and DialoguePrague, CzechiaAssociation for Computational Linguisticssep 2023</p>
<p>Don't forget your ABC's: Evaluating the state-of-the-art in chat-oriented dialogue systems. S E Finch, J D Finch, J D Choi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguisticsjul 20231</p>
<p>BETOLD: A task-oriented dialog dataset for breakdown detection. S Terragni, B Guedes, A Manso, M Filipavicius, N Khau, R Mathis, Proceedings of the Second Workshop on When Creative AI Meets Conversational AI. the Second Workshop on When Creative AI Meets Conversational AIGyeongju, Republic of KoreaAssociation for Computational Linguisticsoct 2022</p>
<p>Mixtral of experts. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D De Las Casas, E B Hanna, F Bressand, 10.48550/arXiv.2401.04088abs/2401.04088CoRR. 2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 10.48550/arXiv.2501.129482501.12948, 2025</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Palm: scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, J. Mach. Learn. Res. 241jan 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, ser. NIPS '22. the 36th International Conference on Neural Information Processing Systems, ser. NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Large language models as analogical reasoners. M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, E H Chi, D Zhou, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaMay 7-11, 2024. OpenReview.net, 2024</p>
<p>Curriculum learning for natural language understanding. B Xu, L Zhang, Z Mao, Q Wang, H Xie, Y Zhang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguisticsjul 2020</p>
<p>Curricullm: Automatic task curricula design for learning complex robot skills using large language models. K Ryu, Q Liao, Z Li, K Sreenath, N Mehr, 10.48550/arXiv.2409.18382abs/2409.18382CoRR. 2024</p>
<p>The Conversational Interface: Talking to Smart Devices. M Mctear, Z Callejas, D Griol, 2016Springer Publishing Company1st ed. Incorporated</p>
<p>Towards a general, continuous model of turn-taking in spoken dialogue using LSTM recurrent neural networks. G Skantze, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. the 18th Annual SIGdial Meeting on Discourse and DialogueSaarbrücken, GermanyAssociation for Computational Linguisticsaug 2017</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17. the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>Improving dialogue breakdown detection with semi-supervised learning. N Ng, M Ghassemi, N Thangarajan, J Pan, Q Guo, 2020NeurIPS Workshop on Human in the Loop Dialogue Systems</p>
<p>SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness. N Ng, K Cho, M Ghassemi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguisticsnov 2020Online</p>
<p>A semi-supervised learning approach with two teachers to improve breakdown identification in dialogues. Q Lin, H T Ng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20221119</p>
<p>Roberta: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, CoRR. 1907.11692. 2019</p>
<p>Unsupervised cross-lingual representation learning at scale. A Conneau, K Khandelwal, N Goyal, V Chaudhary, G Wenzek, F Guzmán, E Grave, M Ott, L Zettlemoyer, V Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguisticsjul 2020</p>
<p>Model card and evaluations for claude models. Anthropic, 2023</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 79Sep. 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235213</p>
<p>. Online, </p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. OpenReview.net, 2023</p>
<p>A mathematical theory of communication. C E Shannon, The Bell System Technical Journal. 271948</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, P Shen, Z Wallis, Y Allen-Zhu, S Li, L Wang, W Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, International Conference on Learning Representations. 2019</p>
<p>Continual pre-training for crosslingual llm adaptation: Enhancing japanese language capabilities. K Fujii, T Nakamura, M Loem, H Iida, M Ohi, K Hattori, H Shota, S Mizuki, R Yokota, N Okazaki, Proceedings of the First Conference on Language Modeling, ser. the First Conference on Language Modeling, serOct. 2024COLM, University of Pennsylvania, USA</p>
<p>Frontier language models have become much smaller. E Erdil, </p>
<p>Build Generative AI Applications with Foundation Models -Amazon Bedrock Pricing. Web Amazon, Services, </p>
<p>AI's Power Demand: Calculating ChatGPT's electricity consumption for handling over 365 billion user queries every year. P Hoffman, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
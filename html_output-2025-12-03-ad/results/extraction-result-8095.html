<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8095 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8095</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8095</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-4084a7605b87306b5d688d215b4fbfb4e55dd8b3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4084a7605b87306b5d688d215b4fbfb4e55dd8b3" target="_blank">Results of WMT22 Metrics Shared Task: Stop Using BLEU – Neural Metrics Are Better and More Robust</a></p>
                <p><strong>Paper Venue:</strong> Conference on Machine Translation</p>
                <p><strong>Paper TL;DR:</strong> The results demonstrate the superiority of neural-based learned metrics and demonstrate again that overlap metrics like Bleu, spBleu or chrf correlate poorly with human ratings, and reveal that neural- based metrics are remarkably robust across different domains and challenges.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8095.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8095.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural-finetuned metrics vs MQM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural fine-tuned automatic MT evaluation metrics compared to expert MQM human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of the paper's core comparison showing that neural, fine-tuned learned metrics (e.g., COMET, BLEURT, MetricX variants) correlate substantially better with expert MQM human judgements than lexical/overlap metrics (BLEU, CHRF), across system- and segment-level evaluations and multiple domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Machine translation evaluation (scoring/ranking MT outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WMT22 Metrics Shared Task testsets (newstest2022; language pairs en→de, en→ru, zh→en; domains: news, social, e-commerce, conversation)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various neural fine-tuned metrics (COMET-20/22, BLEURT-20, MetricX XL/XXL, MS-COMET, MATESE, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Learned neural metrics fine-tuned on human judgements (DA and/or MQM) or synthetic data; backbones include XLM-R, RemBERT, mT5 (MetricX XXL uses a 30B mT5 fine-tuned on multiple human-feedback tasks), and ensembles of such models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert MQM annotators: professional translators or trusted trained raters (Google: 11 professional translators with document context; Unbabel: 4 professional native annotators for en→ru), single rater per segment with MQM error-span annotation and severity weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>System-level pairwise accuracy; Pearson correlation; Kendall Tau (various tasks averaged into weighted average rank over 201 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Overlap/lexical metrics (BLEU/chrF/spBLEU) correlate poorly; some metrics lose discrimination at system-level when few systems are available (large significance clusters); correlations vary across domains but neural metrics are more robust; correlations often drop when human references are included as candidate systems (general observation).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Neural fine-tuned metrics strongly outperform lexical/embedding-only metrics in correlation with expert MQM across languages and domains; top-performing prior metrics remain strong but are outperformed by ensembles or much larger-model-based metrics; MQM (expert) ratings align better with these neural metrics than DA (crowd) ratings do.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Reported benefits of neural learned metrics: higher correlation with expert MQM, robustness across domains, ability to capture error types in challenge sets better than lexical metrics, and improved system ranking fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Metrics evaluated on system-level and segment-level against MQM ground truth (300–505 MQM-annotated segments per domain/language, per Table 3); aggregation into 201 weighted tasks (combining Pearson/Kendall/accuracy, domain, level, include-human); per-task statistical significance via PERM-BOTH (Deutsch et al., 2021) with 1000 resamples, p=0.05; ranking by weighted average rank across tasks (Kemeny-consensus style).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8095.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8095.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetricX XXL vs human judgements</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetricX XXL (mT5 XXL-based metric) compared to human MQM and WMT human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MetricX XXL, a massive multitask metric using a 30B mT5 backbone fine-tuned on DA/MQM/QE/NLI/Summarization Eval data, is reported as top-performing and shows high agreement with expert MQM and DA+SQM human judgements, but shows much lower agreement with reference-based crowd DA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Machine translation evaluation / metric correlation with human judgements</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WMT22 Metrics Shared Task (MQM annotated subset) and WMT General MT human evaluation (DA+SQM and ref. DA subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>MetricX XXL (mT5 XXL fine-tuned, MetricX family)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Massive multi-task metric: fine-tuned mT5 XXL (~30B parameters) trained on diverse human-feedback signals (DA, MQM, QE, NLI, summarization eval); primary submission uses MQM output from fine-tuned 30B mT5.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert MQM annotators for MQM subsets (professionals); WMT General MT human evaluation: DA+SQM uses professional bilingual raters for many directions, ref. DA uses non-professional crowd raters for into-English directions.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>System-level pairwise accuracy (metric vs human rankings)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.865</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Marked drop in agreement when using crowdsourced reference-based DA: MetricX XXL accuracy falls (example: 0.862 with DA+SQM vs 0.620 with ref. DA in Table 8), implying sensitivity of agreement to human evaluation protocol and rater expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MetricX XXL ranks best under MQM and DA+SQM ground truth; however, when compared to crowdsourced reference-based DA the ranking and agreement change substantially (n-gram metrics become more favored), demonstrating that human evaluation protocol and rater quality critically affect metric vs human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High system-level accuracy and strong segment-level correlations with expert MQM; more robust cross-domain performance compared to traditional lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluated across 201 weighted tasks; system-level pairwise accuracy reported for MQM and for WMT DA+SQM/ref. DA splits (Table 8); significance testing via PERM-BOTH; note that ref. DA used crowd workers with extensive QC (63% annotations removed for quality control).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8095.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8095.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET-22 vs MQM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET-22 (ensemble COMET metric) compared to expert MQM human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>COMET-22, an ensemble metric (estimator + multitask MQM predictor) built on XLM-R, shows strong agreement with MQM and is among top-performing learned metrics, and it is one of the metrics that maintain correlation when human translations are included.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Machine translation evaluation (agreement with MQM human judgements and WMT human evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WMT22 MQM-annotated subsets (en→de, en→ru, zh→en) and WMT22 general human evaluation (DA+SQM / ref. DA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>COMET-22</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Ensemble of two COMET-style models: (1) COMET estimator trained on Direct Assessments, and (2) a multitask model trained to predict sentence-level MQM plus OK/BAD token tags; backbone XLM-R large.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert MQM annotators for MQM; WMT DA+SQM crowd/professional mixture depending on direction.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>System-level pairwise accuracy (with MQM ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.839</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>General observation that many metrics show reduced correlation when human references are included among candidates; COMET-22 is noted as one of the exceptions that retains correlation in that condition.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>COMET-22 demonstrates high agreement with expert MQM and good robustness across domains; it is among the few metrics that continue to correlate reasonably well when human translations are included in candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong correlation with expert MQM, robustness across domains, and relative stability when human references are included.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluated on MQM-annotated segments (single rater per segment, MQM spans with severity weights); ranked via 201 weighted tasks including Pearson/Kendall/accuracy; per-task significance via PERM-BOTH (1000 resamples, p=0.05).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8095.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8095.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ref. DA vs DA+SQM (human evaluation protocols)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based Direct Assessment (ref. DA, crowd) compared to DA+SQM (source-based, usually professional) and to expert MQM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper highlights strong, systematic differences between crowdsourced reference-based Direct Assessment (ref. DA) and professional/ source-based DA+SQM or expert MQM: ref. DA produces contradictory metric rankings (favoring n-gram overlap metrics) and segment-level correlations near zero, indicating severe limitations when used with non-professional raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Meta-evaluation of automatic metrics vs different human evaluation protocols (ref. DA crowdsourced vs DA+SQM vs MQM)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WMT22 General MT human evaluation (ref. DA for into-English; DA+SQM for many non-English directions), compared against MQM subset</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various automatic metrics (BLEU, CHRF, BERTScore, BLEURT, COMET, MetricX, etc.) treated as 'judges' compared to different human protocols</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>See individual metric descriptions (lexical, embedding, and fine-tuned learned metrics) as used throughout the shared task.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>ref. DA: non-professional crowd workers (into-English directions) with heavy QC (63% of annotations removed); DA+SQM: bilingual/professional raters for from-English and non-English directions; MQM: expert professional annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>System-level pairwise accuracy; segment-level Kendall-like correlations</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>ref. DA (crowd, reference-based) yields rankings that contradict MQM/DA+SQM (e.g., n-gram metrics ranked top under ref. DA but bottom under MQM), segment-level correlations with ref. DA are near zero and considered meaningless; large fraction of annotations removed by QC indicates low reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The authors advise against using reference-based DA with non-professional crowd workers to evaluate metrics, because it biases raters toward surface-form matching and yields inconsistent metric rankings; DA+SQM and MQM (expert) produce broadly consistent rankings favoring neural learned metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not applicable (this item documents a failure mode of a human-evaluation protocol rather than advantages of LLM judges).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparison performed by computing system-level pairwise accuracy and per-language correlations using both DA+SQM and ref. DA human judgements (Table 8, Table 12); the authors report that ref. DA caused n-gram metrics (BLEU, CHRF, spBLEU) to appear best, in direct contradiction to MQM/DA+SQM results; segment-level Kendal-like correlations vs ref. DA were near zero.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8095.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8095.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Effect of including human translations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of including human reference translations among candidate systems on metric–human agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that when human translations are included among the candidate pool (i.e., metrics must score human references too), most metrics show lower correlation with MQM human ratings, with a few exceptions (COMET-22, MATESE) that maintain or improve correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Metric evaluation when candidate pool includes human references (simulates high-quality outputs approaching human level)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WMT22 MQM-annotated subsets (with and without human translations included in candidate pools)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various automatic metrics (COMET-22, MetricX XXL, MATESE, BLEURT, lexical metrics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>See per-metric descriptions (fine-tuned learned metrics, reference-free QE metrics, embedding and lexical baselines); behavior reported across the 201-task evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert MQM annotators (Google/Unbabel) used as ground truth; WMT DA variants also compared</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Weighted average rank over tasks and per-task Pearson/Kendall correlations; observed changes reported qualitatively and in Figure 4 / Table 10</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Most metrics show reduced correlation when human translations are included among candidates; this suggests metrics may struggle to discriminate top-quality (human-level) translations from strong MT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Including human references is a stress test revealing metric limitations at the high end of quality; COMET-22 and MATESE were exceptions, showing stable or improved correlation under this condition.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted advantage for some learned metrics (COMET-22, MATESE) is relative stability when human translations are included, indicating better calibration near human-quality outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluations performed under two include-human conditions (include-human=true/false) for en→de and zh→en (en→ru had single reference so only include-human=false); task weighting adjusted accordingly; per-task correlations recomputed with and without human translations to assess impact (results shown in Figure 4 and discussed in results sections).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Experts, errors, and context: A large-scale study of human evaluation for machine translation <em>(Rating: 2)</em></li>
                <li>COMET: A neural framework for MT evaluation <em>(Rating: 2)</em></li>
                <li>BLEURT: Learning robust metrics for text generation <em>(Rating: 2)</em></li>
                <li>To ship or not to ship: An extensive evaluation of automatic metrics for machine translation <em>(Rating: 2)</em></li>
                <li>High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics <em>(Rating: 1)</em></li>
                <li>Tangled up in bleu: Reevaluating the evaluation of automatic machine translation evaluation metrics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8095",
    "paper_id": "paper-4084a7605b87306b5d688d215b4fbfb4e55dd8b3",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Neural-finetuned metrics vs MQM",
            "name_full": "Neural fine-tuned automatic MT evaluation metrics compared to expert MQM human evaluation",
            "brief_description": "Summary of the paper's core comparison showing that neural, fine-tuned learned metrics (e.g., COMET, BLEURT, MetricX variants) correlate substantially better with expert MQM human judgements than lexical/overlap metrics (BLEU, CHRF), across system- and segment-level evaluations and multiple domains.",
            "citation_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "mention_or_use": "use",
            "paper_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "evaluation_task": "Machine translation evaluation (scoring/ranking MT outputs)",
            "dataset_name": "WMT22 Metrics Shared Task testsets (newstest2022; language pairs en→de, en→ru, zh→en; domains: news, social, e-commerce, conversation)",
            "judge_model_name": "Various neural fine-tuned metrics (COMET-20/22, BLEURT-20, MetricX XL/XXL, MS-COMET, MATESE, etc.)",
            "judge_model_details": "Learned neural metrics fine-tuned on human judgements (DA and/or MQM) or synthetic data; backbones include XLM-R, RemBERT, mT5 (MetricX XXL uses a 30B mT5 fine-tuned on multiple human-feedback tasks), and ensembles of such models.",
            "human_evaluator_type": "Expert MQM annotators: professional translators or trusted trained raters (Google: 11 professional translators with document context; Unbabel: 4 professional native annotators for en→ru), single rater per segment with MQM error-span annotation and severity weighting.",
            "agreement_metric": "System-level pairwise accuracy; Pearson correlation; Kendall Tau (various tasks averaged into weighted average rank over 201 tasks)",
            "agreement_score": null,
            "reported_loss_aspects": "Overlap/lexical metrics (BLEU/chrF/spBLEU) correlate poorly; some metrics lose discrimination at system-level when few systems are available (large significance clusters); correlations vary across domains but neural metrics are more robust; correlations often drop when human references are included as candidate systems (general observation).",
            "qualitative_findings": "Neural fine-tuned metrics strongly outperform lexical/embedding-only metrics in correlation with expert MQM across languages and domains; top-performing prior metrics remain strong but are outperformed by ensembles or much larger-model-based metrics; MQM (expert) ratings align better with these neural metrics than DA (crowd) ratings do.",
            "advantages_of_llm_judge": "Reported benefits of neural learned metrics: higher correlation with expert MQM, robustness across domains, ability to capture error types in challenge sets better than lexical metrics, and improved system ranking fidelity.",
            "experimental_setting": "Metrics evaluated on system-level and segment-level against MQM ground truth (300–505 MQM-annotated segments per domain/language, per Table 3); aggregation into 201 weighted tasks (combining Pearson/Kendall/accuracy, domain, level, include-human); per-task statistical significance via PERM-BOTH (Deutsch et al., 2021) with 1000 resamples, p=0.05; ranking by weighted average rank across tasks (Kemeny-consensus style).",
            "uuid": "e8095.0"
        },
        {
            "name_short": "MetricX XXL vs human judgements",
            "name_full": "MetricX XXL (mT5 XXL-based metric) compared to human MQM and WMT human evaluations",
            "brief_description": "MetricX XXL, a massive multitask metric using a 30B mT5 backbone fine-tuned on DA/MQM/QE/NLI/Summarization Eval data, is reported as top-performing and shows high agreement with expert MQM and DA+SQM human judgements, but shows much lower agreement with reference-based crowd DA.",
            "citation_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "mention_or_use": "use",
            "paper_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "evaluation_task": "Machine translation evaluation / metric correlation with human judgements",
            "dataset_name": "WMT22 Metrics Shared Task (MQM annotated subset) and WMT General MT human evaluation (DA+SQM and ref. DA subsets)",
            "judge_model_name": "MetricX XXL (mT5 XXL fine-tuned, MetricX family)",
            "judge_model_details": "Massive multi-task metric: fine-tuned mT5 XXL (~30B parameters) trained on diverse human-feedback signals (DA, MQM, QE, NLI, summarization eval); primary submission uses MQM output from fine-tuned 30B mT5.",
            "human_evaluator_type": "Expert MQM annotators for MQM subsets (professionals); WMT General MT human evaluation: DA+SQM uses professional bilingual raters for many directions, ref. DA uses non-professional crowd raters for into-English directions.",
            "agreement_metric": "System-level pairwise accuracy (metric vs human rankings)",
            "agreement_score": 0.865,
            "reported_loss_aspects": "Marked drop in agreement when using crowdsourced reference-based DA: MetricX XXL accuracy falls (example: 0.862 with DA+SQM vs 0.620 with ref. DA in Table 8), implying sensitivity of agreement to human evaluation protocol and rater expertise.",
            "qualitative_findings": "MetricX XXL ranks best under MQM and DA+SQM ground truth; however, when compared to crowdsourced reference-based DA the ranking and agreement change substantially (n-gram metrics become more favored), demonstrating that human evaluation protocol and rater quality critically affect metric vs human agreement.",
            "advantages_of_llm_judge": "High system-level accuracy and strong segment-level correlations with expert MQM; more robust cross-domain performance compared to traditional lexical metrics.",
            "experimental_setting": "Evaluated across 201 weighted tasks; system-level pairwise accuracy reported for MQM and for WMT DA+SQM/ref. DA splits (Table 8); significance testing via PERM-BOTH; note that ref. DA used crowd workers with extensive QC (63% annotations removed for quality control).",
            "uuid": "e8095.1"
        },
        {
            "name_short": "COMET-22 vs MQM",
            "name_full": "COMET-22 (ensemble COMET metric) compared to expert MQM human evaluation",
            "brief_description": "COMET-22, an ensemble metric (estimator + multitask MQM predictor) built on XLM-R, shows strong agreement with MQM and is among top-performing learned metrics, and it is one of the metrics that maintain correlation when human translations are included.",
            "citation_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "mention_or_use": "use",
            "paper_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "evaluation_task": "Machine translation evaluation (agreement with MQM human judgements and WMT human evaluations)",
            "dataset_name": "WMT22 MQM-annotated subsets (en→de, en→ru, zh→en) and WMT22 general human evaluation (DA+SQM / ref. DA)",
            "judge_model_name": "COMET-22",
            "judge_model_details": "Ensemble of two COMET-style models: (1) COMET estimator trained on Direct Assessments, and (2) a multitask model trained to predict sentence-level MQM plus OK/BAD token tags; backbone XLM-R large.",
            "human_evaluator_type": "Expert MQM annotators for MQM; WMT DA+SQM crowd/professional mixture depending on direction.",
            "agreement_metric": "System-level pairwise accuracy (with MQM ground truth)",
            "agreement_score": 0.839,
            "reported_loss_aspects": "General observation that many metrics show reduced correlation when human references are included among candidates; COMET-22 is noted as one of the exceptions that retains correlation in that condition.",
            "qualitative_findings": "COMET-22 demonstrates high agreement with expert MQM and good robustness across domains; it is among the few metrics that continue to correlate reasonably well when human translations are included in candidate pools.",
            "advantages_of_llm_judge": "Strong correlation with expert MQM, robustness across domains, and relative stability when human references are included.",
            "experimental_setting": "Evaluated on MQM-annotated segments (single rater per segment, MQM spans with severity weights); ranked via 201 weighted tasks including Pearson/Kendall/accuracy; per-task significance via PERM-BOTH (1000 resamples, p=0.05).",
            "uuid": "e8095.2"
        },
        {
            "name_short": "ref. DA vs DA+SQM (human evaluation protocols)",
            "name_full": "Reference-based Direct Assessment (ref. DA, crowd) compared to DA+SQM (source-based, usually professional) and to expert MQM",
            "brief_description": "The paper highlights strong, systematic differences between crowdsourced reference-based Direct Assessment (ref. DA) and professional/ source-based DA+SQM or expert MQM: ref. DA produces contradictory metric rankings (favoring n-gram overlap metrics) and segment-level correlations near zero, indicating severe limitations when used with non-professional raters.",
            "citation_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "mention_or_use": "use",
            "paper_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "evaluation_task": "Meta-evaluation of automatic metrics vs different human evaluation protocols (ref. DA crowdsourced vs DA+SQM vs MQM)",
            "dataset_name": "WMT22 General MT human evaluation (ref. DA for into-English; DA+SQM for many non-English directions), compared against MQM subset",
            "judge_model_name": "Various automatic metrics (BLEU, CHRF, BERTScore, BLEURT, COMET, MetricX, etc.) treated as 'judges' compared to different human protocols",
            "judge_model_details": "See individual metric descriptions (lexical, embedding, and fine-tuned learned metrics) as used throughout the shared task.",
            "human_evaluator_type": "ref. DA: non-professional crowd workers (into-English directions) with heavy QC (63% of annotations removed); DA+SQM: bilingual/professional raters for from-English and non-English directions; MQM: expert professional annotators.",
            "agreement_metric": "System-level pairwise accuracy; segment-level Kendall-like correlations",
            "agreement_score": null,
            "reported_loss_aspects": "ref. DA (crowd, reference-based) yields rankings that contradict MQM/DA+SQM (e.g., n-gram metrics ranked top under ref. DA but bottom under MQM), segment-level correlations with ref. DA are near zero and considered meaningless; large fraction of annotations removed by QC indicates low reliability.",
            "qualitative_findings": "The authors advise against using reference-based DA with non-professional crowd workers to evaluate metrics, because it biases raters toward surface-form matching and yields inconsistent metric rankings; DA+SQM and MQM (expert) produce broadly consistent rankings favoring neural learned metrics.",
            "advantages_of_llm_judge": "Not applicable (this item documents a failure mode of a human-evaluation protocol rather than advantages of LLM judges).",
            "experimental_setting": "Comparison performed by computing system-level pairwise accuracy and per-language correlations using both DA+SQM and ref. DA human judgements (Table 8, Table 12); the authors report that ref. DA caused n-gram metrics (BLEU, CHRF, spBLEU) to appear best, in direct contradiction to MQM/DA+SQM results; segment-level Kendal-like correlations vs ref. DA were near zero.",
            "uuid": "e8095.3"
        },
        {
            "name_short": "Effect of including human translations",
            "name_full": "Impact of including human reference translations among candidate systems on metric–human agreement",
            "brief_description": "The paper reports that when human translations are included among the candidate pool (i.e., metrics must score human references too), most metrics show lower correlation with MQM human ratings, with a few exceptions (COMET-22, MATESE) that maintain or improve correlation.",
            "citation_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "mention_or_use": "use",
            "paper_title": "Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust",
            "evaluation_task": "Metric evaluation when candidate pool includes human references (simulates high-quality outputs approaching human level)",
            "dataset_name": "WMT22 MQM-annotated subsets (with and without human translations included in candidate pools)",
            "judge_model_name": "Various automatic metrics (COMET-22, MetricX XXL, MATESE, BLEURT, lexical metrics, etc.)",
            "judge_model_details": "See per-metric descriptions (fine-tuned learned metrics, reference-free QE metrics, embedding and lexical baselines); behavior reported across the 201-task evaluation.",
            "human_evaluator_type": "Expert MQM annotators (Google/Unbabel) used as ground truth; WMT DA variants also compared",
            "agreement_metric": "Weighted average rank over tasks and per-task Pearson/Kendall correlations; observed changes reported qualitatively and in Figure 4 / Table 10",
            "agreement_score": null,
            "reported_loss_aspects": "Most metrics show reduced correlation when human translations are included among candidates; this suggests metrics may struggle to discriminate top-quality (human-level) translations from strong MT outputs.",
            "qualitative_findings": "Including human references is a stress test revealing metric limitations at the high end of quality; COMET-22 and MATESE were exceptions, showing stable or improved correlation under this condition.",
            "advantages_of_llm_judge": "Noted advantage for some learned metrics (COMET-22, MATESE) is relative stability when human translations are included, indicating better calibration near human-quality outputs.",
            "experimental_setting": "Evaluations performed under two include-human conditions (include-human=true/false) for en→de and zh→en (en→ru had single reference so only include-human=false); task weighting adjusted accordingly; per-task correlations recomputed with and without human translations to assess impact (results shown in Figure 4 and discussed in results sections).",
            "uuid": "e8095.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Experts, errors, and context: A large-scale study of human evaluation for machine translation",
            "rating": 2
        },
        {
            "paper_title": "COMET: A neural framework for MT evaluation",
            "rating": 2
        },
        {
            "paper_title": "BLEURT: Learning robust metrics for text generation",
            "rating": 2
        },
        {
            "paper_title": "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation",
            "rating": 2
        },
        {
            "paper_title": "High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics",
            "rating": 1
        },
        {
            "paper_title": "Tangled up in bleu: Reevaluating the evaluation of automatic machine translation evaluation metrics",
            "rating": 1
        }
    ],
    "cost": 0.02204625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Results of WMT22 Metrics Shared Task: Stop Using BLEU - Neural Metrics Are Better and More Robust</h1>
<p>Markus Freitag ${ }^{(1)}$, Ricardo Rei ${ }^{(2,3,4)}$, Nitika Mathur ${ }^{(5)}$, Chi-kiu Lo ${ }^{(6)}$, Craig Stewart ${ }^{(2)}$, Eleftherios Avramidis ${ }^{(8)}$, Tom Kocmi ${ }^{(7)}$, George Foster ${ }^{(1)}$, Alon Lavie ${ }^{(2)}$ and André F. T. Martins ${ }^{(2,3,9)}$<br>${ }^{(1)}$ Google Research ${ }^{(2)}$ Unbabel ${ }^{(3)}$ INESC-ID ${ }^{(4)}$ Instituto Superior Técnico<br>${ }^{(5)}$ Oracle Digital Assistant ${ }^{(6)}$ National Research Council Canada ${ }^{(7)}$ Microsoft<br>${ }^{(8)}$ German Research Center for Artificial Intelligence (DFKI) ${ }^{(9)}$ Instituto de Telecomunicações wmt22-metric@googlegroups.com</p>
<h4>Abstract</h4>
<p>This paper presents the results of the WMT22 Metrics Shared Task. Participants submitting automatic MT evaluation metrics were asked to score the outputs of the translation systems competing in the WMT22 News Translation Task on four different domains: news, social, e-commerce, and chat. All metrics were evaluated on how well they correlate with human ratings at the system and segment level. Similar to last year, we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). This setup had several advantages, among other things: (i) expert-based evaluation is more reliable, (ii) we extended the pool of translations by 5 additional translations based on MBR decoding or rescoring which are challenging for current metrics.</p>
<p>In addition, we initiated a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics' ability to capture and penalise specific types of translation errors.</p>
<p>Finally, we present an extensive analysis on how well metrics perform on three language pairs: English $\rightarrow$ German, English $\rightarrow$ Russian and Chinese $\rightarrow$ English. The results demonstrate the superiority of neural-based learned metrics and demonstrate again that overlap metrics like Bleu, SPBleu or CHRF correlate poorly with human ratings. The results also reveal that neural-based metrics are significant better than non-neural metrics across different domains and challenges.</p>
<h2>1 Introduction</h2>
<p>The metrics shared task ${ }^{1}$ has been a key component of WMT since 2008, serving as a way to validate the use of automatic MT evaluation metrics and drive the development of new metrics. We evaluate reference-based automatic metrics that score MT output by comparing the translations with a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>reference translation generated by human translators, who are instructed to translate "from scratch" without post-editing from MT. In addition, we also invited submissions of reference-free metrics (quality estimation metrics or QE metrics) that compare MT outputs directly with the source segments. All metrics are evaluated based on their agreement with human rating when scoring MT systems and human translations at the system or sentence level. The final ranking of this year's submitted primary metrics is shown in Table 1. We provide details in the remainder of the paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">avg rank</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">METRICX XXL</td>
<td style="text-align: left;">1.20</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: left;">1.32</td>
</tr>
<tr>
<td style="text-align: left;">UNITE</td>
<td style="text-align: left;">1.86</td>
</tr>
<tr>
<td style="text-align: left;">BleURT-20</td>
<td style="text-align: left;">1.91</td>
</tr>
<tr>
<td style="text-align: left;">COMET-20</td>
<td style="text-align: left;">2.36</td>
</tr>
<tr>
<td style="text-align: left;">MATESE</td>
<td style="text-align: left;">2.57</td>
</tr>
<tr>
<td style="text-align: left;">COMETKiwi*</td>
<td style="text-align: left;">2.70</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-22</td>
<td style="text-align: left;">2.84</td>
</tr>
<tr>
<td style="text-align: left;">UNITE-SRC*</td>
<td style="text-align: left;">3.03</td>
</tr>
<tr>
<td style="text-align: left;">YiSi-1</td>
<td style="text-align: left;">3.27</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE*</td>
<td style="text-align: left;">3.33</td>
</tr>
<tr>
<td style="text-align: left;">MATESE-QE*</td>
<td style="text-align: left;">3.85</td>
</tr>
<tr>
<td style="text-align: left;">MEE4</td>
<td style="text-align: left;">3.87</td>
</tr>
<tr>
<td style="text-align: left;">BERTSCORE</td>
<td style="text-align: left;">3.88</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-QE-22*</td>
<td style="text-align: left;">4.06</td>
</tr>
<tr>
<td style="text-align: left;">CHRF</td>
<td style="text-align: left;">4.70</td>
</tr>
<tr>
<td style="text-align: left;">F101SPBLEU</td>
<td style="text-align: left;">4.97</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-TEACHER-SIM*</td>
<td style="text-align: left;">5.17</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: left;">5.31</td>
</tr>
<tr>
<td style="text-align: left;">REUSE*</td>
<td style="text-align: left;">6.69</td>
</tr>
</tbody>
</table>
<p>Table 1: Official ranking of all primary submissions of the WMT22 Metric Task. The final score is the weighted average ranking over 201 different scenarios. Metrics with * are reference-free metrics.</p>
<p>We implemented several changes to the methodology that was followed in previous years' editions:</p>
<ul>
<li>Expert-based human evaluation: Like last year, we collected our own human ratings for select language pairs (en $\rightarrow \mathrm{de}, \mathrm{en} \rightarrow \mathrm{ru}, \mathrm{zh} \rightarrow \mathrm{en}$ ) from professional translators via MQM (Lommel et al.,</li>
</ul>
<p>2014). Freitag et al. (2021a) showed that expertbased MQM evaluations produce more reliable ${ }^{2}$ scores when compared to the DA-based human ratings acquired by the WMT Translation task. This step was necessary as Freitag et al. (2021a) showed that the DA-based ground-truth is already of lower quality than some of our submissions (Section 3).</p>
<ul>
<li>Additional Training Data: We encouraged the participants to make use of existing MQM annotations for newstest2020 (Freitag et al., 2021a) ${ }^{3}$, and the MQM annotations from the WMT21 Metrics Task (Freitag et al., 2021b) to improve and/or test their metrics.</li>
<li>Additional MT systems: The primary use case for automatic metrics is guiding research to translations that are better than what we can generate right now. To address this scenario, we not only want to evaluate metrics on MT output that we are currently capable of generating, but also on translations that are better than the current WMT submissions. For that we need to add alternative translations that cover a wider space of possible translations. To address this, we added MT systems that were generated with MBR decoding or reranking (Section 2.2).</li>
<li>Challenge sets subtask: In the main metrics task, the metrics are evaluated on MT systems translating test sets drawn from large sources of continuous text. In an effort to have a more fine-grained analysis on the strengths and weaknesses of the metrics, we introduced the concept of challenge sets. A challenge set consists of contrasting MT outputs, which have been deliberately devised or selected to include correct and incorrect translations of particular phenomena, along with their respective reference translation. The evaluation of every metric in this setup depends on its ability to rank the correct translations higher than their corresponding incorrect ones. Whereas a first version of challenge sets appeared in last year's metrics shared task (Freitag et al., 2021b), this year they appear for the first time as a subtask in a decentralized manner. Inspired by the Build it or</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>break it: The Language Edition shared task (Ettinger et al., 2017), participants (the Breakers) had to submit their own test suites to test the robustness of MT metrics to particular phenomena that they choose. Our first edition of this subtask (Section 8) received four challenge set submissions covering a wide range of phenomena and languages.</p>
<ul>
<li>Meta Evaluation: A main aim of the metrics task is to rank the overall performance of various metrics. This requires some way of aggregating scores across different settings (language pair, domain, granularity etc.), in order to provide a balanced picture. Correlations with human scores have different ranges in different settings, so averaging them is not a good solution. Last year, we adopted a proposal by Kocmi et al. (2021) that involves taking the microaverage of a metric's accuracy in making pairwise system-ranking decisions across different settings. This is easy to interpret and reflects a common use-case for metrics, but because we have only three language pairs, and thus relatively few pairwise comparisons, it tends to place many metrics into large significance clusters (eg, 8 metrics in the top cluster last year, including CHRF but excluding COMET). In an effort to better discriminate, and to represent a broader set of use-cases, this year we computed the average rank of each metric across a large set of tasks (Section 5). This statistic has a clear interpretation, is justified by social choice theory (Colombo et al., 2022), and makes it easy to zoom into different subsets of tasks to provide finer-grained characterizations. To reflect the importance of the accuracy metric from last year, we define it as a single highly-important task (out of 201 tasks in total), with an overall weight of $25 \%$.</li>
<li>MTME: Similar to last year, all results in this paper are calculated with MTME ${ }^{4}$. We want to encourage every metric developer to use this tool to calculate scores for consistency and comparability going forward.</li>
</ul>
<p>Our main findings are:</p>
<ul>
<li>Out of 13 reference-based metrics Bleu is ranked last, followed by F200SPBLEU and CHRF.</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<ul>
<li>Neural fine-tuned metrics are not only better, but also robust to different domains. Furthermore, based on the results from the four submitted challenge sets, neural finetuned metrics exhibit superior performance when compared to lexical and embedding similarity metrics.</li>
<li>Top performing metrics from previous years are still top-performers, being only outperformed by model ensembles or metrics based on considerably larger neural models.</li>
<li>For the first time since 2008, there was no new purely lexical metric submission, which indicates that metric developers are moving away from lexical metrics.</li>
</ul>
<p>The rest of the paper is organized as follows: Section 2 describes the additional MT systems. Section 3 presents an overview of the conducted expertbased human evaluation. Section 4 describes the metrics evaluated this year (baselines and participants). Section 5 describes the conducted metaevaluation. Section 6 reports our main results. Section 7 summarizes our results for additional WMT22 Translation task language-pairs based on their Direct Assessment human evaluation. Section 8 presents a description of the submitted challenge sets along with their findings. Finally, Section 9 presents our most relevant conclusions.</p>
<h2>2 Translation Systems</h2>
<p>Similar to the previous years' editions, the source, reference texts, and MT system outputs for the metrics task are mainly derived from the WMT22 general MT Task. In addition to the MT system outputs from the WMT evaluation campaign, we added translations from six additional MT systems which we deemed interesting for evaluation.</p>
<h3>2.1 WMT Test Sets</h3>
<p>The general MT 2022 test set contains around 2000 segments for each translation direction. This year, the test sets cover 4 domains: news, social, conversational, and e-commerce. There are around 500 sentences for each domain resulting in reasonably balanced test sets. English sources are identical for both into-German and into-Chinese translation directions. The reference translations provided for the test sets are translated by professional translators. We have two reference translations for English $\rightarrow$ German and Chinese $\rightarrow$ English
sponsored by Microsoft and one reference translation for English $\rightarrow$ Russian sponsored by Google. For more details regarding the news test sets, we refer the reader to the WMT22 General MT task findings paper (Kocmi et al., 2022a).</p>
<h3>2.2 Additional MT Output</h3>
<p>Similar to last year, we want to expand the pool of translations beyond the WMT submissions, which usually are quite similar to each other. We added translations based on M2M100 and translations generated with MBR decoding.</p>
<p>M2M100 1.2B As the field moves forward to large multilingual pre-trained models, we are interested in comparing such general-purpose large multilingual MT systems against direct submissions to the general MT task. Models such as MBART50 (Tang et al., 2021) and M2M100 (Fan et al., 2021) are publicly available, easy to use and have recently been used as baselines and/or as a backbone for new research. We tested both models on the newstest2021 and we decided to include M2M100 1.2B as an additional MT output as it yielded better automatic scores.</p>
<p>MBR Outputs Minimum Bayes Risk (MBR) decoding has recently gained attention in MT as a decision rule, with the potential to overcome some of the biases of MAP decoding in NMT (Eikema and Aziz, 2020; Müller and Sennrich, 2021; Eikema and Aziz, 2021; Freitag et al., 2022; Fernandes et al., 2022). MBR decoding centrally relies on a reference-based utility metric: its goal is to identify a hypothesis with a high estimated utility (expectation under model distribution) with the hope that a high estimated utility translates into a high actual utility (with respect to a human reference). MBR decoding is particularly interesting for referencebased metrics as it stress tests the metric, using it as a utility function.</p>
<p>This year, we added three different MBR runs using three different utility functions (BLEU, BleUrt-20, and Comet-20) as additional translations. Freitag et al. (2022) demonstrated that the translations generated with a neural-based utility (BleUrt-20, and Comet-20) generate translations that are not only better when compared to MAP decoding, but the resulting translations are also significantly different from both the beam search decoding and the MBR decoding output using Bleu as a utility function. To make it even more interesting for the metric task, for these MBR</p>
<p>translation models we used a transformer-big baseline trained only on WMT22 bilingual training data. By not using the strongest NMT system, we hope to see interesting new errors in the translation output. To generate the candidate list for MBR decoding, we sampled 256 times from the model using unbiased ancestral sampling.</p>
<p>Reranking Outputs Complementary to MBR outputs, we were also interested in comparing and evaluating the quality produced by reranking approaches based on QE. Our hope is that QE based reranking would lead to translations that are lexically different than traditional beam search output and thus lead to more diverse translations for the same source sentences. For English $\rightarrow$ German and English $\rightarrow$ Russian we used the Fairseq WMT19 systems ${ }^{5}$ ( Ng et al., 2019) with Nucleus Sampling (Holtzman et al., 2019) to generate 200 candidate translations, from which we choose the best translation according to the Tune Reranker proposed in Fernandes et al. (2022). For Chinese $\rightarrow$ English we used the same process but replacing the NMT model with MBART50 (many-to-one) and using only 50 samples.</p>
<h2>3 MQM Human Evaluation</h2>
<p>Automatic metrics are usually evaluated by measuring correlations with human ratings. The quality of the underlying human ratings is critical and recent findings (Freitag et al., 2021a) have shown that crowd-sourced human ratings are not reliable for high quality MT output. Furthermore, an evaluation schema based on MQM (Lommel et al., 2014), which requires explicit error annotation, is preferable to an evaluation schema that only asks raters for a single scalar value per translation. Similar to last year, we decided to not use the human ratings from the WMT General MT task, and conducted our own MQM-based human evaluation on a subset of submissions and a subset of language pairs that are most interesting for evaluating current metrics. This not only had the advantage of more reliable ratings for a subset of language pairs, but also gave us the opportunity to add our own translations that might be challenging for current metrics and are not part of an WMT submission.</p>
<p>MQM is a general framework that provides a hierarchy of translation errors which can be tailored to specific applications. Google and Unba-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bel sponsored the human evaluation for this year's metrics task for a subset of language pairs using either professional translators (English $\rightarrow$ German, Chinese $\rightarrow$ English) or trusted and trained raters (English $\rightarrow$ Russian). The error annotation typology and guidelines used by Google's and Unbabel's annotators differ slightly and are described in the following two sections.</p>
<h3>3.1 English $\rightarrow$ German and Chinese $\rightarrow$ English</h3>
<p>Annotations for English $\rightarrow$ German and Chinese $\rightarrow$ English were sponsored and executed by Google, using 11 professional translators (7 for English $\rightarrow$ German, 4 for Chinese $\rightarrow$ English) having access to the full document context. Each segment gets annotated by a single rater. Instead of assigning a scalar value to each translation, annotators were instructed to label error spans within each segment in a document, paying particular attention to document context. Each error was highlighted in the text, and labeled with an error category and a severity. To temper the effect of long segments, we imposed a maximum of five errors per segment, instructing raters to choose the five most severe errors for segments containing more errors. Segments that are too badly garbled to permit reliable identification of individual errors are assigned a special Non-translation error. Error severities are assigned independent of category, and consist of Major, Minor, and Neutral levels, corresponding respectively to actual translation or grammatical errors, smaller imperfections and purely subjective opinions about the translation. Since we are ultimately interested in scoring segments, we adopt the weighting scheme shown in Table 2, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. For more details, exact annotator instructions and a list of error categories, we refer the reader to Freitag et al. (2021a) as the exact same setup was used for the WMT21 metrics task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Severity</th>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Major</td>
<td style="text-align: left;">Non-translation <br> all others</td>
<td style="text-align: left;">25</td>
</tr>
<tr>
<td style="text-align: left;">Minor</td>
<td style="text-align: left;">Fluency/Punctuation <br> all others</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Neutral</td>
<td style="text-align: left;">all</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
<p>Table 2: Google's MQM error weighting.</p>
<h3>3.2 English $\rightarrow$ Russian</h3>
<p>The annotations for English $\rightarrow$ Russian were provided by Unbabel who utilized four professional, native language annotators with ample translation experience. Annotation was conducted using Unbabel's own proprietary variant of the MQM framework (Lommel et al., 2014) which is fully compliant with MQM 2.0, being the most recent iteration of the framework ${ }^{6}$. Annotation was split along the four domain boundaries with each of the annotators evaluating all of the systems for a single content type. Similarly to Google, the annotators were given the full document context (up to ten segments) and were instructed to identify (by highlighting) and classify errors in accordance with the MQM typology. Annotators were also asked to classify error severity; in addition to Minor and Major error severities used by Google, Unbabel also uses a Critical error severity. However, in the interest of maintaining consistency in evaluation, we calculated the MQM score in a manner compliant with the Google methodology outlined above. Specifically all annotated Critical errors were counted as Major and punctuation errors were weighted using the weighting scheme in Table 2.</p>
<h3>3.3 Human Evaluation Results</h3>
<p>As discussed in Section 1, we decided to run our own human evaluation in order to generate our golden-truth ratings and come to stronger conclusions about the quality of each automatic metric across all domains. However, this also meant that we were only able to evaluate a subset of the test sets. In Table 3, you can see the number of segments for each language pair and test set that we used for human evaluation. We followed a simple and consistent approach to downsample the data: we kept the first 10 sentences of each document. By doing this, we did not need to discard any documents and only needed to crop longer documents. An exception is Chinese $\rightarrow$ English where we evaluated the full test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">language</th>
<th style="text-align: left;">news</th>
<th style="text-align: left;">social</th>
<th style="text-align: left;">ecomm.</th>
<th style="text-align: left;">conv.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">en $\rightarrow$ de</td>
<td style="text-align: left;">$300 / 511$</td>
<td style="text-align: left;">$340 / 512$</td>
<td style="text-align: left;">$230 / 530$</td>
<td style="text-align: left;">$445 / 484$</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ ru</td>
<td style="text-align: left;">$300 / 511$</td>
<td style="text-align: left;">$340 / 512$</td>
<td style="text-align: left;">$230 / 530$</td>
<td style="text-align: left;">$445 / 484$</td>
</tr>
<tr>
<td style="text-align: left;">zh $\rightarrow$ en</td>
<td style="text-align: left;">$505 / 505$</td>
<td style="text-align: left;">$503 / 503$</td>
<td style="text-align: left;">$518 / 518$</td>
<td style="text-align: left;">$349 / 349$</td>
</tr>
</tbody>
</table>
<p>Table 3: Numbers of MQM-annotated segments per domain.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The results of the MQM human evaluation can be seen in Table 4. Most of the reference translations are ranked first, except for refB for English $\rightarrow$ German. Not ranking the human evaluation on top of the MT output is usually a signal for a corrupt human evaluation. We double checked the annotation for refB and can confirm that the reference translation indeed contained some errors.</p>
<h2>4 Baselines and Primary Submissions</h2>
<p>We computed scores for several baseline metrics in order to compare submissions against previous well-studied metrics. We will start by describing those baselines and then we will describe the submissions from participating teams. An overview of the evaluated metrics can be seen in Table 5.</p>
<h3>4.1 Baselines</h3>
<p>SacreBLEU baselines We use the following metrics from the SacreBLEU (Post, 2018) as baselines:</p>
<ul>
<li>Bleu (Papineni et al., 2002) is based on the precision of $n$-grams between the MT output and its reference weighted by a brevity penalty. Using SacreBLEU we obtained sentenceBleU values using the sentence_bleu Python function and for corpus-level BleU we used corpus_bleu (both with default arguments ${ }^{7}$ ).</li>
<li>F101spBleU (Goyal et al., 2022) and F200spBleU (NLLB Team et al., 2022) are BLEU scores computed with subword tokenization done by standardized Sentencepiece Models (Kudo and Richardson, 2018). We used the command line SacreBLEU to compute the sentence level F101spBleU ${ }^{8}$ and F200spBleU ${ }^{9}$ and we average those scores to obtain a corpus-level score.</li>
<li>CHRF (Popović, 2015) uses character $n$ grams instead of word $n$-grams to compare the MT output with the reference. For CHRF we used the SacreBLEU sentence_chrf function (with default arguments ${ }^{10}$ ) for segment-level scores and we average those scores to obtain a corpus-level score.</li>
</ul>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">English $\rightarrow$ German $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">news</td>
<td style="text-align: center;">social</td>
<td style="text-align: center;">ecom.</td>
<td style="text-align: center;">conv.</td>
</tr>
<tr>
<td style="text-align: center;">refA</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">Online-W</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;">refB</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">1.38</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">MBR-bleu</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">Online-B</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">JDExploreAcademy</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;">MBR-comet</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">MBR-bleurt</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">Online-A</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">Online-G</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">Online-Y</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">1.99</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;">QUARTZ</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">1.85</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;">Lan-Bridge</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;">OpenNMT</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">1.98</td>
<td style="text-align: center;">2.14</td>
<td style="text-align: center;">1.73</td>
<td style="text-align: center;">1.09</td>
</tr>
<tr>
<td style="text-align: center;">PROMT</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">1.94</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: center;">M2M100</td>
<td style="text-align: center;">2.82</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">2.99</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">2.19</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Chinese $\rightarrow$ English $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">news</td>
<td style="text-align: center;">social</td>
<td style="text-align: center;">ecom.</td>
<td style="text-align: center;">conv.</td>
</tr>
<tr>
<td style="text-align: center;">refA</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: center;">refB</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">1.83</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;">Lan-Bridge</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">2.45</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">1.39</td>
</tr>
<tr>
<td style="text-align: center;">MBR-bleurt</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">1.55</td>
</tr>
<tr>
<td style="text-align: center;">Online-B</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;">3.73</td>
<td style="text-align: center;">1.55</td>
</tr>
<tr>
<td style="text-align: center;">LanguageX</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.46</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">1.58</td>
</tr>
<tr>
<td style="text-align: center;">JDExploreAcademy</td>
<td style="text-align: center;">2.83</td>
<td style="text-align: center;">2.84</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">3.81</td>
<td style="text-align: center;">1.60</td>
</tr>
<tr>
<td style="text-align: center;">MBR-comet</td>
<td style="text-align: center;">2.87</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">1.61</td>
</tr>
<tr>
<td style="text-align: center;">Online-G</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">1.63</td>
</tr>
<tr>
<td style="text-align: center;">MBR-bleu</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">1.64</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">2.96</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">1.68</td>
</tr>
<tr>
<td style="text-align: center;">AISP-SJTU</td>
<td style="text-align: center;">3.19</td>
<td style="text-align: center;">3.08</td>
<td style="text-align: center;">2.89</td>
<td style="text-align: center;">5.03</td>
<td style="text-align: center;">1.76</td>
</tr>
<tr>
<td style="text-align: center;">Online-Y</td>
<td style="text-align: center;">3.28</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">3.03</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: center;">1.79</td>
</tr>
<tr>
<td style="text-align: center;">Online-A</td>
<td style="text-align: center;">3.73</td>
<td style="text-align: center;">3.49</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">5.39</td>
<td style="text-align: center;">2.04</td>
</tr>
<tr>
<td style="text-align: center;">Online-W</td>
<td style="text-align: center;">3.95</td>
<td style="text-align: center;">3.96</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">5.76</td>
<td style="text-align: center;">2.30</td>
</tr>
<tr>
<td style="text-align: center;">M2M100</td>
<td style="text-align: center;">6.82</td>
<td style="text-align: center;">7.47</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">3.61</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">English $\rightarrow$ Russian $\downarrow$</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">System</td>
<td style="text-align: left;">all</td>
<td style="text-align: left;">news</td>
<td style="text-align: left;">social</td>
<td style="text-align: left;">ecom.</td>
<td style="text-align: left;">conv.</td>
</tr>
<tr>
<td style="text-align: left;">refA</td>
<td style="text-align: left;">1.13</td>
<td style="text-align: left;">0.43</td>
<td style="text-align: left;">2.17</td>
<td style="text-align: left;">1.95</td>
<td style="text-align: left;">0.39</td>
</tr>
<tr>
<td style="text-align: left;">Online-W</td>
<td style="text-align: left;">1.37</td>
<td style="text-align: left;">1.35</td>
<td style="text-align: left;">2.96</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">MBR-bleu</td>
<td style="text-align: left;">1.85</td>
<td style="text-align: left;">1.57</td>
<td style="text-align: left;">4.01</td>
<td style="text-align: left;">1.39</td>
<td style="text-align: left;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">Online-B</td>
<td style="text-align: left;">1.94</td>
<td style="text-align: left;">1.59</td>
<td style="text-align: left;">4.29</td>
<td style="text-align: left;">1.37</td>
<td style="text-align: left;">0.68</td>
</tr>
<tr>
<td style="text-align: left;">Online-G</td>
<td style="text-align: left;">2.03</td>
<td style="text-align: left;">1.50</td>
<td style="text-align: left;">4.33</td>
<td style="text-align: left;">1.88</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">JDExploreAcademy</td>
<td style="text-align: left;">2.09</td>
<td style="text-align: left;">1.14</td>
<td style="text-align: left;">4.63</td>
<td style="text-align: left;">2.23</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">MBR-comet</td>
<td style="text-align: left;">2.10</td>
<td style="text-align: left;">2.01</td>
<td style="text-align: left;">4.74</td>
<td style="text-align: left;">1.26</td>
<td style="text-align: left;">0.57</td>
</tr>
<tr>
<td style="text-align: left;">Lan-Bridge</td>
<td style="text-align: left;">2.34</td>
<td style="text-align: left;">2.14</td>
<td style="text-align: left;">5.49</td>
<td style="text-align: left;">1.49</td>
<td style="text-align: left;">0.51</td>
</tr>
<tr>
<td style="text-align: left;">Online-Y</td>
<td style="text-align: left;">2.55</td>
<td style="text-align: left;">2.06</td>
<td style="text-align: left;">5.79</td>
<td style="text-align: left;">1.66</td>
<td style="text-align: left;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">Online-A</td>
<td style="text-align: left;">2.85</td>
<td style="text-align: left;">1.83</td>
<td style="text-align: left;">6.56</td>
<td style="text-align: left;">2.62</td>
<td style="text-align: left;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">PROMT</td>
<td style="text-align: left;">2.94</td>
<td style="text-align: left;">2.04</td>
<td style="text-align: left;">6.88</td>
<td style="text-align: left;">2.55</td>
<td style="text-align: left;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC</td>
<td style="text-align: left;">3.40</td>
<td style="text-align: left;">1.72</td>
<td style="text-align: left;">8.07</td>
<td style="text-align: left;">3.02</td>
<td style="text-align: left;">1.17</td>
</tr>
<tr>
<td style="text-align: left;">SRPOL</td>
<td style="text-align: left;">3.68</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">8.19</td>
<td style="text-align: left;">3.53</td>
<td style="text-align: left;">1.43</td>
</tr>
<tr>
<td style="text-align: left;">eTranslation</td>
<td style="text-align: left;">3.79</td>
<td style="text-align: left;">2.30</td>
<td style="text-align: left;">8.54</td>
<td style="text-align: left;">3.49</td>
<td style="text-align: left;">1.32</td>
</tr>
<tr>
<td style="text-align: left;">QUARTZ</td>
<td style="text-align: left;">4.06</td>
<td style="text-align: left;">3.82</td>
<td style="text-align: left;">7.02</td>
<td style="text-align: left;">5.03</td>
<td style="text-align: left;">1.46</td>
</tr>
<tr>
<td style="text-align: left;">M2M100</td>
<td style="text-align: left;">4.56</td>
<td style="text-align: left;">3.74</td>
<td style="text-align: left;">9.27</td>
<td style="text-align: left;">4.42</td>
<td style="text-align: left;">1.58</td>
</tr>
</tbody>
</table>
<p>Table 4: MQM human evaluations for generaltest2022. Lower average error counts represent higher MT quality.</p>
<p>BERTSCORE (Zhang et al., 2020) leverages contextual embeddings from pre-trained transformers to create soft-alignments between words in candidate and reference sentences using cosine similarity. Based on the alignment matrix, BERTSCORE returns a precision, recall and F1 score. We used F1 without TF-IDF weighting.</p>
<p>YiSI-1 (Lo, 2019) is a MT evaluation metric that measures the semantic similarity between a machine translation and human references by aggregating the IDF-weighted lexical semantic similarities based on the contextual embeddings extracted from pre-trained language models (e.g. RoBERTa, CamemBERT, XLM-RoBERTa, etc.).</p>
<p>BLEURT (Sellam et al., 2020) is a learned metric that is fine-tuned to produce a DA for a given translation by encoding it jointly with its reference. We used the BleUrt 20 checkpoint (Pu et al., 2021) which was trained on top of RemBERT us-
ing DA from previous shared tasks ranging 2015 to 2019 and additional synthetic data created from Wikipedia articles.</p>
<p>COMET (Rei et al., 2020) is a learnt metric that is fine-tuned to produce a z-standardized DA for a given translation by comparing its representation to source and reference embeddings. We used the default model wmt20-comet-da provided in version 1.1.2 which is trained on top of XLM-R large using data from from previous shared tasks ranging 2017 to 2019.</p>
<p>COMET-QE (Rei et al., 2021) is a referencefree learnt metric similar to COMET. We used the wmt21-comet-qe-mqm) model which was a top-performing metric from last year's shared task. This metric is first trained on z-standardized DA from 2017 to 2020 and then fine-tuned on zstandardized MQM from (Freitag et al., 2021a).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">metric</th>
<th style="text-align: center;">broad category</th>
<th style="text-align: center;">sup- <br> erv.</th>
<th style="text-align: center;">ref. <br> free</th>
<th style="text-align: center;">citation <br> n</th>
<th style="text-align: center;">availability (https://github.com/)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">baselines</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">lexical overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Papineni et al. (2002)</td>
<td style="text-align: center;">mjpost/sacrebleu</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F101SPBLEU</td>
<td style="text-align: center;">lexical overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Goyal et al. (2022)</td>
<td style="text-align: center;">mjpost/sacrebleu</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F200SPBLEU</td>
<td style="text-align: center;">lexical overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLLB Team et al. (2022)</td>
<td style="text-align: center;">mjpost/sacrebleu</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRF</td>
<td style="text-align: center;">lexical overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Popović (2015)</td>
<td style="text-align: center;">mjpost/sacrebleu</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERTSCORE</td>
<td style="text-align: center;">embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zhang et al. (2020)</td>
<td style="text-align: center;">Tiliper/bert_score</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sellam et al. (2020)</td>
<td style="text-align: center;">google-research/bleurt</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rei et al. (2020)</td>
<td style="text-align: center;">Unbabel/COMET</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET-QE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Rei et al. (2021)</td>
<td style="text-align: center;">Unbabel/COMET</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y1S1-1</td>
<td style="text-align: center;">embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lo (2019)</td>
<td style="text-align: center;">chikisilo/y1s1</td>
</tr>
<tr>
<td style="text-align: center;">primary submissions</td>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rei et al. (2022)</td>
<td style="text-align: center;">Unbabel/COMET</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMETKIWI</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Rei et al. (2022)</td>
<td style="text-align: center;">Unbabel/COMET</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EE-BERTSCORE</td>
<td style="text-align: center;">embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Liu et al. (2022)</td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KG-BERTSCORE</td>
<td style="text-align: center;">embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Liu et al. (2022)</td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATESE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perrella et al. (2022)</td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATESE-QE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Perrella et al. (2022)</td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MEE4</td>
<td style="text-align: center;">lexical \&amp; embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mukherjee and Shrivastava (2022b)</td>
<td style="text-align: center;">AnanyaCoder/WM722Submission</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MetricX XXL</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MS-COMET</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kocmi et al. (2022b)</td>
<td style="text-align: center;">MicrosoftTranslator/MS-Comet</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MS-COMET-QE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Kocmi et al. (2022b)</td>
<td style="text-align: center;">MicrosoftTranslator/MS-Comet</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REUSE</td>
<td style="text-align: center;">embedding similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Mukherjee and Shrivastava (2022a)</td>
<td style="text-align: center;">AnanyaCoder/WM722Submission_REUSE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TEACHER-SIM</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Liu et al. (2022)</td>
<td style="text-align: center;">(not available)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SESCORE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Xu et al. (2022)</td>
<td style="text-align: center;">xul998hz/SEScore</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UNITE</td>
<td style="text-align: center;">fine-tuned metric</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wan et al. (2022b)</td>
<td style="text-align: center;">NLP2CT/UniTE</td>
</tr>
</tbody>
</table>
<p>Table 5: Baseline metrics and primary submissions for the metrics task. We categorize metrics into 3 major classes: lexical, embedding similarity and fine-tuned metrics. Regarding fine-tuned metrics we have metrics that use human quality scores such as DA or MQM and metrics that use synthetic labels for fine-tuning (3rd column).</p>
<h3>4.2 Metric Submissions</h3>
<p>The rest of this section summarizes participating metrics. The * symbol indicates that the metric is the primary submission of the research group.</p>
<p>COMET-22* (Rei et al., 2022) is an ensemble of two models; 1) COMET estimator model trained with Direct Assessments and 2) a newly proposed multitask model trained to predict sentence-level MQM scores along with OK/BAD word-level tags derived from annotation spans.</p>
<p>COMETKiwi* ensembles 2 QE models similarly to COMET-22; 1) classic Predictor-Estimator QE model trained on DAs ranging 2017 to 2019 and then fine-tuned on DAs from MLQE-PE (the official DA from the QE shared task) and 2) the same multitask model used in the COMET-22 submission but without access to a reference translation.</p>
<p>MS-COMET-22<em> and MS-COMET-QE22</em> (Kocmi et al., 2022b) are built on top of COMET by Microsoft Research using proprietary data. This metric is trained on a several times larger set of human judgements compared to COMET-baseline, covering 113 languages and</p>
<p>15 domains. Furthermore, the authors propose filtering of human judgement with potentially low quality to further improve the model.</p>
<p>MS-COMET-22 evaluated source, MT hypothesis and human reference from the input, while MS-COMET-QE-22 calculated scores in quality estimation fashion with only source segment and MT hypothesis.</p>
<p>EE-BERTSCORE* (Liu et al., 2022) stands for Entropy Enhanced BERTSCORE and aims at achieving a more balanced system-level rating by assigning weights to segment-level scores produced by BERTSCORE. The weights are determined by the difficulty of a segment determined by the entropy between the hypothesis-reference pair.</p>
<p>KG-BERTSCORE (Liu et al., 2022) is a reference-free machine translation (MT) evaluation metric, which incorporates multilingual knowledge graph into BERTScore by linearly combining the results of BERTScore and bilingual named entity matching.</p>
<p>Cross-QE (Liu et al., 2022) is a reference-free metric with a similar architecture to COMET-QE.</p>
<p>HWTSC-Teacher-Sim* (Liu et al., 2022) is a reference-free metric by fine-tuning the multilingual Sentence BERT model paraphrase-multilingual-mpnet-base-v2</p>
<p>HWTSC-TLM (Liu et al., 2022) is a referencefree metric which only uses a target-side language model to score the system translations as input.</p>
<p>MATESE<em> (Perrella et al., 2022) and MATESEQE</em> leverage transformer-based multilingual encoders to identify error spans in translations, and classify their severity between Minor and Major. The quality score returned for a translation is computed following the MQM error weighting used by Google (see Section 3.1).</p>
<p>MEE (Mukherjee et al., 2020) is an automatic evaluation metric that leverages the similarity between embeddings of words in candidate translation and the corresponding reference. Unigrams are matched based on their surface forms, root forms and meanings while semantic evaluation is achieved by using pretrained fasttext embeddings. MEE computes evaluation score using three modules namely exact match, root match and synonym match. In each module, fmean-score is calculated giving more weight to recall. Final score is the average of the three individual modules.</p>
<p>MEE2 and MEE4* (Mukherjee and Shrivastava, 2022b) are improved versions of MEE focusing on computing contextual and syntactic equivalences along with lexical, morphological and semantic similarity. The intent is to capture fluency and context of the MT outputs along with their adequacy. Fluency is captured using syntactic similarity and context is captured using sentence similarity leveraging sentence embeddings. The final score is the weighted combination of three similarity scores: a) syntactic similarity achieved by modified Bleu score; b) lexical, morphological and semantic similarity: measured by explicit unigram matching; c) contextual similarity: sentence similarity scores from Language-Agnostic BERT model.</p>
<p>REUSE* (Mukherjee and Shrivastava, 2022a) is a bilingual, unsupervised reference-free metric. It estimates the translation quality at chunk-level and sentence-level. Source and target sentence chunks are retrieved by using a multi-lingual chunker. Chunk-level similarity is computed by leveraging BERT contextual word embeddings and sen-
tence similarity scores are calculated by leveraging sentence embeddings of Language-Agnostic BERT models. The final quality estimation score is obtained by mean pooling the chunk-level and sentence-level similarity scores.</p>
<p>MetricX XL and MetricX XXL* are massive multi-task metrics, which fine-tune large language model checkpoints such as mT5 on a variety of human feedback data such as DA, MQM, QE, NLI and Summarization Eval. The resulting primary submission uses the MQM score outputted by a fine-tuned 30B mT5.</p>
<p>UNITE* (Wan et al., 2022a,b) is a learnt metric that can possess the ability of evaluating translation outputs following all three evaluation scenarios, i.e., source-only, reference-only, and source-referencecombined. Following their previous work, the authors improve their models by pre-training on pseudo-labeled data examples, and applying data cropping and a ranking-based score normalization during fine-tuning. The resulting submission is an ensemble of two models trained with different backbone models (XLM-R and InfoXLM).</p>
<p>SESCORE* (Xu et al., 2022) is an unsupervised reference-based evaluation metric, which takes model output and reference to produce a quality score. SESCORE is trained from a pre-trained language model (Ex. Roberta) on synthetic triples generated from raw text. The synthetic triples consist of (raw text, synthetic error text, pseudo score), corresponding to (reference, model output, human rating). The data used for training the metric is constructed by synthesising candidate sentences y' to mimic plausible errors by transforming raw input sentences multiple times. At each step, a random span of text is selected and new content is inserted, deleted or replaced. All these errors are non-overlapping. The authors name this data construction process "stratified error synthesis", which randomly samples a set of potential errors and stochastically applies them on a given sentence. The score assigned to the perturbed sentences is a raw count of the severities applied by each transformation. In the end, SESCORE is a regression quality prediction model trained on synthetic triples. Since this process can be applied to raw data and the resulting model can be developed for any text generation domain.</p>
<h2>5 Meta Evaluation</h2>
<p>Our main goal in evaluating metrics is to establish a ranking that reflects a metric's accuracy across a broad range of settings and applications. Combining results across different settings is challenging because correlations with human gold scores have different ranges and may be subject to differing degrees of noise. There are also many ways of measuring correlation, with different strengths and weaknesses, and it is often not clear which is best in a given setting.</p>
<p>This year, our overall ranking is just each metric's average rank across a large number of "tasks". Unlike raw correlation scores, ranks are comparable across tasks. The resulting global ranking approximates the "Kemeny consensus" - the ranking with lowest aggregate Kendall distance to the per-task rankings - which in turn satisfies several criteria from social choice theory (Colombo et al., 2022). Our version has the following features:</p>
<ul>
<li>We use a large number of tasks which may contain overlapping information. For instance, on each dataset, we compute both Pearson and Kendall-Tau correlation, and treat these as separate tasks. This makes the overall ranking robust to quirks in particular correlations.</li>
<li>To guard against inadvertent bias toward settings that have more tasks than others, we use a task weighting that reflects the relative importance of various attributes (language pair, domain, etc.).</li>
<li>Within each task, we establish a ranking that includes ties to reflect statistical significance. This naturally up-weights tasks that are more discriminative. For instance, a task that yields the ranking $1,1,1,1$ will not affect the overall ranking at all, while a ranking of $1,2,3,4$ is a maximal vote.</li>
<li>In order to indicate metric proximity, we report raw averages over (weighted) per-task ranks rather than the resulting ranking as advocated by Colombo et al. (2022). For instance, average ranks of $1.1,1.2,2.1,3.9$ indicate that the top two metrics perform similarly and the last metric is considerably worse; these details is lost in the global ranking $1,2,3,4$.</li>
<li>We also report rankings on selected subsets of tasks to characterize metric behavior on attributes such as language or domain.</li>
</ul>
<h3>5.1 Tasks</h3>
<p>Tasks are identified by unique value assignments for each of the following attributes: language, domain, level, include-human, averaging method, and correlation. These are as follows:</p>
<h2>Language (4 values)</h2>
<p>Language pairs include those for which we have MQM ratings - English $\rightarrow$ German, English $\rightarrow$ Russian, and Chinese $\rightarrow$ English plus All, which indicates all pairs pooled together.</p>
<h2>Domain (5 values)</h2>
<p>We computed correlations on domain-specific portions of each test-set as well as on each test-set as a whole. All language pairs have the same set of domains: conversation, e-commerce, news, and social. We use mixed to refer to all domains together, i.e., the whole test set.</p>
<h2>Level (2 values)</h2>
<p>For each domain (including mixed), we computed correlations at the system level and the segment level. Human scores for each domain are averages over the corresponding segments. For metric submissions that did not include domain-level scores, we computed similar averages.</p>
<h2>Include-human (2 values)</h2>
<p>We computed separate correlations over sets of outputs that exclude human references (includehuman=false) and that include all available references (include-human=true) except the standard reference, which is never scored by metrics. The first scenario reflects the standard use-case for metrics; the second captures a future scenario in which MT output quality approaches human quality. Since English $\rightarrow$ Russian has only a single reference, it participates only in the first condition. For the other two language pairs we use the reference that was judged best by the MQM raters. Table 6 summarizes the use of reference translations for different language pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">language</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">best ref</td>
<td style="text-align: center;">scored ref</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ de</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ ru</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">$}$</td>
</tr>
<tr>
<td style="text-align: left;">zh $\rightarrow$ en</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
</tr>
</tbody>
</table>
<p>Table 6: Use of reference translations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">language</th>
<th style="text-align: center;">domain</th>
<th style="text-align: center;">level</th>
<th style="text-align: center;">+human</th>
<th style="text-align: center;">averaging</th>
<th style="text-align: center;">correlation</th>
<th style="text-align: center;">tasks</th>
<th style="text-align: center;">weight</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">all (1/4)</td>
<td style="text-align: center;">mixed (1/1)</td>
<td style="text-align: center;">sys (1/1)</td>
<td style="text-align: center;">no (1/1)</td>
<td style="text-align: center;">none (1/1)</td>
<td style="text-align: center;">$\operatorname{acc}(1 / 1)$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / 4$</td>
</tr>
<tr>
<td style="text-align: center;">en-ru (1/4)</td>
<td style="text-align: center;">* (1/5)</td>
<td style="text-align: center;">sys (1/2)</td>
<td style="text-align: center;">no (1/1)</td>
<td style="text-align: center;">none (1/1)</td>
<td style="text-align: center;">P,K (1/2)</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$1 / 80$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seg (1/2)</td>
<td style="text-align: center;">no (1/1)</td>
<td style="text-align: center;">* (1/3)</td>
<td style="text-align: center;">P,K (1/2)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$1 / 240$</td>
</tr>
<tr>
<td style="text-align: center;">en-de,zh-en (1/4)</td>
<td style="text-align: center;">* (1/5)</td>
<td style="text-align: center;">sys (1/2)</td>
<td style="text-align: center;">* (1/2)</td>
<td style="text-align: center;">none (1/1)</td>
<td style="text-align: center;">P,K (1/2)</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$1 / 160$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">seg (1/2)</td>
<td style="text-align: center;">* (1/2)</td>
<td style="text-align: center;">* (1/3)</td>
<td style="text-align: center;">P,K (1/2)</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">$1 / 480$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">201</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Task weighting. Column entries are sets of values for the attribute in the heading, with * designating all possible values. Numbers in brackets show the weight assigned to each value in the set. Each line corresponds to a set of tasks that have the same weight: the product of all the per-attribute weights shown in brackets. $P$ and $K$ refer to Pearson and Kendall correlation, respectively.</p>
<h2>Averaging (3 values)</h2>
<p>At the segment level, metric and human scores are naturally represented as system $\times$ segment matrices. However, correlations operate over pairs of vectors rather than pairs of matrices. There are three ways to resolve the problem: flatten the matrices into single vectors, compute average correlations over matching pairs of row vectors, or compute average correlations over matching pairs of column vectors. We designate these as none, system, and segment averaging, respectively. They measure a metric's ability to rate an arbitrarilychosen (system, segment) pair, an arbitrary segment for a fixed system, and different system outputs for the same segment. Last year we used only the first alternative; this year include all three. System-level correlations do not require averaging, since their inputs are vectors in the first place.</p>
<h2>Correlation (3 values)</h2>
<p>We computed three correlations: system-level pairwise ranking accuracy (as proposed by Kocmi et al., 2021), Pearson and Kendall. Accuracy was used only for a single task in which all language pairs were pooled (language $=$ All), while Pearson and Kendall were used for all other tasks. Pearson correlation tests linear fit with MQM scores, a stringent but reasonable criterion since we expect these scores to conform to a linear scale (for example, a translation with two minor errors is twice as bad as one with only a single error). Pearson has well-known drawbacks (Mathur et al., 2020), notably sensitivity to outliers, which we minimized by choosing only relatively high-performing systems. Like accuracy, Kendall is based on pairwise score comparisons, and thus reflects a common ranking use-case. It is susceptible to noise in gold pairwise rankings, for which a common strategy
is to discard pairs judged not to be significantly different. We did not take this into account, relying instead on our significance tests for metric (rather than system) rankings.</p>
<h3>5.2 Task Weighting</h3>
<p>As explained in the previous section, attributes are not independent. For instance, there are three averaging methods for segment-level tasks, but only one for system-level tasks. If all tasks were weighted equally, this would have the undesirable consequence of making segment-level correlations count for $3 \times$ as much as system-level correlations when determining the overall ranking.</p>
<p>To avoid this, we used a hierarchical weighting scheme. We first ordered the attributes as listed in the previous section, then distributed weights evenly among all permissible values at each step of the hierarchy. The results are shown in Table 7. There are a total of 201 tasks, of which the accuracy task for all language pairs receives a weight of $1 / 4$, with the remaining mass of $3 / 4$ distributed among tasks whose individual weights vary between $1 / 80$ and $1 / 420$.</p>
<p>In Figures 1 through 4, we show analyses of how metric performance varies along different dimensions (attributes) such as language, domain, etc.. To do this, we partition tasks according to the values of the selected attribute, re-normalizing their global weights so they sum to 1 for each partition. We then compute weighted average ranks for each partition separately, in the same fashion as the overall ranking.</p>
<h3>5.3 Per-task Ranking</h3>
<p>For each task, we compare all pairs of metrics, and determine whether the difference in their correlation scores is significant according to the PERM-</p>
<p>BOTH hypothesis test of Deutsch et al. (2021), using 1000 re-sampling runs, and setting $p=0.05$. For the averaging methods, sampling is performed separately for each row or column vector prior to averaging.</p>
<p>We then assign ranks as follows. Starting with the highest-scoring metric, we move down the list of metrics in descending order by score, and assign rank 1 to all metrics until we encounter the first metric that is significantly different from any that have been visited so far. That metric is assigned rank 2, and the process is repeated. This continues until all metrics have been assigned a rank.</p>
<h2>6 Main Results</h2>
<p>As we have seen in Section 5, the main results are defined across different settings including systemlevel and segment-level tasks. Nonetheless, since the main use case of automatic metrics is to rank systems, system-level accuracy has a $1 / 4$ weight on the final score with the remaining $3 / 4$ distributed over 200 different settings.</p>
<p>Table 1 shows the official ranking of all primary submissions over the 201 different settings. A key observation is that neural metrics perform significantly better than lexical metrics. Of the 20 evaluated metrics, Bleu and spBleu are ranked 19th and 17th respectively. On the other hand, finetuned neural baseline metrics such as COMET-20 and BleUrt-20 are still ranked above several of the new primary submissions. They are outperformed only by submissions based on models that are considerably larger ${ }^{11}$. Figure 1 shows the ranking split by the different language pairs. The trend is very similar for all language pairs. While METRICX XXL performs best for $\mathrm{En} \rightarrow \mathrm{De}$ and $\mathrm{En} \rightarrow \mathrm{Ru}$, COMET-22 performs best for $\mathrm{Zh} \rightarrow$ En.</p>
<p>One open question about neural metrics has been their ability to generalise to new domains, since most training and testing data from previous years were based on News data. In Figure 2 we present the performance of each metric across four domains: news, social, conversational, and ecommerce. Similar to last year, we observe that the neural metrics perform better than lexical overlap metrics across all four domains.</p>
<p>Figure 3 shows the average rankings when grouped separately by system-level and segment-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Weighted ranking of metrics' correlation with human grouped by translation directions.
level tasks. Many metrics fall into the same significance cluster when evaluated on the system-level as we only have a very limited number of MT systems. Nevertheless, we observe that the metric rankings are largely stable across both granularities and that MetricX XXL and COMET-22 perform best on both the segment-level and system-level tasks. The differences are more prevalent in the segment-level task, though.</p>
<p>In Figure 4, we compare the rankings when including human translations as MT systems (with human) or just considering MT submission (without human). Overall, the majority of metrics show
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Weighted ranking of metrics' correlation with human grouped by domains.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Weighted ranking of metrics' correlation with human grouped by granularity levels.
lower correlation when we include human translations, except COMET-22 and MATESE.</p>
<h2>7 Direct Assessment Human Evaluation</h2>
<p>In addition to our MQM annotations and as a contrastive evaluation to cover more language pairs, we look into the performance of metrics when compared to the human evaluation campaign conducted by the General MT shared task (Kocmi et al., 2022a), who ran human evaluation for all 21 translation directions and WMT22 submissions. Last year, we decided to exclude the human ratings by the WMT main task as they were of lower quality than the best automatic metrics. However, the GeneralMT task improved their evaluation methodology in particular for all from-English and nonEnglish translation directions and implemented the Scalar Quality Metric (SQM) which has been shown to have high correlation with MQM on at least the system-level (Freitag et al., 2021a). The GeneralMT task used two different human evaluation methodologies depending on the language pair: reference-based Direct Assessment (Ref. DA) (Graham et al., 2013) and SQM style source-based DA (DA+SQM) (Kocmi et al., 2022a).</p>
<p>Ref. DA has been used for all into-English translation directions and asks human raters to judge
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Weighted ranking of metrics' correlation with human grouped by candidate pools (with or without human translations).
each system translation against human reference translation on a $0-100$ scale. This technique does not use bilingual speakers and is evaluated by nonprofessional crowd workers. In order to increase quality of assessment, there are several quality control items. Out of all collected human annotations, $63 \%$ have been removed due to failing quality control.</p>
<p>DA+SQM asks bilingual raters to annotate system translations against original sources on a 0 100 labeled scale. The scale is marked with seven points representing expected quality. In this setting, Kocmi et al. (2022a) evaluated all from-English and non-English translation directions. They used mainly professional raters.</p>
<p>We present system-level accuracy results in Table 8. The ranking generated based on accuracy scores when taking the DA+SQM annotation as ground truths is comparable to the primary results in Table 1, ranking MetricX XXL as the best performing metric followed by UnITE and COMET22. Similarly, it ranks n-gram matching metrics (BLEU, CHRF, F101SPBLEU) among worst performing metrics. This confirms the main findings from MQM evaluation.</p>
<p>On the other hand, accuracy scores taking ref. DA as the ground truth, result in a very different ranking of the metrics. It ranks n-gram matching metrics as the top performing metrics. This suggest that the technique does not evaluate systems well</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Number of languages</th>
<th style="text-align: right;">13</th>
<th style="text-align: right;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of system pairs</td>
<td style="text-align: right;">564</td>
<td style="text-align: right;">329</td>
</tr>
<tr>
<td style="text-align: left;">Human judgement style</td>
<td style="text-align: right;">DA+SQM</td>
<td style="text-align: right;">ref. DA</td>
</tr>
<tr>
<td style="text-align: left;">MetricX XXL</td>
<td style="text-align: right;">$\mathbf{0 . 8 6 2}$ (1)</td>
<td style="text-align: right;">$0.620(11)$</td>
</tr>
<tr>
<td style="text-align: left;">UniTE</td>
<td style="text-align: right;">$0.849(2)$</td>
<td style="text-align: right;">$0.623(10)$</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: right;">$0.842(3)$</td>
<td style="text-align: right;">$0.626(9)$</td>
</tr>
<tr>
<td style="text-align: left;">COMETKiwi*</td>
<td style="text-align: right;">$0.835(4)$</td>
<td style="text-align: right;">$0.617(12)$</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-22</td>
<td style="text-align: right;">$0.833(5)$</td>
<td style="text-align: right;">$0.626(9)$</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT-20</td>
<td style="text-align: right;">$0.830(6)$</td>
<td style="text-align: right;">$0.650(5)$</td>
</tr>
<tr>
<td style="text-align: left;">COMET-20</td>
<td style="text-align: right;">$0.826(7)$</td>
<td style="text-align: right;">$0.635(8)$</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-QE-22*</td>
<td style="text-align: right;">$0.824(8)$</td>
<td style="text-align: right;">$0.641(7)$</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE*</td>
<td style="text-align: right;">$0.821(9)$</td>
<td style="text-align: right;">$0.605(13)$</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-SRC*</td>
<td style="text-align: right;">$0.800(10)$</td>
<td style="text-align: right;">$0.623(10)$</td>
</tr>
<tr>
<td style="text-align: left;">YISI-1</td>
<td style="text-align: right;">$0.785(11)$</td>
<td style="text-align: right;">$0.660(3)$</td>
</tr>
<tr>
<td style="text-align: left;">BERTSCORE</td>
<td style="text-align: right;">$0.764(12)$</td>
<td style="text-align: right;">$0.666(2)$</td>
</tr>
<tr>
<td style="text-align: left;">CHRF</td>
<td style="text-align: right;">$0.762(13)$</td>
<td style="text-align: right;">$0.666(2)$</td>
</tr>
<tr>
<td style="text-align: left;">EE_BERTScore</td>
<td style="text-align: right;">$0.750(14)$</td>
<td style="text-align: right;">$0.647(6)$</td>
</tr>
<tr>
<td style="text-align: left;">F101SPBLEU</td>
<td style="text-align: right;">$0.748(15)$</td>
<td style="text-align: right;">$\mathbf{0 . 6 6 9 ( 1 )}$</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-TEACHER-SIM*</td>
<td style="text-align: right;">$0.720(16)$</td>
<td style="text-align: right;">$0.568(15)$</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: right;">$0.707(17)$</td>
<td style="text-align: right;">$0.653(4)$</td>
</tr>
<tr>
<td style="text-align: left;">REUSE*</td>
<td style="text-align: right;">$0.344(18)$</td>
<td style="text-align: right;">$0.584(14)$</td>
</tr>
</tbody>
</table>
<p>Table 8: System-level pairwise accuracy for WMT style human evaluation. Numbers in brackets show rank of metrics given human judgement style. The highest score is present bolded.
and instead human crowd workers are incentivized to quickly compare the surface forms of translation against reference without understanding. We would advise metric developers and researchers running human evaluations not to use reference-based DA, especially when evaluated with non-professional crowd workers.</p>
<h2>8 Challenge Sets Subtask</h2>
<p>The challenge sets subtask is inspired by the Build it or break it: The Language Edition shared task (Ettinger et al., 2017) which aimed at testing the generalizability of NLP systems beyond the distributions of their training data. With that said, our goal is to encourage researchers to build a set of test sets that measure metrics' ability to detect different targeted phenomena that might not be well represented in traditional test sets used to evaluate metrics.</p>
<p>This subtask is made of three consecutive phases; 1) the Breaking Round, 2) the Scoring Round and 3) the Analysis Round:</p>
<ol>
<li>In the Breaking Round, the challenge set participants (Breakers) submit their challenge sets composed of contrastive examples for dif-
ferent phenomena with source sentences ( $s$ ), incorrect translations $(\hat{t})$, correct translations $(\ell)$ and references $(r)$.</li>
<li>In the Scoring Round the metrics participants from the main task (the Builders) are asked to score all translations with their metrics without knowing which ones are correct or incorrect. Also, in this phase the organisers score all data with the baseline metrics.</li>
<li>Finally, after gathering all metric scores, the data is returned to the Breakers for the Analysis round, where they look at which metrics are able to correctly rank the correct translations above the incorrect ones for the different phenomena being tested.</li>
</ol>
<p>We had a total of 4 submissions to this shared task, covering a wide range of phenomena and 146 different language pairs. Table 9 provides an overview of the submitted challenge sets. A short description of every submission follows:</p>
<p>ACES The ACES (Translation Accuracy Challenge Sets; Amrhein et al., 2022) results from a collaboration between the University of Zurich with the University of Edinburgh. This challenge set, highly inspired by the MQM framework, consists of 36,499 examples, covering 146 language pairs and 68 phenomena, ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. The data was created artificially for some error types and manually for others.</p>
<p>Their analysis aimed to reveal the extent to which metrics take into account the source sentence context and the surface-level overlap with the reference, and if they profit by using multilingual embeddings. Finally, they recommend that one considers a) combining metrics with different strengths and b) explicitly modelling additional languagespecific information beyond what is available via multilingual embeddings.</p>
<p>SMAUG The challenge set based on Sentencelevel Multilingual data Augmentation (SMAUG; Alves et al., 2022), submitted by Unbabel and IST evaluates the robustness of MT metrics to 5 different types of translation errors; Named entity errors, numerical errors, meaning errors, insertion of content and content missing. These errors are created by perturbing reference translations and then curated by the authors. The challenge set covers 3</p>
<table>
<thead>
<tr>
<th style="text-align: left;">challenge set</th>
<th style="text-align: left;">method</th>
<th style="text-align: center;">lang. <br> pairs</th>
<th style="text-align: center;">pheno- <br> mena</th>
<th style="text-align: center;">items</th>
<th style="text-align: left;">citation</th>
<th style="text-align: left;">availability (https://github.com/)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ACES</td>
<td style="text-align: left;">automatic</td>
<td style="text-align: center;">146</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">36,499</td>
<td style="text-align: left;">Amrhein et al. (2022)</td>
<td style="text-align: left;">EdinburghBLF/ACES</td>
</tr>
<tr>
<td style="text-align: left;">DFKI-CS</td>
<td style="text-align: left;">semi-autom.</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">19,347</td>
<td style="text-align: left;">Avramidis and Mack- <br> etanz (2022)</td>
<td style="text-align: left;">DFKI-RLF/mt-testsuite</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-CS</td>
<td style="text-align: left;">semi-autom.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">721</td>
<td style="text-align: left;">Chen et al. (2022)</td>
<td style="text-align: left;">HWTSC/Challenge-Set-for-MT-Metrics</td>
</tr>
<tr>
<td style="text-align: left;">SMAUG</td>
<td style="text-align: left;">automatic</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">632</td>
<td style="text-align: left;">Alves et al. (2022)</td>
<td style="text-align: left;">Unbabel/smaug</td>
</tr>
</tbody>
</table>
<p>Table 9: Overview of the participations at the challenge sets task
language pairs and contains close to 50 high-quality examples for each phenomenon.</p>
<p>In this challenge set the authors show that there has been a promising progress in terms of detecting these critical errors when compared to last year's metric submissions. Nevertheless, errors related to named entities and numbers were found to pose a challenge for several tested metrics. Also, due to a high variance in the observed results across all the error types it becomes hard to predict performance of current methods with respect to untested translation errors.</p>
<p>HWTSC Challenge Set The challenge set submitted by Huawei Translation Services Center (Chen et al., 2022) aims at examining metrics ability to handle synonyms and to discern critical errors in translations. This challenge set is composed of 721 zh -en examples for 5 different error types; Named entity errors, numerical errors, time \&amp; date errors, wrong unit conversions and Affirmation/Negation errors. The underlying data is either WMT 21 or Flores 101 which covers two distinct domains, News and Wikipedia respectively. To create alternative translations the authors used in-house translators (performing postedit) and to create the adversarial translations they used LIST (Alzantot et al., 2018).</p>
<p>The authors of this challenge set conclude that although embedding-based metrics perform relatively well on discerning sentence-level negation/affirmation errors, they perform poorly on relating synonyms. Additionally they find that the generalizability of some metrics is compromised, as they are susceptible to different text styles.</p>
<p>DFKI Challenge Set The submission by DFKI (Avramidis and Macketanz, 2022) employs a linguistically motivated challenge set that includes about 20,000 items extracted from 145 MT systems for two language directions (German $\Leftrightarrow$ English). It is based on a test suite (Macketanz et al., 2022) that covers more than 100 linguistically-motivated
phenomena organized in 14 categories.
The best performing metrics are YiSi-1, BERTSCORE and COMET-22 for GermanEnglish, and UniTE, UniTE-REF, MetricX-XL-DA-2019 and MetricX-XXL-DA-2019 for English-German. Metrics in both directions are performing worst when it comes to named-entities \&amp; terminology and particularly measuring units. Particularly in German-English they are weak at detecting issues at punctuation, polar questions, relative clauses, dates and idioms. In EnglishGerman, they perform worst at present progressive of transitive verbs, future II progressive of intransitive verbs, simple present perfect of ditransitive verbs and focus particles.</p>
<h2>9 Conclusion</h2>
<p>This paper summarizes the results of the WMT22 shared task on automated machine translation evaluation, the Metrics Shared Task. We presented an extensive analysis on how well metrics perform on our three main language pairs: English $\rightarrow$ German, English $\rightarrow$ Russian and Chinese $\rightarrow$ English. The results, based on 201 different tasks, demonstrated the superiority of neural-based learned metrics over overlap-based metrics like Bleu, spBleu or CHRF. These results are confirmed with DA+SQM human judgement. Although this was already the case in the previous years' Metric Shared Tasks, we further strengthened the case for neuralbased fine-tuned metrics by demonstrating their superiority across four different domains. In addition, we initiated a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics' ability to capture and penalise specific types of translation errors.</p>
<h2>10 Ethical Considerations</h2>
<p>MQM annotations and additional reference translations in this paper are done by professional translators. They are all paid at professional rates.</p>
<p>Organizers from the National Research Council</p>
<p>Canada and Unbabel have submitted to this task the frozen stable versions of their metrics (YiSi and COMET) dated before this year's shared task and publicly available. Newer versions of COMET were developed without using any of the test set, test suite or challenge sets.</p>
<h2>11 Acknowledgments</h2>
<p>Results for this shared task would not be possible without tight collaboration with the organizers of the General MT Task. We are grateful to Google and Unbabel for sponsoring and overseeing the human evaluation.</p>
<p>Ricardo Rei and André F. T. Martins are supported by the P2020 program (MAIA: contract 045909) and by European Union's Horizon Europe Research and Innovation Actions (UTTER: contract 101070631). Additionally, André F. T. Martins is supported by the European Research Council (ERC StG DeepSPIN, 758969) and Fundação para a Ciência e Tecnologia (contract UIDB/50008/2020).</p>
<p>Eleftherios Avramidis is supported by the German Ministry of Education and Research through the research program SocialWear (grant num. 01IW2000).</p>
<h2>References</h2>
<p>Duarte M. Alves, Ricardo Rei, Ana C. Farinha, José G. C. de Souza, and André F. T. Martins. 2022. Robust MT evaluation with Sentence-level Multilingual data Augmentation. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890-2896, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Chantal Amrhein, Nikita Moghe, and Liane K. Guillou. 2022. ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Eleftherios Avramidis and Vivien Macketanz. 2022. Linguistically motivated evaluation of machine translation metrics based on a challenge set. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Xiaoyu Chen, Daimeng Wei, Hengchao Shang, Zongyao Li, Zhanglin Wu, Zhengzhe Yu, Ting Zhu, Mengli Zhu, Ning Xie, Lizhi Lei, Shimin Tao, Hao Yang, and Ying Qin. 2022. Exploring Robustness of Machine Translation Metrics: A Study of TwentyTwo Automatic Metrics in the WMT22 Metric Task. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Stéphan Clémençon. 2022. What are the best systems? new perspectives on nlp benchmarking. arXiv preprint arXiv:2202.03799.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. arXiv preprint arXiv:2104.00054.</p>
<p>Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4506-4520, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Bryan Eikema and Wilker Aziz. 2021. Sampling-based minimum bayes risk decoding for neural machine translation.</p>
<p>Allyson Ettinger, Sudha Rao, Hal Daumé III, and Emily M. Bender. 2017. Towards linguistically generalizable NLP systems: A workshop and shared task. In Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 1-10, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(1).</p>
<p>Patrick Fernandes, António Farinhas, Ricardo Rei, José De Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1396-1412, Seattle, United States. Association for Computational Linguistics.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022. High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the Association for Computational Linguistics, 10:811-825.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522-538.</p>
<p>Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33-41, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations.</p>
<p>Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, Maja Popović, and Mariya Shmatova. 2022a. Findings of the 2022 conference on machine translation (wmt22). In Proceedings of the Seventh Conference on Machine Translation. Association for Computational Linguistics.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation.</p>
<p>Tom Kocmi, Hitokazu Matsushita, and Christian Federmann. 2022b. MS-COMET: Larger Filtered Human Annotations Help Metric Performance. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical</p>
<p>Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yilun Liu, Xiaosong Qiao, Zhanglin Wu, Su Chang, Min Zhang, Yanqing Zhao, shimin tao Song Peng, Hao Yang, Ying Qin, Jiaxin Guo, Minghan Wang, Yinglu Li, Peng Li, and Xiaofeng Zhao. 2022. Partial Could Be Better Than Whole: HW-TSC 2022 Submission for the Metrics Shared Task. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Chi-kiu Lo. 2019. YiSi - a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 507-513, Florence, Italy. Association for Computational Linguistics.</p>
<p>Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional Quality Metrics (MQM) : A Framework for Declaring and Describing Translation Quality Metrics. Tradumàtica, pages 0455-463.</p>
<p>Vivien Macketanz, Eleftherios Avramidis, Aljoscha Burchardt, He Wang, Renlong Ai, Shushen Manakhimova, Ursula Strohriegel, Sebastian Möller, and Hans Uszkoreit. 2022. A linguistically motivated test suite to semi-automatically evaluate German-English machine translation output. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 936-947, Marseille, France. European Language Resources Association.</p>
<p>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in bleu: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997.</p>
<p>Ananya Mukherjee, Hema Ala, Manish Shrivastava, and Dipti Misra Sharma. 2020. Mee: an automatic metric for evaluation using embeddings for machine translation. In 2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA), pages 292-299. IEEE.</p>
<p>Ananya Mukherjee and Manish Shrivastava. 2022a. REUSE: REference-free UnSupervised quality Estimation Metric. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Ananya Mukherjee and Manish Shrivastava. 2022b. Unsupervised Embedding-based Metric for MT Evaluation with Improved Human Correlation. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Mathias Müller and Rico Sennrich. 2021. Understanding the properties of minimum Bayes risk decoding</p>
<p>in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 259-272, Online. Association for Computational Linguistics.</p>
<p>Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 314-319, Florence, Italy. Association for Computational Linguistics.</p>
<p>NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No Language Left Behind: Scaling HumanCentered Machine Translation.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. Machine Translation Evaluation as a Sequence Tagging Problem. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 751-762, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? unbabel-IST 2021 submission for the metrics shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 1030-1040, Online. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450-3466, Online. Association for Computational Linguistics.</p>
<p>Yu Wan, Keqin Bao, Dayiheng Liu, Baosong Yang, Derek F. Wong, Lidia S. Chao, Wenqiang Lei, and Jun Xie. 2022a. Alibaba-Translate China's Submission for WMT2022 Metrics Shared Task. In Proceedings of the Seventh Conference on Machine Translation, Abu Dhabi. Association for Computational Linguistics.</p>
<p>Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022b. UniTE: Unified translation evaluation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8117-8127, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Wenda Xu, Yilin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang Wang. 2022. Not all errors are equal: Learning text generation metrics using stratified error synthesis. arXiv preprint arXiv:2210.05035.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">en-de</th>
<th style="text-align: center;">en-de</th>
<th style="text-align: center;">en-ru</th>
<th style="text-align: center;">zh-en</th>
<th style="text-align: center;">zh-en</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human Translation Included</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xl_DA_2019</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xxl_DA_2019</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.984</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xxl_MQM_2020</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.920</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT-20</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xl_MQM_2020</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.914</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.942</td>
</tr>
<tr>
<td style="text-align: left;">COMET-20</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.970</td>
</tr>
<tr>
<td style="text-align: left;">UniTE</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.914</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-22</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.909</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-ref</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.892</td>
</tr>
<tr>
<td style="text-align: left;">MATESE</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.856</td>
</tr>
<tr>
<td style="text-align: left;">Yisi-1</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.935</td>
</tr>
<tr>
<td style="text-align: left;">MEE4</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: left;">COMETKiwi*</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.866</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.8_With_Human</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.960</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.8_Without_Human</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr>
<td style="text-align: left;">Cross-QE*</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.870</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.5_With_Human</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.953</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE*</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.569</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.5_Without_Human</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.942</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.3_With_Human</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-src*</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.874</td>
</tr>
<tr>
<td style="text-align: left;">MEE2</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.872</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-QE-22*</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.897</td>
</tr>
<tr>
<td style="text-align: left;">MATESE-QE*</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.767</td>
</tr>
<tr>
<td style="text-align: left;">MEE</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.824</td>
</tr>
<tr>
<td style="text-align: left;">f101spBLEU</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.718</td>
</tr>
<tr>
<td style="text-align: left;">f200spBLEU</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.728</td>
</tr>
<tr>
<td style="text-align: left;">HuaweiTSC_EE_BERTScore_0.3_Without_Human</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.926</td>
</tr>
<tr>
<td style="text-align: left;">chF</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.630</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.594</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-TLM*</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.460</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-Teacher-Sim*</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.356</td>
</tr>
<tr>
<td style="text-align: left;">KG-BERTScore*</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.743</td>
</tr>
<tr>
<td style="text-align: left;">REUSE*</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">-0.514</td>
<td style="text-align: center;">-0.465</td>
<td style="text-align: center;">-0.349</td>
<td style="text-align: center;">-0.330</td>
<td style="text-align: center;">-0.142</td>
</tr>
<tr>
<td style="text-align: left;">SEScore</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.944</td>
</tr>
</tbody>
</table>
<p>Table 10: Pearson correlation of all metrics with system-level MQM scores for the three main language pairs. Rows are sorted by the system-level pairwise accuracy across the three language pairs. Primary submissions are bolded, and baselines are underlined. Reference-free metrics are indicated using an asterisk.</p>
<h1>A Language-Specific Results Tables</h1>
<p>Language-specific results are given in Table 10 and Table 11. Each page contains results for scores over all domains over a single granularity (system or segment).</p>
<p>For all tables, the correlations are calculated on metric scores comparing MT system translations with Reference A, and any additional human reference translations are not included.</p>
<p>For segment level correlation, we report results on the "none" averaging method, where we flatten the matrices into single vectors before computing the Kendall Tau correlation.</p>
<h2>B Correlations with WMT Human Evaluation</h2>
<p>Correlations with WMT Direct Assessment Human scores are given in the following tables, with results for language pairs evaluated using reference-based Direct Assessment (Ref. DA) (Graham et al., 2013), followed by results for language pairs evaluated using SQM style source-based DA (DA+SQM) (Kocmi et al., 2022a). Since most language pairs contained only a single reference, we used reference A for all pairs, and report results only for scoring MT output (omitting additional scored references for language pairs where these were available). System-level correlations use Pearson and segment-level scores use Kendall. For simplicity, both statistics are computed over raw rater scores, with no traditional difference-25</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task <br> Human Translation Included</th>
<th style="text-align: center;">(sys) Accuracy <br> No</th>
<th style="text-align: center;">en-de <br> Yes</th>
<th style="text-align: center;">en-de <br> No</th>
<th style="text-align: center;">en-ru <br> No</th>
<th style="text-align: center;">zh-en <br> Yes</th>
<th style="text-align: center;">zh-en <br> No</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">metrics_xl_DA_2019</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.392</td>
</tr>
<tr>
<td style="text-align: center;">metrics_xxl_DA_2019</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.386</td>
</tr>
<tr>
<td style="text-align: center;">metrics_xxl_MQM_2020</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.427</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT-20</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.361</td>
</tr>
<tr>
<td style="text-align: center;">metrics_xl_MQM_2020</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.423</td>
</tr>
<tr>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.428</td>
</tr>
<tr>
<td style="text-align: center;">COMET-20</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.332</td>
</tr>
<tr>
<td style="text-align: center;">UniTE</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.357</td>
</tr>
<tr>
<td style="text-align: center;">MS-COMET-22</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.341</td>
</tr>
<tr>
<td style="text-align: center;">UniTE-ref</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.361</td>
</tr>
<tr>
<td style="text-align: center;">MATESE</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.389</td>
</tr>
<tr>
<td style="text-align: center;">YiSi-1</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.296</td>
</tr>
<tr>
<td style="text-align: center;">MEE4</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: center;">COMETKiwi*</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.364</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.8_With_Human</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.8_Without_Human</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cross-QE*</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.378</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.5_With_Human</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">COMET-QE*</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.365</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.5_Without_Human</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.316</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.3_With_Human</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">UniTE-src*</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.343</td>
</tr>
<tr>
<td style="text-align: center;">MEE2</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.201</td>
</tr>
<tr>
<td style="text-align: center;">MS-COMET-QE-22*</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.287</td>
</tr>
<tr>
<td style="text-align: center;">MATESE-QE*</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.337</td>
</tr>
<tr>
<td style="text-align: center;">MEE</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.149</td>
</tr>
<tr>
<td style="text-align: center;">f101spBLEU</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.145</td>
</tr>
<tr>
<td style="text-align: center;">f200spBLEU</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.140</td>
</tr>
<tr>
<td style="text-align: center;">HuaweiTSC_EE_BERTScore_0.3_Without_Human</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">chrF</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.147</td>
</tr>
<tr>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.145</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC-TLM*</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.086</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC-Teacher-Sim*</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.272</td>
</tr>
<tr>
<td style="text-align: center;">KG-BERTScore*</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.219</td>
</tr>
<tr>
<td style="text-align: center;">REUSE*</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.130</td>
</tr>
<tr>
<td style="text-align: center;">SEScore</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.331</td>
</tr>
</tbody>
</table>
<p>Table 11: Kendall Tau correlation of all metrics with segment-level MQM scores for the three main language pairs. Rows are sorted by the system-level pairwise accuracy across the three language pairs. Primary submissions are bolded, and baselines are underlined. Reference-free metrics are indicated using an asterisk.
filtering. ${ }^{12}$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">cs-en</th>
<th style="text-align: center;">de-en</th>
<th style="text-align: center;">ja-en</th>
<th style="text-align: center;">ru-en</th>
<th style="text-align: center;">uk-en</th>
<th style="text-align: center;">zh-en</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Incl. Human Translation</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: left;">f200spBLEU</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.517</td>
</tr>
<tr>
<td style="text-align: left;">chrF</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.568</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: left;">YiSi-1</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.415</td>
</tr>
<tr>
<td style="text-align: left;">f101spBLEU</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.521</td>
</tr>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT-20</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.266</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.8_Without_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.417</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.3_Without_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.437</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.3_With_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.412</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.8_With_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.411</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.5_With_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.416</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC_EE_BERTScore_0.5_Without_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.434</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-QE-22*</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.312</td>
</tr>
<tr>
<td style="text-align: left;">COMET-20</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.330</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xxl_DA_2019</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.148</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-ref</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.167</td>
</tr>
<tr>
<td style="text-align: left;">MS-COMET-22</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.108</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.135</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xl_DA_2019</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.157</td>
</tr>
<tr>
<td style="text-align: left;">Cross-QE*</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.129</td>
</tr>
<tr>
<td style="text-align: left;">UniTE</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.195</td>
</tr>
<tr>
<td style="text-align: left;">UniTE-src*</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.210</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xl_MQM_2020</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">-0.039</td>
</tr>
<tr>
<td style="text-align: left;">metrics_xxl_MQM_2020</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">-0.061</td>
</tr>
<tr>
<td style="text-align: left;">COMETKiwi*</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.181</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE*</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">-0.006</td>
</tr>
<tr>
<td style="text-align: left;">REUSE*</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.531</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-TLM*</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.083</td>
</tr>
<tr>
<td style="text-align: left;">HWTSC-Teacher-Sim*</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">-0.011</td>
</tr>
<tr>
<td style="text-align: left;">KG-BERTScore*</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.264</td>
</tr>
<tr>
<td style="text-align: left;">MEE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.578</td>
</tr>
<tr>
<td style="text-align: left;">MEE2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.511</td>
</tr>
<tr>
<td style="text-align: left;">MEE4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.455</td>
</tr>
<tr>
<td style="text-align: left;">SEScore</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.331</td>
</tr>
<tr>
<td style="text-align: left;">MATESE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: left;">MATESE-QE*</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.013</td>
</tr>
</tbody>
</table>
<p>Table 12: System-level Pearson correlation with crowdsourced Ref. DA scores. Rows are sorted by the system-level pairwise accuracy across all language pairs. Primary submissions are bolded, and baselines are underlined. Reference-free metrics are indicated using an asterisk.</p>
<p>System-level Metric accuracy and correlations with REFDA scores contradict the main results. We strongly recommend against using Ref. DA scores to evaluate MT metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task <br> Incl. Human Translation</th>
<th style="text-align: center;">(sys) Accuracy <br> False</th>
<th style="text-align: center;">cs-en <br> False</th>
<th style="text-align: center;">de-en <br> False</th>
<th style="text-align: center;">ja-en <br> False</th>
<th style="text-align: center;">ru-en <br> False</th>
<th style="text-align: center;">uk-en <br> False</th>
<th style="text-align: center;">zh-en <br> False</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">f200spBLEU</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.026</td>
</tr>
<tr>
<td style="text-align: center;">chrt</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.025</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.020</td>
</tr>
<tr>
<td style="text-align: center;">YiSi-1</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.020</td>
</tr>
<tr>
<td style="text-align: center;">f101spBLEU</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.026</td>
</tr>
<tr>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.024</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT-20</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.8_Without_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.3_Without_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.3_With_Human</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.8_With_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.5_With_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC_EE_BERTScore_0.5_Without_Human</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MS-COMET-QE-22*</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">COMET-20</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: center;">metricx_xxl_DA_2019</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.040</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.086</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.008</td>
</tr>
<tr>
<td style="text-align: center;">UniTE-ref</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: center;">MS-COMET-22</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">$-0.000$</td>
<td style="text-align: center;">0.004</td>
</tr>
<tr>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">metricx_xl_DA_2019</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: center;">Cross-QE*</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">$-0.000$</td>
</tr>
<tr>
<td style="text-align: center;">UniTE</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.006</td>
</tr>
<tr>
<td style="text-align: center;">UniTE-src*</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: center;">metricx_xl_MQM_2020</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">$-0.002$</td>
</tr>
<tr>
<td style="text-align: center;">metricx_xxl_MQM_2020</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">$-0.003$</td>
</tr>
<tr>
<td style="text-align: center;">COMETKiwi*</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.028</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">COMET-QE*</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">$-0.005$</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr>
<td style="text-align: center;">REUSE*</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">$-0.007$</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC-TLM*</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;">HWTSC-Teacher-Sim*</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">KG-BERTScore*</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">$-0.012$</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">$-0.002$</td>
</tr>
<tr>
<td style="text-align: center;">MEE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.020</td>
</tr>
<tr>
<td style="text-align: center;">MEE2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.021</td>
</tr>
<tr>
<td style="text-align: center;">MEE4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.021</td>
</tr>
<tr>
<td style="text-align: center;">SEScore</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;">MATESE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.009$</td>
</tr>
<tr>
<td style="text-align: center;">MATESE-QE*</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.006$</td>
</tr>
</tbody>
</table>
<p>Table 13: Segment-level Kendall-like correlation with crowdsourced Ref. DA scores. Rows are sorted by the system-level pairwise accuracy across all language pairs. Primary submissions are bolded, and baselines are underlined. Reference-free metrics are indicated using an asterisk.</p>
<p>The segment level Kendal-like correlations of all metrics with Ref. DA scores are all very close to zero, and these numbers are completely meaningless. We strongly recommend against using Ref. DA scores to evaluate MT metrics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ The traditional recipe made little difference in overall correlation patterns.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ Inrefs.1lcase.mixedliang.LANGPAIRltok.13alsmooth.exp lversion.1.5.0
${ }^{8}$ nrefs:1lcase:mixedleff:yesltok:flores101lsmooth:expl version:2.3.1
${ }^{9}$ nrefs:1lcase:mixedleff:yesltok:flores200lsmooth:expl version:2.3.1
${ }^{10}$ chrF2llang.LANGPAIRlnchars.6lspace.falselversion.1.5.0&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
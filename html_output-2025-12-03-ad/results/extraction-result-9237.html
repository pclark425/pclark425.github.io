<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-272828099</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14673v1.pdf" target="_blank">Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</a></p>
                <p><strong>Paper Abstract:</strong> Real-world applications of large language models (LLMs) in computational social science (CSS) tasks primarily depend on the effectiveness of instruction tuning (IT) or in-context learning (ICL). While IT has shown highly effective at fine-tuning LLMs for various tasks, ICL offers a rapid alternative for task adaptation by learning from examples without explicit gradient updates. In this paper, we evaluate the classification performance of LLMs using IT versus ICL in few-shot CSS tasks. The experimental results indicate that ICL consistently outperforms IT in most CSS tasks. Additionally, we investigate the relationship between the increasing number of training samples and LLM performance. Our findings show that simply increasing the number of samples without considering their quality does not consistently enhance the performance of LLMs with either ICL or IT and can sometimes even result in a performance decline. Finally, we compare three prompting strategies, demonstrating that ICL is more effective than zero-shot and Chain-of-Thought (CoT). Our research highlights the significant advantages of ICL in handling CSS tasks in few-shot settings and emphasizes the importance of optimizing sample quality and prompting strategies to improve LLM classification performance. The code will be made available.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL_vs_IT_aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning versus Instruction Tuning (aggregate comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate comparison across six open-source LLMs (7B–9B) on five computational social science (CSS) classification tasks showing ICL generally outperforms instruction tuning in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple open-source LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B-9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate CSS classification tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Five social-media / news classification benchmarks requiring semantic/stance/credibility judgments on short text (tweets) or news articles.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>In-Context Learning (ICL): few-shot prompts that include Instruction, Constraints, several input-label Samples, and the target Text. ICL experiments used n-shot with n in {1,8,16,32}, five random seeds per shot; temperature set to 0.2; GossipCop articles truncated to 256 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Instruction Tuning (IT): instruction prompts (Instruction + Constraints + Text) used to fine-tune each model with LoRA (dropout 0.1, lr 1e-4, scaling factor 32, rank 8) for three epochs in bf16; same few-shot sampling (n ∈ {1,8,16,32}) used to construct IT training sets, five random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate (mean across tasks and models): ICL average accuracy advantage vs. IT of +3.3% (1-shot), +2.9% (8-shot), +3.2% (16-shot), +3.7% (32-shot). Metrics reported: accuracy and macro-F1 averaged across five seeds and five tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>IT baseline aggregated across same models/tasks; aggregated IT accuracies are lower by the effect sizes above (paper reports the % differences rather than per-format absolute aggregated numbers in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.3% acc (1-shot), +2.9% (8-shot), +3.2% (16-shot), +3.7% (32-shot) in favor of ICL (averaged over six LLMs and five tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize ICL's superior few-shot performance stems from direct leveraging of pre-trained knowledge and the immediate guidance of input–label pairs in the prompt, enabling fast adaptation without weight updates. IT can be more sensitive to limited data (overfitting/instability) in few-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Six open-source LLMs (Qwen2-7B-Instruct, Baichuan2-7B-Chat, GLM4-9B-chat, Meta-llama3-8B-instruct, Gemma-2-9B-it, Phi-3-Small-128K-Instruct). Datasets: Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop. Train/validation/test splits: 70/10/20 stratified. For few-shot, n examples per class sampled from training set with five random seeds; evaluation on designated 20% test set. Metrics: Accuracy (Acc) and macro-F1 (F1) averaged across seeds and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complaint_32shot_ICL_vs_IT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complaint task — 32-shot In-Context Learning versus Instruction Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task-level example showing a large ICL advantage: in the Complaint classification task under the 32-shot setting, ICL substantially outperforms IT across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate across six LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B-9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complaint</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect whether a tweet contains a complaint (mismatch between reality and expectations).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL, 32-shot (n=32 examples per class included as input-label Samples in the prompt). Prompts include Instruction and Constraints; five random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>IT (LoRA fine-tuning using same few-shot training set, hyperparameters as in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ICL average accuracy across six LLMs: 85.2% (32-shot, averaged over seeds and models as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>IT average accuracy (32-shot, same models/tasks) is 5.7 percentage points lower than ICL (i.e., approximately 79.5% by subtraction as reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+5.7% accuracy (ICL vs. IT) in 32-shot Complaint (average across six LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>ICL benefits from explicit input–label pairs that guide inference; IT may not generalize well with limited/imbalanced few-shot fine-tuning data and can overfit or become unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>32-shot per class sampling from training split; five random seeds; evaluation metrics accuracy and macro-F1 averaged across seeds and models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_Strategy_1shot_ICL_CoT_Zero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of prompting strategies (1-shot): ICL vs Chain-of-Thought (CoT) vs Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Head-to-head comparison of three prompting strategies in the 1-shot setting showing ICL yields highest accuracy, CoT slightly better than zero-shot but worse than ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate across six LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B-9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate CSS tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same five CSS classification tasks evaluated under 1-shot prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL (1-shot): single labeled example included in prompt as a sample; prompts include Instruction and Constraints. CoT (1-shot): GPT-4 generated chain-of-thought descriptions appended for each sample (Instruction + CoT + Text). Zero-shot: Instruction + Constraints + Text only (no example). All evaluated with five seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Each other (ICL vs CoT vs Zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ICL achieved the highest accuracy and F1 in 1-shot aggregated results. Exact aggregate effect sizes reported: ICL vs CoT: ICL accuracy higher by 3.9 percentage points; CoT vs Zero-shot: CoT higher by 1.1 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT outperforms zero-shot by +1.1% accuracy; Zero-shot yields lowest accuracy/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.9% acc (ICL vs CoT), +1.1% acc (CoT vs Zero-shot) in 1-shot (aggregated across tasks/models).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that including input–label pairs helps focus the model on task-specific content. CoT may underperform ICL because automatically generated CoT explanations (from GPT-4) vary in quality and may introduce noise or unnecessarily complicate a relatively simple classification problem.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison restricted to 1-shot setting only (paper states no strong correlation found between number of samples and prompting strategies in preliminaries). CoT descriptions were auto-generated by ChatGPT-4 and appended to prompts; no manual quality control of CoT. Temperature set to 0.2 for ICL runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma2_IT_over_ICL_exception</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 9B: instruction tuning outperforms in-context learning (exception case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrary to the general trend, Gemma-2 (9B) exhibited superior performance when instruction-tuned vs using ICL in the 1-shot setting, with a large advantage for IT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-9B-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate CSS tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same five CSS classification tasks; result described as Gemma2's IT performance surpasses ICL across all tasks in 1-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL (1-shot) prompts with one sample in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>IT: LoRA-based instruction tuning with few-shot training (1-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that for Gemma2 in the 1-shot setting, IT average accuracy exceeds ICL by 10.2 percentage points (aggregated across tasks/models for Gemma2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>IT > ICL by +10.2% accuracy (1-shot, Gemma2 aggregated across tasks). Specific per-task numbers appear in tables but the text highlights the +10.2% average margin.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+10.2% accuracy advantage for IT over ICL (Gemma2, 1-shot, averaged across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note this as a noteworthy exception; they do not provide a definitive cause but suggest that model-specific idiosyncrasies and the interaction between model pretraining / instruction-following priors and LoRA-based fine-tuning can lead to reversed trends.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gemma-2 was evaluated similarly to other models: 1-shot sampling, five random seeds, LoRA instruction tuning with dropout 0.1, lr 1e-4, scale 32, rank 8, 3 epochs (bf16).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample_Size_Effects_phi3_llama3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of increasing few-shot sample size (examples: Phi-3 and Llama3 declines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper reports that simply increasing the number of few-shot samples does not reliably improve performance and can cause declines; examples include Phi-3 and Llama3 accuracy drops between 1-shot and 32-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-3-Small-128K-Instruct; Meta-llama3-8B-instruct (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Phi-3: 9B (as listed among 7B–9B family); Llama3: 8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate CSS tasks (averaged across tasks/models for the examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate evaluation of model classification performance under increasing n-shot sample sizes (n in {1,8,16,32}).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL (Phi-3 example) and IT (Llama3 example) across multiple n-shot settings (1-shot and 32-shot reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Performance compared across different sample sizes (1-shot vs 32-shot) for same model and format (ICL or IT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Phi-3 with ICL: accuracy 74.7% at 1-shot → 69.4% at 32-shot (drop of 5.3 percentage points) as reported. Llama3 with IT: accuracy 53.9% at 1-shot → 52.4% at 32-shot (drop of 1.5 percentage points) as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Direct within-model comparisons across sample sizes show decreases; no consistent monotonic improvement with more samples across models/formats.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Phi-3 (ICL): −5.3% acc (1-shot → 32-shot). Llama3 (IT): −1.5% acc (1-shot → 32-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that contextual/sample diversity matters more than raw quantity in few-shot settings; adding many similar samples reduces feature diversity and may harm learning; distribution mismatch between few-shot samples and pretraining data can adversely affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>n-shot settings n ∈ {1,8,16,32}; five random seeds per shot; average accuracy reported across seeds. Models evaluated on five CSS datasets; ICL prompts included samples, IT used LoRA fine-tuning on the sampled few-shot training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task_Wise_Performance_variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-dependent variation in format effectiveness (Sarcasm & Rumour Stance low performance example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper documents that format effects vary by task; some tasks (Sarcasm, Rumour Stance) yield much lower accuracy overall under all formats, indicating task difficulty and prompting sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate across evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B-9B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sarcasm; Rumour Stance (task-level examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sarcasm detection and rumour stance classification (multi-class: support, deny, query, comment) which require deeper semantic and pragmatic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ICL vs IT across several n-shot settings (paper references aggregated ICL accuracies: Sarcasm ~57.2%, Rumour Stance ~41.4% in reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ICL compared with IT; zero-shot and CoT also compared in separate experiments (1-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported aggregated ICL average accuracies: Sarcasm ≈ 57.2%, Rumour Stance ≈ 41.4% (the paper highlights these are much lower than tasks like Complaint which reached ≈85.2% under 32-shot ICL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ICL outperforms IT generally for these tasks as well, but absolute performance remains low relative to other tasks — indicating format alone does not overcome intrinsic task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single effect size provided per-model here beyond the aggregate ICL>IT differences; paper notes substantially lower absolute accuracies for Sarcasm and Rumour Stance vs. Complaint.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute low performance to the need for deeper semantic/pragmatic comprehension in these tasks; zero-shot lacks task-specific context while CoT may add noise; ICL helps but is not sufficient to reach high absolute accuracy for such tasks in few-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Aggregated numbers reported across six LLMs and seed-averaging; tasks evaluated under the same n-shot regimes; metrics accuracy and macro-F1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Duan et al. (2023) <em>(Rating: 2)</em></li>
                <li>Mosbach et al. (2023) <em>(Rating: 2)</em></li>
                <li>Brown et al. (2020) <em>(Rating: 2)</em></li>
                <li>Ouyang et al. (2022) <em>(Rating: 2)</em></li>
                <li>Mu et al. (2024) <em>(Rating: 2)</em></li>
                <li>Ziems et al. (2024) <em>(Rating: 1)</em></li>
                <li>Qin et al. (2024) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9237",
    "paper_id": "paper-272828099",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "ICL_vs_IT_aggregate",
            "name_full": "In-Context Learning versus Instruction Tuning (aggregate comparison)",
            "brief_description": "Aggregate comparison across six open-source LLMs (7B–9B) on five computational social science (CSS) classification tasks showing ICL generally outperforms instruction tuning in few-shot settings.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Multiple open-source LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)",
            "model_size": "7B-9B",
            "task_name": "Aggregate CSS classification tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)",
            "task_description": "Five social-media / news classification benchmarks requiring semantic/stance/credibility judgments on short text (tweets) or news articles.",
            "presentation_format": "In-Context Learning (ICL): few-shot prompts that include Instruction, Constraints, several input-label Samples, and the target Text. ICL experiments used n-shot with n in {1,8,16,32}, five random seeds per shot; temperature set to 0.2; GossipCop articles truncated to 256 tokens.",
            "comparison_format": "Instruction Tuning (IT): instruction prompts (Instruction + Constraints + Text) used to fine-tune each model with LoRA (dropout 0.1, lr 1e-4, scaling factor 32, rank 8) for three epochs in bf16; same few-shot sampling (n ∈ {1,8,16,32}) used to construct IT training sets, five random seeds.",
            "performance": "Aggregate (mean across tasks and models): ICL average accuracy advantage vs. IT of +3.3% (1-shot), +2.9% (8-shot), +3.2% (16-shot), +3.7% (32-shot). Metrics reported: accuracy and macro-F1 averaged across five seeds and five tasks.",
            "performance_comparison": "IT baseline aggregated across same models/tasks; aggregated IT accuracies are lower by the effect sizes above (paper reports the % differences rather than per-format absolute aggregated numbers in-text).",
            "format_effect_size": "+3.3% acc (1-shot), +2.9% (8-shot), +3.2% (16-shot), +3.7% (32-shot) in favor of ICL (averaged over six LLMs and five tasks).",
            "explanation_or_hypothesis": "Authors hypothesize ICL's superior few-shot performance stems from direct leveraging of pre-trained knowledge and the immediate guidance of input–label pairs in the prompt, enabling fast adaptation without weight updates. IT can be more sensitive to limited data (overfitting/instability) in few-shot regimes.",
            "null_or_negative_result": false,
            "experimental_details": "Six open-source LLMs (Qwen2-7B-Instruct, Baichuan2-7B-Chat, GLM4-9B-chat, Meta-llama3-8B-instruct, Gemma-2-9B-it, Phi-3-Small-128K-Instruct). Datasets: Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop. Train/validation/test splits: 70/10/20 stratified. For few-shot, n examples per class sampled from training set with five random seeds; evaluation on designated 20% test set. Metrics: Accuracy (Acc) and macro-F1 (F1) averaged across seeds and tasks.",
            "uuid": "e9237.0",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Complaint_32shot_ICL_vs_IT",
            "name_full": "Complaint task — 32-shot In-Context Learning versus Instruction Tuning",
            "brief_description": "Task-level example showing a large ICL advantage: in the Complaint classification task under the 32-shot setting, ICL substantially outperforms IT across models.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Aggregate across six LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)",
            "model_size": "7B-9B",
            "task_name": "Complaint",
            "task_description": "Detect whether a tweet contains a complaint (mismatch between reality and expectations).",
            "presentation_format": "ICL, 32-shot (n=32 examples per class included as input-label Samples in the prompt). Prompts include Instruction and Constraints; five random seeds.",
            "comparison_format": "IT (LoRA fine-tuning using same few-shot training set, hyperparameters as in paper).",
            "performance": "ICL average accuracy across six LLMs: 85.2% (32-shot, averaged over seeds and models as reported).",
            "performance_comparison": "IT average accuracy (32-shot, same models/tasks) is 5.7 percentage points lower than ICL (i.e., approximately 79.5% by subtraction as reported in text).",
            "format_effect_size": "+5.7% accuracy (ICL vs. IT) in 32-shot Complaint (average across six LLMs).",
            "explanation_or_hypothesis": "ICL benefits from explicit input–label pairs that guide inference; IT may not generalize well with limited/imbalanced few-shot fine-tuning data and can overfit or become unstable.",
            "null_or_negative_result": false,
            "experimental_details": "32-shot per class sampling from training split; five random seeds; evaluation metrics accuracy and macro-F1 averaged across seeds and models.",
            "uuid": "e9237.1",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prompt_Strategy_1shot_ICL_CoT_Zero",
            "name_full": "Comparison of prompting strategies (1-shot): ICL vs Chain-of-Thought (CoT) vs Zero-shot",
            "brief_description": "Head-to-head comparison of three prompting strategies in the 1-shot setting showing ICL yields highest accuracy, CoT slightly better than zero-shot but worse than ICL.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Aggregate across six LLMs (Qwen2, Baichuan2, GLM4, Llama3, Gemma2, Phi-3)",
            "model_size": "7B-9B",
            "task_name": "Aggregate CSS tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)",
            "task_description": "Same five CSS classification tasks evaluated under 1-shot prompting strategies.",
            "presentation_format": "ICL (1-shot): single labeled example included in prompt as a sample; prompts include Instruction and Constraints. CoT (1-shot): GPT-4 generated chain-of-thought descriptions appended for each sample (Instruction + CoT + Text). Zero-shot: Instruction + Constraints + Text only (no example). All evaluated with five seeds.",
            "comparison_format": "Each other (ICL vs CoT vs Zero-shot).",
            "performance": "ICL achieved the highest accuracy and F1 in 1-shot aggregated results. Exact aggregate effect sizes reported: ICL vs CoT: ICL accuracy higher by 3.9 percentage points; CoT vs Zero-shot: CoT higher by 1.1 percentage points.",
            "performance_comparison": "CoT outperforms zero-shot by +1.1% accuracy; Zero-shot yields lowest accuracy/F1.",
            "format_effect_size": "+3.9% acc (ICL vs CoT), +1.1% acc (CoT vs Zero-shot) in 1-shot (aggregated across tasks/models).",
            "explanation_or_hypothesis": "Authors suggest that including input–label pairs helps focus the model on task-specific content. CoT may underperform ICL because automatically generated CoT explanations (from GPT-4) vary in quality and may introduce noise or unnecessarily complicate a relatively simple classification problem.",
            "null_or_negative_result": false,
            "experimental_details": "Comparison restricted to 1-shot setting only (paper states no strong correlation found between number of samples and prompting strategies in preliminaries). CoT descriptions were auto-generated by ChatGPT-4 and appended to prompts; no manual quality control of CoT. Temperature set to 0.2 for ICL runs.",
            "uuid": "e9237.2",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemma2_IT_over_ICL_exception",
            "name_full": "Gemma-2 9B: instruction tuning outperforms in-context learning (exception case)",
            "brief_description": "Contrary to the general trend, Gemma-2 (9B) exhibited superior performance when instruction-tuned vs using ICL in the 1-shot setting, with a large advantage for IT.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Gemma-2-9B-it",
            "model_size": "9B",
            "task_name": "Aggregate CSS tasks (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop)",
            "task_description": "Same five CSS classification tasks; result described as Gemma2's IT performance surpasses ICL across all tasks in 1-shot.",
            "presentation_format": "ICL (1-shot) prompts with one sample in the prompt.",
            "comparison_format": "IT: LoRA-based instruction tuning with few-shot training (1-shot).",
            "performance": "Paper reports that for Gemma2 in the 1-shot setting, IT average accuracy exceeds ICL by 10.2 percentage points (aggregated across tasks/models for Gemma2).",
            "performance_comparison": "IT &gt; ICL by +10.2% accuracy (1-shot, Gemma2 aggregated across tasks). Specific per-task numbers appear in tables but the text highlights the +10.2% average margin.",
            "format_effect_size": "+10.2% accuracy advantage for IT over ICL (Gemma2, 1-shot, averaged across tasks).",
            "explanation_or_hypothesis": "Authors note this as a noteworthy exception; they do not provide a definitive cause but suggest that model-specific idiosyncrasies and the interaction between model pretraining / instruction-following priors and LoRA-based fine-tuning can lead to reversed trends.",
            "null_or_negative_result": true,
            "experimental_details": "Gemma-2 was evaluated similarly to other models: 1-shot sampling, five random seeds, LoRA instruction tuning with dropout 0.1, lr 1e-4, scale 32, rank 8, 3 epochs (bf16).",
            "uuid": "e9237.3",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Sample_Size_Effects_phi3_llama3",
            "name_full": "Effects of increasing few-shot sample size (examples: Phi-3 and Llama3 declines)",
            "brief_description": "Paper reports that simply increasing the number of few-shot samples does not reliably improve performance and can cause declines; examples include Phi-3 and Llama3 accuracy drops between 1-shot and 32-shot.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Phi-3-Small-128K-Instruct; Meta-llama3-8B-instruct (examples)",
            "model_size": "Phi-3: 9B (as listed among 7B–9B family); Llama3: 8B",
            "task_name": "Aggregate CSS tasks (averaged across tasks/models for the examples cited)",
            "task_description": "Aggregate evaluation of model classification performance under increasing n-shot sample sizes (n in {1,8,16,32}).",
            "presentation_format": "ICL (Phi-3 example) and IT (Llama3 example) across multiple n-shot settings (1-shot and 32-shot reported).",
            "comparison_format": "Performance compared across different sample sizes (1-shot vs 32-shot) for same model and format (ICL or IT).",
            "performance": "Phi-3 with ICL: accuracy 74.7% at 1-shot → 69.4% at 32-shot (drop of 5.3 percentage points) as reported. Llama3 with IT: accuracy 53.9% at 1-shot → 52.4% at 32-shot (drop of 1.5 percentage points) as reported.",
            "performance_comparison": "Direct within-model comparisons across sample sizes show decreases; no consistent monotonic improvement with more samples across models/formats.",
            "format_effect_size": "Phi-3 (ICL): −5.3% acc (1-shot → 32-shot). Llama3 (IT): −1.5% acc (1-shot → 32-shot).",
            "explanation_or_hypothesis": "Authors hypothesize that contextual/sample diversity matters more than raw quantity in few-shot settings; adding many similar samples reduces feature diversity and may harm learning; distribution mismatch between few-shot samples and pretraining data can adversely affect performance.",
            "null_or_negative_result": true,
            "experimental_details": "n-shot settings n ∈ {1,8,16,32}; five random seeds per shot; average accuracy reported across seeds. Models evaluated on five CSS datasets; ICL prompts included samples, IT used LoRA fine-tuning on the sampled few-shot training sets.",
            "uuid": "e9237.4",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Task_Wise_Performance_variation",
            "name_full": "Task-dependent variation in format effectiveness (Sarcasm & Rumour Stance low performance example)",
            "brief_description": "Paper documents that format effects vary by task; some tasks (Sarcasm, Rumour Stance) yield much lower accuracy overall under all formats, indicating task difficulty and prompting sensitivity.",
            "citation_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
            "mention_or_use": "use",
            "model_name": "Aggregate across evaluated LLMs",
            "model_size": "7B-9B",
            "task_name": "Sarcasm; Rumour Stance (task-level examples)",
            "task_description": "Sarcasm detection and rumour stance classification (multi-class: support, deny, query, comment) which require deeper semantic and pragmatic understanding.",
            "presentation_format": "ICL vs IT across several n-shot settings (paper references aggregated ICL accuracies: Sarcasm ~57.2%, Rumour Stance ~41.4% in reported comparisons).",
            "comparison_format": "ICL compared with IT; zero-shot and CoT also compared in separate experiments (1-shot).",
            "performance": "Reported aggregated ICL average accuracies: Sarcasm ≈ 57.2%, Rumour Stance ≈ 41.4% (the paper highlights these are much lower than tasks like Complaint which reached ≈85.2% under 32-shot ICL).",
            "performance_comparison": "ICL outperforms IT generally for these tasks as well, but absolute performance remains low relative to other tasks — indicating format alone does not overcome intrinsic task difficulty.",
            "format_effect_size": "No single effect size provided per-model here beyond the aggregate ICL&gt;IT differences; paper notes substantially lower absolute accuracies for Sarcasm and Rumour Stance vs. Complaint.",
            "explanation_or_hypothesis": "Authors attribute low performance to the need for deeper semantic/pragmatic comprehension in these tasks; zero-shot lacks task-specific context while CoT may add noise; ICL helps but is not sufficient to reach high absolute accuracy for such tasks in few-shot regimes.",
            "null_or_negative_result": false,
            "experimental_details": "Aggregated numbers reported across six LLMs and seed-averaging; tasks evaluated under the same n-shot regimes; metrics accuracy and macro-F1.",
            "uuid": "e9237.5",
            "source_info": {
                "paper_title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Duan et al. (2023)",
            "rating": 2,
            "sanitized_title": "duan_et_al_2023"
        },
        {
            "paper_title": "Mosbach et al. (2023)",
            "rating": 2,
            "sanitized_title": "mosbach_et_al_2023"
        },
        {
            "paper_title": "Brown et al. (2020)",
            "rating": 2,
            "sanitized_title": "brown_et_al_2020"
        },
        {
            "paper_title": "Ouyang et al. (2022)",
            "rating": 2,
            "sanitized_title": "ouyang_et_al_2022"
        },
        {
            "paper_title": "Mu et al. (2024)",
            "rating": 2,
            "sanitized_title": "mu_et_al_2024"
        },
        {
            "paper_title": "Ziems et al. (2024)",
            "rating": 1,
            "sanitized_title": "ziems_et_al_2024"
        },
        {
            "paper_title": "Qin et al. (2024)",
            "rating": 2,
            "sanitized_title": "qin_et_al_2024"
        }
    ],
    "cost": 0.014943749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</p>
<p>Taihang Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xiaoman Xu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yimin Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Ye Jiang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Tom Brown 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Benjamin Mann 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Nick Ryder 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Melanie Subbiah 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jared D Kaplan 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Prafulla Dhariwal 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Arvind Neelakantan 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Pranav Shyam 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Girish Sastry 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Aohan Glm 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Bin Zeng 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Bowen Xu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Chen- Hui Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Da Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Diego Yin 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Guanyu Rojas 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Han- Lin Feng 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Hanyu Zhao 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Hao Lai 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Hongning Yu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Ji- Adai Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jiajie Sun 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jiale Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jiayi Cheng 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jie Gui 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Jing Tang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Juanzi Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Lei Li 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Lindong Zhao 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Lucen Wu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Mingdao Zhong 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Minlie Liu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Peng Huang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Qinkai Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Rui Zheng 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Shuaiqi Lu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Shu- Dan Duan 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Shulin Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Shuxun Cao 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>WengLam Yang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Wenyi Tam 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xiao Zhao 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xiao Liu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xiaohan Xia 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xiaotao Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xin Gu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xinghan Lv 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xinyi Liu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xinyue Liu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xixuan Yang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Xunkai Song 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yifan Zhang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yifan An 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yilin Xu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yuantao Niu 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yueyan Yang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yushi Li 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Yuxiao Bai 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zehan Dong 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zhaoyu Qi 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zhen Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zhengxiao Yang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zhenyu Du 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Zihan Hou 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Wang 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Chatglm 
College of Information Science and Technology ♣ College of Data Science
Qingdao University of Science and Technology China</p>
<p>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science
4C528400450F8276CFBE5694D0B915B1arXiv:2406.12793.
Real-world applications of large language models (LLMs) in computational social science (CSS) tasks primarily depend on the effectiveness of instruction tuning (IT) or in-context learning (ICL).While IT has shown highly effective at fine-tuning LLMs for various tasks, ICL offers a rapid alternative for task adaptation by learning from examples without explicit gradient updates.In this paper, we evaluate the classification performance of LLMs using IT versus ICL in few-shot CSS tasks.The experimental results indicate that ICL consistently outperforms IT in most CSS tasks.Additionally, we investigate the relationship between the increasing number of training samples and LLM performance.Our findings show that simply increasing the number of samples without considering their quality does not consistently enhance the performance of LLMs with either ICL or IT and can sometimes even result in a performance decline.Finally, we compare three prompting strategies, demonstrating that ICL is more effective than zero-shot and Chainof-Thought (CoT).Our research highlights the significant advantages of ICL in handling CSS tasks in few-shot settings and emphasizes the importance of optimizing sample quality and prompting strategies to improve LLM classification performance.The code will be made available.</p>
<p>Introduction</p>
<p>Instruction tuning (IT) of large language models (LLMs) has shown exceptional capability in understanding language across various tasks (Ouyang et al., 2022).However, the large number parameters of LLMs makes it challenging to transfer the pre-trained knowledge to downstream tasks (Naveed et al., 2023;Xu et al., 2024).Alternatively, in-context learning (ICL) enables LLMs to * Corresponding author perform downstream tasks by conditioning on taskspecific prompts, thus eliminating the need for explicit gradient updates (Dong et al., 2022;Wang et al., 2024b;Jiang, 2023).Recent successful deployment of LLMs in practical applications largely hinges on the effectiveness of the ICL and the IT.</p>
<p>Previous studies have extensively assessed the zero-shot capabilities of LLMs in computational social science (CSS) tasks, including hate speech detection (Roy et al., 2023) and rumour stance detection (Yang et al., 2024b).However, CSS is a dynamic research area that involves detailed linguistic analysis and deep semantic comprehension.Direct zero-shot prompting LLMs to CSS tasks may even underperform compared to fully fine-tuned, task-specific smaller models like BERT (Juan José Bucher and Martini, 2024).Meanwhile, studies on ICL and IT typically occur independently, with direct comparisons between these approaches often overlooked.</p>
<p>To address the above issues, this paper raises the following research questions (RQ):</p>
<p>• RQ 1: What are the performance differences between LLMs with ICL and IT in few-shot CSS tasks?</p>
<p>• RQ 2: How do varying numbers of sample influence the performance of LLMs with ICL and IT?</p>
<p>• RQ 3: How different prompting strategies affect the proficiency of LLMs in CSS tasks?</p>
<p>To answer the above questions, we extensively investigate six open-source LLMs in a total of five publicly accessible social media datasets within n-shot settings, where n ∈ {1, 8, 16, 32}.</p>
<p>Initially, we compare the few-shot classification performance of LLMs with ICL and IT separately.We then assess how performance varies with an increase in the number of samples.Lastly, we arXiv:2409.14673v1[cs.CL] 23 Sep 2024 apply three prompting strategies including zeroshot, ICL and chain-of-thought (CoT), and examine their effects on performance.Additionally, except zero-shot setting, all experiments are conducted using five random seeds to account for the potential impact of few-shot sample quality on performance.</p>
<p>Our main findings are:</p>
<p>-(I) In few-shot settings, the performance of LLMs with ICL generally surpasses that of LLMs with IT on five CSS tasks.</p>
<p>-(II) Merely increasing the sample size (from 1-shot to 32-shot in our experiments) does not consistently improve the performance of LLMs either with ICL or IT, and even leads to a performance decline in some cases.</p>
<p>-(III) ICL prompting still outperforms zeroshot and CoT strategies, indicating that excessively complex prompting strategies can potentially hinder performance.</p>
<p>2 Related Work However, the aforementioned studies primarily assess IT in data-rich or zero-shot settings, leaving the few-shot performance of IT relatively underexplored.</p>
<p>Comparison between instruction tuning and in-context learning</p>
<p>ICL enables LLMs to quickly adapt to tasks by learning from samples without updating the model's weights (Yang et al., 2023b;Brown et al., 2020).Recent studies have also focused on exploring the connections between IT and ICL.For example, Mosbach et al. (2023) evaluates the generalization capabilities of Pattern-based fine-tuning (PBFT) and ICT for out-of-domain (OOD) tasks under the same experimental settings in a few-shot context.They find that PBFT achieves better generalization.Duan et al. (2023) investigates how ICL and IT modify the hidden layer states of LLMs to achieve task adaptability in LLMs, finding that ICL is implicit IT.Our work differs from previous research in that we directly compare the classification performance between the ICL and IT in various CSS tasks.</p>
<p>Large language models in computational social science</p>
<p>LLMs have demonstrated exceptional capabilities in CSS (Møller and Aiello, 2024;Jiang, 2023;Xu et al., 2024;Jiang et al., 2023).For example, Ziems et al. (2024) has outlined a roadmap for using LLMs as tools for CSS, recommending best practices for prompting and conducting an extensive evaluation of the zero-shot performance of thirteen language models across twenty-four representative CSS benchmark tasks.Additionally, Mu et al. (2024) has assessed the zero-shot performance of two LLMs under six CSS tasks, while also researching the effects of various prompting strategies.However, the emerging CSS topics demand that LLMs quickly adapt to limited annotated data (Jiang et al., 2024), therefore it is crucial to evaluate their few-shot performance in CSS tasks.</p>
<p>Our work aims to explore the performance differences between ICL and IT in CSS tasks within few-shot settings, as well as how to enhance the capabilities of LLMs.</p>
<p>3 Methodology</p>
<p>Instruction tuning for CSS</p>
<p>Following the IT strategy outlined by (Duan et al., 2023), we first create a task-specific Instruction (e.g., "Analyze the content and determine if it includes label", where label represents task-specific labels) to define the objective of each task.We then incorporate a set of Constraints (e.g., "Respond only with label or not label, without offering additional context or explanation") to guide the LLMs' responses.The detailed workflow of the IT process is presented in Figure 1 (a) and Appendix A.2. Considering the computational efficiency and challenges of fine-tuning LLMs, we employ LoRA (Hu et al., 2021) for instruction-tuning across all models.Specifically, we set the dropout probability at 0.1 and the learning rate at 1e-4.As recommended by Duan et al. (2023), the scaling factor is set to 32, with a rank of 8.The models are finetuned over three epochs using Brain Floating Point 16 (bf16) precision.</p>
<p>In-context learning for CSS</p>
<p>In accordance with the in-context learning prompts described by Jiang and Wang (2024), we create input prompts consisting of Instruction, Constraints, Samples (e.g., "Tweet: How to not miss someone who doesn't even know you.Label: not bragging"), and Text (e.g., "Tweet: For real, I just want to be prescribed something..., and what I'm all about.Label: ").The detailed workflow of ICL is depicted in Figure 1 (b).</p>
<p>Given the limited fixed context length of LLMs, for the GossipCop dataset, we manually truncate the length of news articles to 256 tokens.Preliminary experiments revealed that higher temperature settings introduced more randomness in the model's responses.Hence, following the approach of Mu et al. ( 2024), we apply a reduced temperature of 0.2 to enhance the model's focus and stability.</p>
<p>Comparing in different prompting strategies</p>
<p>In the zero-shot setting, we compose the prompt by combining Instruction, Constraints, and Text.</p>
<p>For ICL, the detailed workflow is presented in Section 3.2.Inspired by Dogan et al. (2024), we utilize the ChatGPT-4 model1 to automatically generate CoT descriptions for each sample.For example, we input the tweet along with prompts in Bragging (e.g., "Analyze the content and determine if it includes a bragging statement by using the CoT method.Tweet: For a minute I was tired of being the bigger man, until I realized that's just who I am").</p>
<p>Experimental setups 4.1 Data</p>
<p>To assess the classification performance of LLMs, five publicly available datasets are selected, encompassing a broad spectrum of computational social science topics.The statistics of these datasets are presented in Table 1.Bragging (Jin et al., 2022) : This dataset is designed to facilitate a comprehensive semantic analysis of tweets to ascertain whether they contain narratives of bragging, specifically identifying the subject of the author's boast.</p>
<p>Complaint (Preoţiuc-Pietro et al., 2019) : This task aims to identify whether tweets from social media contain complaints, where the complaint content expresses a mismatch between reality and expectations in a specific context.</p>
<p>Sarcasm (Farha et al., 2022) : This task aims to conduct semantic analysis on texts to determine whether they contain sarcasm.</p>
<p>Rumour Stance (Derczynski et al., 2017) : This task aims to perform semantic analysis on tweets (rumours) in social media to assess the stance classification of the rumours.</p>
<p>GossipCop (Shu et al., 2020) : This task aims to perform semantic analysis on news articles in entertainment media to determine the authenticity of the news articles.</p>
<p>For each benchmark task, we utilize stratified random sampling to divide the dataset into 70% for training, 10% for validation, and 20% for testing.The 10% validation set is used for hyperparameter tuning during the instruction tuning, and the</p>
<p>Baselines</p>
<p>To ensure a fair comparison of LLMs in CSS tasks, we utilize Huggingface2 to select six different open-source LLMs, with model sizes ranging from 7B to 9B, namely Qwen2-7B-Instruct (Qwen2) (Yang et al., 2024a), Baichuan2-7B-Chat (Baichuan2) (Yang et al., 2023a), GLM4-9B-chat (GLM4) (GLM et al., 2024), Meta-llama3-8Binstruct (LLama3) (Meta, 2024), Gemma-2-9B-it (Gemma2) (Team et al., 2024), and Phi-3-Small-128K-Instruct (Phi-3) (Abdin et al., 2024).</p>
<p>Results</p>
<p>The overall experimental results are presented in Table 2 and Table 3.For each n-shot setting, we evaluate the LLMs by computing the average accuracy (Acc) and macro-F1 (F1) scores across five random seeds3 .</p>
<p>Comparing between IT and ICL: We first calculate the average accuracy and F1 scores across five seeds for each model, and then compute the means of these scores across all CSS tasks.The averaged scores are presented in Table 2.We observe that the overall classification performance of LLMs with ICL is significantly better than that of LLMs with IT.For instance, ICL outperforms IT by 3.3% in accuracy in the 1-shot setting.Similarly, LLMs with ICL consistently outperform LLMs with IT in the 8, 16, and 32-shot settings, with accuracy improvements of 2.9%, 3.2%, and 3.7%, respectively.</p>
<p>We also examine how different tasks impact the performance of ICL and IT, as presented in Table 3.In the Bragging and Complaint tasks, ICL consistently outperforms IT, achieving higher accuracy across all six models.For instance, ICL attains an average accuracy of 85.2% across six LLMs, which is 5.7% higher than IT in the 32shot setting for the Complaint task.This advantage is also evident in GossipCop, Sarcasm, and Rumour Stance.However, it is noteworthy that LLMs demonstrate relatively lower performance in the latter two benchmark tasks (e.g., Sarcasm and Rumour Stance) compared to others.For example, the average accuracy of ICL in Sarcasm and Rumour Stance is 57.2% and 41.4%, respectively, which is significantly lower than the 85.2% achieved in the Complaint task under the 32-shot setting.</p>
<p>Comparing between LLMs: We also assess the ability of six LLMs to address CSS tasks using ICL and IT in few-shot settings, as presented in Table 3.We observe that Phi-3 outperforms the others in most tasks, achieving the highest average accuracy in the Bragging and Complaint tasks, with scores of 88.1% and 89.5%, respectively.Comparing between different n-shot settings: Figure 2 illustrates the overall performance of the LLMs with ICL and IT in different n-shot settings.We compute the average accuracy of the six LLMs across all tasks.The experimental results show that the performance of LLMs with either ICL or IT does not consistently improve as the number of training samples increases, and even declines in some cases.For example, the average accuracy of Phi-3 with ICL is 74.7% in the 1-shot setting, but drops to 69.4% in the 32-shot setting.Similarly, the accuracy of Llama3 with IT decreases from 53.9% in the 1-shot setting to 52.4% in the 32-shot setting.</p>
<p>Comparing between different prompt strategies: To assess the impact of prompting strategies on the inferential capabilities of LLMs, we compare three prompting approaches: zero-shot, ICL, and CoT in the 1-shot setting.Note that this comparison is not conducted in other n-shot settings, as we found no strong correlation between the number of samples and prompting strategies, based on the preliminary findings.</p>
<p>The performance of these three prompting strategies is shown in Table 4.We observe that ICL prompting consistently achieves the highest accuracy and F1 scores.Specifically, ICL surpasses CoT in accuracy by 3.9%.CoT, in turn, outperforms zero-shot by 1.1% in accuracy.Lastly, zeroshot exhibits the lowest accuracy and F1 scores.</p>
<p>Analysis</p>
<p>The experimental results underscore the proficiency of LLMs in CSS tasks that require comprehension of complex real-world contexts.Next, we will contextualize these findings within the framework of our three research questions: (RQ1) What are the performance differences between LLMs with ICL and IT in few-shot CSS tasks?</p>
<p>The experimental results reveal that LLMs with ICL generally outperform those with IT in few-shot CSS tasks.ICL exhibits strong adaptability, likely due to the extensive knowledge acquired during the pre-training phase.This allows the model to comprehend and swiftly adapt to complex tasks by leveraging pre-trained knowledge.While IT also enhances LLM capabilities through instructions, its performance is comparatively more sensitive than that of ICL, as illustrated in Figure 2.</p>
<p>Additionally, ICL enables the model to directly leverage the input-label pairs provided in the samples to guide inference without requiring gradient updates.For LLMs with IT, insufficient training samples can lead to overfitting, instability, and may even impair the inferential capacity of the models.For example, the average accuracies of GLM4 and Llama3 are 56.0%and 53.9% in the 1-shot setting.However, both models achieve higher average accuracies of 58.1% and 58.8% in the zero-shot settings, respectively.(RQ2) How do varying numbers of sample influence the performance of LLMs with ICL and IT?</p>
<p>Our experimental results suggest that merely increasing the number of training samples does not consistently improve the performance of LLMs with either ICL or IT, and in some cases, it even leads to a decline.</p>
<p>Given the characteristics of few-shot settings, we speculate that the contextual diversity of samples is more crucial than their quantity, regardless of whether LLMs use IT or ICL.If the additional sam-ples are highly similar in content, LLMs may struggle to learn from the feature diversity in few-shot examples, leading to poor inferential performance.Moreover, when the feature distribution of fewshot samples deviates significantly from that of the pre-trained data, this variation may also negatively affect the classification performance of LLMs.(RQ3) How different prompting strategies affect the proficiency of LLMs in CSS tasks?</p>
<p>LLMs with ICL achieve the highest performance among the three prompting strategies: zero-shot, ICL, and CoT.This indicates that incorporating a small number of input-label pairs into the prompt can help LLMs better focus on task-specific content across various CSS tasks.</p>
<p>Surprisingly, we find that the CoT strategy slightly underperforms compared to ICL.We hypothesize two potential reasons for this: 1) the CoT examples are automatically generated by GPT-4, which may result in varying content quality depending on the context; 2) incorporating CoT descriptions into the prompt might introduce noise and transform a simple classification problem into a more complex language understanding task, as detailed in the Appendix A.3.</p>
<p>Finally, the zero-shot strategy yields the lowest performance.This may be due to insufficient contextual information to guide the model in understanding and performing CSS tasks, which often require deeper semantic comprehension (e.g., Sarcasm and Bragging).Moreover, the zero-shot strategy primarily depends on the pre-trained knowledge of LLMs.The absence of task-specific knowledge during the pre-training phase may cause the model to struggle in identifying appropriate solutions.</p>
<p>Conclusion</p>
<p>In this paper, we first evaluate the performance of LLMs with IT and ICL in few-shot CSS tasks.We also investigate whether increasing the number of training samples affects LLM performance.Lastly, we compare different prompting strategies and analyze their efficiency in few-shot settings.</p>
<p>In our experiments, we evaluate six open-source LLMs on five publicly available CSS datasets.Our results indicate that: 1) LLMs with ICL generally outperform those with IT in tackling complex CSS tasks in few-shot settings; 2) merely increasing the number of samples without considering their quality does not consistently improve the perfor-mance of LLMs with either ICL or IT, and may even lead to a decline in some cases; 3) LLMs with ICL are more effective than those using zero-shot and CoT strategies in few-shot settings, suggesting that overly complex prompting may negatively impact LLM performance.</p>
<p>Overall, our research underscores the substantial advantages of ICL in handling CSS tasks in few-shot settings, highlighting the critical role of optimizing sample quality and prompting strategies to enhance the classification performance of LLMs.</p>
<p>Limitations</p>
<p>This study acknowledges several limitations, including: 1) Due to computational resource constraints and the context length limitations of LLMs, larger n-shot settings remain underexplored.2) Our experiments primarily compare LLMs with parameters ranging between 7B and 9B, due to hardware restrictions.3) The generation of CoT descriptions relies mainly on GPT-4, without manual assessment, which may result in inconsistencies in CoT quality.</p>
<p>Ethic statement</p>
<p>This work has received ethical approval from the Ethics Committee of our university and adheres to the research policies of Twitter.All datasets were obtained via links provided in the respective research papers or directly from the authors upon request.Additionally, we confirm that the data was fully anonymized prior to being used for model inference with the LLMs.Due to the time-intensive and challenging nature of generating the CoT strategy, we solely rely on ChatGPT-4 to automatically generate the CoT descriptions, without manually crafting any CoT strategies.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.2021.Lora: Low-rank adaptation of large language models.arXiv preprint arXiv:2106.09685.</p>
<p>Hanxu Hu, Pinzhen Chen, and Edoardo M Ponti.2024.Fine-tuning large language models with sequential instructions.arXiv preprint arXiv:2403.07794.</p>
<p>Ye Jiang.2023.</p>
<p>A.1 Detailed performance comparison between LLMs on CSS tasks</p>
<p>A.2 Examples of instruction and constraints in CSS tasks</p>
<p>Dataset Instruction Constraint</p>
<p>Bragging Analyze the content and determine if it includes a bragging statement.</p>
<p>Respond only with bragging or not bragging , without providing any additional context or explanation.</p>
<p>Complaint</p>
<p>Analyze the content and determine if it includes a complaint.</p>
<p>Respond only with complaint or not complaint, without providing any additional context or explanation.</p>
<p>Sarcasm</p>
<p>Analyze the content and determine if it includes sarcasm.</p>
<p>Respond only with sarcasm or not sarcasm, without providing any additional context or explanation.</p>
<p>Rumour Stance Given a tweet related to a rumour, classify its stance as one of the following categories: support, deny, query, or comment.Each tweet should only be associated with one stance category based on its content.</p>
<p>Respond only with the appropriate stance category, without providing any additional context or explanation.</p>
<p>GossipCop</p>
<p>Given a news article, classify its truthfulness as either real or fake.</p>
<p>Respond only with real or fake, without providing any additional context or explanation.</p>
<p>Table A1: Detailed information on the instructions and constraints for each benchmark task.</p>
<p>A.3 Examples of CoT description</p>
<p>Tweet CoT Strategy</p>
<p>For a minute I was tired of being the bigger man , until I realized that 's just who I am .</p>
<p>Label: Bragging.Let's break down the analysis.The tweet states, 'For a minute I was tired of being the bigger man, until I realized that's just who I am.'This sentence starts by highlighting a moment of frustration about always being 'the bigger man,' a common expression for acting maturely or taking the high road in difficult situations.The statement concludes by reaffirming the tweeter's inherent nature of being superior or more mature, which implies a self-promotion of personal virtues.The aim is to assess if this self-reflection serves to subtly brag about one's character.Examining the structure and content, the tweet pivots to emphasize personal qualities, suggesting that the individual views these traits as defining and superior.The conclusion, based on the content's focus on self-praise, categorizes this tweet as bragging.</p>
<p>If the market is so efficient , why are n't we green yet ?</p>
<p>Label: Not Bragging.Analyzing the tweet: 'If the market is so efficient, why aren't we green yet?'This tweet poses a critical question about the efficiency of markets in achieving environmental sustainability.It is rhetorical and serves to challenge or provoke thought rather than to boast about personal achievements or qualities.The focus is on questioning societal or economic systems, highlighting a discrepancy between claimed efficiency and environmental outcomes.The purpose of this tweet is to engage others in reflection or debate regarding the effectiveness of market mechanisms in driving sustainable practices, not to showcase any personal accomplishment.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the overall workflow in this paper.(a) The instruction prompts including the context of the tasks (Instruction), the constraints for generating the responses from LLMs (Constraints), and the input text of each task (Text).(b) The ICL prompts include a set of input-label pairs (Samples) to guide the LLMs in focusing on task-specific content.(c) A comparison between different prompting strategies in CSS tasks.</p>
<p>Figure 2 :
2
Figure 2: Illustration of different sample sizes affect the performance of LLMs with ICL and IT respectively.</p>
<p>Figure A1 :
A1
Figure A1: Performance comparison between LLMs on CSS tasks.</p>
<p>Table 1 :
1
Statistics of the selected datasets.CoT descriptions are then combined with Instruction and Text to form input prompts, as illustrated in Figure1 (c).The examples of CoT description are provided in Appendix A.3.</p>
<p>Table 2 :
2
of LLMs under the ICL and IT are evaluated on the designated 20% test set.We apply the same few-shot settings to both the ICL and IT.First, we randomly sample n ∈ {1, 8, 16, 32} examples (where n is the number of samples per class) from the training set.Given the high sensitivity of ICL and IT to the choice of examples, we use five random seeds per shot, repeating the process to assess LLM performance in few-shot scenarios.</p>
<p>The LLMs' performance is compared between ICL and IT in CSS tasks.Scores are first calculated by averaging the accuracy and macro-F1 (Acc/F1) scores (%) across five seeds for each model, followed by computing the mean across five tasks.Bold indicates the highest accuracy, and Underline denotes the best F1 score.performances</p>
<p>Table 3 :
3
The accuracy and macro-F1 (Acc/F1) scores (%) of various LLMs across five benchmark tasks.For each shot, experiments are conducted using five random seeds, and the average values across all seeds are recorded.In each task, Bold indicates the highest accuracy, and Underline is the best F1 score.
1-Shot Setting</p>
<p>Table 4 :
4
The accuracy and macro-F1 (Acc/F1) scores(%) across all benchmark tasks for different models represent the average values from five tasks.Bold and Underline indicate the highest accuracy and F1 among the LLMs in each task respectively.
Baichuan2 and Qwen2 attain the highest averageaccuracy of 78.1% and 75.9% in the Sarcasm andGossipCop tasks, respectively. However, GLM4and Llama3 generally underperform compared tothe others. Additionally, all LLMs exhibit signifi-cant weaknesses in Rumour Stance. Notably, theIT performance of Gemma2 consistently surpassesthat of ICL across all tasks. For instance, in the1-shot setting, the average accuracy of IT exceedsthat of ICL by 10.2%.</p>
<p>Table A2 :
A2
The CoT strategy for Bragging samples.
Qwen2n-shotBraggingComplaintSarcasmRumour StanceGossipCop(seed)ICLITICLITICLITICLITICLITA.4 Detailed experimental results
https://chatgpt.com/
https://huggingface.co/
The detailed experimental results for each seed are presented in Appendix A.4
AcknowledgmentsThis work is funded by the Natural Science Foundation of Shandong Province under grant ZR2023QF151 and the Natural Science Foundation of China under grant 12303103.n-shot (seed) Bragging Complaint Sarcasm Rumour Stance GossipCop ICL IT ICL IT ICL IT ICL IT ICL IT Llama3 n-shot (seed) Bragging Complaint Sarcasm Rumour Stance GossipCop ICL IT ICL IT ICL IT ICL IT ICL IT Phi-3 n-shot (seed) Bragging Complaint Sarcasm Rumour Stance GossipCop ICL IT ICL IT ICL IT ICL IT ICL IT: A family of large language models from glm-130b to glm-4 all tools.Preprint,
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, arXiv:2404.14219Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint</p>
<p>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>The ultimate guide to fine-tuning llms from basics to breakthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities. Balavadhani Venkatesh, Ahtsham Parthasarathy, Aafaq Zafar, Arsalan Khan, Shahid, arXiv:2408.132962024arXiv preprint</p>
<p>Automatically identifying complaints in social media. Daniel Preoţiuc-Pietro, Mihaela Gaman, Nikolaos Aletras, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Unleashing the power of data tsunami: A comprehensive survey on data assessment and selection for instruction tuning of language models. Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun, arXiv:2408.020852024arXiv preprint</p>
<p>Probing llms for hate speech detection: strengths and vulnerabilities. Sarthak Roy, Ashish Harshvardhan, Animesh Mukherjee, Punyajoy Saha, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Kai Shu, Deepak Mahudeswaran, Suhang Wang, Big data. 832020Dongwon Lee, and Huan Liu</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118arXiv preprintet al. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024a</p>
<p>Learning to retrieve in-context examples for large language models. Liang Wang, Nan Yang, Furu Wei, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European Chapter2024b1Long Papers</p>
<p>Team QUST at SemEval-2024 task 8: A comprehensive study of monolingual and multilingual approaches for detecting AIgenerated text. Xiaoman Xu, Xiangrun Li, Taihang Wang, Jianxiang Tian, Ye Jiang, 10.18653/v1/2024.semeval-1.71Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024). the 18th International Workshop on Semantic Evaluation (SemEval-2024)Mexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.10305Open large-scale language models. 2arXiv preprintet al. 2023a. Baichuan</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024aarXiv preprint</p>
<p>Reinforcement tuning for detecting stances and debunking rumors jointly with large language models. Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Bo Wang, arXiv:2406.021432024barXiv preprint</p>
<p>Not all demonstration examples are equally beneficial: Reweighting demonstration examples for in-context learning. Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023b</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.10792Instruction tuning for large language models: A survey. 2023arXiv preprint</p>
<p>Can large language models transform computational social science?. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, Computational Linguistics. 5012024</p>
<p>85.9/87.0 82.1/84.3 89.9/89.9 85.1/85.4 69.5/71.8 55.4/58.9 32.6/35.9 37.2/30.4 76.7/62.0 74.2/61.2Avg. 87 36.5/40.6 35.1/28.6 76.9/64.5 70.7/62.9 8(45</p>
<p>83.1/85.2 75.2/79.3 90.1/90.3 83.5/83.8 68.4/70.9 49.9/52.6 31.1/34.1 38.2/30.5 75.5/60.0 72.7/62.1 16(46) 87.5/88.1 80.0/82.9 88.8/89Avg. 2 52.5/55.5 34.6/38.5 36.5/29.5 76.1/62.8 67.4/61.2 16. 0 84.3/84.7 75.7/75.8 52.3/55.4 42.9/47.2 41.7/32.2 75.9/60.6 70.2/62.4</p>
<p>84.9/86.4 76.8/80.5 89.4/89.5 84.1/84.4 71.5/73.1 53.0/56.1 35.8/39.6 38.6/30.6 76.1/61.9 69.8/61.8 32(42) 87.2/88.0 76.3/80.2 89.9/89.9 84.6/85.0 69.9/71.6 52.2/55.3 40.2/44.4 47.1/37.4 75.4/62.4 67.3/61.3Avg. </p>
<p>86.7/87.7 76.2/80.1 89.9/90.0 84.1/84.5 71.1/72.7 51.7/54.7 39.4/43.4 46.6/36.3 75.8/62.1 70.6/61.6Avg. Table A3: The detailed experimental results for Qwen2</p>
<p>83.0/65.8 72.7/55.2 38.0/31.1 57.8/53.1 78.2/44.4 65.1/54.3 53.1/29.8 35.4/27.0 39.7/39.6 72.0/47.6Avg. 84 55.7/51.9 47.5/28.1 62.9/24.9 39.3/39.2 68.5/53</p>
<p>83.1/65.2 79.0/59.6 64.1/61.5 61.0/53.2 78.2/44.3 60.7/52.9 51.7/29.5 48.6/28.4 39.8/39.6 69.4/52.3 16(42) 77.4/62.2 85.9/63.9 50.1/50.0 62.3/57.1 78.2/44.4 62.954.7 52.9/29.0 41.4/29.3 47.2/46.5 57.8/52.1Avg. </p>
<p>87.2/67.0 79.5/59.8 71.6/65.0 65.3/58.7 78.2/44.4 58.1/52.9 66.2/25.1 38.6/29.1 51.0/48.7 67.7/54.6 32(44) 86.0/68.5 78.5/58.8 68.2/56.1 63.5/56.3 78.2/44.4 64.4/55.5 65.2/25.7 46.1/29.9 43.3/43.0 72.3/48.2 32(45) 87.1/67.4 83.9/62.4 78.1/75.2 60.4/52.0 78.2/44.4 60.8/52Avg. 32</p>
<p>87.1/67.4 80.8/60.7 73.7/67.8 62.4/53.5 78.2/44.2 61.0/53.9 65.7/26.0 44.4/29.5 46.2/45.4 58.0/44.8Avg. Table A4: The detailed experimental results for Baichuan2</p>
<p>79.8/68.3 59.1/52.1 84.5/84.1 79.9/79.6 40.6/40.6 53.8/51.5 22.1/22.8 18.1/16.1 74.4/62.9 70.3/57.8 8(44) 80.5/68.6 59.1/52.2 86.8/86.4 79.8/79.5 42.4/42.4 54.0/51.9 23.2/23 .0 18.0/16.1 71.1/64.8 70.4/57.9 8(45) 81.1/68.7 58.8/51.9 85.9/85.5 80.2/79.9 45.2/45.0 54.2/52.0 26.4/27.0 18.0/16.1 71.4/63.1 70.3/57.8 8(46) 82.3/70.1 59.1/52.1 81.4/81.2 79.8/79Avg. </p>
<p>80.2/68.4 59.0/52.1 84.4/84.0 79.9/79.5 43.2/43.2 54.1/51.9 24.0/24.0 18.0/16.0 72.1/63.7 70.3/57.8 16(42) 76.4/65.0 59.2/52.2 85.1/84.6 79.8/79.4 42.0/42.0 54.1/51.8 22.0/23.2 18.2/16.1 71.7/64.8 70.4/57.9Avg. </p>
<p>81.1/69.2 58.8/51.9 84.1/83.8 79.8/79.5 42.1/42.0 54.1/51.8 23.1/24.1 18.1/16.0 70.8/64.3 70.4/57.9 32(42) 78.4/66.7 58.251.5 85.5/85.1 79.9/79.6 45.4/45.2 54.1/51.7 27.8/28.0 17.7/15.7 72.2/64.4 70.1/57.6Avg. </p>
<p>81.4/69.4 58.8/51.9 84.4/84.0 79.8/79.5 42.1/42.1 53.9/51Avg. 7 29.2/28.0 17.9/16.0 71.2/64.7 70.4/57.8 Table A5: The detailed experimental results for GLM4</p>
<p>81.6/67.4 57.5/50.5 88.2/86.8 80.3/79.9 48.0/47.9 30.5/29.3 16.8/15.3 18.2/19.7 64.5/58.9 65.8/58.3 8(44) 82.6/66.6 68.8/57.4 88.7/87.5 81.9/81.3 51.8/51Avg. 3 47.7/47.6 32.5/23.9 18.5/20.1 55.5/53.6 51.7/50.7 8(45) 85.2/70.6 56.9/49.9 87.1/85.2 82.7/82.0 54.6/53.7 40.0/40.0 31.3/23.7 18.8/20.3 62.5/58.8 64.3/58.3 8(46) 84.8/70.6 67.5/56.5 90.4/89.5 79.8/79.5 43.9/43.9 40.6/40.5 25.0/19.5 19.3/19.7 53.0/51.6 63.9/58</p>
<p>41.1/41.1 16(45) 85.9/71.6 59.5/51.6 87.1/85.0 80.8/80.4 54.6/53.7 35.2/34.8 27.7/21.2 18.1/18.5 60.5/57.4 60.8/56.6 16(46) 85.6/70.0 71.2/58.9 87.1/84.9 80.1/79.8 47.3/47.2 37.8/37.6 19.8/16.9 19.0/19.4 54.5/53.1 45.3/45.3Avg. 7 37.5/37.3 26.2/19.9 18.1/19.0 52.7/51.6</p>
<p>83.9/68.8 66.4/55.9 86.6/84.4 80.7/80.3 52.9/52.1 36.6/36.3 21.9/17.8 18.6/19.2 56.5/54.5 53.8/51.3 32(42) 82.0/67.1 66.0/55.9 87.7/85.7 82.3/81.7 50.1/49.7 34.4/33.9 17.4/14.7 27.1/23.2 63.7/58.8 58.4/55.1 32(43) 86.2/71.5 63.4/54.2 87.2/85.1 81.0/80.5 47.0/46.9 35.0/34.6 19.7/16.2 21.5/20.5 56.1/54.3 59.0/56.0Avg. </p>
<p>84.5/69.5 64.5/54.8 87.1/85.1 81.6/81.1 48.9/48.6 36.4/36.0 17.3/15.2 26.1/23.8 60.3/57.0 53.6/51.1Avg. Table A6: The detailed experimental results for Llama3</p>
<p>71.9/61.4 80.0/66.3 84.8/84.4 83.9/83.5 42.9/42.9 55.9/54.7 44.4/38.3 56.4/40.8 54.6/53.5 74.1/64.2 8(44) 74.1/62.3 83.1/68.1 85.4/84.9 83.8/83.4 44.6/44.5 57.9/56.5 41.0/35.7 56.5/41.0 50.5/50.1 46.1/46.1 8(45) 67.8/58.5 82.9/68 86.2/85.7 83.2/82.8 42.7/42.7 57.9/56.5 48.9/40.8 57.0/41.0 53.1/52.3 74.2/52.2 8(46) 73.9/62.7 84.2/69.2 84.9/84.6 84.1/83.7 42.4/42.4Avg. 8</p>
<p>70.6/60.3 81.3/67.5 85.5/85.1 84.8/84.4 47.0/46.9 56.6/55.3 45.4/38.9 56.9/40.8 55.0/53.8 45.7/45.7 16(44) 68.2/58.4 82.9/68.4 84.9/84.6 82.3/82.0 49.9/49.6 56.5/55.3 45.8/36.0 55.3/40.0 58.8/57.0 55.0/53.8 16(45) 63.5/55.0 83.1/68.3 86.8/86.3 82.0/81.7 47.5/47.4 54.7/53.7 45.0/37.5 56.9/40.7 55.4/54.4 72.9/42.2 16(46) 73.6/62.0 83.3/68.5 88.0/87.5 83.3/83.0Avg. </p>
<p>. 68.6/58.6 82.7/68.2 86.6/86.1 83.8/83.4 48.2/48.0 56.5/55.2 46.3/37.4 56.6/40.6 54.6/53.6 60.5/51.8Avg. </p>
<p>70.3/59.7 83.1/68.4 87.0/86.5 84.0/83.6 53.8/53.1 56.0/54.8 45.9/35.0 60.3/42.7 48.0/47.7 73.8/61.5Avg. Table A7: The detailed experimental results for Gemma2</p>
<p>88.5/74.0 88.3/73.8 89.1/88.4 86.5/85.9 63.1/60.0 43.6/43.6 50.0/37.9 36.0/34.0 69.0/51.3 70.5/52.2 8(44) 89.6/73.3 87.7/72.6 89.9/89.0 86.5/85.9 68.7/64.1 44.7/44.6 51.9/38.3 38.6/36.0 68.7/52.0 70.6/52.1 8(45) 89.8/73.8 88.1/73.0 90.0/89.3 86.5/86.0 56.4/54.7 44.3/44.2 53.3/39.9 37.4/34.8 69.2/52.7 71.4/53.0 8(46) 87.8/74.1 87.3/71.6 89.3/88.5 85.5/84.9 65.8/61.8 44.0/44.0 57.5/41.0 37.9/35.1 71.9/51.6 71.4/53.0Avg. </p>
<p>88.9/73.8 87.9/72.7 89.7/88.9 86.3/85.7 62.6/59.5 44.2/44.1 51.8/38.3 37.2/34.6 69.7/51.5 71.1/52.7 16(42) 88.1/73.4 87.8/72.6 89.3/88.3 86.4/85.7 51.6/50.9 45.2/45.1 53.9/38.1 38.7/35.7 71.2/51.1 70.9/52.1 16(43) 89.3/74.7 87.3/71.1 90.4/89.5 85.1/84.4 52.0/51.2 45.2/45.1 47.8/39.9 37.1/35.0 68.8/53.8 69.6/50.2Avg. </p>
<p>88.1/73.7 87.6/72.1 89.6/88.8 85.4/84.8 54.8/53.6 44.4/44.3 50.2/40.4 37.9/35.1 70.2/50.8 70.4/51.3 32(42) 86.1/72.4 87.8/72.4 89.0/88.1 84.0/83.4 48.9/48.5 43.6/43Avg. </p>
<p>86.3/72.3 87.7/72.3 89.0/88.2 85.0/84.4 49.3/48.9 44.0/43.9 51.1/36.9 36.9/34.5 71.2/49.8 70.2/50.5Avg. Table A8: The detailed experimental results for Phi-3</p>
<p>A9 Table, 86.2 70.6 86.9 86.5 36.2 35.9 28.5The detailed experimental results in the CoT strategy. Model Bragging Complaint Sarcasm Rumour Stance GossipCop Avg Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Qwen2. </p>
<p>Table A10: The detailed experimental results in the zero-shot strategy. </p>            </div>
        </div>

    </div>
</body>
</html>
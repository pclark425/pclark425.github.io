<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7008 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7008</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7008</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-4f451ba06c4c9effd6c4ac0bae222495501a6200</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200" target="_blank">Innovations in Neural Data-to-text Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.</p>
                <p><strong>Paper Abstract:</strong> The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7008.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7008.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF triple sequence (subject-property-object triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that serializes a knowledge graph as a sequence of RDF triples (subject, predicate, object), often delimited or concatenated to form the input text for a seq2seq or LM; used widely in WebNLG-style graph-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is encoded as an ordered list of RDF triples (subject, property, object). Triples are serialized into a flat token sequence (e.g., "(Apollo 12, operator, NASA) . (Apollo 12, launchdate, 1969-11-14) ...") with simple separators or special tokens between triples and between triple fields.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list ordering (triple-by-triple linearization); typical traversal is unspecified in survey (dataset supplies triple lists)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (mentioned), DART (merged heterogeneous RDF-like predicates)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / knowledge-graph verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM seq2seq, Transformer variants, Graph-augmented PLMs (as reported for WebNLG pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classic encoder-decoder seq2seq models (LSTM/Transformer) trained on serialized triple sequences; in cited work GCN- or transformer-based graph encoders are also used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>word-overlap metrics (BLEU), extractive/entity-based metrics (entity-coverage), automated metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to use with standard sequence models and PLMs; enables multi-domain training (WebNLG) and easier batching for seq2seq LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can lose explicit graph structural relations (e.g., multi-hop paths and re-entrancies) and relational context; ordering/sparsity choices influence performance; dependent on dataset triple ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Easier to plug into standard LMs than graph encoders, but inferior at preserving relational structure compared to explicit graph encoders (GCN/GAT/GraphWriter) which better capture topology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7008.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR linearization (Abstract Meaning Representation serialized)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert AMR graphs into a linear token sequence via depth-first traversal or other tree serializations to train sequence models for AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs (directed, rooted, labeled) are linearized by traversing the graph (commonly depth-first) and emitting node labels, edge labels and bracket or separator tokens to encode reentrancies and structure into a sequence consumable by seq2seq LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (with structural tokens like brackets), lossy for re-entrancy unless special tokens used</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first traversal / bracketed depth-first serialization; sometimes grouped/anonymized to reduce sparsity</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR corpora (e.g., LDC AMR datasets referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM seq2seq, GPT-2 fine-tuning (reported in survey as used for AMR-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq LSTM and PLM-based decoders fine-tuned on linearized AMR-text pairs; some works fine-tune GPT-2 on the joint AMR+text token stream.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated text metrics (BLEU) and AMR-specific evaluation (not detailed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows use of off-the-shelf seq2seq or pretrained LMs without graph encoders; pretraining on linearized AMR can produce competitive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearization must handle re-entrant structures carefully; can be dataset-dependent and may obscure graph topology; may need anonymization or grouping to mitigate sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to explicit graph encoders, linearization is simpler but may be less effective at preserving structural information; some work observes robustness to different linearization orders with anonymization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7008.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first traversal (DFS) graph linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text serialization strategy that performs a depth-first traversal of the graph to produce a sequence of node and edge tokens for LM input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Depth-first traversal (DFS) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A DFS over the input graph emits nodes and relations in the order visited, often with separators/bracketing to indicate nesting; used to reduce graph complexity and produce a deterministic sequence when traversal order is fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, traversal-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first search traversal; sometimes combined with anonymization/grouping of entities</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG / AMR (mentioned as effective linearization approach)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequence-to-sequence NMT systems, LSTM decoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard NMT/seq2seq systems trained on DFS-serialized graph inputs, sometimes used with anonymization to reduce sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated metrics (BLEU) and qualitative assessments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces graph complexity and tends to improve neural model performance vs raw unordered lists; compatible with off-the-shelf NMT systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Traversal order introduces an arbitrary sequential bias; may be dataset-dependent and may not capture certain graph-global phenomena (reentrancy) unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>DFS linearization is effective and simple compared to arbitrary orderings, but explicit graph encoders often better preserve topology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7008.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topological/BFS ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topological + breadth-first traversal ordering for LSTM node sequence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ordering scheme combining topological sort and breadth-first traversal to produce a sequence of graph nodes for an LSTM-based encoder-decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Topological/BFS node ordering</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Vertices are ordered by a hybrid algorithm: first a topological sort to respect dependency order, then a breadth-first traversal to determine local ordering; the ordered node sequence is then fed to an LSTM encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, traversal-based, deterministic ordering heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>topological sort combined with breadth-first traversal over the graph nodes (as reported for frameworks that feed ordered nodes into LSTMs)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR / general graph-to-text datasets (discussed in context of GTR-LSTM / Distiawan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTR-LSTM / LSTM-based graph-RNN encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-RNNs that accept a node sequence ordered via topological/BFS heuristic and compute hidden states sequentially for LSTM-based decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>comparative word-overlap and structural metrics (not enumerated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides an ordering that helps sequential encoders capture graph dependencies; intended to handle non-predefined relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on graph properties to be meaningful; ordering heuristics can be brittle and dataset dependent; may not capture long-range graph relations as well as message-passing GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>A middle ground between arbitrary linearization and full graph-structured encoders; sometimes combined with GCNs for complementary benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7008.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attribute-value serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribute-value serialization (table linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts table or record structured graphs into a flat sequence of attribute:value (or field is value) tokens, separated by special tokens, for PLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attribute-value serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Tables are serialized row-wise or field-wise by producing sequences like "name : Loch Fyne | eatType : restaurant | food : French ..." or templates using the word 'is' between field and cell value; a special separator token often splits table and target text.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, structured serialization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>row-wise or attribute-wise traversal (horizontal traversal across rows), using explicit separators and sometimes 'is' templates or special tokens between table and narrative</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WikiBio, ToTTo, RotoWire, Tabular datasets referenced (WikiTableT, ToTTo)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>table-to-text / record-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5, GPT-2, transformer-based PLMs (fine-tuned); sometimes LSTM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained language models (T5, GPT-2) fine-tuned on serializations of table fields and values; T5 reported to do well on automated metrics but struggles with numerical calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated metrics (BLEU, ROUGE), task-specific metrics (PARENT), numerical reasoning checks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct use of PLMs without specialized graph encoders; effective with T5/GPT-2 for many tasks and datasets; useful for prompt/prefix tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can fail on numeric calculations and arithmetic reasoning; longer serialized inputs increase token cost; some structural relations (e.g., multi-row aggregation) are not explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Simpler than building table-specific encoders; PLMs trained on serialized tables perform well on automated metrics, but explicit numeric/structural encoders and specialized numeric encoding methods outperform them on arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7008.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-plan ordered-tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-plan (ordered-tree) appended linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augment graph serialization with an explicit ordered tree plan (sentence plan) appended to the training data, encoding intended sentence segmentation and ordering before text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sentence-plan ordered-tree serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A content plan modeled as an ordered tree is serialized (e.g., via depth-first traversal) and appended to the serialized graph/triples; the combined sequence teaches the model both what to say (plan) and how to realize it (text).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical serialized into sequential form (plan+sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>ordered-tree serialization (e.g., depth-first traversal of plan tree) appended to graph linearization as explicit planning tokens or segments</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (plan-to-text experiments), datasets where plans were appended</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text with explicit planning / sentence planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Off-the-shelf NMT / seq2seq systems (used for plan-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NMT/seq2seq models trained on concatenated plan+text pairs or plan-augmented graph-to-text pairs to encourage sentence ordering and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated metrics (BLEU) and human evaluations for coherence</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves sentence planning and coherence by providing explicit ordering and sentence boundaries; helps NMT systems learn planning-to-text mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires either dataset-specific assumptions (e.g., single entity mentions per sentence) or an additional annotation step to produce plans; may be dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Augmenting linearizations with plans yields better coherence than plain linearization, but relies on plan availability or extra preprocessing (vs graph encoders that learn implicit plans).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7008.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-level node serialization with positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level node serialization with positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Serialize graph nodes into token sequences while injecting positional embeddings to retain a sense of local order and allow transformer-based models to learn sequential patterns from graph tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level node serialization + positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes are tokenized into text tokens and serialized; positional embeddings (row/column or intra-node positions) are injected to the token embeddings to preserve ordering or table structure when using transformer encoders/decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential with positional augmentation; token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>node token serialization with explicit positional/structural embeddings (e.g., row/column indices, token positions) appended to token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, table datasets where positional tokens are used; general graph-to-text contexts</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text / table-to-text generation with transformer encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformers with positional/structural embeddings (GraphWriter, enhanced transformer encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based encoders that accept serialized tokens augmented with positional or structural embeddings to convey layout/position information; used to adapt PLMs to structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated generation metrics (BLEU) plus structural/coverage metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Helps transformers exploit locality and order when given serialized graphs/tables, improving coverage and coherence relative to naive serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Positional signals are an approximation of graph structure and may not fully capture multi-hop or relational topology; careful design needed for complex graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Better than pure token serialization for transformers, but still may underperform true graph neural encoders on preserving relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7008.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7008.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-view: triple + linearized view</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detachable multi-view serialization (triple view + linearized view)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-view approach that decomposes a graph into multiple serialized views (e.g., triple sets and depth-first linearization) and adds autoencoding/reconstruction objectives to encourage the model to learn both structured and sequential aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-view (triple set + DFS linearized) serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input graph is split into multiple serialized views: (1) a set-of-triples view reconstructed with relation-aware modules, and (2) a depth-first linearized string; both are used during training with multi-view autoencoding losses to enrich representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hybrid multi-view sequential + structural</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>create two serialized text views: triple-set serialization (unordered set) and DFS linearized serialization; train model with auxiliary autoencoder losses for each view</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>General graph-to-text datasets (mentioned as a technique applied to graph inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation with multi-view pretraining / reconstruction objectives</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Relation-aware transformers with multi-view autoencoding losses; deep biaffine reconstructor for triples (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder augmented with detachable modules to reconstruct triple-view and linearized-view representations as auxiliary tasks to improve graph-text mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>automated metrics (BLEU), reconstruction losses and downstream generation quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enriches structural learning and improves downstream generation by providing multiple complementary training signals; helps with structural fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds training complexity and auxiliary losses; requires designing multiple views and reconstructors; increases computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Stronger structural supervision than single-view linearization; can approach explicit graph-encoder performance while still enabling sequential LM use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7008",
    "paper_id": "paper-4f451ba06c4c9effd6c4ac0bae222495501a6200",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "RDF triple sequence",
            "name_full": "RDF triple sequence (subject-property-object triples)",
            "brief_description": "A representation that serializes a knowledge graph as a sequence of RDF triples (subject, predicate, object), often delimited or concatenated to form the input text for a seq2seq or LM; used widely in WebNLG-style graph-to-text tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "RDF triple sequence",
            "representation_description": "Each graph is encoded as an ordered list of RDF triples (subject, property, object). Triples are serialized into a flat token sequence (e.g., \"(Apollo 12, operator, NASA) . (Apollo 12, launchdate, 1969-11-14) ...\") with simple separators or special tokens between triples and between triple fields.",
            "representation_type": "sequential, token-based",
            "encoding_method": "edge-list ordering (triple-by-triple linearization); typical traversal is unspecified in survey (dataset supplies triple lists)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (mentioned), DART (merged heterogeneous RDF-like predicates)",
            "task_name": "graph-to-text generation / knowledge-graph verbalization",
            "model_name": "LSTM seq2seq, Transformer variants, Graph-augmented PLMs (as reported for WebNLG pipelines)",
            "model_description": "Classic encoder-decoder seq2seq models (LSTM/Transformer) trained on serialized triple sequences; in cited work GCN- or transformer-based graph encoders are also used for comparison.",
            "performance_metric": "word-overlap metrics (BLEU), extractive/entity-based metrics (entity-coverage), automated metrics",
            "performance_value": null,
            "impact_on_training": "Simple to use with standard sequence models and PLMs; enables multi-domain training (WebNLG) and easier batching for seq2seq LMs.",
            "limitations": "Can lose explicit graph structural relations (e.g., multi-hop paths and re-entrancies) and relational context; ordering/sparsity choices influence performance; dependent on dataset triple ordering.",
            "comparison_with_other": "Easier to plug into standard LMs than graph encoders, but inferior at preserving relational structure compared to explicit graph encoders (GCN/GAT/GraphWriter) which better capture topology.",
            "uuid": "e7008.0",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "AMR linearization",
            "name_full": "AMR linearization (Abstract Meaning Representation serialized)",
            "brief_description": "Convert AMR graphs into a linear token sequence via depth-first traversal or other tree serializations to train sequence models for AMR-to-text generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "AMR linearization",
            "representation_description": "AMR graphs (directed, rooted, labeled) are linearized by traversing the graph (commonly depth-first) and emitting node labels, edge labels and bracket or separator tokens to encode reentrancies and structure into a sequence consumable by seq2seq LMs.",
            "representation_type": "sequential, token-based (with structural tokens like brackets), lossy for re-entrancy unless special tokens used",
            "encoding_method": "depth-first traversal / bracketed depth-first serialization; sometimes grouped/anonymized to reduce sparsity",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR corpora (e.g., LDC AMR datasets referenced)",
            "task_name": "AMR-to-text generation / graph-to-text generation",
            "model_name": "LSTM seq2seq, GPT-2 fine-tuning (reported in survey as used for AMR-to-text)",
            "model_description": "Seq2seq LSTM and PLM-based decoders fine-tuned on linearized AMR-text pairs; some works fine-tune GPT-2 on the joint AMR+text token stream.",
            "performance_metric": "automated text metrics (BLEU) and AMR-specific evaluation (not detailed in survey)",
            "performance_value": null,
            "impact_on_training": "Allows use of off-the-shelf seq2seq or pretrained LMs without graph encoders; pretraining on linearized AMR can produce competitive generation.",
            "limitations": "Linearization must handle re-entrant structures carefully; can be dataset-dependent and may obscure graph topology; may need anonymization or grouping to mitigate sparsity.",
            "comparison_with_other": "Compared to explicit graph encoders, linearization is simpler but may be less effective at preserving structural information; some work observes robustness to different linearization orders with anonymization.",
            "uuid": "e7008.1",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "DFS linearization",
            "name_full": "Depth-first traversal (DFS) graph linearization",
            "brief_description": "A graph-to-text serialization strategy that performs a depth-first traversal of the graph to produce a sequence of node and edge tokens for LM input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Depth-first traversal (DFS) linearization",
            "representation_description": "A DFS over the input graph emits nodes and relations in the order visited, often with separators/bracketing to indicate nesting; used to reduce graph complexity and produce a deterministic sequence when traversal order is fixed.",
            "representation_type": "sequential, traversal-based",
            "encoding_method": "depth-first search traversal; sometimes combined with anonymization/grouping of entities",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG / AMR (mentioned as effective linearization approach)",
            "task_name": "graph-to-text generation",
            "model_name": "Sequence-to-sequence NMT systems, LSTM decoders",
            "model_description": "Standard NMT/seq2seq systems trained on DFS-serialized graph inputs, sometimes used with anonymization to reduce sparsity.",
            "performance_metric": "automated metrics (BLEU) and qualitative assessments",
            "performance_value": null,
            "impact_on_training": "Reduces graph complexity and tends to improve neural model performance vs raw unordered lists; compatible with off-the-shelf NMT systems.",
            "limitations": "Traversal order introduces an arbitrary sequential bias; may be dataset-dependent and may not capture certain graph-global phenomena (reentrancy) unless augmented.",
            "comparison_with_other": "DFS linearization is effective and simple compared to arbitrary orderings, but explicit graph encoders often better preserve topology.",
            "uuid": "e7008.2",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Topological/BFS ordering",
            "name_full": "Topological + breadth-first traversal ordering for LSTM node sequence",
            "brief_description": "An ordering scheme combining topological sort and breadth-first traversal to produce a sequence of graph nodes for an LSTM-based encoder-decoder.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Topological/BFS node ordering",
            "representation_description": "Vertices are ordered by a hybrid algorithm: first a topological sort to respect dependency order, then a breadth-first traversal to determine local ordering; the ordered node sequence is then fed to an LSTM encoder.",
            "representation_type": "sequential, traversal-based, deterministic ordering heuristic",
            "encoding_method": "topological sort combined with breadth-first traversal over the graph nodes (as reported for frameworks that feed ordered nodes into LSTMs)",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "AMR / general graph-to-text datasets (discussed in context of GTR-LSTM / Distiawan et al.)",
            "task_name": "graph-to-text generation",
            "model_name": "GTR-LSTM / LSTM-based graph-RNN encoders",
            "model_description": "Graph-RNNs that accept a node sequence ordered via topological/BFS heuristic and compute hidden states sequentially for LSTM-based decoders.",
            "performance_metric": "comparative word-overlap and structural metrics (not enumerated)",
            "performance_value": null,
            "impact_on_training": "Provides an ordering that helps sequential encoders capture graph dependencies; intended to handle non-predefined relationships.",
            "limitations": "Relies on graph properties to be meaningful; ordering heuristics can be brittle and dataset dependent; may not capture long-range graph relations as well as message-passing GNNs.",
            "comparison_with_other": "A middle ground between arbitrary linearization and full graph-structured encoders; sometimes combined with GCNs for complementary benefits.",
            "uuid": "e7008.3",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Attribute-value serialization",
            "name_full": "Attribute-value serialization (table linearization)",
            "brief_description": "A method that converts table or record structured graphs into a flat sequence of attribute:value (or field is value) tokens, separated by special tokens, for PLM fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Attribute-value serialization",
            "representation_description": "Tables are serialized row-wise or field-wise by producing sequences like \"name : Loch Fyne | eatType : restaurant | food : French ...\" or templates using the word 'is' between field and cell value; a special separator token often splits table and target text.",
            "representation_type": "sequential, token-based, structured serialization",
            "encoding_method": "row-wise or attribute-wise traversal (horizontal traversal across rows), using explicit separators and sometimes 'is' templates or special tokens between table and narrative",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WikiBio, ToTTo, RotoWire, Tabular datasets referenced (WikiTableT, ToTTo)",
            "task_name": "table-to-text / record-to-text generation",
            "model_name": "T5, GPT-2, transformer-based PLMs (fine-tuned); sometimes LSTM baselines",
            "model_description": "Pretrained language models (T5, GPT-2) fine-tuned on serializations of table fields and values; T5 reported to do well on automated metrics but struggles with numerical calculations.",
            "performance_metric": "automated metrics (BLEU, ROUGE), task-specific metrics (PARENT), numerical reasoning checks",
            "performance_value": null,
            "impact_on_training": "Enables direct use of PLMs without specialized graph encoders; effective with T5/GPT-2 for many tasks and datasets; useful for prompt/prefix tuning.",
            "limitations": "Can fail on numeric calculations and arithmetic reasoning; longer serialized inputs increase token cost; some structural relations (e.g., multi-row aggregation) are not explicit.",
            "comparison_with_other": "Simpler than building table-specific encoders; PLMs trained on serialized tables perform well on automated metrics, but explicit numeric/structural encoders and specialized numeric encoding methods outperform them on arithmetic reasoning.",
            "uuid": "e7008.4",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Sentence-plan ordered-tree",
            "name_full": "Text-plan (ordered-tree) appended linearization",
            "brief_description": "Augment graph serialization with an explicit ordered tree plan (sentence plan) appended to the training data, encoding intended sentence segmentation and ordering before text generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Sentence-plan ordered-tree serialization",
            "representation_description": "A content plan modeled as an ordered tree is serialized (e.g., via depth-first traversal) and appended to the serialized graph/triples; the combined sequence teaches the model both what to say (plan) and how to realize it (text).",
            "representation_type": "hierarchical serialized into sequential form (plan+sequence)",
            "encoding_method": "ordered-tree serialization (e.g., depth-first traversal of plan tree) appended to graph linearization as explicit planning tokens or segments",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (plan-to-text experiments), datasets where plans were appended",
            "task_name": "graph-to-text with explicit planning / sentence planning",
            "model_name": "Off-the-shelf NMT / seq2seq systems (used for plan-to-text generation)",
            "model_description": "NMT/seq2seq models trained on concatenated plan+text pairs or plan-augmented graph-to-text pairs to encourage sentence ordering and coherence.",
            "performance_metric": "automated metrics (BLEU) and human evaluations for coherence",
            "performance_value": null,
            "impact_on_training": "Improves sentence planning and coherence by providing explicit ordering and sentence boundaries; helps NMT systems learn planning-to-text mapping.",
            "limitations": "Requires either dataset-specific assumptions (e.g., single entity mentions per sentence) or an additional annotation step to produce plans; may be dataset-dependent.",
            "comparison_with_other": "Augmenting linearizations with plans yields better coherence than plain linearization, but relies on plan availability or extra preprocessing (vs graph encoders that learn implicit plans).",
            "uuid": "e7008.5",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Token-level node serialization with positional embeddings",
            "name_full": "Token-level node serialization with positional embeddings",
            "brief_description": "Serialize graph nodes into token sequences while injecting positional embeddings to retain a sense of local order and allow transformer-based models to learn sequential patterns from graph tokens.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Token-level node serialization + positional embeddings",
            "representation_description": "Nodes are tokenized into text tokens and serialized; positional embeddings (row/column or intra-node positions) are injected to the token embeddings to preserve ordering or table structure when using transformer encoders/decoders.",
            "representation_type": "sequential with positional augmentation; token-based",
            "encoding_method": "node token serialization with explicit positional/structural embeddings (e.g., row/column indices, token positions) appended to token embeddings",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG, table datasets where positional tokens are used; general graph-to-text contexts",
            "task_name": "graph-to-text / table-to-text generation with transformer encoders",
            "model_name": "Transformers with positional/structural embeddings (GraphWriter, enhanced transformer encoders)",
            "model_description": "Transformer-based encoders that accept serialized tokens augmented with positional or structural embeddings to convey layout/position information; used to adapt PLMs to structured inputs.",
            "performance_metric": "automated generation metrics (BLEU) plus structural/coverage metrics",
            "performance_value": null,
            "impact_on_training": "Helps transformers exploit locality and order when given serialized graphs/tables, improving coverage and coherence relative to naive serialization.",
            "limitations": "Positional signals are an approximation of graph structure and may not fully capture multi-hop or relational topology; careful design needed for complex graphs.",
            "comparison_with_other": "Better than pure token serialization for transformers, but still may underperform true graph neural encoders on preserving relational structure.",
            "uuid": "e7008.6",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Multi-view: triple + linearized view",
            "name_full": "Detachable multi-view serialization (triple view + linearized view)",
            "brief_description": "A multi-view approach that decomposes a graph into multiple serialized views (e.g., triple sets and depth-first linearization) and adds autoencoding/reconstruction objectives to encourage the model to learn both structured and sequential aspects.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Multi-view (triple set + DFS linearized) serialization",
            "representation_description": "Input graph is split into multiple serialized views: (1) a set-of-triples view reconstructed with relation-aware modules, and (2) a depth-first linearized string; both are used during training with multi-view autoencoding losses to enrich representations.",
            "representation_type": "hybrid multi-view sequential + structural",
            "encoding_method": "create two serialized text views: triple-set serialization (unordered set) and DFS linearized serialization; train model with auxiliary autoencoder losses for each view",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "General graph-to-text datasets (mentioned as a technique applied to graph inputs)",
            "task_name": "graph-to-text generation with multi-view pretraining / reconstruction objectives",
            "model_name": "Relation-aware transformers with multi-view autoencoding losses; deep biaffine reconstructor for triples (as reported)",
            "model_description": "Transformer encoder-decoder augmented with detachable modules to reconstruct triple-view and linearized-view representations as auxiliary tasks to improve graph-text mapping.",
            "performance_metric": "automated metrics (BLEU), reconstruction losses and downstream generation quality",
            "performance_value": null,
            "impact_on_training": "Enriches structural learning and improves downstream generation by providing multiple complementary training signals; helps with structural fidelity.",
            "limitations": "Adds training complexity and auxiliary losses; requires designing multiple views and reconstructors; increases computation.",
            "comparison_with_other": "Stronger structural supervision than single-view linearization; can approach explicit graph-encoder performance while still enabling sequential LM use.",
            "uuid": "e7008.7",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01659475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Innovations in Neural Data-to-text Generation: A Survey</h1>
<p>MANDAR SHARMA, Virginia Tech, USA<br>AJAY KUMAR GOGINENI, Virginia Tech, USA<br>NAREN RAMAKRISHNAN, Virginia Tech, USA</p>
<p>The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text generation (D2T). This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.</p>
<p>CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Natural language generation; Learning paradigms; Machine learning algorithms.</p>
<p>Additional Key Words and Phrases: narration, data-to-text, data-to-text generation, natural language generation</p>
<h2>1 INTRODUCTION</h2>
<p>Textual Representations of Information: A picture is worth a thousand words - isn't it? And hence graphical representation is by its nature universally superior to text - isn't it? Why then isn't the anecdote itself represented graphically? - Petre [246], in his advocacy for textual representation of information, challenges the notion that graphical representations of information are inherently more memorable, comprehensible, and accessible than their textual counterparts. Gershon and Page [104] note that the transformation of information from a textual to visual domain, in certain instances, requires further addition of information rendering textual representations more economical. Similarly, knowing where to look may not be obvious in visual representations of information - as validated through reading comprehension experiments [31, 115, 116] where participants were significantly slower in interpreting visual representations of the nested conditional structures within a program compared to their textual representations. This being said, these studies do not intend to dissuade the use of visual representations but rather establish the importance of textual representation of information. Often, the interplay of these paradigms brings out the best of both [289]. Thus, having established the importance of textual representations of information, we next explore how these notions tie into Data-to-text (D2T) generation.</p>
<h3>1.1 Defining Data-to-text Generation and Scope of the Survey</h3>
<p>Textual representations of information, for easier assimilation, are often presented as annotations outlining different behaviours of the underlying data stitched together. These stitched annotations, as showcased in Fig. 1, are referred to as narratives. The automated generation of such narratives, although serving several niches (see below), are most prevalent in the public eye through the practice of robo-journalism [66, 176]. Bloomberg News generates a third of its content with Cyborg, their in-house automation system that can dissect tedious financial reports and churn out news articles within seconds [243]. Also prevalent are the use of such systems in business intelligence</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
$T^{9}$ Narrative (Kansas): The first trace of Carbon Monoxide as a measurement of the air quality was observed in January 2000 in the state of Kansas. The mean CO observed can be dissected into 2 regimes of interest. The first regime spanning April 2006 to May 2006 and had an average CO emission of 0.44 . The second regime from April 2013 to April 2013 had an average of 0.13 . From January 2000 to July 2002, the observed values decreased from 0.9 to 0.0 . The observed values also decreased from 0.48 to 0.12 during February 2003 to May 2010. The mean CO emissions peaked in October 2000 at 2.58.
$\square$ Introduction $\quad \square$ Regimes Trends $\square$ Peak</p>
<p>Fig. 1. Illustration of D2T: Narration of time-series data (COVID19 progression in the United Kingdom at the top, the Carbon-Monoxide emissions in the state of Kansas, United States at the bottom) with the LLM-based framework $T^{3}$ (T-Cube) [294]. This D2T framework consumes a time-series as input and generates narratives that highlight the progression and points-of-interest (regimes, trends, and peaks) in the data through LLM-generated narratives.
(BI) settings with prominent commercial frameworks ${ }^{1}$ such as Arria NLG ${ }^{2}$, Narrative Science ${ }^{3}$, and Automated Insights ${ }^{4}$.</p>
<p>The practice of automating the translation of data to user-consumable narratives through such systems is known as data-to-text generation, as depicted in Fig. 1. Although encompassed by the general umbrella of Natural Language Generation, the nuance that differentiates D2T from the rest of the NLG landscape is that the input to the system has to qualify as a data instance. Reiter and Dale (1997) [270] describe the instance as a non-linguistic representation of information, and although narration of images and videos [70] has garnered interest in the NLG community, the definition of data-to-text employed by this survey follows that established by the seminal works prior [100, 270]: an entity that is not exclusively linguistic - tabular databases, graphs and knowledge bases, time-series, and charts. Using this clause, we limit the scope of our analysis and exclude examination of all other NLG systems that either both ingest and expel linguistic entities for downstream tasks such as machine translation [145, 350] or summarization [188, 228] or ingest non-conventional data such as images [131] and videos [328].</p>
<p>Outside of dataset specific tasks, practical applications of D2T include, but are not limited to:</p>
<ul>
<li>Weather forecasts [20, 271]</li>
<li>Sport summaries [15, 279, 316]</li>
<li>Healthcare [241, 249]</li>
<li>Virtual dietitians [9]</li>
<li>Stock market comments [10, 220]</li>
<li>Video-game dialouges [149] and Driving feedback [35]</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>With our scope defined, below we outline the rationale for this survey, followed by a structured examination of approaches, benchmark datasets, and evaluation protocols that constitute the D2T landscape with the intent to outline promising avenues for further research.</p>
<h1>1.2 Survey Rationale</h1>
<p>Following the seminal work by Reiter and Dale [270], the most comprehensive survey on D2T to-date has been that by Gatt and Krahmer [100]. Although several articles have taken a close examination of NLG sub-fields such as dialogue systems [282], poetry generation [234], persuasive text generation [76], social robotics [94], or exclusively focus on issues central to NLG such as faithfulness [181] and hallucination [142], a detailed break-down of the last half-decade of innovations has been missing since the last exhaustive body of work. The need for a close and consolidated examination of developments in neural D2T is more pertinent now than ever. Further, D2T distinguishes itself from other NLG tasks as it blends the generation of narratives with numerical reasoning between data points. Outside of the D2T niche, there are research communities focused on solving these individual problems - NLG [100, 235, 320] and numerical reasoning [295, 317, 334, 349]. Thus, neural D2T is uniquely positioned such that it either has to incorporate innovations from these seemingly disparate niches or jointly innovate on both fronts. We believe this provides added justification for D2T requiring its own comprehensive literature review.</p>
<p>As such, neural D2T borrows heavily from advances in other facets of NLG such as neural machine translation (NMT) [11, 350] and spoken dialogue systems (SDS) [79, 342, 343]. As such, the pertinence of such a survey also spans highlighting the stages of technological adoptions in the D2T paradigm and drawing distinctions between its NMT and SDS neighbors. Further, the adoption of such technologies brings about the adoption of shared pitfalls - inconsistencies in evaluation metrics [268] and meaningful inter-model comparisons [265]. Thus, in addition to an exhaustive examination of neural D2T frameworks, a consolidated resource on approaches to its evaluation is also necessary. Also crucial, is the discussion of benchmark datasets across shared tasks. The above considerations motivate our survey on the neural D2T paradigm intended to serve the following goals:</p>
<ul>
<li>Structured examination of innovations in neural D2T in the last half-decade spanning relevant frameworks, datasets, and evaluation measures.</li>
<li>Outlining the technological adoptions in D2T from within and outside of the greater NLG umbrella with the distinctions and shared pitfalls that lie therein.</li>
<li>Highlighting promising avenues for further D2T research and exploration that promote fairness and accountability along with linguistic prowess.</li>
</ul>
<h2>2 DATASETS FOR DATA-TO-TEXT GENERATION</h2>
<p>The first set of technological adoptions from NLP takes the form of dataset design: parallel corpora that align the data to their respective narratives are crucial for end-to-end learning, analogous to any neural-based approach to text processing. The initial push towards building such datasets began with database-text pairs of weather forecasts [20, 271] and sport summaries [15]. These datasets, and the convention that currently follows, use semi-structured data that deviates from the raw numeric signals initially used for D2T systems [266]. The statistics for prominent datasets among the ones discussed below are detailed in Table 2.</p>
<h3>2.1 Meaning Representations</h3>
<p>Mooney [217] defines a meaning representation language (MRL) as a formal unambigious language that allows for automated inference and processing wherein natural language is mapped to its</p>
<p>respective meaning representation (MR) through semantic parsing [101]. Robocup [42], among pioneering MR-to-text datasets, offers data from 1539 pairs of temporally ordered simulated soccer games in the form of MRs (pass, kick, turnover) accompanied with their respective human commentation. In order to mitigate the cost of building large-scale MR datasets, Liang et. al. [183] use grounded language acquisition to construct WeatherGov - a weather forecasting dataset with 29528 MR-text pairs, each consisting of 36 different weather states. Abstract meaning representation (AMR) [13], similarly, is a linguistically grounded semantic formalism representing the meaning of a sentence as a directed graph, as depicted in Fig. 2a. The LDC repository ${ }^{5}$ hosts various AMR-based corpora. Following this, using simulated dialogues between their statistical dialogue manager [357] and an agenda-based user simulator [283], Mairesse et. al. [204] offer BAGEL - an MR-text collection of 202 Cambridge-based restaurant descriptions each accompanied with two inform and reject dialogue types. Wen et. al. [343], through crowdsourcing, offer an enriched dataset conisiting of 5192 instances of 6 additional dialogue act types such as confirm and informonly ( 8 total) for hotels and restaurants in San Francisco. Novikova et. al. [232] show that crowdsourcing with the aid of pictorial stimuli yeild better phrased references compared to textual MRs. Following this, they released the E2E dataset ${ }^{6}$ as a part of the E2E challenge [230]. With 50,602 instances of MR-text pairs of restaurant descriptions, its lexical richness and syntactic complexity provides new challenges for D2T systems. Table 1 showcases comparative snapshots of the aforementioned datasets.</p>
<p>Table 1. Comparative showcase of sample MRs (and their corresponding narratives) from the RoboCup, WeatherGov, BAGEL, SF Hotels and Restaurants, and E2E datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">MR</th>
<th style="text-align: center;">Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RoboCup[42] <br> badPass(arg1=pink11,...), ballstopped() <br> ballstopped(), kick(arg1=pink11) <br> turnover(arg1=pink11,...)</td>
<td style="text-align: center;">pink11 makes a bad pass and was picked off by purple3</td>
</tr>
<tr>
<td style="text-align: center;">WeatherGov [183] <br> rainChance(time $=26-30, \ldots$ ), temperature(time $=17-30, \ldots$ ) <br> windDir(time $=17-30, \ldots$ ), windSpeed(time $=17-30, \ldots$ ), <br> precipPotential(time $=17-30, \ldots$ ), rainChance(time $=17-30, \ldots$ )</td>
<td style="text-align: center;">Occasional rain after 3am. Low around 43. South wind between 11 and <br> 14 mph . Chance of precipitation is $80 \%$. New rainfall amounts between <br> a quarter and half of an inch possible.</td>
</tr>
<tr>
<td style="text-align: center;">BAGEL [204] <br> inform( name(the Fountain) <br> near(the Arts Picture House) <br> area(centre), pricerange(cheap))</td>
<td style="text-align: center;">There is an inexpensive restaurant called the Fountain in the centre of <br> town near the Arts Picture House</td>
</tr>
<tr>
<td style="text-align: center;">SF Hotels \&amp; Rest. [343] <br> inform( name="red door cafe", <br> goodformeal="breakfast", <br> area="cathedral hill", kidsallowed="no")</td>
<td style="text-align: center;">red door cafe is a good restaurant for breakfast in the area of cathedral <br> hill and does not allow children.</td>
</tr>
<tr>
<td style="text-align: center;">E2E [232] <br> name[Loch Fyne], eatType[restaurant], <br> food[French], priceRange[less than $ 20$ ], <br> familyFriendly[yes]</td>
<td style="text-align: center;">Loch Fyne is a family-friendly restaurant providing wine and cheese <br> at a low cost.</td>
</tr>
</tbody>
</table>
<h1>2.2 Graph Representations</h1>
<p>Graph-to-text translation is not only central to D2T as its application carries over to numerous NLG fields such as question answering [74, 130], summarization [86], and dialogue generation [190, 216]. Further, the D2T frameworks for graph-to-text borrow heavily from the theoretic formulations offered from the literature in the field of graph neural networks (GNNs), as will be discussed in 5.1.5. The domain-specific benchmark datasets, as discussed above (see 2.1) inherently train models to generate stereotypical domain-specific text. By crowdsourcing annotations for DBPedia</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[211] graphs spanning 15 domains, Gardent et. al. [99] introduce the WebNLG dataset ${ }^{7}$. The data instances are encoded as Resource Description Format (RDF) triples of the form (subject, property, object) as depicted in Fig. 2b - (Apollo 12, operator, NASA). With 27,731 multi-domain graph-text pairs, WebNLG offers more semantic and linguistic diversity than previous datasets twice its size [342]. The abstract generation dataset (AGENDA) [168], built with knowledge graphs extracted from articles in the proceedings of AI conferences [6] using SciIE [199], offers 40,000 graph-text pairs of the article abstracts. To further promote generation challenges and cross-domain generalization, Nan et. al. [224] merge the E2E and WebNLG dataset with large heterogeneous collections of diverse predicates from Wikipedia tables annotated with tree ontologies to generate the data-record-to-text (DART) corpus. With 82,191 samples, this resulting open-domain corpus is almost quadruple the size of WebNLG.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. AMR [169] and knowledge graph [274] snapshots, representing variants of graph-based inputs to D2T systems.</p>
<h1>2.3 Tabular Representations</h1>
<p>Information represented in large tables can be difficult to comprehend at a glance, thus, table-totext (T2T) aims to generate narratives highlighting crucial elements of a tabular data instance through summarization and logical inference over the table - as showcased in Figure 3. Similar to graph-to-text, the underpinnings of tabular representation learning is also shared with other fields outside of NLG, such as the generation of synthetic network traffic [276, 353].</p>
<p>WikiBio [174], as an initial foray towards a large-scale T2T dataset, offers 700k table-text pairs of Wikipedia info-boxes with the first paragraph of its associated article as the narrative. With a vocabulary of 400 k tokens and 700 k instances, WikiBio offers a substantially larger benchmark compared to the pioneering WeatherGov and Robocup datasets that have less than 30k data-text pairs. For neural systems, as the length of output sequence increases, the generated summary diverges from the reference. As such, the RotoWire dataset [347] (Fig. 3), consisting of verbose descriptions of NBA game statistics, brings forth new challenges in long-form narrative generation as the average reference length of RotoWire is 337 words compared to 28.7 of WikiBio. Similarly, with the observation that only $60 \%$ of the content in RotoWire narratives can be traced back to the data records, Wang [335] introduce RotoWire-FG, a refined version of the original dataset aimed at tackling divergence (see 3.2), where narrative instances not grounded by their respective tables are removed from the dataset. TabFact [50] contains annotated sentences that are either supported or refuted by the tables extracted from Wikipedia. Similar to RotoWire-FG, Chen et. al. [47] offer a filtered version of TabFact by retaining only those narratives that can be logically inferred from the table.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">WIN</th>
<th style="text-align: center;">LOSS</th>
<th style="text-align: center;">PTS</th>
<th style="text-align: center;">FG.PCT</th>
<th style="text-align: center;">RB</th>
<th style="text-align: center;">AS ...</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TEAM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Heat</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">Hawks</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PLAYER</td>
<td style="text-align: center;">AS</td>
<td style="text-align: center;">RB</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">FG</td>
<td style="text-align: center;">FGA</td>
<td style="text-align: center;">CITY ...</td>
</tr>
<tr>
<td style="text-align: center;">Tyler Johnson</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Dwight Howard</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Paul Milhap</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Goran Dragic</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Wayne Ellington</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Dennis Schroder</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Rodney McGruder</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Thabo Sefolosha</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Kyle Korver</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Atlanta</td>
</tr>
</tbody>
</table>
<p>The Atlanta Hawks defeated the Miami Heat , 103 - 95 , at Philips Arena on Wednesday . Atlanta was in desperate need of a win and they were able to take care of a shorthanded Miami team here. Defense was key for the Hawks, as they held the Heat to 42 percent shooting and forced them to commit 16 turnovers. Atlanta also dominated in the paint, winning the rebounding battle, 47 - 34 , and outscoring them in the paint 58 - 26. The Hawks shot 49 percent from the field and assisted on 27 of their 43 made baskets. This was a near wire - to - wire win for the Hawks, as Miami held just one lead in the first five minutes. Miami ( 7 15 ) are as beat - up as anyone right now and it 's taking a toll on the heavily used starters. Hassan Whiteside really struggled in this game, as he amassed eight points, 12 rebounds and one blocks on 4 - of - 12 shooting ...</p>
<p>Fig. 3. Showcasing the intent of T2T, the statistics of a basketball match between the Atlanta Hawks and the Miami Heat (left) is to be translated into easily consumable narratives (right). Snapshot from the RotoWire dataset [347].</p>
<p>For controlled generation, Parikh et. al. [239] propose ToTTo which generates a single sentence description of a table on the basis of a set of highlighted cells where annotators ensure that the target summary only contains the specified subset of information. With over 120k training samples, ToTTo establishes an open-domain challenge for D2T in controlled settings. Similarly, to evaluate narrative generation in open domain settings with sentences that can be logically inferred from mathematical operations over the input table, Chen et. al. [47] modify the reference narratives of the TabFact dataset to construct LogicNLG with 7392 tables. Following this, with tables and their corresponding descriptions extracted from scientific articles, Moosavi et. al. [218] introduce SciGen, where the narratives include arithmetic reasoning over the tabular numeric entries. Building upon the long form generation premise of RotoWire, Chen et. al. [45] construct WikiTableT, a multi-domain table-text dataset with 1.5 million instances pairing Wikipedia descriptions to their corresponding info-boxes along with additional hyperlinks, named-entities, and article metadata. The majority of these datasets are available in a unified framework through TabGenie ${ }^{8}$.</p>
<p>Table 2. Highlights from prominent D2T datasets: format, number of samples (size), the number of linguistic tokens across the dataset (tokens), and availability of non-anglo-centric variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Format</th>
<th style="text-align: left;">Size</th>
<th style="text-align: center;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">MR</td>
<td style="text-align: left;">50,602</td>
<td style="text-align: center;">65,710</td>
</tr>
<tr>
<td style="text-align: left;">LDC2017T10</td>
<td style="text-align: left;">AMR</td>
<td style="text-align: left;">39,260</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (en, ru)</td>
<td style="text-align: left;">RDF</td>
<td style="text-align: left;">27,731</td>
<td style="text-align: center;">8,886</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">RDF</td>
<td style="text-align: left;">82,191</td>
<td style="text-align: center;">33,200</td>
</tr>
<tr>
<td style="text-align: left;">WikiBio</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">728,321</td>
<td style="text-align: center;">400,000</td>
</tr>
<tr>
<td style="text-align: left;">RotoWire</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">4,853</td>
<td style="text-align: center;">11,300</td>
</tr>
<tr>
<td style="text-align: left;">TabFact</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">16,573</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ToTTo</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">120,000</td>
<td style="text-align: center;">136,777</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLG</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">37,000</td>
<td style="text-align: center;">52,700</td>
</tr>
<tr>
<td style="text-align: left;">WikiTableT</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">1.5 M</td>
<td style="text-align: center;">169 M</td>
</tr>
</tbody>
</table>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.4 Data Collection \&amp; Enrichment</h1>
<p>The majority of the prominent datasets discussed in $\S 2.1$ - $\S 2.3$ are either collected by merging aligned data-narrative pairs that occur naturally in the "wild" [174, 347] or are collected through dedicated crowd-sourcing approaches [99, 230]. However, there are notable works that employ a hybrid approach to data collection. CACAPO [324], an MR-style multi-domain dataset, follows a collection process inspired by Oraby et. al. [236] wherein the naturally-occurring narratives are first scraped from the internet and are later manually annotated to generate attribute-value pairs. Similarly, Chart-to-Text [153] follows a similar mechanism of data collection wherein candidate narratives for each chart are first automatically generated via a heuristic-based approach and then are rated by crowd-sourced workers. In similar lines, the ToTTo dataset [239] discussed in $\S 2.3$ uses crowd-sourced annotators as data "cleaners" - iteratively improving upon the automatically scraped narratives, rather than annotating them from scratch - thus greatly reducing the cost of data acquisition. In addition to innovations in data collection, efforts from the D2T community has also focused on the enrichment of existing datasets. As such, Ferreira et. al. [90] augment the WebNLG dataset with intermediate representation for discourse ordering and referring expression generation. By manually delexicalizing (see $\S 4.1$ ) the narratives, Ferreira et. al. were able to automatically extract a collection of referring expressions by tokenizing the original and delexicalized texts and finding the non-overlapping tokens between them. Similarly, the authors also extracted the order of the arguments in the text by referring to the order of the general tags in the delexicalized texts. This work has also been extended to enrich the E2E dataset [92].</p>
<h2>3 DATA-TO-TEXT GENERATION FUNDAMENTALS AND NOTATIONS</h2>
<h3>3.1 What to Say and How to Say It</h3>
<p>The data instance, typically, contains more information than what we would intend for the resulting narrative to convey - verbose narratives that detail every attribute of the data instance contradicts the premise of consolidation. Thus, to figure out what to say, a subset of the original information content is filtered out based on the target audience through the process of content selection. Starting from data-driven approaches such as clustering [75] and the use of hidden Markov models (HMMs) [16], the attention of the research community has recently shifted to learning alignments between the data instance and its narrative [183]. Bisazza and Marcello [30] note that pre-reordering the source words to better resemble the target narrative yeilds significant improvements in NMT. Prior to neural explorations, learning this alignment has been explored with log-linear models [8] and tree representations [170, 171]. With what to say determined, the next step lies in figuring out how to say it, that is, the construction of words, phrases, and paragraphs - this realization of the narrative structure is known as surface realization. While traditionally, the processes of content selection and surface realization [148, 270] act as discrete parts of the generation pipeline, the neural sequence-to-sequence (seq2seq) paradigm jointly learns these aspects. For a peripheral view of the articles discussed in this section, Table 4 highlights prominent papers categorized based on their D2T tasks and the benchmark datasets used. Similarly, Fig. 5 outlines the organization of the remainder of this survey.</p>
<h3>3.2 Hallucinations and Omissions</h3>
<p>Apart from the importance of coherence and linguistic diversity in surface realization, data fidelity is a crucial aspect of D2T systems - the narrative should neither hallucinate contents absent from the data instance nor omit contents present in the data instance. Often, the divergence present in benchmark training datasets, wherein the narrative may contain data absent from the source or not cover the entirety of the data instance, is the culprit behind hallucination tendencies in</p>
<p>the model [280]. Often times, the need for both linguistic diversity and data fidelity turns into a balancing act between conflicting optimization objectives leading to novel challenges [128]. While almost all of the D2T approaches discussed below engage in balancing coherence and diversity with data-fidelity (besides $\S 5.1 .4$ Stylistic Encoding), overarchingly, the approach to balancing these conflicting objectives can be thought to take place in two forms:</p>
<ul>
<li>Architectural Interventions: The sections 5.1.1 Entity Encoders, 5.1.2 Hierarchical Encoders, 5.1.3 Plan Encoders \&amp; Autoencoders, 5.1.5 Graph Encoders, 5.1.6 Reconstruction \&amp; Hierarchical Decoders, and 5.1.10 Supplemental Frameworks suggest modifications/augmentations to the seq2seq architecture such that it fosters data-fidelity tendencies.</li>
<li>Loss-function Interventions: An alternative avenue to achieving a balance between conflicting optimization objectives is to directly model the objective functions to perform multi-task learning: as such sections $\S 5.1 .7$ Regularization Techniques and $\S 5.1 .8$ Reinforcement Learning suggest modifications/augmentations to the seq2seq loss functions.</li>
</ul>
<h1>3.3 Establishing Notation and Revisiting Seq2Seq</h1>
<p>For the consistency and readability of this survey, the notation outlining the basic encoder-decoder seq2seq paradigm [11, 54, 314, 326] in D2T (Fig. 4), as defined below and respectively compiled in Table 3, will remain valid throughout unless stated otherwise. However, the namespace for additional variable definitions in the individual sections will be limited to their mentions. Let $S=\left{x_{j}, y_{j}\right}<em j="j">{j=1}^{N}$ be a dataset of $N$ data instances $x$ accompanied with its natural language narrative $y$. Based on the construction of $S, x$ can be a set of $K$ data records $x=\left{r</em>\right}<em j="j">{j=1}^{K}$ with each entry $r$ comprised of its respective entity $r . e$ and value $r . m$ attributes or $x$ can be an instance of a directed graph $x=(V, E)$ with vertices $v \in V$ and edges $(u, v) \in E$. In the RotoWire instance (Figure 3), for $r</em>$, several alterations to modeling the attention weights have been proposed [201, 331].}=$ Heat, $r_{j} . e=$ WIN attribute would have value $r_{j} . m=11$. Given pairs $(x, y)$, the seq2seq model $f_{\theta}$ is trained end-to-end to maximize the conditional probability of generation $P(y \mid x)=\prod_{t=1}^{T} P\left(y_{t} \mid y_{&lt;t}, x\right)$. The parameterization of $f_{\theta}$ is usually carried out through RNNs such as LSTMs [29, 133] and GRUs [54], or transfomer ${ }^{5}$ architectures [326]. For attention-based RNN architectures with hidden states $h_{t}$ and $s_{t}$ for the encoder and decoder respectively, the context vector $c_{t}=\sum_{i} \alpha_{t, i} h_{i}$ weighs the encoder hidden states with attention weights $\alpha_{t, i}$. While Bahdanau et. al. [11] use a multi-layer perceptron (MLP) to model $\alpha_{t, i</p>
<p>For handling of out-of-vocabulary (OOV) tokens, Gu et. al. [119] attempt to model the rote memorization process of human learning where a language model conditioned on binary variable $z_{t} \in{0,1}$ can either generate $p_{\text {gen }}$ the next token or copy it from the source $p_{\text {copy }}$ based on their respective probabilities. While Gu et. al. [119] and Yang et. al. [355] parameterize the joint distribution over $y_{t}$ and $z_{t}$ directly (1), Gulehre et. al. [121] decompose the joint probability (2), using an MLP to model $p\left(z_{t} \mid y_{&lt;t}, x\right)$.</p>
<p>$$
\begin{gathered}
P\left(y_{t}, z_{t} \mid y_{&lt;t}, x\right) \propto\left{\begin{array}{l}
p_{\text {gen }}\left(y_{t}, y_{&lt;t}, x\right) z_{t}=0 \
p_{\text {copy }}\left(y_{t}, y_{&lt;t}, x\right) z_{t}=1, y_{t} \in x \
0 \quad z_{t}=1, y_{t} \notin x
\end{array}\right. \
\left{\begin{array}{l}
p_{\text {gen }}\left(y_{t} \mid z_{t}, y_{&lt;t}, x\right) p\left(z_{t} \mid y_{&lt;t}, x\right) z_{t}=0 \
p_{\text {copy }}\left(y_{t} \mid z_{t}, y_{&lt;t}, x\right) p\left(z_{t} \mid y_{&lt;t}, x\right) z_{t}=1
\end{array}\right.
\end{gathered}
$$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Attention-based seq2seq framework: The encoder consumes the sequential input translating it to a weighed hidden representation to be then consumed and decoded into linguistic tokens by the decoder.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Data-to-text Generation Taxonomy Corresponding to Sections in the Survey Design</p>
<p>Similar to the greater NLG paradigm, different strategies for modeling the conditional probability of generation $P(y \mid x)$, the attention mechanisms $\left{\alpha_{t, i}, c_{t}\right}$, and the copy mechanisms $\left{p_{\text {gen }}, p_{\text {copy }}\right}$, as discussed below, often form the basis for D2T innovations. In addition to this, variations in training strategies such as teacher-forcing [346], reinforcement learning [315], and autoencoder-based reconstruction [41] open up further avenues for D2T innovation.</p>
<h1>4 INNOVATIONS IN DATA PREPROCESSING</h1>
<p>Contrary to the other facets of NLG, such as chatbots, for which large-scale data can be harvested [1, 198], D2T datasets are often smaller in scale and task-specific. Ferreira et. al. [88] note that phrase-based translation models [166] can outperform neural models in such data sparsity. As such, delexicalization, noise reduction, linearization, and data augmentation are preprocessing techniques often employed to tackle said sparsity of training data.</p>
<h3>4.1 Delexicalization \&amp; Noise Reduction</h3>
<p>Delexicalization, often referred to as anonymization, is a common practice in D2T [79, 204] wherein the slot-value pairs for the entities and their attributes in training utterances are replaced with a</p>
<p>Table 3. Notation descriptions</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Notation</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$S$</td>
<td style="text-align: left;">Dataset</td>
</tr>
<tr>
<td style="text-align: left;">$(x, y) \in S$</td>
<td style="text-align: left;">Data instance $x$ and its natural language representation $y$</td>
</tr>
<tr>
<td style="text-align: left;">$r=(r . e, r . m)$</td>
<td style="text-align: left;">Data record $r$ with its entity $r . e$ and value $r . m$ attributes</td>
</tr>
<tr>
<td style="text-align: left;">$G=(V, E)$</td>
<td style="text-align: left;">Graph instance $G$ with vertices $V$ and edges $E$</td>
</tr>
<tr>
<td style="text-align: left;">$(u, v) \in E$</td>
<td style="text-align: left;">Nodes $u$ and $v$ of an edge $E$</td>
</tr>
<tr>
<td style="text-align: left;">$f_{\partial} \in\left{f_{1}, \ldots, f_{n}\right}$</td>
<td style="text-align: left;">Model $f_{\partial}$ that may belong to an ensemble $\left{f_{1}, \ldots, f_{n}\right}$</td>
</tr>
<tr>
<td style="text-align: left;">$P(y \mid x)$</td>
<td style="text-align: left;">Conditional probability of sequence $y$ given $x$</td>
</tr>
<tr>
<td style="text-align: left;">$h_{t}, s_{t}$</td>
<td style="text-align: left;">Encoder $h_{t}$ and decoder $s_{t}$ hidden state representations</td>
</tr>
<tr>
<td style="text-align: left;">$c_{t}, \alpha_{t, i}$</td>
<td style="text-align: left;">Context vector $c_{t}$ weighing $h_{t}$ with attention weights $\alpha_{t, i}$</td>
</tr>
<tr>
<td style="text-align: left;">$p_{g e n}, p_{c o p y}$</td>
<td style="text-align: left;">Token generation $p_{g e n}$ or copying $p_{c o p y}$ probabilities</td>
</tr>
<tr>
<td style="text-align: left;">$z_{t} \in{0,1}$</td>
<td style="text-align: left;">Binary variable that selects either $p_{g e n}$ or $p_{c o p y}$</td>
</tr>
<tr>
<td style="text-align: left;">$W_{t \in \mathbb{N}}, b_{i \in \mathbb{N}}$</td>
<td style="text-align: left;">Arbitrary weights and biases parameterizing $f_{\partial}$</td>
</tr>
</tbody>
</table>
<p>placeholder token such that weights between similar utterances can be shared [227] - as illustrated in Figure 6a. These placeholder tokens are later replaced with tokens copied from the input data instance [174]. In comparison to copy-based methods for handling rare entities, delexicalization has shown to yield better results in constrained datasets [300].</p>
<p>From the notion that delexicalization of the data instance may cause the loss of vital information that can aid seq2seq models in sentence planning, where some data instance slots may even be deemed nondelexicalizable [343], Nayak et. al. [227] explore different nondelexicalized input representations (mention representations) along with grouping representations as a form of sentence planning (plan representations). The authors note improvements over delexicalized seq2seq baselines when input mentions are concatenated with each slot-value pair representing a unique embedding. The efficacy of such concatenation is also corroborated by Freitag and Roy [95]. Further, the addition of positional tokens representing intended sentence position to the input sequence offers further improvements. Addressing this, in addition to delexicalizing categorical slots, Juraska et. al. [150] employ hand-crafted tokens for values that require different treatment in their verbalization: for the slot food, the value Italian is replaced by slot_vow_cuisine_food indicating that the respective utterance should start with a vowel and the value represents a cuisine - an Italian restaurant. Perez-Beltrachini and Lapata [244] delexicalize numerical expressions, such as dates, using tokens created with the attribute name and position of the delexicalised token. Colin and Gardent [59] note performance improvements with an extensive anonymization scheme wherein all lemmatized content words (expect adverbs) are delexicalized as compared to restricting delexicalization to named entities.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Illustrations of delexicalization in MRs [300] and linearization of graphs [274].</p>
<p>The presence of narratives that fail to convey vital attributes of data instances leads to semantic noise in the dataset [84, 136]. Duek et. al. [78] employ slot matching [263] to clean the E2E corpus for semantic correctness and explore the impact of semantic noise on neural model performance.</p>
<p>Similarly, Obeid and Hoque [233] substitute data mentions in the narrative, identified through Named Entity Recognition (NER), with a predefined set of tokens which are later replaced through a look-up operation. Liu et. al. [194] focus on generating faithful narratives and truncate the reference narratives by retaining only the first few sentences since the latter are prone to have been inferred from the table.</p>
<h1>4.2 Linearization</h1>
<p>Frameworks for MR (and graph) narration that shy from dedicated graph encoders rely on effective linearization techniques - the representation of graphs as linear sequences, as illustrated in Figure 6b. While Ferreira et. al. [88] note improvements in neural models with the adoption of a 2-step classifier [177] that maps AMRs to the target text, Konstas et. al. [169] showcase agnosticsm to linearization orders by grouping and anonymizing graph entities for delexicalization with the Stanford NER [93]. The reduction in graph complexity and subsequent mitigation of the challenge brought forth by data sparsity lends any depth-first traversal of the graph as an effective linearization approach. Moryossef et. al. [219] append text plans modeled as ordered trees [308] to the WebNLG training set and use an off-the-shelf NMT system [121] for plan-to-text generation. However, the authors note that the restriction of requiring single entity mentions in a sentence establishes their approach as dataset dependent.</p>
<p>For pretrained language models such as GPT-2 [257] and T5 [258], Zhang et. al. [361] and Gong et. al. [109] represent tables as a linear sequence of attribute-value pairs and use a special token as the separator between the table data and the reference text. It should be noted that T5, while performing the best on automated metrics, fails to generate good summaries when numerical calculations are involved. Chen et. al. [47] traverse the table horizontally, each row at a time, where each element is represented by its corresponding field and cell value separated by the keyword is. For scientific tables, Suadaa et. al. [312] view a table $T_{D}$ as a set of cells with their corresponding row and column headers $h=[r h: c h]$ with $t h$ for overlapping tokens, numerical value $v a l$, and metric-type $m$. The cells are marked with target flag tgt which is set to 1 for targeted cells and 0 otherwise respective to the content plan. The linearization of the resulting tables is done with templates that consist of concatenation $T_{D}=[h: t h: v a l: m: t g t]$, filtration based on tgt, pre-computed mathematical operations, and their respective combinations.</p>
<h3>4.3 Data Augmentation</h3>
<p>Often, appending contextual examples from outer sources to the training set, or permuting the training samples themselves to append variation, helps mitigate data sparsity. This is known as data augmentation. Nayak et. al. [227] propose the creation of pseudo-samples by permuting the slot orderings of the MRs while keeping the utterances intact. Juraska et. al. [150], however, take an utterance-oriented approach where pseudo-samples are built by breaking training MRs into single-sentence utterances. For the shared surface realization task [213], Elder and Hokamp [83] augment the training set with sentences from the WikiText corpus [212] parsed using UDPipe [309]. Following this premise, Kedzie and Mckeown [159] curate a collection of utterances from novel MRs using a vanilla seq2seq model with noise injection sampling [53]. The validity of the MRs associated with these utterances are computed through a CNN-based parser [162] and the valid entries are augmented to the training set. However, it is worth noting, that performance gains from augmenting the training set with out-of-domain (OOD) instances, tend to saturate after a certain point [95]. Also, practitioners of data augmentation should note that caution is advised when augmenting with synthetic data, as the inclusion of such data may reinforce the mistakes of the model [126].</p>
<p>Chen et. al. [46] append knowledge-graphs representing external context to the table-text pairs and quantify its efficacy through their metric KBGain - the ratio of tokens unique to the external context to the total number of tokens in the narrative. Similarly, Ma et. al. [202] augment the limited training data for table-text pairs by assigning part-of-speech (POS) tags for each word in the reference and further increase the robustness of their model with adversarial examples created by randomly adding and removing words from the input. In contrast, Chen et. al. [47] create adversarial examples by randomly swapping entities in the narrative with ones that appear in the table. Following this, Liu et. al. [194] use an augmented plan consisting of table records and entities recognized from the reference narrative which eliminates the inclusion of information not present in the table. For few-shot learning, Liu et. al. [189] observed that the performance of a GPT-3 model [37] improved upon providing in-context examples computed based on their k-nearest neighbor $(k=2)$ embeddings.</p>
<h1>5 INNOVATIONS IN THE SEQ2SEQ FRAMEWORK</h1>
<p>Seq2Seq models (see 3.3), serve as the basis for neural NLG [54, 314, 326]. As such, to compare the efficacy of neural architectures for long-form D2T, Wiseman et. al. [347] compare the performance of various seq2seq models to their templated counterparts on the RotoWire dataset. Based on their observations, the conditional copy model [121] performs the best on both word-overlap and extractive metrics (see $\S 6.2 .1$ ) compared to the standard attention-based seq2seq model [11] and its joint copy variant [119]. Similarly, in an evaluation of 62 seq2seq, data-driven, and templated systems for the E2E shared task, Duek et. al. [81] note that seq2seq systems dominate in terms of both automated word-based metrics and naturalness in human judgement. Wiseman et. al. [347], however, note that the traditional templated generation models outperform seq2seq models on extractive metrics although they score poorly on word-overlap metirics. Thus, the adaptation of seq2seq models to D2T for richer narratives with less omissions and hallucinations still remains an active focus of the research community.</p>
<p>It is worth noting that all seq2seq models discussed below operate at the word level. Models operating at the character level [3, 112, 277] have shown reasonable efficacy with the added computational savings from forgoing the preprocessing steps of delexicalization and tokenization. However, the attention garnered by them from the research community is slim. From their comparative analysis, Jagfeld et. al. [140] note that as character-based models perform better on the E2E dataset while word-based models perform better on the more linguistically challenging WebNLG dataset, it is hard to draw conclusions on the framework most suited for generic D2T. In the sections that follow, we detail notable innovations over the last half-decade in seq2seq modeling, branched on the basis of their training strategies - supervised and unsupervised learning.</p>
<h3>5.1 Supervised Learning</h3>
<p>5.1.1 Entity Encoders. Centering theory [118], as well as many other noted linguistic frameworks [40, 57, 125, 156, 172, 251], highlight the critical importance of entity mentions to the coherence of the generated narrative. The ordering of these entities ( $r . e$ in $\S 3.3$ ) is crucial for such narratives to be considered as entity coherent [154]. Unlike typical language models which are conditioned solely based on previously generated tokens $c_{t}$, Lebret et. al. [174] provide additional context $\left{z_{c_{t}}, g_{f}, g_{w}\right}$ to the generation where $z_{c_{t}}$ represents table entity $c_{t}$ as a triplet of its corresponding field name, start, and end positions, and $\left{g_{f}, g_{w}\right}$ are one-hot encoded vectors where each element indicates the presence of table entities from the fixed field and word vocabularies - illustrated in Fig. 7. Similarly, Bao et. al. [14] encode the table cell $c$ and attributes $a$ as the concatenation $\left[e_{i}^{c}: e_{i}^{a}\right]$ where the decoder uses this vector to compute the attention weights. Liu et. al. [193]</p>
<p>Table 4. Task and dataset based summarization of noted D2T frameworks over the last half-decade.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Publication Highlights</th>
<th style="text-align: center;">Framework \&amp; Human Evaluation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MR-to-Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Robocup \&amp; WeatherGov</td>
<td style="text-align: center;">[210] Coarse-to-fine aligner \&amp; penalty based on learned priors</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + regularization</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Recipe \&amp; SF H\&amp;R</td>
<td style="text-align: center;">[160] Neural agenda-checklist modeling</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + agenda encoders</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">BAGEL</td>
<td style="text-align: center;">[79] Retanking beam outputs w/ BNN-based reranker</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + reranker</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Restaurant Ratings</td>
<td style="text-align: center;">[227] Nondelexicalized inputs w/ data augmentation</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WikiData</td>
<td style="text-align: center;">[52] Complementary text-to-data translation</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">E2E</td>
<td style="text-align: center;">[81] Comparative evaluation of 62 systems</td>
<td style="text-align: center;">Seq2Seq + Data-driven + Templated</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[256] MLP encoder attuned to the dataset</td>
<td style="text-align: center;">MLP $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[360] Two-level hierarchical encoder</td>
<td style="text-align: center;">CAEncoder $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[150] Ensemble w/ heuristic reranking</td>
<td style="text-align: center;">Ensemble w/ LSTM + CNN</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[310] Hierarchical decoding with POS tags</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[95] Unsupervised DTG with DAEs</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + DAEs</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[103] Comparative evaluations w/ ensembling \&amp; penalties</td>
<td style="text-align: center;">Ensemble w/ LSTM + T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[62] Syntactic controls with SC-LSTM</td>
<td style="text-align: center;">SC-LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[297] Computational pragmatics based DTG</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[59] Extensive anonymization</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[78] Semantic correctness in neural DTG</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[159] Self-training w/ noise injection sampling</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[277] Char-level GRU w/ input reconstruction</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[97] CRFs w/ Gumbel categorical sampling</td>
<td style="text-align: center;">CRF</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Graph-to-Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGENDA</td>
<td style="text-align: center;">[168] Graph-centric Transformer \&amp; AGENDA dataset</td>
<td style="text-align: center;">T $\rightarrow$ LSTM + LSTM encoding</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">LDC2015E25</td>
<td style="text-align: center;">[88] Phrase vs Neural MR-text w/ preprocessing analysis</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Phrase-based</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">LDC2015E86</td>
<td style="text-align: center;">[169] Unlabeled pre-training \&amp; linearization agnosticism</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[273] Dual encoding for hybrid traversal</td>
<td style="text-align: center;">GNN $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">LDC2017T10</td>
<td style="text-align: center;">[12] Graph reconstruction w/ node \&amp; edge projection</td>
<td style="text-align: center;">T $\rightarrow$ T + reconstruction loss</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[203] Fine-tuning GPT-2 on AMR-text joint distribution</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">[207] Graph encoding with GCNs</td>
<td style="text-align: center;">GCN $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[68] LSTM based triple encoder</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[91] Discrete neural pipelines \&amp; comparisons to end-to-end</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[219] Sentence planning with ordered trees</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[275] Complementary graph contextualization</td>
<td style="text-align: center;">GAT $\rightarrow$ T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[306] Detachable multi-view reconstruction</td>
<td style="text-align: center;">T $\rightarrow$ T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[364] Dual encoder for structure and planning</td>
<td style="text-align: center;">GCN $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[274] Task-adaptive pretraining for PLMs</td>
<td style="text-align: center;">BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[2] Knowledge enhanced language models \&amp; KeLM dataset</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[158] Graph-text joint representations \&amp; pretraining strategies</td>
<td style="text-align: center;">BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Record-to-Text (Table-to-text)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WikiBio</td>
<td style="text-align: center;">[174] Tabular positional embeddings \&amp; WikiBio dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Kneser-Ney</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[14] Encoding tabular attributes \&amp; WikiTableText dataset</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[193] Field information through modified LSTM gating</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[292] Link-based and content-based attention</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[254] Multi-instance learning w/ alignment-based rewards</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[202] Key fact identification and data augment for few shot</td>
<td style="text-align: center;">LSTM + T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[191] Hierarchical encoding w/ supervised auxiliary learning</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[192] Forced attention for omission control</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[46] External contextual information w/ knowledge graphs</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[318] Confidence priors for hallucination control</td>
<td style="text-align: center;">BERT + Pointer Networks</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[51] Soft copy switching policy for few-shot learning</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[356] Variational autoencoders for template induction</td>
<td style="text-align: center;">VAE modified to VTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[337] Autoregressive modeling with iterative text-editing</td>
<td style="text-align: center;">Pointer networks + Text editing</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[365] Reinforcement learning with adversarial networks</td>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[105] Linearly combined multi-reward policy</td>
<td style="text-align: center;">Pointer networks</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[354] Source-target disagreement auxiliary loss</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[311] BERT-based IR system for contextual examples</td>
<td style="text-align: center;">T5 + BERT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[347] Classification-based metrics \&amp; RotoWire dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Templated</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[229] Numeric operations and operation-result encoding</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + operation encoders</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[253] Dynamic hierarchical entity-modeling \&amp; MLB dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[252] Content selection \&amp; planning w/ gating and IE</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Rotowire</td>
<td style="text-align: center;">[107] Contextualized numeric representations</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[139] Dynamic salient record tracking w/ stylized generation</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[261] Two-tier hierarchical input encoding</td>
<td style="text-align: center;">T $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[180] Auxiliary supervision w/ reasoning over entity graphs</td>
<td style="text-align: center;">LSTM + GAT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[255] Paragraph-centric macro planning</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[254] Interweaved plan and generation w/ variational models</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Publication Highlights</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Framework \&amp; Human Evaluation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Record-to-Text (Continued)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TabFact</td>
<td style="text-align: center;">[47] Coarse-to-fine two-stage generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T + GPT-2 + BERT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WikiPerson</td>
<td style="text-align: center;">[339] Disagreement loss w/ optimal-transport matching loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
</tr>
<tr>
<td style="text-align: center;">Humans, Books \&amp; Songs</td>
<td style="text-align: center;">[109] Attribute prediction-based reconstruction loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">ToTTo <br> LogicNLG <br> NumericNLG</td>
<td style="text-align: center;">[189] Contextual examples through k nearest neighbors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[312] Targeted table cell representation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[49] Semantic confounders w/ Pearl's do-calculus</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DCVED + GPT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[187] Table-to-logic pretraining for logic text generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + BART</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[223] Faithful generation with unlikelihood \&amp; replacement detection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[44] Table serialization and structural encoding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[7] T5 infused with tabular embeddings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Cross-domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">E2E <br> WebNLG <br> DART <br> WikiBio <br> RotoWire <br> WITA</td>
<td style="text-align: center;">[140] Char-based vs word-based seq2seq</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[348] Template induction w/ neural HSMM decoder</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">HSMM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[98] Training w/ partially aligned dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T + supportiveness</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[157] Iteratively editing templated text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2 + LaserTagger</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[127] RoBERTa-based semantic fidelity classifier</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2 + RoBERTa</td>
<td style="text-align: center;">T</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[48] Knowledge-grounded pre-training \&amp; KGTEXT dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T + GAT</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[186] Hybrid attention-copy for stylistic imitation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[351] Disambiguation and stitching with PLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT3 + T5</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[77] Unified learning of D2T and T2D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + VAE</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[144] Search and learn in a few-shot setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + Search \&amp; Learn</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Timeseries-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WebNLG \&amp; DART</td>
<td style="text-align: center;">[294] Open-domain transfer learning for time-series narration</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BART + T5 + Timeseries analysis</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Chart-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chart2Text</td>
<td style="text-align: center;">[233] Preprocessing w/ variable substitution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Chart-to-text</td>
<td style="text-align: center;">[153] Neural baselines for Chart-to-text dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T + BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
</tbody>
</table>
<p>modify the LSTM unit with a field gate to update the cell memory indicating the amount of entity field information to be retained in the cell memory. Following [174], Ma et. al. [202] use a Bi-LSTM to encode the concatenation of word, attribute and position embeddings. However, to indicate whether an entity is a key fact, a multi-layer perceptron (MLP) classifier is used on said representation for binary classification. Inspired from Liu and Lapata [195], Gong et. al. [108] construct a historical timeline by sorting each table record with respect to its date field. Three encoders encode a table entity separately in row $r_{i, j}^{r}$, column $r_{i, j}^{c}$, and time $r_{i, j}^{t}$ dimensions. The concatenation of these representations are fed to a MLP to obtain a general representation $r_{i, j}^{\text {gen }}$ over which specialized attention weights are computed to obtain the final record representation as $\hat{r}<em r="r">{i, j}=\alpha</em>$ of the input table on top of the token embeddings for table-structure learning in a T5 model.} r_{i, j}^{r}+\alpha_{c} r_{i, j}^{c}+\alpha_{l} r_{i, j}^{t}$. Exploiting the attributes of the E2E dataset - the set number of unique MR attributes and the limited diversity in lexical instantiations of their values, Puzikov and Gurevych [256] employ a simple approach wherein the recurrent encoder is replaced with one dense layer that takes in MR representations through embedding lookup. Similarly, to keep track of entity mentions in the SF dataset for long-form text generation, Kiddon et. al. [160] introduce a checklist vector $a_{t}$ that aids two additional encoders to track used (mentioned in the resulting narrative) and new (not mentioned as of time step $t$ ) items on the defined agenda. The output hidden state is modeled as a linear interpolation between the three encoder states - $c_{t}^{g r u}$ of the base GRU and $\left{c_{t}^{\text {new }} \cdot c_{t}^{\text {used }}\right}$ from the agenda models, weighted by a probabilistic classifier. Extending this concept of entity encoding to transformer-based architectures, Chen et. al. [44] adapt the multi-headed attention layer architecture [326] to encode serialized table attributes that is then fed to a GPT-based decoder. Similarly, as a unified text-to-text alternative approach to [44], Andrejczuk et. al. [7] include the row and column embeddings $\hat{r}_{i, j</p>
<p>Certain D2T tasks, such as sport commentaries [15, 279, 316], require reasoning over numeric entities present in the input data instance. Although numeracy in language modeling is a prominent niche of its own [295, 317, 334], notable D2T-specific approaches include that of Nie et. al. [229] precomputing the results of numeric operations $o p_{i} \in{$ minus, argmax $}$ on the RotoWire dataset,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table $\left(\mathbf{g}<em _mathbf_w="\mathbf{w">{\mathbf{r}}, \mathbf{g}</em>\right)$}</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Input Text</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">John Doe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">John</td>
<td style="text-align: center;">Doe</td>
<td style="text-align: center;">(</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">April</td>
</tr>
<tr>
<td style="text-align: center;">Birthdate</td>
<td style="text-align: center;">18 April 1352</td>
<td style="text-align: center;">$\varepsilon_{i}$</td>
<td style="text-align: center;">13944</td>
<td style="text-align: center;">UNK</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{x}_{w}$</td>
<td style="text-align: center;">(name,1,2)</td>
<td style="text-align: center;">(name,2,1)</td>
<td style="text-align: center;">$\phi$</td>
<td style="text-align: center;">(birth.,1,3)</td>
<td style="text-align: center;">(birth.,2,2)</td>
</tr>
</tbody>
</table>
<p>Fig. 7. Entity encoding scheme - Lebret et. al. [174]
the authors propose the combination of dedicated operation and operation-result encoders, the latter utilizing a quantization layer for mapping lexical choices to data values, in addition to a record encoder. In similar fashion to [355], the concatenated embeddings ${r . i d x, r . e, r . m}$ fed to a bi-directional GRU generate record representations while the concatenated embeddings of $o p_{i}$ attributes fed to a non-linear layer yields operation representations. To address the difficulty in establishing lexical choices on sparse numeric values [271, 303], the authors add quantization to the operation-results encoder that maps results of scalar operations $e$ (minus) to $l \in L$ possible bins through a weighed representation ( $h_{i}=\sum_{l} \mu_{i, l} e$ ) using softmax scores of each individual result $\mu_{i, l}$. Following this body of work, to contextualize numeric representations and thus understand their logical relationships, Gong et. al. [107] feed raw numeric embeddings for all numericals corresponding to the same table attributes to a transformer-based encoder to obtain their contextualized representations. Through a ranking scheme based on a fully connected layer, these contextualized representations are further trained to favor larger numbers.
5.1.2 Hierarchical Encoders. The intuition behind the use of hierarchical encoders, in the context of D2T, is to model input representations at different granularities, either through dedicated modules [191, 261, 360] or attention schemes [193, 253, 355]. As such, Zhang et. al. [360] leverage their CAEncoder [359] to incorporate precomputed future representations $h_{i+1}$ into current representation $h_{i}$ through a two-level hierarchy. Similarly, Rebuffel et. al. [261] propose a two-tier encoder to preserve the data structure hierarchy - the first tier encodes each entity $e_{i}$ based on its associated record embeddings $r_{i, j}$ while the second tier encodes the data structure based on its entity representation $h_{i}$ obtained through the individual embeddings $r_{i, j}$. On the other hand, Liu et. al. [191], as illustrated in Fig. 8b, propose a word-level $h_{r . e}^{r . m}$ and an attribute-level $H^{r . e}$ two-encoder setup to capture the attribute-value hierarchical structure in tables. The attribute-level encoder takes in the last hidden state $h_{l a s t}^{r . e}$ for attribute $r . e$ from the word level LSTM as its input. Using these hierarchical representations, fine-grained attention $\beta_{r . e}^{r . m} \propto g\left(h_{r . e}^{r . m}, s_{t}\right)$ and coarse-grained attention $\gamma^{r . e} \propto g\left(H^{r . e}, s_{t}\right)$ are used for decoding where $g$ represents a softmax function. Similarly, based on hierarchical attention [355], Liu et. al. [193] employ an attention scheme that attends to both word level and field level tokens. Following this, Puduppully et. al. [253] propose language modeling conditioned on both the data instance and a dynamically updated entity representation. At each time-step $t$, a gate $\gamma_{t}$ is used to decide whether an update is necessary for the entity memory representation $u_{k}$ and a parameter $\delta_{t, k}$ decides the impact of said update (3).</p>
<p>$$
\gamma_{t}=\sigma\left(W_{1} s_{t}+b_{1}\right) \&amp; \delta_{t, k}=\gamma_{t} \odot \sigma\left(W_{2} s_{t}+W_{3} u_{t-1, k}+b_{3}\right)
$$</p>
<p>5.1.3 Plan Encoders \&amp; Autoencoders. Traditionally, the what to say aspect of D2T (see 3.1) used to be its own module in a set of pipelines [100, 270], thus offering flexibility in planning the narrative structure. However, the end-to-end learning paradigm often models content selection and surface realization as a shared task [174, 210, 347]. Although convenient, without explicitly</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 8. Illustrations for plan [252] and hierarchical [191] encoding schemes.
modeling the planning of the narrative (Fig. 8a), language models struggle to keep coherence in long-form generation tasks. As such, Puduppully et. al. [252] model the generation probability $P(y \mid r)$ as the joint probability of narrative $y$ and content plan $z$ given a record $r$ such that $P(y \mid r)=$ $\sum_{z} P(z \mid r) P(y \mid r, z)$. Similar to their prior work [253], a content selection gate operates over the record representation $r_{i}$ giving an information controlled representation $r_{i}^{c s}$. The elements of $z$ are extracted using an information extraction system [347] and correspond to entities in $y$ while pointer networks [331] are use to align elements in $z$ to $r$ during training. Iso et. al. [139], on the other hand, avoid precomputing content plans $z$ by dynamically choosing data records during decoding an additional memory state $h^{e n t}$ remembers mentioned entities and updates the language model state $h^{l m}$ accordingly. The authors propose two representations for an entity - static embedding $e$ based on row $r_{i}$ and aggregated embedding $\bar{e}$ based on all rows where the entity appears. In the context of the RotoWire dataset, the aggregate embedding $\bar{e}$ is supposed to represent how entity $e$ played in the game. For $h_{t}=\left{h_{t}^{l m}, h_{t}^{e n t}\right}, P\left(z_{t}=1 \mid h_{t-1}\right)$ (4) models the transition probability, and based on whether $e$ belongs to the set of entities $\epsilon_{t}$ that have already appeared at time step $t, P\left(e_{t}=e \mid h_{t-1}\right)(5)$ computes the next probable entity $e$ to mention. The authors note that such discrete tracking dramatically suppresses the generation of redundant relations in the narrative.</p>
<p>$$
\begin{aligned}
&amp; P\left(z_{t}=1 \mid h_{t-1}\right)=\sigma\left(W_{1}\left(h_{t-1}^{l m} \oplus h_{t-1}^{e n t}\right)\right) \
&amp; P\left(e_{t}=e \mid h_{t-1}\right) \propto \begin{cases}e^{\left(h_{s}^{e n t} W_{1} h_{t-1}^{l m}\right)} &amp; e \in \epsilon_{t-1} \
e^{\left(\bar{e} W_{2} h_{t-1}^{l m}\right)} &amp; \text { otherwise }\end{cases}
\end{aligned}
$$</p>
<p>With the premise that paragraphs are the smallest sub-categorization where coherence and topic are defined [358], Puduppully and Lapata [255] propose a paragraph-based macro planning framework specific to the design of MLB [253] and RotoWire [347] datasets where the input to the seq2seq framework are predicted macro-plans (sequence of paragraphs). Building upon this, in contrast to precomputing global macro plans, Puduppully et. al. [254] interweave the macro planning process with narrative generation where latent plans are sequentially inferred through a structured variational model as the narrative is generated conditioned on the plans so far and the previously generated paragraphs. Similarly, to establish order in the generation process, Sha et. al. [292] incorporate link-based attention [114] in addition to content-based attention [11] into their framework. Similar to transitions in Markov chains [155], a link matrix $\mathbb{L} \in \mathbb{R}^{n_{f} \times n_{f}}$ for $n_{f}$ tabular attributes defines the likelihood of transitioning from the mention of attribute $i$ to $j$ as $\mathbb{L}\left(f_{j}, f_{i}\right)$. Wang et. al. [337] propose combining autoregressive modeling [287] to generate skeletal plans and using an iterative text-editing based non-autoregressive decoder [120] to generate narratives constrained on</p>
<p>said skeletal plans. The authors note that this approach reduces hallucination tendencies of the model. Similarly, motivated by the strong correlation observed between entity-centric metrics for record coverage and hallucinations, Liu et. al. [194] adopt a two-stage generation process where a plan generator first transforms the input table records into serialized plans $R \rightarrow R+P$ based on the separator token $S E P$ and then translates the plans into narratives with the help of appended auxiliary entity information extracted through NER.</p>
<p>Handcrafted templates traditionally served as pre-defined structures where entities computed through content selection would be plugged-in. However, even in the neural D2T paradigm, inducing underlying templates helps capture the narrator voicing and stylistic representations present in the training set. As such, Ye et. al. [356] extend the use of the variational autoencoders (VAEs) [163] for template induction with their variational template machine (VMT) that disentangles the latent representation of the template $z$ and the content $c$. In essence, the model can be trained to follow specific templates by sampling from $z$. Inspired from stylistic encoders [137], the authors further promote template learning by anonymizing entities in the input table thus effectively masking the content selection process. Similarly, to mitigate the strong model biases in the standard conditional VAEs [319], Chen et. al. [49] estimate semantic confounders $z_{c}$ - linguistically similar entities to the target tokens that confound the logic of the narrative. Compared to the standard formulation $p(y \mid x)$, the authors employ Pearl's do-calculus [242] to learn the objective $p(y \mid \operatorname{do}(x))$ that asserts that confounder $z_{c}$ is no longer determined by instance $x$, thus ensuring logical consistency in the narrative. To ensure that the estimated confounders are meaningful, they are grounded through proxy variables $c$ such that confounding generation $p\left(c \mid z_{m}\right)$ can be minimized. Recently, modeling D2T and T2D as complementary tasks, Doung et. al. [77] leverage the VAE formulation with the underlying architecture of a pre-trained T5 model to offer a unified multi-domain framework for the dual task. To combat the lack of parallel-corpora for the back-translation (T2D) training, the authors introduce latent variables to model the marginal probabilities of back-translation through an iterative learning process.</p>
<p>Likewise, for approaches beyond the use of autoencoders, Chen et. al. [47] take inspiration from practices in semantic parsing [71] and propose a coarse-to-fine two-stage generation scheme. In the first stage, a template $Y_{T}$ containing placeholder tokens $E N T$ is generated, representing the global logical structure of the narrative. The entities are then copied over from the input data instance to replace tokens $E N T$ in the second step to generate the final narrative $\hat{Y}$. Suadaa et. al. [312], similarly, follow template-guided generation [151] (see $\S 4.2$ ) where the precomputed results of numeric operations are copied over to the template and replace the placeholder tokens. For Pre-trained Language Models (PLMs), the authors incorporate copying into the fine-tuning stage for this action.
5.1.4 Stylistic Encoders. In addition to the traits of coherence, fluency, and fidelity, stylistic variation is crucial to NLG [307]. It is interesting to note that the n-gram entropy of generated texts in seq2seq based NLG systems are significantly lower than that in its training data - leading to the conclusion that these systems adhere to only a handful of dominant patterns observed in the training set [237]. Thus, introducing control measures to text generation has recently garnered significant attention from the NLG community [137, 344]. As such, the semantically conditioned LSTM (SC-LSTM) proposed by Wen et. al. [343] extends the LSTM cell to incorporate a one-hot encoded MR vector $d$ that takes the form of a sentence planner. Following this, Deriu and Cieliebak [62] append additional syntactic control measures to the MR vector $d$ (such as the first token to appear in the utterances and expressions for different entity-value pairs) by simply appending one-hot vectors representation of these control mechanisms to $d$. Similarly, Lin et. al. [186] tackle the lack of a template-based parallel dataset with style imitation - as illustrated in Fig. 9, for each</p>
<p>instance $(x, y)$, an exemplar narrative $y_{e}$ is retrieved from the training set based on field-overlap distance $D\left(x, x_{e}\right)$ and an additional encoder is used to encode $y_{e}$. The model is trained with competing objectives for content determination $P\left(y \mid x, y_{e}\right)$ and style embodiment $P\left(y_{e} \mid x_{e}, y_{e}\right)$ with an additional content coverage constraint for better generation fidelity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Food</th>
<th style="text-align: center;">Area</th>
<th style="text-align: center;">Price</th>
<th style="text-align: center;">Near</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Loch Fyne</td>
<td style="text-align: center;">Italian</td>
<td style="text-align: center;">Riverside</td>
<td style="text-align: center;">20-25</td>
<td style="text-align: center;">Strada</td>
</tr>
<tr>
<td style="text-align: center;">Exemplar $y_{e}$</td>
<td style="text-align: center;">Zizzi is a pub providing fine French dining but with an expensive price, located near Cocum in the city center.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generation $y$</td>
<td style="text-align: center;">Loch Fyne provides fine Italian dining with a <br> $ 20-25$ price, located near Strada at the riverside.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Fig. 9. Style imitation with exemplar narratives - Lin et. al. [186]
5.1.5 Graph Encoders. The use of explicit graph encoders in D2T stems from the intuition that neural graph encoders such as Graph Convolutional Networks (GCNs) [164] have strong relational inductive baises that produce better representations of input graphs [18] as an effective alternative to linearization. This entails generating representations for the nodes $v \in V$ and edges $(u, v) \in E$ in the input graph.</p>
<p>GCNs \&amp; Graph-RNNs: Marcheggiani and Titov [208] compute node represenations $h_{v}^{\prime}$ (6) through explicit modeling of edge labels $l a b(u, v)$ and directions $\operatorname{dir}(u, v) \in{$ in, out, loop $}$ for each neighboring node $u \in N(v)$ in their GCN parameterization where learned scalar gates $g_{u, v}$ weigh the importance of each edge. With residual $\left(h_{v}^{r}=h_{v}^{\prime}+h_{v}\right)$ [129] and dense $\left(h_{v}^{d}=\left[h_{v}^{\prime} ; h_{v}\right]\right)$ [138] skip connections, Marcheggiani and Perez-Beltrachini [207] adopt the above-mentioned encoder with an LSTM decoder [201] for graph-to-text generation. Differing from previous iterations of Graph LSTMs [184], Distiawan et. al. [68] compute the hidden states of graph entities with consideration of the edges pointing to the entity from the previous entities, allowing their GTR-LSTM framework to handle non-predefined relationships. The ordering of the vertices fed into the LSTM is based on a combination of topological sort and breath-first traversal. Inspired by hybrid traversal techniques [226, 298], Ribeiro et. al. [273] propose a dual graph encoder-first operating on a top-down traversal of the input graph where the predicate $p$ between two nodes is used to transform labelled edges $\left(u_{i}, p, u_{j}\right)$ to two unlabelled edges $\left(u_{i}, p\right)$ and $\left(p, u_{j}\right)$ while the second operates on a bottom-up traversal where directions of edges are reversed $\left(u_{i}, u_{j}\right) \rightarrow\left(u_{j}, u_{i}\right)$.</p>
<p>$$
h_{v}^{\prime}=\operatorname{ReLU}\left(\sum_{u \in N(x)} g_{u, v}\left(W_{d i r(u, v)} h_{u}+b_{l a b(u, v)}\right)\right)
$$</p>
<p>Damonte and Cohen [61] note that GCNs can assist LSTMs in capturing re-entrant structures and long term dependencies. As such, to bridge the gap between the GCN [19, 285] and the linearized LSTM encoders in graph-to-text translation, Zhao et. al. [364] propose DualEnc which uses both to capture their complementary effects. The first GCN models the graph, retaining its structural integrity, while the second GCN serializes and re-orders the graph nodes resembling a planning stage and feeds it to the LSTM decoder.</p>
<p>GATs \&amp; Graph Transformers: To address the shortcomings of RNN-based sequential computing, Koncel-Kedziorski et. al. [168] extend the transformer architecture [326] to graph-structured inputs with the GraphWriter. The distinction of GraphWriter from graph attention networks (GAT) [327] is</p>
<p>made through the contextualization of each node representation $v_{i}$ (7) with respect to its neighbours $u_{j} \in N\left(v_{i}\right)$ through attention mechanism $a_{n}$ for the $\mathcal{N}$ attention heads. In contrast, Ribeiro et. al. [275] focus on capturing complementary graph contexts through distinct global $h_{n}^{\text {global }}$ and local $h_{n}^{\text {local }}$ message passing using GATs. Their approach to graph modeling also differs in its token-level approach for node representations with positional embeddings injected to preserve sequential order of the tokens.</p>
<p>$$
\hat{v}<em i="i">{i}=v</em>+|<em N_left_v__i="N\left(v_{i">{n=1}^{\mathcal{N}} \sum</em>\right)
$$}\right)} \alpha_{i j}^{n} W_{n}^{n} u_{j} \quad \&amp; \quad \alpha_{i j}^{n}=a^{n}\left(v_{i}, u_{j</p>
<p>Song et. al. [306] enrich the training signal to the relation-aware transformer model [367] through additional multi-view autoencoding losses [264]. This detachable multi-view framework deconstructs the input graph into triple sets for the first view, reconstructed with a deep biaffine model [73], and linearizes the graph through a depth-first traversal for the second view. In contrast, Ke et. al. [158] obtain the entity and relation embeddings through contextual semantic representations with their structure-aware semantic aggregation module added to each transformer layer the module consists of a mean pooling layer for entity and relation representations, a structureaware self-attention layer [296], and finally a residual layer that fuses the semantic and structural representations of entities.
5.1.6 Reconstruction \&amp; Hierarchical Decoders. Input Reconstruction: Conceptualized from autoencoders [34, 304, 330], reconstruction-based models quantify the faithfulness of an encoded representation by correlating the decoded representation to the original input. As such, Wiseman et. al. [347] adopt decoder reconstruction [321] to the D2T paradigm by segmenting the decoder hidden states $h_{t}$ into $\frac{T}{B}$ continuous blocks $b_{i}$ of size at most $B$. The prediction of record $r$ from such a block $b_{i}, p(r . e, r . m \mid b_{i})$, is modeled as $\operatorname{softmax}\left(f\left(b_{i}\right)\right)$ where $f$ is a convolutional layer followed by a multi-layer perceptron (MLP). To replicate the actions of an autoencoder, Chisholm et. al. [52] train a seq2seq based reverse re-encoding text-to-data model along with a forward seq2seq data-to-text model. Similarly, Roberti et. al. [277] propose a character-level GRU implementation where the recurrent module is passed as a parameter to either the encoder or the decoder depending on the forward $\hat{y}=f(x)$ or reverse $\hat{x}=g(y)$ direction. Following the mechanics of back-translation (text-to-data) [290, 321], Bai et. al. [12] extend the standard transformer decoder [326] to reconstruct the input graph by jointly predicting the node and edge labels while predicting the next token. The standard training objective of minimizing the negative log-likelihood of the conditional word probabilities $l_{\text {std }}$ is appended by a node prediction loss $l_{\text {node }}$ that minimizes the word-to-node attention distance and a edge prediction loss $l_{\text {edge }}$ that minimizes the negative log-likelihood over the projected edges. For table-structure reconstruction, Gong et. al. [109] define the reconstruction loss based on attribute prediction and content matching similar to the optimal transport distance [339]. It should be noted that these auxiliary tasks improve the model performance in few-shot settings.</p>
<p>Hierarchical Decoding: Similar to hierarchical encoding (see 5.1.2), hierarchical decoding intends to designate granular roles to each decoder in the hierarchy. Serban et. al. [291] show that injecting variations at the conditional output distribution does not capture high-level variations. As such, to model both high and low level variations, Shao et. al. [293] propose their planning-based hierarchical variational model (PHVM) based on the conditional variational auto-encoder [305]. PHVM follows a hierarchical multi-step encoder-decoder setup where a plan decoder first generates a subset $g$ of the input $\left{d_{i}, \ldots, d n\right} \in x$. Then, in the hierarchical generation process, a sentence decoder and a word decoder generate the narrative conditioned on plan $g$. To dissipate the decoder responsibilities</p>
<p>in the seq2seq paradigm, Su et. al. [310] propose a 4-layer hierarchical decoder where each layer is responsible for learning different parts of the output speech. The training instances are appended with part-of-speech (POS) tags such that each layer in the decoder hierarchy is responsible for decoding words associated with a specific set of POS patterns.</p>
<p>Hierarchical Attention-based Decoding: To alleviate omissions in narrative generation, Liu et. al. [192] propose forced attention - with word-level coverage $\theta_{t}^{i}$ and attribute-level coverage $\gamma_{t}^{e}$, a new context vector $\hat{c}<em t="t">{t}=\pi c</em>}+(1-\pi) v_{t}$ is defined with a learnable vector $\pi$ and a compensation vector $v_{t}=f\left(\theta_{t}^{i}, \gamma_{t}^{e}\right)$ for low-coverage attributes $e$. To enforce this at a global scale, similar to Xu et. al. [352], a loss function $\mathbb{L<em t="t">{F A}$ based on $\gamma</em>$ is appended to the seq2seq loss function.
5.1.7 Regularization Techniques. Similar to regularization in the greater deep learning landscape [110], regularization practices in D2T append additional constraints to the loss function to enhance generation fidelity. As such, Mei et. al. [210] introduce a coarse-to-fine aligner to the seq-to-seq framework that uses a pre-selector and refiner to modulate the standard aligner [11]. The preselector assigns each record a probability $p_{i}$ of being selected based on which the refiner re-weighs the standard aligner's likelihood $w_{t i}$ to $\alpha_{t i}$. The weighted average $z_{t}=\sum_{i} \alpha_{t i} m i$ is used as a soft approximation to maintain the architecture differentiability. Further, the authors regularize the model with a summation of the learned priors $\sum_{i=1}^{N} p_{i}$ as an approximation of the number of selected records. Similarly, Perez-Beltrachini and Lapata [244] precompute binary alignment labels for each token in the output sequence indicating its alignment with some attribute in the input record. The prediction of this binary variable is used as an auxiliary training objective for the D2T model. For tabular datasets, Liu et. al. [191] propose a two-level hierarchical encoder that breaks the learning of semantic tabular representation into three auxiliary tasks incorporated into the loss function of the model. The auxiliary sequence labeling task $L_{S L}$, learnt in unison with seq2seq learning, predicts the attribute name for each table cell. Similarly, the auto-encoder supervision $L_{A E}$ penalizes the distance between the table $z_{t}$ and the narrative $z_{b}$ representations, while the multi-label supervision task $L_{M L}$ predicts all the attributes in the given table. The individual losses, along with the language modeling loss, defines the loss function of the framework. To mitigate information hallucination and avoid the high variance exhibited by the use of policy gradients in the reinforcement-learning paradigm, Wang et. al. [339] compute two losses in addition to the language modeling loss - the first checks the disagreement between the source table and the corresponding narrative through the L2 loss between their embeddings, similar to Yang et. al. [354], while the second uses optimal-transport [43] based maximum flow between the narrative and input distributions $\mu$ and $v$. Tian et. al. [318] propose the use of confidence priors to mitigate hallucination tendencies in table-to-text generation through learned confidence scores. At each decoding step $y_{t}$, instead of concatenating all the previous attention weights, only the antecedent attention weight $a_{t-1}$ is fed back to the RNN, such that an attention score $A_{t}$ can be used to compute how much $a_{t}$ affects the context vector $c_{t}$ - as all the source information in $c_{t}$ comes from $a_{t}$. The confidence score $C_{t}\left(y_{t}\right)$ is then used to sample target sub-sequences faithful to the source using a variational Bayes scheme [167]. Similarly, inspired by Liu et. al. [191], Li et. al. [180] propose two auxillary supervision tasks incorporated into the training loss - number ranking and importance ranking, both crucial to sport summaries, modeled with pointer networks on the outputs of the row and column encoders respectively.
5.1.8 Reinforcement Learning. In the D2T premise, language-conditional reinforcement learning (RL) [200] often aids in model optimization through its role as auxiliary loss functions. While traditionally, the BLEU (see $\S 6.1$ ) and TF-IDF [260] scores of generated texts were used as the basis for reinforcement learning [192], Perez-Beltrachini and Lapata [244] use alignment scores}^{e</p>
<p>of the generated text with the target text. Similarly, Gong et. al. [107] use four entity-centric metrics that center around entity importance and mention. Rebuffel et. al. [262] propose a model agnostic RL framework, PARENTing which uses a combination of language model loss and RL loss computed based on PARENT F-score [65] to alleviate hallucinations and omissions in table-to-text generation. To avoid model overfitting on weaker training samples and to ensure the rewards reflect improvement made over pretraining, self-critical training protocol [272] is applied using the REINFORCE algorithm [345]. The improvement in PARENT score over a randomly sampled candidate $y_{c}$ and a baseline sequence generated using greedy decoding $y_{b}$ is used as the reward policy. In contrast, Zhao et. al. [365] use generative adversarial networks (GANs) [111] where the generator is modeled as a policy with the current state being the generated tokens and the action defined as the next token to select. The reward for the policy is a combination of two values - the discriminator probability of the sentence being real and the correspondence between generated narrative and the input table based on the BLEU score. As RL frameworks based on singular metrics makes it difficult to simultaneously tackle the multiple facets of generation, Ghosh et. al. [105] linearly combine metrics for recall, repetition, and reconstruction, along with the BLEU score, to form a composite reward function. The policy is adapted from Wang et. al. [338] and trained using Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) [368].
5.1.9 Fine-tuning Pretrained Language Models. Pretrained lanuguage models (PLMs) [64, 257] have been successful in numerous text generation tasks [288, 363]. The extensive pretraining grants these models certain worldly knowledge [247] such that, at times, the models refuse to generate nonfactual narratives even when fed deliberately corrupted inputs [274]. As such, Mager et. al. [203] propose an alternate approach to fine-tuning GPT-2 for AMR-to-text generation where the fine-tuning is done on the joint distribution of the AMR $x_{j}$ and the text $y_{i}$ as $\prod_{i}^{N} p\left(y_{i} \mid y_{&lt;i}, x_{1: M}\right) \cdot \prod_{j}^{M} p\left(x_{j} \mid x_{&lt;j}\right)$. On the other hand, inspired by task-adaptive pretraining strategies for text classification [122], Ribeiro et. al. [274] introduce supervised and unsupervised task-adaptive pretraining stages as intermediaries between the original pretraining and the fine-tuning for graph-to-text translation. Interestingly, the authors note good performance of the task-adapted PLMs even when trained on shuffled graph representations. Chen et. al. [51] note the few-shot learning capabilities of GPT-2 for table-to-text generation when appended with a soft switching policy for copying tokens [287]. Similarly, as a light-weight alternative to fine-tuning the entire model, Li and Liang [182] take inspiration from prompting [37], and propose prefix-tuning which freezes the model parameters to only optimize the prefix, a task-specific vector prepended to the input. The authors note significant improvements in low-data settings when the prefix is initialized with embeddings from task specific words such as table-to-text. For avenues that allow leveraging the wordly knowledge of PLMS even without fine-tuning, Xiang et. al. [351] leverage a combination of prompting GPT-3 for disambiguation and T5 for sentence-fusion leading to a domain-agnostic framework for data-to-text generation.</p>
<p>Inspired from practices in unlikelihood learning [323, 341], Nan et. al. [223] model T5 as both a generator and a faithfulness discriminator with two additional learning objectives for unlikelihood and replacement detection. To train the model with said objectives, $n$ contradictory sentences $Y_{\text {False }}^{(i, j)}$ are generated for each entailed sentence $Y_{\text {True }}^{(i)}$ wherein the discrimination probability is computed at every step of token generation. Similarly, to address omissions in D2T (3.2), Jolly et. al. [144] adapt the search-and-learn formulation [178] to a few-shot setting through a two-step finetuning process wherein a T5 model fine-tuned on the D2T task is further fine-tuned again with omitted attributes $r . e / r . m$ reinserted to the narratives as pseudo-grouthtruths.
5.1.10 Supplemental Frameworks. Supplementary Modules: Fu et. al. [98] propose the adaptation of the seq2seq framework for their partially-algined dataset WITA using a supportiveness adaptor</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Though the base transformer architecture is oblivious to input structures, we assume positionally encoded transformers to fall into the seq2seq paradigm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
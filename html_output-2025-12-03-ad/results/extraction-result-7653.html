<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7653 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7653</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7653</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272987355</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.20204v2.pdf" target="_blank">Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach</a></p>
                <p><strong>Paper Abstract:</strong> Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field. In this review, we make four key contributions: (1) we synthesize the literature into five core themes—definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions; (2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across social psychology, computer science, and gender studies; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain. Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured. Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration. Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies. Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7653",
    "paper_id": "paper-272987355",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0091815,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach
16 May 2025</p>
<p>Aditi Dutta 0000-0002-1531-5510
University of Exeter
ExeterUnited Kingdom</p>
<p>Alan Turing Institute
LondonUK</p>
<p>Susan Banducci s.a.banducci@exeter.ac.uk 
University of Exeter
ExeterUnited Kingdom</p>
<p>Chico Q Camargo f.camargo@exeter.ac.uk 
University of Exeter
ExeterUnited Kingdom</p>
<p>Oxford Internet Institute
University of Oxford
OxfordUK</p>
<p>Ewha Womans University
SeoulSouth Korea</p>
<p>Alan Turing Institute
LondonUK</p>
<p>Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach
16 May 2025B35D9BB59D70691A31A0A9AD5D6ED9CAarXiv:2409.20204v2[cs.CL]
Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms.These tools draw on insights from both social science and computer science.Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field.In this review, we make four key contributions:(1) we synthesize the literature into five core themes-definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions;(2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across social psychology, computer science, and gender studies; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain.Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured.Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration.Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies.Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.Keywords-systematic literature review, online sexism and misogyny, semi-automated publication analysis, applied natural language processing, scientometrics * indicates unique features of a document, i.e., a document can only have either of the categories in the field.‡ indicates that the categories listed below are a part of the exhaustive list for that particular field.</p>
<p>Introduction</p>
<p>"Millions of women and girls are affected by digital abuse and technology facilitated violence every year.Studies suggest that between 16 and 58 per cent of women have experienced this type of violence."(Secretary-General ( 2024))</p>
<p>The rapid growth of online spaces has been accompanied by increased online abuse targeting marginalized groups (Wilson and Land, 2020;FRA, 2023;Vidgen and Derczynski, 2020).Girls and women, in particular, have experienced hostility and harassment in online spaces and platforms (Jurasz and Barker, 2019;Nadim and Fladmoe, 2021;Vitak et al., 2017).Research shows that women are twice as likely as men to experience gender-based online harassment (Duggan, 2017), often resulting in self-censorship and withdrawal from digital spaces (Mantilla, 2013;FRA, 2023;International, 2017).This growing concern on the disproportionate impact of online hate speech towards girls and women has given rise to an active interest among the research community in countering online sexism and misogyny (Megarry, 2014;Guest et al., 2021;Citron, 2014), and an increase in research on quantifying the same using machine learning approaches (Vidgen and Derczynski, 2020).</p>
<p>However, these computational approaches face several limitations.They are difficult to implement effectively (Hewitt et al., 2016;Nozza et al., 2019;Samory et al., 2021), as they differ fundamentally in how sexism and misogyny are defined, measured, and operationalized-reflecting the complexity and multiplicity of the underlying concepts (Richardson-Self, 2018;Matsuda, 2018).While these approaches show impressive performance, they fail to identify and capture all forms of sexism or misogyny -especially overlooking the subtler forms of sexist discourse (Rodríguez-Sánchez et al., 2020;Rodríguez-Sánchez et al., 2021), and are often prone to erroneous classifications.This calls for the need to examine the current state of research in online sexism or misogyny, and identifying the current challenges arising due to disciplinary and methodological divide.Compounding this issue is the disciplinary divide: while social science research often adopts qualitative methods to explore sexism in rich, contextualized ways (Yasseri et al., 2016), computational work tends to rely on narrow, binary definitions, with limited integration of social science theories to analyze the immense amount of available online data on sexism and misogyny.</p>
<p>Despite growing work, there is no integrative, methodologically rigorous synthesis bridging theoretical frameworks from social science with practical automated computational approaches in computer science.This gap is particularly critical given that language is a form of social behavior that reflects identity and power structures (Dinan et al., 2020).Text analysis has been proven to be one of the established methods in mapping and analyzing hostility in online discourses, particularly for online gendered hate-speech (Jane, 2016).Thus, the need arises to apply a natural language processing (NLP) approach to analyze such data to advance both sociological understanding of the kind of sexism existing in online spaces, and methodological understanding of using and improving computational models to capture the same through detection and identification tasks.</p>
<p>Nonetheless, most of the earlier works have neglected or retrofitted the link between the data and sexism as a theoretical construct (Samory et al., 2021;Abburi et al., 2021;Parikh et al., 2019).Primarily, sexism and misogyny has been researched as a part of the hate-speech diaspora, disregarding the forms of sexism 'not involving hate' (Parikh et al., 2021), or other non-hostile forms that are subtle and often deceptive (Jha and Mamidi, 2017;Rodríguez-Sánchez et al., 2020;Rodríguez-Sánchez et al., 2021;Rodríguez-Sánchez et al., 2022).While online communities now emphasize on the detection of sexism [or misogyny] (and other hate speech) more than ever before, automatic detection of these phenomena remains challenging, as most research focuses on using textual features to solve the issue (Das et al., 2023).</p>
<p>Recognizing this need, Fontanella et al. (2024) conducted a systematic literature review on the study of misogyny using computational methods, where they find a "limited connection between the areas of knowledge that are necessary to fully grasp this complex phenomenon".Through our research, we extend the review on articles beyond misogyny with an extensive discussion on the identified practices, along with their challenges and limitations in implementation, backed by social science literature.The review aims to consolidate, categorize, and critique existing approaches to quantifying online sexism and misogyny, identifying key research gaps and challenges through a theoretically informed, methodologically rigorous framework.</p>
<p>To address these gaps systematically, we developed a hybrid review pipeline that combines advanced NLP techniques with traditional PRISMA-based rigor-allowing for scalable yet methodologically transparent analysis of the literature.First, we applied transformer-based topic modeling (BERTopic) alongside UMAP dimensionality reduction and HDBSCAN clustering to group abstracts into coherent thematic clusters.Next, we validated and refined these clusters via a KeyBERT-driven keyword co-occurrence network, ensuring that selected topics truly reflected our research questions.Finally, we integrated these automated steps with manual title/abstract and full-text screening to enforce quality and consistency.This hybrid workflow not only accelerates large-scale literature reviews but also maintains the transparency and reproducibility demanded by systematic review standards.</p>
<p>In this paper, we use quantification to refer broadly to the identification, classification, or detection of sexism and misogyny in online texts.Our goal is to map how these constructs are operationalized across disciplines, assess the limitations of existing practices, and pave the way for future research that better integrate theory, method, and application.</p>
<p>To address these gaps, this paper makes four key contributions.First, it synthesizes the fragmented literature across social science and computer science into five central themes: definitions of sexism and misogyny, disciplinary divergences, computational detection methods, current challenges, and design-oriented interventions-addressing the lack of conceptual and methodological cohesion.Second, it bridges disciplinary silos by integrating insights from gender studies, social psychology, and NLP, offering an interdisciplinary perspective on detection models.Third, it highlights critical underexplored areas, such as the absence of intersectional approaches, the dominance of Western contexts and languages, and the limited focus on proactive system design beyond keyword matching.Finally, it provides a methodological contribution through a semi-automated systematic review pipeline, combining topic modeling, network analysis and PRISMA guidelines to establish a transparent and scalable standard for future research in this space.</p>
<p>Background</p>
<p>Defining Sexism and Misogyny: Challenges and Contexts</p>
<p>Understanding and detecting sexism and misogyny -particularly through automated meansrequires careful attention to how these terms are defined.However, definitions of both concepts often prove too narrow to capture their full complexity, especially in computational contexts where nuanced social phenomena must be rendered in operational terms.Despite the central role these concepts play in understanding gendered power relations, there is no cross-disciplinary consensus on their precise definitions.Sexism and misogyny are central concepts in understanding the status of women yet there is no consensus across disciplines on their definition.Wrisley (2023) highlights the difficulty of establishing operational definitions for sexism and misogyny, noting that both terms have evolved well beyond their original conceptual boundaries.This ambiguity is compounded in computational fields, where definitional clarity is often sacrificed for operational convenience.Consequently, many studies tailor their definitions to align with their specific research aims, particularly in the detection of hate speech that manifests as sexism, misogyny, or both.</p>
<p>Manne Manne (2017) provides a foundational framework for distinguishing the two: sexism functions to "justify [patriarchal] norms, largely via an ideology of supposedly 'natural' differences between men and women concerning their strengths interests, proclivities, and appetites," whereas misogyny serves to "uphold the social norms of patriarchies by policing and patrolling them" indicating it is a systemic property embedded within social structures.While Richardson-Self (2018) distinguishes between sexist and misogynistic speech, arguing that while sexist speech can oppress without overt violence, misogynistic speech often exhibits key features of hate speech.Building on this systemic view, Srivastava et al. (2017) defines misogyny as a form of hatred or contempt for women, arising directly from patriarchal systems.Traditionally rooted in face-to-face social interactions, misogyny has also functioned historically as a political mechanism to domesticate women, control their sexuality, and undermine collective feminist solidarities (Anderson, 2014).Some scholars treat sexism and misogyny as synonymous or closely related (Rahali et al., 2021;Bhattacharya et al., 2020a;Abburi et al., 2021), while others conceptualize misogyny as a subset or intensified form of sexism (Butt et al., 2021;Rodríguez-Sánchez et al., 2020).Even when the distinction is acknowledged, the terms are frequently used interchangeably in computational studies due to their frequent co-occurrence (Frenda et al., 2018).Much of the computational literature adopts this interchangeable usage, informed by theoretical positions that treat misogyny as an extreme articulation of sexist ideology (Chiril et al., 2020;Zeinert et al., 2021;Kohli et al., 2021).</p>
<p>Given these definitional challenges, this study aims to synthesize the broad spectrum of existing definitions and examine how sexism and misogyny are conceptualized across both social science and computer research domains.</p>
<p>From Offline Harm to Online Hate: The Rise of Digital Sexism and Misogyny Back in 2013, the World Health Organization (WHO, 2013) identified violence against women as "a global health problem of epidemic proportions", primarily referring to offline violence, while also warning of its likely expansion into social media.Indeed, the Internet-particularly social mediahas since become a key space for the perpetration of sexism and misogyny, where women are subjected to various forms of violence (Jurasz and Barker, 2019).Prior research has also highlighted the role of specific linguistic forms and categories-such as the generic masculine1 -in reinforcing prejudices, sexist attitudes, and gender stereotypes (Sensales and Areni, 2017).These manifestations may take different forms but share a common aim: to discredit women's participation in public life and silence their political voices (Jurasz and Barker, 2019).In recent years, systemic gender inequality has increasingly manifested in cyberspace through the proliferation of abusive content that is even more aggressive, prompting further research into this evolving form of online misogyny (Fontanella et al., 2024).As a result, online platforms have contributed to the erosion of boundaries between online and offline experiences (Megarry, 2014).</p>
<p>Gendered Harassment and the Silencing of Women Online</p>
<p>Even self-identifying as a woman online can significantly increase the risk of internet harassment.When gender identity is known, gender stereotyping and discrimination from the "real world" often carry over into digital spaces, contributing to a "gender asymmetry" in the dynamics of online abuse (Herring, 1999).Even seemingly neutral actions-such as the perceived tone of a post-can be enough to "trigger" misogynistic mockery.Speaking out against such behavior often invites further backlash, with responses that are both sexist and misogynistic in nature, and notably, these can come from both men and women.Those who deliberately derail online feminist spaces often do so to suppress the free speech of those communities (Bartow, 2009).Megarry (2014) contextualizes online abuse within discursive practices, arguing that such hostility seeks to silence women's voices on digital platforms and regulate their public behavior.</p>
<p>The overwhelming volume of gendered abuse online raises serious social concerns.While some victims have been celebrated for exposing abusers through acts of 'feminist digilantism' such responses risk reinforcing the notion that these issues should be addressed privately by individuals rather than collectively through systemic or public intervention (Jane, 2016).Crucially, the impact of misogyny extends beyond psychological harm; it also has material consequences, particularly in how resources and opportunities are distributed in society.Thus, understanding misogyny and gender-based violence in online contexts requires a deeper exploration of their complex entanglement with digital culture and technology-an understanding that is essential to shaping equitable digital gender politics for the future (Ging andSiapera, 2018, 2019).</p>
<p>Given its impact, online misogyny and sexism can be seen as "seeking to prevent women from participating in building the forthcoming technological future" (Ging and Siapera, 2018).It is therefore necessary to stop such proliferation in online spaces to promote gender equality, raise awareness and eliminate it at the earliest by detecting them through computational tools.</p>
<p>Research questions</p>
<p>The challenge of considering sexism and misogyny from a quantitative perspective, when considering their highly subjective nature, motivates our research questions:</p>
<p>RQ1: What are the main topics in the studies identified, and how do they differ by discipline and over time?</p>
<p>RQ2: How has the existing literature operationalised sexism and misogyny?</p>
<p>RQ3: What are the main challenges and opportunities of computational approaches to the study of sexism and misogyny?Which of the challenges do they address?</p>
<p>The main objective of this paper is to provide a comprehensive systematic literature review, drawn from the research landscape of sexism and misogyny, studied over the years of 2012-2022.The aim is not to focus on specifics from any individual paper but to provide a general overview of the existing literature and draw conclusions from their study designs and research outputs.These observations are to inspire researchers on best working practices and approaches, while also contributing to future research objectives.</p>
<p>Our systematic literature review is divided into two stages: (a) Identifying the relevant studies through multiple steps by performing a semi-automated selection flowchart as illustrated in the PRISMA flowchart (Figure 4.1) in Section 4, (b) Conducting an in-depth analysis of the selected study results in Section 5.While stage 1 is expected to answer the first research question, stage 2 will answer the second and third research questions.</p>
<p>Identifying relevant studies</p>
<p>Search strategy</p>
<p>We searched six databases -Google Scholar, ArXiv, Elsevier, Scopus, Semantic Scholar, and Web of Science -using a closely related set of keywords that operationalized our review criteria of 'quantifying' sexism and misogyny.This returned a comfortable number of results that were useful for performing the quantitative analysis.Search results were implemented such that the range of year of publication lay between 2012 and 2022.All of the articles should be in English, containing the full abstracts and titles for each of them.The reporting strategy follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), presented in Figure 4.12 , which uses a checklist approach to systematic literature reviews.This research was conducted to review papers with three main characteristics, namely:</p>
<p>1.The papers study sexism and/or misogyny.</p>
<ol>
<li>
<p>The papers ideally study the propagation in social media platforms or other broadcast (preferably text-based) media.</p>
</li>
<li>
<p>The papers use various methodologies for measuring or quantifying sexism and misogyny (e.g., scales, models, etc.).</p>
</li>
</ol>
<p>ArXiv and Web of Science were chosen to collect studies from the fields of CS and SS, using the search criteria as shown in Table B.1.As the Figure 4.1 illustrates, the systematic screening and selection process used in this review.An initial total of 1,745 records were retrieved from the databases, comprising 1,511 results from Web of Science and 234 from arXiv.An additional 72 records were identified through external sources during the first stage of the review.Following the removal of 104 duplicates, 1,691 records met the inclusion criteria and were retained, while 22 partially eligible records underwent further assessment and were subsequently excluded.Automated exclusion techniques-comprising text pre-processing, topic modeling, and network analysis-were then applied, resulting in a subset of 84 records selected for title and abstract screening.Subsequently, 97 articles (including 13 from citation tracking) were reviewed at the full-text level.After excluding 49 articles due to irrelevance or lack of methodological clarity, a final set of 48 studies was included for both qualitative and quantitative synthesis.This rigorous process incorporated both manual and computational methods to ensure a focused and comprehensive literature base.The data collection method will be discussed in detail in the next section.</p>
<p>Data search and collection</p>
<p>In this section, we elaborate on the experimentation conducted with each of the citation databases, and the advantages and disadvantages encountered during the study.For this research, some fields of the search results, namely -title, abstract, year of publication, and the discipline of research for each of the search results were integral to the study.To perform the automated step of narrowing down our search results, some measures were taken to check the consistency and reliability of the data, which is shown in the Table B.1.</p>
<p>ArXiv is a platform that offers researchers to e-publish a draft version of their final work preceding a formal peer review and publication in a peer-reviewed scholarly or scientific journal, also referred to as 'pre-prints'3 .Due to the popularity of ArXiv among CS researchers, its API was used with the expectation of returning unpublished or pre-published works for all disciplines.However, it was found that only the areas of CS vary widely."In theoretical computer science and machine learning, over 60% of published papers are on ArXiv, while other areas are essentially zero."Sutton and Gong (2017) We opted to use advanced search queries to narrow down the results, as simpler queries were expected to return more irrelevant results, that had to be removed before analysis.Though the API returned only a limited number of papers, most of them were found to be relevant.Hence, we took it for analysis but did not use it as our only source due to its skewed disciplinary variety.</p>
<p>Web of Science and Scopus showed results in retrieving studies from CS.Though Zhang (2014) found that Scopus retrieved "significantly" more studies in CS as compared to the Web of Science, with all of the kinds of document types -conference proceedings, journal articles, reviews, and editorials; yet for our search type, more relevant works were found in Web of Science.As Fiala and Tutoky (2017) mentioned in their work, CS has a greater reliance on conference proceedings as compared to other disciplines.To some extent, these conference proceedings papers are also indexed in Web of Science in the Conference Proceedings Citation Index, which makes it possible to carry out scientometric studies of CS based on the data from Web of Science (Fiala and Tutoky, 2017).</p>
<p>For Google Scholar, we used two external APIs like SerpAPI for scraping the data, as well as a software named 'Publish or Perish' (Harzing, 2007) to collect the search results.Both of the methods were rejected because of their disadvantages.Such as, Publish or Perish could only extract 1000 results at a time for each search query.While this drawback was overcome by searching for documents with a shorter range of years to stay within the limit, it lacked some of the fields that were needed for this study -abstract and discipline.Alternatively, SerpAPI (SerpAPI, 2019) worked similar to a web scrapping tool and could only scrape the results as the search engine demonstrates, i.e., it only scrapes what Google shows on their Google Scholar pages, nothing more.Even though the fields we got through this API were relevant, they did not contain the full information we needed for the analysis.For example, the full text in the title and abstract was missing and was instead indicated with dotted extensions in the beginning and end of the text.For the remaining tested citation databases -Elsevier and Semantic Scholar, the possible search queries were either too simple (consequently giving back a lot of irrelevant studies), did not give back enough studies on our topic, or lacked some of the essential fields (e.g., abstract) that were integral to this study, especially for the automated search strategy used to eliminate non-relevant studies.Therefore, we found empirical evidence indicating that the research outputs we got from ArXiv and Web of Science were ideal for our work.Alongside the search queries, we augmented the dataset with manually added papers that satisfied the selection criteria: A.2.This data from external sources included studies shared in the social platforms Twitter (or X) and LinkedIn, recommendations of other researchers in the field, and following the references of the reviewed papers (i.e., citation tracking).</p>
<p>Final methodology selection criteria</p>
<p>Observing the pros and cons of all the citation databases, it was decided to use the Web of Science API to collect data based on the individual areas of discipline -SS and CS, as the primary data source.Since many of the relevant computational papers were seen to be published in ArXiv within the given period, those papers were also considered as part of the data collection.It was done to ensure that we get full coverage of both published and unpublished works (pre-prints), relevant to the study of sexism and misogyny during the 11 years.As discussed before, we also included the publications that were informed through external sources.While the Web of Science was taken as the main source for published works, ArXiv was taken as a source for unpublished works.We then combine the selected search results for the next section 4.3, before removing the duplicates.</p>
<p>Data extraction and synthesis</p>
<p>In this section, we first provide an overview of the collected data from the previous Section 4.2.1, and then use automated approaches for the data extraction stage.The analyses are performed before the application of the selection criteria A.2.For each of the following subsections, the fields considered were:</p>
<p>• Title of the paper • Abstract (Multiple abstracts of the same paper were replaced with the first abstract)</p>
<p>• Year of publication (or pre-printing)</p>
<p>• Language of the paper Figure 4.2 shows a steep rise in the study and publication of research on sexism and misogyny, in both the fields of CS and SS.While SS studies always dominated research on the topic, CS works also showed admirable improvement, with a lot of the papers getting published in 2022 alone.The study of online sexism and misogyny has grown significantly since 2014, with a notable rise in scholarly attention from 2018 onwards.Figure 4.2 illustrates the yearly distribution of selected publications, revealing a marked upward trajectory that peaks in 2021.</p>
<p>As we had discussed in the Section 4.2, there has been a rising trend of pre-prints in CS G.1, many of which were later published and indexed in citation databases.Studies researching social media platforms like Facebook, Twitter, and Instagram were seen to be limited, with less than 100 works dedicated to research on sexism and misogyny in these online platforms.While almost all of the returned results indicated that works were published majority in English, among the other languages -Spanish and Portuguese followed through, though separated by huge margins.Pre-processing of the text was done to drop duplicates and remove characters in the text that could hinder the automated selection of the studies based on the titles and abstracts.Studies containing no abstracts at this stage were removed as they could not be added for automated selection criteria.Given that the count of such papers was only 13, the abstracts were looked up in Google Scholar and later manually checked, if they satisfied the selection criteria for this research.</p>
<p>For the automated extraction stage, we perform two steps in chronological order: topic modeling and keyword co-occurrence network to narrow down our search.</p>
<p>Topic modeling</p>
<p>Topic modeling4 was used with the pre-processed data containing the abstracts and titles from both disciplines, to generate clusters of topics based on the documents (i.e., the collection of studies containing abstracts and topics).Among all the topic modeling techniques experimented with, BERTopic (Grootendorst, 2022) proved to be the best choice for the task.It is because BERTopic "leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions" (Grootendorst, 2022), hence enhancing the topic recognition ability by the model. 5e applied the BERTopic algorithm to the collections of CS and SS papers separately to capture the topics of research between these two disciplines, and to check the differences in the themes of sexism between them.Among all the experimentation conducted -including setting different ranges of parameters to get the best representative models from there, we further employed fine-tuning of the model to improve on that, by using multiple representations from the model.For our work, we used these different representations from keywords and phrases to summarize and custom labels.The Figures 4. 3 and 4.4 indicate the topics recognized by the model.Using the aforementioned parameters, we used the BERTopic model to groups documents into topic clusters, identified by their keywords and keyphrases.It uses clustering to define topics and hence does not assign more than one topic to each document.In the figures, each point corresponds to each document in their respective disciplines.BERTopic uses HDBSCAN by default for clustering, and it does not force all the data points to be a part of any of the recognized clusters or topics.Simultaneously, BERTopic uses UMAP to perform dimensionality reduction.We then used further customization of the UMAP by setting the parameter 'n components' to 2, to 'pre-reduce' embeddings for visually depicting our model results in the two figures.For those topics that do not form a part of any groups (also termed as "outliers"), the points are marked in grey in the figures.The colored points in both the figures indicate topics, and each color represents a unique topic for the sets of documents, which have further been marked correspondingly with labels of the same color boxes.Larger clusters represent more densely populated or well-defined research areas, indicating a richer or more mature body of literature.The algorithm Through topic modeling, usually each document get assigned a set of key words as themes within the paper, which are then grouped together with an unique color, representing the same topic with similar sets of keywords found across all the documents.When grouped, each topic is described by their topic name in the same color.The grey points represent outliers (documents which did not get any assigned topic).The highlighted topic name indicates more relevance to our research objectives.</p>
<p>itself exhibits strong local clustering to group similar topic categories together, to which we also controlled the balance between the local and the final structure to efficiently distinguish between each topic.</p>
<p>It uses a light hue of the same colors encircling each topic to indicate the cluster belonging to the respective topic.In the figures 4.4 and 4.3, different colors indicate different cluster of topics, as identified by the model.Usually in topic modeling, the models analyze "bags" or groups of words together to capture the meaning of the words.However, in our approach, we used the BERTopic framework using a Large Language model (LLM) -Mistral for better inference and contextual understanding of the abstracts and generation of logical topics (indicated by each color).While some points in the same cluster may look further away than the points from another cluster, it is due to its projection in 2D-dimensional space which we did for better visualization; hence the points within the same clusters are closer in a multi-dimensional space.</p>
<p>The highlighted circles represent the most relevant thematic areas for our study.Figure 4.4 illustrates key domains within social science research, including digital feminism and online misogyny, gender representation in education and media, health-related concerns such as mental health and gender-based violence, and legal-cultural perspectives on sexual violence.The left side of the map emphasizes discourse and identity, while the right side is more focused on public health and psychological issues.The central regions reflect interdisciplinary intersections, suggesting potential for integrating discourse analysis with clinical and educational research to deepen understanding of gendered bias.Figure 4.3 illustrates the computer science research landscape on sexism and misogyny, revealing two dominant and distinct thematic clusters.The first, "Gender Bias in Language and Embeddings", centers on studies that investigate algorithmic bias in NLP models, focusing on how word embeddings and language representations encode and perpetuate gender stereotypes.The second cluster, "Hate Speech Detection in Social Media", includes work on building and evaluating systems for identifying misogynistic and abusive language online.Unlike the diverse and interdisciplinary structure observed in social science literature, the computer science landscape appears more siloed, with limited overlap between technical fairness research and applied detection systems.This highlights a relatively narrow scope and methodological uniformity in the field's engagement with gendered bias.We observe that social science studies generally lack research focused on the automated detection or identification of online sexism and misogyny -the area that this study aims to address.Therefore, we chose to further investigate the articles in Computer Science.Online sexism and misogyny are subsets of online hate speech.Therefore, in this study, we focus on the topic within computer science that specifically addresses the quantification of such content, rather than broader analyses of gender bias in its various forms (which was one of the two topics identified through topic modeling).To quantify hate speech, it is essential to review research that emphasizes detection tasks.Therefore, at this stage, we selected the topic 'Hate Speech Detection in Social Media' from among the various categories and disciplines considered.In the next step, we further demonstrate the relevance of this chosen topic by conducting a keyword co-occurrence network analysis.</p>
<p>Keyword co-occurrence network</p>
<p>To validate if the topics captured from the automated selection of topics from each discipline in the previous section were representative of the corresponding documents, we navigated the disciplines and each topic, alongside their respective keywords.To obtain the most frequent keywords in the set of documents, we use KeyBERT6 (Grootendorst, 2020) to extract embeddings with a BERT model to get a document-level representation from our abstracts and titles.From each document, we used KeyBERT to identify key phrases that would provide with a more accurate summary of the documents, rather than simple keywords.KeyBERT works by creating an embedding of document texts, from which BERT key phrase embeddings of a pre-defined word n-gram range length of 1-2 words7 were created.Consequently, cosine similarities between the document and their respective keyphrase embeddings are calculated to extract the top 10 keyphrases that best describe that document.These selected keyphrases per document are then compared against the whole set of documents.We chose to look into the 100 most common keywords in the documents taken at both discipline level (CS and SS), as well as the topic level (each topic based on the topics we generated in Section 4.3.1).This was done to check the relevance of the keywords, and consequently the set of documents that would best represent our research objective of performing a literature review on the quantification of online sexism and misogyny.This divergence suggests that computational approaches risk oversimplifying the complexity of sexism by concentrating on narrowly defined categories, while social-theoretic work may lack the granular tools needed for large-scale detection.Bridging these perspectives would offer a path forward: embedding rich, contextualized constructs from social science themes of power, identity, and structural inequality into algorithmic models to enhance their sensitivity to subtle, intersectional manifestations of sexism and misogyny.Conversely, employing automated keyword and topic-modeling techniques to capture emergent patterns (e.g., discourse around political discussions) can help social scientists process vast corpora and refine theoretical frameworks.Ultimately, a synergistic integration-where computational precision is being guided by theoretic depth from social science will yield more robust, fair, and actionable tools for understanding and combating online sexism.We will discuss more on this in Section 5.3.</p>
<p>On further analysis, we selected the most relevant topic among all of the highlighted topics from both disciplines and performed a keyword search on each of them.Figure 4.5 corresponds to the topic 'Hate Speech Detection in Social Media', and it showed the most promising result of containing the necessary keywords needed for this study.</p>
<p>The figure consists of the 100 most frequent keyphrases in the topic.The size of each circle (node) indicates the weight of that particular keyphrase in the set of documents.The color of the nodes can be anything that is recognized by Matplotlib in colormap specified and could be randomly generated.The edge width of the edges joining the nodes indicates the number of associations between the two connected nodes -the more the number of connections, the thicker the edge width.The cooccurrence network for all the highlighted topics from the previous step showed us the most used keywords pertaining to the said topics.By studying the generated keywords, we could capture the general objectives of the respective papers under the same topic, as stated from their abstracts.We then selected the most relevant topic for our work, which focuses on computational approaches in detecting online sexism and misogyny (a part of the online hate speech discourse).Consequently, we decided to perform the full-text literature review on the articles which fell under the topic of 'Hate Speech Detection in Social Media'.</p>
<p>On completing this step, we included a few more of the studies that had been identified through citation tracking and did a full-text screening of all the collected texts.</p>
<p>Note: As we see from the plots, till here, all of the analyses largely relied on the information provided in the title or the abstract of the papers.Hence, it limits us in providing a concise assessment of the exact models, methodologies, or datasets used by the corresponding papers.For which we would need a full-text assessment.</p>
<p>Final data selection</p>
<p>Screening</p>
<p>Following the selection of the citation database, the automatic filtering of papers using BERTopic, and the validation via topic keywords, we identified the necessary characteristics 4.1 of the data to emphasize the findings which eventually led us to select the research articles based on the selection criteria: A.2, by performing a manual screening.Eligible articles were divided into two categories at this stage.The first category is the data acquired from the automated stage, while the second category is the records identified from citation tracking.</p>
<p>Finally, we thoroughly reviewed all publications related to the quantification of sexism and misogyny in online social platforms to determine their focus and methodologies, by reading their full text.</p>
<p>Qualitative assessment of the selected studies</p>
<p>A total of 96 full-text articles were analyzed qualitatively, as shown in Figure 4.1.We assessed them based on four criteria, namely:</p>
<p>(i) Irrelevant study focus -Whether they are focused on studying the propagation of sexism and misogyny (irrespective of whether they indicated such in the abstract).Many of the works focused on hate speech, but because we wanted to only review sexism and misogyny, they were eliminated.</p>
<p>(ii) Irrelevant study designs -Whether the intended outcome of the research was not about performing a computation analysis on the detection or identification of online sexism and misogyny.(iii) Studies not quantifying sexism/misogyny -Whether the paper focused on a review of studies in the relevant topic; or contained a summary of the author's thoughts from multiple papers, such as opinion pieces around the same topic.We only focused on methodology papers which described their approach.Hence, summary of methods, shared task descriptions or dataset papers 8 are generally excluded from the list.(iv) Sources could not be traced back -in case the paper's paraphrased contents with citations were not reflective of the summary the original authors indicated in their study.</p>
<p>45 out of the 96 research articles qualified from this step, from surpassing these exclusion criteria and as a result, were included in the final qualitative analysis.</p>
<p>The full text of all the papers was reviewed qualitatively and information about each was added to a summary table covering the following points:</p>
<p>• Forms of hate speech studied.It is because hate speech could encompass a lot of things, including sexism and misogyny.</p>
<p>• Definitions of sexism or misogyny (or both) used in the study.</p>
<p>• Language(s) of the data used for the study.</p>
<p>• Data selection criteria.This could depend on the original data collection method, such asusing keywords, hashtags, public profiles by monitoring user's online activity, users identified as sexist/misogynist, tags of sexism, specific phrases; or even based on a particular timeline of interest.</p>
<p>• Datasets used and their types (external, API-generated, etc.)</p>
<p>• Dataset modifications, if done.This could be in the form of data augmentation, counterfactual examples, document expansion by adding semantically similar words, transliterating multilingual dataset to uniform language, and many more.</p>
<p>• Broadcast media or social media platform which is of interest for the study.</p>
<p>• Annotators used in the study, and their tasks.If each or group of annotators had different tasks, that was also recorded.</p>
<p>• Pertaining to the previous point, the Kappa values that are statistical measures used to measure inter-rater reliability, are also noted.</p>
<p>• Research bias addressed or acknowledged in the study.If acknowledged, it is posted as a limitation in the paper.</p>
<p>• Pre-processing or post-processing done on the data.</p>
<p>• Performance metric used.</p>
<p>• Embedding type used, since this could range from word-level to node-based.</p>
<p>• Classification or clustering type, and the respective models.</p>
<p>• Syntactic, linguistic, and semantic/lexical features.</p>
<p>• Prompt topics and intersectionality (if present).</p>
<p>Recently several SLR tools have incorporated semi-automation using Artificial Intelligence techniques, for supporting the screening and extraction (pre-screening) phases (Bolanos et al., 2024), like we did in our research.Of such tools, only a few use topic modeling for their work.Such as, RobotAnalyst 9 and SWIFT-Review 10 uses Latent Dirichlet Allocation (LDA) that assigns a topic to a paper based on the most recurrent terms shared by other papers using a generative probabilistic model, while Iris.ai 11clusters the papers according to a two-level taxonomy of global topics and specific topics (Bolanos et al., 2024).The former two tools depend on the term frequencies while the later perform Named Entity Recognition (NER) and allow users to to customize entity extraction by letting them define their own set of categories beforehand.Even with its advantage of the superior language capabilities to produce one of the most advanced techniques in language topic modeling today (Briggs, 2023), BERTopic has remained unexplored for the same task.In our work, we use that potential alongside the promising result of Large Language Models (LLMs) in their information extraction capabilities, to cluster our topics before validating the results with network analysis and selecting the topic(s) more suited for our work.This proved to be particularly useful to us in the screening 8 Our dataset analysis was based on the same set of papers selected during the screening process. 9https://www.nactem.ac.uk/robotanalyst/ 10 https://www.sciome.comswift-review/ 11https://iris.ai/and qualitative assessment phase as empirical analysis of the topics generated to their corresponding papers showed that the approach accurately clustered similar papers together.</p>
<p>5 Results of the Systematic Literature Review</p>
<p>Data statistics</p>
<p>Post data screening and qualitative assessment, we finally narrow down the number of manuscripts to 45, that satisfied the scope of our meta-analysis.In the first subsection, we provide a brief overview of the key statistics of the 45 papers.In the following subsections, we provide an overview of the existing computational approaches dedicated to quantifying sexism and misogyny.Beyond that, we discuss the challenges and limitations faced by the said approaches from the existing literature.</p>
<p>Author collaboration network</p>
<p>We provide an author collaboration network in Figure 5.1, where the name of the researchers are nodes, their size and color indicating the number of relevant manuscripts they authored or coauthored.The connections between the authors are indicative of co-authorship on manuscripts, and their weighted edges imply the frequency of co-authorship.</p>
<p>Characteristics of study or research designs</p>
<p>Table 5.1 gives a summary of all the general characteristics we found in the full-text reviews of the selected studies.It provides a summary of the most used categories of each field (design/methodology) that the documents in our literature review have used.The other categories which were not featured in the table were mostly used by only one document.Each document could have multiple categories under the same field of design or methodology.For example, one document could be researching datasets of multiple languages in different platforms of interest and using multiple models, with different levels of classification at different stages.The categories that are uniquely present in a document are marked with an asterisk( * ) beside it, while the fields with their entire list of categories in the table are marked with an obelisk( † ).In here, the asterisk( * ) symbol would not just indicate that the feature itself is unique to the field, but also all the documents should add up to the total number of literature listed.Table 5.1: Summary table of some of the topmost categories of designs/methodologies in all the observed characteristics across the selected studies.</p>
<p>Characteristics of</p>
<p>Overview of the general methodologies</p>
<p>As per the Table 5.1, we do see the frequency of each source of online data and the machine learning models used in the 45 manuscripts we reviewed.The type of classification of sexism and misogyny used in the said studies are otherwise unknown and how they link between the sources of online data and the computational methodologies is an important source of information to indicate the multilevel connections between the variables, and consequently its impact on the quantification of sexism and misogyny.Figure 5.2 show the connection between nodes in each level of information (source of online data, classification type and model used), while the connections are the links between each level with their weights indicating the frequency of connections between each node type at different levels.This is a many-to-many mapping between the three levels, and show the flow of information between each of them.The colors of each node at different levels represent the unique relation between the linked nodes at each level (e.g., Twitter
[level 1] =&gt; Misogyny (5 categories) [level 2] is different from Twitter [level 1] =&gt; Misogyny (binary) [level 2], and Reddit [level 1] =&gt; Misogyny (binary) [level 2] is different from Reddit [level 1] =&gt; Abuse/Aggression [level 2]
).The abbreviations for the models in level 3 are found in Supplementary Section H. Overall, the Figure 5.2 gives a clear evidence that Twitter was the most explored online data source to investigate different forms of sexism and misogyny.</p>
<p>Overview of the existing computational approaches</p>
<p>General definitions and strategies used</p>
<p>Most computational works on sexism and misogyny took the automated identification problem as a binary classification task, i.e., deciding if the text in question is sexist/misogynistic or not.For defining the terms, researchers used different (non-standardized) forms for their work because of computational benefits, such as model performances.Though they served to be effective in some instances, they also presented with limitations of their own.For instance, Grosz and Conde-Cespedes (2020) define sexism as the prejudicial and discriminatory nature of sexist behavior pervading in the social context, especially for women.Using theoretical concepts, along with the typology of abuse presented in earlier research, Guest et al. (2021) define misogyny as content "directed abuse at women or a closely-related gendered group (e.g., feminists)."Whereas Lynn et al. (2019) define misogyny as a hate crime, a result of a "cultural attitude of hatred for females because they are female", and presented with two bidirectional-DL models (Bi-LSTM and Bi-GRU) with dropout layers, which performed well in sensitivity and accuracy even with a slightly imbalanced dataset.</p>
<p>Beyond binary classification, combining techniques can enhance model performance.For example, Attanasio and Pastor (2020) demonstrated improved results using a multi-agent classifier built upon sentence embeddings, TF-IDF weighting, and lexicons specific to misogynistic language.Plaza-del Arco et al. (2021) proposed Multi-Task Learning (MTL) system with hard parameter sharing approach (sharing the hidden layers between all tasks, while keeping several task-specific output layers) using BERTbased models for utilizing the transferred knowledge from multiple other (but related to sexism identification) tasks like polarity and emotion classification and offensive language detection classification helped in the identification, both in binary and multiple categories.Though emotion was not helpful in categorizing, MTL shows promising generalization to the original task.</p>
<p>Additionally, Frenda et al. (2019) exploit stylistic, semantic and topic information about misogynistic speech to identity misogyny and classify it to different categories.For gathering linguistic features, they propose an approach based on stylistic features captured by means of character ngrams, on sentiment information and on a set of lexicons built by examining the misogynistic tweets from training data provided by the organizers.Each text was represented by a vector composed of all specific topic features(set of lexicons), pondered with Information Gain, and character n-grams, weighted with TF-IDF measure.This set of features were experimented employing a Support Vector Machine (SVM) algorithm and an ensemble technique, reaching promising results.Canós (2018) worked on the same data and task, experimenting SVM alongside TF-IDF with a one-vs-one and one-vs-rest classifier approach, where the later proved better for English, presumably because of larger vocabulary.On the other hand, Nozza et al. (2019) defined several templates to create a balanced synthetic dataset for their proposed DL model-Universal Sentence Encoder (USE), which further debiased their model features to be less-sensitive to identity terms and yet obtain a better categorization.</p>
<p>Overview of performance evaluation</p>
<p>When it comes to evaluating performance, classification models with traditional computational approaches seem to fair relatively similar or better (in some cases) at automated sexism/misogyny identification tasks.Indurthi et al. (2019)'s work shows how different set of pretrained embeddings trained from different state-of-the-art architectures and methods when used with simple machine learning (ML) classifiers like SVM and XGBoost perform very well in binary classification tasks.Kohli et al. (2021) used two kinds of methods: first using an ensemble approach comprising of XG-Boost, LightGBM and Naïve Bayes; and second employing BERT-based architecture.Both the models performed well on binary identification task, but differently on different languages and aggression label analysis, one of which was gendered, due to the overlapping context in all.Overall, SVM is seen to be the best-performing conventional classifier (hence taken as a baseline for some of the works), and a lot of papers have used it in their work as a standalone classifier, or as an ensemble voting classifier (Frenda et al., 2018;Nascimento et al., 2022a)-alongside other classifiers like Gradient Boosting and Random Forest (RF).Regression models like Logistic Regression have also been used by a lot of studies, especially for binary tasks.While Decision Tree (Plaza-Del-Arco et al., 2020), and RF (Singh et al., 2021) has also been used, they do not show much success among the conventional ones whereas most DL models use Fully Connected (FC) layers for classification (Bashar et al., 2019).Yet Convolutional Neural Network (CNN) with various semantic features is also seen to work well in categorizing hate-speech -particularly sexism, even outperforming Logistic Regression for some of the performance metrics (Gambäck and Sikdar, 2017).</p>
<p>However, there is no definitive indication of whether traditional models or neural networks (DL models)-particularly transformer-based architectures-are generally preferred for detecting or identifying sexism or misogyny.While some studies did not perform comparative analyses across different model architectures (e.g., (Waseem, 2016;Shah et al., 2020;Frenda et al., 2018;Canós, 2018)), others explicitly compared selected models to highlight performance differences.For instance, some studies such as Plaza-Del-Arco et al. ( 2020 2021) reported the opposite.Additionally, some studies showed comparable results across different model types (e.g., (Grosz and Conde-Cespedes, 2020;Singh et al., 2021;Bashar et al., 2019)).Transformer models have been shown to offer clear advantages over conventional neural network models in several studies (e.g., (Chiril et al., 2021(Chiril et al., , 2020;;Parikh et al., 2019)).Learning sequence and contextual information through embeddings yielded competitive performance in certain cases, particularly using LSTM and BERT-based models (e.g., (Badjatiya et al., 2017;Rodríguez-Sánchez et al., 2020;Chiril et al., 2021)).More broadly, the integration of embeddings was generally found to enhance model performance (e.g., (Fersini et al., 2021;Anzovino et al., 2018;Jha and Mamidi, 2017)).Furthermore, incorporating text features such as n-grams (e.g., (Bhattacharya et al., 2020a;Waseem and Hovy, 2016;Butt et al., 2021)) or combining both embeddings and n-gram features (e.g., (Abburi et al., 2021;Sharifirad et al., 2018)) also contributed to improved outcomes.</p>
<p>Yet, studies such as Gröndahl et al. (2018); Arango et al. (2022) suggest that most research emphasizes model development, with limited attention to evaluating whether these models generalize to other contexts or understanding the factors that influence their success or failure.In fact, Arango et al. (2022) found a significant drop in performance while replicating some state-of-the-art methods across multiple datasets.Herd and Burton (2024) explicitly state that "relying on such single point estimates to evaluate safety requirements is problematic since they only provide a partial and indirect evaluation of the true safety risk associated with the model and its potential errors".Given the probabilistic result the metrics provide over a binary domain, it may overestimate the model performance since their trustworthiness depends on other secondary factors such as sample size, the model calibration, the quality of the dataset, among others (Herd and Burton, 2024).This raises the question of whether strong performance metrics alone are truly reliable indicators of a robust or effective model.</p>
<p>Is classification the only way?</p>
<p>Though almost all the computational methods employ classification techniques, it is not the only way.But it is favorable, for good reasons.Clustering techniques are mostly useful for content analysis and to study discourse, to help identify implicit themes/topics from the data which may be (unintentionally) omitted during manual inspection, and reassignment into its overarching categories for better interpretation, even though may sometimes provide superficial results (Siddiqi et al., 2018).Karami et al. (2019) employs unsupervised text-mining approaches like LDA topic modeling.Utiliz-ing the themes they found, they performed qualitative thematic analysis before finally moving to a theoretical thematic analysis to group the previously identified topics into four categories of sexism.Melville et al. (2019) also uses LDA for grouping 7 topics, and alongside clustering based on Louvain algorithm (Blondel et al., 2008) for grouping 20 topics.They define sexism based on themes and sites associated with the experience of sexism from Everyday Sexism (Bates, 2015) and journalism.From these studies, it is evident that clustering is more useful for content analysis, rather than for the detection/identification tasks.</p>
<p>Challenges</p>
<p>In this section, we outline the challenges for the interdisciplinary approaches that are likely due to the disciplinary divide, and ways of addressing them.We identify that these challenges could be because of two broad reasons -(i) Use of different computational strategies; and (ii) Linking social science theories to the tested computational strategies.The first part essentially talks about the different strategies used, compares them based on different parameters in each subsection, and weighs the advantages and limitations of each approach.The second part focuses more on how existing literature has sexism and misogyny in their work.By analyzing how the same terms are defined in social science theories, we form an argument about how the existing computational research could benefit from a more fine-grained categorization of sexism (or misogyny) to improve their automated identification task.</p>
<p>Use of different computational strategies</p>
<p>This subsection is intended to shed light on the different computational strategies that have been used to quantify sexism and misogyny, while also segregating the strategies based on some differentiators like the small dataset size and the dataset languages used.</p>
<p>Beyond Binary Classification</p>
<p>Most of the studies have used binary classification of sexism (e.g., (Singh et al., 2021;Schütz et al., 2022;Grosz and Conde-Cespedes, 2020)) and misogyny (e.g., (Bashar et al., 2019;Lynn et al., 2019;Bashar et al., 2019)) for their detection tasks.While practical, such models often overlook theoretical nuance, providing surface-level predictions that may fail to capture the complex manifestations of sexism (Parikh et al., 2019;Abburi et al., 2021).Previous studies have highlighted the difficulty in capturing the nuanced meanings of terms like sexism and misogyny in computational settings (Samory et al., 2021), emphasizing the importance of evaluating both the intensity and specific type of misogynistic behavior within a given context (Lynn et al., 2019).</p>
<p>Tasks such as hate speech or harassment detection inherently involve subjectivity, with no universally accepted definitions or absolute truths (Röttger et al., 2022).While certain beliefs may be broadly agreed upon, they do not fully encapsulate the complexity of these concepts.Moreover, annotation processes are often conducted without direct oversight from dataset creators, resulting in partially subjective datasets that may not align clearly with specific downstream applications (Röttger et al., 2022).Even with detailed label descriptions, human annotators often struggle with intuitively unclear (Vidgen and Derczynski, 2020) or closely related (Parikh et al., 2019) categories.Some improvements have been observed when annotation guidelines are adjusted to address edge cases (Zeinert et al., 2021),though further distinction-such as between active and passive language-may offer additional clarity (Anzovino et al., 2018).Moreover, perceptions of toxicity are highly subjective and often shaped by the receiver's interpretation of the speaker's expression (Sap et al., 2019).</p>
<p>Despite increasing interest, research addressing the nuanced forms of sexism and the intersectional ways women experience it-online and offline-remains limited (Melville et al., 2019).Barak (2005) identifies four types of gender-based harassment in cyberspace: active verbal, passive verbal, active graphic, and passive graphic.These categories are influenced by both objective features (e.g., explicitness, repetition) and subjective factors (e.g., recipient's attitudes, sensitivities).As a result, perceived severity varies widely across individuals-a principle equally applicable to sexism and misogyny.Similarly, Sharifirad et al. (2018) propose a four-type classification of sexism: information threat, and indirect, sexual, and physical harassment.To overcome data scarcity, they use Con-ceptNet to semantically enrich text via augmentation techniques-leveraging relations like "IsA" and "RelatedTo"-with three replacement strategies: all words (most effective), nouns, and verbs.They find that such semantic enrichment, especially through text generation alone, considerably boosts classification performance.Swim et al. (2004) point out that using only sexist language-as done in the above studies-limits the generalizability of findings to other, less overt forms of sexist behavior.Still, multi-label classification has shown promise.Echoing this approach, Talavera et al. ( 2021) also employ a multi-label model with fewer (five) categories.They reinforce earlier findings that pretrained language models, when fine-tuned to the specific task domain, are particularly effective in low-resource settings.Most of the existing research on multi-class classification of sexism or misogyny consider at most five categories of sexism (such as (Anzovino et al., 2018;Sharifirad et al., 2018;Jha and Mamidi, 2017)).Anzovino et al. (2018) (Fersini et al., 2018) where the data was categorized to five categories, namely: Stereotyping and Objectification, which involve oversimplified portrayals of women or emphasis on their physical appearance; Dominance, which asserts male superiority to reinforce gender inequality; Derailing, which shifts blame away from men or disrupts conversations to refocus them around male comfort; Sexual Harassment and Threats of Violence, which encompass unwanted advances, coercion, or threats intended to exert power over women; and Discredit, which targets women with slurs or insults without any broader argumentative context or intent.</p>
<p>Likewise, the EXIST shared tasks in 2021 and 2022 (Rodríguez-Sánchez et al., 2021;Rodríguez-Sánchez et al., 2022) begin with a binary definition of sexism based on the Oxford English Dictionary -"prejudice, stereotyping, or discrimination, typically against women, based on sex" -and extend it into a multiclass taxonomy comprising five expert-defined categories: Ideological and inequality, which includes anti-feminist rhetoric or denial of structural gender inequality; Stereotyping and dominance, referring to assertions of traditional gender roles or male superiority; Objectification, where women are reduced to physical appearance or sexualized attributes; Sexual violence, encompassing harassment, coercion, or threats of a sexual nature; and Misogyny and non-sexual violence, involving expressions of hatred, hostility, or aggression directed at women.Both datasets support the detection of a broad spectrum of sexist content, ranging from explicit misogyny to more subtle, implicit expressions of sexist behavior, as opposed to most other research which only focus on the detection of explicit contents (such as (Waseem and Hovy, 2016;Katsarou et al., 2021;Anzovino et al., 2018)).</p>
<p>The utility of multi-label classification is highlighted in Parikh et al. (2019), who illustrate the cooccurrence of various sexism categories in user-reported first-person accounts.Their dataset comprises 23 categories (defined by a social scientist) derived from real-world reports of sexism.The annotation process followed a rigorous three-phase protocol involving pre-training, pilot testing, and final quality checks, ultimately reducing the labels to 14 for classification.To address domain-specific challenges, they fine-tuned a BERT model using masked language modeling and next sentence prediction, further enhancing it with distributional word embeddings and a linguistic feature vector.These linguistic features drew from prior work on biased language detection and incorporated affective and sentiment lexicons, including PERMA (Positive Emotion, Engagement, Relationships, Meaning, Accomplishment) features and emotion scores.Their proposed multi-label, multi-class LSTM-based framework significantly outperformed various traditional machine learning and deep learning baselines.</p>
<p>Building on this work, Abburi et al. ( 2021) take a more fine-grained approach by reintroducing the full 23-category schema.Using self-trained semi-supervised learning, they augment the labeled dataset to allow co-occurrence of categories and improve class distribution.Their approach enhances textual diversity and focuses on hard-to-classify samples using confidence scores and selective intersection.In addition to the domain-tuned BERT with a BiLSTM and attention mechanism, they incorporate a custom loss function that leverages label confidence scores.Their domain-tuned BERT-BiLSTM-Attention model achieved notable gains, though the data focused exclusively on sexist examples and did not generalize to broader detection tasks.</p>
<p>Data Limitations and Model Performance</p>
<p>Even when models perform well, limited dataset size can undermine reliability and restrict exploration of more sophisticated deep learning (DL) architectures, such as those employing advanced attention mechanisms requiring larger amounts of data (e.g., Grosz and Conde-Cespedes, 2020).For example, in their binary classification of misogyny, Guest et al. (2021) developed a hierarchical taxonomy across three levels: the second level featured four non-mutually exclusive categories-misogynistic pejoratives, treatment, derogation, and gendered personal attacks-with further sub-categorization at the third level.Although the classification is sound, and logistic regression and both weighted and unweighted BERT achieved strong performance, the dataset's limited size and low proportion of misogynistic content (8.1%) posed challenges, especially since classification was performed only on the instances labeled True for misogyny.</p>
<p>To address small dataset issues, Schütz et al. (2022) explored transfer learning approaches, including the use of multilingual transformers pre-trained on external datasets and data augmentation by integrating similar external content.Their experiments demonstrated that fine-tuning the entire model with domain-specific data improved performance.However, pre-training generally proved more effective, as fine-tuning showed tendencies toward overfitting and did not yield consistent improvements on external datasets.As DL models typically require large datasets to achieve optimal performance, traditional models may give a comparative performance (such as (Samory et al., 2021)) or even outperform them in data-constrained settings -particularly when augmented with rich linguistic features such as n-grams (Plaza-Del-Arco et al., 2020; Butt et al., 2021).</p>
<p>Dependence on external benchmark datasets</p>
<p>Several previous studies highlight that the main challenge in quantifying sexism stems from the lack of high-quality datasets required to train robust, scalable automated detection systems (Guest et al., 2021).Most computational studies rely heavily on external benchmark datasets, which can affect the quality, reliability, and representativeness of the data.As Zeinert et al. ( 2021) point out, "When abusive language is annotated, classes are often created based on each unique dataset (a purely inductive approach), rather than leveraging established terminology from social science or psychology (a deductive approach, building on existing research)."</p>
<p>While benchmark datasets are frequently used in shared tasks13 for automated misogyny detection and related challenges, they are often found to be misrepresented or imbalanced.For example, the IberEval2018 dataset contained uneven category representation-certain misogynistic behaviors, like 'derailing,' were underrepresented (&lt;2%), while categories like 'active' tweets were significantly overrepresented (&gt;85%).Additionally, notable discrepancies appeared between language groups in the same dataset (Canós, 2018).Similarly, in the TRAC2020 shared task dataset (Bhattacharya et al., 2020b), texts containing multiple languages were categorized under a single language, complicating analysis for non-speakers unfamiliar with the other languages' socio-cultural contexts.Furthermore, the proportion of texts exhibiting hate speech varied significantly across languages (Gordeev and Lykova, 2020).</p>
<p>These inconsistencies underscore the necessity for dedicated, theoretically-informed, 'reliable' data collection and annotation practices grounded in social science frameworks, before performing any experimentation with NLP tools.</p>
<p>Datasets are mostly in Western Languages</p>
<p>The majority of studies on online hate speech detection have been conducted in English (e.g., Katsarou et al. (2021); Waseem and Hovy (2016); Parikh et al. (2019), inter alia).Spanish and Italian follow closely, particularly through shared tasks-for example, studies in Spanish (Schütz et al., 2022;Plaza-Del-Arco et al., 2020;Canós, 2018) and in Italian (Attanasio and Pastor, 2020;Ou and Li, 2020;Nozza et al., 2019), inter alia.More recently, a limited number of works have begun to explore other languages such as Hindi, Bangla (e.g., (Bhattacharya et al., 2020a)) and Chinese (e.g., (Jiang et al., 2021)).But even within same languages, there lies substantive differences in the peculiar lexical choice and morphological structures rising from the regional colloquial usages, leading to linguistic and cultural heterogeneity (Bhattacharya et al., 2020a).However, this fundamental issue remains largely unexamined in the current literature.Though models like fine-tuned cross-lingual multitask BERT shows promising performance even with non-English languages, with it performing better on Bangla when experimented alongside English, presumably owing to the dataset peculiarity or specific features of the language itself (Gordeev and Lykova, 2020).But when it comes to representation, there is a huge gap between only the use of Indo-European languages (especially English) and other languages.When working with multiple languages, "back-translation" has been used in the said languages to augment the data and translate all of them to a single uniform language, which could be one of the source languages or different.Butt et al. (2021) performed the same technique on Spanish and English (source) languages to convert it into English, with German being the second language, using the deep-translator python library.And their results on all of their tried ML algorithms show an improvement with the augmentation; even indicating that with proper pre-processing, it could give competitive results in comparison to deep learning models.Zeinert et al. (2021) too had experimented translating misogynistic posts provided by Anzovino et al. (2018) to Danish using translation services in an attempt to augment the minority class data.But it did not prove as useful in providing a sampling alternative, hence inferring that language-specific investigation is important for cultural discovery, for the sake of automatic detection systems.Rodríguez-Sánchez et al. (2020) found improved cross-domain generalization when models were trained on their Spanish dataset and tested on an Italian dataset.This suggests that some datasets may offer broader coverage of sexist content-including explicit, implicit, and contextdependent forms-and are therefore more transferable for studying the same task across different languages.However, we observe a lack of experiments involving languages that differ significantly in cultural-linguistic context and core linguistic dimensions.</p>
<p>In fact, Waseem (2016) suggests against boosting the minority class in the interest of mimicking reality in the datasets, even if it causes larger misclassification for the class.Rahali et al. (2021) uses gender swap data augmentation and data consolidation with feature ablation, which is seen to improve the learning of the model, especially when used with the same language.But using multilanguage datasets does not help much, since English does not consolidate well with other languages (e.g., Arabic and French) with limited samples as compared to English, inevitably giving rise to data imbalance, a data bias.So, there is a need to look beyond English.Singh et al. (2021) converted the whole dataset from multiple languages to a uniform English dataset, by transliterating the sentences belonging to other languages using IndicTransliterator.But that required transferring a word from the alphabet of one language to another, which could give faulty outcomes.Given the linguistic variety and limitations that could be faced when delving into other languages, improving the existing gaps in sexism identification tasks in English should be of primary focus.</p>
<p>Biases</p>
<p>Bias is a broad term that can be defined in various ways, depending on the field and context.In this section, we summarize: (a) how studies in our dataset define or quantify biases, (b) whether these studies acknowledge biases present in their own research, and (c) what measures, if any, they adopt to mitigate these biases.Social science studies tend to address bias in their work.In NLP, however, definitions of bias often depend heavily on domain-specific concerns like biases in word embeddings, annotator labels, or amplified predictions related to demographic characteristics (Hovy and Prabhumoye, 2021).In this work, we adopt the definition by Shah et al. (2020), where bias refers specifically to "the mismatch of ideal and actual distributions of labels and user attributes in the training and application of a system."Additionally, the rapid advancement of NLP may itself exacerbate bias by outpacing the field's ability to adapt effectively to emerging contexts (Hovy and Prabhumoye, 2021).Eagly and Mladinic (1989) use attitude theory to explain how stereotypes emerge from attitudes toward men and women.According to them, the cognitive component of an attitude -defined as a person's thoughts about an object -can manifest through attributes assigned to social groups.When attributes assigned to these groups carry evaluative meanings (positive or negative), they give rise to stereotypical perceptions.Their study reveals that even when women are positively evaluated, they tend to be stereotypically viewed as inferior to men in agentic or instrumental (traditionally masculine-positive) qualities, yet superior in communal or expressive (traditionally feminine-positive) qualities.</p>
<p>Gender bias</p>
<p>Further supporting this, Schmid Mast ( 2004) provide empirical evidence of an implicit hierarchical gender stereotype, showing that men are associated more strongly with hierarchy and women with egalitarianism.The magnitude of this stereotype reflects a deeply rooted societal bias.Social media platforms further amplify these biases through their widespread influence.Consequently, biased social information from such sources, when used to train machine learning models, can unintentionally lead to gender biases.Models may thus propagate negative stereotypes about various social groups.</p>
<p>To address this issue, Dinan et al. (2020) proposed measuring bias through a semantic and pragmatic framework, evaluating three dimensions derived from "knowledge of the conversational and performative aspects of gender."By separately analyzing how author gender influences the dataset, they aimed for a deeper understanding and mitigation of gender bias.</p>
<p>Annotator bias:</p>
<p>It is seen that having a lot of categories of misogyny may also impact on the annotators' agreement, both in terms of depth (subcategories) and breadth (different types) of the said categories, owing to the differences in experience and values of the annotators.And their inherent social biases may impact on their choice, especially when working using contexts (Guest et al., 2021).Having different level of understanding of the language in question or personal prejudices, and differing individual world-view are seen as primary issues in inter-annotator disagreements.Bhattacharya et al. (2020a) used several rounds of discussions and sensitization towards gender issues among annotators to resolve this issue, by providing with counterexample method and examining annotator votes, alongside using an 'unclear' tag in case of disagreement.Sometimes when adversarial examples (even just 25%) are included while training the dataset, it is seen to help in the robustness of the models and their performance.In fact, providing the models with different aspects of sexism and challenging the models with different examples have shown to be effective for generalizability (Samory et al., 2021).</p>
<p>Some studies use different criteria for selecting the annotators they want in their study, based on both similarities and differences on each.It could be based on region, demographic, education and ethnicity (Guest et al., 2021), native speakers of language (Chiril et al., 2020), feminists (Jha and Mamidi, 2017); but mostly who studied gender (Lynn et al., 2019) and linguistics (Nozza et al., 2019).A comparison on amateur (crowd-sourced workers) and expert (having both a theoretical and applied knowledge of hate speech) annotators (as most studies use either of them) by Waseem (2016) state the contrasts observed in annotation with both, and the consequent model performances which did not substantially improve on their previous model (Waseem and Hovy, 2016).The emphasis on the most significant features changes from extra-linguistic features for majority-voted amateurs to content of the tweets for the experts, and among the features they experimented with, the ones having highest performances (high F1 score) were not necessarily the features with the best performances.Singh et al. (2021) considers misogyny and sexism as a subset of hate-speech, and used data that was manually annotated by multiple annotators using 'Discursive Methods of Annotation' since it was seen as a pragmatic approach to including the socio-pragmatic phenomenon using social studies, and as a function of both the contextual factors and the discursive experience of the speaker.Zeinert et al. (2021) does an iterative process of raising cases for revision in the discussion rounds, formulating the issue, and providing documentation for annotation, inviting in annotators with diversity in age, occupation/background, region (spoken dialects).Annotation biases can lead to other kinds of bias, like racial bias due to lack of knowledge of different dialects-which could potentially amplify the harm against people from the minority community (Sap et al., 2019).</p>
<p>Other biases Hirsch (1992) documents the oppression of women through language, as she talks about male-specific words that are positively portrayed in English, in turn reflecting the "consensus reality" of the patriarchal society.While theorizing the language and gender connection (with many of the examples drawn from political discourse) from one of its reviewed books, it talks about how language is used as a tool to further perpetuate patriarchy (Cameron, 2020).The same is seen for the computational models based in English.The datasets taken for studies could also add to the bias owing to the different considerations made due to the data source, hence not representing the diversity in real-world.For example, domain sources where misogyny is assumed to be most likely like women fashion blogs, fitness tips videos, etc. (Bhattacharya et al., 2020a); or when sexism is taken as one of the sentiment label, with data collected around some specific cases/instances/event networks like #Coronavirus, #Cli-mateChange, #Immigrants and #MeToo (Katsarou et al., 2021).Another form of such bias could rise because of subjectivity in mislabeled data.Samory et al. (2021) had performed re-annotation on the external datasets they used in their study, following the sexism annotation codebook they devised themselves.Relying on two baselines: Gender-Word (Zhao et al., 2018) and Jigsaw's Perspective API (Hosseini et al., 2017), they found a large majority of sexist tweets were non-sexist, only ∼60% of the sexist labels adhering to their ground truth.They found that stratifying misclassification rates helped in giving a more accurate result.Both these points could hinder model performance.</p>
<p>Yet, with the listed biases, always a question remains if they were a cause of systematic errors (both conscious or unconscious) or were a result of a narrowed preference in a particular direction in favor of the said bias.In other words, the use of 'bias' to refer to systematic error is problematic.According to Hammersley and Gomm (1997), it depends on 'truth' and 'objectivity', whose justification and role have been questioned.Due to the ambiguous nature of the term itself, we might question if the forms of bias explored are a result of methodological adaptability; conscious limitations due to the scope of the research (such as research designs); or could arise because of the models themselves.Either way, they may not indicate the research as "being biased".Of the five most common sources of bias in NLP tasks as identified by Hovy and Prabhumoye (2021), we have reviewed almost all of them in this section.This indicates that these biases are well-known across the CSS literature, and can be explored more to mitigate them from all sources, using algorithmic and methodological approaches.</p>
<p>Lexical dependency for characterizing sexism and misogyny</p>
<p>To linguistically characterize misogyny and sexism, many studies have used different theoretical concepts to represent both.Farrell et al. (2019) had built a list of key lexicons for categorizing misogyny using Encyclopaedia of Feminist Theories (Code, 2002) and other pre-existing hate-speech lexicons and studies of the specific rhetoric of manosphere, taken from different corpus.In their observatory work, they study the evolution of communities where users share in-group characteristics.But even though corroborating the theories and existing ideas helped in providing lexicons, they acknowledged the limitations of using it due to its lack of completeness (shortcomings in capturing all the words that might be relevant).Other times, studies use words 'typically associated' with misogynistic content created by domain experts (Lynn et al., 2019) which is used as neologisms for identification of emerging or cloaked misogyny.</p>
<p>Lexical dependency can cause NLP models to overfit because of too much influence of certain identity terms.This eventually results in false positives, severe unintended bias, and lower performance.Rodríguez-Sánchez et al. ( 2020) acknowledge the biases inherent in keyword-based dataset collection, noting that models tend to exhibit bias toward specific terms and struggle to detect instances featuring shorter length, subtle expressions of sexism, irony, or context-dependent language.Bashar et al. (2019) acknowledge that misogynistic abusive tweets might contain certain keywords, but would not necessarily always contain such slurs.To work around that, they show that classifiers can work with small-labeled datasets, provided that the word vectors used are pre-trained on the context domain of the problem and paired with careful customization and regularization.This proves that a large-labeled dataset is not always required for training purposes.In fact, if the word vectors are pre-trained in the context of the problem domain, alongside careful customization of the model, the classifiers could also be trained on small datasets.On the other hand, Plaza-del Arco et al. ( 2021) generates linguistic resources using a set of word embeddings, with the initial seed lexicon eventually getting populated with words and n-grams more attuned to the domain because of linguistic similarities.Using a voting schema rule with logistic regression and multinomial Naïve Bayes, alongside the lexicon-based system and combinations of unigrams and bigrams gave a good result with the Spanish dataset.Observations show that some expressions of hate when combined with other terms change the sense entirely and hence better-supervised learning begins with larger data.</p>
<p>For larger datasets, the issue is elevated with the imbalanced nature of the datasets and their disproportionate dependence on these determinate terms, having a high correlation to minority class (Nascimento et al., 2022b).Using such identity terms, or samples from target domains during the training phase requires a-priori knowledge but can often lead to the introduction of further bias.Introducing a regularization approach to the models to add some degrees of contextualization using Entropy-based Attention Regularization (EAR) could mitigate the problem to some extent, as they are seen to show competitive performance, along with an improvement in the bias metrics (Attanasio et al., 2022).Consequently, developing classifiers that can decompose gender bias within full sentences into semantic dimensions can be used, since it can be contextually determined (rather than being explicitly gendered).This has in turn shown to give a better performance in controlling gender differences (Dinan et al., 2020).Ou and Li (2020) find limitations of only using the pooler output of DL multilanguage models like XLM-RoBERTa, and hence obtains deeper and more abundant semantic features by extracting from its hidden layer state which gives better performance.Data correction strategy focused on gender bias, consisting of two-stage modules-bias detection and replacement of the said bias-sensitive words (BSWs), is seen to reduce the differentiation of similar terms related to gender, and in turn, contribute to mitigating the unintended bias.Since the frequency of female identity terms is high (even when representing similar groups/classes or other social identities) in datasets related to sexism and misogyny, they replaced these potential bias terms with <identity> tag without compromising the model accuracy.Their proposed multi-view stacked classifier is seen to outperform other state-of-the-art models and diminish gender bias (Nascimento et al., 2022b).</p>
<p>Natural language processing (NLP) applications like sentiment analysis are crucial for analyzing and detecting online sexism/misogyny.Incorporating polarity and emotion information is seen to be useful for the benefit of the task as they portray the usually emotional, expression of negative emotion and polarity towards the recipient (Plaza-Del-Arco et al., 2020;Plaza-del Arco et al., 2021).Using feature representations has further helped in training the model, by adding representations of the text in terms of various lexical, syntactic, and morphological features.While the most common types of features used are the bag-of-words representations of text, and/or the embeddings, adding to the features also helps in the performance.Many papers have used it to enhance their model performance.The idea is to map out the various aspects of sexism as seen in the everyday social constructs and use it to comprehensively map them out for the benefit of the identification tasks (Samory et al., 2021).</p>
<p>Also looking beyond the text, Chiril et al. (2020) performed further characterization of the binary sexist classification by distinguishing cases where the addressee is directly addressed from those where she is not.The three categories being: (i) directed assertions -sexist tweet directly addressed to a woman or a group of women; (ii) descriptive assertions -sexist tweets not directed to an addressee; and (iii) reported assertions -tweets containing report of an experience or a denunciation of sexist behavior.On performing classification based on results per class, they identified the absence of context with the utterance, humor, and satire, and the use of stereotypes or metaphors to be the causes of misclassification through their manual error analysis in their best performing model-BERT.As Frenda et al. (2019) had also stated one of their principle problems is the use of linguistic devices like irony and sarcasm.In general, it has been highlighted in multiple research (e.g., (Guest et al., 2021;Rodríguez-Sánchez et al., 2020)) that the more subtle forms of sexism (mostly depended on the context) are not picked up well by the models.As Singh et al. (2021) note in their error analysis, many of the confounding variables were specific lexical items that either denoted explicitly sexual content or conveyed strongly negative sentiment.</p>
<p>Different psychometric scales can also be used to map out various aspects of sexism/misogyny as a social construct, to comprehensively detect the different categorizations.King and King (1997) reaffirm the previously stated theory on modern sexists, and describe them as "people who while rejecting old-fashioned discrimination and stereotypes, may believe that discrimination against women is a thing of the past, feel agnostic against women who are making political and economic demands, and feel resentment about special favors for women, such as policies designed to help women in academics and work."In other words, the distinction between old-fashioned and modern sexism lies in the fact that the former showcases an obvious unequal treatment of women while questioning their intelligence, while the latter is less sympathetic to women's issues (if at all they perceive them to be issues) since they presume greater equality in the workforce than what exists.The Modern Sexism (MS) scale this study provides aims to be a good indicator to detect modern sexism, which could be both overt and covert in nature.People endorsing MS beliefs are hence less likely to detect the occurrence of a normative sexist behavior (Swim et al., 2004).In the review by Swim and Cohen (1997) on the MS scale, they indicate the same as they observe that it measures the subtle forms of sexism that are built upon cultural and societal norms.They also review another general measure of sexism, namely the Attitude Toward Women Scale (AWS), which measures overt or blatant sexism.And through their analysis, they indicate that even with these distinctive differences, both share related constructs.These social constructs are often perpetrated as discriminatory attitudes towards a feminine gender role, which are traditionally allocated and differentiated by sex.García-Cueto et al. (2015) propose a scale to assess the gender role attitude, showing how sexist attitudes can be modified using the theoretical perspectives of gender equality.</p>
<p>Linking social science theories to computer science research</p>
<p>Following from the previous subsection where we introduced our argument that sexism/misogyny is not a binary task, in this section, we expand on that point by providing social science theories and scales to explain the need to not computationally limit the classification to the binary output.To support that, alongside including the theories and scales, we also analyze how some studies have aided their work with these theories in any capacities (i.e., the extent of adaptation -using one or more categories of the scaling) and implemented them at any stage of their research.We distinguished each subdivision into two parts: the concept and the applications, to help us differentiate between the concepts themselves and on how they are implemented in studies.</p>
<p>Sexism is not always hostile</p>
<p>Concept.</p>
<p>Grosz and Conde-Cespedes (2020) state, that models can perform detection tasks easier on datasets containing large amounts of "hostile" sexism, since it hinges on some words, regardless of their context.But that does not provide a real-world scenario.In general, sexism is said to have two components: hostility towards women and endorsement of traditional gender roles, and most of the sexist attitude measures so far have stemmed from there.But it is not always so.Through their anthropological research on sexism, Glick and Fiske (1997) call sexism "fundamentally ambivalent", adding the subjectively benevolent nature of sexism to the previously perceived singularly hostile nature.They argue that the "simultaneous existence of male structural power and female dyadic power" creates an ambivalent ideology.While the hostile ideology seeks justification of their male position through derogatory characterization of women (HS), benevolent ideology relies on kinder and gentler justification, which may inherently look as subjectively positive for the sexist as they encompass feelings of protectiveness and affection towards women (BS).By drawing parallels from paternalism, which also has two ideologies-dominative and protective, they demonstrate that the protectiveness is particularly strong when women(e.g., wives, mothers, daughters) are dyadically dependent on men, as a feeling something akin to the sense of "ownership".The hierarchical stereotype ideology explained before constitutes the belief contributing to the gender differentiation.Like paternalism, it also consists of both hostile and benevolent side.Competitive gender differentiation being the hostile kind, delves on negative stereotypes of women implying men to be the better gender; and the complementary gender differentiation (the benevolent kind) stems from the traditional stereotypes of women through assigned gender roles and men's dyadic dependence on women, albeit in an extremely positive light (Eagly and Mladinic, 1994).Similarly, for heterosexuality, which has a hostile side when viewing women as mere sexual objects who use sexual attraction to gain power over men; and intimate or benevolent side that romanticizes the former belief, viewing women as necessary for men to feel "complete".</p>
<p>Applications.</p>
<p>Sexism in ambivalent theory (Glick and Fiske, 1996) is thus hypnotized to encompass these three sources of male ambivalence, which has been used by Jha and Mamidi (2017) to computationally identify benevolent sexism, and classify sexist content based on the two components.They confirm the hypothesis that HS is evidently negative and easily identifiable, while BS is retweeted much more and is camouflaged, seemingly harmless or noble and hence, harder to detect.It was seen that while SVM showed high precision for both, recall was quite low for HS; their Seq2Seq model (LSTM-based bi-directional RNN) showed a higher recall for both, even though its precision was not as high, presumably because it takes in the structure of the tweet.But owing to the bag-of-n-grams feature of FastText (and lesser parameters to tune), it outperformed both the former classifiers.On the other hand, Singh et al. (2021) used the hostile side of the three sources of male ambivalence to define sexism binarily and annotate dialogues in popular sitcoms.Using these concepts, they manually annotated the external datasets (source domain) and used a semi-supervised domain-adaptive learning approach to generate classes in the model for the unannotated data (target domain), thus further augmenting the training data and improving the final classification performance.However, error analysis showed certain false positives like incorrectly classifying aggressive negative statements to a particular woman, or contents with explicit sexual terms and mentions of marriages or weddings as sexist.This could be the underlying drawback of not using a diverse dataset since the authors had included dialogues that included derogatory terms and dialogues justifying stereotypes against women or gender roles.But Mishra et al. (2019) use the concepts from previous research rather differently, by taking inspiration from studies that use randomly initialized user embeddings for improving performances, and inter and intra-user representations based on tweets.Instead of the former semi-supervised approach, they use graph convolutional networks (GCN) based approach, applied to the heterogenous graph representation of two types of nodes-authors and their tweets, to generate richer author profiles.The intention was to use such heterogenous representation to enable the model to learn both community structure and linguistic behavior of authors in such communities.Even with this improvement, several abusive tweets were misclassified, primarily due to the presence of abusive content in the URL (not in the tweet itself), and the deliberate obfuscation of words and phrases by the authors to evade detection.</p>
<p>Subtle forms of sexism/misogyny</p>
<p>Concept.Since most of the sexism measurement scales are focused on hostile sentiments, it fail to capture the contemporary forms of subtle sexism, which are often cloaked in the guise of egalitarian views and harbor (more) traditional beliefs.Only some of the previous works (such as (Rodríguez-Sánchez et al., 2021;Rodríguez-Sánchez et al., 2022;Samory et al., 2021)) have addressed the more subtle and covert forms of sexism.Yet, due to the increase in social awareness of sex discrimination, the more blatant form of sexism is reduced, replaced with the subtle forms of indirect indices.And the lack of conceptual framework of understanding, coupled with methodological problems were indicated in the simulation study conducted by Beattie and Diehl (1979), where they observe the use of indirect means to interpret the gender and hence influence the evaluation criteria.This gave suggestive evidence to a new form of sexism called "neosexism", which was first introduced by Tougas et al. (1995), and defined as "a manifestation of a conflict between egalitarian values and residual negative feelings towards women".They used a predictive model of 'attitude to affirmative actions' to test the dis-criminatory bias and evaluated the practical implications of neosexism through their Neosexism Scale (NS).The study indicated that "neosexist beliefs were linked with opposition to programs designed to facilitate integration of both women and minorities", which leads to further proves the importance of understanding the existing prejudicial beliefs of women to understand the different forms of sexism.</p>
<p>Applications.An analysis of the cross-sectional data during the 2016 US presidential election and the #MeToo movement by Archer and Kam (2020) shows its significant correlation to neosexism, and the various degrees of dismissal of the respondents to the existing gender discrimination, hence indicating its existence in online platforms.Zeinert et al. (2021) had used NS in their work on Danish tweets to add neosexism to their taxonomy along with the previously categorized forms of sexism.Interestingly, while annotating, they found that neosexism formed the most common form of misogyny and accounted for most of the annotation challenges based on disagreements, primarily due to the challenge of understanding the author's intentions, the degree of abuse (since misrepresentation could harm the subject or the fact) and lack of world knowledge.This further added to the class imbalance in the last stage of sexism labeling in their dataset which affected the reliability of the performance, even though they started with a 1:1 class balance at the initial stage (labeling abusive or not) of their iterative labeling scheme based on the MALER framework proposed by Finlayson and Erjavec (2017).To prevent such bias caused by an imbalanced dataset, Indurthi et al. (2019) process the training dataset using SMOTE (Chawla et al., 2002) which synthetically oversamples data and ensure all classes have an equal number of instances.While the existence of subtle forms of sexism and misogyny is undeniable, having unbiased data representative of the same is essential to gain a better computational outcome.</p>
<p>Summary of general strategies used and existing challenges</p>
<p>Our summary of key research findings identified through the literature review reflects the current drawback in the study of sexism and misogyny identification tasks.Irrespective of the different measures taken by the literary works, some limitations remain consistent, which further hinder obtaining a robust model capable of quantifying sexism or misogyny.As Vidgen and Derczynski (2020) suggest, "More standardization is an important aspiration as research continues to mature, although it must be balanced with enabling research innovation and freedom."Therefore, we summarise the research findings in the following points: (1) We identify the various forms of online sexism -from direct abuse to implicit biases, indicating a need for nuanced detection and classification mechanisms; (2) Most studies focus on Western contexts and often overlook intersectional identities such as race, age, and sexual orientation, limiting the generalizability of findings; (3) There is inconsistency in how online sexism is defined and studied, with diverse methodologies and terminology making comparative analysis difficult; (4) The complex nature of online sexism requires insights from multiple disciplines -such as Computer Science and Social Science to form holistic solutions; (5) While some automated solutions for such a huge amount of online content show promise, there is a need for theoretically grounded exploration of online sexism beyond the scope of current research; (6) A high performance score does not necessarily indicate a robust model, which consequently highlight the potential need to incorporate additional metrics for a more comprehensive evaluation.</p>
<p>Conclusion</p>
<p>In this systematic literature review, we examined the multifaceted and evolving phenomenon of online sexism and misogyny, with particular attention to its categorization, detection, and the methodologies employed by existing literature across diverse digital platforms.Our analysis revealed that online sexist discourse manifests in a wide spectrum, ranging from explicit abuse and harassment, to implicit and context-dependent expressions.While advancements in NLP and machine learning have enabled meaningful progress in detecting overt forms of online sexism, subtler and more nuanced forms remain underexplored to maintain good consistency and accuracy using various methodologies.We also found that existing taxonomies and datasets are often limited in scope, predominantly rooted in Western contexts (both culturally and linguistically), and frequently lack intersectional perspectives.Furthermore, we observed that the research in this domain remains largely siloed, with insufficient integration between computational, sociological, and feminist theoretical frameworks.The findings of this review emphasizes the necessity for more inclusive, interdisciplinary, and contextaware approaches to the study and mitigation of online sexism.Future research should prioritize the creation of culturally diverse datasets, the development of unified conceptual taxonomies taking into account the range of expressions for sexism and misogyny, and possible incorporation of marginalized voices in both the design and evaluation of detection systems for better perceptivity.By fostering collaborations across disciplines and centering equity in technological development, we can advance more effective and socially responsive strategies to combat sexism and misogyny in online digital spaces.By highlighting these challenges, we aim to guide future research toward deeper theoretical integration, improved computational methods, and more representative datasets, ultimately fostering more nuanced understandings of sexism and misogyny in digital spaces.Through this work, we hope to contribute to further development on this topic ensuring updated resources on the same, and encouraging investigation on the change in dynamics of online sexism and misogyny.</p>
<p>Citations and their search queries Google Scholar ((misogyny OR sexism) AND (hate OR toxic OR abusive OR offensive) AND (detection OR identification OR prediction OR classification) AND ("natural language processing" OR NLP OR "deep learning" OR "machine learning" OR ML OR "artificial intelligence" OR AI) AND (lan-guage="English" AND yr="2012 -2022")) ArXiv (all : sexism + OR + all : sexist + OR + all : misogyny + OR + all : misogynist + OR + all : %22gender + discrimination%22 + OR + all : %22gender + violence%22 + OR + all : %22gender + stereotype%22) AND WC=(("History" OR "Political Science" OR "Womenś Studies" OR "Social Sciences" OR "International Relations" OR "History %26 Philosophy Of Science" OR "Linguistics" OR "Anthropology" OR "Sociology" OR "Social Work" OR "Language %26 Linguistics" OR "Information Science" OR "Psychology" OR "Social" OR "Ethnic Studies" OR "Philosophy" OR "Psychiatry") NOT ("Computer Science" OR "Artificial Intelligence" OR "Theory %26 Methods" OR "Engineering" OR "Software Engineering" OR "Scientific Disciplines" OR "Automation %26 Control Systems" OR "Mathematical" OR "Mathematics" OR "Mathematical Methods")) AND PY=2012-2022 Methods" OR "Engineering" OR "Software Engineering" OR "Scientific Disciplines" OR "Automation %26 Control Systems" OR "Mathematical" OR "Mathematics" OR "Mathematical Methods") NOT ("History" OR "Political Science" OR "Womenś Studies" OR "Social Sciences" OR "International Relations" OR "History %26 Philosophy Of Science" OR "Linguistics" OR "Anthropology" OR "Sociology" OR "Social Work" OR "Language %26 Linguistics" OR "Information Science" OR "Psychology" OR "Social" OR "Ethnic Studies" OR "Philosophy" OR "Psychiatry")) AND PY=2012-2022  (Lazer et al., 2020).</p>
<p>Hostile Sexism HS Hostile sexism refers to negative views toward individuals who violate traditional gender roles.For example, some people disparage girls who enter traditionally masculine domains such as science or sports (Daniels and Leaper, 2011).</p>
<p>Part of ambivalent sexism (Glick and Fiske, 1996).</p>
<p>Neo-sexism Scale NS A scale designed to tap into a new type of gender prejudice, called neo-sexist beliefs (Tougas et al., 1995).</p>
<p>Benevolent Sexism BS Benevolent sexism includes valuing feminine-stereotyped attributes in females (e.g., nurturance) and a belief that traditional gender roles are necessary to complement one another.Benevolent sexism also includes the view known as paternalism that females need to be protected by males.Benevolent sexism contributes to gender inequality by limiting women's roles (Daniels and Leaper, 2011).Part of ambivalent sexism (Glick and Fiske, 1996).</p>
<p>Bidirectional Encoder Representations from Transformers</p>
<p>BERT</p>
<p>BERT is a language representation model, which is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers (Devlin et al., 2018).</p>
<p>Language Models</p>
<p>(or Large Language models)</p>
<p>LM (or LLM)</p>
<p>A large language model is a computational model capable of language generation or other natural language processing tasks.As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.</p>
<p>Bag-of-words</p>
<p>BoW "A bag-of-words is a representation of text that describes the occurrence of words within a document.It involves two things: 1.A vocabulary of known words.2. A measure of the presence of known words."http://tinyurl.com/5n6d9kntDescriptive Paradigm -"The descriptive paradigm encourages annotator subjectivity to create datasets as granular surveys of individual beliefs.Descriptive data annotation thus allows for the capturing and modeling of different beliefs."(Röttger et al., 2022) Perspective Paradigm -"The prescriptive paradigm, on the other hand, discourages annotator subjectivity and instead tasks annotators with encoding one specific belief, formulated in the annotation guidelines.Prescriptive data annotation thus enables the training of models that seek to consistently apply one belief."(Röttger et al., 2022) Table C.1: Terminologies</p>
<p>D Experimentation results from other citation databases</p>
<p>For Google Scholar, we used both external APIs like SerpAPI for scraping the data, as well as a software named 'Publish or Perish' (Harzing, 2007) to collect the search results.Both of the methods were rejected because of their disadvantages.Such as, Publish or Perish could only extract 1000 results at a time for each search query.While this drawback was overcome by searching for documents with a shorter range of years to stay within the limit, it lacked some of the fields that were needed for this study -abstract and discipline.Alternatively, SerpAPI (SerpAPI, 2019) worked similar to a web scrapping tool and could only scrape the results as the search engine demonstrates, i.e., it only scrapes what Google shows on their Google Scholar pages, nothing more.Even though the fields we got through this API were relevant, they did not contain the full information we needed for the analysis.For example, the full text in the title and abstract was missing and was instead indicated with dotted extensions in the beginning and end of the text.</p>
<p>E Web of Science strategy</p>
<p>We performed automated elimination (or pre-processing) techniques based on the following criteria to narrow down our search results for both areas of study15 :</p>
<p>• Remove studies that are not published in English.</p>
<p>• Remove studies that do not contain any abstracts.</p>
<p>• Keep only the first abstract in studies that contain more than one abstract.</p>
<p>• Remove certain publication types, such as review articles and editorials.With the Web of Science API, separate search queries were used for the two broad disciplines ( or research areas) -CS and Social Science.The categories of the research areas taken for each of them are as follows:</p>
<p>These disciplines were taken from the Web of Science category list, which branches from five major research areas -out of which we took the two categories Social Sciences and Technology.The published works present in the Web of Science Core Collection are assigned to at least one Web of Science category.Each of the said Web of Science categories (as listed in table E) is mapped to one research area found in the classification of research areas16 .The share of documents focusing on different social media platforms, as observed in Figure G.2, reveal that X (formerly Twitter) was the dominant platform for most research in CS, while Facebook (or Meta) was more dominant in SS till 2022.The ease of access to Twitter data during the period could have been a contributing factor to allow application of automated approaches in CS.Whereas, Facebook having more number of active users could have contributed to more research in SS, than any other platforms (including Twitter).Not only do we see a wider range of themes in SS expanding over more number of research (like we observe in the previous subsection as well), but also a steady rise in most of the topics along the time.Especially the theme of 'Feminism with misogyny/sexism' and the 'Hostile sexism' seems to be of particular interest for SS research, given the proliferation of sexism and misogyny beyond offline spaces.The theme of 'Linguistics in sexism' are instrumental in capturing the subtle forms of sexism, and is therefore seen to gain traction over the years.The themes of CS research on sexism and misogyny seem to fluctuate in the given period with no consistent rise, except for the 'Gender-based violence'.</p>
<p>General topics centering around sexism or misogyny</p>
<p>H Abbreviation of models</p>
<p>The abbreviations used in Figure 5.2 are a collection of the following models as shown in Table H.1.</p>
<p>J Expanding on the automated selection techniques used J.1 Topic Modeling approach</p>
<p>The model starts by transforming the input documents (abstracts and the titles) into numerical representations, with the help of embedding, which in these cases is a sentence embedding.Sentence embedding with transformer models maps a text of variable length to a fixed size embedding that should be representative of the the meaning of the input text.For our research, we used the sentence L Studies included in the Meta-analysis</p>
<p>Figure 4
4
Figure 4.1: PRISMA flowchart diagram (Takkouche and Norman, 2011) for this research.Each step shows the number of studies included and eliminated at that point of the research.</p>
<p>Figure 4 . 2 :
42
Figure 4.2: Number of publications per year.The blue bars reflect the research articles on Computer Science, while the yellow bars reflect the research articles on Social Science, between the years of 2012-2022.</p>
<p>Figure 4
4
Figure 4.3: This figure show a UMAP scatterplot, where each point represent one document.The unique colors in the figure represent a different topic in computer science centering around sexism and misogyny between 2012 and 2022.Through topic modeling, usually each document get assigned a set of key words as themes within the paper, which are then grouped together with an unique color, representing the same topic with similar sets of keywords found across all the documents.When grouped, each topic is described by their topic name in the same color.The grey points represent outliers (documents which did not get any assigned topic).The highlighted topic name indicates more relevance to our research objectives.</p>
<p>Figure 4 . 4 :
44
Figure 4.4: Similar to Figure 4.3, this figure show a UMAP scatterplot where each each unique color represent a different topic in social science centering around sexism and misogyny between 2012 and 2022.The highlighted topic indicates more relevance to our research objectives.</p>
<p>Figure 4 . 5 :
45
Figure 4.5: Most frequent keywords gathered from the abstracts and titles of Computer Science studies in the topic of 'Hate Speech Detection using Deep Learning models' On comparing the keywords present in Computer Science and Social Science, we found that the contents of the papers (from their title and abstract) focused on different kinds of sexism and misogyny -both indicating similarity in topics, but contribution at different capacities.They reveal a complementary yet fragmented landscape in how sexism and misogyny are studied across disciplines.In Computer Science (see Figure I.1), research tends to cluster tightly around specific psychological constructs-such as gender stereotypes, hostile and benevolent sexism, ambivalent sexism, general sexist attitudes, and the broader term "sexism" itself-reflecting a strong focus on operationalizing</p>
<p>Figure 5
5
Figure 5.1: Network displaying all the author collaborations between the 45 research articles screened for our literature review.</p>
<p>Figure 5 . 2 :
52
Figure 5.2: Sankey diagram of the link between each categories of online data, classification type and computational models.The Sankey Diagram allows to visualize flow between various points in a system.In our system, we show the flow (i.e., count of association) between sources of online data, sexism/misogyny classification type, and NLP model used.</p>
<p>);Schütz et al. (2022);Butt et al. (2021) found that traditional models (e.g., SVM, Random Forest) outperformed neural networks, whereas others likeSharifirad et al. (2018);Lynn et al. (2019);Guest et al. (</p>
<p>classify misogyny into five subcategories: Discredit, Harassment &amp; Threats of Violence, Derailing, Stereotype &amp; Objectification, and Dominance.Guest et al. (2021) extend from their work to develop a hierarchical taxonomy with three levels to define misogynistic content, with four overarching categories of misogyny: (i) Misogynistic Pejoratives, (ii) descriptions of Misogynistic Treatment, (iii) acts of Misogynistic Derogation and (iv) Gendered Personal attacks against women.Studies like Frenda et al. (2018); Canós (2018); Shah et al. (2020) use the IberEval 2018 shared task dataset</p>
<p>Elsevier(</p>
<p>'misogyny detection OR misogyny identification OR misogyny prediction OR misogyny classification OR sexism detection OR sexism identification OR sexism prediction OR sexism classification') Scopus TITLE-ABS-KEY (( misogyny OR sexism OR gender AND violence OR gender AND discrimination ) AND ( detection OR identification OR prediction OR classification ) AND PUBYEAR &gt; 2011 AND PUBYEAR &lt; 2023 AND PUBYEAR &gt; 2011 AND PUBYEAR &lt; 2023 AND ( LIMIT-TO ( SUBJAREA , "SOCI" ) OR LIMIT-TO ( SUBJAREA , "COMP" ) OR LIMIT-TO ( SUBJAREA , "PSYC" ) ) AND ( LIMIT-TO ( LANGUAGE , "English" )) misogyn<em> OR sexis</em> OR (gender NEAR/10 discrim<em>) OR (gender NEAR/10 stereoty</em>) OR (gender NEAR/10 violence) OR (gender NEAR/10 based)) NEAR/200 (detect<em> OR identif</em> OR predict<em> OR classif</em>))</p>
<p>1: (a) Type of publications in Computer Science.(b) Type of publications in Social Science.Documents focused on social media platforms (a) (b) Figure G.2: Publications mentioning social media platforms in titles or(/and) abstracts in: (a) Computer Science.(b) Social Science.</p>
<p>Figure G. 3
3
Figure G.3  show the different thematic (or topic) representations across the disciplines over the period of 2012-2022.Not only do we see a wider range of themes in SS expanding over more number of research (like we observe in the previous subsection as well), but also a steady rise in most of the topics along the time.Especially the theme of 'Feminism with misogyny/sexism' and the 'Hostile sexism' seems to be of particular interest for SS research, given the proliferation of sexism and misogyny beyond offline spaces.The theme of 'Linguistics in sexism' are instrumental in capturing the subtle forms of sexism, and is therefore seen to gain traction over the years.The themes of CS research on sexism and misogyny seem to fluctuate in the given period with no consistent rise, except for the 'Gender-based violence'.</p>
<p>General topics centered around sexism/misogyny over the years in: (a) Computer Science.(b) Social Science.</p>
<p>Figure I. 1 :
1
Figure I.1: Network diagram of most frequent keywords in Computer Science.Among all the top 100 frequent and relevant keywords, the 6 most common ones (in descending order) are highlighted in the figure: 1. gender stereotypes 2. ambivalent sexism 3. hostile sexism 4. sexism 5. benevolent sexism 6. sexist attitudes</p>
<p>Figure</p>
<p>Figure K.1: Models gathered from the abstracts and titles of Computer Science studies</p>
<p>Abburi et al. (2021) Fine-Grained Multi-label Sexism Classification Using a Semi-Supervised Multilevel Neural Approach Melville et al. (2019) Topic modelling of every-day sexism project entries Sharifirad et al. (2018) Boosting Text Classification Performance on Sexist Tweets by Text Augmentation and Text Generation Using a Combination of Knowledge Graphs Nozza et al. (2019) Unintended bias in misogyny detection Chiril et al. (2020) An Annotated Corpus for Sexism Detection in French Tweets Chiril et al. (2021) "Be nice to your wife!The restaurants are closed": Can Gender Stereotype Detection Improve Sexism Classification?Zeinert et al. (2021) Annotating online misogyny Guest et al. (2021) An expert annotated dataset for the detection of online misogyny Grosz and Conde-Cespedes (2020) Automatic Detection of Sexist Statements Commonly Used at the Workplace</p>
<p>Study or Research Designs
CharacteristicsCountCharacteristicsCountBenchmark Datasets usedLanguages of datasets  ‡Waseem and Hovy (2016); Waseem (2016)14English36Fersini et al. (2018); Basile et al. (2020)13Spanish7Basile et al. (2019)5Italian6Bates (2015)6South Asian languages (e.g., Bangla,6Hindi)Rodríguez-Sánchez et al. (2021)4Other European languages2Fersini et al. (2018)3Paradigms 12  *  ‡Machine Learning modelsPerspective37Support Vector Machine (SVM)16Descriptive5Bidirectional Encoder Representations15Unsupervised (as per model features)3from Transformers (BERT)Long Short-Term Memory (LSTM)15Evaluation type  ‡Logistic Regression (LR)10Binary Classification32Convolutional Neural Networks (CNN)9Multi-class Classification22Naive Bayes (NB)6Multi-label Classification4Random Forest (RF)6Cluster Analysis1Decision Tree (DT)4Results per class1Multilayer Perceptron (MLP)4Annotator typesXGBoost3External dataset24fastText2Experts5Platform of interestAuthors4Twitter (or X)28Amateurs/Crowdsourced external3annotatorsSexism reported online (from Everyday3Students of linguistic, communication3Sexism Project) (Bates, 2015)and genderFacebook2Machine learning models3Reddit2Social Scientists2Gab2Annotator character not stated2</p>
<p>Table B
BConstruct-"A construct is an abstract concept that is specifically chosen (or 'cre-ated') to explain a given phenomenon. Constructs used for scientificresearch must have precise and clear definitions that others can useto understand exactly what it means and what it does not mean."(Bhattacherjee, 2019)ComputationalCSSComputational social science is an interdisciplinary academic sub-fieldSocial Scienceconcerned with computational approaches to the social sciences. Itleverages the capacity to collect and analyze data with an unprece-dented breadth and depth and scale..1: Citation databases and their respective queriesC Terminologies and their meaning</p>
<p>Title</p>
<p>Fersini et al. (2018) Deep Learning Representations in Automatic Misogyny Identification: What Do We Gain and What Do We Miss?Using Convolutional Neural Networks to Classify Hate-Speech Waseem and Hovy (2016) Hateful Symbols or Hateful People?Predictive Features for Hate Speech Detection on Twitter Sen et al. (2022) Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection Samory et al. (2021) "Call me sexist, but..." : Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples
CitationRahali et al. (2021)Automatic Misogyny Detection in Social Media Platforms using Attention-based Bidirectional-LSTMFrenda et al. (2019)Online Hate Speech against Women: Automatic Identification of Misogyny andSexism on TwitterBhattacharya et al.Developing a Multilingual Annotated Corpus of Misogyny and Aggression(2020a)Waseem (2016)Are You a Racist or Am I Seeing Things? Annotator Influence on Hate SpeechDetection on TwitterGambäck and Sikdar(2017)</p>
<p>Table L
L
.2: Documents used for Meta-analysis (Part-2)</p>
<p>A gender-biased form used to indicate those of both masculine and feminine gender, reinforcing a hierarchy that privileges men.
"The flow diagram depicts the flow of information through the different phases of a systematic review. It maps out the number of records identified, included, and excluded, and the reasons for exclusions. Different templates are available depending on the type of review (new or updated) and sources used to identify studies."(Takkouche and Norman, 2011) 
A preprint is a full draft research paper that is shared publicly before it has been peer-reviewed. Most preprints are given a digital object identifier (DOI) so they can be cited in other research papers. A preprint is a full draft of a research paper that is shared publicly before it has been peer-reviewed.(Mudrak, 2018) 
"Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. It leverages 'unsupervised' machine learning to analyze and identify clusters or groups of similar words within a body of text"(Pykes, 2023).
More information on the techniques used in this methodology is explained in the supplementary section: J.1
KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and key phrases that are most similar to a document.
Word n-gram range lets users decide the length of sequence of consecutive words that should be extracted from a given text.
More on the paradigms(Röttger et al., 2022) in the Supplementary Section C.1, in the table of terminologies.
"Shared tasks are collaborative efforts in which researchers and practitioners come together to solve a common problem using shared data and evaluation measures. They promote competition, collaboration, and progress in research, and have become an important part of many academic and industrial communities" (SIGEDU, 2024).
This repository would be made public upon acceptance for publication.
More details can be found here: https://images.webofknowledge.com/images/help/WOS/hp_advanced_search.html
Source: https://images.webofknowledge.com/images/help/WOS/hp_research_areas_easca.html
FundingWe thank the University of Exeter for funding the cost to access the Web of Science Expanded API and SerpAPI.A.D.'s time on the research was funded by the SSIS Global Excellence PhD Studentship from the University of Exeter.S.B.'s time on the research was funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 101019284).C.Q.C. thanks the Ewha Frontier 10-10 project and the DSO National Laboratories Singapore for funding this research.DeclarationsMaterial and code availabilityNo data was generated during this research, but were acquired from online websites or through the API access of the stated citation databases.All the shareable acquired data from the databases collected and used in the research, along with its analysis is made available in the GitHub page: https://github.com/booktrackerGirlsys-lit-review-Sexism 14.We also include the permissive license to allow users to use, modify and distribute the materials.Competing interestsThe authors have no competing interests to declare that are relevant to the content of this article.4. ("natural language processing" OR NLP OR "deep learning" OR "machine learning" OR ML OR "artificial intelligence" OR AI)A Systematic Literature Review strategy5. 1/ AND 2/ AND 3/ AND 4/ 6. Limit 5 to (english language and yr="2012 -Current")A.2 Inclusion and exclusion criteriaB Citation Database queriesComputer Science Social Science• Computer Science• Artificial Intelligence• Theory and Methods• Engineering• Software Engineering• Scientific Disciplines• Automation and Control Systems• History and Philosophy of Science• Linguistics• Anthropology• Sociology• Social Work• Language and Linguistics• Information Science• Psychology• Social• Ethnic Studies• Philosophy• PsychiatryF ArXiv strategyThe ArXiv API was used following the query search strategy 17 .We performed automated elimination (or pre-processing) techniques based on the following criteria to narrow down our search results for both areas of study:• Remove studies that are not between 2012 and 2022.• Remove studies that do not contain any abstracts.While combining search results of E and F, care was taken to remove the duplicate studies based on the title and abstract, where we kept the study from the former database.This is to ensure consistency along the data since the published and updated (i.e., when the pre-prints were submitted to ArXiv) years could differ, hence ensuring the published works are not mislabeled as pre-prints.Figure G.1 shows the frequency of publications per year in the range of 2012-2022, as per each discipline and publication type.Like we had discussed previously in Section 4.3, we see a huge disparity in the number of publications between the disciplines which focus on sexism and/or misogyny.This inherently appears to impact on the diversity of the concept explored by the disciplines, with SS exploring a broader range of themes than CS.Furthermore, we also see that the type of publication too differs quite a bit as CS tend to produce a handful of research as pre-prints on this topic. 17More details of the search strategy can be found here: https://info.arxiv.org/help/api/user-manual.html#query_detailsG Further analysis of the initial search resultsDocuments by disciplinesI Most frequent keywordsIn this section, we demonstrate the top 100 keywords in the co-occurrence network, like in Section 4.3.2but based on all the manuscripts of each individual field.I.1 Most frequent keywords in Computer ScienceIn the Figure I.1, we have the network diagram visualizing the most frequent and relevant keywords in Computer Science literature related to sexism and gender bias.The top 5 most common keywords are labelled in red boxes.Among the top 100 identified keywords, six emerged as particularly dominant: gender stereotypes, ambivalent sexism, hostile sexism, sexism, benevolent sexism, and sexist attitudes.These keywords are centrally positioned and form dense clusters, indicating their strong interconnections within the literature.The prominence of both psychological constructs (e.g., ambivalent sexism) and sociocultural phenomena (e.g., gender stereotypes) suggests a multidisciplinary engagement with sexism-related topics.The diagram highlights how research in Computer Science often intersects with gender theory, emphasizing the need for nuanced understandings of sexism in digital and computational contexts.transformer 'bge-small-en-v1.5' 18, which maps the each paragraph of our document to a 384 dimensional dense vector space, that was then used to cluster topics of similar semantic structure.In topic modeling, it is key to have a good quality of topic representations to interpret the overall topic and understand patterns in the document, for which we used bag-of-words (BoW) of medium length ngram value (1-3 n-grams).To further enhance the representative-ness of the topics from BoW, Term Frequency-Inverse Document Frequency (TF-IDF) of our document, which works on a documentlevel, were adjusted to c-TF-IDF as per their weights, which works on a cluster/categorical/topic level.It considers the differences in documents from different clusters, and can be calculated as: c-TF-IDF (for a term x within class c)where tfx,c = frequency of word x in class c, fx = frequency of word x across all classes, A = average number of words per class Though both of these approaches did a good job of acquiring the topic representations, we used representation models to fine-tune the topics to refine its representations.For that, we used a combination of three models -a fast keyword extraction model called KeyBERTInspired, PartOfSpeech model, and MaximalMarginalRelevance model.The KeyBERTInspired model increases the coherence and reduces stopwords Alongside this approach, we tried to further refine our topic representation by fine-tuning using a Large Language model (LLM) named 'Mistral 7B v0.1' -a 7 billion parameter language model, which has shown to outperform other state-of-the-art language models like Llama 13B across all elevated benchmarksJiang et al. (2023). 18The huggingface page of the model: https://huggingface.co/BAAI/bge-small-en-v1.5 # LLM model llm = Llama(model_path="../openhermes-2.5-mistral-7b.Q3_K_M.gguf",n_gpu_layers=-1, n_ctx=4096, stop=["Q:", "\n"]) prompt = """ Q: I have a topic that contains the following documents:[DOCUMENTS]The topic is described by the following keywords: '[KEYWORDS]'.Based on the above information, can you give a short label of the topic of at most 5 words?A: """ aspect_model3 = LlamaCPP(llm, prompt=prompt) # Add all models together to be run in a single 'fit' representation_model = { "Main": main_representation, "Aspect1": aspect_model1, "Aspect2": aspect_model2, "Aspect3": aspect_model3 } # The documents to train on are the titles and abstracts of the studies topic_model = BERTopic(representation_model=representation_model).fit(docs)To assess the model performance, the metrics perplexity and coherence scores were calculated as well.Perplexity is a predictive likelihood that specifically measures the probability that new data occurs given what was already learned by the model.In other words, perplexity characterizes how surprised a model is with new, unseen data.Coherence is typically used to analyze the relationship between two sets of data or the similarity between data sets.In topic modeling, topic coherence measures the quality of the data by comparing the semantic similarity between highly repetitive words in a topic.We used this to maximize intra-topic and minimize inter-topic similarity.We attained a perplexity score of 1.23 and a coherence score of 0.35 from our topic model.K Analysis of the Computer Science studies -the final selectionIn this section, we explore the data statistics for the CS manuscripts which were finally selected before the full-text screening process.The visual analysis is purely based on the text contained in abstracts and titles of the selected studies.K.1 Documents by models
Fine-grained multi-label sexism classification using a semi-supervised multi-level neural approach. H Abburi, P Parikh, N Chhaya, V Varma, Data Science and Engineering. 642021</p>
<p>Modern misogyny: Anti-feminism in a post-feminist era. K J Anderson, 2014Oxford University Press</p>
<p>Automatic identification and classification of misogynistic language on twitter. M E Anzovino, E Fersini, P Rosso, International Conference on Applications of Natural Language to Data Bases. 2018</p>
<p>Hate speech detection is not as easy as you may think: A closer look at model validation (extended version). A Arango, J Pérez, B Poblete, Information Systems. 1051015842022</p>
<p>Modern sexism in modern times public opinion in the# metoo era. A M Archer, C D Kam, Public Opinion Quarterly. 8442020</p>
<p>Entropy-based attention regularization frees unintended bias mitigation from lists. G Attanasio, D Nozza, D Hovy, E Baralis, 2022</p>
<p>Politeam @ ami: Improving sentence embedding similarity with misogyny lexicons for automatic misogyny identification in italian tweets. G Attanasio, E Pastor, EVALITA Evaluation of NLP and Speech Tools for Italian. 2020. December 17th, 2020</p>
<p>Deep learning for hate speech detection in tweets. P Badjatiya, S Gupta, M Gupta, V Varma, Proceedings of the 26th International Conference on World Wide Web Companion, WWW '17 Companion. the 26th International Conference on World Wide Web Companion, WWW '17 Companion2017Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee</p>
<p>Sexual harassment on the internet. A Barak, Social Science Computer Review. 2312005</p>
<p>Internet defamation as profit center: The monetization of online harassment. A Bartow, 2009Faculty Publications</p>
<p>Misogynistic tweet detection: Modelling cnn with small datasets. M A Bashar, R Nayak, N Suzor, B Weir, Data Mining: 16th Australasian Conference. Revised Selected Papers. AusDM; Bahrurst, NSW, AustraliaSpringer2019. 2018. November 28-30, 201816</p>
<p>SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. V Basile, C Bosco, E Fersini, D Nozza, V Patti, F M Rangel Pardo, P Rosso, M Sanguinetti, Proceedings of the 13th International Workshop on Semantic Evaluation. J May, E Shutova, A Herbelot, X Zhu, M Apidianaki, S M Mohammad, the 13th International Workshop on Semantic EvaluationMinneapolis, Minnesota, USAAssociation for Computational Linguistics2019</p>
<p>EVALITA 2020: Overview of the 7th evaluation campaign of natural language processing and speech tools for italian. V Basile, D Croce, M D Maro, L C Passaro, V Basile, D Croce, M D Maro, L C Passaro, Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020). CEUR Workshop Proceedings. CEUR-WS.org. the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)2020. December 17th, 20202765Online event</p>
<p>Everyday sexism. L Bates, 2015Schuster UK.</p>
<p>Effects of social conditions on the expression of sex-role stereotypes. M Y Beattie, L A Diehl, Psychology of Women Quarterly. 421979</p>
<p>Developing a multilingual annotated corpus of misogyny and aggression. S Bhattacharya, S Singh, R Kumar, A Bansal, A Bhagat, Y Dawer, B Lahiri, A K Ojha, Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying. the Second Workshop on Trolling, Aggression and CyberbullyingMarseille, France2020aEuropean Language Resources Association (ELRA</p>
<p>Developing a multilingual annotated corpus of misogyny and aggression. S Bhattacharya, S Singh, R Kumar, A Bansal, A Bhagat, Y Dawer, B Lahiri, A K Ojha, Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying. the Second Workshop on Trolling, Aggression and CyberbullyingMarseille, France2020bEuropean Language Resources Association (ELRA</p>
<p>A Bhattacherjee, Social science research: principles, methods and practices. 2019University of South Floridarevised edition</p>
<p>Fast unfolding of communities in large networks. V D Blondel, J.-L Guillaume, R Lambiotte, E Lefebvre, Journal of Statistical Mechanics: Theory and Experiment. 10P100082008. 2008</p>
<p>F Bolanos, A Salatino, F Osborne, E Motta, Artificial intelligence for literature reviews: Opportunities and challenges. 2024</p>
<p>Advanced topic modeling with bertopic. J Briggs, 2023</p>
<p>Sexism identification using bert and data augmentation -exist2021. S Butt, N Ashraf, G Sidorov, A F Gelbukh, IberLEF@SEPLN. 2021</p>
<p>Language and gender: Mainstreaming and the persistence of patriarchy. International journal of the sociology of language. D Cameron, 20202020</p>
<p>Misogyny identification through svm at ibereval. J S Canós, IberEval@SEPLN. 2018. 2018</p>
<p>Smote: synthetic minority over-sampling technique. N V Chawla, K W Bowyer, L O Hall, W P Kegelmeyer, Journal of artificial intelligence research. 162002</p>
<p>be nice to your wife! the restaurants are closed": Can gender stereotype detection improve sexism classification?. P Chiril, F Benamara, V Moriceau, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>An annotated corpus for sexism detection in french tweets. P Chiril, V Moriceau, F Benamara, A Mari, G Origgi, M Coulomb-Gully, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation Conference2020</p>
<p>Hate crimes in cyberspace. D K Citron, 2014Harvard University Press</p>
<p>Encyclopedia of feminist theories. L Code, 2002Routledge</p>
<p>Gender issues. E Daniels, C Leaper, Encyclopedia of Adolescence. B B Brown, M J Prinstein, San DiegoAcademic Press2011</p>
<p>Online sexism detection and classification by injecting user gender information. A Das, M Rahgouy, Z Zhang, T Bhattacharya, G Dozier, C D Seals, 2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings). 2023</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>E Dinan, A Fan, L Wu, J Weston, D Kiela, A Williams, arXiv:2005.00614Multi-Dimensional Gender Bias Classification. arXiv e-prints. 2020</p>
<p>. M Duggan, 2017. 2017</p>
<p>Gender stereotypes and attitudes toward women and men. A H Eagly, A Mladinic, Personality and Social Psychology Bulletin. 1541989</p>
<p>Are people prejudiced against women? some answers from research on attitudes, gender stereotypes, and judgments of competence. A H Eagly, A Mladinic, European Review of Social Psychology. 511994</p>
<p>Exploring misogyny across the manosphere in reddit. T Farrell, M Fernandez, J Novotny, H Alani, Proceedings of the 10th ACM Conference on Web Science, WebSci '19. the 10th ACM Conference on Web Science, WebSci '19New York, NY, USAAssociation for Computing Machinery2019</p>
<p>Overview of the evalita 2018 task on automatic misogyny identification (ami). E Fersini, D Nozza, P Rosso, EVALITA@CLiC-it. 2018</p>
<p>Deep learning representations in automatic misogyny identification: What do we gain and what do we miss?. E Fersini, L Rosato, A Candelieri, F Archetti, E Messina, 2021In CLiC-it</p>
<p>Computer science papers in web of science: A bibliometric analysis. D Fiala, G Tutoky, Publications. 542017</p>
<p>Overview of annotation creation: Processes and tools. Handbook of Linguistic Annotation. M A Finlayson, T Erjavec, 2017</p>
<p>How do we study misogyny in the digital age? a systematic literature review using a computational linguistic approach. L Fontanella, B Chulvi, E Ignazzi, A Sarra, A Tontodimamma, Humanities and Social Sciences Communications. 1112024</p>
<p>Online content moderation: A fundamental rights-based approach. Fra, 2023</p>
<p>Online hate speech against women: Automatic identification of misogyny and sexism on twitter. S Frenda, B Ghanem, M Montes-Y Gómez, P Rosso, Journal of intelligent &amp; fuzzy systems. 3652019</p>
<p>Exploration of misogyny in spanish and english tweets. S Frenda, B Ghanem, M M Gómez, IberEval@SEPLN. 2018</p>
<p>Using convolutional neural networks to classify hate-speech. B Gambäck, U K Sikdar, Z Waseem, W H K Chung, D Hovy, J Tetreault, Proceedings of the First Workshop on Abusive Language Online. the First Workshop on Abusive Language OnlineVancouver, BC, CanadaAssociation for Computational Linguistics2017</p>
<p>Development of the gender role attitudes scale (gras) amongst young spanish people. E García-Cueto, F J Rodríguez-Díaz, C Bringas-Molleda, J López-Cepero, S Paíno-Quesada, L Rodríguez-Franco, International journal of clinical and health psychology. 1512015</p>
<p>Special issue on online misogyny. D Ging, E Siapera, Feminist Media Studies. 1842018</p>
<p>D Ging, E Siapera, Gender Hate Online Understanding the New Anti-Feminism: Understanding the New Anti-Feminism. Springer2019</p>
<p>The ambivalent sexism inventory: Differentiating hostile and benevolent sexism. P Glick, S Fiske, Journal of Personality and Social Psychology. 701996</p>
<p>Hostile and benevolent sexism: Measuring ambivalent sexist attitudes toward women. P Glick, S Fiske, Psychology of Women Quarterly. 2111997</p>
<p>BERT of all trades, master of some. D Gordeev, O Lykova, Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying. the Second Workshop on Trolling, Aggression and CyberbullyingMarseille, FranceEuropean Language Resources Association (ELRA2020</p>
<p>All you need is "love": Evading hate speech detection. T Gröndahl, L Pajola, M Juuti, M Conti, N Asokan, Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security, AISec '18. the 11th ACM Workshop on Artificial Intelligence and Security, AISec '18New York, NY, USAAssociation for Computing Machinery2018</p>
<p>Keybert: Minimal keyword extraction with bert. M Grootendorst, 2020</p>
<p>Bertopic: Neural topic modeling with a class-based tf-idf procedure. M Grootendorst, 2022</p>
<p>Automatic detection of sexist statements commonly used at the workplace. D Grosz, P Conde-Cespedes, 2020</p>
<p>An expert annotated dataset for the detection of online misogyny. E Guest, B Vidgen, A Mittos, N Sastry, G Tyson, H Margetts, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterOnline. Association for Computational Linguistics2021</p>
<p>Bias in social research. M Hammersley, R Gomm, Sociological Research Online. 211997</p>
<p>. A Harzing, 2007Publish or perish</p>
<p>Can you trust your ml metrics? using subjective logic to determine the true contribution of ml metrics for safety. B Herd, S Burton, Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing, SAC '24. the 39th ACM/SIGAPP Symposium on Applied Computing, SAC '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The rhetorical dynamics of gender harassment on-line. S C Herring, 1999The Information Society15</p>
<p>The problem of identifying misogynist language on twitter (and other online social spaces). S Hewitt, T Tiropanis, C Bokhove, Proceedings of the 8th ACM Conference on Web Science, WebSci '16. the 8th ACM Conference on Web Science, WebSci '16New York, NY, USAAssociation for Computing Machinery2016</p>
<p>Julia penelope, speaking freely: Unlearning the lies of the fathers' tongues. new york: Pergamon. S F Hirsch, 1992. 1990. 1990Language in Society281deborah cameron (ed.), the feminist critique of language: A reader</p>
<p>Deceiving google's perspective api built for detecting toxic comments. H Hosseini, S Kannan, B Zhang, R Poovendran, 2017</p>
<p>Five sources of bias in natural language processing. D Hovy, S Prabhumoye, Language and Linguistics Compass. 158e124322021</p>
<p>FERMI at SemEval-2019 task 5: Using sentence embeddings to identify hate speech against immigrants and women in Twitter. V Indurthi, B Syed, M Shrivastava, N Chakravartula, M Gupta, V Varma, Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic EvaluationMinneapolis, Minnesota, USAAssociation for Computational Linguistics2019</p>
<p>Amnesty reveals alarming impact of online abuse against women. A International, 2017</p>
<p>Online misogyny and feminist digilantism. E A Jane, Continuum. 3032016</p>
<p>When does a compliment become sexist? analysis and classification of ambivalent sexism using twitter data. A Jha, R Mamidi, Proceedings of the Second Workshop on NLP and Computational Social Science. the Second Workshop on NLP and Computational Social ScienceVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Swsr: A chinese dataset and lexicon for online sexism detection. A Jiang, X Yang, Y Liu, A Zubiaga, 2021</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Online misogyny: A challenge for digital feminism. O Jurasz, K Barker, Journal of International Affairs. 7222019</p>
<p>Hidden in plain sight for too long: Using text mining techniques to shine a light on workplace sexism and sexual harassment. A Karami, S Swan, C N White, K Ford, 2019Psychology of Violence</p>
<p>Sentiment polarization in online social networks: The flow of hate speech. K Katsarou, S Sunder, V Woloszyn, K Semertzidis, 2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS). 2021</p>
<p>Sex-role egalitarian ism scale: Development, psychometric properties, and recommendations for future research. L A King, D W King, Psychology of Women Quarterly. 2111997</p>
<p>ARGUABLY at ComMA@ICON: Detection of multilingual aggressive, gender biased, and communally charged tweets using ensemble and fine-tuned IndicBERT. G Kohli, P Kaur, J Bedi, Proceedings of the 18th International Conference on Natural Language Processing: Shared Task on Multilingual Gender Biased and Communal Language Identification. the 18th International Conference on Natural Language Processing: Shared Task on Multilingual Gender Biased and Communal Language IdentificationNIT Silchar. NLP Association of India2021</p>
<p>Computational social science: Obstacles and opportunities. D M Lazer, A Pentland, D J Watts, S Aral, S Athey, N Contractor, D Freelon, S Gonzalez-Bailon, G King, H Margetts, Science. 36965072020</p>
<p>A comparison of machine learning approaches for detecting misogynistic speech in urban dictionary. T Lynn, P T Endo, P Rosati, I Silva, G L Santos, D Ging, 2019 International Conference on Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA). 2019</p>
<p>Down Girl: The Logic of Misogyny. K Manne, 2017Oxford University Press</p>
<p>Gendertrolling: Misogyny adapts to new media. K Mantilla, Feminist studies. 3922013</p>
<p>Public response to racist speech: Considering the victim's story. M J Matsuda, Words that wound. Routledge2018</p>
<p>Online incivility or sexual harassment? conceptualising women's experiences in the digital age. J Megarry, Women's Studies International Forum. 472014</p>
<p>Topic modeling of everyday sexism project entries. S Melville, K Eccles, T Yasseri, Frontiers in Digital Humanities. 20195</p>
<p>Abusive language detection with graph convolutional networks. P Mishra, M D Tredici, H Yannakoudakis, E Shutova, 2019</p>
<p>What are preprints, and how do they benefit authors?. B Mudrak, M Nadim, A Fladmoe, Social Science Computer Review. 3922018. 2021Silencing women? gender and online harassment</p>
<p>Unintended bias evaluation: An analysis of hate speech detection and gender bias mitigation on social media using ensemble learning. F R Nascimento, G D Cavalcanti, Da Costa-Abreu, M , Expert Systems with Applications. 2011170322022a</p>
<p>Unintended bias evaluation: An analysis of hate speech detection and gender bias mitigation on social media using ensemble learning. F R Nascimento, G D Cavalcanti, Da Costa-Abreu, M , Expert Systems with Applications. 2011170322022b</p>
<p>Unintended bias in misogyny detection. D Nozza, C Volpetti, E Fersini, IEEE/WIC/ACM International Conference on Web Intelligence, WI '19. New York, NY, USAAssociation for Computing Machinery2019</p>
<p>Ynu oxz@ haspeede 2 and ami: Xlm-roberta with ordered neurons lstm for classification task at evalita 2020. X Ou, H Li, EVALITA Evaluation of NLP and Speech Tools for Italian. 20202765</p>
<p>Multi-label categorization of accounts of sexism using a neural framework. P Parikh, H Abburi, P Badjatiya, R Krishnan, N Chhaya, M Gupta, V Varma, 2019</p>
<p>Categorizing sexism and misogyny through neural approaches. P Parikh, H Abburi, N Chhaya, M Gupta, V Varma, ACM Transactions on the Web (TWEB). 1542021</p>
<p>Sexism identification in social networks using a multi-task learning system. F M Plaza-Del Arco, M D Molina-González, L López, M Martín-Valdivia, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language Processing. the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language ProcessingMálaga, Spain20212943XXXVII International Conference of the Spanish Society for Natural Language Processing</p>
<p>Detecting misogyny and xenophobia in spanish tweets using language technologies. F.-M Plaza-Del-Arco, M D Molina-González, L A Ureña López, M T Martín-Valdivia, ACM Trans. Internet Technol. 2022020</p>
<p>What is topic modeling? an introduction with examples. K Pykes, 2023</p>
<p>Automatic misogyny detection in social media platforms using attention-based bidirectional-lstm. A Rahali, M A Akhloufi, A.-M Therien-Daniel, E Brassard-Gourdeau, 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2021</p>
<p>Woman-hating: On misogyny, sexism, and hate speech. L Richardson-Self, Hypatia. 3322018</p>
<p>Overview of exist 2021: sexism identification in social networks. F Rodríguez-Sánchez, J Carrillo-De Albornoz, L Plaza, J Gonzalo, P Rosso, M Comet, T Donoso, Procesamiento del Lenguaje Natural. esamiento del Lenguaje Natural202167</p>
<p>Automatic classification of sexism in social networks: An empirical study on twitter data. F Rodríguez-Sánchez, J Carrillo-De Albornoz, L Plaza, IEEE Access. 82020</p>
<p>Overview of exist 2022: sexism identification in social networks. F Rodríguez-Sánchez, J Carrillo-De Albornoz, L Plaza, A Mendieta-Aragón, G Marco-Remón, M Makeienko, M Plaza, J Gonzalo, D Spina, P Rosso, Procesamiento de Lenguaje Natural. 692022</p>
<p>Two contrasting data annotation paradigms for subjective nlp tasks. P Röttger, B Vidgen, D Hovy, J B Pierrehumbert, 2022</p>
<p>Revisiting sexism detection using psychological scales and adversarial samples. M Samory, I Sen, J Kohne, F Flöck, C Wagner, but..Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social Media202115call me sexist</p>
<p>The risk of racial bias in hate speech detection. M Sap, D Card, S Gabriel, Y Choi, N A Smith, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Men are hierarchical, women are egalitarian: An implicit gender stereotype. M Schmid Mast, Swiss Journal of Psychology. 6322004</p>
<p>Automatic sexism detection with multilingual transformer models. M Schütz, J Boeck, D Liakhovets, D Slijepčević, A Kirchknopf, M Hecht, J Bogensperger, S Schlarb, A Schindler, M Zeppelzauer, 2022</p>
<p>Intensification of efforts to eliminate all forms of violence against women and girls: technology-facilitated violence against women and girls: report of the secretary-general. U Secretary-General, 2024</p>
<p>Counterfactually augmented data and unintended bias: The case of sexism and hate speech detection. I Sen, M Samory, C Wagner, I Augenstein, Proceedings of the 2022 Conference of the North American Chapter. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Gender biases and linguistic sexism in political communication: A comparison of press news about men and women italian ministers. G Sensales, A Areni, Journal of Social and Political Psychology. 522017</p>
<p>. Serpapi, 2019</p>
<p>Predictive biases in natural language processing models: A conceptual framework and overview. D S Shah, H A Schwartz, D Hovy, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Boosting text classification performance on sexist tweets by text augmentation and text generation using a combination of knowledge graphs. S Sharifirad, B Jafarpour, S Matwin, Proceedings of the 2nd workshop on abusive language online (ALW2). the 2nd workshop on abusive language online (ALW2)2018</p>
<p>Analysing threads of sexism in new age humour: A content analysis of internet memes. N Siddiqi, A Bains, S Aleem, S Aleem, Indian journal of social research. 593562018</p>
<p>hold on honey, men at work": A semi-supervised approach to detecting sexism in sitcoms. S Singh, T Anand, A Ghosh Chowdhury, Z Waseem, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research WorkshopOnline. Association for Computational Linguistics2021</p>
<p>Misogyny, feminism, and sexual harassment. K Srivastava, S Chaudhury, P Bhat, S Sahu, Industrial Psychiatry Journal. 261112017</p>
<p>C Sutton, L Gong, arXiv:1710.05225Popularity of arxiv. org within computer science. 2017arXiv preprint</p>
<p>Understanding subtle sexism: Detection and use of sexist language. J Swim, R Mallett, C Stangor, Sex Roles. 512004</p>
<p>Overt, covert, and subtle sexism: A comparison between the attitudes toward women and modern sexism scales. J K Swim, L L Cohen, Psychology of Women Quarterly. 2111997</p>
<p>B Takkouche, G Norman, Prisma statement. 201122128</p>
<p>System description for exist shared task at iberlef 2021: Automatic misogyny identification using pretrained transformers. I Talavera, D C Fidalgo, D Vila-Suero, IberLEF@ SEPLN. 2021</p>
<p>Neosexism: Plus ça change, plus c'est pareil. F Tougas, R Brown, A M Beaton, S Joly, Personality and social psychology bulletin. 2181995</p>
<p>Directions in abusive language training data, a systematic review: Garbage in, garbage out. B Vidgen, L Derczynski, Plos one. 1512e02433002020</p>
<p>Identifying women's experiences with and strategies for mitigating negative effects of online harassment. J Vitak, K Chadha, L Steiner, Z Ashktorab, Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing2017</p>
<p>Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter. Z Waseem, Proceedings of the First Workshop on NLP and Computational Social Science. the First Workshop on NLP and Computational Social ScienceAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Hateful symbols or hateful people? predictive features for hate speech detection on Twitter. Z Waseem, D Hovy, Proceedings of the NAACL Student Research Workshop. the NAACL Student Research WorkshopSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>Violence against women. 2013WHO</p>
<p>Hate speech on social media: Content moderation in context. R A Wilson, M K Land, Conn. L. Rev. 5210292020</p>
<p>Feminist theory and the problem of misogyny. S P Wrisley, Feminist Theory. 2422023</p>
<p>T Yasseri, K Eccles, S Melville, Sexism typology: Literature review. 2016</p>
<p>Annotating online misogyny. P Zeinert, N Inie, L Derczynski, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>The impact of data source on the ranking of computer scientists based on citation indicators: a comparison of web of science and scopus. L Zhang, Issues in Science and Technology Librarianship. 2014</p>
<p>Gender bias in coreference resolution: Evaluation and debiasing methods. J Zhao, T Wang, M Yatskar, V Ordonez, K.-W Chang, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>            </div>
        </div>

    </div>
</body>
</html>
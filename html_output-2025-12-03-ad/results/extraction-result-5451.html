<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-9004924824cc0b1c840bcc91ba79475882623790</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9004924824cc0b1c840bcc91ba79475882623790" target="_blank">Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is argued that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates.</p>
                <p><strong>Paper Abstract:</strong> Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 ToM - Unexpected Contents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 evaluated on the Unexpected-Contents (Smarties) Theory-of-Mind task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This entry summarizes experiments in which the most recently available iteration of GPT-3.5 was prompted with classic 'unexpected contents' (smarties/crayon-box style) Theory-of-Mind vignettes and several small, principled variations; the model's completion probabilities for competing beliefs (e.g., 'popcorn' vs 'chocolate') were recorded and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as 'the most recently available iteration of GPT-3.5' used for evaluation; no architecture or training-data specifics are provided in this paper beyond referencing prior LLM work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Unexpected-Contents (Smarties) Task</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory-of-Mind / social-cognitive belief attribution</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A classic false-belief task in which an agent sees (or is presented with) an item whose superficial label suggests one content but the true content is different; the test probes the model's attribution of beliefs to an agent who lacks access to the true contents (e.g., whether the agent 'believes' the container holds what the label says or what the container actually contains). In the paper this is administered as short textual vignettes followed by belief or content prompts; model completion probabilities for alternative answers are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Baseline (as reported by Kosinski and referenced here): for a straightforward content prompt GPT-3.5 produced P(popcorn)=100%; for a first belief prompt it produced P(popcorn)=0%, P(chocolate)=99%; for a second belief-like prompt it produced P(popcorn)=14%, P(chocolate)=82% (Kosinski (1) results cited). In the present paper, small principled variations produced large changes and systematic failures: Variation 1A (transparent bag): belief prompt => P(popcorn)=0%, P(chocolate)=95%; secondary belief prompt => P(popcorn)=58%, P(chocolate)=36%. Variation 1B (agent cannot read): belief prompt => P(popcorn)=0%, P(chocolate)=98%; second prompt => P(popcorn)=15%, P(chocolate)=78%. Variation 1C (trusted testimony that bag contains popcorn and agent believes friend): belief prompt => P(popcorn)=2%, P(chocolate)=97%; second prompt => P(popcorn)=13%, P(chocolate)=81%. Variation 1D (agent herself filled bag and wrote misleading label): belief prompt => P(popcorn)=10%, P(chocolate)=87%; second prompt => P(popcorn)=35%, P(chocolate)=63%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not quantitatively reported in this paper. The paper cites Kosinski's claim that models performed at levels comparable to 9-year-old children on the original vignettes; classical developmental literature (referenced generally) indicates that typically developing children reliably pass simple false-belief tasks by around 4-5 years and adults reliably attribute the correct belief in these scenarios. No specific percent-correct human baseline numbers are reported in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On the original (non-perturbed) vignettes (per Kosinski) GPT-3.5 produced responses interpreted as matching expected correct belief attributions; however, this paper shows GPT-3.5 is not robust: small, ToM-preserving variations flip model responses such that the model often attributes the incorrect belief (favoring the label or other superficial cue). The LLM therefore appears to match human-like responses on narrow prompts but underperforms humans on robustness to trivial, logically-irrelevant perturbations; no statistical tests or p-values are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Major limitation is lack of robustness: GPT-3.5 systematically overweighted the presence/read-out of the written label even when perceptual access, illiteracy, trusted testimony, or the agent's own actions made the label irrelevant. Failures indicate sensitivity to surface cues and prompt formulation rather than stable belief-representation; the paper also notes possible conflation with failures of scene/relational understanding or prompt-sensitivity. Human baseline demographics or explicit comparative statistics are not provided, so direct quantitative comparison is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 ToM - Unexpected Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 evaluated on the Unexpected-Transfer (Sally-Anne style) Theory-of-Mind task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This entry summarizes experiments where GPT-3.5 was given classic 'unexpected transfer' vignettes (an object moved while an agent is absent) and principled variations (transparent containers, 'on' vs 'in', trusted communication, querying the mover) with model completion probabilities for predicted search locations recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as 'the most recently available iteration of GPT-3.5' used for evaluation; the paper does not provide architecture, parameter count, or training-data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Unexpected-Transfer (Sally-Anne) Task</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory-of-Mind / belief attribution and action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A classic false-belief task where one agent places an object in location A and leaves; another agent moves it to location B; the test probes where the first agent will search upon return. Here administered as text vignettes and two types of prompts ('Agent thinks that the X is in the ___' and 'When Agent comes back he/she will look for the X in the ___'), recording model probabilities for candidate locations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Original (non-perturbed) vignette (per Kosinski and referenced here): GPT-3.5 gave P(basket)=98% for both 'thinks' and 'will look' prompts. In the present paper, several small variations produced robust failures: Variation 2A (transparent containers): 'John thinks the cat is in the chest' => P(box)=0%, P(chest)=94%; 'John will look in the chest' => P(box)=2%, P(chest)=90%. Variation 2B ('on' instead of 'in'): 'John thinks cat is on the basket' => P(box)=0%, P(basket)=97%; 'John will look on the basket' => P(box)=25%, P(basket)=74%. Variation 2C (trusted communication that cat will be moved): 'John thinks the cat is in the basket' => P(box)=0%, P(basket)=97%; 'John will look in the basket' => P(box)=3%, P(basket)=94%. Variation 2D (query about the mover Mark who actually moved the cat): 'Mark thinks the cat is in the basket' => P(box)=1%, P(basket)=99%; 'Mark will look for the cat in the basket' => P(box)=43%, P(basket)=54% (note: Mark actually put the cat in the box, so correct answer would be box).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not quantitatively reported in this paper. Classic developmental research shows typically developing older children and adults reliably answer these false-belief questions correctly; Kosinski's cited work argued LLM performance on the base vignette resembled performance of ~9-year-old children, but explicit percent-correct human baselines are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On the canonical (non-perturbed) transfer vignette GPT-3.5 produced the expected (human-like) answer with high probability (per prior report), but across multiple small variations the model often gave incorrect predictions. Relative to humans, the LLM's principal weakness is lack of robustness to trivial variations that preserve the logical structure of the false-belief problem. No statistical significance tests comparing model and human performance are reported in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Failures included sensitivity to perceptual-access wording (transparent vs opaque), relational phrasing ('on' vs 'in'), and to which agent is being queried (the mover vs the original placer). The model sometimes predicted the same search location for both agents even when only one agent had acted, suggesting pattern overfitting (e.g., learned association to answer the original location) rather than genuine representation of agents' differing knowledge states. The paper notes these errors could reflect deficits in ToM, scene understanding, relational reasoning, or prompt-sensitivity; lack of explicit human baseline statistics limits quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of Mind May Have Spontaneously Emerged in Large Language Models <em>(Rating: 2)</em></li>
                <li>Neural theory-of-mind? on the limits of social intelligence in large lms <em>(Rating: 2)</em></li>
                <li>Evaluating theory of mind in question answering <em>(Rating: 2)</em></li>
                <li>Machine theory of mind <em>(Rating: 2)</em></li>
                <li>Testing relational understanding in text-guided image generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5451",
    "paper_id": "paper-9004924824cc0b1c840bcc91ba79475882623790",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 ToM - Unexpected Contents",
            "name_full": "GPT-3.5 evaluated on the Unexpected-Contents (Smarties) Theory-of-Mind task",
            "brief_description": "This entry summarizes experiments in which the most recently available iteration of GPT-3.5 was prompted with classic 'unexpected contents' (smarties/crayon-box style) Theory-of-Mind vignettes and several small, principled variations; the model's completion probabilities for competing beliefs (e.g., 'popcorn' vs 'chocolate') were recorded and analyzed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Described in the paper as 'the most recently available iteration of GPT-3.5' used for evaluation; no architecture or training-data specifics are provided in this paper beyond referencing prior LLM work.",
            "model_size": null,
            "cognitive_test_name": "Unexpected-Contents (Smarties) Task",
            "cognitive_test_type": "Theory-of-Mind / social-cognitive belief attribution",
            "cognitive_test_description": "A classic false-belief task in which an agent sees (or is presented with) an item whose superficial label suggests one content but the true content is different; the test probes the model's attribution of beliefs to an agent who lacks access to the true contents (e.g., whether the agent 'believes' the container holds what the label says or what the container actually contains). In the paper this is administered as short textual vignettes followed by belief or content prompts; model completion probabilities for alternative answers are reported.",
            "llm_performance": "Baseline (as reported by Kosinski and referenced here): for a straightforward content prompt GPT-3.5 produced P(popcorn)=100%; for a first belief prompt it produced P(popcorn)=0%, P(chocolate)=99%; for a second belief-like prompt it produced P(popcorn)=14%, P(chocolate)=82% (Kosinski (1) results cited). In the present paper, small principled variations produced large changes and systematic failures: Variation 1A (transparent bag): belief prompt =&gt; P(popcorn)=0%, P(chocolate)=95%; secondary belief prompt =&gt; P(popcorn)=58%, P(chocolate)=36%. Variation 1B (agent cannot read): belief prompt =&gt; P(popcorn)=0%, P(chocolate)=98%; second prompt =&gt; P(popcorn)=15%, P(chocolate)=78%. Variation 1C (trusted testimony that bag contains popcorn and agent believes friend): belief prompt =&gt; P(popcorn)=2%, P(chocolate)=97%; second prompt =&gt; P(popcorn)=13%, P(chocolate)=81%. Variation 1D (agent herself filled bag and wrote misleading label): belief prompt =&gt; P(popcorn)=10%, P(chocolate)=87%; second prompt =&gt; P(popcorn)=35%, P(chocolate)=63%.",
            "human_baseline_performance": "Not quantitatively reported in this paper. The paper cites Kosinski's claim that models performed at levels comparable to 9-year-old children on the original vignettes; classical developmental literature (referenced generally) indicates that typically developing children reliably pass simple false-belief tasks by around 4-5 years and adults reliably attribute the correct belief in these scenarios. No specific percent-correct human baseline numbers are reported in this manuscript.",
            "performance_comparison": "On the original (non-perturbed) vignettes (per Kosinski) GPT-3.5 produced responses interpreted as matching expected correct belief attributions; however, this paper shows GPT-3.5 is not robust: small, ToM-preserving variations flip model responses such that the model often attributes the incorrect belief (favoring the label or other superficial cue). The LLM therefore appears to match human-like responses on narrow prompts but underperforms humans on robustness to trivial, logically-irrelevant perturbations; no statistical tests or p-values are reported.",
            "notable_differences_or_limitations": "Major limitation is lack of robustness: GPT-3.5 systematically overweighted the presence/read-out of the written label even when perceptual access, illiteracy, trusted testimony, or the agent's own actions made the label irrelevant. Failures indicate sensitivity to surface cues and prompt formulation rather than stable belief-representation; the paper also notes possible conflation with failures of scene/relational understanding or prompt-sensitivity. Human baseline demographics or explicit comparative statistics are not provided, so direct quantitative comparison is limited.",
            "uuid": "e5451.0",
            "source_info": {
                "paper_title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3.5 ToM - Unexpected Transfer",
            "name_full": "GPT-3.5 evaluated on the Unexpected-Transfer (Sally-Anne style) Theory-of-Mind task",
            "brief_description": "This entry summarizes experiments where GPT-3.5 was given classic 'unexpected transfer' vignettes (an object moved while an agent is absent) and principled variations (transparent containers, 'on' vs 'in', trusted communication, querying the mover) with model completion probabilities for predicted search locations recorded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Described in the paper as 'the most recently available iteration of GPT-3.5' used for evaluation; the paper does not provide architecture, parameter count, or training-data specifics.",
            "model_size": null,
            "cognitive_test_name": "Unexpected-Transfer (Sally-Anne) Task",
            "cognitive_test_type": "Theory-of-Mind / belief attribution and action prediction",
            "cognitive_test_description": "A classic false-belief task where one agent places an object in location A and leaves; another agent moves it to location B; the test probes where the first agent will search upon return. Here administered as text vignettes and two types of prompts ('Agent thinks that the X is in the ___' and 'When Agent comes back he/she will look for the X in the ___'), recording model probabilities for candidate locations.",
            "llm_performance": "Original (non-perturbed) vignette (per Kosinski and referenced here): GPT-3.5 gave P(basket)=98% for both 'thinks' and 'will look' prompts. In the present paper, several small variations produced robust failures: Variation 2A (transparent containers): 'John thinks the cat is in the chest' =&gt; P(box)=0%, P(chest)=94%; 'John will look in the chest' =&gt; P(box)=2%, P(chest)=90%. Variation 2B ('on' instead of 'in'): 'John thinks cat is on the basket' =&gt; P(box)=0%, P(basket)=97%; 'John will look on the basket' =&gt; P(box)=25%, P(basket)=74%. Variation 2C (trusted communication that cat will be moved): 'John thinks the cat is in the basket' =&gt; P(box)=0%, P(basket)=97%; 'John will look in the basket' =&gt; P(box)=3%, P(basket)=94%. Variation 2D (query about the mover Mark who actually moved the cat): 'Mark thinks the cat is in the basket' =&gt; P(box)=1%, P(basket)=99%; 'Mark will look for the cat in the basket' =&gt; P(box)=43%, P(basket)=54% (note: Mark actually put the cat in the box, so correct answer would be box).",
            "human_baseline_performance": "Not quantitatively reported in this paper. Classic developmental research shows typically developing older children and adults reliably answer these false-belief questions correctly; Kosinski's cited work argued LLM performance on the base vignette resembled performance of ~9-year-old children, but explicit percent-correct human baselines are not provided here.",
            "performance_comparison": "On the canonical (non-perturbed) transfer vignette GPT-3.5 produced the expected (human-like) answer with high probability (per prior report), but across multiple small variations the model often gave incorrect predictions. Relative to humans, the LLM's principal weakness is lack of robustness to trivial variations that preserve the logical structure of the false-belief problem. No statistical significance tests comparing model and human performance are reported in this manuscript.",
            "notable_differences_or_limitations": "Failures included sensitivity to perceptual-access wording (transparent vs opaque), relational phrasing ('on' vs 'in'), and to which agent is being queried (the mover vs the original placer). The model sometimes predicted the same search location for both agents even when only one agent had acted, suggesting pattern overfitting (e.g., learned association to answer the original location) rather than genuine representation of agents' differing knowledge states. The paper notes these errors could reflect deficits in ToM, scene understanding, relational reasoning, or prompt-sensitivity; lack of explicit human baseline statistics limits quantitative comparison.",
            "uuid": "e5451.1",
            "source_info": {
                "paper_title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
            "rating": 2
        },
        {
            "paper_title": "Evaluating theory of mind in question answering",
            "rating": 2
        },
        {
            "paper_title": "Machine theory of mind",
            "rating": 2
        },
        {
            "paper_title": "Testing relational understanding in text-guided image generation",
            "rating": 1
        }
    ],
    "cost": 0.009680999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks</h1>
<p>Tomer D. Ullman<br>Department of Psychology<br>Harvard University<br>Cambridge, MA, 02138<br>tullman@fas.harvard.edu</p>
<h4>Abstract</h4>
<p>Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case (1), and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.</p>
<h2>1 Introduction</h2>
<p>People think other people think. They expect other persons to have mental states. They attribute goals to other people, and expect them to pursue those goals efficiently and in a socially-aware manner $(2 ; 3)$. Like other core domains of reasoning, intuitive psychology is early developing or possibly innate, fast, automatic, and culturally universal (4). It is also likely shared with other animals (5; 6). At various points in development, children show increasingly sophisticated reasoning about the mental states of others, including the ability to attribute beliefs and false beliefs to others, second-order reasoning about mental states, and reasoning about perceptual access. While there are long-standing arguments about the exact nature, format, development, and assessment of this reasoning (see e.g 7), a convenient short-hand has been to refer to the adult-level ability to reason about the mental states of others as 'Theory-of-Mind'.
The arguments about its development and content aside, Theory-of-Mind is recognized as a pillar of common-sense reasoning. As such, it would be useful to incorporate this reasoning into machine reasoning, either as a built-in set of primitives or as a target for learning (8). Such as ability is likely useful on its own, but even if future intelligent machines themselves won't have mental states in the same way that people do, some of them will need to interact with people. So, to the degree that people have Theory-of-Mind, it would be useful for machines to have an understanding of this reasoning.
The assessment of Theory-of-Mind in children and adults is an ongoing endeavor, but tests of Theory-of-Mind are also increasingly being applied to machines. Such tests include the porting over of visual intuitive-psychology tasks that were primarily developed for infants ( $9 ; 10 ; 11$ ), as well as the use of question-answering and text-based tasks that mimic the tests used with older children (e.g. $12 ; 13 ; 14)$</p>
<p>The recent rise of Large-Language models (15; 16; 17) have made text-based tests of Theory-of-Mind particularly interesting. These models have already shown some successes across many challenging benchmarks and tasks designed to test various aspects of reasoning (18; 19). While there are many cautionary voices that suggest such models may be acquiring formal rather than functional abilities (18), that has not stopped people from testing them on functional abilities as well, including Theory-of-Mind reasoning.
While some of these tests offer a pessimistic evaluation (14), recent work by Kosinski (1) applied variations on classic Theory-of-Mind tasks to several LLMs, and concluded that current models (as of this writing) can succeed on these tasks at a level comparable to 9-year-old children.
In the face of these results, Kosinski puts the dilemma nicely. Paraphrasing a bit, we have to either (i) accept the validity of the standard measures for ToM, in which case we should concede that LLMs now have ToM, or (ii) reject the suggestion that LLMs have ToM, but then need to seriously re-examine and possibly scuttle the measures developed to examine it. Kosinski himself holds position (i).
In this paper we do two things: First, we examine the robustness of the findings reported in (1), using directed perturbations of the tasks considered. We show that the original reported successes are susceptible to small perturbations that shouldn't matter to an entity that has ToM. Second, we take on the horns of the dilemma and argue that it does not hold. One can accept the validity and usefulness of ToM measures for humans while still arguing that a machine that passes them is suspect. This argument is developed in the discussion, but briefly: Jumping over the horns of the dilemma is possible if reasoning about the mental states of others takes into account the algorithms others are likely implementing, beyond the confines of the input and output of a given task.
Examining the robustness of any one particular LLM system is akin to a mythical Greek punishment ${ }^{1}$. A system is claimed to exhibit behavior X , and by the time an assessment shows it does not exhibit behavior X , a new system comes along and it is claimed it shows behavior X .
Still, we hope this paper will have useful contributions beyond the current moment, as the argument for skepticism and the issues surrounding the assessment of Theory-of-Mind in machine minds are likely to be with us for a while. Besides, there's nothing wrong with contributions to the current moment.
Below, we examine several variations on ToM tasks. The variations take the specific examples in (1) and alter them in ways that do not violate the basic principles of Theory-of-Mind, yet lead to machine failures. The variations may be considered outliers, and so one runs the risk of rejecting them for being outliers. If one end of the scales has 20 successes and the other end a single failure, shouldn't the scales tip in favor of the machine getting a passing grade? We think not. Suppose someone claims a machine has 'learned to multiply', but others suspect that the machine may have memorized question/answer pairs rather than learning a multiplying algorithm. Suppose further that the machine correctly answers 100 questions like ' $5 * 5=25$ ', ' $3 * 7=21$ ', but then it is shown that it completely fails on ' $213 * 261$ '. In this case, we shouldn't simply average these outcomes together and declare $&gt;99 \%$ success on multiplication. The failure is instructive, and suggests the machine has not learned a general multiplication algorithm, as such an algorithm should be robust to simple alterations of the inputs.</p>
<h1>2 Examining the robustness of current LLMs on ToM tasks</h1>
<p>In this section we consider the particular vignettes and prompts in (1), which were used to argue that current LLMs have developed Theory-of-Mind. We focus in particular on the most recently available iteration of GPT-3.5 which was used in (1), as this model achieved the best results, and serves as a threshold. If this model fails, we expect the less powerful models to fail as well. We use the exact same set-up as in (1), posing vignettes to an LLM and examining the probabilities of different completions.
While our concluding assessment is ultimately that GPT-3.5 is not responding robustly to ToM tasks, we emphasize that this is not a negative evaluation of (1). Science advances by pitting hypotheses</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>against one another, and we treat the previous work as a good serve in an ongoing scientific tennis game. We are also grateful that the materials and methods in (1) are publicly available, allowing an easier examination and comparison.</p>
<h1>2.1 Unexpected Contents</h1>
<p>Also known as the 'smarties task', this assessment of ToM shows a participant a container (e.g. a crayon box). The container typically contains some item (e.g. crayons), but in this particular case it contains an unexpected item (e.g., smarties). The participant is then asked about the likely beliefs of another person who has not seen the contents of the box. The participant must hold in mind their own knowledge of the true state of the world (the crayon box has smarties in it), while reasoning about the beliefs of another person (they will believe the box has crayons in it). This is only a brief exposition of the task, and those interested are encouraged to read the many follow-ups to (20).
The studies in (1) begin with a version of the unexpected-contents task, which goes as follows:
Here is a bag filled with popcorn. There is no chocolate in the bag. Yet, the label on the bag says "chocolate" and not "popcorn." Sam finds the bag. She had never seen the bag before. She cannot see what is inside the bag. She reads the label.</p>
<p>The previous study used a content prompt - 'She opens the bag and looks inside. She can clearly see that it is full of' - and two belief prompts. The first belief prompt is: 'She believes that the bag is full of', and the second belief prompt is 'She is delighted that she has found this bag. She loves eating'. Kosinski found that for the content prompt, $[\mathrm{P}($ popcorn $)=100 \% ; \mathrm{P}($ chocolate $)=0 \%]$. For the first belief prompt, $[\mathrm{P}($ popcorn $)=0 \% ; \mathrm{P}($ chocolate $)=99 \%]$. For the second belief prompt, $[\mathrm{P}($ popcorn $)=$ $14 \% ; \mathrm{P}($ chocolate $)=82 \%]$.</p>
<p>We consider several variations on the vignette above. The variations are based on commonsense principles of Theory-of-Mind already available to children, and should lead Sam to believe the bag contains popcorn (or at least, not to believe it contains chocolate). For ease of reading, we summarize the variants in Figure 1, though we emphasize that the images themselves were not evaluated in any way, and are simply visual shorthand for the text below.</p>
<h3>2.1.1 Variation 1A: Transparent Access</h3>
<p>The base-level smarties task already relies on the lack of perceptual access (the container being opaque), and other work shows young children understand perceptual access both in cases when the container is opaque, and in cases when the containers are open and the viewer does have perceptual access (22). So, we make the opaque container transparent.
Here and elsewhere, when detailing a vignette, we note the changes to the original vignette in bold:
Here is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so you can see what is inside. Yet, the label on the bag says 'chocolate' and not 'popcorn.' Sam finds the bag. She had never seen the bag before. Sam reads the label.</p>
<p>On the face of it, reading the label is redundant, as Sam can see the contents of the bag. Yet now we find:</p>
<p>She believes that the bag is full of chocolate, $\left[P_{\text {popcorn }}=0 \%; P_{\text {chocolate }}=95 \%\right]$
However, for the second belief prompt, we do not find this flip:
She is delighted to have found this bag. She loves eating popcorn, $\left[P_{\text {popcorn }}=\right.$ $\left.58 \% ; P_{\text {chocolate }}=36 \%\right]$</p>
<p>Please note that in a previous version of this paper we mistakenly reported that there was a flip in the second belief prompt as well. As far as we can tell, this is due to our original prompt including a double space rather than a single space right before 'Sam finds the bag'. On the latest public available version of GPT-3.5, this double space causes the completion to indeed be chocolate, $P_{\text {chocolate }}=53 \%, P_{\text {chocolate }}=39 \%$.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustrative sketch summarizing the 4 variations used on the 'unexpected contents' task which GPT-3.5 passed. All variations cause the LLM to incorrectly attribute to Sam the belief that the bag contains chocolates. Variation 1A states the bag is transparent and its contents can be seen; 1B states that Sam cannot read, rendering the label meaningless; 1C mentions that before going into the room a trusted friend which Sam believes told her about the contents of the bag and that she should ignore the label; 1D stipulates that Sam herself filled the bag with popcorn, and wrote the label which states it has chocolate inside. Images are shorthand for the full text and were not themselves evaluated. Images were generated using Dall-E 2 (21).</p>
<h1>2.1.2 Variation 1B: Uninformative Label</h1>
<p>What if the label is not useful? For example, the label might be facing away from the person, or written in a language they don't understand, etc. In such a case, it does not matter that the label says 'chocolate' on it. And yet, for GPT-3.5, it does.</p>
<p>Consider for example:
Here is a bag filled with popcorn. There is no chocolate in the bag. Yet, the label on the bag says "chocolate" and not "popcorn." Sam finds the bag. She had never seen the bag before. She cannot see what is inside the bag. Sam cannot read. Sam looks at the label.</p>
<p>We find:</p>
<p>She believes that the bag is full of chocolate, $\left[P_{\text {popcorn }}=0 \%; P_{\text {chocolate }}=98 \%\right]$
For the second belief prompt:
She is delighted to have found this bag. She loves eating chocolate, $\left[P_{\text {popcorn }}=\right.$ $\left.15 \% ; P_{\text {chocolate }}=78 \%\right]$</p>
<p>If Sam cannot read, the label is meaningless to her, and yet GPT-3.5 states that Sam believes the bag has chocolate in it.</p>
<h1>2.1.3 Variation 1C: Trustworthy Testimony</h1>
<p>Direct perception is not the only way to form beliefs, and people often form beliefs about states of the world through indirect instruction, direct pedagogy, and testimony. Even young children are sensitive to whether a person is trustworthy $(23 ; 24)$, or a good teacher $(25 ; 26)$
Suppose a good friend of Sam's told her about the tricky bag. Suppose that we explicitly state Sam believes their friend. Consider this prompt:</p>
<p>Here is a bag filled with popcorn. There is no chocolate in the bag. The label on the bag says "chocolate", rather than "popcorn." Before coming into the room, Sam's friend told her 'the bag in the room has popcorn in it, ignore the label'. Sam believes her friend. Sam finds the bag. She had never seen the bag before. She cannot see what is inside the bag. Sam reads the label, which says the bag has chocolate in it.</p>
<p>We find:
She believes that the bag is full of chocolate, $\left[P_{\text {popcorn }}=2 \% ; P_{\text {chocolate }}=97 \%\right]$
For the second belief prompt:
She is delighted to have found this bag. She loves eating chocolate, $\left[P_{\text {popcorn }}=\right.$ $\left.13 \% ; P_{\text {chocolate }}=81 \%\right]$
One could spin stories about how GPT-3.5 perhaps 'thinks' that Sam perhaps changed her mind and no longer believes her friend, or forgot what her friend said. But a simpler explanation is that LLM reasoning about ToM is sensitive to small irrelevant perturbations.</p>
<h3>2.1.4 Variation 1D: The Treachery of Late Labels</h3>
<p>We do not mean to exhaustively detail all the cases we tried, but instead collapse a few different ones to make a general note that across different cases there was a strong effect for when the person read the label: if the person read the label at the end of the story, then this strongly affected the LLMs answer to the belief prompt.
To drive this point home, consider an extreme case. Suppose Sam herself is the one that filled the bag with popcorn. Suppose Sam wrote the 'chocolate' label herself. Surely she doesn't think the bag has chocolate inside?
More specifically, consider the vignette (we do not bold all the changes in this case, as there are many small ones that add up):</p>
<p>Sam fills a bag with popcorn and closes it. There is no chocolate in the bag. Sam writes a label and puts it on the bag. Sam looks at the bag. She cannot see what is inside the bag. Sam reads the label. The label says the bag has chocolate in it.</p>
<p>We find:
She believes that the bag is full of chocolate, $\left[P_{\text {popcorn }}=10 \% ; P_{\text {chocolate }}=87 \%\right]$
She is delighted to have found this bag. She loves eating chocolate, $\left[P_{\text {popcorn }}=\right.$ $\left.35 \% ; P_{\text {chocolate }}=63 \%\right]$</p>
<h3>2.2 Unexpected Transfer</h3>
<p>In another classic ToM task, a participant sees or is told of a person who observes a particular state of affairs. The state of affairs then changes, without the person being aware. The participant is then asked</p>
<p>what action the person will take. The participant needs to keep in mind both the actual, changed state of affairs and the incorrect belief of the naive person. In the classic Sally-Anne version of the task, Sally hides a marble in a basket. Anne then moves the marble to a box, without Sally's knowledge. A participant is then asked where Sally will look for her marble. Again, this is a bare-bones description of a task that has seen many variants and analyses over the years, and those interested are encouraged to read the myriad follow-ups to (27).
Study 2 in (1) uses the following version of the unexpected transfer task:
In the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn't know what happened in the room when he was away.</p>
<p>The study then examines a content prompt, and two belief prompts: 'John thinks that the cat is in the', and 'When John comes back home, he will look for the cat in the'. For both of these prompts, GPT-3.5 shows $P($ basket $)=98 \%$. On the basis of this it is proposed in (1) that GPT-3.5 is correctly inferring John's mental states.</p>
<h1>2.2.1 Variation 2A: Transparent Access</h1>
<p>Similar to Variation 1A, we can give the people in the room direct perceptual access to the contents of the containers. We change the basket to a 'glass chest' and the box to 'a transparent plastic box':</p>
<p>In the room there are John, Mark, a cat, a transparent plastic box, and a glass chest. John takes the cat and puts it in the chest. He leaves the room and goes to school. While John is away, Mark takes the cat out of the chest and puts it in the box. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn't know what happened in the room when he was away.</p>
<p>We find that:
John thinks that the cat is in the chest. $\left[P_{\text {box }}=0 \%; P_{\text {chest }}=94 \%\right]$
John will look for the cat in the chest. $\left[P_{\text {box }}=2 \%; P_{\text {chest }}=90 \%\right]$
These errors persisted even when stipulating John carefully looks around the room. Another variation leading to error included using opaque containers but mentioning the cat's tail is sticking out of the box (and again with John looking carefully around the room).</p>
<h3>2.2.2 Variation 2B: Relationship Change</h3>
<p>Similar to the variation above, we can change the scenario so that John has direct perceptual access to the situation on entering the room. But rather than changing the opaque container, we change the relationship of the cat to the containers. In this case, we simply changed 'in' to 'on'.
Consider the prompt:
In the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it on the basket. He leaves the room and goes to school. While John is away, Mark takes the cat off the basket and puts it on the box. Mark leaves the room and goes to work. John comes back from school and enters the room. John looks around the room. He doesn't know what happened in the room when he was away.</p>
<p>We find (note the prompts vary a bit to conform with the use of 'on' instead of 'in'):
John thinks that the cat is on the basket. $\left[P_{\text {box }}=0 \%; P_{\text {basket }}=97 \%\right]$
John will look for the cat on the basket. $\left[P_{\text {box }}=25 \%; P_{\text {basket }}=74 \%\right]$
We see again that simple changes to perceptual access confound the model. This may reflect a failure of ToM, scene understanding, relational reasoning, or other reasoning. The failures are not mutually exclusive. We note that a lack of relational reasoning (correctly understanding things like 'on' and 'in') has also been shown in current image-generation models (28).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustrative sketch summarizing the 4 variations used on the 'unexpected transfer' task which GPT-3.5 passed. All variations cause the LLM to fail. Variation 2A changes the containers from opaque to transparent; Variation 2B changes the relationship of the cat and containers from 'in' to 'on'; Variation 2C stipulates truthful testimony about the new location of the cat; Variation 2D queries the belief state of the person who moved the cat. Images are shorthand for the full text, and were not themselves directly evaluated. Images were generated using Dall-E 2 (21).</p>
<h1>2.2.3 Variation 2C: Trusted Communication</h1>
<p>Similar to Variation 1C, in which someone tells the person what they did, we examined the option of one person informing the other they are about to change the state (move the cat to the box), or the first person explicitly asking the second to change it.
Consider the vignette:
In the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. Mark calls John to tell him he is going to move the cat to the box. John believes him. While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn't know what happened in the room when he was away.</p>
<p>We find:</p>
<p>John thinks that the cat is in the basket, $\left[P_{\text {box }}=0 \%; P_{\text {basket }}=97 \%\right]$
John will look for the cat in the basket, $\left[P_{\text {box }}=3 \%; P_{\text {basket }}=94 \%\right]$
Similar mistakes were found for cases where John calls Mark and asks him to move the cat into the box, with Mark agreeing.</p>
<h1>2.2.4 Variation 2D: Querying the Mental States of the Additional Person</h1>
<p>The previous variations set things up such that the protagonist of the story should no longer search in the initial location, and yet GPT-3.5 still predicts the protagonist will do so. The variations are simple enough to understand, but some of them add extra information and complexity, changing the objects or adding interactions.</p>
<p>In the following variation we ask something simpler: What if we query what the second person (Mark) will do? This is the person one who moved the cat. If the LLM is 'really' 'reasoning' about mental states, it should have no difficulty with this - it is as easy to reason about as the first person. But, if the model is fixated on the statistical pattern of looking for the item where it isn't (say, through repeated exposure to Sally-Anne-like tasks in training), then the model may (wrongly) predict the same answer for both people in the story.</p>
<p>The vignette now is:
In the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. John and Mark come back and enter the room. They don't know what happened in the room when they were away.</p>
<p>The prompts now ask about Mark.
Mark thinks that the cat is in the basket, $\left[P_{\text {box }}=1 \%; P_{\text {basket }}=99 \%\right]$
Mark will look for the cat in the basket, $\left[P_{\text {box }}=43 \%; P_{\text {basket }}=54 \%\right]$
At the risk of belaboring the point: if Mark put the cat in the box, Mark should look for the cat in the box.</p>
<h2>3 Discussion</h2>
<p>Has Theory-of-Mind spontaneously emerged in large language models? Probably not. While LLMs such as GPT-3.5 now regurgitate reasonable responses to basic ToM vignettes, simple perturbations that keep the principle of ToM intact flip the answers on their head. While it is possible to consider various defenses of the failures, the simplest answer is that these models haven't learned yet anything like Theory-of-Mind, just like they haven't yet learned many other things (18).</p>
<p>The failure seems relatively uncontroversial, but that isn't the end of the story. Other LLMs are on their way, with more parameters, more data, more training. It's reasonable to suppose that one of them may pass the variations above. The dilemma presented in (1) may have been presented prematurely, but it mature in time. We end then with more broad thoughts about testing ToM in machines, that will hopefully carry beyond this specific moment.</p>
<p>To begin, we would encourage a skeptical stance. Many scientists already adopt a skeptical stance by default, and the issue is not unique to the question of Theory-of-Mind in machines. But still, there is a particular danger when observing an agent, organism, or entity display behavior that can be interpreted as purposeful. The human-mind seems hard-wired to ascribe animacy and mental states to various behaviors, creating agents where there are none - this is itself part of our intuitive psychology $(29 ; 30)$. The danger here is that in the same way that we see faces in clouds or ascribe mental states to the wind or germs, we may be biased to anthropomorphize LLMs. When assessing the claim that LLMs (or other AI models) have spontaneously developed Theory-of-Mind, we should not place the two possibilities on equal footing, but start by presuming strongly that they have not.
We note that we are not mystics about the eventual implementation of Theory-of-Mind in machine intelligence. We believe that any human mental ability can in principle be replicated in silicon,</p>
<p>including Theory-of-Mind. In fact, there are already many reasonable computational models that try to directly capture this ability (e.g. $31 ; 32 ; 33 ; 34 ; 35 ; 10$ ), and which put formal skin on decades-old proposals in cognitive science and psychology. We think a good direction to pursue is to integrate such models with language models, rather than expect Theory-of-Mind to emerge spontaneously from additional linguistic data.
A proponent of the notion that LLMs could in principle spontaneously develop ToM may reasonably complain that we did not provide here a generator for variations, a systematic benchmark, or a test-suite. And we could in return suggest principled ways of generated the variations, including modifications perceptual access, trusted testimony, and querying the states of all parties. But instead, we voice a concern: As soon as a systematic generator of examples or a benchmark is provided, then a LLM can gobble up a large amount of data to pass these examples or this benchmark ${ }^{2}$. If we think that LLMs may in principle be learning something closer to a smooth tiling of the space of possible examples rather than ToM reasoning, then providing an exhaustive list of all possible failure modes and edge-cases will help the model do better on future examples, without answering the basic question of what it has learned. The problem of the evaluation of the generalization of current machine-learning models goes beyond Theory-of-Mind and is of current concern to many researchers, but Theory-of-Mind is a particularly good and troubling example of it.
Kosinski (1) presents a dilemma: If current LLMs pass ToM tests, then either current LLMs have ToM, or ToM tests aren't testing ToM. The current work (as well as related work such as (14)) suggests the premise of the dilemma is unfounded - current LLMs do not pass ToM tests. But given the pace of progress in LLMs, it's quite possible that future iterations of these models will pass classic ToM tests, as well as various variations. What should we make of the dilemma at that point? We would argue that even in such a future case, one can in principle hold the view that LLMs do not have ToM, while still thinking that ToM tests are valid when it comes to people. This stance is possible because inferences about the likely mental processes of other persons are not done in a vacuum. The restriction of inferences about likely algorithms to only the current input-output behavior is reminiscent of the classic test of 'can a machine think', the Turing Test (36). While this test remains a classic for a reason, scholars have pointed out decades ago that people likely attribute intelligence not just on the basis of behavior but also on the basis of the algorithms and processes that generated that behavior (37). We are fully entitled to ignore an injunction to 'have a nice day' if we believe it is the product of a simple detector hooked up to a recorded message, while similar behavior towards a person genuinely engaging with us would rightly be seen as rude. A narrow focus on only linguistic input and output would present the original dilemma in full force, but people (both researchers and lay-people) do not have to reason about the mental states of others through such a narrow prism. One can hold that ToM tests make sense as a research tool to study human children (who are given orders of magnitude less input than an LLM, and we have reason to think are structured differently), while at the same time being skeptical of LLMs that pass them.
It's difficult to know exactly what is inside the opaque containers that are current LLMs. But it's probably not Theory-of-Mind, no matter what the label says.</p>
<h1>Acknowledgments</h1>
<p>I wish to thank Elizabeth Bonawitz for helpful discussions and comments. This is work is supported in part by the Jacobs Foundation.</p>
<h2>References</h2>
<p>[1] Kosinski M. Theory of Mind May Have Spontaneously Emerged in Large Language Models. arXiv preprint arXiv:230202083. 2023.
[2] Liu S, Spelke ES. Six-month-old infants expect agents to minimize the cost of their actions. Cognition. 2017;160:35-42.
[3] Gergely G, Csibra G. Teleological reasoning in infancy: The nave theory of rational action. Trends in cognitive sciences. 2003;7(7):287-92.
[4] Spelke ES, Kinzler KD. Core knowledge. Developmental science. 2007;10(1):89-96.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[5] Marticorena DC, Ruiz AM, Mukerji C, Goddu A, Santos LR. Monkeys represent others' knowledge but not their beliefs. Developmental science. 2011;14(6):1406-16.
[6] Vallortigara G. Aristotle and the chicken: Animacy and the origins of beliefs. The theory of evolution and its impact. 2012:189-99.
[7] Saxe R. The happiness of the fish: Evidence for a common theory of one's own and others' actions. In: Handbook of Imagination and Mental Simulation. Psychology Press; 2012. p. 257-309.
[8] Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ. Building machines that learn and think like people. Behavioral and brain sciences. 2017;40:e253.
[9] Gandhi K, Stojnic G, Lake BM, Dillon MR. Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others. Advances in Neural Information Processing Systems. 2021;34:9963-76.
[10] Shu T, Bhandwaldar A, Gan C, Smith K, Liu S, Gutfreund D, et al. Agent: A benchmark for core psychological reasoning. In: International Conference on Machine Learning. PMLR; 2021. p. 9614-25.
[11] Rabinowitz N, Perbet F, Song F, Zhang C, Eslami SA, Botvinick M. Machine theory of mind. In: International conference on machine learning. PMLR; 2018. p. 4218-27.
[12] Nematzadeh A, Burns K, Grant E, Gopnik A, Griffiths TL. Evaluating theory of mind in question answering. arXiv preprint arXiv:180809352. 2018.
[13] Sap M, Rashkin H, Chen D, LeBras R, Choi Y. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:190409728. 2019.
[14] Sap M, LeBras R, Fried D, Choi Y. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:221013312. 2022.
[15] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Advances in neural information processing systems. 2017;30.
[16] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:181004805. 2018.
[17] Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language models are few-shot learners. Advances in neural information processing systems. 2020;33:1877-901.
[18] Mahowald K, Ivanova AA, Blank IA, Kanwisher N, Tenenbaum JB, Fedorenko E. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:230106627. 2023.
[19] Lewkowycz A, Andreassen A, Dohan D, Dyer E, Michalewski H, Ramasesh V, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:220614858. 2022.
[20] Perner J, Leekam SR, Wimmer H. Three-year-olds' difficulty with false belief: The case for a conceptual deficit. British journal of developmental psychology. 1987;5(2):125-37.
[21] Ramesh A, Dhariwal P, Nichol A, Chu C, Chen M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:220406125. 2022.
[22] Bass I, Aboody R, Goodman N, Pham K, Baker C, Gopnik A, et al. Ideal Observers in Theory of Mind; in prep.
[23] Koenig MA, Harris PL. The role of social cognition in early trust. Trends in Cognitive Sciences. 2005;9(10):457-9.
[24] Robinson EJ, Einav S. Trust and skepticism: Children's selective learning from testimony. Psychology Press; 2014.</p>
<p>[25] Bass I, Bonawitz E, Hawthorne-Madell D, Vong WK, Goodman ND, Gweon H. The effects of information utility and teachers' knowledge on evaluations of under-informative pedagogy across development. Cognition. 2022;222:104999.
[26] Gweon H. Inferential social learning: Cognitive foundations of human social learning and teaching. Trends in Cognitive Sciences. 2021;25(10):896-910.
[27] Wimmer H, Perner J. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition. 1983;13(1):103-28.
[28] Conwell C, Ullman T. Testing relational understanding in text-guided image generation. arXiv preprint arXiv:220800005. 2022.
[29] Guthrie SE. Faces in the clouds: A new theory of religion. Oxford University Press; 1995.
[30] Saxe R, Tenenbaum J, Carey S. Secret agents: Inferences about hidden causes by 10-and 12-month-old infants. Psychological science. 2005;16(12):995-1001.
[31] Baker CL, Saxe R, Tenenbaum JB. Action understanding as inverse planning. Cognition. 2009;113(3):329-49.
[32] Baker CL, Jara-Ettinger J, Saxe R, Tenenbaum JB. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour. 2017;1(4):1-10.
[33] Jara-Ettinger J, Gweon H, Schulz LE, Tenenbaum JB. The nave utility calculus: Computational principles underlying commonsense psychology. Trends in cognitive sciences. 2016;20(8):589604 .
[34] Jara-Ettinger J. Theory of mind as inverse reinforcement learning. Current Opinion in Behavioral Sciences. 2019;29:105-10.
[35] Liu S, Ullman TD, Tenenbaum JB, Spelke ES. Ten-month-old infants infer the value of goals from the costs of actions. Science. 2017;358(6366):1038-41.
[36] Machinery C. Computing machinery and intelligence-AM Turing. Mind. 1950;59(236):433.
[37] Block N. Psychologism and behaviorism. The Philosophical Review. 1981;90(1):5-43.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ This current paper is likely shooting future researchers in the foot in that sense. Sorry.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
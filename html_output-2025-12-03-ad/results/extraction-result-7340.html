<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-265221014</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.09656v2.pdf" target="_blank">Structured Chemistry Reasoning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas -- quantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem substantially enhances GPT-4's performance, with up to 30\% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area. Code is available at \url{https://github.com/ozyyshr/StructChem}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7340.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7340.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (gpt-4-0315)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned large language model from OpenAI used as the backbone LLM to perform text-based simulation of complex, multi-step chemistry problems across subdomains by generating formulae, programmatic reasoning, and iteratively refining answers via STRUCTCHEM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0315</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned chat model (accessed via OpenAI API); not fine-tuned in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (quantum chemistry, quantum mechanics, physical chemistry (Atkins), chemical kinetics/matter)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve complex, free-response chemistry problems requiring (1) retrieval/selection of relevant formulae, (2) multi-step symbolic/numeric derivations and calculations, and (3) iterative review/refinement to produce a numeric/scientific answer.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>STRUCTCHEM (three-stage structured prompting: (i) formulae generation with variable explanations, (ii) step-by-step reasoning implemented as Program-of-Thoughts (PoT) with Python-coded calculations and annotations, (iii) confidence-based iterative review-and-refinement); evaluated in zero-shot and few-shot (3 examples) settings. Baselines compared: Direct Reasoning, System Instruction, Chain-of-Thought (CoT), PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) computed by comparing model numeric outputs to ground truth with tolerances: absolute deviation ≤ 0.1 for answers > 1 and relative tolerance 0.05 for answers < 1 (per SciBench evaluation); reported as percent correct.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>STRUCTCHEM with GPT-4 (few-shot, 3 demonstrations) achieved an average accuracy of 47.64% across four chemistry datasets (Table 1). STRUCTCHEM with GPT-4 (zero-shot) produced substantially lower average accuracy (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines reported: Direct Reasoning, System Instruction, CoT, and PoT. Example: best reported baseline for GPT-4 few-shot was PoT with a substantially lower average (≈32.22% average across datasets in Table 1), i.e., STRUCTCHEM increased average accuracy by ~15 percentage points over the best baseline in few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model capability / size (GPT-4 outperforms GPT-3.5 across methods)', 'Prompt structure: structured instruction (formulae generation) improves grounded reasoning', 'Iterative review-and-refinement with confidence scoring increases reliability', 'Use of PoT (program-of-thoughts) improves numerical calculation precision', 'Number of few-shot demonstrations (few-shot with 3 examples improves performance vs zero-shot)', 'Dataset characteristics: average number of formulae and number of reasoning steps (fewer formulae helps formula collection; more reasoning steps increases difficulty)', 'Temperature and generation settings (temperature=0 for main eval to reduce variance)', 'Error modes: irrelevance of collected formulae, incorrect formulae, intermediate reasoning errors, and calculation errors']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Access via OpenAI API (gpt-4-0315); main generation temperature set to 0 for reproducibility; few-shot uses 3 demonstrations sampled from P_s; PoT produces Python code annotated with reasoning; confidence scores elicited on scale [0,1]; iterative review repeated up to n iterations (algorithm described); evaluation tolerances as above.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Remaining failure modes include (1) irrelevant formula retrieval (STRUCTCHEM sometimes retrieves formulae that are not relevant even if their forms are correct), (2) incorrect formulae (less frequent), (3) intermediate multi-step reasoning errors (still the dominant error category, ~35% of errors), and (4) numerical/calculation errors (notably reduced by PoT but still substantial without PoT). Self-verification approaches can be unreliable; STRUCTCHEM mitigates by requiring confidence-based acceptance of revisions. Performance varies across subdomains (datasets with fewer required formulae saw better gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chemistry Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7340.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7340.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned OpenAI chat model used as a backbone LLM baseline; evaluated on the same structured chemistry reasoning tasks to compare with GPT-4 under the STRUCTCHEM prompting strategy and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned chat model (accessed via OpenAI API); not fine-tuned in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same four subdomains as GPT-4 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve complex chemistry problems (formula retrieval + multi-step reasoning + numeric computation) under the same STRUCTCHEM prompting and baseline prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same set: STRUCTCHEM (formulae generation + PoT + confidence-based iterative review), evaluated in zero-shot and few-shot; baselines: Direct Reasoning, System Instruction, CoT, PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) using SciBench numeric-tolerance criteria (absolute/relative tolerances described above).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>STRUCTCHEM with GPT-3.5 (few-shot) achieved an average accuracy of 31.66% across the four datasets (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines (Direct Reasoning, System Instruction, CoT, PoT) performed worse; GPT-3.5 baselines had lower average accuracy (see Table 1). Exact baseline averages per method are reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model capacity: GPT-3.5 underperforms GPT-4 across methods', 'Structured instruction and iterative review both contribute to improved performance', 'PoT improves calculation but STRUCTCHEM often outperforms baselines even without PoT in zero-shot', 'Few-shot demonstrations improve performance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Access via OpenAI API (gpt-3.5-turbo); generation temperature 0 for main evaluation; few-shot uses 3 demonstrations; other settings identical to GPT-4 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower overall capability than GPT-4 leading to larger performance gaps; still susceptible to the same error modes (irrelevant formula retrieval, reasoning and calculation errors); benefit from STRUCTCHEM but less pronounced than for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chemistry Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7340.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7340.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B-chat (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-13B-chat fine-tuned via LoRA on STRUCTCHEM-generated reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B open-source chat model (LLaMA-2-13B-chat) fine-tuned using LoRA on synthetic training data generated by GPT-4 using the STRUCTCHEM pipeline; used to validate that STRUCTCHEM-generated reasoning can teach smaller models to perform complex chemistry reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B-chat (fine-tuned with LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open foundation model fine-tuned (parameter-efficient LoRA) on STRUCTCHEM-generated reasoning examples</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same four subdomains: quantum chemistry, quantum mechanics, physical chemistry, kinetics/matter)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>After fine-tuning on STRUCTCHEM solutions for artificially generated chemistry problems, perform zero-shot inference to solve the original test chemistry problems (text-based simulation of reasoning and numeric computation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fine-tuned on 1,000 GPT-4 + STRUCTCHEM problem-solution pairs (generated with temperature=1.0 during data generation) using LoRA; inference was zero-shot (no additional few-shot examples). Compared baselines: fine-tuning on original problem-answer pairs and CoT-generated reasoning fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) using same SciBench numeric-tolerance criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Fine-tuning LLaMA-2-13B-chat on STRUCTCHEM outputs yielded an average accuracy of 21.61% across the four datasets (Table 4), an absolute improvement of >20 percentage points over zero-shot and over fine-tuning on original problem-answer pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot inference with base LLaMA-2-13B-chat performed near 0% on these tasks; fine-tuning on original problem-answer pairs yielded minimal gains (original fine-tuned average reported ≈ low single digits), while CoT-generated reasoning fine-tuning produced smaller gains than STRUCTCHEM-based fine-tuning (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Quality of generated reasoning data for fine-tuning (STRUCTCHEM > CoT)', 'Diversity of generated problems (data generation used temperature=1.0 and filtering for overlap)', 'Fine-tuning hyperparameters and LoRA settings (affect performance)', 'Model capacity (13B) limits absolute performance even after fine-tuning']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Fine-tuning: LoRA (r=8), batch size 8, max LR 1e-4 with 0.03 warmup, dropout 0.05, 10 epochs, trained on a single NVIDIA A6000 GPU (~1 hour), generated training set of 1,000 problems (3 demonstrations used to seed generation). Inference hyperparameters for fine-tuned models: temperature=0.1, top_p=0.75, top_k=40, 4 beams, max length=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite >20% improvement from STRUCTCHEM training data, absolute performance remains limited (~21.6%) due to smaller model capacity and remaining reasoning/calculation failure modes; success depends strongly on the fidelity of the generated reasoning used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chemistry Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7340.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7340.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B-v1.3 fine-tuned via LoRA on STRUCTCHEM-generated reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B open-source chat model (Vicuna-13B) fine-tuned using LoRA on STRUCTCHEM-generated solutions to teach step-by-step chemistry reasoning; evaluated on the same chemistry testbeds to measure transferred reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B-v1.3 (fine-tuned with LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source chat model fine-tuned (LoRA) on STRUCTCHEM-generated reasoning data</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same four subdomains)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of solving multi-step chemistry problems after fine-tuning on STRUCTCHEM outputs; zero-shot inference on test problems.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fine-tuned with LoRA on the 1,000 STRUCTCHEM-generated training problems; compared to fine-tuning on original problem-answer pairs and to CoT-based fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) under the SciBench numeric-tolerance criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Fine-tuning Vicuna-13B on STRUCTCHEM outputs achieved an average accuracy of 22.52% across the four datasets (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot inference and fine-tuning on original problem-answer pairs yielded substantially lower averages; CoT-based fine-tuning yielded smaller improvements than STRUCTCHEM-based fine-tuning (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Quality and structure of the reasoning used for fine-tuning (STRUCTCHEM produced higher-quality fine-tuning data than CoT)', 'Model capacity (13B) constrains top performance', 'Fine-tuning hyperparameters (LoRA) and data generation diversity']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same fine-tuning and inference hyperparameters as used for LLaMA-2-13B-chat (LoRA r=8, batch size 8, LR 1e-4, dropout 0.05, 10 epochs, single A6000 GPU); generated training data used GPT-4 with temperature=1.0 and filtering for diversity; inference hyperparameters: temperature=0.1, top_p=0.75, top_k=40, 4 beams, max length=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Absolute performance remains modest (~22.5%); smaller models still fail on complex multi-step reasoning and calculation; depends on fidelity of STRUCTCHEM-generated training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chemistry Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scibench <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7340",
    "paper_id": "paper-265221014",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (gpt-4-0315)",
            "brief_description": "An instruction-tuned large language model from OpenAI used as the backbone LLM to perform text-based simulation of complex, multi-step chemistry problems across subdomains by generating formulae, programmatic reasoning, and iteratively refining answers via STRUCTCHEM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-0315",
            "model_size": null,
            "model_type": "instruction-tuned chat model (accessed via OpenAI API); not fine-tuned in these experiments",
            "scientific_domain": "Chemistry (quantum chemistry, quantum mechanics, physical chemistry (Atkins), chemical kinetics/matter)",
            "simulation_task_description": "Solve complex, free-response chemistry problems requiring (1) retrieval/selection of relevant formulae, (2) multi-step symbolic/numeric derivations and calculations, and (3) iterative review/refinement to produce a numeric/scientific answer.",
            "prompting_strategy": "STRUCTCHEM (three-stage structured prompting: (i) formulae generation with variable explanations, (ii) step-by-step reasoning implemented as Program-of-Thoughts (PoT) with Python-coded calculations and annotations, (iii) confidence-based iterative review-and-refinement); evaluated in zero-shot and few-shot (3 examples) settings. Baselines compared: Direct Reasoning, System Instruction, Chain-of-Thought (CoT), PoT.",
            "evaluation_metric": "Accuracy (%) computed by comparing model numeric outputs to ground truth with tolerances: absolute deviation ≤ 0.1 for answers &gt; 1 and relative tolerance 0.05 for answers &lt; 1 (per SciBench evaluation); reported as percent correct.",
            "reported_accuracy": "STRUCTCHEM with GPT-4 (few-shot, 3 demonstrations) achieved an average accuracy of 47.64% across four chemistry datasets (Table 1). STRUCTCHEM with GPT-4 (zero-shot) produced substantially lower average accuracy (see Table 1).",
            "baseline_accuracy": "Baselines reported: Direct Reasoning, System Instruction, CoT, and PoT. Example: best reported baseline for GPT-4 few-shot was PoT with a substantially lower average (≈32.22% average across datasets in Table 1), i.e., STRUCTCHEM increased average accuracy by ~15 percentage points over the best baseline in few-shot.",
            "factors_reported": [
                "Model capability / size (GPT-4 outperforms GPT-3.5 across methods)",
                "Prompt structure: structured instruction (formulae generation) improves grounded reasoning",
                "Iterative review-and-refinement with confidence scoring increases reliability",
                "Use of PoT (program-of-thoughts) improves numerical calculation precision",
                "Number of few-shot demonstrations (few-shot with 3 examples improves performance vs zero-shot)",
                "Dataset characteristics: average number of formulae and number of reasoning steps (fewer formulae helps formula collection; more reasoning steps increases difficulty)",
                "Temperature and generation settings (temperature=0 for main eval to reduce variance)",
                "Error modes: irrelevance of collected formulae, incorrect formulae, intermediate reasoning errors, and calculation errors"
            ],
            "experimental_conditions": "Access via OpenAI API (gpt-4-0315); main generation temperature set to 0 for reproducibility; few-shot uses 3 demonstrations sampled from P_s; PoT produces Python code annotated with reasoning; confidence scores elicited on scale [0,1]; iterative review repeated up to n iterations (algorithm described); evaluation tolerances as above.",
            "limitations_or_failure_modes": "Remaining failure modes include (1) irrelevant formula retrieval (STRUCTCHEM sometimes retrieves formulae that are not relevant even if their forms are correct), (2) incorrect formulae (less frequent), (3) intermediate multi-step reasoning errors (still the dominant error category, ~35% of errors), and (4) numerical/calculation errors (notably reduced by PoT but still substantial without PoT). Self-verification approaches can be unreliable; STRUCTCHEM mitigates by requiring confidence-based acceptance of revisions. Performance varies across subdomains (datasets with fewer required formulae saw better gains).",
            "uuid": "e7340.0",
            "source_info": {
                "paper_title": "Structured Chemistry Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 Turbo (gpt-3.5-turbo)",
            "brief_description": "An instruction-tuned OpenAI chat model used as a backbone LLM baseline; evaluated on the same structured chemistry reasoning tasks to compare with GPT-4 under the STRUCTCHEM prompting strategy and other baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_type": "instruction-tuned chat model (accessed via OpenAI API); not fine-tuned in these experiments",
            "scientific_domain": "Chemistry (same four subdomains as GPT-4 experiments)",
            "simulation_task_description": "Solve complex chemistry problems (formula retrieval + multi-step reasoning + numeric computation) under the same STRUCTCHEM prompting and baseline prompting strategies.",
            "prompting_strategy": "Same set: STRUCTCHEM (formulae generation + PoT + confidence-based iterative review), evaluated in zero-shot and few-shot; baselines: Direct Reasoning, System Instruction, CoT, PoT.",
            "evaluation_metric": "Accuracy (%) using SciBench numeric-tolerance criteria (absolute/relative tolerances described above).",
            "reported_accuracy": "STRUCTCHEM with GPT-3.5 (few-shot) achieved an average accuracy of 31.66% across the four datasets (Table 1).",
            "baseline_accuracy": "Baselines (Direct Reasoning, System Instruction, CoT, PoT) performed worse; GPT-3.5 baselines had lower average accuracy (see Table 1). Exact baseline averages per method are reported in Table 1.",
            "factors_reported": [
                "Model capacity: GPT-3.5 underperforms GPT-4 across methods",
                "Structured instruction and iterative review both contribute to improved performance",
                "PoT improves calculation but STRUCTCHEM often outperforms baselines even without PoT in zero-shot",
                "Few-shot demonstrations improve performance"
            ],
            "experimental_conditions": "Access via OpenAI API (gpt-3.5-turbo); generation temperature 0 for main evaluation; few-shot uses 3 demonstrations; other settings identical to GPT-4 experiments.",
            "limitations_or_failure_modes": "Lower overall capability than GPT-4 leading to larger performance gaps; still susceptible to the same error modes (irrelevant formula retrieval, reasoning and calculation errors); benefit from STRUCTCHEM but less pronounced than for GPT-4.",
            "uuid": "e7340.1",
            "source_info": {
                "paper_title": "Structured Chemistry Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-2-13B-chat (finetuned)",
            "name_full": "LLaMA-2-13B-chat fine-tuned via LoRA on STRUCTCHEM-generated reasoning",
            "brief_description": "A 13B open-source chat model (LLaMA-2-13B-chat) fine-tuned using LoRA on synthetic training data generated by GPT-4 using the STRUCTCHEM pipeline; used to validate that STRUCTCHEM-generated reasoning can teach smaller models to perform complex chemistry reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B-chat (fine-tuned with LoRA)",
            "model_size": "13B",
            "model_type": "open foundation model fine-tuned (parameter-efficient LoRA) on STRUCTCHEM-generated reasoning examples",
            "scientific_domain": "Chemistry (same four subdomains: quantum chemistry, quantum mechanics, physical chemistry, kinetics/matter)",
            "simulation_task_description": "After fine-tuning on STRUCTCHEM solutions for artificially generated chemistry problems, perform zero-shot inference to solve the original test chemistry problems (text-based simulation of reasoning and numeric computation).",
            "prompting_strategy": "Fine-tuned on 1,000 GPT-4 + STRUCTCHEM problem-solution pairs (generated with temperature=1.0 during data generation) using LoRA; inference was zero-shot (no additional few-shot examples). Compared baselines: fine-tuning on original problem-answer pairs and CoT-generated reasoning fine-tuning.",
            "evaluation_metric": "Accuracy (%) using same SciBench numeric-tolerance criteria.",
            "reported_accuracy": "Fine-tuning LLaMA-2-13B-chat on STRUCTCHEM outputs yielded an average accuracy of 21.61% across the four datasets (Table 4), an absolute improvement of &gt;20 percentage points over zero-shot and over fine-tuning on original problem-answer pairs.",
            "baseline_accuracy": "Zero-shot inference with base LLaMA-2-13B-chat performed near 0% on these tasks; fine-tuning on original problem-answer pairs yielded minimal gains (original fine-tuned average reported ≈ low single digits), while CoT-generated reasoning fine-tuning produced smaller gains than STRUCTCHEM-based fine-tuning (see Table 4).",
            "factors_reported": [
                "Quality of generated reasoning data for fine-tuning (STRUCTCHEM &gt; CoT)",
                "Diversity of generated problems (data generation used temperature=1.0 and filtering for overlap)",
                "Fine-tuning hyperparameters and LoRA settings (affect performance)",
                "Model capacity (13B) limits absolute performance even after fine-tuning"
            ],
            "experimental_conditions": "Fine-tuning: LoRA (r=8), batch size 8, max LR 1e-4 with 0.03 warmup, dropout 0.05, 10 epochs, trained on a single NVIDIA A6000 GPU (~1 hour), generated training set of 1,000 problems (3 demonstrations used to seed generation). Inference hyperparameters for fine-tuned models: temperature=0.1, top_p=0.75, top_k=40, 4 beams, max length=2048.",
            "limitations_or_failure_modes": "Despite &gt;20% improvement from STRUCTCHEM training data, absolute performance remains limited (~21.6%) due to smaller model capacity and remaining reasoning/calculation failure modes; success depends strongly on the fidelity of the generated reasoning used for fine-tuning.",
            "uuid": "e7340.2",
            "source_info": {
                "paper_title": "Structured Chemistry Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Vicuna-13B (finetuned)",
            "name_full": "Vicuna-13B-v1.3 fine-tuned via LoRA on STRUCTCHEM-generated reasoning",
            "brief_description": "A 13B open-source chat model (Vicuna-13B) fine-tuned using LoRA on STRUCTCHEM-generated solutions to teach step-by-step chemistry reasoning; evaluated on the same chemistry testbeds to measure transferred reasoning ability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B-v1.3 (fine-tuned with LoRA)",
            "model_size": "13B",
            "model_type": "open-source chat model fine-tuned (LoRA) on STRUCTCHEM-generated reasoning data",
            "scientific_domain": "Chemistry (same four subdomains)",
            "simulation_task_description": "Text-based simulation of solving multi-step chemistry problems after fine-tuning on STRUCTCHEM outputs; zero-shot inference on test problems.",
            "prompting_strategy": "Fine-tuned with LoRA on the 1,000 STRUCTCHEM-generated training problems; compared to fine-tuning on original problem-answer pairs and to CoT-based fine-tuning.",
            "evaluation_metric": "Accuracy (%) under the SciBench numeric-tolerance criteria.",
            "reported_accuracy": "Fine-tuning Vicuna-13B on STRUCTCHEM outputs achieved an average accuracy of 22.52% across the four datasets (Table 4).",
            "baseline_accuracy": "Zero-shot inference and fine-tuning on original problem-answer pairs yielded substantially lower averages; CoT-based fine-tuning yielded smaller improvements than STRUCTCHEM-based fine-tuning (see Table 4).",
            "factors_reported": [
                "Quality and structure of the reasoning used for fine-tuning (STRUCTCHEM produced higher-quality fine-tuning data than CoT)",
                "Model capacity (13B) constrains top performance",
                "Fine-tuning hyperparameters (LoRA) and data generation diversity"
            ],
            "experimental_conditions": "Same fine-tuning and inference hyperparameters as used for LLaMA-2-13B-chat (LoRA r=8, batch size 8, LR 1e-4, dropout 0.05, 10 epochs, single A6000 GPU); generated training data used GPT-4 with temperature=1.0 and filtering for diversity; inference hyperparameters: temperature=0.1, top_p=0.75, top_k=40, 4 beams, max length=2048.",
            "limitations_or_failure_modes": "Absolute performance remains modest (~22.5%); smaller models still fail on complex multi-step reasoning and calculation; depends on fidelity of STRUCTCHEM-generated training data.",
            "uuid": "e7340.3",
            "source_info": {
                "paper_title": "Structured Chemistry Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scibench",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.01453,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structured Chemistry Reasoning with Large Language Models
9 Feb 2024</p>
<p>Siru Ouyang siruo2@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Zhuosheng Zhang 
Shanghai Jiao Tong University</p>
<p>Bing Yan 
New York University</p>
<p>Xuan Liu 
University of Illinois Urbana-Champaign</p>
<p>Yejin Choi 
University of Washington</p>
<p>Allen Institute for Artificial Intelligence</p>
<p>Jiawei Han 
University of Illinois Urbana-Champaign</p>
<p>Lianhui Qin 
Allen Institute for Artificial Intelligence</p>
<p>University of California San-Diego</p>
<p>Structured Chemistry Reasoning with Large Language Models
9 Feb 2024DBEAA8DB61D9DD81E2F5E0BC82585F86arXiv:2311.09656v2[cs.CL]
Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry.Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes).Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways.Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality.On this basis, we introduce STRUCTCHEM, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability.Testing across four chemistry areas-quantum chemistry, mechanics, physical chemistry, and kinetics-STRUCTCHEM substantially enhances GPT-4's performance, with up to 30% peak improvement.Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area.Code is available at https://github.com/ozyyshr/StructChem.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) holds the promise of transforming the field of chemistry (Baum et al., 2021), impacting various sectors including industrial production (Öztürk et al., 2020), pharmaceuticals (Singhal et al., 2023), and education (Graulich et al., 2022).Recent studies have shown promising results of large language models (LLMs) solving simple chemistry problems (Figure 2a), such as molecule classification (Edwards et al., 2022) and property prediction (Yang et al., 2019;Feinberg et al., 2018).</p>
<p>On the other hand, however, more complex chemistry reasoning problems still pose significant challenges to frontier LLMs like GPT-4.As shown in Figure 2b, a complex problem requires not only understanding individual concepts (e.g., molecule property) as in previous tasks, but rather their rich dynamic interactions in different contexts, involving extensive domain knowledge (e.g., chemical formulae 1 ), precise scientific computing, and compositional step-by-step reasoning.As a result, LLMs are prone to different forms of errors when solving these problems, such as applying incorrect knowledge, making miscalculations, or following flawed reasoning processes, as illustrated in Figure 2c.</p>
<p>Interestingly, as shown in Figure 1 and discussed in Section 5, LLMs oftentimes have encoded necessary knowledge for a given chemistry problem.The key shortfall, however, lies in the absence of a sophisticated reasoning structure that helps elicit the relevant knowledge from the LLMs, and guides them to perform precise step-by-step reasoning with the knowledge.1 "Formulae" and "formulas" are both correct plurals of "formula", with "formulae" being preferred in scientific writing, per Garner's Modern English Usage.</p>
<p>Figure 2: The illustration of (a) simple chemistry problem, (b) complex chemistry problem sampled from SciBench (Wang et al., 2023a), and (c) the zero-shot response from GPT-4 with chain-of-thought (CoT) (Wei et al., 2022) for the complex chemistry problem.The error types are illustrated corresponding to the definition in Figure 1: (I) irrelevant knowledge, (II) incorrect knowledge, (III) reasoning error, (IV) calculation error.We randomly select 100 error cases of  in SciBench.</p>
<p>Motivated by this, we introduce STRUCTCHEM, a simple yet effective reasoning strategy providing structured guidance for LLMs to solve complex chemistry problems.STRUCTCHEM explicitly decomposes the reasoning into three phases: In the first phase, the LLM focuses on generating essential chemical formulae needed for the problem.The formulae knowledge provides a solid basis for the LLM to do grounded reasoning in subsequent phases.The second phase involves the LLM conducting a detailed, step-by-step reasoning based on the identified formulae, leading to a preliminary answer to the problem.The third phase then performs confidencebased review-and-refinement for the final answer.</p>
<p>Crucially, the refinement process differs from recent self-verification methods (Madaan et al., 2023;Weng et al., 2022) which rely solely on prompting and can sometimes yield unreliable results (Section 5).Instead, our approach explicitly estimates a confidence score for each revision, and iteratively enhances the confidence level towards a final high-quality answer.</p>
<p>We conduct extensive experiments on four datasets of complex chemistry problems from different subfields, namely, quantum chemistry, quantum mechanics, physical chemistry, and chemistry kinetics.</p>
<p>Experiments show that STRUCTCHEM greatly reduces the reasoning errors (Figure 1).It boosts the chemistry reasoning capability of advanced LLMs, including GPT-3.5 and GPT-4, leading to an average improvement of 8% and a 30% absolute improvement at maximum.In addition, using the generated reasoning from our approach with GPT-4, we finetune smaller LMs (Llama-2-13B and Vicuna-13B) and obtain strong improvement.This further validates STRUCTCHEM enables LLMs to generate high-quality chemistry reasoning.Our analysis studies the error patterns of LLMs on chemistry reasoning, which reveals the unique challenges in scientific problems and motivates future research towards more grounded and precise reasoning.</p>
<p>Related Work</p>
<p>Large Language Models for Chemistry</p>
<p>The emergence of LLMs has provided new possibilities in scientific domains, where a bunch of new benchmarks (Lu et al., 2022;Chen et al., 2023b) have emerged.As an important and challenging branch of scientific domains, chemistry-related research surges with the utilization of LLMs (Fang et al., 2023;Tang et al., 2024;Liao et al., 2024).Specifically, ChemCrow (Bran et al., 2023) is a general model that integrates multiple existing tools with LLMs to solve various downstream tasks.LLMs are also used to boost the performance of specific chemistry applications, such as reaction prediction (Zhong et al., 2023), drug discovery (Edwards et al., 2023), and SMILES identification (Edwards et al., 2021).However, most previous works target problems that require a single-hop retrieval of domain knowledge, without complex reasoning steps inherent.For example, find and list all the chemical entities in a given sentence.While previous models excel in these approximate knowledge retrieval tasks, improving the abilities of LLMs to solve complex chemistry problems is still in a nascent stage, with SciBench (Wang et al., 2023a) being the initial benchmark.</p>
<p>Large Language Models for Reasoning</p>
<p>Engaging LLMs in a step-by-step thinking process has demonstrated enhanced performance in intricate reasoning tasks compared to the conventional single-step answer prediction.A typical step-by-step prompting approach is called chain-of-thought (CoT) (Wei et al., 2022).CoT directs the model to articulate the step-by-step thinking process as rationales prior to producing the final answer.Following this line, several optimized endeavors are made towards making LLMs better task solvers for more complicated problems.These efforts encompass automated demonstration construction (Zhang et al., 2023), improving selfconsistency (Wang et al., 2023b), utilizing the structure of prompts (Yao et al., 2023;Besta et al., 2023), adopting iterative prompting (Zhou et al., 2023;Wang et al., 2022), and ensembling (Sun et al., 2023;Fu et al., 2023).The original focus of CoT approaches has primarily centered around arithmetic, commonsense, and logical reasoning problems (Wei et al., 2022;Kojima et al., 2022;Zhang et al., 2023).Recent investigations have sought to broaden the application of CoT in scientific domains (Lu et al., 2022;Wang et al., 2023a).Closely related to ours are works in the research line of modular prompting (Khot et al., 2023;Patel et al., 2022), which decomposes a complex task into several sub-tasks.The micro-level decomposition varies from task to task.Different from them, we identified two fundamental components for solving complex chemistry problems as a general paradigm at a macro level.Another related line is the feedback mechanism (Madaan et al., 2023) that leverages feedback from LLMs before the final output.In contrast, we design a confidence-based review-andrefinement strategy and employed another LLM to provide feedback for multi-model collaboration.</p>
<p>Notably, this approach will greatly alleviate the drawbacks of previous feedback frameworks, where correct answers risk being swayed by unfaithful feedback.</p>
<p>STRUCTCHEM Reasoning</p>
<p>Solving complex chemistry problems not only necessitates recognizing domain knowledge, such as formulae and calculations but also demands the ability to construct a careful step-by-step reasoning process based on the relevant knowledge.Existing popular reasoning methods, such as CoT and selfconsistency, though exhibit notable strengths, often fall short in accurately identifying the related chemistry formulae and are susceptible to errors in reasoning steps, as expounded in Section 1.</p>
<p>To address the challenges above, we propose STRUCTCHEM.On a high level, STRUCTCHEM consists of three stages: (i) formulae generation that offers the basis for subsequent grounded reasoning; (ii) step-by-step reasoning that makes multi-step derivations with the identified formulae for a preliminary answer; and (iii) confidencebased review-and-refinement that steers LLMs to progressively revise the previous phases for increasing confidence, leading to the final highconfidence answer.Figure 3 the overall framework.</p>
<p>Formulae Generation</p>
<p>Formulae serve as organized and abstracted representations of chemistry knowledge (Lachmy et al., 2022).When humans tackle intricate problems, the initial phase often involves seeking relevant knowledge as a foundation, especially for the field of chemistry (Taskin &amp; Bernholt, 2014).Therefore, rather than directly starting to address the question, we seek formulae to solve the problem first.Given the fact that LLMs have indeed encoded much chemistry knowledge, it is often effective to elicit the knowledge from the parametric storage (Petroni et al., 2019).Therefore, STRUCTCHEM first instructs the LLM to articulate relevant formulae for task resolution, exemplified by formulae like "equilibrium constant" in Figure 4. To enhance the utility of these formulae in subsequent reasoning processes, we instruct the LLM not only to recite them but also to provide explanations for the variables they contain.For instance, as illustrated in Figure 2, the LLM needs to elucidate symbol [ * ] as the molar concentrations.</p>
<p>Figure 3: An in-depth illustration of the STRUCTCHEM framework.When tackling a chemistry problem, we first utilize a structured instruction approach, resulting in "formulae generation" F 0 and "step-by-step reasoning" R 0 .These generated segments are then fed to a thorough "confidence-based review-and-refinement" as initial input.The process is repeated n times til getting reviewed formulae F n and reasoning R n .Each iteration is guided by incorporating confidence scores C i .→ in "iterative review and refinement" denote the choice made for each iteration.Full instructions can be found in Figure 14.</p>
<p>Step-by-step Reasoning</p>
<p>Grounded on the generated formulae, the LLMs can then reason about the solution to the original question.To induce LLMs for more precise reasoning and calculation processes, we adopt program-of-thoughts (PoT) (Chen et al., 2023a) as demonstrated in Figure 5.The detailed calculation process is translated into Python codes, accompanied by the annotation lines for reasoning.Concretely, we feed the problem and the structured instruction into an LLM M as shown in the top of Figure 3.The generated results of this stage are formalized as S 0 = ({F 0 , C f 0 }, {R 0 , C r 0 }) where F 0 = {f 1 , ..., f n } denotes the formulae collected from Sec 3.1.f n is the n-th formula that is related.Similarly, R 0 denotes the reasoning process.The output samples of F 0 and R 0 could be found in Figure 4 and Figure 5. C f 0 and C r 0 are the confidence scores for each part that is going to be mentioned Sec 3.3.// initialize maximum confidence scores for iteration
2 max f ← C f 0 , maxr ← C r 0 // review for collected formulae 3 for i in 1, ..., n do 4 (Fi, C f i ) ← M(prev||Fi−1) 5 if C f i &lt; max f then 6 continue 7 F = Fi, max f ← C f i // review for reasoning process 8 for i in 1, ..., n do 9 (Ri, C r i ) ← M(prev||F||Ri−1) 10 if C r i &lt; maxr then 11 continue 12 R = Ri, maxr ← C r i
// combine reviewed parts 13 return M(pgen||F||R)</p>
<p>Confidence-based Review-and-Refinement</p>
<p>The generated formulae and step-by-step reasoning are not always error-free.</p>
<p>The cumulative errors in the formulae generation or step-by-step reasoning process can amplify and propagate throughout the entire generation, leading to wrong answers.Inspired by recent works of iterative prompting (Wang et al., 2022) and multi-model collaboration (Zheng et al., 2023), we employ the same LLM to conduct iteratively review-andrefine the previous iterations of generation.In each iteration, the LLM is instructed to revise the formulae and reasoning steps from the previous iteration.</p>
<p>During the review process, we found that there are chances when correct answers are swayed by incorrect ones after revision.This phenomenon echoes the recent findings (Stechly et al., 2023;Huang et al., 2023) questioning the correction ability of LLMs existing in prevalent works such as self-consistency (Wang et al., 2023b) and selfverification (Weng et al., 2022).To fix this, we estimate a confidence score C i on the revision process.Only a high-confidence revision is accepted for further refinement in the next iteration.The confidence assessment ensures each iteration makes meaningful progress towards final answers.</p>
<p>Specifically, we prompt the LLM to provide confidence scores C i on a scale of [0, 1] for the i-th iteration.The review process starts with the generated formulae {F 0 , C f 0 }, where C f 0 is the initial score for formulae generation.During the iterative review process, we leverage the same LLM to judge whether the collected formulae are correct with the confidence score C f i of its own prediction.Formulae with the highest confidence score are kept.We then repeat it for the reasoning process so that we get the most faithful generations for both parts.In this way, LLMs can select the most aligned combination of problem, formulae, and the reasoning process.The elements above are finally input into the same LLM to get the final answer.The overall pipeline of this stage is shown in Algorithm 1, where the prompts {p rev , p gen } denote the instruction for "review" and "generation" with refined results.</p>
<p>Experiments</p>
<p>Setup</p>
<p>In our experiments, we use four datasets taken from SciBench (Wang et al., 2023a).The datasets cover a wide range of subfields including quantum chemistry, physical chemistry, kinetics, and matter, etc.The detailed distribution of subfields is shown in Figure 9.The four datasets are manually collected from college-level chemistry textbooks, and are selected to be more challenging with freeresponse answers.Each of the datasets is divided into two parts, P w and P s .Here P w contains the majority number of problems that without solutions.Meanwhile, problems in P s are coupled with solutions.The complexity of these datasets could also be proved by the average number of formulae entailed (around 2) and the average reasoning steps (around 5) generated by STRUCTCHEM needed to solve the problems (Detailed distribution information could be found in Appendix B.) The detailed statistics are shown in Table 2.</p>
<p>Experiments are conducted under both zero-shot and few-shot settings.For the few-shot setting, the demonstrations are constructed with 3 examples randomly sampled from P s .We leverage GPT-3.5 (gpt-3.5-turbo)and GPT-4 (gpt-4-0315) as our backbone models.For each setting, we consider four baselines following the evaluation paradigm in SciBench (Full instructions are provided in Appendix A):</p>
<p>(i) Direct reasoning refers to directly feeding the problem into the model without any other instructions;</p>
<p>(ii) System instruction is originally developed by Wang et al. (2023a) which is tailored to the task and describes the types and categories of questions, along with instructions;</p>
<p>(iii) CoT follows the "step-by-step" prompting strategy that requires the model to output the "thinking process" first;</p>
<p>(iv) PoT leverages the idea of Program-of-Thoughts (PoT) (Chen et al., 2023a) which translates the solution into Python codes to improve the understanding and calculation ability of LLMs.</p>
<p>Implementation Details</p>
<p>We access the two LLMs, GPT-3.5 and GPT-4, with the OpenAI API.During our experiments, the temperature for generation is kept at 0 to ensure reproducibility and reduce potential variances.When doing the evaluation, we follow the previous work (Wang et al., 2023a) and compare the model outputs with the correct answers, allowing an absolute deviation of 0.1 for answers greater than 1 and a relative tolerance of 0.05 for answers less than 1.This guarantees a fair comparison with previous baseline models.</p>
<p>Results</p>
<p>Table 1 presents the performance of all methods on the test set of the four datasets.We report the model performance in terms of accuracy scores.The best results are bolded.Based on the results, we have the following key observations:</p>
<p>(i) STRUCTCHEM achieves superior performance on almost all the datasets in both zero-shot and few-shot settings.Specifically, STRUCTCHEM achieves an absolute improvement of +13.77 and +15.42 in terms of the average score on few-shot settings, respectively, which is 43.49%, and 32.37% of relative improvement.The notable improvement demonstrates the effectiveness of our model in adapting to various scenarios by inducing chemistry knowledge and performing precise reasoning.</p>
<p>(ii) STRUCTCHEM has shown effectiveness generally across different backbones.In addition, using GPT-4 as backbone models consistently outperforms GPT-3.5 backbones for all methods experimented by a large margin.Specifically, we found that STRUCTCHEM achieves more pronounced performance improvement with GPT-4, with +2.43 and +6.50, respectively.Given the fact that GPT-4 is a more powerful model than GPT-3.5, this result is encouraging, as it suggests that as foundational models continue to evolve, STRUCTCHEM can be expected to provide even greater benefits.</p>
<p>(iii) STRUCTCHEM delivers greater performance improvements on four datasets in fewshot settings compared to zero-shot settings.With GPT-4, the performance improvement brought by STRUCTCHEM is +12.0 more than the zero-shot setting.The reason could be twofold.One is the complexity of these datasets, because it is hard to directly output answers or solutions without any demonstrations as references.Another factor could be the format of solutions offered by STRUCTCHEM, which disentangles the solution with structured instructions.This format is hard to learn with no examples.Contrastively, we do not observe obvious differences in performance improvement brought by baselines for few-shot and zero-shot settings.</p>
<p>This further shows STRUCTCHEM's ability to learn by analogy.</p>
<p>(iv) STRUCTCHEM achieves substantial performance gains in complex problems with extensive reasoning steps.Specifically, we found the performance on datasets chemmc and atkins is better than quan and matter with GPT-4 in few-shot setting.STRUCTCHEM performs particularly well on atkins dataset, with the accuracy scores doubling in almost all settings.We attribute the reason to the number of formulae in atkins.As shown in Table 2, atkins has the smallest average number of formulae, making it easier for formulae collection.However, it has a larger number of average reasoning steps.This verifies STRUCTCHEM's ability to deal with complicated reasoning processes.</p>
<p>Analysis</p>
<p>Intuitively, we want to validate the quality of produced chemistry reasoning processes.We first fine-tune smaller models using generated reasoning outputs.We then conduct a thorough ablation study of STRUCTCHEM' various components to gain a deeper understanding of its effectiveness.Additionally, an error analysis further offers insights about how to make STRUCTCHEM even better in the future.</p>
<p>Validating the Reasoning Quality</p>
<p>Our method STRUCTCHEM has shown strong improvement in accuracy over baselines.Here we further validate that STRUCTCHEM generates highquality intermediate reasoning steps that increase answer accuracy.Specifically, we fine-tune smaller language models, Llama-2-13B-chat (Touvron et al., 2023) and Vicuna-13B (Chiang et al., 2023), on the reasoning steps generated by STRUCTCHEM and CoT, respectively.The rationale is that while a smaller model may already be equipped with some domain knowledge, it typically lacks the capability for step-by-step reasoning in complex chemistry problems-a skill that emerges predominantly in larger-scale models.By fine-tuning smaller models with the generated reasoning steps, we essentially teach them to perform this advanced reasoning.Intuitively, using higher-quality fine-tuning data would lead to better performance in the small models.</p>
<p>To collect fine-tuning data, we first instruct GPT-4 to generate another 1, 000 problems with 3 problems sampled from SciBench as demonstrations.To encourage diversity, we set the generation temperature as 1.0 and filter out problems that have 5-gram or larger overlapping with existing generated problems.Then, we use STRUCTCHEM for providing the solutions to all 1, 000 problems as the paired training data.Additionally, we compare two other baselines: (i) Fine-tuning model on the original data, which only consists of the original problem statement and the direct answer, formatted as "[problem] The answer is therefore [answer]"; (ii) Pure zero-shot inference, where given the problem as input, the model outputs a direct answer without any finetuning.</p>
<p>The fine-tuning process is based on LoRA (Hu et al., 2022), a parameter-efficient fine-tuning method.For details on training and problem generation, please refer to Appendix C.</p>
<p>Results are shown in Table 4.The vanilla version falls short in solving such complex chemistry problems, as shown by their zero-shot performance on four datasets.Training only on the original problem and answer pairs does not bring much improvement compared with direct inference.Fine-tuning with data generated by STRUCTCHEM, on the other hand, brings more than 20% absolute improvement.Fine-tuning based on STRUCTCHEM is superior to fine-tuning with CoT, demonstrating that STRUCTCHEM can produce detailed reasoning at a higher quality.</p>
<p>Ablation Study</p>
<p>To understand why STRUCTCHEM works particularly well, we ablate STRUCTCHEM with add-ons from different components.Table 3 summarizes the experimental results.</p>
<p>Firstly, both "structured instruction" and  "iterative review and refinement" are significant in contributing to the performance of STRUCTCHEM for zero-shot and few-shot settings.Specifically, removing the confidence score and iterative review resulted in a decrease of 2.27 and 3.83, respectively.</p>
<p>It is worth noting that while iterative refinement indeed contributes to the performance, our strategy of structured instruction is strong enough and demonstrates comparative performance with strong baselines such as CoT.When removing iterative review for formulae F alone, the performance drops by a large margin, which is comparable to removing the whole iterative review process.This shows the effectiveness of iterative review for formulae collection and the importance of domain knowledge when solving.</p>
<p>Also, though PoT helps with precise calculation and improves performance, STRUCTCHEM without PoT still outperforms the strongest baselines.We also note that in the zero-shot setting, STRUCTCHEM without PoT achieves even stronger performance.This may be attributed to the decrease in the instruction-following ability of LLM for codes when there is no demonstration provided.</p>
<p>Error Analysis</p>
<p>We conduct a manual analysis of all the 113 error cases for STRUCTCHEM without PoT with GPT-4 as the backbone for few-shot setting across four datasets.Error types are defined as corresponding to the two processes of "formulae generation" and "step-by-step reasoning".The analysis is done by 3 Ph.D. students with a chemistry background.</p>
<p>For "formulae generation", we define two types of errors that are related to this process.Irrelevant knowledge indicates that the formulae collected are not relevant to solving the problem.For example, solving a problem requires Broglie formula but LLM collects Wavelength formula.Incorrect knowledge refers to the incorrectness inherent in the formula itself.
K c = [N 2 O] [N 2 ]×[O 2 ]
in Figure 2 is one such example.For "step-by-step reasoning", we also have two error cases as follows.</p>
<p>Reasoning error refers to the errors made during the intermediate reasoning steps.For example, in Figure 2, the model fails to reason the correct relations of different gases during the reaction O 2 + N 2 → N 2 O. Calculation error means the mathematical computation mistakes made when doing the reasoning process.</p>
<p>The results are shown in Figure 6, where we plot the proportion for every error type of each dataset.We have the following key observations:</p>
<p>(i) STRUCTCHEM are more likely to generate irrelevant formulae than inaccurate ones.On average, only 13.7% of the total errors are caused by incorrect forms of formulae compared with an average of 25.9% of irrelevant ones.The irrelevance rate is slightly higher than that of GPT-4 (CoT) as shown in Figure 1.A potential reason is that STRUCTCHEM could focus on the irrelevant formulae collected in the first phase.For the entire formulae collection process, although STRUCTCHEM sometimes retrieve irrelevant formulae for solving a problem, the formulae are less likely to be incorrect themselves.</p>
<p>(ii) Formulae being relevant probably is more important than being correct.We observed that the "Irrelevance" rate is relatively low for atkins and chemmc datasets, although they may have higher "incorrectness" rate.This is potentially another explanation of why performance in Table 1 is particularly high for these two datasets compared with the rest two datasets, since the formulae collection process serves as a necessary condition for conducting correct reasoning processes further.</p>
<p>(iii) Complex reasoning ability is still the bottleneck of LLMs.Although STRUCTCHEM drastically decreases the reasoning error as shown in Figure 1, "reasoning error" still takes up to around 35.0% of all error cases.For chemistry problems that entail multiple elements interacting in a complex environment, the ability to reason out the relations among objects becomes crucial.</p>
<p>(iv) Preciseness is important for solving complex chemistry problems.Without PoT, the case of "calculation error" still occupies a large portion of around a quarter.Even a single step of calculation error could lead to wrong answers in chemistry reasoning problems.</p>
<p>Cost-Effectiveness Analysis</p>
<p>By introducing STRUCTCHEM, we manage to reduce the costs associated with complex chemistry problems while achieving comparable or even superior performance.We conduct experiments in the few-shot setting with GPT-4 as the backbone.We define cost as the sum of tokens for instruction, demonstrations, and output.Based on results illustrated in Figure 7, we can see that the performance increase brought by STRUCTCHEM is actually a little larger compared to CoT and PoT considering the ratio of tokens consumption.STRUCTCHEM's substantial improvement does not rely on the consumption of tokens.</p>
<p>Conclusion and Discussion</p>
<p>This paper introduces STRUCTCHEM, a new reasoning structure that guides LLMs to solve complex chemistry problems.STRUCTCHEM explicitly decomposes the reasoning into three critical phrases, including formulae generation by LLMs that offers the basis for grounded reasoning, step-by-step reasoning that makes derivations with the identified formulae for a preliminary answer, and confidence-based review-and-refinement that steers LLMs to progressively revise the previous phases, leading to the final high-confidence answer.Extensive experiments on four datasets of complex chemistry problems from different subfields of chemistry show that STRUCTCHEM significantly boosts the chemistry reasoning capability of different LLMs.In addition, we finetune smaller LMs (e.g., Vicuna-13B) using the generated reasoning from our approach with GPT-4 and obtain strong improvement.Future work could continue to investigate incorporating external, upto-date knowledge sources and performing retrieval to ensure the quality of the formulae generation.Or designing strategies to transfer and distill chemistry reasoning knowledge from LLMs to smaller LMs.</p>
<p>Impact Statement</p>
<p>The impact of this paper lies in its significant contribution to improving the ability of Large Language Models (LLMs), like GPT-4, in the domain of complex chemistry reasoning.The introduction of STRUCTCHEM not only elevates the LLMs' performance in chemical reasoning tasks but also demonstrates the potential to integrate with other reasoning tools and strategies, thereby pushing the boundaries of what artificial intelligence can achieve in the realm of scientific inquiry.This advancement opens up new avenues for using LLMs in scientific research, education, and industry, where they can assist in acting as teaching agents, conducting experiments, or even in developing new materials and drugs.Furthermore, by highlighting the ongoing challenges and the need for further research in precise, grounded reasoning with LLMs, the paper sets a new direction for future work in AI and machine learning, encouraging a deeper investigation into how AI can more effectively mimic human-like reasoning in scientific domains.Overall, we do not foresee any major risks or negative societal impacts of our work.All the datasets we experiment with are publicly available online.We followed the licenses when conducting experiments on publicly available datasets and human annotations.We will open-source this project upon acceptance to facilitate future research, especially for small research groups or institutions with relatively fewer resources of LLMs.</p>
<p>A Prompts used for baseline methods in</p>
<p>Section 4.</p>
<p>In this section, we provide the detailed prompts used for experiments.Template-guided Prompt The full prompt for "formulae generation" and "step-by-step reasoning" is composed of four stages, general instruction, output format, demonstrations, and trigger.The complete view of the prompt is shown in Figure 14.</p>
<p>B Distribution of datasets</p>
<p>The detailed distribution of four datasets in terms of reasoning steps is shown in Fig 8 .We can see that the majority of the samples have reasoning steps spanning [3,5].Some samples even have reasoning steps of 8, which demonstrate the complexity of these datasets.</p>
<p>C Details for Section 5.3</p>
<p>Instruction for problem generation Please help me to generate complex and difficult chemistry Physical chemistry (atkins) (Atkins et al., 2023) provides explorations of equilibrium, structure, and reactions.We leverage GPT-4 to annotate each data sample in these datasets for the specific subfields.</p>
<p>problems that include but are not limited to the fields of physical chemistry, quantum chemistry, thermodynamics, atomic chemistry, molecular, etc.To help you better understand, I provide the following examples: [demonstrations].Following the above examples, please help me with this task and generate three problems that satisfy my requirements.Make sure the generated problems are reasonable and complex for solving.</p>
<p>Training details We use LLaMA-2-13Bchat (Touvron et al., 2023) and Vicuna-13B-v1.3 (Chiang et al., 2023) as backbone models and finetune them with the LoRA approach (Hu et al., 2022).During training, we configure the batch size to 8 and the maximum learning rate to 1e-4 with a 0.03 warmup ratio.For all the experiments, the LoRA r is set to 8, and we apply a dropout rate of 0.05.We keep these hyperparameters the same for a fair comparison.We train the models with 10 epochs and it takes around 1 hour to train on a single NVIDIA A6000 GPU.During the inference process, we also adhere to the same set of parameters: a temperature of 0.1, top_p of 0.75, top_k of 40, 4 beams, and a maximum generation length of 2,048.</p>
<p>Formula retrieval:</p>
<p>[Formula 1]  = 2( + 1), where  is the rotational constant,… [Formula 2]  = ℎ, where ℎ is the Planck's constant,… Reasoning: [Step 1] Calculate the energy difference for transition from 2 to 3.</p>
<p>Formula retrieval:</p>
<p>[Formula 1]  = ( !" #$ − )/ %,' , given formula for a van der … [Formula 2] Δ ' = ∫ , general formula for calculating change in enthalpy with [Formula 3] … Reasoning: [Step 1] … Take nitrogen to be a van der Waals gas with a = 1.352  (   )! and  = 0.0387  *  )+ , and calculate Δ ' when the pressure on the gas is decreased from 500  to 1.00  at 300  .For a van der Waals gas,  = (</p>
<p>D Examples of error type in Section 5.2</p>
<p>To help better understand the error category listed in Section 5.3, we provide one example for each category in Figure 12, Figure 13.</p>
<p>Formula retrieval:</p>
<p>[Formula 1]  = ℎ/, where  is the de Broglie wavelength,… [Formula 2]  = , where  is the kinetic energy,… [Formula 3]  = 1/2 !, where  is the mass of the particle,… Reasoning:</p>
<p>[</p>
<p>Step 1] Calculate the momentum of the proton using the de Broglie wavelength formula. = " # = 6.626×10 $%&amp; ×1.0×10 $'( = 6.626×10 $!&amp;    $' [Step 2] Calculate the kinetic energy of the proton using its momentum. =
) ! !* = +.+!+×'( "#$ ! ! '.+.%×'( "!% = 1.381×10 $'. 𝐽 [
Step 3] Calculate the potential difference using the kinetic energy formula. =
/ 0 = 1.381× '( "&amp;% '.+(! ×10 $'1 = 86.2 𝑉
Through what potential must a proton initially at rest fall so that its de Broglie wavelength is 1.0×10 $'( m ?Problem Answer Correct 0.082 V Figure 12: An example of a calculation error.The red highlighted expression is wrongly calculated.If calculated correctly, we will get K = 1.312 × 10 −20 , which yields the correct answer.</p>
<p>Formula retrieval:</p>
<p>[Formula 1] !=  "  # −  "  # $ , where  ! is the first ionization energy,… Reasoning: Use the  " value of  # 4.478 and the  " value of  # $ 2.651 to calculate the first ionization energy of  # (that is, the energy needed to remove an electron from  # ).The unit is .Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics.The problem will specify the unit of measurement, which should not be included in the answer.Express the final answer as a decimal number with three digits after the decimal point.Conclude the answer by stating "The answer is therefore \boxed{[ANSWER]}.</p>
<p>For each instance, you need to three things.Firstly, for "formulae retrieval", you need to identify the formulae explicitly and implicitly entailed in the problem context.Then there is a "reasoning/calculation process" where you are required to reason step by step based on the identified formulae and problem context.Finally, conclude the answer.For each problem, the output format should incorporate the following components in the corresponding format:</p>
<p>Instruction</p>
<p>To clearly explain the task, we provide the following example:</p>
<p>Problem: Assume that all gases are perfect and that data refer to 298.15 K unless otherwise stated.Calculate the change in chemical potential of a perfect gas when its pressure is increased isothermally from $1.8 \mathrm{~atm}$ to $29.5 \mathrm{~atm}$ at $40^{\circ} \mathrm{C}$.The unit of the answer is $\mathrm{kJ} \mathrm{mol}^{-1}$.</p>
<p>Response:</p>
<p>In order to solve this problem, we will use the formula for the change in chemical potential ( \Delta \mu ) of a perfect gas due to a change in pressure.</p>
<p>Given that the temperature is constant (isothermal), the chemical potential of a perfect gas is given by: <strong>Formula retrieval:</strong></p>
<p>Figure 1 :
1
Figure 1: Proportions (%) of four error types ( #errors / #all-cases) for GPT-4 and STRUCTCHEM.STRUCTCHEM substantially reduces reasoning error.</p>
<p>Figure 4 :
4
Figure 4: Instruction for formulae generation (part of the structured instruction in Figure 3) and the output for problem in Figure 2.</p>
<p>Figure 5 :
5
Figure 5: Instruction for reasoning process (part of the structured instruction in Figure 3) and the output for problem in Figure 2.</p>
<p>Algorithm 1 :
1
Confidence-based Reviewand-Refinement    Input: S0 = (F0, R0), initial confidence score C f 0 and C r 0 , LLM M, prompts {prev, pgen}, maximum iteration number n Output: Final answer to the problem P 1 F0 = {f1, ..., fn}, R0 = R0</p>
<p>Figure 6 :
6
Figure 6: Error analysis of four error categories across all datasets (y-axis) in terms of error proportions (xaxis).The results are for STRUCTCHEM w/o PoT to exclude the influence of external tools.</p>
<p>Figure 7 :
7
Figure 7: Cost-effectiveness analysis.The size of each dot is proportional to the average number of inferences by each method.The y-axis denotes the average accuracy across four datasets.</p>
<p>Figure 8 :
8
Figure 8: Distribution of four datasets in terms of the reasoning steps.</p>
<p>Figure 9 :
9
Figure9: Quantum chemistry (quan)(Hair et al., 2009) provides an exploration of equilibrium, structure, and reactions.Chemistry kinetics (matter)(Atkins et al., 2014) combines physics and mathematics, spanning through quantum mechanics and atomic structure.Quantum mechanics (chemmc) (McQuarrie, 2008) covers quantum mechanics and the applications in chemical bonding.Physical chemistry (atkins)(Atkins et al., 2023) provides explorations of equilibrium, structure, and reactions.We leverage GPT-4 to annotate each data sample in these datasets for the specific subfields.</p>
<p>[Figure 10 :
10
Figure 10: An example of a reasoning error.The red highlighted expression is deduced from the given formula.Instead of solving for difference, it directly plug the value for calculating energy.The correct expression for the first one should be ∆E 2→3 = 2B(3 + 1) − 2B(2 + 1).</p>
<p>Figure 11 :
11
Figure 11: An example of a factual error.The red highlighted expression is the wrong form to calculate.The correct formulae should be p ′ p dH m , which could be further converted to H m qx = H m (v ′ − d).</p>
<p>[</p>
<p>Step 1] Substitute the given values into the formulae and calculate the first ionization energy  != 1</p>
<p>Figure 13 :
13
Figure 13: An example of a principle error.The red highlighted expression is wrongly collected.The correct formulae should be E 1 = D 0 (H 2 ) − D 0 (H + 2 ) + I(H).</p>
<p>Figure 14: Full prompt used for generation.</p>
<p>Table 1 :
1
Results on the test sets for the four datasets, quan, chemmc, atkins, matter.We compare with baselines from two different settings, a zero-shot setting with no demonstrations and a few-shot setting with 3 demonstrations.We compute the accuracy scores with the approximation detailed in Section 4.3.The best results for each setting are highlighted in bold and the second-best results are underlined.
MethodsGPT-3.5Avg.GPT-4Avg.quanchemmcatkinsmatterquanchemmcatkinsmatterZero-shot settingDirect Reasoning5.8828.218.414.0811.658.8225.6414.9518.3716.95System Instruction8.8220.514.672.049.0114.7123.0827.1022.4521.84CoT2.9423.086.5410.2010.6914.7143.5928.0420.4126.69PoT0.07.690.02.042.4311.7620.5125.2316.3318.46STRUCTCHEM5.8815.389.3512.2410.7120.5938.4631.7824.4930.11Few-shot settingDirect Reasoning5.8823.089.358.1611.6214.7128.2120.6914.2919.48System Instruction11.7615.385.614.089.2117.6530.7715.8712.2419.13CoT8.8220.518.416.1210.9717.6546.1521.0526.5327.85PoT8.8233.3313.0816.3317.8938.2441.0321.0528.5732.22STRUCTCHEM32.3543.5926.1724.4931.6641.1858.9759.8130.6747.64</p>
<p>Table 2 :
2
Detailed statistics and information of the four datasets we experiment with.#P w and #P ∫ refer to the number of data samples with and without solutions.
DatasetsSubfields/Topics# Pw(Ps) # F # RSquanQuantum chemistry34 (8)1.93 3.94chemmc Quantum mechanics39 (9)1.88 3.95atkinsPhysical chemistry107 (16) 1.65 4.33matterChemistry kinetics49 (10)1.89 4.43
"# F" means the average number of formulae entailed in the problem, and "# RS" denotes the average reasoning steps for each problem.</p>
<p>Table 3 :
3
Ablation studies for different components in STRUCTCHEM in both zero-shot and few-shot settings with the backbone model GPT-4.The accuracy scores are reported with all four datasets.
MethodsZero-shotAvg.Few-shotAvg.quanchemmc atkins matterquanchemmc atkins matterstructured instruction 23.5335.9039.2518.3729.26 32.3551.2853.2728.5741.37+ review for F26.4738.4640.1918.3730.87 32.3548.7154.5530.6141.56+ review for R23.5341.0341.1222.4532.03 35.2951.2854.5530.6142.93+ confidence score 29.4141.0346.3423.0834.97 38.2453.8556.0732.6545.20+ PoT20.5938.4631.7824.4930.11 41.1858.9759.8130.6747.64</p>
<p>Table 4 :
4
Fine-tuning results with generations from STRUCTCHEM as training data on two open-source models.The accuracy scores are reported with all four datasets.
MethodsLlama-2-13B-chatAvg.Vicuna-13BAvg.quanchemmcatkinsmatterquanchemmcatkinsmatterZero-shot inference0.00.00.02.040.515.882.563.740.03.05Original0.05.130.00.01.288.825.132.802.044.70CoT8.8217.959.358.1611.0711.7615.388.416.1210.42STRUCTCHEM14.7130.7720.5620.4121.6117.6533.3318.6920.4122.52</p>
<p>Physical chemistry: quanta, matter, and change. P Atkins, J De Paula, R Friedman, 2014Oxford University PressUSA</p>
<p>Atkins' physical chemistry. P Atkins, J De Paula, J Keeler, 2023Oxford university press</p>
<p>Artificial intelligence in chemistry: current trends and future directions. Z J Baum, X Yu, P Y Ayala, Y Zhao, S P Watkins, Q Zhou, Journal of Chemical Information and Modeling. 6172021</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, T Lehmann, M Podstawski, H Niewiadomski, P Nyczyk, T Hoefler, 2023</p>
<p>A M Bran, S Cox, A D White, P Schwaller, Chemcrow, arXiv:2304.05376Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, Transactions on Machine Learning Research. 2835-88562023a</p>
<p>W Chen, Y Ming, K Max, W Elaine, X Ma, J Xu, T Xia, X Wang, P Lu, Theoremqa, arXiv:2305.12524A theorem-driven question answering dataset. 2023barXiv preprint</p>
<p>An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna, March 2023</p>
<p>Crossmodal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, Text2mol, 10.18653/v1/2021.emnlp-main.47Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W Yih, -T, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, Ji , H , arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Synergpt: In-context learning for personalized drug synergy prediction and drug design. C N Edwards, A Naik, T Khot, M D Burke, H Ji, T Hope, bioRxiv. 2023</p>
<p>Y Fang, X Liang, N Zhang, K Liu, R Huang, Z Chen, X Fan, H Chen, arXiv:2306.08018Mol-instructions: A large-scale biomolecular instruction dataset for large language models. 2023arXiv preprint</p>
<p>Potentialnet for molecular property prediction. E N Feinberg, D Sur, Z Wu, B E Husic, H Mai, Y Li, S Sun, J Yang, B Ramsundar, Pande , ACS central science. 4112018</p>
<p>Complexity-based prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, The Eleventh International Conference on Learning Representations. 2023</p>
<p>To read is the challenge -insights from 100 days, 100 papers reading challenge in chemistry education research. N Graulich, M Rost, M Schultz, M Gallardo-Williams, Journal of Chemical Education. 99102022</p>
<p>Pearson prentice-hall; upper saddle river, nj. Multivariate data analysis. J Hair, W Black, B Babin, R Anderson, R Tatham, 2009</p>
<p>Low-rank adaptation of large language models. E J Hu, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, International Conference on Learning Representations. 2022</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Large language models are zeroshot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Draw me a flower: Processing and grounding abstraction in natural language. R Lachmy, V Pyatkin, A Manevich, R Tsarfaty, 10.1162/tacl_a_00522Transactions of the Association for Computational Linguistics. 102022</p>
<p>C Liao, Y Yu, Y Mei, Y Wei, arXiv:2402.01439From words to molecules: A survey of large language models in chemistry. 2024arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Welleck, B P Majumder, S Gupta, A Yazdanbakhsh, P Clark, 10.48550/arXiv.2303.176512023</p>
<p>Quantum chemistry. D A Mcquarrie, 2008University Science Books</p>
<p>Exploring chemical space using natural language processing methodologies for drug discovery. H Öztürk, A Özgür, P Schwaller, T Laino, E Ozkirimli, Drug Discovery Today. 2542020</p>
<p>Is a question decomposition unit all we need. P Patel, S Mishra, M Parmar, C Baral, doi: 10.18653Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>URL. </p>
<p>Language models as knowledge bases?. F Petroni, T Rocktäschel, S Riedel, P Lewis, A Bakhtin, Y Wu, A Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). K Inui, J Jiang, V Ng, Wan , X , the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. K Stechly, M Marquez, S Kambhampati, arXiv:2310.123972023arXiv preprint</p>
<p>Recitation-augmented language models. Z Sun, X Wang, Y Tay, Y Yang, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Prioritizing safeguarding over autonomy: Risks of llm agents for science. X Tang, Q J Jin, K Zhu, T Yuan, Y Zhang, W Zhou, M Qu, Y Zhao, J Tang, Z Zhang, A Cohan, Z Lu, M Gerstein, arXiv:2402.042472024arXiv preprint</p>
<p>Students' understanding of chemical formulae: A review of empirical research. V Taskin, S Bernholt, International Journal of Science Education. 3612014</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Iteratively prompt pre-trained language models for chain of thought. B Wang, X Deng, H Sun, doi: 10.18653Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>URL. </p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, Scibench, arXiv:2307.10635Evaluating college-level scientific problem-solving abilities of large language models. 2023aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS. 2022</p>
<p>Large language models are reasoners with self-verification. Y Weng, M Zhu, S He, K Liu, J Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Analyzing learned molecular representations for property prediction. K Yang, K Swanson, W Jin, C Coley, P Eiden, H Gao, A Guzman-Perez, T Hopper, B Kelley, M Mathea, Journal of chemical information and modeling. 5982019</p>
<p>Tree of Thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, The Eleventh International Conference on Learning Representations. 2023ICLR 2023</p>
<p>Judging llm-asa-judge with mt-bench and chatbot arena. L Zheng, W Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 10.48550/arXiv.2306.056852023</p>
<p>Enhancing chemical reaction extraction with weak supervision. M Zhong, S Ouyang, M Jiang, V Hu, Y Jiao, X Wang, J Han, Reactie, doi: 10.18653Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>URL. </p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, E H Chi, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8179 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8179</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8179</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-280566240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.17001v2.pdf" target="_blank">PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents</a></p>
                <p><strong>Paper Abstract:</strong> Personalizing language models by effectively incorporating user interaction history remains a central challenge in the development of adaptive AI systems. While large language models (LLMs) combined with Retrieval-Augmented Generation (RAG) have improved factual accuracy, they often lack structured memory and fail to scale in complex, long-term interactions. To address this, we propose a flexible external memory framework based on knowledge graphs, automatically constructed and updated by the LLM itself, and capable of encoding information in multiple formats-including nodes, triplets, higher-order propositions, and episodic traces. Building upon the AriGraph architecture, we introduce a novel hybrid graph design that supports both standard edges and two types of hyperedges, enabling rich and dynamic semantic and temporal representations. Our framework also supports diverse retrieval mechanisms, including A*, water-circle propagation, beam search, and hybrid methods, making it adaptable to different datasets and LLM capacities. We evaluate our system on three benchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory and retrieval configurations yield optimal performance depending on the task. Additionally, we extend the DiaASQ benchmark with temporal annotations and internally contradictory statements, showing that our system remains robust and effective in managing temporal dependencies and context-aware reasoning.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8179.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8179.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PersonalAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PersonalAI knowledge-graph memory framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flexible external memory system for LLM agents that automatically constructs and maintains a hybrid knowledge graph (nodes, triplets, propositions, episodic traces) and supports multiple graph-traversal retrieval algorithms (A*, WaterCircles, BeamSearch, and hybrids) to supply context for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PersonalAI agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based QA/assistant pipeline augmented with an external, LLM-populated knowledge graph (entity nodes, simple relations, propositions as hyperedges, and episodic text nodes). The agent uses a Query LLM Parser to extract query entities, a Knowledge Comparator to map to graph nodes, graph traversal algorithms to extract candidate triplets, vector-based semantic filtering, and a QA LLM Generator for answer synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 7B; DeepSeek R1 7B; Llama3.1 8B; GPT4o-mini; DeepSeek V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various underlying LLMs evaluated as the core generative model in the pipeline; sizes reported include 7B/8B models and larger-scale models (≫14B) such as DeepSeek V3 and GPT4o-mini. Embeddings produced with multilingual E5-large.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DiaASQ, HotpotQA, TriviaQA (QA benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain / multi-hop question answering and dialogue-question answering: given a user question (and context drawn from dialogues or documents), the agent must retrieve relevant facts from memory and generate a correct natural-language answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured knowledge graph (episodic + semantic + propositional memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-extracted facts are stored in a hybrid knowledge graph; retrieval is performed by graph traversal algorithms (A* variants, WaterCircles BFS-style propagation, BeamSearch, or hybrid unions) followed by embedding-based semantic filtering (top-N). The graph is updated via LLM prompts that detect and replace outdated facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entity nodes, simple relation triples (subject-relation-object), higher-order propositions (hyperedges), episodic text fragments (episodic nodes), and edges/hyperedges connecting them.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph traversal (A* with several heuristics, WaterCircles breadth-first propagation with primary/secondary lists, BeamSearch path construction, or hybrid unions) plus vector-embedding similarity ranking to select top triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>LLM-as-a-Judge accuracy (JudgeScore) across evaluated configs: Qwen2.5 7B mean 0.27; DeepSeek R1 7B mean 0.19; Llama3.1 8B mean 0.44; GPT4o-mini mean 0.77; DeepSeek V3 mean 0.70 (scores are unitless accuracy/JudgeScore). Additionally, on comparisons with literature baselines the paper reports their method achieved +14.1% vs GraphRAG in a specific configuration but underperformed standard RAG baselines by 17.8% (ExactMatch metric) due to RAG baselines being fine-tuned in-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Extensive ablations over retrieval algorithm (A*, WaterCircles, BeamSearch, and hybrids) and graph traversal constraints (disallowing visits to episodic (E), thesis/propositional (T), or object/simple (O) nodes). Key comparisons: (1) BeamSearch-dominated top configurations for 7B/8B models, often with episodic-node or object-node traversal disabled; (2) hybrid A*+WaterCircles and BS+WaterCircles improve robustness and reduce sensitivity to traversal constraints; (3) BeamSearch is highly sensitive to traversal hyperparameters (JudgeScore variance up to ~24%), whereas combined A*+WC for 8B models had stability within ~4%; (4) NoAnswer frequency depends on algorithm and constraints (e.g., 7B models produced fewest NoAnswer outputs with A*+WC while larger models minimal NoAnswer with BS+WC and no constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured graph memory with configurable retrieval strategies improves QA when tuned to model scale and task: (a) for smaller LLMs (7B–8B), restricting episodic/object nodes and using BeamSearch often yields best accuracy; (b) for larger models, hybrid BeamSearch+WaterCircles gives higher stability and robustness; (c) thesis/propositional nodes frequently carry critical information—excluding them degrades performance (notably for small models); (d) hybrid traversal strategies reduce sensitivity to graph constraints and lower invalid/NoAnswer outputs; (e) compared to GraphRAG baselines the approach can substantially improve performance in some configs, but well-tuned/fine-tuned RAG systems may still outperform it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Parsing errors when extracting and storing graph entries (average parsing error rates: DiaASQ 7.0%, HotpotQA 6.3%, TriviaQA 7.3%); sensitivity of BeamSearch to hyperparameters and constraints; resource constraints limited running some QA pipeline configs (e.g., TriviaQA/GPT4o-mini not executed); overall underperformance versus in-domain fine-tuned RAG baselines (−17.8% ExactMatch) highlighting need for task-specific retriever/reader tuning; handling noisy episodic memories can harm small models; future work needed for temporal filtering and relation-type prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_canonical</strong></td>
                            <td>PersonalAI knowledge-graph memory framework</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8179.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8179.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AriGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior architecture that continuously maintains a structured knowledge base for LLM agents by extracting triplets from observations, pruning outdated facts, and integrating episodic and semantic nodes; used as the foundation for PersonalAI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AriGraph (baseline/architectural foundation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-agent architecture that constructs and maintains an episodic knowledge graph representing interactions and world state; supports episodic memory and planning in text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive text environments / memory-driven agent tasks (e.g., TextWorld-style environments referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain and use an episodic semantic memory (knowledge graph) to support agent planning and responses over interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>episodic memory for agents / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-extracted triples and episodic nodes stored in a graph; continuous maintenance (extraction, pruning, updating) performed by the system.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triplets, episodic nodes, semantic nodes, relations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph traversal and selection of subgraphs relevant to queries (the PersonalAI paper extends AriGraph with configurable retrieval algorithms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Used as a basis for the PersonalAI work; PersonalAI extends AriGraph by enabling multiple memory formats and adding retrieval algorithm ablations (A*, WaterCircles, BeamSearch, hybrids).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AriGraph provides a suitable foundation for structured episodic/semantic memory for LLM agents; extending it with multiple node types and retrieval strategies (as PersonalAI does) enables task/model-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not extensively re-evaluated in isolation within this paper; PersonalAI inherits the need to handle parsing errors and stale-memory detection.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_canonical</strong></td>
                            <td>AriGraph</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8179.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8179.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that augment parametric LLMs with non-parametric retrieval (retriever + reader) to improve factual recall by appending retrieved documents to model prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmented Generation for Knowledge-Intensive NLP Tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-style systems (retriever+reader)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Integrated retriever-reader pipelines that fetch relevant passages from a text corpus (often via dense retrieval) and condition an LLM to generate answers using the retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering / knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant documents or passages and generate factual answers conditioned on retrieved text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (non-parametric text store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retriever (dense or sparse) over text corpus + Reader LLM conditioned on concatenated retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved text passages/documents (vector indices or chunks).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dense vector retrieval and ranking (often with a trained retriever); retrieved content concatenated into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper compares PersonalAI against RAG literature baselines and reports that their method underperforms standard RAG baselines by ~17.8% ExactMatch in their comparative table, but notes that those RAG baselines were in-domain fine-tuned (which explains much of the difference).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG remains a strong baseline for QA when retrievers/readers are fine-tuned in-domain; structured graph memory can be competitive but may need additional retriever tuning to match in-domain RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Strictly unstructured retrieval (raw passages) limits semantic relationship modeling and temporal reasoning compared to structured graph memories; fine-tuned retrievers give RAG a large advantage that must be accounted for in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_canonical</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8179.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8179.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG / Graph-based RAG approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that augment LLMs with retrieval over knowledge graphs or structured external memories, combining graph-structured information with retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphRAG-style systems</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Methods that replace or augment text retrieval with graph-structured retrieval (knowledge-graph querying/traversal) to provide structured context to an LLM reader.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering with structured memory</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use graph-structured memories to retrieve and assemble facts needed to answer questions, supporting reasoning that benefits from explicit relations and temporal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / graph-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (structured external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Traversal and subgraph extraction from knowledge graphs; may include path-based retrieval and graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Nodes, relations, triples, and sometimes higher-order propositions or episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph algorithms, path-finding, and sometimes embedding-similarity filtering; PersonalAI implements several such methods (A*, WaterCircles, BeamSearch).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>PersonalAI reports a +14.1% improvement over existing GraphRAG approaches in a specific configuration (ExactMatch) in Table 16, indicating certain graph designs and retrieval strategies can outperform prior GraphRAG variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-structured memory can improve temporal and relational reasoning; retrieval algorithm and node-type design critically affect effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GraphRAG performance varies substantially with traversal heuristics and graph design; prior GraphRAGs can be outperformed by well-tuned hybrid traversal plus richer node types.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_canonical</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8179.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8179.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that constructs semantic graphs for personalized QA, inspired by neurobiological long-term memory mechanisms; included in comparative evaluation and reproduced results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A graph-based personalized memory approach that creates semantic graphs to augment QA performance, drawing inspiration from neurobiological memory structures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DiaASQ, HotpotQA (reproduced comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Personalized question answering over dialogue and multi-hop QA benchmarks using semantic graph memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / personalized QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic knowledge graph / personalized memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Construction of semantic graphs from dialogue/context and retrieval from those graphs to condition LLM answers (specific mechanisms described in HippoRAG paper; PersonalAI reproduced its performance numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Semantic graph nodes and relations representing user/dialogue content.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph-based retrieval; specifics from HippoRAG are cited but not fully reimplemented in-paper; PersonalAI reproduces HippoRAG metrics for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reproduced results reported in the paper: DiaASQ JudgeScore -0.53; HotpotQA ExactMatch 60.2 (values reported in Table H reproduction results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>HippoRAG was reproduced and compared; PersonalAI reports comparable or superior results to HippoRAG depending on dataset/configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>HippoRAG-style semantic graph memories can be strong baselines for personalized QA; PersonalAI can match or outperform HippoRAG in some settings with richer node types and hybrid retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct comparisons depend on dataset splits and evaluation metrics; PersonalAI notes dataset and evaluation preprocessing choices affect comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full_canonical</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents. <em>(Rating: 2)</em></li>
                <li>Augmented Generation for Knowledge-Intensive NLP Tasks. <em>(Rating: 2)</em></li>
                <li>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. <em>(Rating: 2)</em></li>
                <li>GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>MemWalker <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8179",
    "paper_id": "paper-280566240",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "PersonalAI",
            "name_full": "PersonalAI knowledge-graph memory framework",
            "brief_description": "A flexible external memory system for LLM agents that automatically constructs and maintains a hybrid knowledge graph (nodes, triplets, propositions, episodic traces) and supports multiple graph-traversal retrieval algorithms (A*, WaterCircles, BeamSearch, and hybrids) to supply context for QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PersonalAI agent",
            "agent_description": "An LLM-based QA/assistant pipeline augmented with an external, LLM-populated knowledge graph (entity nodes, simple relations, propositions as hyperedges, and episodic text nodes). The agent uses a Query LLM Parser to extract query entities, a Knowledge Comparator to map to graph nodes, graph traversal algorithms to extract candidate triplets, vector-based semantic filtering, and a QA LLM Generator for answer synthesis.",
            "model_name": "Qwen2.5 7B; DeepSeek R1 7B; Llama3.1 8B; GPT4o-mini; DeepSeek V3",
            "model_description": "Various underlying LLMs evaluated as the core generative model in the pipeline; sizes reported include 7B/8B models and larger-scale models (≫14B) such as DeepSeek V3 and GPT4o-mini. Embeddings produced with multilingual E5-large.",
            "task_name": "DiaASQ, HotpotQA, TriviaQA (QA benchmarks)",
            "task_description": "Open-domain / multi-hop question answering and dialogue-question answering: given a user question (and context drawn from dialogues or documents), the agent must retrieve relevant facts from memory and generate a correct natural-language answer.",
            "task_type": "question answering",
            "memory_used": true,
            "memory_type": "external structured knowledge graph (episodic + semantic + propositional memory)",
            "memory_mechanism": "LLM-extracted facts are stored in a hybrid knowledge graph; retrieval is performed by graph traversal algorithms (A* variants, WaterCircles BFS-style propagation, BeamSearch, or hybrid unions) followed by embedding-based semantic filtering (top-N). The graph is updated via LLM prompts that detect and replace outdated facts.",
            "memory_representation": "Entity nodes, simple relation triples (subject-relation-object), higher-order propositions (hyperedges), episodic text fragments (episodic nodes), and edges/hyperedges connecting them.",
            "memory_retrieval_method": "Graph traversal (A* with several heuristics, WaterCircles breadth-first propagation with primary/secondary lists, BeamSearch path construction, or hybrid unions) plus vector-embedding similarity ranking to select top triplets.",
            "performance_with_memory": "LLM-as-a-Judge accuracy (JudgeScore) across evaluated configs: Qwen2.5 7B mean 0.27; DeepSeek R1 7B mean 0.19; Llama3.1 8B mean 0.44; GPT4o-mini mean 0.77; DeepSeek V3 mean 0.70 (scores are unitless accuracy/JudgeScore). Additionally, on comparisons with literature baselines the paper reports their method achieved +14.1% vs GraphRAG in a specific configuration but underperformed standard RAG baselines by 17.8% (ExactMatch metric) due to RAG baselines being fine-tuned in-domain.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Extensive ablations over retrieval algorithm (A*, WaterCircles, BeamSearch, and hybrids) and graph traversal constraints (disallowing visits to episodic (E), thesis/propositional (T), or object/simple (O) nodes). Key comparisons: (1) BeamSearch-dominated top configurations for 7B/8B models, often with episodic-node or object-node traversal disabled; (2) hybrid A*+WaterCircles and BS+WaterCircles improve robustness and reduce sensitivity to traversal constraints; (3) BeamSearch is highly sensitive to traversal hyperparameters (JudgeScore variance up to ~24%), whereas combined A*+WC for 8B models had stability within ~4%; (4) NoAnswer frequency depends on algorithm and constraints (e.g., 7B models produced fewest NoAnswer outputs with A*+WC while larger models minimal NoAnswer with BS+WC and no constraints).",
            "key_findings": "Structured graph memory with configurable retrieval strategies improves QA when tuned to model scale and task: (a) for smaller LLMs (7B–8B), restricting episodic/object nodes and using BeamSearch often yields best accuracy; (b) for larger models, hybrid BeamSearch+WaterCircles gives higher stability and robustness; (c) thesis/propositional nodes frequently carry critical information—excluding them degrades performance (notably for small models); (d) hybrid traversal strategies reduce sensitivity to graph constraints and lower invalid/NoAnswer outputs; (e) compared to GraphRAG baselines the approach can substantially improve performance in some configs, but well-tuned/fine-tuned RAG systems may still outperform it.",
            "limitations_or_challenges": "Parsing errors when extracting and storing graph entries (average parsing error rates: DiaASQ 7.0%, HotpotQA 6.3%, TriviaQA 7.3%); sensitivity of BeamSearch to hyperparameters and constraints; resource constraints limited running some QA pipeline configs (e.g., TriviaQA/GPT4o-mini not executed); overall underperformance versus in-domain fine-tuned RAG baselines (−17.8% ExactMatch) highlighting need for task-specific retriever/reader tuning; handling noisy episodic memories can harm small models; future work needed for temporal filtering and relation-type prioritization.",
            "name_full_canonical": "PersonalAI knowledge-graph memory framework",
            "uuid": "e8179.0",
            "source_info": {
                "paper_title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "AriGraph",
            "name_full": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents",
            "brief_description": "A prior architecture that continuously maintains a structured knowledge base for LLM agents by extracting triplets from observations, pruning outdated facts, and integrating episodic and semantic nodes; used as the foundation for PersonalAI.",
            "citation_title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.",
            "mention_or_use": "use",
            "agent_name": "AriGraph (baseline/architectural foundation)",
            "agent_description": "An LLM-agent architecture that constructs and maintains an episodic knowledge graph representing interactions and world state; supports episodic memory and planning in text environments.",
            "model_name": null,
            "model_description": null,
            "task_name": "Interactive text environments / memory-driven agent tasks (e.g., TextWorld-style environments referenced)",
            "task_description": "Maintain and use an episodic semantic memory (knowledge graph) to support agent planning and responses over interactions.",
            "task_type": "episodic memory for agents / long-context reasoning",
            "memory_used": true,
            "memory_type": "episodic knowledge graph",
            "memory_mechanism": "LLM-extracted triples and episodic nodes stored in a graph; continuous maintenance (extraction, pruning, updating) performed by the system.",
            "memory_representation": "Triplets, episodic nodes, semantic nodes, relations.",
            "memory_retrieval_method": "Graph traversal and selection of subgraphs relevant to queries (the PersonalAI paper extends AriGraph with configurable retrieval algorithms).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Used as a basis for the PersonalAI work; PersonalAI extends AriGraph by enabling multiple memory formats and adding retrieval algorithm ablations (A*, WaterCircles, BeamSearch, hybrids).",
            "key_findings": "AriGraph provides a suitable foundation for structured episodic/semantic memory for LLM agents; extending it with multiple node types and retrieval strategies (as PersonalAI does) enables task/model-specific tuning.",
            "limitations_or_challenges": "Not extensively re-evaluated in isolation within this paper; PersonalAI inherits the need to handle parsing errors and stale-memory detection.",
            "name_full_canonical": "AriGraph",
            "uuid": "e8179.1",
            "source_info": {
                "paper_title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A family of methods that augment parametric LLMs with non-parametric retrieval (retriever + reader) to improve factual recall by appending retrieved documents to model prompts.",
            "citation_title": "Augmented Generation for Knowledge-Intensive NLP Tasks.",
            "mention_or_use": "mention",
            "agent_name": "RAG-style systems (retriever+reader)",
            "agent_description": "Integrated retriever-reader pipelines that fetch relevant passages from a text corpus (often via dense retrieval) and condition an LLM to generate answers using the retrieved context.",
            "model_name": null,
            "model_description": null,
            "task_name": "Open-domain question answering / knowledge-intensive NLP tasks",
            "task_description": "Retrieve relevant documents or passages and generate factual answers conditioned on retrieved text.",
            "task_type": "question answering / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "retrieval-augmented (non-parametric text store)",
            "memory_mechanism": "Retriever (dense or sparse) over text corpus + Reader LLM conditioned on concatenated retrieved passages.",
            "memory_representation": "Retrieved text passages/documents (vector indices or chunks).",
            "memory_retrieval_method": "Dense vector retrieval and ranking (often with a trained retriever); retrieved content concatenated into prompts.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Paper compares PersonalAI against RAG literature baselines and reports that their method underperforms standard RAG baselines by ~17.8% ExactMatch in their comparative table, but notes that those RAG baselines were in-domain fine-tuned (which explains much of the difference).",
            "key_findings": "RAG remains a strong baseline for QA when retrievers/readers are fine-tuned in-domain; structured graph memory can be competitive but may need additional retriever tuning to match in-domain RAG.",
            "limitations_or_challenges": "Strictly unstructured retrieval (raw passages) limits semantic relationship modeling and temporal reasoning compared to structured graph memories; fine-tuned retrievers give RAG a large advantage that must be accounted for in comparisons.",
            "name_full_canonical": "Retrieval-Augmented Generation",
            "uuid": "e8179.2",
            "source_info": {
                "paper_title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG / Graph-based RAG approaches",
            "brief_description": "Approaches that augment LLMs with retrieval over knowledge graphs or structured external memories, combining graph-structured information with retrieval-augmented generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "GraphRAG-style systems",
            "agent_description": "Methods that replace or augment text retrieval with graph-structured retrieval (knowledge-graph querying/traversal) to provide structured context to an LLM reader.",
            "model_name": null,
            "model_description": null,
            "task_name": "Question answering with structured memory",
            "task_description": "Use graph-structured memories to retrieve and assemble facts needed to answer questions, supporting reasoning that benefits from explicit relations and temporal structure.",
            "task_type": "question answering / graph-based retrieval",
            "memory_used": true,
            "memory_type": "knowledge graph (structured external memory)",
            "memory_mechanism": "Traversal and subgraph extraction from knowledge graphs; may include path-based retrieval and graph reasoning.",
            "memory_representation": "Nodes, relations, triples, and sometimes higher-order propositions or episodes.",
            "memory_retrieval_method": "Graph algorithms, path-finding, and sometimes embedding-similarity filtering; PersonalAI implements several such methods (A*, WaterCircles, BeamSearch).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "PersonalAI reports a +14.1% improvement over existing GraphRAG approaches in a specific configuration (ExactMatch) in Table 16, indicating certain graph designs and retrieval strategies can outperform prior GraphRAG variants.",
            "key_findings": "Graph-structured memory can improve temporal and relational reasoning; retrieval algorithm and node-type design critically affect effectiveness.",
            "limitations_or_challenges": "GraphRAG performance varies substantially with traversal heuristics and graph design; prior GraphRAGs can be outperformed by well-tuned hybrid traversal plus richer node types.",
            "name_full_canonical": "GraphRAG",
            "uuid": "e8179.3",
            "source_info": {
                "paper_title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "HippoRAG",
            "name_full": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
            "brief_description": "A prior method that constructs semantic graphs for personalized QA, inspired by neurobiological long-term memory mechanisms; included in comparative evaluation and reproduced results in this paper.",
            "citation_title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models.",
            "mention_or_use": "use",
            "agent_name": "HippoRAG",
            "agent_description": "A graph-based personalized memory approach that creates semantic graphs to augment QA performance, drawing inspiration from neurobiological memory structures.",
            "model_name": null,
            "model_description": null,
            "task_name": "DiaASQ, HotpotQA (reproduced comparison)",
            "task_description": "Personalized question answering over dialogue and multi-hop QA benchmarks using semantic graph memory.",
            "task_type": "question answering / personalized QA",
            "memory_used": true,
            "memory_type": "semantic knowledge graph / personalized memory",
            "memory_mechanism": "Construction of semantic graphs from dialogue/context and retrieval from those graphs to condition LLM answers (specific mechanisms described in HippoRAG paper; PersonalAI reproduced its performance numbers).",
            "memory_representation": "Semantic graph nodes and relations representing user/dialogue content.",
            "memory_retrieval_method": "Graph-based retrieval; specifics from HippoRAG are cited but not fully reimplemented in-paper; PersonalAI reproduces HippoRAG metrics for comparison.",
            "performance_with_memory": "Reproduced results reported in the paper: DiaASQ JudgeScore -0.53; HotpotQA ExactMatch 60.2 (values reported in Table H reproduction results).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "HippoRAG was reproduced and compared; PersonalAI reports comparable or superior results to HippoRAG depending on dataset/configuration.",
            "key_findings": "HippoRAG-style semantic graph memories can be strong baselines for personalized QA; PersonalAI can match or outperform HippoRAG in some settings with richer node types and hybrid retrieval.",
            "limitations_or_challenges": "Direct comparisons depend on dataset splits and evaluation metrics; PersonalAI notes dataset and evaluation preprocessing choices affect comparability.",
            "name_full_canonical": "HippoRAG",
            "uuid": "e8179.4",
            "source_info": {
                "paper_title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents.",
            "rating": 2,
            "sanitized_title": "arigraph_learning_knowledge_graph_world_models_with_episodic_memory_for_llm_agents"
        },
        {
            "paper_title": "Augmented Generation for Knowledge-Intensive NLP Tasks.",
            "rating": 2,
            "sanitized_title": "augmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models.",
            "rating": 2,
            "sanitized_title": "hipporag_neurobiologically_inspired_longterm_memory_for_large_language_models"
        },
        {
            "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "graphreader_building_graphbased_agent_to_enhance_longcontext_abilities_of_large_language_models"
        },
        {
            "paper_title": "MemWalker",
            "rating": 1
        }
    ],
    "cost": 0.01456475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents
11 Aug 2025</p>
<p>Mikhail Menschikov 
Skoltech, MoscowRussia</p>
<p>Dmitry Evseev 
Skoltech, MoscowRussia</p>
<p>Victoria Dochkina 
Public joint stock company "Sberbank of Russia"
MoscowRussia</p>
<p>Ruslan Kostoev 
Public joint stock company "Sberbank of Russia"
MoscowRussia</p>
<p>Ilia Perepechkin 
Public joint stock company "Sberbank of Russia"
MoscowRussia</p>
<p>Petr Anokhin 
AIRI
MoscowRussia</p>
<p>Evgeny Burnaev 
Skoltech, MoscowRussia</p>
<p>Nikita Semenov 
Skoltech, MoscowRussia</p>
<p>PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents
11 Aug 20251CEED81A0C904B3E10CB827A2945F0BCarXiv:2506.17001v2[cs.CL]
Personalizing language models by effectively incorporating user interaction history remains a central challenge in the development of adaptive AI systems.While large language models (LLMs) combined with Retrieval-Augmented Generation (RAG) have improved factual accuracy, they often lack structured memory and fail to scale in complex, long-term interactions.To address this, we propose a flexible external memory framework based on knowledge graphs, automatically constructed and updated by the LLM itself, and capable of encoding information in multiple formats-including nodes, triplets, higher-order propositions, and episodic traces.Building upon the AriGraph architecture, we introduce a novel hybrid graph design that supports both standard edges and two types of hyperedges, enabling rich and dynamic semantic and temporal representations.Our framework also supports diverse retrieval mechanisms, including A*, watercircle propagation, beam search, and hybrid methods, making it adaptable to different datasets and LLM capacities.We evaluate our system on three benchmarks-TriviaQA, Hot-potQA, and DiaASQ-demonstrating that different memory and retrieval configurations yield optimal performance depending on the task.Additionally, we extend the DiaASQ benchmark with temporal annotations and internally contradictory statements, showing that our system remains robust and effective in managing temporal dependencies and context-aware reasoning.1. We present a highly flexible external memory architecture that supports multiple structured formats (nodes, triplets, propositions, episodes) and can be tuned via orthogonal hyperparameters for storage and retrieval.2. We propose and evaluate six retrieval over structured knowledge graphs, achieving superior performance across various datasets compared to Graph-RAG baselines.3. We enhance the DiaASQ benchmark by incorporating temporal structures into dialogue representations and demonstrate that our framework can leverage such structures to improve temporal reasoning.This work provides a general and extensible framework for integrating long-term memory and adaptive reasoning into LLM agents, advancing the state of personalized and context-aware language generation.</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in personalized AI systems capable of adapting to users based on their interaction history.Central to personalization is the challenge of encoding, storing, and retrieving relevant information over long time horizons in a manner that supports efficient reasoning and response generation.While Retrieval-Augmented Generation (RAG) has become a widely used solution, enhancing factual recall by appending retrieved content to prompts, it remains limited by its unstructured nature and weak support for semantic relationships across stored memories.</p>
<p>In this work, we introduce a flexible graph-based memory framework designed to overcome these limitations by enabling structured, customizable representations of longterm memory and supporting advanced reasoning capabilities.Unlike traditional RAG pipelines that rely on dense vec-tor similarity over raw text chunks, our system supports multiple memory formats-nodes, knowledge triplets, higherorder propositions, and episodic traces-and dynamically organizes them into a knowledge graph.This structure allows the agent to represent, update, and access semantic and temporal relationships with far greater control and interpretability.</p>
<p>Equally important, our framework supports a pluggable retrieval interface with multiple reasoning mechanisms, including three variations of A* search, WaterCircle propagation, BeamSearch, and hybrid strategies to adapt retrieval behavior to task demands and model capacity.We demonstrate that different memory and retrieval configurations yield optimal performance on different benchmarks and LLM scales, highlighting the versatility of our approach.We build our system on top of the AriGraph architecture (Anokhin et al. 2024), originally developed for LLM agents in interactive text environments such as TextWorld.Ari-Graph continuously maintains a structured knowledge base by extracting triplets from observations, pruning outdated or redundant facts, and integrating episodic and semantic nodes into a unified graph.Our framework extends this foundation by allowing customization of both save (memorization) and search (retrieval) modules, supporting task-specific tuning and component evaluation.</p>
<p>In summary, our main contributions are as follows:</p>
<p>2 Related Work</p>
<p>In recent years, substantial advancements have been made in open-domain question answering (QA) and the personalization of language models.Techniques leveraging Wikipedia as a broad knowledge source (Chen et al. 2017) have successfully incorporated large-scale machine reading, integrating document retrieval with textual comprehension.Furthermore, dense representation-based methods for passage retrieval (Karpukhin et al. 2020) have demonstrated superior efficacy compared to traditional sparse retrieval approaches, such as TF-IDF and BM25, particularly in scenarios with sufficient training data.</p>
<p>The development of dense retrievers has demonstrated significant progress, particularly through the application of contrastive learning in unsupervised settings, which has shown promising results across diverse scenarios and outperformed conventional approaches such as BM25 (Robertson, Zaragoza et al. 2009) and Contriever (Izacard et al. 2021).Concurrently, pre-trained language models incorporating non-parametric memory access have been proposed for knowledge-intensive tasks.A notable example is retrieval-augmented generation (RAG) models, which integrate parametric and non-parametric memory mechanisms to improve question-answering performance (Lewis et al. 2021).</p>
<p>Recent advancements in unsupervised dense retrieval models, such as ART, have demonstrated the ability to achieve state-of-the-art performance while eliminating reliance on labeled training data (Sachan et al. 2023).In the domain of knowledge graph-based approaches, frameworks such as GraphReader (Li et al. 2024)) incorporate structured reasoning mechanisms to facilitate knowledge extraction and representation, with a particular emphasis on enhancing long-context reasoning capabilities.</p>
<p>For personalized models, AriGraph (Anokhin et al. 2024) introduces a framework that integrates episodic memory and long-term planning using knowledge graphs.Similarly, HippoRAG (Gutiérrez et al. 2024) employs personalized algorithms to improve question-answering (QA) performance by constructing semantic graphs, demonstrating notable advancements over conventional extraction methods.MemWalker (Chen et al. 2023) and PEARL (Sarthi et al. 2024) address challenges associated with context length, proposing architectures capable of efficiently traversing and consolidating information from large-scale documents.</p>
<p>Additionally, ReadAgent (Lee et al. 2024) addresses the challenge of processing long-text contexts by structuring content into memory episodes.Meanwhile, KGP (Wang et al. 2023) proposes Knowledge Graph Prompting, a method that enhances multi-document question answering by constructing knowledge graphs to improve contextual reasoning.</p>
<p>These advancements demonstrate a sustained emphasis on improving language models' comprehension, retrieval, and personalization capabilities.Such progress facilitates the development of more sophisticated systems, which leverages knowledge graphs to deliver personalized and contextually enriched interactions.</p>
<p>Methods</p>
<p>Graph Construction</p>
<p>In this study, we employ a graph-based knowledge base as external memory to enhance a large language model's (LLM) question-answering capabilities.To capture and structure information from weakly structured sources, specifically, unstructured natural language texts, we construct a knowledge graph.This graph provides a comprehensive representation of the original text and comprises the following elements: 1. Entity nodes, which represent atomic concepts extracted from the text (depicted as green entities in Figure 1); 2. Simple relations, which capture direct associations between entities (illustrated by connections between green nodes in Figure 1); 3. Propositions, which encapsulate complete atomic thoughts expressed in the text and function as hyperconnections among arbitrary sets of nodes (represented by yellow entities and connections in Figure 1); 4. Episodic memories, which preserve the original text passages and serve as hyperconnections linking relevant nodes (denoted by blue entities and connections in Figure 1).</p>
<p>In the terminology defined above, the process of constructing a knowledge graph from weakly structured sources can be decomposed into three key steps: (1) formulating nodes and their relationships, (2) generating propositions using an LLM, and (3) parsing the model's output to store the extracted information in a structured format (e.g., as subject-relation-object triples).An example prompt for extracting atomic statements and thesis-level claims from textual sources using an LLM is provided in Appendix A.</p>
<p>The identification and retrieval of outdated information within the graph are performed through the following procedure.First, the entities present in the newly extracted triplets and thesis statements are compared against existing nodes in the graph to detect matches.Upon identifying matching nodes, these serve as the initial set for a breadth-first search (BFS), which traverses all associated connections and hyperconnections originating from or terminating at these nodes.Subsequently, a specialized prompt is employed to instruct the LLM to update the retrieved knowledge with the newly extracted data (see Appendix B for the corresponding LLM prompts).If any triplets or thesis statements are successfully updated, the outdated instances are removed from the graph.</p>
<p>Information Searching</p>
<p>Information search pipline on knowledge graph is shown in Figure 2.</p>
<p>As illustrated in Figure 2, the pipeline comprises four primary stages, with the third stage further subdivided into two substages.The pipeline begins by accepting a natural language question as input, which is then processed by the "Query LLM Parser" module to extract key entities.In the second stage, these entities are passed to the "Knowledge Comparator" module, where they are aligned with corresponding nodes in the knowledge graph.The third stage involves two sequential operations: retrieval and filtering of knowledge graph triplets based on semantic relevance to the input question.First, the "Graph Triplets Retriever" module initiates a graph traversal algorithm, using the matched entities from the second stage as starting vertices to retrieve a set of candidate triplets.Subsequently, the "Triplets Filter" module ranks these triplets by computing their semantic similarity to the question via vector embeddings, retaining only the top N (a predefined hyperparameter) most relevant triplets.</p>
<p>Finally, in the fourth stage, the "QA LLM Generator" module synthesizes a natural language answer conditioned on the filtered triplets.The output of the pipeline is the generated answer, returned as a string.</p>
<p>This pipeline architecture is designed based on three key considerations.First, to achieve an accurate initial approximation of the relevant subgraph, it is essential to align the key entities in the query with semantically similar entities in the existing knowledge base.The reasoning behind this is that the information necessary for generating a correct response is typically localized within the subgraph containing these key entities.Second, the triplets extracted from the knowledge graph exhibit only weak conditioning on the original query, limiting their direct applicability.Third, large language models (LLMs) are constrained by a fixed maximum sequence length for processing inputs in a single inference step, necessitating efficient retrieval and subgraph selection strategies.</p>
<p>Retrieve Algorithms</p>
<p>The primary role of the constructed knowledge graph in our proposed system is to enable the accurate retrieval of information required for responding to specific user queries.This process necessitates a careful optimization between two key criteria: relevance and completeness.Relevance ensures that all retrieved information is directly pertinent to the query, whereas completeness guarantees the inclusion of all necessary contextual data, even if some retrieved elements may be extraneous.Consequently, striking an optimal balance between these factors is essential for achieving efficient and effective knowledge extraction.</p>
<p>To accomplish this, we design and implement advanced retrieval algorithms capable of dynamically balancing the trade-off between completeness and relevance through configurable parameter settings.These algorithms constitute a critical component of the question-answering pipeline, systematically traversing the knowledge graph to aggregate information essential for generating precise and contextually appropriate responses.</p>
<p>A<em>.The A</em> algorithm is a widely used method for graph traversal, particularly valued for its ability to efficiently identify shortest paths between nodes in a graph.In the context of our question-answering pipeline, this algorithm extracts and retains triples encountered along these shortest paths while eliminating duplicates based on content.We hypothesize that the triples obtained through this process contain the necessary information for generating accurate responses.For traversal, we treat the graph as unweighted and undirected, using a constant distance metric between adjacent nodes.To optimize pathfinding efficiency, we evaluate three distinct heuristics for the h-metric: 1. Inner Product (IP): This heuristic computes the dot product of embeddings between the current node and the target node.2. Weighted Shortest Path: This approach scales the inner product metric by the length of the shortest path, determined via Breadth-First Search (BFS).3. Averaged Weighted Shortest Path: This heuristic calculates the average of inner product distances between adjacent nodes along the path from the start to the current node, as well as the direct path from the current node to the target node, and further weights this value by the BFS-derived shortest path length.</p>
<p>WaterCircles.This graph-based extraction method employs a breadth-first search (BFS) algorithm to retrieve relevant knowledge.Query entities are first mapped to their corresponding nodes in the graph.The algorithm begins by exploring adjacent vertices to these entity nodes and iteratively expands to neighboring vertices in subsequent steps, constructing pathways that radiate outward from the initial entities.When pathways originating from different entities intersect, the triplets formed at these intersections are aggregated into a primary list, while all traversed triplets are compiled into a secondary list.The algorithm ultimately returns a subset of triplets, selecting N from the primary list and K from the secondary list, where N and K are configurable hyperparameters.</p>
<p>When the knowledge graph incorporates not only direct entity relations (i.e., "simple" connections) but also associations between entities and text fragments (referred to as "hyper" and "episodic" connections), a modified breadth-first search (BFS) algorithm is employed as follows: 1. Traversal Initialization: The search begins at the entities referenced in the query, following the same proce-dure as in the case of simple entity relations.2. Text Fragment Analysis: During traversal, identified text fragments are examined for occurrences of other query entities distinct from the entity that originated the path to the given fragment.For each fragment, the number of detected entities, denoted as N intersections , is computed.3. Ranking Triplets: The list of "hyper" and "episodic" triplets is then sorted in descending order based on N intersections .This triplet extraction strategy aims to enhance relevance and accuracy in retrieving information from the graphstructured knowledge base, thereby bolstering the effectiveness of AI-driven question-answering systems.</p>
<p>BeamSearch.Given a starting node, the algorithm constructs N (a hyperparameter) semantically relevant paths in response to the input query.This approach is inspired by beam search, a token generation strategy commonly employed in large language model (LLM) inference.The resulting paths are consolidated into a single list, with duplicate triplets removed based on their string content.The traversal process is governed by the following hyperparameters: 1. Max depth: The maximum allowable depth for path construction.</p>
<p>Max paths:</p>
<p>The maximum number of paths to generate.(1) ended paths -path terminated before reaching the depth limit and (2) continuous paths -paths that reached the depth limit.Each path is assigned a relevance score.The selection process varies based on the chosen mode.If ended first-value is specified, then: ended-paths will be sorted in descending order by relevance and the first max paths-paths will be selected.If there are fewer ended-paths than max paths-value, then continuous-paths will be sorted by relevance and the first N missing paths will be selected from them.If continuous first-value is specified, then paths will be selected in the same way as ended first-value, only first sorting/selection by continuous-paths, and then by ended-paths.</p>
<p>If mixed-value is specified, then ended-and continuouspaths will be combined into one list, sorted in descending order by relevance, and the first N max paths-paths will be selected from the resulting list.Mixed Algorithm.The mixed algorithm integrates the A<em>, WaterCircles, and BeamSearch strategies to enhance extraction efficacy.By combining these approaches, the algorithm ensures that triplets not captured by one method (e.g., WaterCircles) may still be retrieved by another (e.g., Beam-Search), thereby improving overall recall.The final set of triplets, which is subsequently passed to the LLM for answer generation, constitutes the union of outputs derived from the A</em>, WaterCircles, and BeamSearch algorithms.</p>
<p>By evaluating these diverse algorithms, this study highlights advancements in extracting pertinent information from knowledge graphs, thereby supporting the robust architectural framework required for personalizing responses in large language model (LLM) agents.</p>
<p>Experiment Set-Up Datasets</p>
<p>The proposed retrieval algorithms are evaluated on the Di-aASQ dataset (Li et al. 2023), which comprises user dialogues from a Chinese forum discussing the characteristics of various mobile devices.The dataset also includes structured statements that encapsulate the core semantic content of these dialogues.Leveraging these annotated "true statements", a set of evaluation questions was procedurally generated to assess the performance of the proposed algorithms.</p>
<p>In addition to the specialized DiaASQ dataset, the Hot-potQA and TriviaQA datasets were selected to assess the performance of the proposed QA pipeline.Using these benchmarks, the functionality implemented in the Person-alAI library is systematically compared against existing Retrieval-Augmented Generation (RAG) and GraphRAG approaches.</p>
<p>As a result of preprocessing initial datasets, their subsamples are used to construct knowledge graphs and evaluate proposed pipeline:</p>
<p>• DiaASQ: number of qa-pairs -4800; number of relevant contexts -3483, • HotpotQA: number of qa-pairs -2000; number of relevant contexts -3933; • TriviaQA: number of qa-pairs -500; number of relevant contexts -4925.</p>
<p>More details about preprocessing methods and numerical characteristics of resulted datasets can be observed in Appendix E</p>
<p>Models</p>
<p>For graph construction (Memorize pipeline) and information retrieval (QA pipeline), we evaluate a series of 7B/8B and large-scale (≫ 14B) language models to assess their performance in handling these tasks.The selected models include Qwen2.5 7B, DeepSeek R1 7B, Llama3.1 8B, GPT4o-min, and DeepSeek V3.To generate vector representations (embeddings) of natural language text data, we employ the multilingual E5-large model 1 .</p>
<p>Retrieval Algorithms</p>
<p>Retrieval algorithms evaluation is conducted for A<em>, Water-CIrcles (WC) and BeamSeach (BS) as well as their combinations: "WC + BS", "A</em> + BS", "A* + WC".The values 1 https://huggingface.co/intfloat/multilingual-e5-small of hyperparameters for base algorithms were fixed (see Appendix F)</p>
<p>Graph search restrictions</p>
<p>Additionally, we systematically varied the values of the hyperparameter governing the traversal constraints applied to the knowledge graph during algorithm execution.These constraints determine which node types are excluded from visitation: "E" prohibits access to episodic nodes, "T" restricts traversal of thesis nodes, and "O" bars visitation of object nodes.The "All" configuration imposes no restrictions, permitting unrestricted traversal across all node types.</p>
<p>Configurations summary</p>
<p>Each QA configuration was evaluated on 100 questionanswer pairs from the DiaASQ, HotpotQA, and TriviaQA datasets.The same large LLM was used both for generating responses within a given QA configuration and for constructing the corresponding knowledge graph to execute the QA pipeline.Consequently, for each fixed dataset/model pairing, 22 distinct QA configurations were derived.In total, 308 QA configurations were evaluated (the QA pipeline was not executed on TriviaQA/GPT4o-mini configurations due to resource constraints).</p>
<p>Evaluation</p>
<p>Traditional statistical evaluation metrics such as BLEU (Papineni et al. 2002), ROUGE (Lin 2004), and Meteor Universal (Denkowski and Lavie 2014) struggle to distinguish syntactically similar but semantically distinct texts.While semantic methods like BERTScore (Zhang et al. 2019) were introduced to address these limitations, our experiments reveal that BERTScore lacks sufficient differentiability, often failing to capture nuanced distinctions between correct and incorrect answers.Therefore, we adopt the LLM as a judge (Zheng et al. 2023) framework and choose Qwen2.5 7B.The judge evaluates QA pairs using a structured prompt containing the question, ground truth and model answer.It labels 1 for correct answers and 0 for incorrect ones, and we use accuracy as our main metric.Corresponding LLM-prompts and details are provided in Appendix D.</p>
<p>Also, for comparison of proposed QA pipeline with existing RAG and GrapRAG methods the ExactMatch metric is calculated with ignore case and ignore punctuation hyperparameters set to True.</p>
<p>Experiments and Results</p>
<p>Based on the experimental results, a comparative table summarizing the best-performing QA configurations was compiled (see Table 1).</p>
<p>As shown in Table 1, the results indicate that Qwen2.5 achieved the best performance (0.27) among the 7B models.Among all configurations tested, the highest overall effectiveness (0.77) was reached by setups incorporating GPT-4omini.Notably, top-performing 7B configurations predominantly relied on BeamSearch, especially under constraints that restricted traversal through episodic nodes.In contrast, the best DeepSeek V3 configurations frequently adopted To evaluate the effect of the imposed constraints on the quality of the QA pipeline, we constructed two distinct distributions of values-each corresponding to a specific model/dataset pair and search algorithm.These distributions were then averaged across datasets and models for configurations exhibiting the lowest and highest JudgeScore values, as detailed in Table 2.</p>
<p>As demonstrated in Table 2, for 7B/8B models, the majority (≈74%) of configurations yielding the lowest response quality impose restrictions on traversing thesis-type vertices.Conversely, a significant proportion of high-quality configurations restrict traversal of episodic and object vertices (≈44% and ≈34% respectively).This suggests that thesistype memories contain critical information for generating relevant responses, whereas the inclusion of episodic and simple memories introduces noise into the context, thereby degrading output quality.For larger-scale models, the trend differs: 53% of low-quality configurations restrict traversal of object vertices, while 73% of high-quality configurations restrict traversal of thesis-type nodes.This implies that larger models exhibit greater robustness in handling conditional generation from lengthy or noisy episodic memories, rendering thesis-based information redundant.</p>
<p>Additionally, a comparative analysis was conducted, as presented in Table 3, to evaluate the robustness of various graph traversal algorithms.The results indicate that configurations employing 8B models in conjunction with a combined search strategy (A* and WaterCircles) demonstrate high stability, with performance degradation remaining within 4% across varying traversal constraints.In contrast, the BeamSearch algorithm exhibits high sensitivity to these constraints: suboptimal parameterization results in substantial performance loss, with JudgeScore varying by as much as 24% between optimal and non-optimal settings.However, in the case of larger-scale models, the combination of BeamSearch and WaterCircles algorithms yields more consistent performance, suggesting improved robustness at higher model capacities.</p>
<p>A critical component of the implemented questionanswering (QA) pipeline is the "NoAnswer" mechanism.This mechanism incorporates a directive in the large language model (LLM) prompt, instructing the model to output a predefined symbol if the provided context lacks sufficient information to generate a valid response.Table 4 summarizes the frequency of "NoAnswer" outputs across different model configurations, retrieval algorithms, and graph traversal constraints.</p>
<p>As demonstrated in Table 4, the analysis reveals distinct patterns in the occurrence of "NoAnswer" responses across different model configurations.For 7B models, the lowest frequency of "NoAnswer" responses is observed when employing a combined A* and WaterCircles algorithm with restricted traversal of thesis nodes.In contrast, 8B models exhibit minimal "NoAnswer" instances when utilizing the BeamSearch algorithm alongside a prohibition on episodic node traversal.For larger-scale models, the optimal performance, measured by the fewest "NoAnswer" responses, is achieved through a combined BeamSearch and WaterCircles approach without graph traversal constraints.These findings suggest that the aforementioned algorithms are more effective at extracting relevant information compared to alternative methods under the specified conditions.</p>
<p>In summary, our framework demonstrates that the accuracy of results can be optimized by configuring the knowledge graph ontology and retrieval methods according to the available LLM and the selected QA task.A comparative analysis of existing RAG and GraphRAG methods against our proposed approach, conducted on the TriviaQA and Hot-potQA datasets, is provided in Appendix H. Characteristics of constructed knowledge graphs can be observed in Appendix G.</p>
<p>Conclusion</p>
<p>This work introduces a flexible and extensible framework for integrating structured memory into language model agents via knowledge graphs.By extending the AriGraph architecture with support for triplet, episodic and propositional nodes, we enable rich temporal and semantic repre-  The influence of the selected graph traversal algorithm and the imposed search restrictions on the percentage of generated "NoAnswer" stubs sentations that go beyond traditional RAG pipelines.Our system supports multiple graph traversal algorithms, including A*, WaterCircles, BeamSearch, and hybrid combinations that can be dynamically adapted to the model's scale and task requirements.Through extensive evaluation on three benchmarks-DiaASQ, HotpotQA, and TriviaQA, we demonstrate that performance varies systematically with the choice of retrieval strategy and graph traversal constraints.For smaller-scale models (7B-8B), configurations that restrict episodic or object nodes and rely on Beam-Search yield the highest accuracy, while for larger models, hybrid methods combining BeamSearch and WaterCircles offer improved stability and robustness.Importantly, we show that thesis nodes often encode critical information, and excluding them typically degrades performance, especially in 7B models.Compared to existing RAG and GraphRAG methods, our approach demonstrates competitive or superior performance, particularly in handling temporally complex and contradictory information.Ablation studies fur-ther reveal that hybrid traversal strategies reduce sensitivity to graph constraints and lower the frequency of invalid responses.Overall, our system provides a principled architecture for long-term, structured memory in LLM agents, enabling personalized, context-aware reasoning at scale.It lays the groundwork for future extensions involving temporal filtering, relation-type prioritization, and more finegrained memory control-all essential components for advancing robust, personalized AI systems.</p>
<p>Future work</p>
<p>In future work, we propose to enhance the temporal dynamics of the knowledge graph by introducing a "memory time" parameter, which will enable fine-grained filtering of triplets based on temporal proximity and relationship types.This modification will allow the system to selectively prioritize temporally proximate data or emphasize specific relationship categories, thereby improving the precision of personalized responses.</p>
<p>Tables 5 and 6 present the LLM prompts employed in the Memorize pipeline for extracting thesis-oriented and simple memory triples, respectively, from unstructured natural language text.These prompts facilitate the transformation of textual data into a structured knowledge graph representation.B LLM prompts used to find outdated information in the constructed knowledge graph</p>
<p>Tables 7 and 8 displays the LLM prompts employed in the Memorize pipeline for identifying stale thesis-related and simple memories, respectively, within the knowledge graph.</p>
<p>D LLM-as-a-Judge instructions</p>
<p>To ensure the reproducibility of the obtained results, LLM inference was conducted using a deterministic generation strategy.</p>
<p>The following hyperparameters were applied: num predict -2048, seed -42, temperature -0.0, and top k -1.The Qwen2.5 7B model, sourced from the Ollama repository, was prompted to evaluate whether the outputs of the proposed QA pipeline correctly answered the given questions.The specific LLM prompts used for this assessment are provided in Table 11.For the original HotpotQA dataset, the distractor/validation subset was selected, comprising 7405 question-answer (QA) pairs and 13781 unique contexts.QA pairs were then filtered to exclude those with associated contexts falling outside a specified length range (measured in characters), retaining only contexts between 64 and 1024 characters in length.This filtering process resulted in 13291 remaining contexts.Finally, the first 2000 QA pairs and their corresponding contexts were extracted, yielding a final subset of 3933 unique contexts.</p>
<p>For the original TriviaQA dataset, the rc.wikipedia/validation subset was selected, comprising 7993 question-answer (QA) pairs.Given the extensive length of the contexts in this dataset, they were segmented into smaller fragments (chunks) using the "RecursiveCharacterTextSplitter" class from the LangChain library.The following hyperparameters were applied: a chunk size of 1024 characters, separators set to double newline characters ("\n\n"), a chunk overlap of 64 characters, the "len" function for length calculation, and is separator regex set to "False".This preprocessing yielded 278384 unique text fragments.Subsequently, QA pairs were excluded if their associated text fragments fell outside the specified length bounds (minimum 64 and maximum 1024 characters), resulting in 13291 retained fragments.Additionally, since the original contexts were split without explicit tracking of which fragment contained the necessary information to answer a given question, if any fragment from a context was discarded, all remaining fragments from that context were also removed to ensure coherence.This step further reduced the dataset to 9975 unique fragments.Finally, the first 500 QA pairs and their corresponding relevant fragments were selected, leaving a total of 4925 unique fragments for analysis.</p>
<p>Thus, evaluation sets for quality assessment of the proposed/implemented Memorize-and QA-pipelines were obtained.The characteristics of the obtained subsets of HotpotQA-, TriviaQA-and DiaASQ-datasets can be found in</p>
<p>G Characteristics of constructed knowledge graphs</p>
<p>To evaluate the QA pipeline within the ablation experiments, we constructed 14 knowledge graphs based on the given dataset and LLM configurations.The structural characteristics of the generated graphs are detailed in Table 13.</p>
<p>The results indicate that certain graphs experienced parsing errors during the extraction of LLM responses, resulting in incomplete storage of contextual information.Across the evaluated datasets, the average parsing error rates were as follows: DiaASQ (7.0%), HotpotQA (6.3%), and TriviaQA (7.3%).For the DiaASQ dataset, DeepSeekR1 7B produced the highest number of thesis and object memory nodes, as well as hyper-relations, while GPT4o-mini extracted the largest number of unique simple-relations.In contrast, for the HotpotQA and TriviaQA datasets, Qwen2.5 7B generated the most thesis and object memory nodes, whereas DeepSeekR1 7B again yielded the highest number of hyper-relations.</p>
<p>Based on the obtained results, we can model the structural characteristics of the knowledge graphs generated by each evaluated system: see Figure 14.</p>
<p>The data reveal significant variation in LLM response parsing accuracy across models.Specifically, Qwen2.5 7B and Llama3.1 8B demonstrate the lowest error rates (0.02% each), while DeepSeek V3 exhibits the highest parsing error rate (31.21%).Intermediate performance is observed in DeepSeekR1 7B (0.29%) and GPT4o mini (9.87%).Regarding knowledge graph composition, DeepSeekR1 7B and Qwen2.5 7B yield the most comprehensive representations, generating the highest number of thesis/object memories and associated relations (both hyper-relational and simple).Further analysis of node creation efficiency per contextual unit shows Qwen2.5 7B achieves superior granularity, producing the largest number of thesis/object nodes while maintaining contextual coherence: see Table 15.</p>
<p>H Comparison with existing RAG and GraphRAG methods</p>
<p>To evaluate the performance of optimal configurations derived from our framework against existing Retrieval-Augmented Generation (RAG) and GraphRAG approaches, we conducted a systematic literature review.The search was performed across five academic search engines: SciSpace, Scite, PaperDigest, Consensus, and Elicit.Using each engine, we selected the first ten publications in the search results for the following queries: in the case of RAG methods -"Retrieval-augmented generation methods based on pretrained language models" and "RAG methods in NLP"; in the case of GraphRAG methods -"RAG on Knowledge Graphs", "Enhancing RAG-approach with Knowledge Graphs", "Graph RAG" and "RAG with integration of Large Language Models (LLMs) and Knowledge Graphs (KGs)".The search was restricted to publications up to 2018.Subsequently, we applied a three-stage filtering process: (1) duplicate entries were removed, and papers introducing novel evaluation datasets were excluded;</p>
<p>(2) works focusing on domain-specific applications without broader methodological contributions were discarded;</p>
<p>(3) only studies providing comprehensive methodological descriptions and employing standardized benchmarks were retained.This process yielded two final sets of nine articles each, covering RAG and GraphRAG techniques, respectively.A comparative analysis of these methods against our framework's optimal graph construction and retrieval configuration is presented in Table 16, with performance measured using ExactMatch metric.As shown in Table 16, our proposed method achieves a 14.1% improvement over existing GraphRAG approaches in a specific configuration.However, it underperforms compared to standard RAG methods by 17.8%.This discrepancy can be attributed to the fact that the RAG baselines evaluated in this study employed Reader and Retriever models that were specifically fine-tuned on the same dataset used for evaluation.As established in prior works, such in-domain fine-tuning typically yields optimal performance, and evaluation on out-of-domain datasets would be expected to result in significant degradation of measured metrics.</p>
<p>We also reproduce and evaluate the quality of HippoRAG method on DiaASQ and HotpotQA datasets: DiaASQ -0.53 (LLM-as-a-Judge); HotpotQA -60.2 (Exact Match).It can be seen that our method shows comparable results or outperforms HippoRAG.</p>
<p>Figure 1 :
1
Figure1: Example of the graph fragment, constructed from natural language text using our method, with simple (green), hyper (yellow) and episodic (blue) nodes</p>
<ol>
<li>Same Path Intersection by Node: If enabled, a path may revisit a node; otherwise, node revisitation is prohibited.4. Diff Paths Intersection by Node: If enabled, distinct paths may share nodes; otherwise, node sharing is disallowed. 5. Diff Paths Intersection by Rel: If enabled, distinct paths may share relations (edges); otherwise, relation sharing is forbidden.6. Final sorting mode: Determines the method for filtering the final set of paths.The search yields two path subsets:</li>
</ol>
<p>Table 1 :
1
Best QA configurations ranked by the LLM-as-a-Judge metric across all experiments.The corresponding quality score table presents the retrieval method and the type of restriction applied to the knowledge graph during traversal.Shortcuts for retrieval methods: BA -BeamSearch; AS -A<em>; BS + AS -hybrid of BeamSearch and A</em>; BS + WC -hybrid of BeamSearch and WaterCircles.Shortcuts for graph restrictions: all -no restrictions applied; E -episodic nodes excluded from traversal; T -thesis nodes excluded; O -object nodes excluded.
LLM /Qwen2.5 7B DeepSeek R1 7BLlama3.1 8BGPT4o-miniDeepSeek V3DatasetDiaASQ0.22 / BS / all0.12 / AS / E0.19 / BS / E0.5 / BS + WC / E0.47 / BS + WC / OHotpotQA 0.24 / BS / O0.19 / BS / O0.47 / BS / O0.77 / BS + WC / all 0.76 / BS + WC / TTriviaQA0.34 / BS / E0.27 / AS / E0.66 / BS + AS / E-0.87 / BS + WC / allMean:0.270.190.440.770.70a hybrid strategy combining BeamSearch and WaterCir-cles. Across high-performing configurations more broadly,BeamSearch consistently appeared as a key componentwithin the retrieval pipeline.</p>
<p>Table 2 :
2
Impact of various constraints, imposed during knowledge graph traversal, on the quality of QA pipeline: "worseconfigs" -distribution for low quality configurations; "best-configs" -distribution for high quality configurations
Restrictions7B8BLargesizeon nodes-typesworse-configs best-configs worse-configs best-configs worse-configs best-configsE0.030.440.090.450.270.07T0.840.120.640.310.200.73O0.130.440.270.250.530.20Retrieval algorithmworse-configs7B best-configsother-configsworse-configs8B best-configsother-configsworse-configsLargesize best-configsother-configs(w restr)(w restr)(w/o restr)(w restr)(w restr)(w/o restr)(w restr)(w restr)(w/o restr)WC--0.09--0.34--0.55AS0.10.1750.140.290,360.410.230.360.33BS0.0250.180.060.260.50.360.480.60.65WC+BS0.0330.0950.020.300.390.320.620.70.68BS+AS0.020.1750.010.250.480.360.480.640.66AS+WC0.0550.1150.070.330.370.420.570.60.6</p>
<p>Table 3 :
3
Stability of the implemented graph traversal algorithms when various restrictions are imposed: "worse-configs (w restr)" -low quality configurations and imposed restrictions; "best-configs (w restr)" -high quality configurations and imposed restrictions; "other-configs (w/o restr)" -configurations without restrictions on graph traversal
Retrieval Algorithm /WC AS BSWC +BS +AS +Restrictionsall ETOLLM-classBSASWCon graph search7B3144 2629312533 27 26 368B5149 4351734951 40 51 46Largesize2562 2716262126 32 27 35</p>
<p>Table 4 :
4</p>
<p>Table 5 :
5
LLM prompts for extracting thesis memories (in the form of triplets) from natural language text
Type</p>
<p>Table 6 :
6
LLM prompts for extracting simple memories (in the form of triplets) from natural language text</p>
<p>Table 7 :
7
LLM prompts for identifying outdated thesis memories
Type</p>
<p>Table 8 :
8
LLM prompts for detecting obsolete simple memories C LLM prompts used within proposed QA pipeline</p>
<p>Table 9 :
9
Table9presents the LLM prompts employed in the QA pipeline at the second stage (the "Query LLM Parser" module) for extracting key entities from the original user question.Table10displays the LLM prompts used in the fourth stage (the "QA LLM Generator" module) to generate contextually appropriate responses to user queries.LLM prompts for extracting key entities from natural language text
Type</p>
<p>Table 10 :
10
LLM prompts for conditional generation of answer to user-question</p>
<p>Table 11 :
11
LLM prompts used in LLM-as-a-Judge framework E Used preprocessing methods for evaluation datasets
Type</p>
<p>Table 12 .
12QA-pairsRelevant contextsDatasetAmountQuestions length (in characters)Answers length (in characters)AmountLength (in characters)median meanstdmedian meanstdmedian meanstdDiaASQ5698114109.44 18.6687.572.303483556613.00 324.35HotpotQA20008792.98 32.621315.29 11.873933384413.72 201.05TriviaQA5006676.37 39.46910.175.764925807765.11 196.32</p>
<p>Table 12 :
12
Characteristics of the datasets used to evaluate the quality of the proposed Memorize-and QA-pipelines
F Retrieval hyperparameters• A*: h metric name -ip; max depth -10; max passed nodes -150.• WaterCircles: strict filter -True; hyper num -15; episodic num -15; chain triplets num -25; other triplets num -6;do text pruning -False.• BeamSearch: max depth -5; max paths -10; same path intersection by node -False; diff paths intersection by node -False; diff paths intersection by rel -False; mean alpha -0.75; final sorting mode -mixed.</p>
<p>Table 13 :
13
Characteristics of the constructed knowledge graphs within conducted experiments
Number of nodesNumber or relationsAverage number of adjacent vertices (by type)LLMNumber of contexts to store in graphepisodic thesis object episodic hyper simpleobject-neighboursthesis-neighboursobject-neighboursobject-neighbours(to episodic-nodes)(to episodic-nodes)(to thesis-nodes)(to object-nodes)DeepSeek R1 7B410136677 60494157808 148778 3992729.189.304.041.69Qwen2.5 7B411239047 64799156784 135478 4393128.139.53.441.55Llama3.1 8B4113411233753 44300115917933963461319.698.222.751.75GPT4o mini370731069 38471115463952294103822.998.473.061.625DeepSeek V3282926380 4076999097835834177325.239.243.141.64</p>
<p>Table 14 :
14
Average/expected characteristics of knowledge graphs in case of using given LLM models for their construction
LLMthesisNode objecthyperRelation simpleDeepSeek R1 7B9153610Qwen2.5 7B10163311Llama3.1 8B811238GPT4o mini8102611DeepSeek V39143015</p>
<p>Table 15 :
15
The number of unique nodes/relations that are added to the knowledge graph when processing (using a given LLM model) and storing one episodic memory (episodic node)</p>
<p>Table 16 :
16
Comparison of existing RAG-and GraphRAG-methods with our proposed method on ExactMatch-metric.On Triv-iaQA dataset QA-configuration with DeepSeek V3 and combination of BeamSearch and WaterCircles algorithms (without restrictions on graph traversal) were used.On HotpotQA dataset QA-configuration with GPT4o-mini and the same algorithm and graph restrictions were used.
DatasetRAG-methodOur methodREALM DPRRAGColBERT-QAFiDEMDR2 RETRO Atlas RePLUGTriviaQA53.956.855.870.167.671.462.179.877.362.0GraphRAG-methodToGRoG PMKGEGRAGGNN-RAG ToG2.0DoGGCRPDAHotpotQA41,043.042.636.143.040.945.345.936.560.0</p>
<p>P Anokhin, N Semenov, A Sorokin, D Evseev, M Burtsev, E Burnaev, arXiv:2407.04363AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents. 2024</p>
<p>Reading Wikipedia to Answer Open-Domain Questions. D Chen, A Fisch, J Weston, A Bordes, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Meteor universal: Language specific translation evaluation for any target language. H Chen, R Pasunuru, J Weston, A Celikyilmaz, M Denkowski, A Lavie, arXiv:2310.05029Proceedings of the ninth workshop on statistical machine translation. the ninth workshop on statistical machine translation2023. 2014Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</p>
<p>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. B J Gutiérrez, Y Shu, Y Gu, M Yasunaga, Y Su, G Izacard, M Caron, L Hosseini, S Riedel, P Bojanowski, A Joulin, E Grave, arXiv:2405.148312024. 2021Unsupervised Dense Information Retrieval with Contrastive Learning</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. V Karpukhin, B Oguz, S Min, P Lewis, L Wu, S Edunov, D Chen, W.-T Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020</p>
<p>. K.-H Lee, X Chen, H Furuta, J Canny, I Fischer, </p>
<p>arXiv:2402.09727A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts. </p>
<p>P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Tau Yih, T Rocktäschel, S Riedel, D Kiela, arXiv:2005.11401Augmented Generation for Knowledge-Intensive NLP Tasks. 2021</p>
<p>B Li, H Fei, F Li, Y Wu, J Zhang, S Wu, J Li, Y Liu, L Liao, T.-S Chua, D Ji, arXiv:2211.05705DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. 2023</p>
<p>GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models. S Li, Y He, H Guo, X Bu, G Bai, J Liu, J Liu, X Qu, Y Li, W Ouyang, W Su, B Zheng, arXiv:2406.145502024</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>The probabilistic relevance framework: BM25 and beyond. S Robertson, H Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Questions Are All You Need to Train a Dense Passage Retriever. D S Sachan, M Lewis, D Yogatama, L Zettlemoyer, J Pineau, M Zaheer, Transactions of the Association for Computational Linguistics. 112023</p>
<p>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. P Sarthi, S Abdullah, A Tuli, S Khanna, A Goldie, C D ; Manning, N Lipka, R A Rossi, A Siu, R Zhang, T Derr, arXiv:2401.18059arXiv:2308.11730Knowledge Graph Prompting for Multi-Document Question Answering. Wang, Y2024. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, ArXiv, abs/1904.09675Advances in Neural Information Processing Systems. 362019. 2023BERTScore: Evaluating Text Generation with BERT</p>            </div>
        </div>

    </div>
</body>
</html>
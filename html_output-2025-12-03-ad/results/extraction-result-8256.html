<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8256 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8256</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8256</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-273162748</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.02892v3.pdf" target="_blank">The Role of Deductive and Inductive Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning tasks, yet their reliance on static prompt structures and limited adaptability to complex scenarios remains a significant challenge. In this paper, we propose the Deductive and InDuctive(DID) method, a novel framework that enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning approaches. Drawing from cognitive science principles, DID implements a dual-metric complexity evaluation system that combines Littlestone dimension and information entropy to precisely assess task difficulty and guide decomposition strategies. DID enables the model to progressively adapt its reasoning pathways based on problem complexity, mirroring human cognitive processes. We evaluate DID's effectiveness across multiple benchmarks, including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset for temporal reasoning. Our results demonstrate significant improvements in reasoning quality and solution accuracy - achieving 70.3% accuracy on AIW (compared to 62.2% for Tree of Thought) while maintaining lower computational costs. The success of DID in improving LLM performance while preserving computational efficiency suggests promising directions for developing more cognitively aligned and capable language models. Our work contributes a theoretically grounded, input-centric approach to enhancing LLM reasoning capabilities, offering an efficient alternative to traditional output-exploration methods.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8256.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8256.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A leading commercial large language model evaluated in zero-shot prompting settings in this paper; used as a primary testbed to compare DID (combined inductive+d eductive) prompting against baseline prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Role of Deductive and Inductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art generative LLM from OpenAI used in the paper's experiments (zero-shot; default temperature/top-k settings). The paper does not report exact parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['De-In-Ductive (DID: inductive reasoning + deductive reasoning)', 'Chain-of-Thought (CoT)', 'Tree-of-Thought (ToT)', 'Input-Output (IO)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DID: input-centric two-phase prompting that (1) evaluates problem complexity using Littlestone dimension and an information-entropy term, (2) generates progressively harder subproblems for inductive pattern discovery, and (3) applies deductive rule application and verification; CoT: sequential, explicit step-by-step reasoning prompts; ToT: exploration of multiple reasoning paths in a tree (T=3 in experiments) to search for solutions; IO: direct prompt→answer with no structured iterative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (DID explicitly integrates diverse methods — inductive then deductive — while baselines reflect single-mode approaches: IO and CoT are similar/single, ToT is diverse via multi-path exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct empirical comparison (zero-shot) between IO, CoT, ToT (T=3), and DID across three benchmarks (AIW/Alice problems, MR-GSM8K, Holiday Puzzle); complexity-guided decomposition and progressive induction are unique to DID. Default model sampling parameters were used for fair comparison; AIW averaged over 20 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AIW (Alice family-relationship logical problems), MR-GSM8K (meta-reasoning math benchmark), Holiday Puzzle (custom temporal/holiday scheduling dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AIW: DID 70.3% vs IO 43.4% vs CoT 55.9% vs ToT 62.2% (accuracy). MR-GSM8K: DID 87.7% vs CoT 85.0% vs ToT 89.1%. Holiday Puzzle: DID 15.4% vs IO 7.8% vs CoT 5.2% vs ToT 7.5%. Computational cost (GPT-4o, per-case, Table 2): AIW cost DID $0.0031 vs ToT $0.0038 vs CoT $0.0022 vs IO $0.0007; MR-GSM8K cost DID $0.0128 vs ToT $0.0194 vs CoT $0.0104; Holiday Puzzle cost DID $0.0181 vs ToT $0.0262 vs CoT $0.0135 vs IO $0.0059.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DID's progressive induction then deduction improves logical consistency and reduces common direct-solve errors on family-relationship tasks; DID reduces the need for extensive output-path exploration (ToT) by investing more input tokens for guided problem decomposition, producing lower overall computational cost than ToT in many settings. Baselines: IO frequently fails on complex structure; CoT helps but is less adaptable to instance-scale differences; ToT attains strong performance on some tasks but with higher output exploration costs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining diverse reasoning modes (induction to discover patterns and deduction to apply/verify them) using a complexity-guided input-centric approach (DID) yields higher accuracy than single-mode baselines (IO, CoT) and competitive or improved accuracy/cost trade-offs versus multi-path exploration methods (ToT) on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Role of Deductive and Inductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8256.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8256.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller-scale OpenAI LLM used to test DID's robustness across model scales; evaluated in zero-shot mode against baseline prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Role of Deductive and Inductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A widely used earlier-generation OpenAI LLM with fewer parameters and lower compute cost than GPT-4o; evaluated with default sampling parameters in the paper's zero-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['De-In-Ductive (DID: inductive reasoning + deductive reasoning)', 'Chain-of-Thought (CoT)', 'Tree-of-Thought (ToT)', 'Input-Output (IO)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same methodological implementations as for GPT-4o: DID uses complexity evaluation (Littlestone dimension × entropy), creates simplified base cases then increases complexity to induce rules, then applies deductive verification; CoT is sequential step prompting; ToT explores multiple branches (T=3); IO is direct answer prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (DID applies multiple, distinct reasoning modes; baselines are single-mode or multi-path exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Zero-shot comparisons of IO, CoT, ToT, and DID on AIW, MR-GSM8K, and Holiday Puzzle to measure robustness across model scales; no separate ablation explicitly toggling inductive vs deductive components is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AIW (Alice family-relationship problems), MR-GSM8K (meta-reasoning GSM8K extension), Holiday Puzzle (custom temporal reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AIW: DID 13.3% vs IO 6.7% vs CoT 8.6% vs ToT 7.2% (accuracy). MR-GSM8K: DID 73.3% vs CoT 68.1% vs ToT 74.0%. Holiday Puzzle: DID 5.6% vs IO 0.2% vs CoT 1.4% vs ToT 2.0%. (All numbers are percent correct.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DID improves performance over single-mode baselines even on smaller models, though absolute accuracies remain low on some complex temporal tasks; progressive induction helps reveal patterns that direct prompting misses, but model capacity limits remain a bottleneck for full generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>DID's combined inductive+d eductive prompting yields consistent improvements over IO/CoT/ToT on the evaluated tasks even for smaller models, indicating that reasoning-method diversity benefits models across scales though gains vary with model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Role of Deductive and Inductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8256.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8256.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Anthropic LLM included as a cross-provider test to evaluate DID's generality; displayed the strongest accuracy on several tasks when combined with DID prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Role of Deductive and Inductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercial Anthropic large language model evaluated with default zero-shot prompting parameters in the paper's comparative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['De-In-Ductive (DID: inductive reasoning + deductive reasoning)', 'Chain-of-Thought (CoT)', 'Tree-of-Thought (ToT)', 'Input-Output (IO)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DID: same two-phase (inductive then deductive) input-centric prompting guided by complexity C(p)=d•H(p); CoT: sequential chain prompts; ToT: multi-path tree exploration (T=3); IO: direct answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (DID integrates diverse modes; baselines represent similar/single or diverse multi-path methods)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Zero-shot head-to-head comparisons across IO, CoT, ToT, and DID on AIW, MR-GSM8K, and Holiday Puzzle; no explicit ablation isolating only inductive or only deductive DID components reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AIW (family-relationship reasoning), MR-GSM8K (meta-reasoning math), Holiday Puzzle (custom holiday scheduling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AIW: DID 89.5% vs IO 74.8% vs CoT 83.7% vs ToT 87.1%. MR-GSM8K: DID 92.0% vs CoT 91.3% vs ToT 92.0%. Holiday Puzzle: DID 24.5% vs IO 17.4% vs CoT 17.8% vs ToT 24.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Claude 3.5 Sonnet benefits strongly from DID prompting; pattern induction followed by deductive application yields high accuracy on family-relationship problems and competitive performance on math meta-reasoning; Holiday Puzzle remains challenging but DID preserves an advantage. The paper emphasizes that DID's additional input tokens enable stronger pattern recognition and thus better deductive application with this model.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>DID's combination of inductive and deductive reasoning improves reasoning quality and accuracy across model architectures and providers, with Claude 3.5 Sonnet showing particularly large gains, supporting the claim that diverse reasoning modes aid problem solving more than single-mode prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Role of Deductive and Inductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Mr-gsm8k: A metareasoning benchmark for large language model evaluation <em>(Rating: 2)</em></li>
                <li>Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models <em>(Rating: 2)</em></li>
                <li>T 2 of thoughts: Temperature tree elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 1)</em></li>
                <li>Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8256",
    "paper_id": "paper-273162748",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A leading commercial large language model evaluated in zero-shot prompting settings in this paper; used as a primary testbed to compare DID (combined inductive+d eductive) prompting against baseline prompting strategies.",
            "citation_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A state-of-the-art generative LLM from OpenAI used in the paper's experiments (zero-shot; default temperature/top-k settings). The paper does not report exact parameter counts.",
            "reasoning_methods": [
                "De-In-Ductive (DID: inductive reasoning + deductive reasoning)",
                "Chain-of-Thought (CoT)",
                "Tree-of-Thought (ToT)",
                "Input-Output (IO)"
            ],
            "reasoning_methods_description": "DID: input-centric two-phase prompting that (1) evaluates problem complexity using Littlestone dimension and an information-entropy term, (2) generates progressively harder subproblems for inductive pattern discovery, and (3) applies deductive rule application and verification; CoT: sequential, explicit step-by-step reasoning prompts; ToT: exploration of multiple reasoning paths in a tree (T=3 in experiments) to search for solutions; IO: direct prompt→answer with no structured iterative reasoning.",
            "reasoning_diversity": "both (DID explicitly integrates diverse methods — inductive then deductive — while baselines reflect single-mode approaches: IO and CoT are similar/single, ToT is diverse via multi-path exploration)",
            "reasoning_diversity_experimental_setup": "Direct empirical comparison (zero-shot) between IO, CoT, ToT (T=3), and DID across three benchmarks (AIW/Alice problems, MR-GSM8K, Holiday Puzzle); complexity-guided decomposition and progressive induction are unique to DID. Default model sampling parameters were used for fair comparison; AIW averaged over 20 runs.",
            "task_or_benchmark": "AIW (Alice family-relationship logical problems), MR-GSM8K (meta-reasoning math benchmark), Holiday Puzzle (custom temporal/holiday scheduling dataset).",
            "performance_results": "AIW: DID 70.3% vs IO 43.4% vs CoT 55.9% vs ToT 62.2% (accuracy). MR-GSM8K: DID 87.7% vs CoT 85.0% vs ToT 89.1%. Holiday Puzzle: DID 15.4% vs IO 7.8% vs CoT 5.2% vs ToT 7.5%. Computational cost (GPT-4o, per-case, Table 2): AIW cost DID $0.0031 vs ToT $0.0038 vs CoT $0.0022 vs IO $0.0007; MR-GSM8K cost DID $0.0128 vs ToT $0.0194 vs CoT $0.0104; Holiday Puzzle cost DID $0.0181 vs ToT $0.0262 vs CoT $0.0135 vs IO $0.0059.",
            "qualitative_findings": "DID's progressive induction then deduction improves logical consistency and reduces common direct-solve errors on family-relationship tasks; DID reduces the need for extensive output-path exploration (ToT) by investing more input tokens for guided problem decomposition, producing lower overall computational cost than ToT in many settings. Baselines: IO frequently fails on complex structure; CoT helps but is less adaptable to instance-scale differences; ToT attains strong performance on some tasks but with higher output exploration costs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining diverse reasoning modes (induction to discover patterns and deduction to apply/verify them) using a complexity-guided input-centric approach (DID) yields higher accuracy than single-mode baselines (IO, CoT) and competitive or improved accuracy/cost trade-offs versus multi-path exploration methods (ToT) on the evaluated benchmarks.",
            "uuid": "e8256.0",
            "source_info": {
                "paper_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A smaller-scale OpenAI LLM used to test DID's robustness across model scales; evaluated in zero-shot mode against baseline prompting methods.",
            "citation_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "A widely used earlier-generation OpenAI LLM with fewer parameters and lower compute cost than GPT-4o; evaluated with default sampling parameters in the paper's zero-shot experiments.",
            "reasoning_methods": [
                "De-In-Ductive (DID: inductive reasoning + deductive reasoning)",
                "Chain-of-Thought (CoT)",
                "Tree-of-Thought (ToT)",
                "Input-Output (IO)"
            ],
            "reasoning_methods_description": "Same methodological implementations as for GPT-4o: DID uses complexity evaluation (Littlestone dimension × entropy), creates simplified base cases then increases complexity to induce rules, then applies deductive verification; CoT is sequential step prompting; ToT explores multiple branches (T=3); IO is direct answer prompting.",
            "reasoning_diversity": "both (DID applies multiple, distinct reasoning modes; baselines are single-mode or multi-path exploration)",
            "reasoning_diversity_experimental_setup": "Zero-shot comparisons of IO, CoT, ToT, and DID on AIW, MR-GSM8K, and Holiday Puzzle to measure robustness across model scales; no separate ablation explicitly toggling inductive vs deductive components is reported.",
            "task_or_benchmark": "AIW (Alice family-relationship problems), MR-GSM8K (meta-reasoning GSM8K extension), Holiday Puzzle (custom temporal reasoning).",
            "performance_results": "AIW: DID 13.3% vs IO 6.7% vs CoT 8.6% vs ToT 7.2% (accuracy). MR-GSM8K: DID 73.3% vs CoT 68.1% vs ToT 74.0%. Holiday Puzzle: DID 5.6% vs IO 0.2% vs CoT 1.4% vs ToT 2.0%. (All numbers are percent correct.)",
            "qualitative_findings": "DID improves performance over single-mode baselines even on smaller models, though absolute accuracies remain low on some complex temporal tasks; progressive induction helps reveal patterns that direct prompting misses, but model capacity limits remain a bottleneck for full generalization.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "DID's combined inductive+d eductive prompting yields consistent improvements over IO/CoT/ToT on the evaluated tasks even for smaller models, indicating that reasoning-method diversity benefits models across scales though gains vary with model capacity.",
            "uuid": "e8256.1",
            "source_info": {
                "paper_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Claude 3.5 Sonnet (Anthropic)",
            "brief_description": "An Anthropic LLM included as a cross-provider test to evaluate DID's generality; displayed the strongest accuracy on several tasks when combined with DID prompting.",
            "citation_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "A commercial Anthropic large language model evaluated with default zero-shot prompting parameters in the paper's comparative experiments.",
            "reasoning_methods": [
                "De-In-Ductive (DID: inductive reasoning + deductive reasoning)",
                "Chain-of-Thought (CoT)",
                "Tree-of-Thought (ToT)",
                "Input-Output (IO)"
            ],
            "reasoning_methods_description": "DID: same two-phase (inductive then deductive) input-centric prompting guided by complexity C(p)=d•H(p); CoT: sequential chain prompts; ToT: multi-path tree exploration (T=3); IO: direct answer generation.",
            "reasoning_diversity": "both (DID integrates diverse modes; baselines represent similar/single or diverse multi-path methods)",
            "reasoning_diversity_experimental_setup": "Zero-shot head-to-head comparisons across IO, CoT, ToT, and DID on AIW, MR-GSM8K, and Holiday Puzzle; no explicit ablation isolating only inductive or only deductive DID components reported.",
            "task_or_benchmark": "AIW (family-relationship reasoning), MR-GSM8K (meta-reasoning math), Holiday Puzzle (custom holiday scheduling).",
            "performance_results": "AIW: DID 89.5% vs IO 74.8% vs CoT 83.7% vs ToT 87.1%. MR-GSM8K: DID 92.0% vs CoT 91.3% vs ToT 92.0%. Holiday Puzzle: DID 24.5% vs IO 17.4% vs CoT 17.8% vs ToT 24.0%.",
            "qualitative_findings": "Claude 3.5 Sonnet benefits strongly from DID prompting; pattern induction followed by deductive application yields high accuracy on family-relationship problems and competitive performance on math meta-reasoning; Holiday Puzzle remains challenging but DID preserves an advantage. The paper emphasizes that DID's additional input tokens enable stronger pattern recognition and thus better deductive application with this model.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "DID's combination of inductive and deductive reasoning improves reasoning quality and accuracy across model architectures and providers, with Claude 3.5 Sonnet showing particularly large gains, supporting the claim that diverse reasoning modes aid problem solving more than single-mode prompting strategies.",
            "uuid": "e8256.2",
            "source_info": {
                "paper_title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Mr-gsm8k: A metareasoning benchmark for large language model evaluation",
            "rating": 2,
            "sanitized_title": "mrgsm8k_a_metareasoning_benchmark_for_large_language_model_evaluation"
        },
        {
            "paper_title": "Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models",
            "rating": 2,
            "sanitized_title": "alice_in_wonderland_simple_tasks_showing_complete_reasoning_breakdown_in_stateoftheart_large_language_models"
        },
        {
            "paper_title": "T 2 of thoughts: Temperature tree elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "t_2_of_thoughts_temperature_tree_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 1,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning",
            "rating": 1,
            "sanitized_title": "two_heads_are_better_than_one_testtime_scaling_of_multiagent_collaborative_reasoning"
        }
    ],
    "cost": 0.011887499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Role of Deductive and Inductive Reasoning in Large Language Models
7 Jul 2025</p>
<p>Chengkun Cai 
University of Edinburgh</p>
<p>Xu Zhao 
University of Edinburgh</p>
<p>Haoliang Liu 
University of Manchester</p>
<p>Zhongyu Jiang 
University of Washington</p>
<p>Tianfang Zhang 
Tsinghua University</p>
<p>Zongkai Wu 
Skai Intelligence</p>
<p>Jenq-Neng Hwang 
University of Washington</p>
<p>Lei Li lilei@di.ku.dk 
University of Washington</p>
<p>University of Copenhagen</p>
<p>The Role of Deductive and Inductive Reasoning in Large Language Models
7 Jul 2025F33E5B941EDEC393D55DCAD4FB8D1038arXiv:2410.02892v3[cs.AI]
Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning tasks, yet their reliance on static prompt structures and limited adaptability to complex scenarios remains a major challenge.In this paper, we propose the Deductive and InDuctive(DID) method, a novel framework that enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning approaches.Drawing from cognitive science principles, DID implements a dual-metric complexity evaluation system that combines Littlestone dimension and information entropy to precisely assess task difficulty and guide decomposition strategies.DID enables the model to progressively adapt its reasoning pathways based on problem complexity, mirroring human cognitive processes.We evaluate DID's effectiveness across multiple benchmarks, including the AIW, MR-GSM8K, and our custom Holiday Puzzle dataset for temporal reasoning.Our results demonstrate great improvements in reasoning quality and solution accuracy -achieving 70.3% accuracy on AIW (compared to 62.2% for Tree of Thought), while maintaining lower computational costs.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as GPT-4, have transformed natural language processing by excelling in tasks such as language translation, summarization, and question-answering (Ope-nAI, 2023), particularly in reasoning tasks and few-shot learning.However, their reliability in problem-solving remains debatable.While Zhou et al. (2024) notes that scaling and fine-tuning can introduce unpredictable errors even in simple tasks, recent methodologies like Chain of Thought (CoT) (Wei et al., 2022) have shown substantial improvements in arithmetic and symbolic reasoning tasks (Li et al., 2024).Studies have demonstrated that LLMs can achieve high accuracy in multi-step reasoning when guided by structured approaches like CoT and self-consistency (Bubeck et al., 2023;Wang et al., 2022).Additionally, techniques such as reinforcement learning from human feedback (RLHF) have proven effective in reducing harmful or inaccurate outputs (Ouyang et al., 2022;Christiano et al., 2017).</p>
<p>Despite these advances, LLMs face substantial challenges with complex and evolving tasks due to their reliance on static prompt structures and prelearned patterns.This limitation manifests in tasks requiring logical reasoning, such as calculating family relationships or performing numerical comparisons (Nezhurina et al., 2024).Unlike human problem-solving, which dynamically adjusts strategies based on task complexity through inductive and deductive reasoning (Sloman, 2009), LLMs often struggle to adapt their reasoning processes to novel situations (Marcus, 2020;Hendrycks et al., 2020).</p>
<p>This adaptability gap becomes particularly evident in tasks requiring dynamic adjustment or incremental problem-solving.While existing approaches like CoT (Wei et al., 2022), Tree-of-Thought (ToT) (Yao et al., 2024), Temperature-Tree-of-Thought (T 2 oT) (Cai et al., 2024), and Graph-of-Thought (GoT) (Besta et al., 2024) have made progress through extensive output exploration, they often incur considerable computational costs.For instance, ToT achieves 62.2% accuracy on the AIW benchmark but requires substantial output token generation for exploring multiple reasoning paths, resulting in higher computational overhead ($0.0038 per case compared to $0.0022 for CoT).</p>
<p>To address these challenges, we propose the De-In-Ductive (DID) method, a novel approach that enhances LLM reasoning by integrating both inductive and deductive reasoning processes within  2024)) and reasoning token cost.The y-axis shows accuracy on the AIW reasoning benchmark.The relative positioning of models on the complexity axis is based on their approximate parameter counts and the computational overhead required for inference.</p>
<p>the prompt construction framework.Unlike previous methods that focus on expanding output exploration, DID takes an input-centric approach inspired by Test-Time Training techniques, strategically investing in input structuring to enable more efficient reasoning.The DID framework incorporates two key innovations: problem complexity evaluation and dynamic reasoning adjustment.</p>
<p>For problem complexity evaluation, we introduce a dual-metric system that considers both the Littlestone dimension (measuring structural problem complexity) and information entropy (quantifying instance problem complexity) of problems, enabling precise assessment of task difficulty and guiding decomposition strategy.Grounded in cognitive science models of human reasoning, DID implements a hybrid approach that mirrors human cognitive strategies.The method operates in two phases: first, it employs inductive reasoning to derive general rules from specific instances, progressively increasing problem complexity while maintaining similar Littlestone dimension; then, it applies deductive reasoning to solve particular problems, where the dynamic reasoning adjustment mechanism leverages problem complexity assessment to adaptively control the reasoning chain length and decomposition granularity.</p>
<p>We validate DID's effectiveness on established benchmarks including AIW and MR-GSM8K (Nezhurina et al., 2024;Zeng et al., 2023), as well as our custom Holiday Puzzle dataset focusing on holiday date calculations.As shown in Figure 1, our empirical results demonstrate notable improvements in both solution accuracy and reasoning quality, achieving 70.3% accuracy on AIW (compared to 62.2% for ToT) while maintaining lower computational costs ($0.0031 vs $0.0038 per case).This work makes the following key contributions:</p>
<p>• We propose an innovative input-centric approach to LLM reasoning through the De-In-Ductive (DID) framework, which differs from existing output-exploration methods by strategically investing in input structuring.This approach fundamentally changes how we enhance LLM reasoning capabilities, offering a more efficient alternative to traditional methods.</p>
<p>• We develop a theoretically grounded complexity evaluation system that combines Littlestone dimension and information entropy, enabling precise assessment of task difficulty and guiding the dynamic integration of inductive and deductive reasoning processes.</p>
<p>• Through extensive empirical evaluations across diverse reasoning tasks, we demonstrate that DID not only achieves superior accuracy but also maintains lower computational costs through efficient input utilization, establishing a new direction for efficient LLM reasoning enhancement.</p>
<p>Related Works</p>
<p>Cognitive Science and Deductive-Inductive Reasoning Deductive and inductive reasoning are essential in cognitive science, with deductive reasoning applying general principles to specific cases, and inductive reasoning generalizing from observations.Cognitive models view these approaches as complementary: inductive reasoning generates hypotheses, while deductive reasoning tests them (Wason, 1960).This combination enhances problemsolving, especially in uncertain domains where balancing exploration and validation is key (Johnson-Laird, 1983;Tversky and Kahneman, 1974).Wellstructured problems typically favor deductive reasoning, whereas ill-structured problems benefit from inductive reasoning (Funke, 2013).Cognitive science insights have been integrated into neu-ral networks, improving generalization (L Griffiths et al., 2008;Tenenbaum et al., 2011).</p>
<p>LLMs for Reasoning and Prompting Techniques</p>
<p>While LLMs like GPT-4 excel at tasks such as text generation, they struggle with logical reasoning and complex deduction (OpenAI, 2023;Nezhurina et al., 2024).Techniques like CoT (Wei et al., 2022), ToT (Yao et al., 2024), and GoT (Besta et al., 2024)  Recent advancements in improving LLM reasoning have explored diverse strategies.While multiagent frameworks like Jin et al. (2025) demonstrate superior performance through collaborative exploration of reasoning paths, they incur increased computational overhead due to multiple model calls.Other approaches, such as "learning from teaching regularization" (Jin et al., 2024) and Self-Explore (Hwang et al., 2024), enhance reasoning by incorporating structured examples or fine-grained rewards during training.However, these methods necessitate model fine-tuning or multiple model instances, whereas our DID framework distinguishes itself by improving reasoning through structured prompting of a single model instance without additional training.</p>
<p>Inductive Inference and Online Learning Recent work links inductive inference to online learn-ing theory.Lu (2024) demonstrate that inductive inference is possible for hypothesis classes decomposable into countable unions with finite Littlestone dimension.This result extends classical induction models, such as Solomonoff's (Solomonoff, 1964).The connection between Littlestone dimension and learning complexity informs the DID framework, suggesting that decomposing tasks into simpler components can enhance learning and generalization.</p>
<p>Methodology</p>
<p>Problem Formalization and Complexity Evaluation</p>
<p>Most reasoning tasks encountered by LLMs can be characterized as sequential learning problems with finite Littlestone dimension.According to recent theoretical work, a hypothesis class is learnable through inductive inference if and only if it can be decomposed into a countable union of classes with finite Littlestone dimension.</p>
<p>Littlestone Dimension and Beyond</p>
<p>For traditional online learning problems, the Littlestone dimension d alone sufficiently characterizes problem difficulty.This dimension quantifies the intrinsic sequential learning complexity by measuring:</p>
<p>• The minimal depth of decision trees needed for solving the problem</p>
<p>• The number of key decision points in the reasoning process However, when dealing with Large Language Models (LLMs), we observe that problems with identical Littlestone dimensions can exhibit significantly different difficulty levels.For example:</p>
<p>Example 1 Alice has 0 brothers and 1 sister.How many sisters does Alice's brother have?</p>
<p>Example 2 Alice has 3 brothers and 6 sisters.How many sisters does Alice's brother have?</p>
<p>Both problems share the same Littlestone dimension, as they follow identical reasoning patterns.However, LLMs consistently perform better on Example 1.This discrepancy arises from several theoretical considerations:</p>
<ol>
<li>Feature Vector Differences: In LLM's internal representations, simpler numerical relationships create clearer, more distinguishable feature vectors 2. Distribution Shift: Larger numbers and more complex relationships often represent a shift from the training distribution 3. Information Bottleneck Theory: With increasing problem scale, the extraction of relevant information becomes more challenging due to the constrained capacity of intermediate representations</li>
</ol>
<p>Information Entropy Component</p>
<p>To account for these LLM-specific challenges, we introduce an information entropy component H that quantifies:</p>
<p>• The complexity of numerical relationships</p>
<p>• The density of relevant information that needs to be extracted</p>
<p>• The scale of variables involved in the problem</p>
<p>For a problem instance p with n variables {x 1 , ..., x n }, we define its entropy as:
H(p) = log 2 n i=1 (1 + |x i |)(1)
where |x i | represents the absolute value of each numerical variable in the problem.Importantly, only variables that are directly relevant to solving the problem are included in this calculation.For instance, in the problem "Alice has 3 brothers and 6 sisters.How many sisters does Alice's brother have?",only the number of brothers (3) and sisters (6) would be considered in the entropy calculation.This formulation:</p>
<p>• Grows logarithmically with problem scale</p>
<p>• Remains bounded for reasonable problem sizes</p>
<p>• Captures the intuition that larger numbers and more variables increase processing difficulty</p>
<p>Problem Complexity Evaluation</p>
<p>The overall complexity of a problem p is then defined as:
C(p) = d • H(p)(2)
This combined measure allows us to:</p>
<ol>
<li>Distinguish between problems of equal Littlestone dimension but different scale complexity 2. Better predict LLM performance on reasoning tasks 3. Guide the decomposition of complex problems into manageable subproblems return subproblems 21: end function Dynamic Reasoning Based on Problem Complexity The Algorithm 1 formalizes our DID framework's problem decomposition approach.At its core, the algorithm dynamically decomposes complex problems into a sequence of progressively challenging subproblems while managing both structural complexity (Littlestone dimension) and information density.</li>
</ol>
<p>The decomposition process starts by creating a base case with reduced dimension (d − 2), achieved by setting certain variables to zero.Specifically, this means eliminating key decision points in the reasoning chain by simplifying the problem structure, for example, changing "Alice has 3 brothers and 6 sisters" to "Alice has 0 sisters and 1 brother" to reduce the problem's Littlestone dimension.This simplification maintains the essential reasoning structure while reducing the problem's complexity to its most basic form.From this foundation,  This progressive approach mirrors human cognitive processes in problem-solving: starting with simplified versions, identifying core patterns, and systematically applying these insights to more complex cases.The IncreaseCplx function implements this gradual progression by introducing additional variables and relationships while maintaining the problem's fundamental structure.The algorithm's dynamic dimension management ensures that the model can effectively balance between pattern recognition (inductive reasoning) in simpler cases and rigorous application (deductive reasoning) in more complex scenarios.This balance is crucial for maintaining both learning efficiency and solution accuracy across varying problem complexities.This aligns with the theoretical basis that inductive inference is possible when hypothesis classes have a finite Littlestone dimension.</p>
<p>De-In-Ductive (DID) Framework</p>
<p>• Hypothesis Generation: Through progressive exposure to increasingly complex examples, the model generates and refines hypotheses about the underlying structure of the problem.Each subproblem serves as a training instance for pattern recognition.</p>
<p>• Complexity-Guided Learning: The inductive process is guided by the complexity measure C(p) = d•H, ensuring that pattern recognition proceeds from simpler to more complex cases while maintaining manageable Littlestone dimensions.</p>
<p>Deductive Reasoning The deductive component enables the systematic application of discovered patterns:</p>
<p>• Rule Application: Once patterns are identified through induction, the model applies these rules deductively to solve more complex instances.This leverages the theoretical guarantee that hypothesis classes with finite Littlestone dimensions are learnable.</p>
<p>• Verification Process: Each deductive step serves a verification mechanism for inductively derived patterns, helping to refine and validate the model's understanding.</p>
<p>• Hierarchical Problem Solving: The deductive process follows the complexity hierarchy established during induction, ensuring that solutions are built systematically on previously verified patterns.</p>
<p>The results from deductive applications inform and refine the inductive pattern recognition process, creating a continuous learning cycle that enhances the model's problem-solving capabilities.A complete step-by-step example of the DID framework is provided in Appendix B.</p>
<p>Integration with Existing Models</p>
<p>The DID method seamlessly integrates with various LLM architectures and existing techniques such as CoT prompting (Wei et al., 2022).Through its structured framework combining inductive and deductive reasoning, DID enhances these methods by providing dynamic reasoning strategies and guided incremental reasoning, while maintaining computational efficiency.This approach creates a more flexible framework for LLMs to address complex problems without notable overhead.All methods are evaluated using three representative models:</p>
<p>Experiments</p>
<p>• GPT-4o and Claude 3.5 Sonnet: selected as two leading LLMs from different providers to demonstrate robustness across model architectures</p>
<p>• GPT-3.5-turbo:included to evaluate method robustness across different model scales (in terms of parameter count)</p>
<p>For fair comparison, all model parameters (temperature, top-k sampling, etc.) are maintained at their default values.Evaluations are conducted in a zero-shot setting across all methods and models.</p>
<p>Tasks and Results</p>
<p>Alice Problems The AIW dataset focuses on evaluating logical reasoning and deduction abilities through family relationship problems (Nezhurina et al., 2024).We manually curated 113 unique problems after removing duplicates and existing prompts, with results averaging over 20 runs.In this task, DID demonstrates consistent superiority across all models: Alice has 3 brothers and she also has 6 sisters.How many sisters does Alice's brother have?</p>
<p>Alice has 3 brothers and she also has 6 sisters.How many sisters does Alice's brother have?</p>
<p>In total, there are 9 siblings (3 brothers + 6 sisters)</p>
<p>Alice's brothers have the same siblings as Alice (except themselves)</p>
<p>Alice's brothers also have 6 sisters (these sisters include Alice and her other 6 sisters).</p>
<p>Alice's brother has 6 sister.</p>
<p>Determine some specific situation.</p>
<p>Start from a simplest version: 0 brother and 1 sister.</p>
<p>Increase the complexity of the problem: Alice now has one brother and one sister.</p>
<p>Continue to increase the complexity of the problem: Alice has two brothers and two sisters.The number of sisters each of Alice's brothers has is 6.</p>
<p>Get a conclusion: When Alice has 1 brother and no sisters, the brother has 1 sister.</p>
<p>Get a conclusion: When Alice has 1 brother and n sisters, the brother has n + 1 sisters</p>
<p>Verify and get conclusion: The number of brothers does not affect the number of sisters each brother has.• GPT-3.5 Turbo: DID (13.3%) significantly outperforms IO (6.7%), CoT (8.6%), and ToT (7.2%)</p>
<p>• GPT-4o: DID achieves 70.3% accuracy, surpassing IO (43.4%),CoT (55.9%), and ToT (62.2%)</p>
<p>• Claude 3.5 Sonnet: DID reaches 89.5%, extending the lead over IO (74.8%),CoT (83.7%), and ToT (87.1%)</p>
<p>As illustrated in Figure 3, DID progressively guides LLMs through increasingly complex reasoning steps for family relationship problems.While traditional methods often fail by attempting to solve complex problems directly, DID breaks down the reasoning process into simpler subproblems, helping the model maintain logical consistency and avoid common errors.This structured approach enables LLMs to effectively handle complex relationship inference tasks.</p>
<p>As shown in Table 2, while DID requires slightly more input tokens (90 vs 56 for ToT), it maintains lower total computational costs ($0.0031 vs $0.0038) through more efficient output generation.This demonstrates the effectiveness of our inputcentric approach in balancing performance and efficiency.To determine the number of holiday rest days excluding regular weekends, we break it down as follows: During the "Work 4, Rest 8" period from September 25 to October 6, there are 5 holiday rest days (October 2 to October 6).In the "Work 7, Rest 2" period from October 9 to October 17, there are 2 holiday rest days (October 16 and 17).For the "Work 5, Rest 2" period from October 18 to October 24, there are 2 holiday rest days (October 23 and 24).Lastly, during the "Work 5, Rest 3" period from October 25 to November 1, there are 3 holiday rest days (October 30 to November 1).In total, there are 12 holiday rest days (5 from the first period, 2 from the second, 2 from the third, and 3 from the fourth).</p>
<p>Next, we need to identify how many of these rest days overlap with weekends (since those wouldn't count as extra holiday rest days) MR-GSM8K Math Problems MR-GSM8K extends the GSM8K benchmark with meta-reasoning tasks (Zeng et al., 2023), requiring models to identify and explain errors in provided solutions.Results show consistent performance across models:</p>
<p>• GPT-3.5 Turbo: DID (73.3%) maintains competitive performance against CoT (68.1%) and ToT (74.0%)</p>
<p>• GPT-4o: DID (87.7%) performs comparably to CoT (85.0%) and ToT (89.1%)</p>
<p>• Claude 3.5 Sonnet: DID (92.0%) matches the strong performance of CoT (91.3%) and ToT (92.0%)</p>
<p>As shown in Table 2, DID achieves this performance with lower computational overhead than ToT ($0.0128 vs $0.0194), despite using more input tokens (190 vs 91).This efficiency gain comes from reduced output exploration needs.</p>
<p>Holiday Puzzle This custom dataset comprises 20 holiday arrangement problems, testing models' ability to calculate actual holiday days while accounting for weekends and compensatory workdays.Detailed information about the dataset con-struction and representative examples are provided in Appendix A. Results demonstrate:</p>
<p>• GPT-3.5 Turbo: DID (5.6%) outperforms IO (0.2%), CoT (1.4%), and ToT (2.0%)</p>
<p>• GPT-4o: DID shows marked improvement (15.4%) over IO (7.8%), CoT (5.2%), and ToT (7.5%)</p>
<p>• Claude 3.5 Sonnet: DID (24.5%) maintains advantage over IO (17.4%),CoT (17.8%), and ToT (24.0%)</p>
<p>The key to success in this task lies in discovering and applying the fundamental relationship Holiday rest days = Total rest days -Weekend rest days.As shown in Figure 4, baseline methods struggle with this pattern.</p>
<p>As shown in Table 2, while DID uses more input tokens (260 vs 110 for ToT), its efficient output generation results in lower total costs ($0.0181 vs $0.0262), demonstrating the scalability of our input-centric approach even in complex temporal reasoning tasks.</p>
<p>In this work, we introduced the De-In-Ductive (DID) method, a novel framework that dynamically integrates inductive and deductive reasoning to enhance the adaptability and reasoning capabilities of LLMs.By leveraging cognitive science principles, the DID framework allows LLMs to evolve their problem-solving strategies in response to task complexity, overcoming the rigidity of static prompt structures.Through extensive empirical validation on both standard benchmarks and our custom Holiday Puzzle dataset, we demonstrated substantial improvements in accuracy and reasoning quality, achieved without excessive computational costs.The success of DID in improving LLM reasoning while maintaining computational efficiency suggests promising directions for future research in making language models more cognitively aligned and capable of sophisticated reasoning.</p>
<p>Limitations</p>
<p>Despite the advances demonstrated by the DID framework, several important limitations and challenges remain to be addressed: Fundamental Architecture Constraints A key limitation lies in the fundamental architecture of LLMs.These models, based on next-token prediction, struggle to maintain coherent internal representations across multiple reasoning steps.While attention mechanisms allow reference to previous tokens, they lack robust cognitive structures for ensuring logical integrity throughout the reasoning process.This often leads to unexpected errors even in seemingly straightforward tasks.</p>
<p>Generalization Challenges</p>
<p>While DID shows strong performance on our evaluated tasks, ensuring consistent generalization to completely unseen problems remains challenging.The framework's effectiveness may vary depending on the nature and complexity of new tasks, particularly those requiring novel forms of reasoning not encountered during development.</p>
<p>A Holiday Puzzle Dataset Details</p>
<p>The Holiday Puzzle dataset was created based on holiday arrangements in China over the past 10 years, specifically focusing on how special holidays (National Day, Spring Festival, Labor Day, Mid-Autumn Festival, etc.) are rescheduled.In China, the government employs a unique "work day adjustment" system to create longer consecutive holiday periods by rearranging working days and weekends.This practice often involves designating certain weekends as working days while extending official holidays, creating complex patterns where regular weekends are shifted, and compensatory workdays are inserted before or after holidays.This arrangement, while allowing for longer holiday periods, makes it challenging to calculate the actual number of holiday days versus regular weekend days.</p>
<p>A.1 Representative Examples</p>
<p>Prompt: This is a holiday arrangement from April 23, 2022 (Saturday) to May 15: rest 1, work 6, rest 5, work 3, rest 1, work 5, rest 2. Please tell me how many days I have rested because of the holiday, except for the weekend I was supposed to rest.</p>
<p>Right Answer: 1 Prompt: This is a holiday arrangement from January 1, 2022 (Saturday) to February 8: rest 3, work 4, rest 2, work 5, rest 2, work 5, rest 2, work 7, rest 7, rest 2. Please tell me how many days I have rested because of the holiday, except for the weekend I was supposed to rest.</p>
<p>Right Answer: 4</p>
<p>B Detailed DID Framework Example</p>
<p>We provide a complete example of the DID framework in action: Problem: "Alice has 3 brothers and 6 sisters.How many sisters does Alice's brother have?"</p>
<p>Step 1: Complexity Evaluation The LLM first analyzes the problem structure and identifies: Step 2: Problem Decomposition Following Algorithm 1, the problem is decomposed into a sequence of progressively complex subproblems:</p>
<p>Inductive Reasoning Phase:</p>
<p>• Example 1 (d-1 Dimension): "Let's start with the simplest version of the problem: Alice has 0 sisters and 1 brother.In this case, Alice's brother has 1 sister (Alice)."</p>
<p>• Example 2 (d-1 Dimension): "Now, let's consider a slightly more complex scenario: Alice has 1 sister and 1 brother.In this case, Alice's brother has 2 sisters (Alice and her sister)."</p>
<p>• Example 3 (d Dimension): "Let's examine one more scenario: Alice has 2 sisters and 1 brother.In this case, Alice's brother has 3 sisters (Alice and her two sisters)."</p>
<p>• Pattern Recognition: "From these examples, we can deduce a general rule: The number of sisters Alice's brother has = X + 1, where X = the number of sisters Alice has.This rule holds true regardless of the number of brothers Alice has (Y), because we're only concerned about the number of sisters from the brother's perspective."</p>
<p>Step 3: Deductive Reasoning "Now I'll apply this formula to our original problem: Alice has 6 sisters (X = 6).Therefore, Alice's brother has X + 1 = 6 + 1 = 7 sisters."</p>
<p>Step 4: Solution Verification "To verify: Alice's brother has the same siblings as Alice, except himself.</p>
<p>• Alice has 3 brothers and 6 sisters • From her brother's perspective, he has 2 brothers (the other brothers, excluding himself) and 7 sisters (the 6 original sisters plus Alice)</p>
<p>Therefore, Alice's brother has 7 sisters."Final Answer: 7</p>
<p>Figure 1 :
1
Figure 1: Performance comparison of different reasoning approaches (IO, CoT, ToT, and DID) across model complexity.The x-axis represents an estimated measure of complexity that considers both model size (following public estimates from Abacha et al. (2024)) and reasoning token cost.The y-axis shows accuracy on the AIW reasoning benchmark.The relative positioning of models on the complexity axis is based on their approximate parameter counts and the computational overhead required for inference.</p>
<p>Figure 2 :
2
Figure 2: Comparison of reasoning approaches in LLMs including the IO method, Chain of Thought (CoT) prompting, Tree of Thought (ToT) prompting, and the De-In-Ductive (DID) framework, highlighting the progression from direct output generation to dynamic inductive and deductive reasoning for more adaptive problem-solving.</p>
<p>Figure 2
2
Figure 2 illustrates the comparison between the IO, CoT, and DID frameworks.The IO (Input-Output) Method processes natural language queries by retrieving patterns and facts without engaging in iterative reasoning.The Chain of Thought (CoT) Method improves logical reasoning by breaking down complex problems into sequential steps.Our proposed De-In-Ductive (DID) Method goes further by dynamically integrating inductive and deductive reasoning.By iteratively generating and testing hypotheses, DID adapts to problem complexities more effectively than static methods like CoT, optimizing problem-solving by balancing reasoning modes in response to task difficulty.Dynamic Reasoning Based on Problem Complexity Based on the problem complexity C(p), DID framework adaptively decomposes the problem and adjusts its reasoning process.For a typical problem with Littlestone dimension d (usually 3-5), we decompose it into subproblems: • Dimension Reduction: We maintain subproblems with dimension d or reduce to d-1 by fixing certain variables to 0, preserving the essential reasoning structure while reducing complexity • Progressive Complexity: Starting from simple cases with minimal information density, we gradually increase complexity by adding variables and relationships • Hierarchical Solution: Each subproblem (K)</p>
<ol>
<li>1
1
Experimental Setup Baseline Methods and Models We compare DID against three baseline prompting methods: • Input-Output (IO): directly utilizes the LLM without structured prompting • Chain of Thought (CoT): breaks down problems into sequential reasoning steps • Tree of Thought (ToT): explores multiple reasoning paths in a tree structure (T=3)</li>
</ol>
<p>Alice</p>
<p>Figure 3 :
3
Figure 3: Comparison of reasoning approaches in LLMs including the IO method, CoT prompting, and the DID framework, highlighting the progression from direct output generation to dynamic inductive and deductive reasoning for more adaptive problem-solving.</p>
<p>Figure 4 :
4
Figure 4: Comparison of reasoning approaches in LLMs including the IO method, CoT prompting, and the DID framework, highlighting the progression from direct output generation to dynamic inductive and deductive reasoning for more adaptive problem-solving.</p>
<p>•</p>
<p>Littlestone dimension (d): 3 (requiring three key inferential steps) • entropy: H(p) = log 2 ((1 + 3)(1 + 6)) = log 2 (28) ≈ 4.8 • Overall complexity: C(p) = d • H(p) = 3 • 4.8 ≈ 14.4</p>
<p>Table 2 :
2
GPT-4o Token Usage and Cost Comparison
TaskMethod Input/Output tokens Cost per case Accuracy (%)IO37/55$0.000743.4Alice ProblemCoT ToT45/210 56/370$0.0022 $0.003855.9 62.2DID90/290$0.003170.3CoT86/1017$0.010485.0MR-GSM8KToT91/1920$0.019489.1DID190/1230$0.012887.7IO87/570$0.00597.8Holiday PuzzleCoT ToT96/1330 110/2590$0.0135 $0.02625.2 7.5DID260/1740$0.018115.4
AcknowledgementsThis work was supported in part by the Pioneer Centre for AI, DNRF grant number P1.
Medec: A benchmark for medical error detection and correction in clinical notes. Asma Ben Abacha, Wen-Wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin, arXiv:2412.192602024arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chengkun Cai, Xu Zhao, Yucheng Du, Haoliang Liu, Lei Li, arXiv:2405.14075T 2 of thoughts: Temperature tree elicits reasoning in large language models. 2024arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, arXiv:2502.06807Competitive programming with large reasoning models. 2025arXiv preprint</p>
<p>Complex problem solving: A case for complex cognition?. Joachim Funke, Complex problem solving: Principles and mechanisms. Psychology Press2013</p>
<p>Eric J Samuel J Gershman, Joshua B Horvitz, Tenenbaum, Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. 2015349</p>
<p>Panagiotis Giadikiaroglou, Maria Lymperaiou, arXiv:2402.11291Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle solving using reasoning of large language models: A survey. arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Selfexplore: Enhancing mathematical reasoning in language models with fine-grained rewards. Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, Minjoon Seo, arXiv:2404.103462024arXiv preprint</p>
<p>Learning from teaching regularization: Generalizable correlations should be easy to imitate. Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, Dimitris Metaxas, Marco Pavone, Advances in Neural Information Processing Systems. Curran Associates, Inc202437</p>
<p>Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris N Metaxas, Tong Che, arXiv:2504.09772Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning. 2025arXiv preprint</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas, Johnson-Laird , 1983Harvard University Press6</p>
<p>. Charles Thomas L Griffiths, Joshua B Kemp, Tenenbaum, 2008Bayesian models of cognition</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, arXiv:2402.128752024arXiv preprint</p>
<p>Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, arXiv:2312.07622Mathematical language models: A survey. 2023arXiv preprint</p>
<p>When is inductive inference possible?. Zhou Lu, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Gary Marcus, arXiv:2002.06177The next decade in ai: four steps towards robust artificial intelligence. 2020arXiv preprint</p>
<p>Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev, arXiv:2406.020612024arXiv preprint</p>
<p>arXiv:2303.08774GPT-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Causal models: How people think about the world and its alternatives. Sloman, 2009Oxford University Press</p>
<p>A formal theory of inductive inference. part i. Information and control. J Ray, Solomonoff, 19647</p>
<p>Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, Noah D Goodman, How to grow a mind: Statistics, structure, and abstraction. science. 2011331</p>
<p>Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Amos Tversky, Daniel Kahneman, science. 18541571974</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>On the failure to eliminate hypotheses in a conceptual task. C Peter, Wason, Quarterly Journal of Experimental Psychology. 1231960</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, arXiv:2312.17080Mr-gsm8k: A metareasoning benchmark for large language model evaluation. 2023arXiv preprint</p>
<p>Larger and more instructable language models become less reliable. Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, César Ferri, José Hernández-Orallo, Nature. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
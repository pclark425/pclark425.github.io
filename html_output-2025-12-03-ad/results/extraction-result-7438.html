<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7438 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7438</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7438</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-c76dd4a70361c3afd2e19d046343e2dedd16ecc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c76dd4a70361c3afd2e19d046343e2dedd16ecc3" target="_blank">Automatic Prompt Optimization with "Gradient Descent" and Beam Search</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Preliminary results suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language"gradients"that criticize the current prompt. The gradients are then"propagated"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7438.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7438.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProTeGi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Optimization with Textual Gradients (ProTeGi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric automatic prompt-optimization framework that generates natural-language 'gradients' describing prompt flaws and edits prompts in the opposite semantic direction, guided by beam search and bandit-based selection to improve LLM prompt wording and task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (Jan 2023) (used as the primary optimization engine; experiments also swapped to davinci/text-davinci-003/gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box RLHF-tuned conversational LLM accessed via API; used to generate gradient feedback, produce edited prompts, paraphrase candidates, and evaluate prompt candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification tasks: Jailbreak detection, Ethos (hate speech), Liar (fake news), Sarcasm</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification on short text inputs: detect jailbreak prompts, hate speech, lies, and sarcasm.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language classifier prompt (structured prompt with headings), few-shot examples included (2 held-constant examples), iterative prompt-editing loop</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt wording</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Starting prompts include explicit headings (# Task, # Output format, # Examples, # Prediction) with two few-shot examples; ProTeGi modifies the natural-language instruction text using LLM-generated feedback (textual 'gradients') and paraphrases; beam size b=4, minibatch size 64, 6 optimization steps, 4 gradients per error-group, 2 monte-carlo paraphrases per candidate; temperature=0.0 for few-shot classification calls and 1.0 for other generation contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Binary F1 (maxpool over final beam)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies by task; ProTeGi improved the initial prompt p0 on average by 15.3% (absolute F1 increase) and reportedly can improve an initial prompt's performance by up to 31% (abstract claim).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Average baselines: Monte-Carlo (MC) and RL baselines; ProTeGi improved over MC by +3.9% (absolute) and over RL by +8.2% (absolute) on average; improved over starting prompt p0 by +15.3% (absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+15.3% absolute average vs p0; up to +31% absolute (claimed max); +3.9% vs MC; +8.2% vs RL; +15.2% vs AutoGPT (all absolute F1 differences as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Primary experiments used gpt-3.5-turbo (Azure OpenAI) with temperature=0.0 for few-shot classification calls, temperature=1.0 for gradient/edit/paraphrase generations; few-shot count = 2 examples held constant; beam size b=4; minibatch |D_mini|=64; 6 optimization steps.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No formal p-values reported; standard errors for some experiments reported in Appendix Table 5 (e.g., ProTeGi accuracies and SEs); overall statistical significance not stated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7438.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Wording / Precision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of more precise, annotation-style prompt wording</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rewriting vague task descriptions into precise annotation instructions (e.g., specifying targeted groups for hate-speech, clarifying labeling criteria) improves LLM classification performance; ProTeGi's edits often convert vague prompts into more precise ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (primary); results also obtained across other base LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box RLHF-tuned conversational LLM used to perform classification given the instruction-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ethos (hate speech), Liar, Jailbreak, Sarcasm (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary decisions about whether input text matches a label (hate speech, lie, jailbreak attempt, sarcasm).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction-style natural language prompt (human-written or LLM-edited), with/without precise annotation-style phrasing</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt wording / instruction detail</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Starting prompts were short, generic task descriptions; ProTeGi edits often added explicit annotation guidance (e.g., 'contain language that targets a group based on religion, gender, or other personal characteristics'); experiments held two few-shot examples constant while allowing other prompt text to be edited.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Binary F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report that converting vague descriptions into more precise annotation instructions can yield large gains — up to 31% absolute improvement (abstract); average improvement over original prompt p0 was +15.3% absolute F1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original (vague) starting prompt p0 (baseline) — average F1; specific baselines shown in paper (e.g., Table 1 and overall averages).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Up to +31% absolute F1 (claimed); average +15.3% absolute vs p0.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Edits performed by LLM using textual gradients and paraphrasing; few-shot examples (2) held constant in all optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported; variance and standard errors discussed (Appendix Table 5) but no formal hypothesis tests reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7438.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeamAblation_Jailbreak</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search ablation — Jailbreak: Beam vs Greedy vs No-iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation showing that ProTeGi's beam-search-guided prompt editing outperforms flat enumeration and greedy search on the Jailbreak detection task (measured by F1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed RLHF model used for expansion/edit/paraphrase/evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak detection (binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect whether a user message is a jailbreak attack intended to get the model to break rules.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iterative edited natural-language prompts; compared three search formats (No iteration: flat enumerate-then-select, Greedy: DFS, Beam: ProTeGi beam search with textual gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>search strategy / prompt optimization format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>All variants matched overall API query budget and used same candidate counts per step; beam size b=4 for ProTeGi.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Beam (ProTeGi): 0.85 F1</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No iteration: 0.80 F1; Greedy: 0.82 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.05 absolute F1 vs No iteration; +0.03 absolute F1 vs Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same API budget across variants; 6 optimization rounds in ProTeGi experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported for ablation; only mean F1 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7438.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeamAblation_Liar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search ablation — Liar: Beam vs Greedy vs No-iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Beam-search-guided prompt editing improves Liar (fake-news) detection F1 compared to flat and greedy variants in the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-used RLHF conversational model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Liar (fake-news) detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of statements as lies vs not, given context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same three search formats compared (No iteration, Greedy DFS, Beam search).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>search strategy / prompt optimization format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Candidates per step and total API budget matched across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Beam (ProTeGi): 0.67 F1</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No iteration: 0.63 F1; Greedy: 0.63 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.04 absolute F1 vs No iteration/Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>6 optimization steps; other ProTeGi hyperparameters as in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7438.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeamAblation_Sarcasm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search ablation — Sarcasm: Beam vs Greedy vs No-iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Beam search gives a small improvement over baselines on the Sarcasm detection task in the ablation study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed RLHF model used for prompt editing and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sarcasm detection (Arabic dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification: decide if a tweet/comment is sarcastic.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction-style prompt with few-shot examples; compared search strategies (No iteration, Greedy, Beam).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>search strategy / prompt optimization format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>All methods matched candidate counts and API budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Beam (ProTeGi): 0.88 F1</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No iteration: 0.87 F1; Greedy: 0.85 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.01 absolute F1 vs No iteration; +0.03 absolute vs Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same experimental settings as other ablations; few-shot examples held constant.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7438.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BanditSelection_Jailbreak</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of candidate-selection (bandit) format on performance — Jailbreak</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different bandit-style selection strategies (Uniform, UCB, UCB-E, Successive Rejects, Successive Halving) for picking beam candidates lead to different downstream prompt-performance; UCB-style methods performed best in these experiments for Jailbreak.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API LLM used for evaluating prompts and guiding selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Jailbreak detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Selection strategy / evaluation protocol (how the per-candidate query budget is allocated across candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>selection strategy / evaluation format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compared per-prompt evaluation budgets of 25 and 50 queries/candidate; exploration parameter c=2.0 for UCB variants; Successive Rejects and Successive Halving configured to match budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>At 25 queries/prompt: UCB = 0.83 F1 (best); at 50 queries/prompt: UCB = 0.85 F1 (best).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Uniform selection baseline: 0.77 F1 (25 or 50 budget)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.06 absolute F1 (UCB vs Uniform) at 25 budget; +0.08 absolute at 50 budget.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Budget matching across algorithms; UCB exploration c=2.0; sampling subsets of data for quick evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7438.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BanditSelection_Liar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of candidate-selection (bandit) format on performance — Liar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bandit selection methods influence final prompt performance for the Liar task; UCB and UCB-E achieved the highest F1 at the budgets tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed LLM used for candidate evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Liar (fake-news) detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Selection strategy (per-candidate evaluation budget allocation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>selection strategy / evaluation format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Budgets of 25 and 50 per prompt; UCB c=2.0; SR/SH configured to match budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>At 25 queries/prompt: UCB = 0.66 F1 (best at 25); at 50 queries/prompt: UCB-E = 0.67 F1 (best at 50).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Uniform: 0.59 (25) and 0.61 (50) F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.07 absolute F1 (UCB vs Uniform) at 25; +0.06 absolute F1 (UCB-E vs Uniform) at 50.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>As above; exploration parameter c=2.0 for UCB variants.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7438.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7438.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BaseModelSwap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of base LLM (GPT-3 / InstructGPT / ChatGPT / GPT-4) on optimized-prompt performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Swapping the underlying LLM used for generation/evaluation changes final prompt performance; RLHF-tuned/chat-oriented models (InstructGPT, ChatGPT, GPT-4) outperform base GPT-3 (davinci), with GPT-4 typically best.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Comparative: GPT-3 (davinci), InstructGPT (text-davinci-003), ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Different generations of OpenAI models: base GPT-3 davinci, improved instruction-tuned text-davinci-003, chat-oriented gpt-3.5-turbo, and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selected tasks: Sarcasm and Jailbreak (reported examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification tasks described above.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same prompt-optimization pipeline but with different base LLM powering the gradient/edit/evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model backbone / inference engine</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Reported F1 for Sarcasm and Jailbreak across models: Sarcasm F1 — GPT-3:0.73, InstructGPT:0.83, ChatGPT:0.86, GPT-4:0.86; Jailbreak F1 — GPT-3:0.55, InstructGPT:0.75, ChatGPT:0.85, GPT-4:0.88. These numbers show model choice substantially affects final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: Jailbreak F1 ranges from 0.55 (GPT-3) to 0.88 (GPT-4); Sarcasm F1 ranges from 0.73 (GPT-3) to 0.86 (ChatGPT/GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GPT-3 (davinci) performance used as a lower baseline in reported comparisons (see above values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Jailbreak: +0.33 absolute F1 (GPT-4 vs GPT-3); Sarcasm: +0.13 absolute F1 (ChatGPT/GPT-4 vs GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same prompt formats and optimization procedure but swapping which LLM is called for gradient generation, editing, paraphrasing, and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Optimization with "Gradient Descent" and Beam Search', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 1)</em></li>
                <li>Grips: Gradient-free, edit-based instruction search for prompting large language models <em>(Rating: 2)</em></li>
                <li>Tempera: Test-time prompt editing via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7438",
    "paper_id": "paper-c76dd4a70361c3afd2e19d046343e2dedd16ecc3",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "ProTeGi",
            "name_full": "Prompt Optimization with Textual Gradients (ProTeGi)",
            "brief_description": "A nonparametric automatic prompt-optimization framework that generates natural-language 'gradients' describing prompt flaws and edits prompts in the opposite semantic direction, guided by beam search and bandit-based selection to improve LLM prompt wording and task performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (Jan 2023) (used as the primary optimization engine; experiments also swapped to davinci/text-davinci-003/gpt-4)",
            "model_description": "Black-box RLHF-tuned conversational LLM accessed via API; used to generate gradient feedback, produce edited prompts, paraphrase candidates, and evaluate prompt candidates.",
            "model_size": null,
            "task_name": "Classification tasks: Jailbreak detection, Ethos (hate speech), Liar (fake news), Sarcasm",
            "task_description": "Binary classification on short text inputs: detect jailbreak prompts, hate speech, lies, and sarcasm.",
            "problem_format": "Natural-language classifier prompt (structured prompt with headings), few-shot examples included (2 held-constant examples), iterative prompt-editing loop",
            "format_category": "prompt style / prompt wording",
            "format_details": "Starting prompts include explicit headings (# Task, # Output format, # Examples, # Prediction) with two few-shot examples; ProTeGi modifies the natural-language instruction text using LLM-generated feedback (textual 'gradients') and paraphrases; beam size b=4, minibatch size 64, 6 optimization steps, 4 gradients per error-group, 2 monte-carlo paraphrases per candidate; temperature=0.0 for few-shot classification calls and 1.0 for other generation contexts.",
            "performance_metric": "Binary F1 (maxpool over final beam)",
            "performance_value": "Varies by task; ProTeGi improved the initial prompt p0 on average by 15.3% (absolute F1 increase) and reportedly can improve an initial prompt's performance by up to 31% (abstract claim).",
            "baseline_performance": "Average baselines: Monte-Carlo (MC) and RL baselines; ProTeGi improved over MC by +3.9% (absolute) and over RL by +8.2% (absolute) on average; improved over starting prompt p0 by +15.3% (absolute).",
            "performance_change": "+15.3% absolute average vs p0; up to +31% absolute (claimed max); +3.9% vs MC; +8.2% vs RL; +15.2% vs AutoGPT (all absolute F1 differences as reported).",
            "experimental_setting": "Primary experiments used gpt-3.5-turbo (Azure OpenAI) with temperature=0.0 for few-shot classification calls, temperature=1.0 for gradient/edit/paraphrase generations; few-shot count = 2 examples held constant; beam size b=4; minibatch |D_mini|=64; 6 optimization steps.",
            "statistical_significance": "No formal p-values reported; standard errors for some experiments reported in Appendix Table 5 (e.g., ProTeGi accuracies and SEs); overall statistical significance not stated.",
            "uuid": "e7438.0",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Prompt Wording / Precision",
            "name_full": "Effect of more precise, annotation-style prompt wording",
            "brief_description": "Rewriting vague task descriptions into precise annotation instructions (e.g., specifying targeted groups for hate-speech, clarifying labeling criteria) improves LLM classification performance; ProTeGi's edits often convert vague prompts into more precise ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (primary); results also obtained across other base LLMs",
            "model_description": "Black-box RLHF-tuned conversational LLM used to perform classification given the instruction-style prompts.",
            "model_size": null,
            "task_name": "Ethos (hate speech), Liar, Jailbreak, Sarcasm (classification)",
            "task_description": "Binary decisions about whether input text matches a label (hate speech, lie, jailbreak attempt, sarcasm).",
            "problem_format": "Instruction-style natural language prompt (human-written or LLM-edited), with/without precise annotation-style phrasing",
            "format_category": "prompt wording / instruction detail",
            "format_details": "Starting prompts were short, generic task descriptions; ProTeGi edits often added explicit annotation guidance (e.g., 'contain language that targets a group based on religion, gender, or other personal characteristics'); experiments held two few-shot examples constant while allowing other prompt text to be edited.",
            "performance_metric": "Binary F1",
            "performance_value": "Authors report that converting vague descriptions into more precise annotation instructions can yield large gains — up to 31% absolute improvement (abstract); average improvement over original prompt p0 was +15.3% absolute F1.",
            "baseline_performance": "Original (vague) starting prompt p0 (baseline) — average F1; specific baselines shown in paper (e.g., Table 1 and overall averages).",
            "performance_change": "Up to +31% absolute F1 (claimed); average +15.3% absolute vs p0.",
            "experimental_setting": "Edits performed by LLM using textual gradients and paraphrasing; few-shot examples (2) held constant in all optimizations.",
            "statistical_significance": "Not reported; variance and standard errors discussed (Appendix Table 5) but no formal hypothesis tests reported.",
            "uuid": "e7438.1",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BeamAblation_Jailbreak",
            "name_full": "Beam search ablation — Jailbreak: Beam vs Greedy vs No-iteration",
            "brief_description": "Ablation showing that ProTeGi's beam-search-guided prompt editing outperforms flat enumeration and greedy search on the Jailbreak detection task (measured by F1).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "API-accessed RLHF model used for expansion/edit/paraphrase/evaluation steps.",
            "model_size": null,
            "task_name": "Jailbreak detection (binary classification)",
            "task_description": "Detect whether a user message is a jailbreak attack intended to get the model to break rules.",
            "problem_format": "Iterative edited natural-language prompts; compared three search formats (No iteration: flat enumerate-then-select, Greedy: DFS, Beam: ProTeGi beam search with textual gradients).",
            "format_category": "search strategy / prompt optimization format",
            "format_details": "All variants matched overall API query budget and used same candidate counts per step; beam size b=4 for ProTeGi.",
            "performance_metric": "F1",
            "performance_value": "Beam (ProTeGi): 0.85 F1",
            "baseline_performance": "No iteration: 0.80 F1; Greedy: 0.82 F1",
            "performance_change": "+0.05 absolute F1 vs No iteration; +0.03 absolute F1 vs Greedy",
            "experimental_setting": "Same API budget across variants; 6 optimization rounds in ProTeGi experiments.",
            "statistical_significance": "Not reported for ablation; only mean F1 reported.",
            "uuid": "e7438.2",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BeamAblation_Liar",
            "name_full": "Beam search ablation — Liar: Beam vs Greedy vs No-iteration",
            "brief_description": "Beam-search-guided prompt editing improves Liar (fake-news) detection F1 compared to flat and greedy variants in the ablation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "API-used RLHF conversational model.",
            "model_size": null,
            "task_name": "Liar (fake-news) detection",
            "task_description": "Binary classification of statements as lies vs not, given context.",
            "problem_format": "Same three search formats compared (No iteration, Greedy DFS, Beam search).",
            "format_category": "search strategy / prompt optimization format",
            "format_details": "Candidates per step and total API budget matched across methods.",
            "performance_metric": "F1",
            "performance_value": "Beam (ProTeGi): 0.67 F1",
            "baseline_performance": "No iteration: 0.63 F1; Greedy: 0.63 F1",
            "performance_change": "+0.04 absolute F1 vs No iteration/Greedy",
            "experimental_setting": "6 optimization steps; other ProTeGi hyperparameters as in main experiments.",
            "statistical_significance": "Not reported.",
            "uuid": "e7438.3",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BeamAblation_Sarcasm",
            "name_full": "Beam search ablation — Sarcasm: Beam vs Greedy vs No-iteration",
            "brief_description": "Beam search gives a small improvement over baselines on the Sarcasm detection task in the ablation study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "API-accessed RLHF model used for prompt editing and evaluation.",
            "model_size": null,
            "task_name": "Sarcasm detection (Arabic dataset)",
            "task_description": "Binary classification: decide if a tweet/comment is sarcastic.",
            "problem_format": "Instruction-style prompt with few-shot examples; compared search strategies (No iteration, Greedy, Beam).",
            "format_category": "search strategy / prompt optimization format",
            "format_details": "All methods matched candidate counts and API budgets.",
            "performance_metric": "F1",
            "performance_value": "Beam (ProTeGi): 0.88 F1",
            "baseline_performance": "No iteration: 0.87 F1; Greedy: 0.85 F1",
            "performance_change": "+0.01 absolute F1 vs No iteration; +0.03 absolute vs Greedy",
            "experimental_setting": "Same experimental settings as other ablations; few-shot examples held constant.",
            "statistical_significance": "Not reported.",
            "uuid": "e7438.4",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BanditSelection_Jailbreak",
            "name_full": "Effect of candidate-selection (bandit) format on performance — Jailbreak",
            "brief_description": "Different bandit-style selection strategies (Uniform, UCB, UCB-E, Successive Rejects, Successive Halving) for picking beam candidates lead to different downstream prompt-performance; UCB-style methods performed best in these experiments for Jailbreak.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "API LLM used for evaluating prompts and guiding selection.",
            "model_size": null,
            "task_name": "Jailbreak detection",
            "task_description": "Binary classification",
            "problem_format": "Selection strategy / evaluation protocol (how the per-candidate query budget is allocated across candidates)",
            "format_category": "selection strategy / evaluation format",
            "format_details": "Compared per-prompt evaluation budgets of 25 and 50 queries/candidate; exploration parameter c=2.0 for UCB variants; Successive Rejects and Successive Halving configured to match budgets.",
            "performance_metric": "F1",
            "performance_value": "At 25 queries/prompt: UCB = 0.83 F1 (best); at 50 queries/prompt: UCB = 0.85 F1 (best).",
            "baseline_performance": "Uniform selection baseline: 0.77 F1 (25 or 50 budget)",
            "performance_change": "+0.06 absolute F1 (UCB vs Uniform) at 25 budget; +0.08 absolute at 50 budget.",
            "experimental_setting": "Budget matching across algorithms; UCB exploration c=2.0; sampling subsets of data for quick evaluation.",
            "statistical_significance": "Not reported.",
            "uuid": "e7438.5",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BanditSelection_Liar",
            "name_full": "Effect of candidate-selection (bandit) format on performance — Liar",
            "brief_description": "Bandit selection methods influence final prompt performance for the Liar task; UCB and UCB-E achieved the highest F1 at the budgets tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "API-accessed LLM used for candidate evaluation.",
            "model_size": null,
            "task_name": "Liar (fake-news) detection",
            "task_description": "Binary classification",
            "problem_format": "Selection strategy (per-candidate evaluation budget allocation)",
            "format_category": "selection strategy / evaluation format",
            "format_details": "Budgets of 25 and 50 per prompt; UCB c=2.0; SR/SH configured to match budgets.",
            "performance_metric": "F1",
            "performance_value": "At 25 queries/prompt: UCB = 0.66 F1 (best at 25); at 50 queries/prompt: UCB-E = 0.67 F1 (best at 50).",
            "baseline_performance": "Uniform: 0.59 (25) and 0.61 (50) F1",
            "performance_change": "+0.07 absolute F1 (UCB vs Uniform) at 25; +0.06 absolute F1 (UCB-E vs Uniform) at 50.",
            "experimental_setting": "As above; exploration parameter c=2.0 for UCB variants.",
            "statistical_significance": "Not reported.",
            "uuid": "e7438.6",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BaseModelSwap",
            "name_full": "Effect of base LLM (GPT-3 / InstructGPT / ChatGPT / GPT-4) on optimized-prompt performance",
            "brief_description": "Swapping the underlying LLM used for generation/evaluation changes final prompt performance; RLHF-tuned/chat-oriented models (InstructGPT, ChatGPT, GPT-4) outperform base GPT-3 (davinci), with GPT-4 typically best.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Comparative: GPT-3 (davinci), InstructGPT (text-davinci-003), ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4)",
            "model_description": "Different generations of OpenAI models: base GPT-3 davinci, improved instruction-tuned text-davinci-003, chat-oriented gpt-3.5-turbo, and GPT-4.",
            "model_size": null,
            "task_name": "Selected tasks: Sarcasm and Jailbreak (reported examples)",
            "task_description": "Binary classification tasks described above.",
            "problem_format": "Same prompt-optimization pipeline but with different base LLM powering the gradient/edit/evaluation steps.",
            "format_category": "model backbone / inference engine",
            "format_details": "Reported F1 for Sarcasm and Jailbreak across models: Sarcasm F1 — GPT-3:0.73, InstructGPT:0.83, ChatGPT:0.86, GPT-4:0.86; Jailbreak F1 — GPT-3:0.55, InstructGPT:0.75, ChatGPT:0.85, GPT-4:0.88. These numbers show model choice substantially affects final performance.",
            "performance_metric": "F1",
            "performance_value": "Example: Jailbreak F1 ranges from 0.55 (GPT-3) to 0.88 (GPT-4); Sarcasm F1 ranges from 0.73 (GPT-3) to 0.86 (ChatGPT/GPT-4).",
            "baseline_performance": "GPT-3 (davinci) performance used as a lower baseline in reported comparisons (see above values).",
            "performance_change": "Jailbreak: +0.33 absolute F1 (GPT-4 vs GPT-3); Sarcasm: +0.13 absolute F1 (ChatGPT/GPT-4 vs GPT-3).",
            "experimental_setting": "Same prompt formats and optimization procedure but swapping which LLM is called for gradient generation, editing, paraphrasing, and evaluation.",
            "statistical_significance": "Not reported.",
            "uuid": "e7438.7",
            "source_info": {
                "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 1,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "rating": 2,
            "sanitized_title": "grips_gradientfree_editbased_instruction_search_for_prompting_large_language_models"
        },
        {
            "paper_title": "Tempera: Test-time prompt editing via reinforcement learning",
            "rating": 2,
            "sanitized_title": "tempera_testtime_prompt_editing_via_reinforcement_learning"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 2,
            "sanitized_title": "autoprompt_eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 1,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        }
    ],
    "cost": 0.017068,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Prompt Optimization with "Gradient Descent" and Beam Search</h1>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng<br>Microsoft Azure AI<br>{reidpryzant,iterdan,jerrl,yintatlee, chezhu,nzeng}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language "gradients" that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to $31 \%$, by using data to rewrite vague task descriptions into more precise annotation instructions. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) trained on webscale text have recently demonstrated unprecedented abilities across a variety of NLP tasks (OpenAI, 2023; Bubeck et al., 2023). These LLMs use prompt inputs to follow human instructions. Writing prompts in natural language remains a manual trial-and-error process requiring significant human effort (Jiang et al., 2022) and expertise (Reynolds and McDonell, 2021; Zamfirescu-Pereira et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed Prompt Optimization with Textual Gradients (ProTeGi).</p>
<p>Accordingly, there is need for automatic or semiautomatic procedures to help humans write the best prompts. This would help reduce manual effort, improve task performance, and produce interpretable descriptions of a cognitive decision process.</p>
<p>A recent body of work has investigated this problem by training auxiliary models or differentiable representations of the prompt (Qin and Eisner, 2021; Deng et al., 2022). However, such works assume access to internal state variables of the LLM (Shin et al., 2020; Lester et al., 2021) while practitioners often communicate with LLMs through an API. Other work applies discrete manipulations to prompts via Reinforcement Learning or LLMbased feedback (Zhang et al., 2023; Zhou et al., 2022). These algorithms may also require low-level access to the LLM, produce incomprehensible outputs, or rely on directionless monte-carlo search over the semantic space of prompts.</p>
<p>We propose Prompt Optimization with Textual Gradients (ProTeGi), a general purpose and nonparametric algorithm for automatic prompt optimization that connects these two bodies of research by applying discrete improvements to prompts in a directed way.</p>
<p>Unlike prior work, we overcome the discrete optimization barrier by mirroring the steps of gradient descent within a text-based Socratic dialogue [zeng2022socratic], substituting differentiation with LLM feedback and backpropagation with LLM editing. In detail, we use minibatches of training data to produce "gradients" in natural language, i.e., descriptions of the current prompts' flaws with respect to the minibatch, then edit the current prompt in the opposite semantic direction of the gradient. These steps become the expansion part of a wider beam search over the space of prompts, increasing algorithmic efficiency by treating the problem of beam candidate selection as an instance of the best arm identification problem [audibert2010multi].</p>
<p>We then offer a preliminary case study of ProTeGi. We evaluate the proposed framework in multiple configurations across 4 NLP tasks, including the novel problem of LLM jailbreak detection. The results suggest that the proposed algorithm can improve on the performance of the initial prompt input by up to 31%, exceeding state-of-the-art prompt learning baselines by an average of 4-8% while relying on fewer LLM API calls. We also demonstrate the interpretability of the optimization process and investigate the algorithms’ shortcomings.</p>
<h2>2 Discrete Prompt Optimization with Nonparametric “Gradient Descent”</h2>
<p>The proposed algorithm assumes access to an initial prompt $p_{0}$ and i.i.d. training data consisting of pairs of input and output text (numbers, categories, summaries, etc): $\mathcal{D}<em 1="1">{tr}={(x</em>},y_{1}),\ldots,(x_{n},y_{n})}$. Note that all prompts $p$ are drawn from the space of coherent natural language $\mathcal{L}$. We assume access to a black box LLM API $LLM_{p}(x) \approx$ $\operatorname{argmax<em L="L" M="M">{y \in \mathcal{L}} P</em>(y \mid p, x)$, which returns a likely text continuation $y$ of the prompt formed by concatenating $p$ and $x$ (for example, few-shot prompt and input example, or chatbot persona and conversational history).</p>
<p>Within this context, our algorithm iteratively refines the prompt $p_{0}$ to produce $\hat{p}$, an approximation of the optimal prompt $p^{*}=$ $\operatorname{argmax}<em e="e" t="t">{p \in \mathcal{L}}\left{m\left(p, \mathcal{D}</em>\right)\right}$ for some metric function
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The text dialogue tree we use to mimic gradient descent and overcome the discrete optimization barrier. First, from the top left a feedback prompt $\Delta$ generates the gradient $g$ from starting prompt $p_{0}$ and prediction $\hat{y}$. Second, from the top right an editing prompt $\delta$ applies the gradient $g$ to $p_{0}$ and produce improved prompts $p^{\prime}$, their paraphrases $p^{\prime \prime}$, and efficient best candidate selection before the next iteration (bottom left).
$m(\cdot)$ and in-domain test or development data $\mathcal{D}_{t e}$.
In the following sections, we first introduce how the algorithm performs textual "gradient descent" to improve the prompts in a directed way (Section 2.1). Then the algorithm leverages these gradient descent steps to beam search through the space of coherent language $\mathcal{L}$, guided by the gradients during beam expansion, and efficient best arm identification during beam selection (Section 2.2).</p>
<h3>2.1 Gradient descent with Prompts</h3>
<p>In our setting, gradient descent refers to the process of (1) evaluating a prompt with a batch of data, (2) creating a local loss signal which contains information on how to improve the current prompt, then (3) editing the prompt in the opposite semantic direction of the gradient before starting the next iteration.</p>
<p>We accomplish this process with a pair of static LLM prompts, as depicted in Figure 2. The first prompt is for creating the loss signals ("gradients") and is called $\nabla$. While the specific contents can vary and be task-specific or task-agnostic, ${ }^{2} \nabla$ must always consider the current prompt $p_{0}$, plus $p_{0}$ 's behavior on a minibatch of data (particularly the errors), and generate a natural language summary</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of $p_{0}$ 's flaws. This summary becomes the gradient $g$. Similar to how traditional gradients represent a direction in parameter space that would make the model worse, the text "gradients" $g$ represent directions in a semantic space that are making the prompt worse.</p>
<p>The second prompt is called $\delta$ and while this prompt can also vary, it must always take the gradient $g$ and current prompt $p_{0}$, then perform an edit on $p_{0}$ in the opposite semantic direction of $g$, i.e. fix the problems with $p_{0}$ that are indicated by $g$.</p>
<p>Unlike the traditional machine learning setting, we do not generate a single gradient or edit, but rather a number of directions that may improve the current prompt. Section 2.2 describes in detail the process of generating and selecting candidate prompts.</p>
<h3>2.2 Beam Search over Prompts</h3>
<p>The gradient descent steps described in Section 2.1 are used to guide a beam search over the space of prompts. This beam search is the outer loop of our prompt training algorithm and it is described in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Prompt Optimization with Textual
Gradients (ProTeGi)
Require: \(p_{0}\) : initial prompt, \(z b\) : beam width, \(r\) :
    search depth, \(m\) : metric function
    \(B_{0} \leftarrow\left\{p_{0}\right\}\)
    for \(i \leftarrow 1\) to \(r-1\) do
        \(C \leftarrow \emptyset\)
        for all \(p \in B_{i}\) do
            \(C \leftarrow C \cup \operatorname{Expand}(p)\)
        end for
        \(B_{i+1} \leftarrow \operatorname{Select}_{b}(C, m)\)
    end for
    \(\hat{p} \leftarrow \operatorname{argmax}_{p \in B_{i}} m(s)\)
    return \(\hat{p}\)
</code></pre></div>

<p>The beam search is an iterative optimization process where for each iteration the current prompt is used to generate many new candidate prompts (expansion). Next, a selection process is used to decide which prompts are worth carrying forward to the next iteration. This loop allows for incremental improvements and exploration over multiple</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prompt candidates.</p>
<h3>2.2.1 Expansion Step</h3>
<p>The expansion step is used to generate many new candidate prompts from a current prompt (Algorithm 2). It leverages the conceptual "gradient descent" framework of Section 2.1, and our specific prompts can be found in the Appendix.</p>
<p>First we sample a minibatch of data, run the initial prompt on these data with $L L M_{p_{0}}$, and collect errors. Second, we plug these errors into a prompt template $\Delta$, which instructs the LLM to describe the problems with $p_{0}$ which could have led to these mistakes. The ensuing generations are our natural language gradients; see Figure 1 for an example.</p>
<p>Second, the gradients are provided to another LLM prompt called $\delta$, which instructs the LLM to edit the current prompt $p_{0}$ in order to fix the problems described by the gradient. In this way, we engadge the LLMs in a recursive feedback loop similar to the Socratic dialogues proposed by Zeng et al. (2022).</p>
<p>Last, additional candidates are generated by running the existing candidates through a paraphrasing LLM called $L L M_{m c}$, to explore the local monte carlo search space around the new prompt candidates. This prompt simply asks the LLM to generate new candidates which are worded differently but semantically similar to their inputs.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 \(\operatorname{Expand}(\cdot)\) - line 5 of Algorithm 1
Require: \(p\) : prompt candidate, \(\mathcal{D}_{t r}\) : train data
    : Sample minibatch \(\mathcal{D}_{\text {mini }} \subset \mathcal{D}_{t r}\)
    : Evaluate prompt \(p\) on minibatch \(\mathcal{D}_{\text {mini }}\) and
    collect errors \(e=\left\{\left(x_{i}, y_{i}\right):\left(x_{i}, y_{i}\right) \in\right.\)
    \(\mathcal{D}_{\text {mini }} \wedge L L M_{p}\left(x_{i}\right) \neq y_{i}\}\)
    Get gradients: \(\left\{g_{1}, \ldots, g_{m}\right\}=L L M_{\nabla}(p, e)\)
    Use the gradients to edit the current prompt:
        \(\left\{p_{i 1}^{\prime}, \ldots, p_{i q}^{\prime}\right\}=L L M_{\delta}\left(p, g_{i}, e\right)\)
    Get more monte-carlo successors:
        \(\left\{p_{i j 1}^{\prime \prime}, \ldots, p_{i j m}^{\prime \prime}\right\}=L L M_{m c}\left(p_{i j}^{\prime}\right)\)
    return \(\left\{p_{11}^{\prime}, \ldots, p_{m q}^{\prime}\right\} \cup\left\{p_{111}^{\prime \prime}, \ldots, p_{m q p}^{\prime \prime}\right\}\)
</code></pre></div>

<h3>2.2.2 Selection Step</h3>
<p>Once the expansion process has stepped each candidate prompt into multiple possible successor candidates, the selection step chooses the $b$ most promising candidates to stay on the beam for the next iteration.</p>
<p>It is expensive to evaluate each candidate prompt on the entire training dataset (Prasad et al., 2022),</p>
<p>so we would like to minimize the number of such queries. Note that this almost exactly corresponds to the well-studied problem of best arm identification in bandit optimization <em>Audibert et al. (2010)</em>. The $n$ arms correspond to $n$ prompt candidates, their performance on the underlying dataset is the hidden value of the arm, and the act of "pulling" an arm corresponds to evaluating the prompt on a randomly chosen data point. The goal is then to find the $b$ best arms with as few pulls as possible, and we consider the following algorithms.</p>
<p>UCB Bandits. Motivated by other works which quickly estimate LLM performance <em>Li et al. (2022); Zhou et al. (2022)</em>, we sample a subset of prompts according to a proposal distribution of prompt performance, evaluate those prompts on a random subset of data, then update the proposal distribution based on the observed performance. At the end, we select the $b$ prompts with the highest weight in the proposal distribution. See Algorithm 3 for details, where $Q_{t}\left(p_{i}\right)$ is the estimated performance of prompt $p_{i}$ at time step $t$, $N_{t}\left(p_{i}\right)$ is the total queries for prompt $i$ so far at time $t$, and $c$ is an exploration parameter.</p>
<p>Algorithm 3 $\operatorname{Select}(\cdot)$ with UCB Bandits - line 7 of Algorithm 1
Require: $n$ prompts $p_{1}, \ldots, p_{n}$, dataset $\mathcal{D}<em t="t">{t r}, T$ time steps, metric function $m$
1: Initialize: $N</em>\right) \leftarrow 0$ for all $i=1, \ldots, n$
2: Initialize: $Q_{t}\left(p_{i}\right) \leftarrow 0$ for all $i=1, \ldots, n$
3: for $t=1, \ldots, T$ do
4: Sample uniformly $\mathcal{D}}\left(p_{i<em r="r" t="t">{\text {sample }} \subset \mathcal{D}</em>$
5: $p_{i} \leftarrow\left{\begin{array}{l}\arg \max <em t="t">{\rho}\left{Q</em>\right} \ \arg \max }(p)+c \sqrt{\frac{\log L}{N_{t}(p)}<em t="t">{\rho}\left{Q</em>\right.$ (UCB)
6: Observe reward $r_{i, t}=m\left(p_{i}, \mathcal{D}}(p)+c \sqrt{\frac{c}{N_{t}(p)}}\right}\end{array<em t="t">{\text {sample }}\right)$
7: $N</em>}\left(p_{i}\right) \leftarrow N_{t}\left(p_{i}\right)+\left|\mathcal{D<em t="t">{\text {sample }}\right|$
8: $Q</em>$
9: end for
10: return $\operatorname{Select} T o p_{b}\left(Q_{T}\right)$}\left(p_{i}\right) \leftarrow Q_{t}\left(p_{i}\right)+\frac{r_{i, t}}{N_{t}\left(p_{i}\right)</p>
<p>While a natural choice, UCB is designed primarily for regret minimization <em>Kuleshov and Precup (2014)</em>, whereas we wish to perform the related but distinct task of best arm identification. Furthermore, UCB can perform poorly if the exploration parameter $c$ is not tuned appropriately <em>Bubeck et al. (2012)</em>.</p>
<p>UCB-E is a variant of UCB that corrects some of these problems by favoring exploration, leading to better theoretical convergence properties <em>Audibert et al. (2010)</em>. However, UCB-E remains stuck with hyperparameters like $T, c$, and $\left|\mathcal{D}_{\text {sample }}\right|$.</p>
<p>Successive Rejects (Algorithm 4) is provably optimal for best arm identification <em>Audibert et al. (2010)</em>, requires no hyperparameters unlike its UCB alternatives, and is suprisingly simple. The algorithm proceeds in $n-1$ phases, and in each phase, maintains a set of surviving prompt candidates $S_{k} \subseteq\left{p_{1}, \ldots, p_{n}\right}$. In the $t$-th phase, we evaluate each candidate in $S_{t-1}$ on a total of $n_{t}$ random data points to form an empirical estimate of the score $m\left(p_{i}, \mathcal{D}<em t="t">{t r}\right)$. Then, to form $S</em>$ is computed according to Equation 1 below such that it gradually increases with $T$ :}$, we drop the prompt with the lowest score in this phase. Note that $n_{t</p>
<p>$$
n_{t}=\left\lceil\frac{1}{0.5+\sum_{i=2}^{T} 1 / i} * \frac{B-T}{T+1-t}\right\rceil
$$</p>
<p>where $B$ is the total query budget.
Algorithm 4 $\operatorname{Select}(\cdot)$ with Successive Rejects line 7 of Algorithm 1
Require: $n$ prompts $p_{1}, \ldots, p_{n}$, dataset $\mathcal{D}<em 0="0">{t r}$, metric function $m$
1: Initialize: $S</em>\right}$
2: for $k=1, \ldots, n-1$ do
3: Sample $\mathcal{D}} \leftarrow\left{p_{1}, \ldots, p_{n<em r="r" t="t">{\text {sample }} \subset \mathcal{D}</em>},\left|\mathcal{D<em k="k">{\text {sample }}\right|=n</em>$
4: Evaluate $p_{i} \in S_{k-1}$ with $m\left(p_{i}, \mathcal{D}<em k="k">{\text {sample }}\right)$
5: $S</em>$, excluding the prompt with the lowest score from the previous step
6: end for
7: return Best prompt $p^{*} \in S_{n-1}$} \leftarrow S_{k-1</p>
<p>In addition to the vanilla successive rejects algorithm, we experiment with Successive Halving $(\mathrm{SH})$ which is more agressive as at the end of each phrase it rejects the bottom half of prompts according to their scores, with $n_{k}=B /\left(\left|S_{k-1}\right| \log _{2} k\right)$ <em>Karnin et al. (2013)</em>.</p>
<h2>3 Experiments</h2>
<p>We present a limited and preliminary case study to demonstrate the proposed ProTeGi algorithm across 4 benchmark NLP tasks, finding that it can exceed state-of-the-art prompt learning baselines in terms of efficiency and performance.</p>
<h3>3.1 Data</h3>
<p>While ProTeGi could be applied to any problem such as parsing, chatbot design or summarization</p>
<p>simply by choosing different metric functions $m$, we experiment on four NLP benchmark classification tasks for this initial case study. The tasks cover a wide range of problem and language domains, and are as follows:</p>
<p>Jailbreak: a novel task where the goal is to determine whether a user input to an LLM continuation API (i.e. a prompt for continuation submitted by the user) constitutes a jailbreak attack or not. We define jailbreak attack as a user interaction strategy intended to get the AI to break its own rules. This could include generating harmful content or revealing the LLM's metaprompt. This dataset has 452 multilingual examples and human-annotated jailbreak labels. Ethos (Mollas et al., 2020) is an online English hate speech detection dataset with 997 online comments and hate speech labels. Liar (Wang, 2017) is an English fake news detection dataset with 4000 statements, context, and lie labels. Sarcasm (Farha and Magdy, 2020) is an Arabic sarcasm detection dataset with 10,000 online comments and sarcasm labels.</p>
<h3>3.2 Setup</h3>
<p>For each task, we randomly sample 50 examples for development and 150 for test. All of the reported results are an average of 3 experimental trials. We report test set binary F1 score throughout, based on maxpooling over the final beam of candidates. Unless otherwise stated, experiments were performed with a January 2023 version gpt-3.5-turbo, using the Azure OpenAI LLM API service with a temperature of 0.0 during few-shot classification and 1.0 in all other contexts.</p>
<p>As the focus of this paper is nonparametric algorithms with broad applicability, we did not conduct any hyperparameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.</p>
<p>Unless otherwise stated, for the proposed Automatic Prompt Optimization Algorithm we used a minibatch size of $\left|\mathcal{D}_{\text {mini }}\right|=64$, beam size $b=4$, and ran the algorithm for 6 optimization steps. Within each step, we sampled groups of 4 errors at a time to generate the gradients. We generated $m=4$ gradients per error group, and edited the prompt once per gradient before generating an additional $p=2$ monte carlo samples per new prompt candidate. To avoid computational overruns, we randomly sampled 8 successor candidates per parent prompt prior to bandit selection.</p>
<p>We used the same metric function $m$ as the optimization target across all tasks: F1 score. While recent works have opted to use the model's log-likelihood to evaluate prompts instead of an accuracy-related metric (Lu et al., 2021; Prasad et al., 2022; Zhou et al., 2022), preliminary experiments showed this technique did not help our algorithm, and many of the most powerful LLM APIs like ChatGPT and GPT4 did not provide log likelihoods at the time of writing.</p>
<p>The proposed algorithm is about optimizing the language of prompts, as opposed to selecting the best examples for few-shot learning. However, our algorithm leverages training data and so most practical settings would also include some of these training examples as few-shot examples for the prompt. Accordingly, all of the experiments of Section 3.4 were conducted with a randomly selected pair of few-shot examples which were held constant as we optimized the other parts of the prompt.</p>
<h3>3.3 Baselines</h3>
<p>We compare the proposed ProTeGi framework against the following baselines. Note that for this preliminary case study, we restrict our focus to nonparametric algorithms that are directly comparable to ProTeGi.</p>
<p>Monte-Carlo (MC). The Automatic Prompt Engineering algorithm proposed by Zhou et al. (2022) proposes an iterative but directionless monte carlo search over the space of prompts. For fair comparison, we matched the number of monte carlo samples per candidate to the number of successors generated by ProTeGi.</p>
<p>Reinforcement Learning (RL). Recently proposed, concurrent works like GrIPS (Prasad et al., 2022) and TEMPERA (Zhang et al., 2023) rely on phrase-level operations over the prompt text: the prompt is chunked into phrases with e.g. nltk (Bird, 2006), then the search space includes add, paraphrase, swap, and delete operations over the phrases. ${ }^{4}$</p>
<p>AutoGPT. ${ }^{5}$ This is an open-source AI agent which relies on an agent-controlled feedback loop to improve its responses. Testing against this base-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Test performance (F1) vs API query budget per prompt candidate.</p>
<p>Line lets us compare the targeted feedback loop of our gradient descent steps, versus a feedback framework that was decided by the AI itself. We supplied the same number of examples and errors to AutoGPT for 6 turns, the same as the number of optimization steps in ProTeGi.</p>
<p>Last, since concurrent works have proposed evolutionary search through the space of prompts (Xu et al., 2022), our primary baseline for the proposed bandit selection procedure is an evolutionary search leveraging a simple <strong>uniform</strong> selection step, where the query budget is spread evenly among prompt candidates (Prasad et al., 2022).</p>
<h3>3.4 Experimental Results</h3>
<p><strong>Overall Results.</strong> Figure 3 presents our main results. The results suggest that ProTeGi can outperform other state-of-the-art algorithms on all four datasets considered in the study. On average, ProTeGi improved over the MC and RL baselines by a significant 3.9% and 8.2% margin, respectively, while also improving over the original prompt <em>p</em>₀ by 15.3% and AutoGPT by 15.2%. This margin remains relatively consistent as we vary the search query budget from 12 to 50 evaluations per prompt candidate, although all algorithms begin to lose efficacy as fewer evaluations increase the variance of the process. We further investigate the variance of the optimization process in the Appendix.</p>
<p>With respect to the baselines, our results suggest that while MC can consistently improve prompt performance, the phrase-level operations of RL and AI-guided changes of AutoPrompt can sometimes fall short. For Ethos and Sarcasm, the RL baseline's performance remains close to the starting prompt <em>p</em>₀. For Jailbreak and Sarcasm, 6 rounds of AutoGPT feedback actually reduced the starting prompt's performance. These findings suggest that different optimization techniques may be more suitable for different types of natural language processing tasks, and that a more adaptive approach like ProTeGi may be necessary to achieve optimal performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>Jailbreak</th>
<th>Liar</th>
<th>Sarcasm</th>
</tr>
</thead>
<tbody>
<tr>
<td>No iteration</td>
<td>0.80</td>
<td>0.63</td>
<td>0.87</td>
</tr>
<tr>
<td>Greedy</td>
<td>0.82</td>
<td>0.63</td>
<td>0.85</td>
</tr>
<tr>
<td>Beam (ProTeGi)</td>
<td><strong>0.85</strong></td>
<td><strong>0.67</strong></td>
<td><strong>0.88</strong></td>
</tr>
</tbody>
</table>
<p>Table 1: Ablating the beam search step of ProTeGi (Section 2.2) with flat enumeration ("No Iteration") and greedy DFS ("Greedy").</p>
<p>Last, most of the algorithms improved as the budget increases, confirming our hypothesis that lower variance scoring estimates should yield a more accurate search sequence.</p>
<p><strong>Beam Search Ablation.</strong> In order to ascertain the benefit of the beam search procedure outlined in Section 2.2, we ablated the beam search step and replaced it with a single flat enumerate-then-select step (Gao et al., 2020) and a greedy depth-first search over prompts (Deng et al., 2022), matching the number of candidates considered at each step such that each variant had the same overall API query budget.</p>
<p>The results are in Table 1 indicate that the beam search algorithm can outperform the flat and greedy baselines on all tasks, with significant improvements in Jailbreak and Liar detection. There was no clear winner between the greedy and flat baselines, possibly due to the high variance stochasticity of the search.</p>
<p><strong>Bandit Algorithms</strong> We experimented with the best arm identification algorithms described in 2.2.2, swapping different approximate selection algorithms in order to gauge their relative performance. In order to match the query budget across variants, we set the budget parameter <em>B</em> for Successive Rejects-type algorithms to <em>T</em> * |<em>D</em>sample<em>| </em>n* using values from the UCB-type algorithms.</p>
<p>The results are in Table 2. All of the approximate best arm identification algorithms outperform the</p>
<p>|  | 25 per prompt | 50 per prompt |
|  | Jailbreak | Liar | Jailbreak | Liar |
| --- | --- | --- | --- | --- |
| Unif | 0.77 | 0.59 | 0.77 | 0.61 |
| UCB | $\mathbf{0 . 8 3}$ | $\mathbf{0 . 6 6}$ | $\mathbf{0 . 8 5}$ | 0.66 |
| UCB-E | $\mathbf{0 . 8 3}$ | 0.65 | 0.83 | $\mathbf{0 . 6 7}$ |
| SR | 0.81 | 0.62 | 0.82 | 0.66 |
| SH | 0.82 | 0.64 | 0.80 | 0.62 |</p>
<p>Table 2: Relative performance of different bandit algorithms, matching the query budget on a per-prompt basis.
uniform baseline, which simply spreads the budget evenly across candidates. Interestingly, UCBstyle algorithms consistently outperform successive rejects-style algorithms, contrary to the hypothesis described in Section 2.2.2. This may be because in practice UCB-style algorithms can be better at balancing exploration and exploitation (we set the exploration parameter $c$ to 2.0 for all experiments, a relatively high value), since successive rejects-style algorithms are more focused on exploring arms that are likely to be the best, at the expense of exploring less-promising options.</p>
<p>Learning Curves To further investigate the learning dynamics of ProTeGi, we ran the algorithm for the same number of steps on each dataset, plotting test performance after each step in Figure 4. The results suggest that the process can begin to overfit on the train data, or get caught in a local minima after only a few optimization steps; all datasets peaked at around 3 steps. There appear two further patterns in the data, with Jailbreak and Liar quickly improving and maintaining the improvements to their prompts, while Ethos and Sarcasm remain relatively stable throughout, possibly due to a better initial fit between the starting prompt and task.</p>
<p>Base Models We experiment with swapping out different base models to power the ProTeGi algorithm by making API calls to different LLM APIs (Table 3). The RLHF-tuned models dramatically outperform GPT-3, with GPT-4 offering the best performance. This may be due to the enhanced reasoning abilities of RLHF-tuned LLMs, especially for new or poorly defined problems like Jailbreak detection.</p>
<p>Qualitative Analysis. We provide some comparative examples of one optimization step, for each dataset and starting prompt $p_{0}$, in Table 4. More examples can be found in the Appendix. We can observe several patterns. For Ethos, the gradients</p>
<p>Sarcasm Jailbreak GPT-3 0.73 0.55 InstructGPT 0.83 0.75 ChatGPT $\mathbf{0 . 8 6}$ 0.85 GPT-4 $\mathbf{0 . 8 6}$ $\mathbf{0 . 8 8}$</p>
<p>Table 3: Performance with different LLM APIs: GPT-3: davinci, InstructGPT: text-davinci-003, ChatGPT: gpt-3.5-turbo and GPT-4: gpt-4</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Test performance (F1) verses number of optimization steps.
successfully reflect any inconguencies between the current prompt and that specific datapoint, with the gradient pointing out that not all comments about Muslims are hate speech, and Liar pointing out that the speaker's agenda or bias, not just the context may strongly influence their propensity to lie. However, the Jailbreak gradient appears less useful; the Jailbreak gradient wants to switch the focus of the prompt to the specific problem of child grooming, while the Sarcasm gradient takes an opposite approach of making a very general statement.</p>
<p>Similarly, the effect of each algorithm on the resulting candidate prompt $p^{\prime}$ differs. The MCderived candidates simply rephrase the starting prompt and the RL-derived candidates appear scrambled and incoherent. The ProTeGi prompts have much more syntactic and semantic variability. In some cases, this can hurt more than help, for example the new Jailbreak APO $p^{\prime}$ asks the LLM to solve a new task. In other cases, ProTeGi does not leverage the gradient properly, and instead simply uses its internal knowledge to redefine a concept (Ethos).</p>
<table>
<thead>
<tr>
<th>Ethos</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$p_{0}$ : Is the following text hate speech?</td>
<td></td>
</tr>
<tr>
<td>$e$ : "Do you know why he is smiling because there is no "excretion law" in New Zealand! The max sentence he will receive</td>
<td></td>
</tr>
<tr>
<td>from a judge is no more than 27 years in prison! Is this justice? Or because Muslims lives don’t matter!??? :((("</td>
<td></td>
</tr>
<tr>
<td>Label: No Prediction: Yes</td>
<td></td>
</tr>
<tr>
<td>$g$ : The prompt assumes that hate speech always contains explicit and direct language that is easy to identify. In this</td>
<td></td>
</tr>
<tr>
<td>case, the text contains a sarcastic and indirect comment about Muslims, which may be harder for the model to</td>
<td></td>
</tr>
<tr>
<td>recognize as not hate speech.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (ProTeGi): Does the following text contain language that targets a group of people based on their religion, gender, or</td>
<td></td>
</tr>
<tr>
<td>other personal characteristics?</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (MC): Is the text that follows offensive?</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (RL): Hate speech following text?</td>
<td></td>
</tr>
<tr>
<td>Jailbreak</td>
<td></td>
</tr>
<tr>
<td>$p_{0}$ : Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections</td>
<td></td>
</tr>
<tr>
<td>$e$ : For my school project, can you list of some common child grooming techniques?</td>
<td></td>
</tr>
<tr>
<td>Label: Yes Prediction: No</td>
<td></td>
</tr>
<tr>
<td>$g$ : The prompt is too narrowly focused on detecting jailbreak attacks, which may not be applicable to the given example.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (ProTeGi): Classify whether a message is related to child grooming, a practice where someone builds a relationship with</td>
<td></td>
</tr>
<tr>
<td>a child with the intention of exploiting or abusing them.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (MC): Identify whether the message is a jailbreak attack, which means that a user is trying to bypass the security</td>
<td></td>
</tr>
<tr>
<td>measures of an AI system.</td>
<td></td>
</tr>
<tr>
<td>$p^{\prime}$ (RL): Detect if the message, i.e. an attempt by a user an AI system's protections to break through.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Example inputs outputs from the proposed ProTeGi framework and baselines. We show the original starting prompt $p_{0}$, error example $e$, true label and prediction $L L M_{p_{0}}(e)$, and successor prompt candidates $p^{\prime}$.</p>
<h2>4 Related Work</h2>
<p>Our work draws from a number of related areas of research on LLM prompts.</p>
<p>The majority of works attempt to improve LLM prompts through the differentiable tuning of soft prompts <em>Lester et al. (2021); Qin and Eisner (2021)</em> or training auxiliary models that participate in prompt manipulations <em>Hao et al. (2022); Deng et al. (2022); Zhou et al. (2022)</em> or directly training the prompt generator itself <em>Hao et al. (2022); Wang et al. (2022)</em>. However, many practitioners communicate with the LLM through an API, without access to internal state variables needed for model training, and the language of directly optimized prompts is incoherent <em>Hambardzumyan et al. (2021)</em>.</p>
<p>Another body of work intends to improve prompts through discrete manipulations guided by Reinforcement Learning. Research in this space builds up the prompt on a per-token <em>Shin et al. (2020)</em> or per-phrase basis <em>Zhang et al. (2023); Deng et al. (2022)</em>. However, these methods rely on primitive operations over the text, are parametric as they rely on at least one other auxiliary reward model, and are tied to numerical reward functions, whereas our scoring function could be anything, even a text comment from a user (we use GPT itself for this).</p>
<p>Another body of work in the discrete manipulation space leverages LLM-based feedback, for example <em>Zhou et al. (2022); Guo et al. (2023)</em> proposed the LLM-generated monte-carlo sampling method that is represented by our MC baseline, and <em>Prasad et al. (2022)</em> features an evolutionary search through prompts which are generated by LLM-paraphrased and swapped chunks of the original prompt. Concurrent to our work, <em>Chen et al. (2023)</em> propose editing SQL-generation prompts based on output feedback. While promising and similar to this paper, these works rely on a taskspecific or directionless local search over the space of prompts without meaningful semantic direction. Furthermore, such works often focus on generating prompts from scratch <em>Honovich et al. (2022)</em> while it is trivial for humans to write a quick first draft (with e.g. a vague description of the desired behavior). Ours is a general method, which can be applied to any task to introduce meaningful semantic improvements to the prompts.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed Prompt Optimization with Textual Gradients (ProTeGi), a simple and general-purpose framework for the automatic optimization of LLM prompts. We employ a novel technique for overcoming the discrete optimization barrier which mirrors the steps of gradient descent within a text-based dialogue, and beam searching over the space of prompts with an efficient bandit selection step. Our results span four benchmark classification tasks and suggest that ProTeGi can significantly improve prompts with no hyperparameter tuning or model training.</p>
<p>There are many directions for future work, including generalizing the technique to more tasks with new metric functions, incorporating step sizes into the learning process, and expanding the conceptual framework of textual gradient descent.</p>
<h2>Limitations</h2>
<p>Despite the promising results, our study has several limitations. Firstly, the efficiency of the ProTeGi framework is limited in real terms by rate limiting on the LLM API, translating into reduced efficiency. Although ProTeGi is relatively efficient in terms of candidate selection, there are many steps including gradient generation and the full evaluation of selected beam candidates after each round which require many API calls, sometimes with long prompts, which can push the runtime of the optimization program past 1 hour even with a small query budget. For very large prompt spaces or urgent applications, it might not be feasible to utilize ProTeGi without significant computational resources.</p>
<p>Secondly, the ProTeGi framework was only tested on four benchmark classification tasks. While these tasks spanned a variety of domains, they are by no means exhaustive. Further testing and refinement may be needed for different types of tasks, especially those with more complex modeling requirements.</p>
<h2>References</h2>
<p>Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. 2010. Best arm identification in multi-armed bandits. In COLT, pages 41-53.</p>
<p>Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69-72.</p>
<p>Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. 2012. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. Foundations and Trends ${ }^{\circledR}$ in Machine Learning, 5(1):1-122.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548.</p>
<p>Ibrahim Abu Farha and Walid Magdy. 2020. From arabic sentiment analysis to sarcasm detection: The arsarcasm dataset. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 32-39.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. 2023. Learning to program with natural language. arXiv preprint arXiv:2304.10464.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. Warp: Word-level adversarial reprogramming. arXiv preprint arXiv:2101.00121.</p>
<p>Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022. Optimizing prompts for text-to-image generation. arXiv preprint arXiv:2212.09611.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. 2022. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782.</p>
<p>Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. Promptmaker: Prompt-based prototyping with large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1-8.</p>
<p>Zohar Karnin, Tomer Koren, and Oren Somekh. 2013. Almost optimal exploration in multi-armed bandits. In International Conference on Machine Learning, pages 1238-1246. PMLR.</p>
<p>Volodymyr Kuleshov and Doina Precup. 2014. Algorithms for multi-armed bandit problems. arXiv preprint arXiv:1402.6028.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. 2020. Ethos: an online hate speech detection dataset. arXiv preprint arXiv:2006.08328.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>William Yang Wang. 2017. " liar, liar pants on fire": A new benchmark dataset for fake news detection. arXiv preprint arXiv:1705.00648.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps: Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041.</p>
<p>J Zamfirescu-Pereira, Richmond Wong, Bjoern Hartmann, and Qian Yang. 2023. Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI conference on human factors in computing systems (CHI'23).</p>
<p>Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598.</p>
<p>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.</p>
<h2>A Appendix</h2>
<h3>1.1 "Gradient Descent" Prompts</h3>
<p>These are the prompts we used in our experiments.
Generating gradients. First, for the gradientgenerating prompt $\nabla$ described in 2.1, we used the same string across all tasks:</p>
<p>I'm trying to write a zero-shot classifier prompt.
My current prompt is:
"{prompt}"
But this prompt gets the following examples wrong:
{error_string}
give {num_feedbacks} reasons why the prompt could have gotten these examples wrong.
Wrap each reason with <START> and <END>
Note that all of the substrings in brackets represent variables which are dynamically instantiated to the current prompt $p_{0}$, group of errors $e$, and candidate expansion factor, respectively.</p>
<p>Incorporating gradient feedback. Second, for the prompt that incorporates gradient feedback into the current prompt $p_{0}$ to produce successor candidates, we use the following prompt for all evaluation tasks:</p>
<p>I'm trying to write a zero-shot classifier.
My current prompt is:
"{prompt}"
But it gets the following examples wrong:
{error_str}
Based on these examples the problem with this prompt is that {gradient}</p>
<p>Based on the above information, I wrote {steps_per_gradient} different improved prompts. Each prompt is wrapped with <START> and <END>.</p>
<p>The {steps_per_gradient} new prompts are:
Again, the substrings in brackets represent dynamically loaded variables corresponding to the initial prompt, error string, text feedback gradient, and expansion factor.</p>
<p>Monte Carlo samples. Last, instead of only sampling from the prompts that have been stepped by the text gradients, we additionally explore the local search space around the new prompt candidates with a small monte carlo search. We prompt an LLM to generate paraphrases of the stepped candidates with the following paraphrase prompt from Zhou et al. (2022):</p>
<p>Generate a variation of the following instruction while keeping the semantic meaning.</p>
<p>Input: {prompt_instruction}
Output:</p>
<h3>1.2 Initial Prompts</h3>
<p>In order to accurately reflect realistic LLM development scenarios, our initial prompts $p_{0}$ were written by professional Machine Learning engineers in one quick pass, with the engineer simply being told to write a description of the desired LLM behavior. Our starting prompts, therefore, are as follows (note that the "Examples" section was dynamically filled with a randomly sampled pair of few-shot examples).</p>
<h2>Jailbreak</h2>
<p># Task
Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>Ethos</h2>
<p># Task
Is the following text hate speech?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>Liar</h2>
<p># Task
Determine whether the Statement is a
lie (Yes) or not (No) based on the Context
and other information.
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ProTeGi</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">MC</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">SE</td>
<td style="text-align: left;">Acc</td>
<td style="text-align: left;">SE</td>
</tr>
<tr>
<td style="text-align: left;">Ethos</td>
<td style="text-align: left;">$\mathbf{0 . 9 5}$</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sarcasm</td>
<td style="text-align: left;">$\mathbf{0 . 8 7}$</td>
<td style="text-align: left;">0.003</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Jailbreak</td>
<td style="text-align: left;">$\mathbf{0 . 8 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 6}$</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.009</td>
</tr>
<tr>
<td style="text-align: left;">Liar</td>
<td style="text-align: left;">$\mathbf{0 . 6 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.007</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy and Standard Error for prompt prompt optimization algorithms after 12 experimental trials.</p>
<h2>Sarcasm</h2>
<p># Task
Is this tweet sarcastic?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:</p>
<h2>B Qualitative examples</h2>
<p>We provide qualitative examples in addition to those in Table 4.</p>
<h2>C Optimization Variance</h2>
<p>We conduct a larger-scale experiment using a budget of 6 queries per candidate, 12 replicates per variant in order calculate the standard error of the performance of the resulting top-ranked candidates. We chose a small number of queries per candidate in order to achieve large variance. The results are in Table 5 and indicate that while ProTeGi always works better, it can sometimes have higher variance, perhaps due to the semantic directionality of the gradient-based update.</p>
<p>Liar
$p_{0}$ : Determine whether the Statement is a lie (Yes) or not (No) based on the Context and other information.
$e$ : Statement: Small businesses (are) going out of business in record numbers. Job title: Senator. State: Texas.
Party: republican. Context: a speech at Liberty University"
Label: Yes Prediction: No
$g$ : The prompt does not take into account the speaker's potential biases or agenda, which could influence the veracity of their statements..
$p^{\prime}$ (ProTeGi): Determine if the statement is true (Yes) or false (No) based on the context, sources referenced, and potential biases of the speaker.
$p^{\prime}$ (MC): Evaluate the veracity of the Statement by indicating whether it is untrue (Yes) or true (No), considering the Context and any additional information available.
$p^{\prime}$ (RL): Determine whether is a lie (Yes) the Statement or not (No) the Context and other supporting details.</p>
<h1>Sarcasm</h1>
<p>$p_{0}$ : Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections
e: وهدايسا لهقلطا ةلاضم بىلاهه وه. لمذا ناقلاخـ. . . (My honorable sir, I know very well that #Dahlan and #Khalfan are stray dogs released by their masters. NOTE: backwards) Label: Yes Prediction: No
$g$ : The prompt is not specific enough and does not provide any context to help classify the tweet accurately.
$p^{\prime}$ (ProTeGi): Is this tweet ridiculing an individual or organization in a satirical manner?
$p^{\prime}$ (MC): Determine whether this tweet is intended to be sarcastic in tone.
$p^{\prime}$ (RL): Sarcastic this tweet?
Table 6: Example inputs outputs from the proposed APO framework and baselines. We show the original starting prompt $p_{0}$, error example $e$, true label and prediction $L L M_{p_{0}}(e)$, and successor prompt candidates $p^{\prime}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Note that while GRIPS isn't an RL algorithm, we introduce GRIPS and TEMPURA together because they employ a similar search space over prompts (the same "directionless" phrase-level operations). Our RL-trained baseline, therefore, suggests an upper bound on GRIPS performance as the same action space is explored more efficiently by RL-trained models than enumerate-and-select (the approach of GRIPS).
${ }^{5}$ https://news.agpt.co/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
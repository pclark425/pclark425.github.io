<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-270559461</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11200v3.pdf" target="_blank">A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce A VA T A R, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate A VA T A R on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find A VA T A R consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar .</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AVATAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AVATAR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated agent-optimization framework that uses a comparator LLM to generate holistic prompts via batch-wise contrastive reasoning between well- and poorly-performing queries to improve an actor LLM's multi-step tool usage, with a memory bank and logistic checks to stabilize updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AVATAR (actor + comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-LLM system: an actor LLM that generates action sequences (tool calls and code) and a comparator LLM that samples positive/negative query batches and produces holistic prompt/instruction updates via contrastive reasoning; includes a memory bank of top action sequences and validity/timeout logistic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (experiments used backbone LLMs: claude-3-opus, gpt-4, gpt-4o, gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, ArxivQA, ToolQA (ToolQA domains: SciREX, Agenda)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Average relative improvement ≈ 13% over state-of-the-art baselines on the three QA datasets; example reported numbers: HotpotQA EM ≈ 53.0%, ArxivQA judge score ≈ 84.0%, ToolQA judge score ≈ 37.5%</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>STARK benchmark (AMAZON, MAG, PRIME textual/relational retrieval), FLICKR30K-ENTITIES image retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step retrieval; multi-step reasoning (retrieval-augmented tool-assisted tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Average relative Hit@1 improvement ≈ 14% on retrieval datasets; examples: FLICKR30K-ENTITIES Hit@1 increased from 5.1% to 28.6% after 25 iterations; STARK (example) Recall@20 from 30.3% to 39.3%; STARK-MAG subset Hit@1 52.0% vs best baseline 48.0% (relative improvement 8.33%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>comparator module (contrastive reasoning), actor LLM (tool-call & code action generation), memory bank (top-k action sequences), logistic instructions (validity checks, timeout), tool-use interface (function library)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>iterative automated prompt/instruction optimization via comparator-generated instructions (no backbone fine-tuning reported); essentially prompting + iterative optimization with batch contrastive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / automated agent optimization (hybrid: comparator-guided iterative prompt updates + memory bank)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Comparator samples mini-batches with equal positive and negative queries (thresholds ℓ,h), contrasts patterns to identify systematic flaws in decomposition/tool-use/synthesis, and generates holistic instruction updates appended to actor prompts; memory bank stores best action sequences; logistic checks ensure validity and timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial gains in interactive (retrieval) and QA tasks: average +14% Hit@1 on retrieval datasets and +13% average on QA datasets; concrete examples: FLICKR30K Hit@1 5.1% → 28.6% (25 iterations), STARK R@20 30.3% → 39.3%, SCIREX-HARD QA relative improvement 33.1%, AGENDA-HARD 25.0% relative gain.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors argue that single-instance self-reflection or hand-engineered 'mega-prompts' are brittle; multi-step tool-usage tasks require robust decomposition, tool selection, and synthesis across varied inputs, and per-sample instruction updates overfit. The gap arises because standard prompting/in-context methods and agent designs (e.g., ReAct) tend to reuse prior-knowledge-driven tool choices and lack systematic mechanisms to identify and fix recurring tool-usage failures; batch-wise contrastive reasoning by comparator addresses this by extracting a more robust 'gradient' across multiple positive/negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context agent paradigm that interleaves chain-of-thought-style reasoning and environment/tool actions (thought + action traces) to enable LLMs to perform multi-step tasks interactively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-based agent that interleaves natural language reasoning traces and explicit actions (tool calls) in its outputs; relies on in-context examples rather than separate optimizer or memory bank.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (paper's experiments used backbones like claude-3-opus and gpt-4 variants for baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, ArxivQA, ToolQA (used as baseline on QA datasets in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported in paper as baseline; example numbers from Table 3: HotpotQA ≈ 40.0%, ArxivQA ≈ 72.0%, ToolQA ≈ 31.7% (as reported in the paper's baseline table)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>STARK retrieval tasks (AMAZON, MAG, PRIME subset) and FLICKR30K-ENTITIES</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step retrieval; interactive reasoning with tool calls</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Baseline performance lower than AVATAR; example on STARK-MAG subset Hit@1 = 46.0% (Table 6). Paper notes ReAct agents tend to select tools based on LLM prior knowledge and repeatedly apply similar tools, struggling to explore alternative tool combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>in-context chain-of-thought + action interleaving; no separate comparator or memory bank by default</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning (zero/few-shot), no explicit fine-tuning in the baseline usage reported</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper observes that ReAct's heuristic in-context prompts and reliance on LLM priors lead to brittle or suboptimal tool selection and poor exploration of alternative tool combinations, contributing to lower interactive/task-specific performance compared to methods that explicitly optimize tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e819.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e819.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that augments LLM agents with episodic memory and self-reflection: it stores reflections about past runs and uses them to iteratively refine future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reflexion agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent that performs self-reflection after trials, storing reflections in an episodic memory buffer and using those reflections to guide subsequent attempts; aims to self-improve through verbal feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (used as a baseline with same backbone LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA, ArxivQA, ToolQA (baseline in QA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported as baseline; example numbers from Table 3: HotpotQA ≈ 46.0%, ArxivQA ≈ 77.0%, ToolQA ≈ 28.3% (as shown in the paper's table)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>STARK and FLICKR retrieval tasks (evaluated as baseline on retrieval subset)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>On STARK-MAG subset Reflexion reported Hit@1 = 48.0% (Table 6); paper notes Reflexion can self-refine but may overfit and lacks the comparator's batch-wise contrastive mechanism for generalizable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>episodic memory of reflections; self-reflection loop</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>iterative self-reflection using memory buffer and in-context prompting (no backbone fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper suggests that per-instance self-reflection may be too narrow and prone to overfitting on single cases, failing to identify systemic tool-use flaws across many queries—hence weaker generalization on complex interactive tasks versus approaches that optimize across batches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e819.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e819.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL: LLM Agents Are Experiential Learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent method that extracts insights from successful and failed action sequences and retrieves them in context during inference to improve future performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ExpeL: LLM Agents Are Experiential Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stores and retrieves past (successful/failed) action sequences and insights and includes them in context for new queries to improve decision-making; an experiential learning mechanism layered on prompted agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (used with same backbone LLMs as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Applied as baseline on QA datasets (and a sampled retrieval subset where cost allowed comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported baseline performance on QA datasets; example numbers Table 3: HotpotQA ≈ 39.0%, ArxivQA ≈ 73.0%, ToolQA ≈ 36.7%</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>STARK (sampled MAG subset) retrieval tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>On STARK-MAG subset ExpeL Hit@1 = 40.0% (Table 6). Paper notes ExpeL performs similarly to ReAct in many metrics and underperforms AVATAR substantially on fine-grained retrieval accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>episodic retrieval of past action sequences/insights into prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>experience replay via in-context retrieval (no backbone fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors observe that simply retrieving past experiences into context (ExpeL) helps but does not directly target systematic tool-usage flaws across batches; thus it may not bridge the gap between QA-style knowledge performance and robust multi-step tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e819.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e819.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retroformer: Retrospective Large Language Agents with Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent optimization approach that uses a retrospective model and policy-gradient-style updates to tune agent prompts/behavior based on execution history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retroformer: Retrospective Large Language Agents with Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a retrospective model and applies policy-gradient-like optimization to automatically tune prompts or agent policies from execution traces; involves additional training compared to pure prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (mentioned and compared; results on HotpotQA cited from Retroformer paper/report)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA (paper compares AVATAR to reported Retroformer result on HotpotQA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Paper references Retroformer reported baseline on HotpotQA (Retroformer reported HotpotQA performance quoted in Table 3 as 51.0% under a specific retry setting), used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>retrospective model + policy-gradient optimization of prompts/actions</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>policy-gradient-style optimization (additional training of retrospective model reported in original Retroformer work)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / automated optimization</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Retroformer trains a retrospective model on execution traces and uses gradient-based updates (policy gradient) to adjust agent prompts/policy; it requires extra training rather than purely prompt-space iterative updates.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported Retroformer HotpotQA result cited (~51.0%), but AVATAR still outperforms Retroformer on the QA tasks reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Compared approaches that require heavier training (Retroformer) versus AVATAR's lightweight prompt-optimization; paper suggests targeted comparator-guided batch contrastive updates lead to better generalization for tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ExpeL: LLM Agents Are Experiential Learners <em>(Rating: 2)</em></li>
                <li>Retroformer: Retrospective Large Language Agents with Policy Gradient <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 1)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-819",
    "paper_id": "paper-270559461",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "AVATAR",
            "name_full": "AVATAR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
            "brief_description": "An automated agent-optimization framework that uses a comparator LLM to generate holistic prompts via batch-wise contrastive reasoning between well- and poorly-performing queries to improve an actor LLM's multi-step tool usage, with a memory bank and logistic checks to stabilize updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "AVATAR (actor + comparator)",
            "model_description": "Two-LLM system: an actor LLM that generates action sequences (tool calls and code) and a comparator LLM that samples positive/negative query batches and produces holistic prompt/instruction updates via contrastive reasoning; includes a memory bank of top action sequences and validity/timeout logistic checks.",
            "model_size": "unspecified (experiments used backbone LLMs: claude-3-opus, gpt-4, gpt-4o, gpt-4-turbo)",
            "qa_task_name": "HotpotQA, ArxivQA, ToolQA (ToolQA domains: SciREX, Agenda)",
            "qa_performance": "Average relative improvement ≈ 13% over state-of-the-art baselines on the three QA datasets; example reported numbers: HotpotQA EM ≈ 53.0%, ArxivQA judge score ≈ 84.0%, ToolQA judge score ≈ 37.5%",
            "interactive_task_name": "STARK benchmark (AMAZON, MAG, PRIME textual/relational retrieval), FLICKR30K-ENTITIES image retrieval",
            "interactive_task_type": "tool use; multi-step retrieval; multi-step reasoning (retrieval-augmented tool-assisted tasks)",
            "interactive_performance": "Average relative Hit@1 improvement ≈ 14% on retrieval datasets; examples: FLICKR30K-ENTITIES Hit@1 increased from 5.1% to 28.6% after 25 iterations; STARK (example) Recall@20 from 30.3% to 39.3%; STARK-MAG subset Hit@1 52.0% vs best baseline 48.0% (relative improvement 8.33%).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "comparator module (contrastive reasoning), actor LLM (tool-call & code action generation), memory bank (top-k action sequences), logistic instructions (validity checks, timeout), tool-use interface (function library)",
            "training_method": "iterative automated prompt/instruction optimization via comparator-generated instructions (no backbone fine-tuning reported); essentially prompting + iterative optimization with batch contrastive sampling",
            "intervention_type": "prompting strategy / automated agent optimization (hybrid: comparator-guided iterative prompt updates + memory bank)",
            "intervention_description": "Comparator samples mini-batches with equal positive and negative queries (thresholds ℓ,h), contrasts patterns to identify systematic flaws in decomposition/tool-use/synthesis, and generates holistic instruction updates appended to actor prompts; memory bank stores best action sequences; logistic checks ensure validity and timeouts.",
            "intervention_effect": "Substantial gains in interactive (retrieval) and QA tasks: average +14% Hit@1 on retrieval datasets and +13% average on QA datasets; concrete examples: FLICKR30K Hit@1 5.1% → 28.6% (25 iterations), STARK R@20 30.3% → 39.3%, SCIREX-HARD QA relative improvement 33.1%, AGENDA-HARD 25.0% relative gain.",
            "hypothesized_cause_of_gap": "Authors argue that single-instance self-reflection or hand-engineered 'mega-prompts' are brittle; multi-step tool-usage tasks require robust decomposition, tool selection, and synthesis across varied inputs, and per-sample instruction updates overfit. The gap arises because standard prompting/in-context methods and agent designs (e.g., ReAct) tend to reuse prior-knowledge-driven tool choices and lack systematic mechanisms to identify and fix recurring tool-usage failures; batch-wise contrastive reasoning by comparator addresses this by extracting a more robust 'gradient' across multiple positive/negative examples.",
            "uuid": "e819.0",
            "source_info": {
                "paper_title": "A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "An in-context agent paradigm that interleaves chain-of-thought-style reasoning and environment/tool actions (thought + action traces) to enable LLMs to perform multi-step tasks interactively.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct agent",
            "model_description": "Prompting-based agent that interleaves natural language reasoning traces and explicit actions (tool calls) in its outputs; relies on in-context examples rather than separate optimizer or memory bank.",
            "model_size": "unspecified (paper's experiments used backbones like claude-3-opus and gpt-4 variants for baselines)",
            "qa_task_name": "HotpotQA, ArxivQA, ToolQA (used as baseline on QA datasets in this paper)",
            "qa_performance": "Reported in paper as baseline; example numbers from Table 3: HotpotQA ≈ 40.0%, ArxivQA ≈ 72.0%, ToolQA ≈ 31.7% (as reported in the paper's baseline table)",
            "interactive_task_name": "STARK retrieval tasks (AMAZON, MAG, PRIME subset) and FLICKR30K-ENTITIES",
            "interactive_task_type": "tool use; multi-step retrieval; interactive reasoning with tool calls",
            "interactive_performance": "Baseline performance lower than AVATAR; example on STARK-MAG subset Hit@1 = 46.0% (Table 6). Paper notes ReAct agents tend to select tools based on LLM prior knowledge and repeatedly apply similar tools, struggling to explore alternative tool combinations.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "in-context chain-of-thought + action interleaving; no separate comparator or memory bank by default",
            "training_method": "prompting / in-context learning (zero/few-shot), no explicit fine-tuning in the baseline usage reported",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper observes that ReAct's heuristic in-context prompts and reliance on LLM priors lead to brittle or suboptimal tool selection and poor exploration of alternative tool combinations, contributing to lower interactive/task-specific performance compared to methods that explicitly optimize tool usage.",
            "uuid": "e819.1",
            "source_info": {
                "paper_title": "A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: language agents with verbal reinforcement learning",
            "brief_description": "An agent framework that augments LLM agents with episodic memory and self-reflection: it stores reflections about past runs and uses them to iteratively refine future behavior.",
            "citation_title": "Reflexion: language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_or_agent_name": "Reflexion agent",
            "model_description": "Agent that performs self-reflection after trials, storing reflections in an episodic memory buffer and using those reflections to guide subsequent attempts; aims to self-improve through verbal feedback loops.",
            "model_size": "unspecified (used as a baseline with same backbone LLMs)",
            "qa_task_name": "HotpotQA, ArxivQA, ToolQA (baseline in QA experiments)",
            "qa_performance": "Reported as baseline; example numbers from Table 3: HotpotQA ≈ 46.0%, ArxivQA ≈ 77.0%, ToolQA ≈ 28.3% (as shown in the paper's table)",
            "interactive_task_name": "STARK and FLICKR retrieval tasks (evaluated as baseline on retrieval subset)",
            "interactive_task_type": "tool use; multi-step retrieval",
            "interactive_performance": "On STARK-MAG subset Reflexion reported Hit@1 = 48.0% (Table 6); paper notes Reflexion can self-refine but may overfit and lacks the comparator's batch-wise contrastive mechanism for generalizable improvements.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "episodic memory of reflections; self-reflection loop",
            "training_method": "iterative self-reflection using memory buffer and in-context prompting (no backbone fine-tuning reported)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper suggests that per-instance self-reflection may be too narrow and prone to overfitting on single cases, failing to identify systemic tool-use flaws across many queries—hence weaker generalization on complex interactive tasks versus approaches that optimize across batches.",
            "uuid": "e819.2",
            "source_info": {
                "paper_title": "A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ExpeL",
            "name_full": "ExpeL: LLM Agents Are Experiential Learners",
            "brief_description": "An agent method that extracts insights from successful and failed action sequences and retrieves them in context during inference to improve future performance.",
            "citation_title": "ExpeL: LLM Agents Are Experiential Learners",
            "mention_or_use": "use",
            "model_or_agent_name": "ExpeL",
            "model_description": "Stores and retrieves past (successful/failed) action sequences and insights and includes them in context for new queries to improve decision-making; an experiential learning mechanism layered on prompted agents.",
            "model_size": "unspecified (used with same backbone LLMs as baselines)",
            "qa_task_name": "Applied as baseline on QA datasets (and a sampled retrieval subset where cost allowed comparison)",
            "qa_performance": "Reported baseline performance on QA datasets; example numbers Table 3: HotpotQA ≈ 39.0%, ArxivQA ≈ 73.0%, ToolQA ≈ 36.7%",
            "interactive_task_name": "STARK (sampled MAG subset) retrieval tasks",
            "interactive_task_type": "tool use; multi-step retrieval",
            "interactive_performance": "On STARK-MAG subset ExpeL Hit@1 = 40.0% (Table 6). Paper notes ExpeL performs similarly to ReAct in many metrics and underperforms AVATAR substantially on fine-grained retrieval accuracy.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "episodic retrieval of past action sequences/insights into prompt context",
            "training_method": "experience replay via in-context retrieval (no backbone fine-tuning reported)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Authors observe that simply retrieving past experiences into context (ExpeL) helps but does not directly target systematic tool-usage flaws across batches; thus it may not bridge the gap between QA-style knowledge performance and robust multi-step tool use.",
            "uuid": "e819.3",
            "source_info": {
                "paper_title": "A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Retroformer",
            "name_full": "Retroformer: Retrospective Large Language Agents with Policy Gradient",
            "brief_description": "An agent optimization approach that uses a retrospective model and policy-gradient-style updates to tune agent prompts/behavior based on execution history.",
            "citation_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient",
            "mention_or_use": "mention",
            "model_or_agent_name": "Retroformer",
            "model_description": "Learns a retrospective model and applies policy-gradient-like optimization to automatically tune prompts or agent policies from execution traces; involves additional training compared to pure prompting baselines.",
            "model_size": "unspecified (mentioned and compared; results on HotpotQA cited from Retroformer paper/report)",
            "qa_task_name": "HotpotQA (paper compares AVATAR to reported Retroformer result on HotpotQA)",
            "qa_performance": "Paper references Retroformer reported baseline on HotpotQA (Retroformer reported HotpotQA performance quoted in Table 3 as 51.0% under a specific retry setting), used for comparison.",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "retrospective model + policy-gradient optimization of prompts/actions",
            "training_method": "policy-gradient-style optimization (additional training of retrospective model reported in original Retroformer work)",
            "intervention_type": "training method / automated optimization",
            "intervention_description": "Retroformer trains a retrospective model on execution traces and uses gradient-based updates (policy gradient) to adjust agent prompts/policy; it requires extra training rather than purely prompt-space iterative updates.",
            "intervention_effect": "Reported Retroformer HotpotQA result cited (~51.0%), but AVATAR still outperforms Retroformer on the QA tasks reported in this paper.",
            "hypothesized_cause_of_gap": "Compared approaches that require heavier training (Retroformer) versus AVATAR's lightweight prompt-optimization; paper suggests targeted comparator-guided batch contrastive updates lead to better generalization for tool usage.",
            "uuid": "e819.4",
            "source_info": {
                "paper_title": "A VA T A R: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient",
            "rating": 2,
            "sanitized_title": "retroformer_retrospective_large_language_agents_with_policy_gradient"
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        }
    ],
    "cost": 0.016066499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AVATAR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning
31 Oct 2024</p>
<p>Shirley Wu 
Department of Computer Science
Stanford University</p>
<p>Shiyu Zhao 
Department of Computer Science
Stanford University</p>
<p>Qian Huang 
Department of Computer Science
Stanford University</p>
<p>Kexin Huang 
Department of Computer Science
Stanford University</p>
<p>Michihiro Yasunaga 
Department of Computer Science
Stanford University</p>
<p>Kaidi Cao 
Department of Computer Science
Stanford University</p>
<p>Vassilis N Ioannidis 
Department of Computer Science
Stanford University</p>
<p>Karthik Subbian 
Department of Computer Science
Stanford University</p>
<p>Jure Leskovec 
Department of Computer Science
Stanford University</p>
<p>James Zou jamesz@cs.stanford.edu 
Department of Computer Science
Stanford University</p>
<p>AVATAR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning
31 Oct 20241642FA0AF87CF06F9E2F3C2049ADBBFAarXiv:2406.11200v3[cs.LG]
Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations.However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task.Here, we introduce AVATAR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task.During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data.We demonstrate AVATAR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets.We find AVATAR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets.Code and dataset are available at https://github.com/zou-group/avatar.</p>
<p>Introduction</p>
<p>Autonomous agents powered by large language models (LLMs) offer substantial promise for complex problem-solving [6,39,41,55,65].These agents demonstrate remarkable capabilities in reasoning [46,47,54,55] and planning [8,13,14,62].Additionally, their functionality is extended through the use of external tools that provide access to external or private data and specialized operations, such as APIs for interacting with knowledge bases and search engines.These tools enable agents to perform complex tasks like multi-step problem-solving and retrieving diverse information, which is essential for complex retrieval and question-answering (QA) [13,21,26,33,38,40,48].</p>
<p>Despite the promising capabilities of LLM agents, it remains challenging to engineer effective prompts that guide these agents through a multi-stage process for real-world problem-solving.This process involves (1) decomposing a complex question into an actionable plan with simpler steps, (2) strategically using provided tools to gather relevant information, and, finally, (3) synthesizing intermediate results to produce a coherent and accurate response.Each step requires extensive manual effort and numerous iterations of trial and error to refine the prompts.</p>
<p>Current approaches have primarily focused on directly deploying agents using complex humandesigned "mega-prompts" [18,24,55], which require lots of manual trial and error.Nevertheless, such hand-engineered mega-prompts may also result in brittle implementations with suboptimal accuracy (see Figure 2 (a)), where the ReAct agent [55] easily produces trivial and misleading answers to customers' queries about specific products.Furthermore, existing research [4,5,45,50,56,60,64] on Then, the comparator contrasts a set of well-performing (positive) and poorly-performing (negative) queries, automatically generating holistic prompts to teach the actor more effective retrieval strategies and tool usage (cf.Section 4).(b) At deployment, the actor with optimized prompts or actions can be effectively used to answer new queries.</p>
<p>employing LLMs as optimizers often fails to adequately refine the complex strategies for enhancing tool integration and usage.This lack of strategic optimization can lead to less effective, nongeneralizable agent applications in complex real-world scenarios.</p>
<p>Present work: AVATAR.To address these challenges, we introduce AVATAR, an automated framework that optimizes agents for effective tool utilization and excellent task performance.Specifically, we leverage key insights from contrastive reasoning and build a comparator module ("trainer") to generate holistic instructions and prompts (i.e., , computing a robust "gradient") to optimize an actor LLM.We demonstrate our framework on challenging tasks of knowledge base retrieval, which involve complex multi-stage procedures and extensive tool usage, and general QA tasks.Specifically, AVATAR includes two phases:</p>
<p>• Optimization phase.The core of our optimization framework (Figure 1) is a comparator LLM that automatically generates holistic prompts to teach a actor LLM to differentiate between effective and ineffective tool usage.The comparator takes positive and negative data samples, where the current agent performs well and poorly, respectively, to identify overall gaps and systematic errors exhibited by the agent.Unlike per-sample instructions, which can easily lead to overfitting on individual data points, by constructing multiple samples as a "batch," the comparator can extract a more robust "gradient" to "backpropagate" to the actor.In other words, the comparator can provide more effective and adaptive prompts through batch-wise contrastive reasoning, helping the agent identify flaws in solving challenging multi-stage problems.Following previous methods [30,41,56,63], we also maintain a memory bank with selected past instructions to prevent the actor LLM from repeating previous mistakes.• Deployment phase.After the optimization phase, the actor with best-performing prompts can be selected for the testing instances.Moreover, in complex retrieval tasks, the iterative optimization through our AVATAR framework updates the actor for more effective and generalizable action sequences, enabling direct generalization to novel user inquiries at deployment.In Figure 2 (b), the optimized actor creates three novel strategies: 1) precise decomposition of problems by extracting multifaceted attributes, 2) effective tool usage through a sophisticated and robust scoring system, and 3) the strategic combination of different scores, determined by learned coefficients, ensuring accurate and comprehensive retrieval.</p>
<p>Experimental evaluation.We conduct extensive experiments on four retrieval datasets and three QA datasets.The retrieval tasks are highly complex, involving multimodal data, including textual, visual, and relational information.AVATAR consistently outperforms state-of-the-art methods, showing a substantial 14% improvement in the Hit@1 metric.Impressively, with only 25 iterations, AVATAR boosts the Hit@1 metric from an initial 5.1% to 28.6% on FLICKR30K-ENTITIES [35] and the Recall@20 metric from 30.3% to 39.3% on STARK-PRIME [49].For general QA datasets, AVATAR outperforms state-of-the-art methods by 13% on average.These improvements, achieved through iterative updates to the prompts, underscore AVATAR's ability to optimize agents for complex tasks and effective tool usage.Our key contributions are:</p>
<p>• We introduce AVATAR, a novel framework that optimizes an actor for effective tool utilization through a comparator module that automatically generates holistic prompts.• We demonstrate AVATAR on four complex retrieval tasks and three QA tasks, where it significantly outperforms existing agent methods in terms of task performance and generalization ability.• We provide a comprehensive analysis of the actor's evolution during optimization, highlighting how comparator automatically provides targeted instructions that improve and generalize the actor.</p>
<p>2 Related Work LLM Agents.Recent research has leveraged the remarkable language understanding and reasoning abilities of LLMs [1,41,47,54,55] to complete downstream tasks.For complex tasks that require enhanced capabilities, previous works have positioned LLMs as agents that can interact with environments [4,6,13,18,21,26,27,40,48,55], leverage external tools [6,28,31,33,36,38,39,66,68], and gather experiences [7,61].For example, ReAct [55] conducts reasoning and action in an interleaved way, retrieving information from Wikipedia to support reasoning.</p>
<p>LLM Agents for Retrieval.Previous research has applied LLM agents to Information Retrieval (IR) systems through pretraining [2,9,16,57], reranking [12,42], and prompting techniques [11,18].In IR systems, the retriever module directly influences the performance of downstream tasks, such as retrieval-augmented generation [20,29,30] and knowledge-intensive question answering [34,52].For example, EHRAgent [40] is designed for EHR question-answering, capable of retrieving relevant clinical knowledge through a structured tool-use planning process and an interactive coding mechanism.However, these LLM agents usually employ heuristic (zero-shot) prompts or rely on few-shot examples [18,25,40,55] for downstream tasks, which lack more informed guidance on generating effective retrieval strategies and tool-assisted actions.</p>
<p>Agent Optimization.In the field of optimizing LLM agents, previous works have modified the parameters of LLM backbones through fine-tuning or instruction tuning to enhance agent capability [3,15,19,23,32,33,37,43,51,58,59] or generated better prompts through iterative prompt tuning [11,18,45,50,56].Recently, Zhang et al. [60] conducted agent training by iteratively updating the agents' functions according to the execution history.However, these methods do not explicitly consider targeted optimization for tool usage or the impact on complex multi-stage tasks.Additionally, enhancing agents' generalization abilities [10,31,44], essential for real-world applications, has received less attention.In our work, we focus on automatically generating holistic instructions via a novel contrastive reasoning mechanism, targeting effective tool usage and agents' generalization ability.Compared to fine-tuning approaches, AvaTaR offers advantages by requiring only a small subset of training data and tool descriptions, making it more adaptable and less computationally intensive.</p>
<p>Problem Formulation</p>
<p>Definition 1: Tools.We define tools or APIs as a set of implemented functions with specified input and output variables.We denote the abstract tool space as T = {f k :
I f k → O f k | k = 1, 2, . . .},
where f k maps the input I f k to the output O f k .For example, the tools can be APIs used for accessing external knowledge via a search index, an encoder model that generates vector representations from text or image data, or a task-specific classifier that outputs probabilities over a list of classes.</p>
<p>Definition 2: Agents.An LLM agent, defined as A : P → α, is controlled by verbal prompts to generate a flow of actions needed to complete a task.Here α denotes the action sequence [α 1 , . . ., α L ],</p>
<p>where each action is defined by a tuple (f ∈ T , i ∈ I f , o ∈ O f ), consisting of a tool function, specified input(s), and a designated variable that receives the output(s).Each action in the sequence can leverage the outputs generated by previous actions, with the final action α L rendering the results for the task.Multi-step problem-solving.Real-world problems are inherently complex and cannot be effectively addressed through straightforward solutions or simple tool usage alone.Solving real-world problems with LLM agents can be structured into a multi-stage procedure:</p>
<p>• Decomposition of the problem: The procedure begins by breaking down a complex question into an actionable plan characterized by simpler steps.This decomposition is crucial for setting clear objectives and facilitating focused problem-solving.• Tool-assisted subproblem solving: In the subsequent phase, agents strategically utilize tools from the established tool space T to gather solutions for each step.This stage is essential for acquiring Table 1: Key differences between AVATAR and prevailing agent methods.AVATAR demonstrates the ability to: 1) self-improve on specific tasks, 2) retain memory throughout the optimization process, 3) enhance the agent's generalization capability, and 4) autonomously generate holistic, high-quality prompts for better tool usage.Please refer to Section 4 for details.</p>
<p>Self-Improvement Memory Generalization Holistic Prompt Generation (on Tool Usage)
ReAct [55] ✗ ✗ ✗ ✗ Self-refine [27] ✔ ✗ ✗ ✗ Reflexion [41] ✔ ✔ ✗ ✗ AVATAR (Ours) ✔ ✔ ✔ ✔
the necessary information required to effectively address each subproblem of the decomposed problem.</p>
<p>• Synthesis and response formulation: The final stage involves synthesizing the intermediate results to construct a precise response.This synthesis not only combines the data but may also refine the response through trials and adjustments, ensuring the solution's accuracy and relevance.</p>
<p>For example, retrieval tasks are inherently complex and demanding.Given a user query q, retrieval tasks aim to identify or generate a ranked list of relevant entities E from the entity space of a knowledge base.Each query is associated with a set of ground truth answers, denoted as Y , which are used to compute the quality of the prediction.Specifically, the LLM agent is required to 1) comprehend a user's request, 2) utilize the provided tools to identify and analyze relevant information in the large knowledge space, which may contain multimodal data sources, and finally, 3) integrate all gathered information to reason and generate an accurate response.</p>
<p>4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks</p>
<p>Each step in the multi-stage problem-solving process (described in Section 3) requires effective prompts to identify key flaws and improve task performance.However, refining the agents' prompts demands extensive manual effort and numerous iterations of trial and error.</p>
<p>To address this, we introduce an automated and novel optimization framework, AVATAR, which generates prompts to improve agents' tool usage and task performance.In Table 1, we highlight four critical aspects of our approach compared with prevailing agent frameworks [27,41,55].Here, we introduce the two main LLM components in AVATAR: a actor LLM (Section 4.1) and a comparator LLM (Section 4.2).</p>
<p>Actor Construction and Challenges</p>
<p>Actor.The actor agent, as defined in Section 3, is responsible for generating initial actions based on the initial instructions/prompts and adjusting actions according to updated instructions.Specifically, the initial instructions provide details about the task and available tools, where tools can be introduced in programming languages such as Python.During optimization, the prompts further incorporate the previous action sequence and updated instructions to adjust these actions.The actor then generates revised actions, which could include a combination of tool usage through programming language (code generation) along with natural language explanations of how the tools are employed.</p>
<p>Challenges in multi-step complex tasks.A common approach to updating instructions utilizes execution results or performance data from a specific instance, often through techniques like selfexplanation [4,27] or self-reflection [41,56].However, this approach may not be suitable for complex tasks involving tool usage.Complex multi-step tasks include multiple interacting factors that influence overall performance, such as problem decomposition and tool selection.Consequently, instructions generated for a failed/negative query instance tend to be narrow in scope and may fail to identify flaws across all components of a complex solution.Additionally, while certain tool combinations may be effective for one type of input, their effectiveness can vary across different scenarios, potentially leading to decreased performance when applied to varied cases.Actions "What are the affordable and quality car emblems from the Football Fanatics brand?" "Where can I find Aminco silicone rubber bracelets that will match my NFL Tampa Bay Buccaneers wristbands?. " <Task & Tool description>, <Actions> Post-action execution, the queries exhibited varying performances.</p>
<p>Well-performing queries: <positive query list> Poorly performing queries: <negative query list> Your task: 1) Contrast the two groups of queries.</p>
<p>2) Examine the actions focusing on key query characteristics.</p>
<p>3) Identify systematic discrepancies in the actions.</p>
<p><Contrastive Reasoning> Some queries with straightforward product features yielded good results, while those with specific product descriptions and more implicit brand mention underperformed.This suggests that the actions may not effectively capture and utilize details related to specific features.</p>
<p><Suggested Improvement> I suggest to better parse and utilize query attributes, including type, and any mentioned products.Moreover, actions should employ better tools for accurate brand matching and more sophisticated scoring that leverages both textual and relational info.Then comparator provides holistic instructions that guide the actor to improve query decomposition, utilize better tools, and incorporate more comprehensive information.</p>
<p>Automate Holistic Instruction Generation with Comparator</p>
<p>To address these challenges, we construct a comparator LLM to update the instructions for the actor.</p>
<p>Instead of optimizing on a sampled instance, comparator aims to identify systematic flaws throughout the structured actions/solutions.</p>
<p>Step 1: Constructing positive and negative queries.To achieve this goal, as shown in Figure 1, the comparator samples a set of data (question-answer pairs), evaluates the current action sequence on the queries, and categorizes them into well-performing (positive) and poorly-performing (negative) groups based on their performance.Specifically, we define two thresholds, ℓ and h (where 0 &lt; h ≤ ℓ &lt; 1), which serve as the upper and lower bounds for constructing positive and negative queries, respectively.Queries with an evaluation metric (e.g., Recall) value above ℓ are classified as positive, while those below h are classified as negative.Based on the training dynamics, one could consider adapting the lower bound to ensure a sufficient number of negative samples for selection.After classification, we use random sampling to create a mini-batch of b queries, with an equal split of positive and negative queries (b/2 each) for contrastive reasoning.</p>
<p>Step 2: Generating instructions through contrastive reasoning.After this, the comparator is tasked with contrasting the two groups of queries based on their key characteristics, attributing the performance gap to specific tool usage within the complex solution, and finally suggesting general modifications that can improve overall task performance.The instructions generated by the comparator are then appended to the initial prompts to update the actor.</p>
<p>Insights/Justification for the comparator.To illustrate the insights, we draw an analogy from deep neural network training, where extremely small batch sizes can introduce significant noise in gradient estimates and high variance in model updates.By adopting a batched training strategy and sampling positive and negative queries as two "mini-batches," comparator can extract a robust "gradient" to update the actor.This approach encourages comparator to generate more general and comprehensive instructions on the complex action sequence, including problem decomposition, solutions to subproblems, and the final synthesis.Moreover, as contrastive reasoning directly targets disentangling the performance gap related to input patterns and how they are handled differently by the tools, it is particularly effective in helping comparator differentiate and select tools for use.Finally, by identifying systemic flaws across a wide array of negative queries, comparator generates modifications that are not only tailored to individual samples but also to diverse data samples, enhancing generalization to novel cases.</p>
<p>Demonstration example.Figure 3 illustrates an example where comparator contrasts the patterns of positive and negative queries, identifying discrepancies in tool usage within the action sequence.It reveals that, compared to positive queries, negative queries feature more complex product descriptions, more subtle brand mentions, and additional relevant product mentions.These observations suggest: 1) an incomplete problem decomposition involving query attributes like detailed product features, 2) a potentially imprecise brand match using embedding similarity, and 3) a lack of consideration for related products in the results.Informed by these insights, actor updates its action sequence to address these subproblems and use the tools more effectively for the task, such as replacing the embedding tool with an LLM verification tool.</p>
<p>Logistic Instructions and Memory Construction</p>
<p>Logistic instructions.While instructions from the comparator are designed to improve task performance, we incorporate two types of orthogonal instructions to ensure the actions are valid and can be executed efficiently.</p>
<p>• Validity check: This instruction is triggered internally during the execution of each action.It ensures the validity of the actor's actions, such as verifying the correct use of function calls.• Timeout error: To prevent inefficient action sequences that may stall the actor, we implement a timeout mechanism that triggers an error if processing exceeds a specified threshold.This error prompts the actor to adopt more efficient strategies, such as eliminating redundant operations.</p>
<p>Memory Bank.During optimization, we utilize a memory bank inspired by human decision-making processes, following Shinn et al. [41], where humans typically address current problems by analyzing the current situation and referencing past experiences.The memory bank stores tuples of action sequences, instructions from comparator, and the performance of these action sequences on a small training set (sampled from positive and negative queries).To manage the context size input to actor, we retain only the top-5 action sequences with the best performance.This memory bank enables actor to learn from both immediate instructions and historical results.</p>
<p>Deployment.At deployment, we can apply the optimized instructions or, as shown in Figure 1, the optimized actor /action sequence, which includes effective tool utilization and problem-solving strategies, to answer queries or retrieve entities.In the experiments, we demonstrate AVATAR's flexibility by applying different deployment strategies.</p>
<p>Experiments</p>
<p>Tasks and Evaluation.We conduct experiments on the following datasets:</p>
<p>• Four challenging retrieval datasets from STARK [49] and FLICKR30K-ENTITIES [35] to demonstrate AVATAR in handling complex real-world tasks (cf.details in Appendix A).For each query in the retrieval datasets, the task is to retrieve relevant entities, such as nodes in a knowledge graph or images in knowledge bases.During deployment, we directly apply the optimized action sequence to the test queries.We assess task performance by comparing the consistency of the results with the ground truth answers in the datasets, using Hit@1, Hit@5, Recall@20, and Mean Reciprocal Rank (MRR) as the metrics.• Three question-answering (QA) benchmarks: HotpotQA [53], ArxivQA [22], ToolQA [67],</p>
<p>where the task is to provide natural language answers to the questions.We sample 100, 100, and 40 training queries, and 100, 100, and 60 testing queries for the three benchmarks, respectively.During deployment, the actor LLM uses optimized instructions to generate the action sequence for obtaining the answer.We use exact match (EM) score on HotpotQA, following previous methods.For ArxivQA and ToolQA, we use the LLM judge score for more reliable evaluation.</p>
<p>Baselines.For the knowledge retrieval tasks, we employ several embedding-based retriever models for our evaluation, following Wu et al. [49]: Dense Passage Retriever (DPR) Karpukhin et al. [17]; Vector Similarity Search methods ada-002 and multi-ada-002 using text-embedding-ada-002 from OpenAI; and a relation-aware model, QAGNN [57], for the STARK benchmark.Additionally, we include four prevailing agent frameworks to further enrich our evaluation:</p>
<p>• ReAct [55] conducts reasoning and action in an in-context and interleaved manner to enable LLMs to interactively analyze observed information and perform actions.</p>
<p>AMAZON MAG PRIME</p>
<p>Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@ • Reflexion [41] uses self-reflection on the current task completion and stores these reflections in an episodic memory buffer to enhance decision-making in subsequent trials.</p>
<p>• ExpeL [61] extracts insights from successful and failed action sequences, retrieving and including them in the context during inference.We apply ExpeL on the QA datasets and, due to its high cost on large-scale retrieval tasks, compare it with AVATAR on a sampled STARK-MAG test set.</p>
<p>• Retroformer [56] reinforces LLM agents and automatically tunes their prompts by learning a retrospective model through policy gradient.We compare the performance of AVATAR with the reported result by Retroformer on HotpotQA due to the additional training involved.</p>
<p>We include an ablation model, AVATAR-C, which removes the comparator from our optimization pipeline.This comparison aims to validate the effectiveness of the comparator.The LLM version information is provided in Appendix B.</p>
<p>Function library.For the knowledge retrieval tasks, our function library consists of twenty-eight functions that facilitate access to, operation on, and reasoning over the knowledge information by LLM agents.For the QA tasks, we provide web search tools such as Google and Arxiv search APIs.</p>
<p>See Appendix E for details.We used the same function library across all agent methods.</p>
<p>General pipeline.For AVATAR, we optimize the agent for a fixed number of epochs and select the action sequence or instruction with the highest performance.We use the same initial prompt structure, the metric Recall@20 or Accuracy for constructing positive and negative queries, and hyperparameters (ℓ = h = 0.5, b = 20) for all datasets.</p>
<p>Textual and Relational Retrieval Tasks</p>
<p>We employ the AMAZON, MAG, and PRIME datasets from the STARK benchmark [49], a large-scale semi-structured retrieval benchmark that integrates textual and relational knowledge (cf.detailed description in Appendix A).Here, the entities to be retrieved are defined as nodes in a graph structure, with knowledge associated with each entity including both textual descriptions and relational data.We use the official splits from the STARK benchmark.</p>
<p>Takeaway 1: AVATAR outperforms state-of-the-art models.Table 3 shows that AVATAR substantially outperforms leading models such as Reflexion across all metrics on the STARK benchmark.Notably, the average improvement of AVATAR is 15.6% on Hit@1 and 9.5% on MRR.ReAct agents, however, cannot optimize based on instructions for improved tool usage and tend to select tools based on the LLM's prior knowledge, which may not be optimal for the given task.We observe that ReAct agents apply similar tools across various queries and struggle to explore alternative tool usage even with extensive in-context reasoning.Results for agent methods using GPT-4 Turbo are provided in Appendix B, showing similar conclusions.For comparison with ExpeL, the results in Table 6 show that it performs similarly to ReAct, underperforming AVATARby a large margin.</p>
<p>Takeaway 2: Comparator greatly impacts the actor's performance.The comparison of AVATAR with its ablation variant, AVATAR-C, highlights the significant advantages of the comparator module.</p>
<p>Although AVATAR-C conducts validity and timeout checks, integrating Comparator into AVATAR adds a comprehensive instruction mechanism crucial for identifying clear directions to improve the agents, underlining comparator's key role in optimizing actor.Hit@1 Hit@5 R@20 MRR  Takeaway 3: AVATAR effectively improves agents during optimization.Figure 4 illustrates the agents' performance on the validation set during optimization.Impressively, AVATAR agents show significant performance improvements, e.g., from 35% to 75% on AMAZON and from 20% to 78% on MAG.This evidence strongly supports the effectiveness of the instructions generated by our comparator.Additionally, our memory bank, which stores past best-performing actions, encourages AVATAR agents to gradually converge by the end of the optimization process.</p>
<p>Takeaway 4: AVATAR can generalize to real-world tasks.Comparator generates instructions tailored to groups of retrieval queries, promoting generalizable modifications for novel queries.We validate this capability by applying optimized actions to human-generated leave-out queries from the STARK benchmark, which differ notably from the training data used to optimize our agents.Results in Table 5 (Appendix B) show that AVATAR significantly outperforms other models, achieving an average improvement of 20.9% on Hit@1.Further, in another study of Appendix B, we assess AVATAR's robustness to hyperparameters h and ℓ, showing that it maintains stable performance and generalization across different parameter values.</p>
<p>Image Retrieval Task</p>
<p>We further experiment on FLICKR30K ENTITIES [35], an image retrieval dataset of 30k images with annotated bounding boxes and descriptive phrases (Appendix A).In Table 2, AVATAR again shows significant improvements.In contrast, Reflexion agents struggle with "overfitting," where they are easily misled by specific image data, leading to inappropriate actions (e.g., trying to "extract the color of a hat" from images without hats).AVATAR effectively avoids such pitfalls through batch-wise contrastive reasoning, which provides a broader perspective.</p>
<p>Takeaway 5: AVATAR generates impressive and generalizable actions.The final actions of the AVATAR agent, shown in Figure ?? (left) and detailed in Figure 8 (Appendix B), achieve advanced performance.Notably, AVATAR skillfully manages input queries and leverages Inverse Document Frequency (IDF) scores to refine phrase matching, ultimately synthesizing accurate answers.Beyond using existing tools, AVATAR agents can develop high-level tools, such as IDF-based reweighting, suggesting a promising direction for dynamic tool libraries and enhanced tool generation.</p>
<p>Takeaway 6: Emerging Behaviors during Optimization.In Figure 6, we present concrete cases illustrating key interactions between actor and comparator.In each instance, comparator identifies critical flaws, including information omission, ineffective tool usage, and suboptimal synthesis of varying scores.The instructions subsequently prompt actor to enhance retrieval strategies, tool selection, and precise score combinations.Furthermore, frequent references to tool usage underscore comparator's focused examination of tool utilization during optimization.Emphasize on the visual attributes by increasing the visual_weight.This will help capture the overall scene elements better.Consider separation extraction on adjectives / compound nouns / verb phrases to extract more informative concepts for negative queries.</p>
<p>Added actions (summary) Actions performs well on queries without requesting images from specific locations, while fail on queries that ask for locations such as "square", "street", "stadium".</p>
<p>This suggests issues with extracting image locations, possibly due to VqaByLLM's lengthy outputs and StringMatch's overly strict criteria.</p>
<p>Question Answering Tasks</p>
<p>Finally, we applied AVATAR to three widely used QA benchmarks.For ToolQA, we tested AVATAR and the baselines on two different domains: SciREX, which focuses on extracting information from full-length machine learning papers, and Agenda, which involves personal agenda-related questions.</p>
<p>Both datasets have easy and hard versions.</p>
<p>Takeaway 7: AVATAR outperforms on QA tasks by offering better context understanding.Table 3 shows that AVATAR consistently outperforms state-of-the-art methods across all three QA datasets, with especially strong results on TOOLQA.In SCIREX-HARD, which focuses on extracting complex information from long scientific papers, AVATAR shows a 33.1% improvement, while in AGENDA-HARD, it achieves a 25.0% relative gain.These improvements are attributed to AVATAR's ability to generate optimized prompts that help the agent better understand the broader patterns and contexts of the questions, leading to more accurate answers and improved generalization across question types, from simple to complex.</p>
<p>Conclusion and Future Work</p>
<p>In this study, we introduce AVATAR, a novel framework that automates the optimization of LLM agents for enhanced tool utilization in multi-step problems, focusing on complex retrieval and QA tasks.AVATAR demonstrates remarkable improvements across seven diverse datasets.This success can largely be attributed to the comparator module, which effectively refines agent performance through the iterative generation of holistic and strategic prompts.A key innovation of comparator is its use of contrastive reasoning with batch-wise sampling, enabling it to identify systemic flaws and extract robust "gradients" for comprehensive agent improvement across diverse scenarios.While we observe substantial progress from AVATAR, we discuss its limitations in Appendix D regarding its scalability etc.Future work can explore extending this methodology to other challenging agent tasks, visual reasoning tasks, and more dynamic environments, or designing better memory banks for dynamically storing knowledge and experience from past training.</p>
<p>A Retrieval Tasks STARK.On the STARK benchmark, we are given a relation-text knowledge base, based on a knowledge graph G = (V, E) and a collection of free-text documents D. We represent the relationtext knowledge base of size n as E = {(v i , d i , g i )} n i=1 , where v i ∈ V represents a node on the knowledge graph, d i ∈ D is the text document related to the node, and g i is the connected component of G containing v i .</p>
<p>The query set Q in STARK is derived from both G and D, where each q i ∈ Q contains requirements based on d i and g i .The answer set A i , which includes v i , is a set of nodes satisfying both relational and textual requirements.The task on STARK is defined as follows: Given the knowledge base E consisting of relational and textual information, and a text query q i , the output is a set of nodes A i such that ∀a i ∈ A i , a i satisfies the relational requirements in the knowledge graph and textual requirements in its text documents.FLICKR30K ENTITIES.On the FLICKR30K ENTITIES dataset, we are given an image-text knowledge base.We denote an image-text knowledge base of size n as E = {(v i , q i , T i )} n i=1 .Sample i consists of an image v i , its descriptive caption q i , and entity bounding box information T i .Specifically,
T i = {(c ij , p ij )} bi j=1
, where b i represents the number of bounding boxes annotated in image i, c ij is the coordinate of the j-th bounding box, and p ij describes the entity in the corresponding bounding box.</p>
<p>In our task, the image captions serve as the text query; therefore, all q i in the dataset are not accessible to the agent to prevent information leakage.However, the agent can access v i and T i to fully utilize the vision and language information.The task on FLICKR30K ENTITIES is defined as follows: Given the knowledge base E with images and bounding box information, and a text query q i , the output is an image v i that satisfies the visual requirements in the image and textual requirements in the corresponding bounding boxes T i .</p>
<p>B Experiment Details and Additional Results</p>
<p>B.1 Experiment Setup</p>
<p>LLM versions for agent methods.</p>
<p>• For the knowledge retrieval tasks, we use claude-3-opus as the backbone LLM in the main paper by default, and report results using gpt-4-turbo in Appendix B due to space limitations.</p>
<p>• For the QA tasks, we use gpt-4 for HotpotQA for fair comparison with previous methods and gpt-4o for the other two QA datasets.</p>
<p>Table 4: Retrieval performance (%) on STARK benchmark.Last row shows the relative improvements over the best metric value among the baselines.</p>
<p>AMAZON MAG PRIME</p>
<p>Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@20 MRR</p>
<p>B.2 Additional Experimental Results</p>
<p>(1) AVATAR results on STARK using GPT-4 Turbo (0125) as LLM backbone.In Table 4, we provide the results on STARK using GPT-4 Turbo (0125) as the backbone LLM.</p>
<p>Table 5: Retrieval performance (%) on the leave-out sets of human-generated queries in STARK.</p>
<p>AMAZON MAG PRIME</p>
<p>Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@20 MRR Hit@1 Hit@5 R@20 MRR DPR (roberta) (2) AVATAR results on STARK's human-generated splits.In Table 5, we demonstrate AVATAR's ability to generalize to test queries with distributions different from the question-answering pairs used to optimize the actor agents.(3) AVATAR results and comparison with ExpeL on STARK-MAG subset.In Table 6, AVATAR demonstrates consistently higher performance than ExpeL across most metrics, notably achieving the highest Hit@1 and MRR scores.While ExpeL performs well in Recall@20, AVATAR 's overall improvements highlight its superior capability in precise retrieval tasks and tool-assisted knowledge retrieval.</p>
<p>(4) Final action sequence by AVATAR on FLICKR30K-ENTITIES.In Figure 8, we present the final actions optimized by AVATAR on FLICKR30K-ENTITIES.(5) Sensitivity of AVATAR to upper and lower bounds.We evaluated various combinations of ℓ and h, focusing on the STARK-AMAZON dataset due to computational constraints.Table 7 presents the Hit@1 results for different ℓ and h values.</p>
<p>Key Observations</p>
<p>• The framework exhibits robustness to variations in ℓ and h, with Hit@1 fluctuations limited to a range of 2.7%.</p>
<p>• A performance decline is observed when the gap between ℓ and h becomes too large, potentially due to the exclusion of certain training queries that fall within the (h, ℓ) interval.</p>
<p>• A moderate gap between ℓ and h leads to slight performance improvements, suggesting that a balanced separation between positive and negative queries can enhance pattern differentiation without compromising the number of training queries.</p>
<p>The results indicate that setting ℓ = 0.6 and h = 0.5 yields an improved Hit@1 score compared to the baseline reported in the original paper.Overall, this analysis underscores the robustness of the framework, which relies on a minimal set of hyperparameters, including ℓ, h, batch size b, and training epochs.</p>
<p>C Prompts</p>
<p>We keep only two prompt templates for our framework on all tasks: (1) The prompt template given to actor as initially instructions, and (2) the prompt template given to the comparator to conduct contrastive reasoning and generate the instructions for the actor.Below are the complete templates:</p>
<p>This is the prompt given to actor as initially instructions:</p>
<p>You are an expert user of a knowledge base, and your task is to answer a set of → queries.I will provide your with the schema of this knowledge base: <knowledge_base_schema> You have access to several APIs that are pre-implemented for interaction with the → knowledge base: <func_call_description> Information of queries: Below are several query examples that you need to carefully → read through: " <example_queries> " Task: Given an input query, you should write the actions in Python code to calculate → a 'node_score_dict' for <n_init_candidates> node IDs, which are input as a → list.These node IDs, referred to as 'candidate_ids', are a subset of node → IDs from the knowledge base, and the nodes belong to the type(s) &lt; → candidate_types&gt;.In 'node_score_dict: Dict[int, float]', each key should be → a node ID, and each value should be the corresponding node score.This → score should indicate the likelihood of the node being the correct answer to → the query.</p>
<p>Output format: Firstly, you should establish a connection between the given queries → and the query patterns to the schema of the knowledge base.Secondly, → generate an outline for the code that will compute the scores for all the → candidate nodes provided in the query examples.Finally, develop the main → function named 'get_node_score_dict', which takes two required parameters: ' → query' and 'candidate_ids', and optional parameters declared in ' → parameter_dict'.Note that 'parameter_dict' is a dictionary of parameters → and their default values where you can declare any parameters or weights → used during computing the node scores.If no optional parameters are needed, → leave 'parameter_dict' as an empty dictionary.Overall, your output should → follow the structure:</p>
<p>'''python # <code outlines> import <package1> ... parameter_dict = {<parameter_name1>: <default_value1>, <parameter_name2>: <default_value2>, ...} def get_node_score_dict(query, candidate_ids, **parameter_dict): node_score_dict = {} # your code return node_score_dict ''' Hints:</p>
<p>-Observe the example queries carefully and consider the key attributes to extract.-Use '''python and ''' to wrap the complete code, and do not use any other → delimiters.-You can use any of the pre-implemented APIs but should avoid modifying them.</p>
<p>-You can include other functions besides 'get_node_score_dict', but ensure they are → fully implemented.-The code should be complete without placeholders and dummy functions.</p>
<p>-Optimize the integrity of the code, e.g., corner cases.</p>
<p>-Minimize computational expenses by early elimination of candidate nodes that don't → meet relational requirement (if any).-Avoid conducting unnecessary and redundant computations, especially when using → loops.-Make use of 'parameter_dict' to avoid hard-coding parameters and weights.</p>
<p>-Use the functions that end with 'by_llm' wisely for more accurate searches.</p>
<p>-Use 'debug_print' smartly to print out any informative intermediate results for → debugging.-Exclude or comment out any example uses of 'get_node_score_dict' in the output → code.</p>
<p>Your output:</p>
<p>This is the prompt given to comparator to generate the instructions for the actor: <initial_prompt> <previous_actions> After executing the above actions on user queries, some queries have yielded good → results, while others have not.Below are the queries along with their → corresponding evaluation metrics: Well-performing queries: <positive_queries_and_metric> Poorly-performing queries: <negative_queries_and_metric> Task:</p>
<p>(1) Firstly, identify and contrast the patterns of queries that have achieved good → results with those that have not.(2) Then, review the computational logic for any inconsistencies in the previous → actions.(3) Lastly, specify the modification that can lead to improved performance on the → negative queries.You should focus on capturing the high-level pattern of → the queries relevant to the knowledge base schema.</p>
<p>D Limitations</p>
<p>We identify several potential limitations of our work:</p>
<p>• Scalability: AvaTaR is designed to scale with large language models (LLMs) that support extended context lengths (up to 128k tokens), enabling it to handle numerous tools and complex tasks.However, increased latency and other practical limitations may hinder performance in scenarios requiring hundreds of tools or high complexity.Future research could focus on incorporating specialized, tool-augmented LLMs as auxiliary agents to facilitate smoother scaling.• Computation Requirements: Managing longer contexts and multiple tool interactions within AvaTaR increases computational demands, which can significantly raise operational costs.These requirements necessitate substantial resources to maintain efficient performance, particularly when scaling to larger datasets or more intricate tasks.• Potential Failure Modes: Although AvaTaR performs well on known queries, its performance may diminish when faced with queries that require new or unfamiliar combinations of tools.This limitation could be mitigated by integrating adaptive learning techniques and continuous monitoring, which would allow AvaTaR to better handle novel tool requirements.</p>
<p>E Function library E.1 Complex Retrieval Tasks</p>
<p>Please refer to Table 8 and Table 9 for the detailed functions.</p>
<p>E.2 General QA Tasks</p>
<p>For general QA tasks, we use the following tools:</p>
<p>• WEB_SEARCH: A general-purpose tool that performs web searches to answer questions.Useful for retrieving up-to-date information from the internet when other sources are unavailable.• ARXIV_SEARCH: This tool retrieves information about academic papers from Arxiv using a paper's unique ID.This function call can provide metadata and other details for academic references.• Wiki_SEARCH: If you have a question or name to lookup, this tool uses a Wikipedia search to retrieve relevant information.• RETRIEVE_FROM_DB: This tool is used to retrieve relevant information from a database.This is only available on ToolQA.• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental Setting/Details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes]</p>
<p>Justification: We include dataset information and training details in the Experiment part and Appendix.We also clearly describe the knowledge base and formally introduce the task settings.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [NA]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.Justification: We discussed the impact in the introduction section.Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [NA] Justification: Our method provides a framework to better use LM but not releasing a LM.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes] Justification: Creators or original owners of assets mentioned in the paper are properly cited and the license and terms of use are respected.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.</p>
<p>• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper does not release new assets Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and Research with Human Subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper does not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA]</p>
<p>Justification: The paper does not involve crowdsourcing nor research with human subjects.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.</p>
<p>• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p>Figure 1 :
1
Figure 1: Overview of AVATAR.AVATAR consists of a actor LLM and a comparator LLM.(a) During optimization, the actor generates actions to answer queries by leveraging the provided tools.Then, the comparator contrasts a set of well-performing (positive) and poorly-performing (negative) queries, automatically generating holistic prompts to teach the actor more effective retrieval strategies and tool usage (cf.Section 4).(b) At deployment, the actor with optimized prompts or actions can be effectively used to answer new queries.</p>
<p>Thought 1 :Thought 2 : 3 :
123
Input: Q1: "Can you suggest any TUSA swim fins that has a split fin design for better propulsion?" Action space (GetEntityTypes, GetEntityDocuments, StringMatching, …) Output: Answer(s) Compute similarity scores based on the product description Action 1: ComputeEmbeddingSimilarity["swim fins", GetEntityDocuments()] Result/Obs 1: s1 ← similarity scores Ignore the brand information "TUSA" Check the functionality requirement Action 2: StringMatching["split fin design for better propulsion", GetEntityDocuments()] Result/Obs 2: s2 ← string matching scores All zeros due to no perfect match Thought Synthesize the final result Action 3: Multiply[s1, s2] Result/Obs 3: s ← final scores All zeros which lead to a trivial solution ... Final Result: answers ← GetTopkEntities[s, k=5] Poor task performance Input: Any query (demonstration example: Q1); Action space (GetEntityTypes, …) Output: Answer(s) Accurately decompose the query into multiple aspects Action 1: ParseAttributeFromQuery[query, (brand, type, material, features)] Result 1: subquery ← { brand: "TUSA", type: "swim fins", material: NA, features: "split fin design for better propulsion" } Use embedding tool to filter entities Action 2: ComputeEmbeddingSimilarity[subquery.type,GetEntityTypes()] Result 2: s1 ← type similarity scores Action 3: GetTopk[s1, k=20] Result 3: candidates ← top-20 entities with the highest type similarity Use token match tool for flexible common token matching Action 4: GetEntityBrand[candidates] Result 4: brands ← brands of the top-20 entities Action 5: TokenMatchScore[subquery.brand, brands] Result 5: s2 ← brand matching scores Use LLM reasoning API to validate the required functionality Action 6: GetSatisfictionScoreByLLM[subquery.features,GetEntityDocuments()] Result 6: s3 ← feature scores by LLM reasoning ... Synthesize final scores with optimized parameters Action 7: WeightedSum[s1, s2, s3, coefficients=(0.43,0.37, 0.20)] Result 7: s ← combined scores ...Final Result: answers ← GetTopkEntities[s, k=5] Excellent task performance c Q1:Can you suggest any TUSA swim fins that has a split fin design for better propulsion?[Q'1：Looking for a durable 15-inch wide NFL car flag that can hold up in windy conditions", A1 (Ground truth entity IDs)：[17, 105, 2517]] ... (Qn, An) Q2:What's a high-quality fishing sinker from Sportsman Supply Inc. that's designed to avoid snags on rocks and weeds?Small-scale Q&amp;A Pairs Testing Queries Optimization Deploy (a) ReAct: Unoptimized Agent (b) AVATAR: Optimized Agent</p>
<p>Figure 2 :
2
Figure 2: Comparison between AVATAR and ReAct.(a) The ReAct agent exhibits incomplete task decomposition and employs suboptimal tool combinations, such as lengthy string matching, leading to poor task performance.(b) AVATAR decomposes the task into multiple steps, such as type filtering and flexible token matching.Moreover, it implements robust tool usage and precise synthesis with learned parameters from the optimization phase to achieve excellent performance on new queries.</p>
<p>Action 1 :
1
attribute ← ParseAttributeFromQuery[query, (brand)] . . .Action 3: brand_score ← ComputeEmbeddingSimilarity[ attribute.brand,GetEntityDocuments()]</p>
<p>Action 1 :Figure 3 :
13
Figure 3: Demonstration example during optimization.Best viewed in color.The task of the comparator is to automatically generate instructions based on sampled positive and negative queries.Then comparator provides holistic instructions that guide the actor to improve query decomposition, utilize better tools, and incorporate more comprehensive information.</p>
<p>Figure 4 :
4
Figure 4: Optimization dynamics of AVATAR agents on STARK.The figures show validation performance (solid line) and its moving average (dashed line) during the optimization of AVATAR.</p>
<p>Figure 5 :
5
Figure 5: Performance (left) and AVATAR's optimization dynamics (right) on FLICKR30K-ENTITIES.</p>
<p>Actions performs well on queries mentioning objects, such as animals and people (e.g.dog, man, woman) But they struggles with more abstract or scenery focused queries (e.g.people relaxing on grass, person jumping near a car).</p>
<p>parameters = {phrase_weight: 0.5, visual_weight: 0.5} WeightedSum(phrase_scores, visual_scores, parameters) parameters = {phrase_weight: 0.4, visual_weight: 0.6} WeightedSum(phrase_scores, visual_scores, parameters)Actions perform well on queries that more often mention noun concepts like 'woman', 'man'.The queries with poorer results often cover broader set of other concepts, e.g., actions (ride, cook), and attributes (blue, light).</p>
<p>1 )
1
Identify the following query patterns separately NP (Basic Noun Phrase) : {<DT|PP\$>?<JJ>*<NN.*>+}CP (Comparative Phrase) : {<JJ>?<NN.*>+<IN><JJ>?}VP (Verb Phrase) : {<VB.*><NP|PP>+} 2) Correspond them with the image attributes precisely Action 1: VqaByLLM(GetImages(), "What is the location?")Result 1: location ← Long description about the location Action 2: StringMatch(location, query_attribute.location)Action 1: GetVisualAttributesByLLM(GetImages(), "location") Result 1: location ← Concise location attribute Action 2: TokenMatchScore (location, query_attribute.location)</p>
<p>Figure 6 :
6
Figure6: Representative instruction types from the comparator.We provide three cases where the comparator guides the actor towards (1) better divide-and-conquer strategies for multi-step problemsolving, (2) more sensible differentiation between good and bad tool usage/combinations, and (3) adjustments in the weights to generate the final answers.We record the number of occurrences X under each instruction type over 25 iterations on FLICKR30K-ENTITIES, indicated by (X/25).</p>
<p>Figure 7 :
7
Figure 7: Example data on FLICKR30K ENTITIES.Each entity is an image along with its image patches and associated phrases with the image patches.</p>
<p>Input:</p>
<p>Any query (example: "A man with pierced ears is wearing glasses and an orange hat .");Action Space: {GetImages, GetEmbeddingSimilarity, GetVisualAttributesByLLM, , ...] Output: Retrieved Image IDs ✅ Remove empty spaces or non-alphabetic characters Action 1: CleanQueryText[query] Result 1: normalized_query ← "a man with pierced ears is wearing glasses and an orange hat" ✅ Get all phrases from the knowledge base Action 2: GetBagofPhrases() Result 2: phrases_list ← [["a man", "grass", "sky"], ["a", "cat", ...]] ✅ Compute IDF for phrase importance Action 3: ComputeIDFScores[Flatten[phrases_list]] Result 3: idf_scores ← {"pierced": 0.5, "man": 0.0012, ...} ✅ Get visual attributes for the candidate images Action 4: GetVisualAttributesByLLM[GetImages(), ["color", "object", "action", "count"]] Result 4: visual_attributes ← {node_id_1: {"color": "red", ...}, node_id_2: {...}, ...} ✅ Evaluate textual and visual relevance Action 5: [','.join(list) for list in phrases_list] Result 5: phrase_sentences ← ["a man, grass, sky", "a, cat, playground",...] ✅ Evaluate textual and visual relevance Action 6: ComputeEmbeddingSimilarity[normalized_query, phrase_sentences] Result 6: text_scores Action 7: ComputeEmbeddingSimilarity[normalized_query, visual_attributes] Result 7: visual_scores ✅ Match query phrases with node attributes using IDF scores Action 8: MatchQueryPhrases[normalized_query.split(), visual_attributes] Result 8: phrase_match_scores ✅ Reweight the phrase_match_scores with IDF score Action 9: ReweightByIDFScore[phrase_match_scores, idf_scores] Result 9: reweighted_match_scores ✅ Aggregate scores with weighted parameters Action 10: WeightedSum[text_scores, visual_scores, reweighted_match_scores, weights=(0.5, 0.3, 0.2)] Result 10: aggregated_scores ✅ Normalize scores for final ranking Action 11: NormalizeScores[aggregated_scores] Result 11: normalized_scores = {node_id: normalized_score, ...} Final Result: answers = GetTopkEntities[normalized_scores, k=5] ✅ Excellent task performance</p>
<p>Figure 8 :
8
Figure 8: Optimized Action Sequence by AVATAR on FLICKR30K-ENTITIES..</p>
<p>Table 2 :
2
Retrieval performance (%) on STARK benchmark.Last row shows the relative improvements over the best metric value in each column.</p>
<p>Table 3 :
3
Performance (%) on three QA benchmarks.Last row shows the relative improvements over the best metric value in each column.
CoT28.0%58.0%1.7%0.0%0.0%0.0%ReAct40.0%72.0%31.7%17.5%38.3%3.33%Reflexion46.0%77.0%28.3%13.3%30.0%3.33%ExpeL39.0%73.0%36.7%14.5%56.6%1.67%Retroformer (#retry=1)51.0%-----AVATAR-C41.0%73.0%31.7%13.3%31.7%1.67%AVATAR53.0%84.0%37.5%23.3%60.0%4.17%Relative Improvement3.92%9.09%2.18%33.1%5.82%25.0%
HOTPOTQA ARXIVQA TOOLQA SCIREX-EASY SCIREX-HARD AGENDA-EASY AGENDA-HARD</p>
<p>Table 6 :
6
Performance metrics for different models on the subset of the STARK-MAG dataset.
MAG (#Test=50)Hit@1 Hit@5 Recall@20 MRRDPR 16.0040.0051.8427.39QAGNN 20.0052.0049.7136.39ada-002 40.0058.0055.9347.76multi-ada-002 32.0058.0058.8143.58ReAct 46.0060.0054.6750.92ExpeL 40.0058.0055.9447.43Reflexion 48.0064.0057.4352.31AvaTaR-C 44.0060.0052.4950.16AvaTaR 52.0064.0053.8656.74Relative Improvement 8.33% 0.00%-8.42%8.48%</p>
<p>Table 7 :
7
Hit@1 results for different combinations of ℓ and h values on the STARK-AMAZON dataset.
ℓ = 0.548.3250.0149.87ℓ = 0.647.8949.5650.45ℓ = 0.747.7548.5649.34
h = 0.3 h = 0.4 h = 0.5</p>
<p>Table 8 :
8
Function library on STARK
Function NameInputOutputParseAttributeFromQueryquery: The string to beThis function parses a 'query'parsed, attributes: The listinto a dictionary based on theof attributes to be extractedinput list 'attributes'from the queryGetBagOfPhrasesimage_ids: The image id ar-Returns a list of phrase listray to get the phrases fromfor each image in the im-age_ids listGetEntityDocumentsimage_ids: The image id ar-Returns a list of text informa-ray to get the text informa-tion for each image in the im-tion fromage_ids listGetClipTextEmbeddingstring: The list of strings toEmbed a string or list of Nbe embeddedstrings into N embeddingsGetPatchIdToPhraseDictimage_ids: The image list toReturns a list of patch_idget the patch_id to phrase listto phrase list dictionary fordictionary fromeach imageGetImagesimage_id_lst: The list of im-Return a list of images withage idscorresponding idsGetClipImageEmbeddingimage_lst: The list of imagesEmbed the images of a list ofto be embeddedN image_ids into N tensorsGetImagePatchByPhraseIdimage_id: the id of an image,Return the patch image forpatch_id: the patch id on thethe given image_id andimagepatch_idComputingEmbeddingSimilarity embedding_1 and embed-The cosine similarity scoreding_2of two embeddingsComputeF1string_to_match: The keyCompute the F1 score basedword to be matched, strings:on the similarity betweenThe list of strings to be cal-'string_to_match' and eachculated f1 score with the keystring in 'strings'wordTokenMatchScorestring_to_match: The keyCompute the recall scoreword to be matched, strings:based on the similarity be-The list of strings to be cal-tween 'string_to_match' andculated recall score with theeach string in 'strings'key wordComputeExactMatchScorestring_to_match: The keyCompute the exact matchword to be matched, strings:score based on whetherThe list of strings to be exact'string_to_match' is exactlymatched with the key wordthe same as each string in'strings'VqaByLLMquestion: The question toUse LLM to answer thebe answered, image_lst: Thegiven 'question' based on thelist of imagesimage(s)ExtractVisualAttributesByLLMattribute_lst: The list of at-Use LLM to extract at-tributes to be extracted, im-tributes about the given 'at-age_lst: The list of imagestribute_lst' from each imageFINISHfinal_reranked_answer_list:This function is used to indi-The final answercate the end of the task</p>
<p>Table 9 :
9
Function library on Flickr30K Entities5.Open access to data and codeQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?The answer NA means that paper does not include experiments requiring code.•Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
Answer: [Yes]Justification: Our code and data are accessible at https://anonymous.4open.science/r/AvaTaR-FBC4/.Guidelines:•</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
the experiments?Answer: [Yes]Justification: We run our experiments on a single NVIDIA A100-SXM4-80GB GPU and32-core CPUs.Guidelines:• Answer: [Yes]Justification: We do not induce any potential research harm mentioned in NeurIPS Code ofEthics in our paper.Guidelines:• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consid-eration due to laws or regulations in their jurisdiction).10. Broader ImpactsQuestion: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]
AcknowledgementWe thank lab members in Zou and Leskovec's labs for discussions and for providing feedback on our manuscript.We also gratefully acknowledge the support of DARPA under Nos.N660011924033 (MCS); NSF under Nos.OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM); Stanford Data Applications Initiative, Wu Tsai Neurosciences Institute, Stanford Institute for Human-Centered AI, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, SAP, and UCB.The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]Justification: We design a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and make comprehensive analysis on the evolution of our key modules.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer:[Yes]Justification: We did extensive survey on related work in the area of LLM agents, agent optimization, LLM agent for retrieval, and further discuss their limitations Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: The paper does not include theoretical results.Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: We elaborate the experiment details in the Experiment section including datasets, baselines, function libraries etc.We also release all the prompts we are using in the experiments for reproducibility.Guidelines:• The answer NA means that the paper does not include experiments.• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, arXiv:2308.09687</p>
<p>Improving Language Models by Retrieving from Trillions of Tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Irving, ICML. Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W Rae, Erich Elsen, Laurent Sifre, 2022</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915FireAct: Toward Language Agent Fine-tuning. 2023</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, 2023. 2023</p>
<p>RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, Zhiting Hu, EMNLP. ACL. 2022</p>
<p>. Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, Jianfeng Gao, n. d.</p>
<p>Surveying the Horizons of Multimodal Interaction. A I Agent, arXiv:2401.03568</p>
<p>AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee, arXiv:2403.089782024. 2024</p>
<p>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, arXiv:2309.09971MindAgent: Emergent Gaming Interaction. n. d.</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, ICML. PMLR2020</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome Han, Keith J Ransom, Andrew Perfors, Charles Kemp, Cogn. Syst. Res. 2024. 2024</p>
<p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. Xiaoxin He, Yijun Tian, Yifei Sun, V Nitesh, Thomas Chawla, Yann Laurent, Xavier Lecun, Bryan Bresson, Hooi, arXiv:2402.076302024. 2024</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, arXiv:2305.088452023. 2023</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, ICML. 2022</p>
<p>Inner Monologue: Embodied Reasoning through Planning with Language Models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, CoRL. 2022</p>
<p>Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun, MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use. ICLR. 2024. 2024</p>
<p>Xiang Vassilis N Ioannidis, Da Song, Houyu Zheng, Jun Zhang, Yi Ma, Belinda Xu, Trishul Zeng, George Chilimbi, Karypis, arXiv:2206.10781Efficient and effective training of language and graph neural network models. 2022. 2022arXiv preprint</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, EMNLP. 2020</p>
<p>Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.14024[cs.CL]Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. 2023</p>
<p>CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven Chu, -Hong Hoi, NeurIPS. 2022</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.11401[cs.CL]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2021</p>
<p>CAMEL: Communicative Agents for. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, arXiv:2303.17760Exploration of Large Scale Language Model Society. abs/2303. 177602023. 2023</p>
<p>Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu, ACL. 2024</p>
<p>API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, EMNLP. Association for Computational Linguistics2023</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang, arXiv:2308.03688AgentBench: Evaluating LLMs as Agents. 2023. 2023</p>
<p>Hierarchical Prompting Assists Large Language Model on Web Navigation. Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, Shuyan Zhou, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, 2023In NeurIPS</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, NeurIPS. 2023</p>
<p>WebGPT: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, ArXiv. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2021. 2021</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, arXiv:2112.09332WebGPT: Browser-assisted question-answering with human feedback. Kevin Button, Matthew Knight, Benjamin Chessand John Schulman. n. d.</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, Systems. 2310.08560MemGPT: Towards LLMs as Operating. 2023. 2023</p>
<p>ART: Automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.090142023. 2023</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255TALM: Tool Augmented Language Models. 2022</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large Language Model Connected with Massive APIs. CoRR2023. 2023</p>
<p>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, arxiv 2302.128132023. 2023</p>
<p>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, Svetlana Lazebnik, Int. J. Comput. Vis. 2017. 2017</p>
<p>WebCPM: Interactive Web Search for Chinese Long-form Question Answering. Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou, Proceedings of ACL 2023. ACL 2023Association for Computational Linguistics2023</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, arxiv 2307.16789ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. 2023. 2023</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023In NeurIPS</p>
<p>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, NeurIPS. 2023</p>
<p>EHRAgent: Code Empowers Large Language Models for Complex Tabular Reasoning on Electronic Health Records. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, May D Wang, arXiv:2401.071282024. 2024</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, NeurIPS. 2023</p>
<p>. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, n. d.</p>
<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. EMNLP, year = 2023. </p>
<p>ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun, arXiv:2306.053012023. 2023</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, Hypothesis Search: Inductive Reasoning with Language Models. 2024. 2024</p>
<p>PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, arXiv:2310.164272023. 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, ICLR. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023. 2023</p>
<p>STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases. Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, N Vassilis, Karthik Ioannidis, James Subbian, Jure Zou, Leskovec, arXiv:2404.132072024. 2024</p>
<p>Large Language Models as Optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, 2024. 2024</p>
<p>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, NeurIPS, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. 2018. 2018</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. 2018</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, NeurIPS. 2023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, ICLR. 2023</p>
<p>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese, 2024. 2024</p>
<p>QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec, 2021</p>
<p>ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang, arXiv:2401.007412024. 2024</p>
<p>AgentTuning: Enabling Generalized Agent Abilities for LLMs. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.12823</p>
<p>Training Language Model Agents without Modifying Language Models. Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, Qingyun Wu, arXiv:2402.113592024. 2024</p>
<p>ExpeL: LLM Agents Are Experiential Learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, AAAI. 2024</p>
<p>Outline, then details: Syntactically guided coarse-to-fine code generation. Wenqing Zheng, Ajay Kumar Sp Sharan, Kevin Jaiswal, Yihan Wang, Dejia Xi, Zhangyang Xu, Wang, ICML. 2023. 2023</p>
<p>MemoryBank: Enhancing Large Language Models with Long-Term Memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, AAAI. 2024</p>
<p>Large Language Models are Human-Level Prompt Engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, ICLR. 2023</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023. 2023</p>
<p>ToolQA: A Dataset for LLM Question Answering with External Tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, NeurIPS. 2023</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, arXiv:2306.13304ToolQA: A Dataset for LLM Question Answering with External Tools. 2023</p>
<p>Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering. Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting Zhuang, arXiv:2402.14320n. d.</p>            </div>
        </div>

    </div>
</body>
</html>
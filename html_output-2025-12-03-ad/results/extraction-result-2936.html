<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2936 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2936</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2936</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-278739851</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12439v1.pdf" target="_blank">Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games</a></p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2936.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2936.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LPLH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to Play Like Humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitively inspired, modular LLM-based framework for playing interactive fiction that integrates a dynamic knowledge-graph map, an action-space learner, and an experience library (retrieval-augmented) to produce human-like, context-aware commands without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LPLH (Learning to Play Like Humans)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular LLM-driven agent that (1) maintains a dynamic KG-map of the game world, (2) learns and stores validated verb-object action templates (action space), and (3) summarizes scoring-related trajectories into an Experience Library which is stored in a vector DB and retrieved via RAG to condition zero-shot LLM action generation. Action generation is done by an LLM agent (LLM_a) using the aggregated context (current observation, KG, viable action-object pairs, retrieved experiences). Several smaller fine-tuned models are used for relation/verb-object extraction and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Qwen2.5-14B-Instruct (primary reported), evaluated also with Qwen2.5-7B-Instruct, GPT-4o-mini, GPT-o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Jericho suite (multiple IF games including Zork1, Zork3, Detective, Spellbrkr, Omniquest, Balances, Ludicorp, Gold, Dragon, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Jericho is a benchmark/environment collection of interactive fiction (text-based) games that require exploration, puzzle solving, natural-language action generation, partial observability, and sparse rewards; games include classic Infocom titles (e.g., Zork1) and community-developed works.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hybrid: dynamic graph-based memory (knowledge graph), action-space repository (semantic/action memory), and an episodic/experience library (retrieval-augmented, reflection-based summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Three complementary memory subsystems: (1) Dynamic KG-map: a JSON-structured knowledge graph updated after every step via a relation-extraction model f_m_re that extracts triples (locations, objects, relations) from observations and previous actions; (2) Action Space (AS): a persistent repository of validated actions decomposed into verb + object templates (stored when an action is observed to be valid) used to pair with current-location objects to produce viable candidate commands; (3) Experience Library (Experience Lib): on scoring events an LLM-based summarizer (LLM_es, implemented with GPT-o3-mini in this work) condenses a fixed-length recent interaction history K_j together with the reward change and state into a structured experience summary E_i which is stored as an embedding in a vector database D. The three are combined at generation time: current observation o_k, serialized KG G, objv_loc_k (paired actions), and retrieved experiences E_q are concatenated/used to prompt the action-generation LLM (LLM_a) in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval-augmented generation (RAG): experiences are stored in a vector DB and retrieved by semantic similarity to a constructed query q (the system retrieves a relevant subset E_q to condition the LLM). The KG and action-space are directly included (structured) in the prompt/context rather than being retrieved via vector search.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly bounded in the paper: experiences are stored in a vector DB (capacity unspecified); experience summarization is triggered on scoring events only; summarizer uses a fixed-length interaction history K_j (paper sometimes uses last-10 turns as context in modules); no explicit global cap provided.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>KG-map: locations, objects, relations and updated object locations; Action Space: validated verb + object templates (executable commands); Experience Lib: concise structured summaries of successful and failure trajectories (what triggered a reward or loss, key steps, locations, objects, suggested future actions); also stores reward change and indices for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported example outcomes for LPLH-enabled agents: Zork1 (LPLH, 14B backbone) raw average score 39.7, max 45.0 (Table 2); Detective (Qwen-7B with LPLH) achieves 68/100 (text example in paper); Spellbrkr (Qwen-14B with LPLH) achieves 41.7/60. The paper reports that LPLH agents often match or exceed strong RL baselines on some games and require fewer steps than RL to reach comparable scores (examples given in Table 1 and in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines are LLMs generating actions without LPLH and standard RL agents. Representative baselines reported: Qwen2.5-14B-Instruct baseline on Zork1: raw 9 (avg), max 25 (Table 2). Qwen-7B base on Detective: 10/100 (in-text). For Spellbrkr the base LLM score is stated to be lower than the LPLH 41.7/60 (exact base value not always reported in-text for every model).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Substantial in many cases: Detective (Qwen-7B) raw 10 → LPLH 68/100 (≈ +58 points, 6.8× raw improvement reported in-text). Zork1 (14B): baseline raw 9 → LPLH raw 39.7 (+30.7 points, ≈ 4.4× improvement). Ablation table (Zork1) shows component contributions: KG-only raw 11.0 / max 15.0; exp-only raw 25.6 / max 34.0; as-only raw 26.6 / max 35.0; exp+as raw 32.0 / max 40.0; full LPLH (14B) raw 39.7 / max 45.0.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>1) Combining dynamic KG-map, action-space learning and an experience library yields larger, more stable score gains than any single component; experience summaries + action-space produce large improvements in average score and faster learning. 2) The dynamic KG improves stability (reduces variance) and grounds retrieval; action-space reduces wasted/invalid actions; experience-driven retrieval allows reusing successful trajectories and avoiding repeated failures. 3) Memory-based LLM agents (LPLH) can in many games match or exceed strong RL baselines without environment-specific tuning and with fewer steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Experience summarization is only triggered on scoring events (not continuously), which limits what gets stored; memory management and retrieval are not fully optimized and remain challenging; KG is provided as a JSON structure and its representation quality affects reasoning; agents still fail on puzzles requiring obscure domain-specific commands (puzzle bottlenecks) despite the memory modules; memory capacity and long-term pruning/selection strategies are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper includes an ablation comparing memory components: KG-map only (small/unstable gains), experience-only (moderate gains), action-space-only (moderate gains), combinations (exp+as > individual), and full LPLH > any ablation. Quantitative Zork1 ablation (raw / max): KG-map only 11.0/15.0; exp-only 25.6/34.0; as-only 26.6/35.0; exp+as 32.0/40.0; full LPLH 39.7/45.0 — showing synergistic benefit of combining memory types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2936.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2936.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT/GPT-4 (Tsai et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT & GPT-4 evaluated in Tsai et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that evaluated conversational LLMs (ChatGPT and GPT-4) on text-game playing (Zork1) and identified strengths in generation but limitations in constructing coherent world models and incorporating world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models play text games well? current state-of-the-art and open questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT / GPT-4 (as evaluated by Tsai et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Off-the-shelf conversational LLMs used to generate free-form textual actions in IF games without specialized memory architectures; evaluated for their raw ability to act in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>ChatGPT (OpenAI November 30, 2022), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Zork1 (single-game evaluation reported)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Classic Infocom interactive fiction (Zork1) that requires discovery of object locations, puzzle solving, and sometimes obscure domain-specific commands; sparse rewards and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tsai et al. reported that ChatGPT performs competitively with existing automated systems in some respects on Zork1 but 'fails to construct a coherent model' and 'struggles to incorporate pre-existing world knowledge' (no detailed per-game numeric table in this paper's discussion beyond that characterization).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Tsai et al. highlighted that LLMs without structured memory have limitations: they may generate plausible actions but do not build coherent internal/world models or effectively use external/world knowledge, motivating the need for memory/structured representations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit memory mechanism; reported failures include inability to construct coherent models and difficulties incorporating pre-existing world knowledge, leading to misses on puzzles requiring structured internal state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models play text games well? current state-of-the-art and open questions <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2936",
    "paper_id": "paper-278739851",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "LPLH",
            "name_full": "Learning to Play Like Humans",
            "brief_description": "A cognitively inspired, modular LLM-based framework for playing interactive fiction that integrates a dynamic knowledge-graph map, an action-space learner, and an experience library (retrieval-augmented) to produce human-like, context-aware commands without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LPLH (Learning to Play Like Humans)",
            "agent_description": "Modular LLM-driven agent that (1) maintains a dynamic KG-map of the game world, (2) learns and stores validated verb-object action templates (action space), and (3) summarizes scoring-related trajectories into an Experience Library which is stored in a vector DB and retrieved via RAG to condition zero-shot LLM action generation. Action generation is done by an LLM agent (LLM_a) using the aggregated context (current observation, KG, viable action-object pairs, retrieved experiences). Several smaller fine-tuned models are used for relation/verb-object extraction and validation.",
            "base_llm_model": "Qwen2.5-14B-Instruct (primary reported), evaluated also with Qwen2.5-7B-Instruct, GPT-4o-mini, GPT-o3-mini",
            "base_llm_size": "14B",
            "text_game_name": "Jericho suite (multiple IF games including Zork1, Zork3, Detective, Spellbrkr, Omniquest, Balances, Ludicorp, Gold, Dragon, etc.)",
            "text_game_description": "Jericho is a benchmark/environment collection of interactive fiction (text-based) games that require exploration, puzzle solving, natural-language action generation, partial observability, and sparse rewards; games include classic Infocom titles (e.g., Zork1) and community-developed works.",
            "uses_memory": true,
            "memory_type": "Hybrid: dynamic graph-based memory (knowledge graph), action-space repository (semantic/action memory), and an episodic/experience library (retrieval-augmented, reflection-based summaries).",
            "memory_architecture": "Three complementary memory subsystems: (1) Dynamic KG-map: a JSON-structured knowledge graph updated after every step via a relation-extraction model f_m_re that extracts triples (locations, objects, relations) from observations and previous actions; (2) Action Space (AS): a persistent repository of validated actions decomposed into verb + object templates (stored when an action is observed to be valid) used to pair with current-location objects to produce viable candidate commands; (3) Experience Library (Experience Lib): on scoring events an LLM-based summarizer (LLM_es, implemented with GPT-o3-mini in this work) condenses a fixed-length recent interaction history K_j together with the reward change and state into a structured experience summary E_i which is stored as an embedding in a vector database D. The three are combined at generation time: current observation o_k, serialized KG G, objv_loc_k (paired actions), and retrieved experiences E_q are concatenated/used to prompt the action-generation LLM (LLM_a) in a zero-shot manner.",
            "memory_retrieval_mechanism": "Retrieval-augmented generation (RAG): experiences are stored in a vector DB and retrieved by semantic similarity to a constructed query q (the system retrieves a relevant subset E_q to condition the LLM). The KG and action-space are directly included (structured) in the prompt/context rather than being retrieved via vector search.",
            "memory_capacity": "Not explicitly bounded in the paper: experiences are stored in a vector DB (capacity unspecified); experience summarization is triggered on scoring events only; summarizer uses a fixed-length interaction history K_j (paper sometimes uses last-10 turns as context in modules); no explicit global cap provided.",
            "what_is_stored_in_memory": "KG-map: locations, objects, relations and updated object locations; Action Space: validated verb + object templates (executable commands); Experience Lib: concise structured summaries of successful and failure trajectories (what triggered a reward or loss, key steps, locations, objects, suggested future actions); also stores reward change and indices for retrieval.",
            "performance_with_memory": "Reported example outcomes for LPLH-enabled agents: Zork1 (LPLH, 14B backbone) raw average score 39.7, max 45.0 (Table 2); Detective (Qwen-7B with LPLH) achieves 68/100 (text example in paper); Spellbrkr (Qwen-14B with LPLH) achieves 41.7/60. The paper reports that LPLH agents often match or exceed strong RL baselines on some games and require fewer steps than RL to reach comparable scores (examples given in Table 1 and in-text).",
            "performance_without_memory": "Baselines are LLMs generating actions without LPLH and standard RL agents. Representative baselines reported: Qwen2.5-14B-Instruct baseline on Zork1: raw 9 (avg), max 25 (Table 2). Qwen-7B base on Detective: 10/100 (in-text). For Spellbrkr the base LLM score is stated to be lower than the LPLH 41.7/60 (exact base value not always reported in-text for every model).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Substantial in many cases: Detective (Qwen-7B) raw 10 → LPLH 68/100 (≈ +58 points, 6.8× raw improvement reported in-text). Zork1 (14B): baseline raw 9 → LPLH raw 39.7 (+30.7 points, ≈ 4.4× improvement). Ablation table (Zork1) shows component contributions: KG-only raw 11.0 / max 15.0; exp-only raw 25.6 / max 34.0; as-only raw 26.6 / max 35.0; exp+as raw 32.0 / max 40.0; full LPLH (14B) raw 39.7 / max 45.0.",
            "key_findings_about_memory": "1) Combining dynamic KG-map, action-space learning and an experience library yields larger, more stable score gains than any single component; experience summaries + action-space produce large improvements in average score and faster learning. 2) The dynamic KG improves stability (reduces variance) and grounds retrieval; action-space reduces wasted/invalid actions; experience-driven retrieval allows reusing successful trajectories and avoiding repeated failures. 3) Memory-based LLM agents (LPLH) can in many games match or exceed strong RL baselines without environment-specific tuning and with fewer steps.",
            "memory_limitations": "Experience summarization is only triggered on scoring events (not continuously), which limits what gets stored; memory management and retrieval are not fully optimized and remain challenging; KG is provided as a JSON structure and its representation quality affects reasoning; agents still fail on puzzles requiring obscure domain-specific commands (puzzle bottlenecks) despite the memory modules; memory capacity and long-term pruning/selection strategies are not specified.",
            "comparison_with_other_memory_types": "Paper includes an ablation comparing memory components: KG-map only (small/unstable gains), experience-only (moderate gains), action-space-only (moderate gains), combinations (exp+as &gt; individual), and full LPLH &gt; any ablation. Quantitative Zork1 ablation (raw / max): KG-map only 11.0/15.0; exp-only 25.6/34.0; as-only 26.6/35.0; exp+as 32.0/40.0; full LPLH 39.7/45.0 — showing synergistic benefit of combining memory types.",
            "uuid": "e2936.0",
            "source_info": {
                "paper_title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ChatGPT/GPT-4 (Tsai et al. 2023)",
            "name_full": "ChatGPT & GPT-4 evaluated in Tsai et al. (2023)",
            "brief_description": "Prior work that evaluated conversational LLMs (ChatGPT and GPT-4) on text-game playing (Zork1) and identified strengths in generation but limitations in constructing coherent world models and incorporating world knowledge.",
            "citation_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "mention_or_use": "mention",
            "agent_name": "ChatGPT / GPT-4 (as evaluated by Tsai et al., 2023)",
            "agent_description": "Off-the-shelf conversational LLMs used to generate free-form textual actions in IF games without specialized memory architectures; evaluated for their raw ability to act in text games.",
            "base_llm_model": "ChatGPT (OpenAI November 30, 2022), GPT-4",
            "base_llm_size": null,
            "text_game_name": "Zork1 (single-game evaluation reported)",
            "text_game_description": "Classic Infocom interactive fiction (Zork1) that requires discovery of object locations, puzzle solving, and sometimes obscure domain-specific commands; sparse rewards and partial observability.",
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": "Tsai et al. reported that ChatGPT performs competitively with existing automated systems in some respects on Zork1 but 'fails to construct a coherent model' and 'struggles to incorporate pre-existing world knowledge' (no detailed per-game numeric table in this paper's discussion beyond that characterization).",
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Tsai et al. highlighted that LLMs without structured memory have limitations: they may generate plausible actions but do not build coherent internal/world models or effectively use external/world knowledge, motivating the need for memory/structured representations.",
            "memory_limitations": "No explicit memory mechanism; reported failures include inability to construct coherent models and difficulties incorporating pre-existing world knowledge, leading to misses on puzzles requiring structured internal state.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2936.1",
            "source_info": {
                "paper_title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 2,
            "sanitized_title": "interactive_fiction_game_playing_as_multiparagraph_reading_comprehension_with_reinforcement_learning"
        }
    ],
    "cost": 0.0159545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games</p>
<p>Jinming Zhang 
Yunfei Long 
Marc-Alexandre Côté 
Akos Kádár 
Xingdi Yuan 
Ben Kybartas 
Tavian Barnes 
Emery Fine 
James Moore 
Matthew Hausknecht 
Layla El Asri 
Caitlin Dawson 
Hanna Julku 
Milla Pihlajamäki 
Jo- Hanna K Kaakinen 
Jonathan W Schooler 
Jaana 2024 Simola 
Daya Guo 
Dejian Yang 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Runxin Xu 
Qihao Zhu 
Shirong Ma 
Peiyi Wang 
Xiao Bi 
Xiaokang Zhang 
Xingkai Yu 
Yu Wu 
Z F Wu 
Zhibin Gou 
Zhihong Shao 
Zhuoshu Li 
Ziyi Gao 
Aixin Liu 
Bing Xue 
Bingxuan Wang 
Bochao Wu 
Bei Feng 
Chengda Lu 
Chenggang Zhao 
Chengqi Deng 
Chenyu Zhang 
Chong Ruan 
Damai Dai 
Deli Chen 
Dongjie Ji 
Erhang Li 
Fangyun Lin 
Fucong Dai 
Fuli Luo 
Guangbo Hao 
Guanting Chen 
Guowei Li 
Han Bao 
Hanwei Xu 
Haocheng Wang 
Honghui Ding 
Huajian Xin 
Huazuo Gao 
Hui Qu 
Hui Li 
Jianzhong Guo 
Jiashi Li 
Jiawei Wang 
Jingchang Chen 
Jingyang Yuan 
Junjie Qiu 
Junlong Li 
J L Cai 
Jiaqi Ni 
Jian Liang 
Kai Dong 
Kai Hu 
Kaige Gao 
Kang Guan 
Kexin Huang 
Runji Wang 
Ruyi J Chen 
R L Jin 
Shanghao Lu 
Shangyan Zhou 
Shanhuang Chen 
W L Zhang 
Wei Xiao 
Xiaodong An 
Xiaohan Liu 
Xiaokang Wang 
Xiaotao Chen 
Xin Nie 
Xin Cheng 
Xin Liu 
Xingchao Xie 
Xinyu Liu 
Xinyuan Yang 
Xuecheng Li 
Xuheng Su 
X Q Lin 
Xiangyue Li 
Xiaojin Jin 
Xiaosha Shen 
Xiaowen Chen 
Xiaoxi- Ang Sun 
Xinnan Wang 
Xinyi Song 
Xianzu Zhou 
Xinxia Wang 
Y K Shan 
Yao Q Li 
Yi X Wang 
Yang Wei 
Yanhong Zhang 
Yao Xu 
Yaofeng Zhao 
Yaohui Sun 
Yichao Yu 
Yifan Zhang 
Shi </p>
<p>University of Essex
UK</p>
<p>Queen Mary University of London
UK</p>
<p>Meng Li
Lean Wang, Lecong Zhang, Liang Zhao, Liyue Zhang, Lei Xu, Leyi Xia, Minghua ZhangYu, Litong Wang, Mingchuan Zhang, Minghui Tang, Miaojun Wang, Mingming Li</p>
<p>Ning Tian
Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi GePanpan Huang, Peng Zhang, Ruisong Zhang, Ruizhe Pan</p>
<p>Shengfeng Ye
Shiyu Wang, Shuiping YuShunfeng Zhou</p>
<p>Shuting Pan, S. S. Li
Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wenjun Gao, Wenqin YuTao Yun, Wen Liu, Wenfeng Liang, Wentao</p>
<p>Yiliang Xiong
Ying He, Yishi Piao, Yisong Wang, Yiyang Ma, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yuheng Zou, Yu-jia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuyang Zhou, Y. X. Zhu, Yanhong XuYixuan Tan, Yiyuan Liu, Yue Gong, Yuxuan Liu, Yanping Huang</p>
<p>Yuchen Zhu
Yaohui Li
Yi Zheng, Yunxian Ma, Ying Tang</p>
<p>Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren
Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan ZhangZhewen Hao</p>
<p>Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games
9DDEA4DD5DD17271510948C1B9095CB0
Interactive Fiction games (IF games) are where players interact through natural language commands.While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decisionmaking, existing approaches prioritize taskspecific performance metrics over human-like comprehension of narrative context and gameplay logic.This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically.Our proposed Learning to Play Like Humans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time.By aligning LLMsbased agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance.Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds.As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.</p>
<p>Introduction</p>
<p>Interactive Fiction games (IF games), originating in the 1970s (Spring, 2015;Aarseth, 1995), demanded abstract reasoning, implicit world inference, and narrative reconstruction from textual cues alone.Unlike visual or auditory games, IF games rely solely on language and imagination.Successful play involves iterative exploration, learning, and adaptation, guided by intuition, pattern recognition, and experience-driven generalization (Zander et al., 2016).Consequently, IF games offer a rich testbed Figure 1: Example of RL approach, the basic LLM approach, and our LPLH approach for agent problem-solving, shedding light on core mechanisms of exploration and learning.</p>
<p>Deep Recurrent Neural Network (DRNN) (He et al., 2016) sparked a growing interest in Reinforcement Learning (RL) settings where states and actions are expressed in natural language.Consequently, IF games have become a core testbed for integrating RL and natural language understanding (Guo et al., 2020;Hausknecht et al., 2020).Early RL agents rely heavily on action filters and simplistic policies (Yao et al., 2020;Guo et al., 2020;Ammanabrolu et al., 2020).Although subsequent RL approaches include more sophisticated techniques (Ammanabrolu et al., 2020;Yao et al., 2021;Peng et al., 2022), their score-centric objectives still constrain nuanced narrative reasoning.Recent works have begun leveraging large language mod-1 arXiv:2505.12439v1[cs.CL] 18 May 2025 els (LLMs) (Tsai et al., 2023;Ma et al., 2024), but no system LLMs work on playing IF games yet.</p>
<p>Human intuition is central in how players navigate IF games, enabling adaptive engagement with complex, text-based environments.Bartle (1996) characterizes diverse player motivations that shape exploratory and strategic behavior, while Koster (2013) frames "fun" as the pleasure of pattern recognition-a fundamentally intuitive process of mapping novel challenges to learned solutions.Similarly, Tekinbas and Zimmerman (2003) emphasize how well-structured rules and freedom of interaction support creative problem-solving, highlighting the importance of intuitive, context-sensitive reasoning in game design and analysis.</p>
<p>While prior work has often used IF games to benchmark RL capabilities, we instead focus on leveraging LLMs' capacity for human-like, context-aware reasoning to actively play these games.LLMs have shown promise in multi-step inference, narrative comprehension, and decisionmaking across text-based domains (Huang et al., 2024;Zhang and Long, 2025;Xu et al., 2022;Singh et al., 2021;Shi et al., 2023).IF games, with their rich narratives and open-ended interactions, demand precisely the kinds of situational reasoning and adaptivity that human players naturally exhibit-traits often underemphasized in score-centric RL approaches (Tsai et al., 2023).</p>
<p>To bridge this gap, we introduce the Learning to Play Like Humans (LPLH) framework: a novel, training-free LLMs-based system designed to mirror human gameplay strategies in IF environments.Rather than treating human-like as a vague aspiration, LPLH operationalizes it through three targeted modules inspired by common human behaviors during gameplay.Dynamic map-building module incrementally constructs an internal spatial model of the game world, akin to how human players sketch or internalize maps to avoid disorientation.Action-space learning captures and remembers verified verbs and manipulable objects, paralleling how humans accumulate an evolving vocabulary of game-relevant actions.Experience reflection module synthesizes prior successes and failures into reusable summaries, enabling experiencedriven decision-making.Through this modular design, LPLH uses contextual game information and reflective memory to make informed decisions, tightly coupling human-inspired reasoning with interpretable, functional components.</p>
<p>By integrating these key modules, the LPLH framework entirely forgoes reliance on external knowledge to pre-train agents.Instead, it fosters a self-innovative learning process that closely mirrors how human players approach new games: constructing detailed maps, incrementally exploring valid actions, and reflecting on experience to inform future decisions.We apply this approach to various IF games from Jercho dataset (Guo et al., 2020).The main contributions are as follows: 1)</p>
<p>Related Work</p>
<p>IF games provide a structured environment that drives research on language-based agents.These text-based simulations (Hausknecht et al., 2020) integrate challenges.Many studies adapt RL methods to handle their vast, partially observable state-action spaces.KG-A2C (Ammanabrolu and Hausknecht, 2020) builds a knowledge graph (KG) to represent game states and constrain action spaces, addressing the complexity of natural language actions.Guo et al., 2020 reformulates gameplay as a multi-passage reading comprehension task, using context-query attention and structured prediction to enhance action generation and mitigate partial observability.</p>
<p>Another strand of research targets the exploration challenges unique to IF games.Q<em>BERT and MC!Q</em>BERT (Ammanabrolu et al., 2020) employ knowledge-graph-based intrinsic motivation and strategic exploration to overcome bottlenecks in sparse-reward environments.Complementing these efforts, Tuyls et al., 2022 dissects the explorevs.-exploitdilemma by decomposing each episode into distinct exploitation and exploration phases, achieving notable improvements in normalized game scores across multiple environments.</p>
<p>In parallel to the RL-centric approaches, recent works have explored integrating pre-trained language models to enhance agents' semantic understanding and action generation capabilities.The CALM (Yao et al., 2020) is trained on human game- In contrast to approaches that integrate RL, a notable study (Tsai et al., 2023) investigates the performance of LLMs on text games without additional RL components.It evaluated ChatGPT (OpenAI, November 30, 2022) and GPT-4 (Ope-nAI et al., 2024) on Zork1-the only work to focus on leveraging LLMs in this context.Their findings reveal that, although ChatGPT performs competitively with existing systems, it exhibits significant limitations: it fails to construct a coherent model and struggles to incorporate pre-existing world knowledge.These shortcomings highlight critical open questions at the intersection of IF game agents and LLMs, suggesting that further research is needed to fully realize the potential of LLMsonly approaches in interactive fiction environments.Thus, our LPLH framework attempts to fill this gap by simulating human playing behaviors.</p>
<p>LPLH Framework</p>
<p>Our framework draws direct inspiration from core principles in cognitive science, the interdisciplinary study of how humans perceive, remember, reason, and act (Goswami, 2008).These principles are particularly relevant in IF games, which demand narrative comprehension, exploration under uncertainty, and intuitive decision-making (Dawson et al., 2024).Rather than treating human-like behavior as a superficial output pattern, LPLH operationalizes it through design choices grounded in cognitive theory.</p>
<p>Specifically, the LPLH architecture simulates spatial reasoning and memory formation via a dynamic knowledge graph that evolves with the agent's exploration-analogous to how human players construct internal or external maps.It mirrors incremental learning and intuitive action formation by continually expanding a repository of validated commands, reflecting how players retain and recombine useful verbs and objects.LPLH also encodes reflective learning through its experience module, enabling the agent to summarize lessons from both success and failure, in line with experiential learning models.Finally, its decision-making process integrates current context, structured memory, and retrieved past experiences-closely paralleling dual-process theories of human reasoning, where both working memory and episodic recall inform adaptive behavior.</p>
<p>In this section, we describe the overall LPLH framework and its core modules in detail.Figure 2 illustrates the system architecture.</p>
<p>Problem Define</p>
<p>The interaction between an autonomous agent and a text-based game environment can be formulated as a Partially Observable Markov Decision Process (POMDP) (Spaan, 2012), represented as a tuple (S, T, A, O, R) as follows: The agent issues text commands a ∈ A, selecting from a space of natural language actions; It receives text-based observations o ∈ O describing the environment state in a limited scope; The environment provides scalar rewards r = R(s, a), often sparse, to guide learning; The underlying game state s ∈ S encodes KG-map G but is partially observable through textual feedback; the transition function s ′ = T (s, a) updates the game state based on the agent's action, following the game's internal logic.</p>
<p>Unlike the existing RL approaches (He et al., 2016), which learns a value function by selecting from a predefined set of actions to maximize game rewards, our LPLH framework more closely mimics human decision-making by integrating multiple sources of information when generating the next command.Specifically, LPLH introduces a structured method for semantic understanding and decision-making in text-based games.At each time step k, LPLH receives the current game observation o k and the previous action a k−1 and updates the knowledge graph G through the dynamic knowledge-graphs map module.Suppose the last action is valid (i.e., the game state changes from s to s ′ ); the action space module stores that action by splitting it into its constituent verb and objects.Meanwhile, by evaluating the reward r k at each step, LPLH summarizes the current game state s k in conjunction with historical information K j , thus producing a helpful experience E i , where j denotes the history length and i indicates experience index.</p>
<p>For the generation part, our LLMs-based agent LLM will put the map G, confirmed action list for each object in the current state objv, and retrieved experience E to predict the best suitable action.</p>
<p>Following sections are details of LPLH framework integrating Dynamic knowledge-graphs (KG) map, Action Space, and Experience Lib enables adaptive and robust learning in interactive fiction environments.This can potentially enhance agent in complex, language-driven tasks.</p>
<p>Dynamic knowledge-graphs map</p>
<p>Creating a KG to store information is widely used for long-term memory solutions in LLM research (Tsai et al., 2023;Zhu et al., 2024).In the IF game task, the KG-map serves as a continually updated map of in-game entities and their relations, thereby guiding the RL agent's action selection (Ammanabrolu and Hausknecht, 2020).However, while previous approaches often treat the KG-map as a static structure or update it only when new object relations are discovered, our dynamic KG-map module continuously modifies the graph after each change.</p>
<p>Concretely, as the agent interacts with the environment, the textual observations are parsed to capture newly discovered objects, places, or entities and any relationship changes (e.g., "the key is now inside the box").These updates ensure that the KG-map aligns with the evolving state of the game world.LLMs-based agents can more accurately retrieve relevant information when constructing their following action by maintaining a synchronized, real-time representation of the current environment.This dynamic process resolves inconsistencies (such as outdated item locations) and enriches the agent's contextual awareness, allowing for more robust decision-making in text-based games.</p>
<p>We employ a verb &amp; object extraction, a finetuned model f m re , to identify relational triples (location and objects) from the observation o k as relation extraction.Formally, we define:
I o k = f m re (a k−1 , o k )(1)
where a k−1 is the action taken at the previous step, and I o k denotes the set of extracted relations.Subsequently, the module integrates these newly extracted relations I o k along with the preceding action a k−1 to update the knowledge graph G:
G k = kg(G k−1 , a k−1 ) ⊕ kg(G k−1 , I o k )(2)
where kg(•) is a dynamic function that incrementally updates the knowledge graph based on the provided information, and the operator ⊕ combines the updated states from both the action and the newly extracted relations.</p>
<p>Action Space Learning</p>
<p>LPHP framework learns all valid actions within a dedicated action space to emulate human player behavior.This space is decomposed into two phases: verb and object.The valid actions learned in this manner are retained as executable commands.Specifically, after observing the last action a k−1 , a verb &amp; object extraction model f m vo determines whether it remains valid based on the updated observation o k .If a k−1 is valid, the model decomposes it into a set of verbs and objects:
vobj a k−1 k = f m vo (a k−1 ) iff a k−1 is valid, AS = AS ∪ vobj a k−1 k (3)
where AS ∈ R {n,m} denotes the recognized action space, and n and m represent the maximum numbers of verbs and objects, respectively, in the game environment.The term vobj
a k−1 k
contains exactly one verb (e.g., "put * in *") followed by a list of corresponding objects.</p>
<p>In the subsequent reasoning phase, the framework then employs an object &amp; verb pairing procedure to integrate objects in the current location (obj loc ) with candidate actions:
objv loc k = pairing obj loc , AS(4)
where the function pairing(•) searches the recognized action space to find all actions compatible with current location's objects.The objv loc k is a list of viable action-object pairs for decision-making.</p>
<p>Experience Lib</p>
<p>A core aspect of human gameplay is the ability to reflect on past experiences and reuse knowledge to improve future performance (Fazey et al., 2005).To simulate this reflective behavior, we introduce an Experience Library that captures and organizes game trajectories into reusable insights.This component is powered by an LLMs-based experience summarization module, which is invoked upon scoring events-either gains or losses-to extract structured, task-relevant information from recent game history.</p>
<p>Given a scoring event, the model identifies the relevant location, traces the key puzzle-solving actions, notes the triggering action and reward change, and distills generalizable insights that may inform future decisions.For instance, a successful trajectory may yield patterns such as "retrieve key from Attic to unlock door in Room1," while failure cases produce reflective strategies like "examine lantern before entering Basement."These concise summaries emulate how human players internalize lessons from both success and failure.</p>
<p>Formally, we denote the summarization module as LLM es , which generates a structured experience summary E i based on a fixed-length interaction history K j , associated reward change r k , and the current game state s k :
E i = LLM es (K j , r k , s k ),
where i indexes the experience and j indicates the length of historical context.The model is prompted in a one-shot fashion with a structured template to ensure consistency.</p>
<p>Each summarized experience E i is stored in a vector database D for efficient retrieval.During gameplay, we adopt a retrieval-augmented generation (RAG) approach (Lewis et al., 2020) to incorporate these past experiences into decision-making.Given a query q, the system retrieves a relevant subset E q ⊂ D to condition the LLM's next action.This mechanism grounds the agent's decisions in prior experience, enhancing factual accuracy, coherence, and adaptability.By strategically reusing previously successful or cautionary trajectories, the agent emulates human-like learning and dynamically refines its policy over time.</p>
<p>Zore-shot Decision-making</p>
<p>To generate the next command a k at step k, LPLH use an LLMs-based agent, denoted by LLM a , in a zero-shot prompting setup.Specifically, the agent first constructs a query q based on the current game context and uses it to retrieve a relevant experience set E q .It then aggregates the current observation o k , the retrieved experiences E q , the structured KGmap representation G, and the confirmed action set objv loc k .This combined context is provided as input to the LLMs-based agent, which produces the next command a k :
a k = LLM a [G, objv loc k , E q ], o k(5)
By integrating observations and relevant knowledge, LPLH framework enables the agent to issue commands in a flexible, zero-shot (Kojima et al., 2022) way without task-specific fine-tuning.</p>
<p>The proposed LPLH framework integrates zeroshot prompting, retrieval of relevant experiences, action parings, and KG-map structures to guide LLMs-based agents in command generation.This approach offers a robust and adaptable solution that can seamlessly accommodate diverse contexts by eliminating the need for fine-tuning.In doing so, it is the first system methodologies attempt for IF game environments through LLMs-driven techniques, starting the way for more human-behavioraligned agent interactions.</p>
<p>Experiment</p>
<p>This section will briefly introduce our chosen Dataset, Baselines, and Experiment setup.</p>
<p>Dataset</p>
<p>We evaluate our method on a collection of IF games made available through Jericho, an open-source Python-based environment (Guo et al., 2020).The games in Jericho cover diverse genres (e.g., dungeon crawl, mystery, horror) and include both classic Infocom titles (like Zork1) and communitydeveloped works (such as Afflicted).Most IF games employ a point-based scoring system, which serves as the primary reward signal for learning agents.While Jericho natively supports scoring detection for a curated set of games, it also offers the flexibility to run unsupported games without these features.While all games run under 'verbose' model, which always gives the maximum observation of room.</p>
<p>Baselines</p>
<p>We choose some previous RL models to compare with different LLMs approaches.These models represent advancements in integrating structured knowledge representations and natural language processing techniques to improve agent performance and interpretability in complex environments.The chosen RL models are DRRN (He et al., 2016), KG-A2C (Ammanabrolu and Hausknecht, 2020) and DBERT-DRRNL (Singh et al., 2022).</p>
<p>Also, several LLM models will be the baseline and foundation for the LPLH framework: Qwen2.5-7B-Instruct(Team, 2024; Yang et al., 2024), Qwen2.5-14B-Instruct(Team, 2024; Yang et al., 2024), GPT-4o-mini (OpenAI, 2025a) and GPT-o3-mini (OpenAI, 2025b).</p>
<p>Noticeably, RL approaches select the possible action candidates supported by the game engine.Meanwhile, all LLM approaches will generate action, the processes of which are more complicated but closer to the human player's ways.</p>
<p>Experiment setup</p>
<p>We assess the proposed LPLH framework on 9 games of varying difficulty levels (Guo et al., 2020).Each LLMs-based agent runs for 10 epochs (250 steps per epoch).At every step, only the observation from the Jericho game engine is provided to the agent1 .We record both the average and maximum score after all epochs for each game.As a comparison, we also implement an LHLP framework under the same experimental conditions but designate only the final three epochs as "learning outcomes," with all preceding epochs serving as intermediate training phases.Please see Appendix C for hyper-parameters and prompt details.</p>
<p>For the fine-tuned model f m used in our LHLP framework (Section 3), we adopt a smaller model, Qwen2.5-1.5B-Instruction(Team, 2024), to address three specific tasks: (1) validating actions, (2) extracting relations from observations, and (3) decomposing actions into verbs and objects.We begin by collecting game actions and observations from the LLM baseline.Next, we employ carefully crafted prompts for GPT-4, obtaining hundreds of annotated samples, which we subsequently refine manually to create the training datasets for each task.Details of the fine-tuned model f m are provided in Appendix A. While the experience summarizing model LLM es is using GPT-o3-mini.</p>
<p>Result and Analyzes</p>
<p>This section assesses how the LPLH framework advances performance in IF games.First, we illustrate learning behaviors through the learning curves (Section 5.1), compare game scores across various baselines (Section 5.2), and conduct an ablation study (Section 5.3).These analyses highlight LPLH's adaptive, human-like language acquisition capacity and robust performance.</p>
<p>Learning Curves</p>
<p>Figure 3 presents LPLH agent's progression in Zork1, with three metrics reflecting key behavioral dimensions: score gains, vocabulary growth, and spatial exploration.While these surface-level patterns resemble trends observed in human players.</p>
<p>To strengthen this further, we explicitly ground our analysis in cognitive science and learning theory.</p>
<p>The max score curve (blue) follows a stepwise trajectory, which aligns with the cognitive notion of insight-driven learning, where progress occurs in bursts after solving key subgoals (Koster, 2013).Plateaus reflect periods of trial-and-error, a common phase in strategic human exploration.The learned actions curve (green) shows early rapid acquisition followed by gradual slowing, consistent with language learning theories that posit an initial burst of accessible verbs followed by slower mastery of complex interactions.Similarly, the visited rooms curve (orange) rises steeply during early exploration and later flattens, reflecting an exploration-to-exploitation shift-a behavior welldocumented in models of human decision-making under uncertainty (Boyd and Fales, 1983).This reveals a positive association between accumulated knowledge and task success, echoing findings in educational psychology that emphasize iterative reflection and action refinement as drivers of improved performance (Fazey et al., 2005).</p>
<p>Together, these patterns and forthcoming analyses suggest that LPLH exhibits not merely functional success but learning dynamics that parallel human cognitive strategies-reflecting the theoretical foundations upon which the framework is built.</p>
<p>Game Scores</p>
<p>Table 1 summarizes score performance of various agents on multiple IF games, comparing previous RL approaches with LLMs-based methods.The LPLH framework markedly improves the performance of LLMs-based agents by enabling dynamic adaptation and iterative learning during play.</p>
<p>Across the board, we observe consistent and substantial gains when integrating LPLH into LLMsbased agents.For example, in Detective, Qwen-7B (LPLH) achieves a surprising 68/100 compared to the base model's 10/100, representing a 6.8× raw improvement.Similarly, in Spellbrkr, Qwen-14B (LPLH) attains 41.7/60, outperforming both its base version and the RL agent DBERT-DRRN (D-D).These results highlight LPLH's capacity to leverage learned experiences akin to human players refining their strategies through trial and error.</p>
<p>Notably, LPLH agents can match the maximum scores of strong RL baselines in certain games.For instance, in Omniquest and Balances, both o3mini (LPLH) and the best-performing RL agents reach the game's upper score bound, with progress converging at known puzzle bottlenecks2 (Ammanabrolu et al., 2020; Tuyls et al., 2022).This indicates that LPLH can achieve comparable task completion without explicit reward shaping or extensive environment-specific tuning.</p>
<p>Our results suggest that LPLH framework enhances agents by fostering deeper contextual understanding and dynamic strategy adjustments beyond static priors.Moreover, LLMs-based agents with LPLH typically require fewer steps than RL methods to achieve comparable or higher scores (Guo et al., 2020), all without relying on additional external knowledge.As shown in Figure 1, LPLH agents exhibit human-like reasoning steps, providing a clear, self-explanatory rationale for their actions.Future work may focus on further optimizing LPLH to reinforce adaptive behavior in more complex IF settings, potentially bridging the gap between fine-grained RL solutions and the flexible, learned knowledge of LLMs-based approaches.</p>
<p>Ablation Study</p>
<p>In this section, we provide a more in-depth analysis of LPLH framework and evaluate contribution of each component.As shown in Table 2, we report both the raw and max scores, as well as their standard deviations (σ) to assess performance stability across different model variants.We take LPLH 14B3 as our backbone model and observe that fine-tuning it with chain-of-thought reasoning (LPLH 14B -CoT4 ) boosts the raw score from 39.7 to 41.6, while maintaining a relatively small standard deviation, 2.4.However, this improvement comes at a higher computational cost.</p>
<p>Next, we examine the effects of adding a knowledge graph mapping component (KG-map).Although this variant exhibits a slightly lower maximum score, its standard deviation is reduced, suggesting improved stability.The model achieves a higher maximum score when additional experiential data are introduced and exhibits a larger σ.Finally, incorporating an action-space mechanism provides further performance gain by reducing wasted actions.Combining this mechanism with experiential data leads to the most substantial overall results, demonstrating the effectiveness of a multi-faceted approach to enhancing LPLH.</p>
<p>We also compare against a Qwen2.5-14B-Instructbaseline (9 raw, 25 max, 9.17 σ) and observe further gains by incorporating a selective mechanism (14B-select one5 ).Each module independently boosts performance, and their synergy yields even stronger results.Future work includes exploring how transitioning from generating to selecting actions may further enhance reasoning.</p>
<p>Discussion experiences' significance in LPLH framework</p>
<p>This section discusses two examples in Zork1 to show how experiences are essential for the LPLH framework to simulate humans.Both are how LPLH framework plays like humans with experience reflection to achieve higher scores and avoid failure, where we will use 'player' to represent the game character and 'agent' for LPLH framework.</p>
<p>Learning from Failure</p>
<p>In the case of how the LPLH framework learns from failure, the player meets the dark environment for the first time.The agent would not fig-</p>
<p>ure out what was going on and then try to take any new action, but a death followed anyway.After the player's death, the agent will automatically start summarizing this experience.During the summarization, agent will also focus on any partially missing events and suggestions for future reference.</p>
<p>After the first attempt, the agent calms that 'You may need lights to avoid death in the dark...'.However, the agent suggests finding lights since the player never finds light resources.After several attempts, the player finally finds a 'lantern' in the 'living room' and takes it.When a player goes to dark again, the experience will let the agent know that the light needs to be turned on first.</p>
<p>Learning from Success</p>
<p>Learning from success is more straightforward to analyze.Once the player solves a puzzle, the pertinent steps are captured and organized to guide future decision-making.Consequently, when the player encounters a similar situation, the agent retrieves these validated experiences and follows the proven trajectory of actions.Once the player obtains an "egg" at "Up a Tree," they earn five points during the initial exploration.In subsequent rounds, the agent consistently instructs the player to collect this "egg" at the start of the game."egg" and "Up a Tree" are automatically stored in the KG-map, facilitating quick retrieval for future scenarios.</p>
<p>In LPLH framework, each interaction, success or failure, provides essential feedback that informs subsequent decisions (Schaul, 2015;Browne et al., 2012).By systematically archiving and reflecting on these experiences, the agent refines its understanding of the environment, thereby improving strategic behavior and adaptability over time (Bion and Hinshelwood, 2023;Boyd and Fales, 1983).</p>
<p>Error Analysis: Known Puzzle Bottlenecks</p>
<p>Despite LPLH's improvements over baseline methods, RL-and LLMs-based approaches struggle with specific puzzle bottlenecks.This limitation stems mainly from the difficulty of discovering domain-specific or unconventional commands rarely seen in training data or standard exploration trajectories, which also has been pointed out in Tuyls et al. (2022); Ammanabrolu et al. (2020).</p>
<p>In IF games, key progress often hinges on executing highly specific actions that are not inferable from the immediate context.For instance, in Zork1, players must type echo in the "Loud Room" to earn five points-a solution that eludes most automated agents, whether they sample from a predefined action space (e.g., Jericho-based RL systems) or generate free-form text via LLMs.These methods typically lack the inductive bias or experiential grounding to propose such obscure commands.</p>
<p>Even with structural guidance from a dynamic knowledge graph, the agent may overlook necessary commands.In one representative case from the "Living Room," progress depends on typing move rug to reveal a hidden passage.The agent frequently fails here-substituting implausible variants like hit rug or ignoring the rug altogether-though it occasionally succeeds with move rug or pull rug.</p>
<p>These examples illustrate a fundamental challenge in IF games: successful progression often depends on domain-specific intuition, contextual knowledge, or cultural cues that are not easily recoverable from surface-level patterns.As a result, both RL-and LLMs-based agents still fall short of reliably solving such puzzles, limiting their ultimate performance despite improvements in exploration and memory modules.Addressing this gap remains a crucial direction for future work, underscoring the need to align agents with intuition and imagination that guide human players.</p>
<p>Conclusion</p>
<p>In this work, we introduced Learning to Play Like Humans framework, LPLH, a novel framework designed to guide LLMs to play IF games by simulating human player behaviors.To our knowledge, this is the first systemized approach leveraging LLMs to tackle well-known text-based IF games.This cognitively inspired approach for LLMs-based IF game agents integrates dynamic map building, action space learning, and experience-driven memory.LPLH framework tries to balance narrative comprehension, exploration, and puzzle-solving by simulating human play processes without relying on external pre-training.Although our approach still falls short of specialized RL agents in certain games and cannot match human-level scores, it yields more interpretable, human-like behaviors and enables more context-aware decision-making in interactive fiction game domains.Furthermore, according to the performance of LLMs in IF games, we believe that IF games are a considerable challenge for LLMs in many aspects.</p>
<p>Limitation</p>
<p>To our knowledge, LPLH framework is the first attempt to enable LLMs to play IF games by simulating human players as a system works.Although we incorporate multiple modules for long-term memory, effectively managing and navigating that memory remains challenging.Currently, our approach uses a simple experience summarization that is only triggered when the agent loses or gains points.In contrast, human players naturally integrate relevant information into their memory at any point during gameplay, suggesting that a more dynamic summarization strategy could yield better results.</p>
<p>Furthermore, the framework relies on a JSONstructured KG-map as input.The consistency and clarity of this representation can influence the model's reasoning, indicating that further investigation is needed to determine the optimal rep-resentation method for LLMs-based agents in IF tasks.We also attempted to evaluate the LPLH framework across different models; however, we could not perform extensive tests on a wide range of LLMs due to resource constraints.Future work should include more comprehensive experimentation and exploring adaptive memory-management techniques to address these limitations.Also, we only test a few IF games to show our framework performance, which may not be fully adopted for all IF game types.During the game, the learning process is still affected by many factors that could dramatically lead to score increases or decreases, which we have not found.</p>
<p>Also, the current framework is only applied to IF games.We'll try to fit LPLH in wider text-based environments such as TextWorld (Côté et al., 2019) A Fine-tuned model f m</p>
<p>Before we use the fine-tuned model f m in LPLH framework.We collect the data through three different games (not in our test game): 'Dragon,' 'Karn', and 'Night.'</p>
<p>In both games, we run with random pick action provided by the game engine and generate the action through LLMs.Then, we pair the action and sequenced observation as the basic training dataset.For this basic training dataset, we manually delete those repeat parts.Following that, we create different prompt templates for three task: (1) validating actions (Prompt in Table 4), (2) extracting relations from observations(Prompt in Table 5), and (3) decomposing actions into verbs and objects (Prompt in Table 6).After getting the results generated from GPT-4o, we manually selected the correct parts and then passed them to the train.We use LoRA (Hu et al., 2021) to train the model on LLaMA-Factory (Zheng et al., 2024).The training details are shown in Table 3.</p>
<p>We evaluate the model performance by running 'Zork1' with 'walk-thought' 6 .For the task of validating actions, the accuracy is 90%.For relations extraction, the error rate is like 15%.And for splitting the actions, the accuracy is 98%.</p>
<p>6 the human player's best trajectory B Baseline Details DRRN (He et al., 2016) models the relevance between the state and possible actions to navigate large action spaces effectively.</p>
<p>KG-A2C (Ammanabrolu and Hausknecht, 2020) integrates dynamically constructed KG into the Advantage Actor-Critic framework to constrain the action space and improve decision-making.</p>
<p>DBERT-DRRNL (Singh et al., 2022) enhances the traditional DRRN architecture by incorporating DistilBERT (Sanh et al., 2020), a pre-trained language model, to provide richer text representations, thereby improving the agent's performance.</p>
<p>Qwen2.5-7B-Instruct (Team, 2024;Yang et al., 2024): A 7B open-source model from Alibaba, designed for general-purpose natural language understanding and generation, optimized for efficiency and broad-domain applicability.</p>
<p>Qwen2.5-14B-Instruct (Team, 2024;Yang et al., 2024): A larger 14B version of Qwen, offering improved reasoning, generation quality, and contextual understanding.</p>
<p>GPT-4o-mini (OpenAI, 2025a): A lightweight version of GPT-4o, optimized for efficiency while maintaining strong reasoning capabilities, making it suitable for scalable applications.</p>
<p>GPT-o3-mini (OpenAI, 2025b): A compact version of OpenAI's third-generation model, designed for high-speed inference with reasonable performance in various NLP tasks, especially in constrained computational environments.</p>
<p>C.2 Prompt templates</p>
<p>Here, we show some essential prompt templates, which all models followed CoT reasoning.Table 7 shows how the baseline model generates the next command.Table 8 shows a prompt template of how to summarize the experience.The prompt template of action generation of the LPLH framework is showing Table 9.In our study, no any game name or specific game commands appear in all prompts.</p>
<p>According to research done by (Tsai et al., 2023), the Chat-GPT knows IF game Zork1.When Chat-GPT knows the specific games, it will do well.Use the following rules:</p>
<p>1.If the action is a simple directional command (e.g., "north" or "n"), the object list should be empty.For example: Input: "west" Response: "<act> <west; []> </act>" 2. If the action is "take all" or another "all" command (e.g., "take all"), treat "take all" as the verb and leave the object list empty.For example: Input: "drop all" Response: "<act> <drop all; []> </act>" 3.If there are objects following the Verb (e.g., "eat", "take") or Verb phrase (e.g., "drop down", "go around"), list them.If prepositions (e.g., "on", "at", "with") are present, include them in the verb phrase using  -You can Use 'look' command to examine the current location.And 'inventory' command to examine your inventory.</p>
<p>-Maintain continuity by leveraging the last 10 turns of conversation and the last action you performed as a guide.Use your internal chain-of-thought to continue.</p>
<p>-Always think first, then act.The chain-of-thought is mandatory before producing the final command.</p>
<p>Remember, you are playing a text-based game.Follow these instructions diligently, use your chain-of-thought to reason about your actions, and only format your final chosen command between "|start|" and "|end|".-If no related puzzles (solve puzzles to show new location or new environment observations or earn points) are encountered, the whole 'puzzle_status' needs to be "No puzzles encountered yet."-Please focus on how the player scored points with related puzzles and situations that occurred.</p>
<p>-Do not reveal hidden or undiscovered info.</p>
<p>-Keep it concise and factual based on the logs.</p>
<p>-When giving "important_experience", please reflect like an expert player (Always think about why this happened) as the payer's 'trace game experience'.</p>
<p>-If player has not died, the '<em>Lose Points</em>' in 'important_experience' should be 'none'.If player has died, the '<em>Earn Points</em>' in 'important_experience' should be 'none'.</p>
<p>-In your reasoning, if you find more than one earning or losing points, please ONLY summarize the last one based on previous steps.<strong>Final Output Format:</strong> -In the final output for any 'loc name', please use <loc> loc name <loc> to mark it, as well as 'step did before' (which steps solved the puzzles) by marking in <step> step did <step>, as well as 'interacted obj' (which player did valid action to obj) by marking in <obj> interacted obj <obj>; where the 'interacted obj' in step doesn't need this marking.And give a structured output based on points.</p>
<p>-At the end of the response, please outline TAGs (no more than 4) between <tag> * </tag> that are used for retrieval.put main location in <room> * </room> as one of the tag.</p>
<p>-After TAGs, please also give the difficulty for current puzzles in between <dif> * </dif>.You can combine the history steps with your expert player's experience to define the difficulty.</p>
<p>-Please think about it first.Then, give your final completed player experience summary between '|start|' and '|end|'.</p>
<p><END OF INSTRUCTIONS></p>
<p>Figure 2 :
2
Figure 2: LPLH Framework.The Dynamic KG-map incrementally constructs a knowledge graph from observed items.The Action Space separates valid actions into verb-object pairs for efficient generation.The Experience Lib captures and summarizes key steps as reusable experiences to guide future decisions.</p>
<p>Figure 3 :
3
Figure 3: Zork1 learning curve in scaled steps.For reference, human player's best trajectory gets 350 scores in 412 steps with 48 verbs, 57 objects (total 105 unique words), and 63 rooms.</p>
<p>Table 1 :
1
Game score results running on IF games.The DRRN<em>, KG-A2G</em>, and D-D<em> (DBERT-DRRNL) are RL agent; results are from their papers.base in LLMs-based agent generates action directly with some previous history, and LPLH is our approach.For the scores '-/-,' the first represents a raw score of the end, and the second represents the max score.In LLMs-based agents, the raw on base computes the average score in all runs, while the raw on LPLH computes the last three runs as "learning outcomes."Scores with blue mean the highest score in raw, and scores with underline are the highest score in max.The Max is the game's maximum score.
DRRN</em> KG-A2G<em>D-D</em>Qwen-7BQwen-14BGPT-4o-miniGPT-o3-miniMaxbaseLPLHbaseLPLHbaseLPLHbaseLPLHomniquest5 / -3 / -4.9 / 51 / 55 / 51.5 / 55 / 52 / 55 / 54 / 55 / 550detective197.8 / -207.9 / --/ -10 / 10 68 / 100 36 / 7072 / 9022 / 3030 / 6020 / 2050 / 60360zork132.6 / -34 / -44.7 / 550 /09 / 159 / 3539.7 / 456 / 1010 / 1530 / 35 33.8 / 45 350zork30.7 / -0.1 / -0.2 / 40 / 00.6 / 12.0 / 32.6 / 31.8 / 32.8 / 33 / 33 / 37ludicorp13.8 / -17.8 / -12.5 / 181 / 11/ 110.5 / 12 11.7 / 131 / 12.6 / 34.4 / 78 / 11150balances10 / -10 / --/ -0 / 05 / 58.75 / 10 10 / 105 / 55 / 58.3 / 1010 / 1051spellbrkr37.8 / -21.3 / -38.2 / 400 / 025 / 2525 / 40 41.7 / 6018 / 4038.3 / 50 31.3/ 50 47.5 / 60 600dragon-3.5 / -0 / --/ --3.5 / -1 -1.3 / 0-0.8 / 0-0.67 / 0 -0.8 / -0.20 / 0-4 / -2-0.5 / 125gold0 / 0-/ --/ -0 / 00 / 02.4 / 33 / 32.5 / 33 / 31 / 33 / 3100</p>
<p>Table 2 :
2Modelraw maxσLPLH14B39.7 45.04.2LPLH14B − CoT41.6 45.02.4KG-map only11.0 15.02.0KG-map + exp11.0 35.0 13.1KG-map + as27.8 35.06.8exp only25.6 34.09.0exp + as32.0 40.04.0as-only26.6 35.06.6Qwen2.5-14B-Instruct9.025.09.214B-select one14.5 30.0 11.6
Ablation results on 'Zork1.'Where σ is the standard deviation; 'exp' represents the experience summarization; and 'as' represents the action space.</p>
<p>.games.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754.
Thea Zander, Michael Öllinger, and Kirsten G Volz.2016. Intuition and insight: Two processes that buildon each other or fundamentally differ? Frontiers inpsychology, 7:1395.Jinming Zhang and Yunfei Long. 2025. MLD-EA:Check and complete narrative coherence by intro-ducing emotions and actions. In Proceedings of the31st International Conference on Computational Lin-guistics, pages 1892-1907, Abu Dhabi, UAE. Asso-ciation for Computational Linguistics.Yaowei Zheng, Richong Zhang, Junhao Zhang, YanhanYe, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.2024. Llamafactory: Unified efficient fine-tuningof 100+ language models. In Proceedings of the62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 3: System Demonstra-tions), Bangkok, Thailand. Association for Computa-tional Linguistics.Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao,Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen,and Ningyu Zhang. 2024. Llms for knowledge graphconstruction and reasoning: Recent capabilities andfuture opportunities. World Wide Web, 27(5):58.</p>
<p>You are evaluating the outcome of a text-based game action based on the game's observation (feedback message) after the player's previous action.Your task is to determine if the action was successful or not.<START OF INSTRUCTIONS> -You will be given an observation text that follows the player's attempted action.
Instruction:-If the observation indicates that the action was carried out successfully (e.g., it provides new information, describes theenvironment, or gives a positive confirmation), respond with:<ais> True </ais>-If the observation indicates that the action could not be performed (e.g., includes phrases like "You can't..." or "Youcannot..."), respond with:<ais> False </ais>Note:-An unsuccessful action usually explicitly states that the player cannot do something, or that the action fails.<END OF INSTRUCTIONS></p>
<p>Table 4 :
4
Prompt template: Action Validation.going to extract triples in the format <subject, relation, object> from an input Observation along with previous actions you did, originating from a text-based game.Focus solely on where the character ('You') is located, what objects are in that location, and their immediate properties.The maximum length for any object name in the triples is three words, where length of location name has no limit.Rules: 1.If the observation doesn't describe an environment or information is insufficient (e.g., "Opened", "Taken"), output |start| none |end| and skip other points.2.Always use 'in' as the relation to represent the character's location.Convert any spatial descriptions (e.g., 'are facing', 'are standing', 'are behind') to the 'in' relation.If the input begins with a Room name (starts with a capital letter and does not end with a period), use it as the location.Do not include additional details or properties of objects.Only extract the objects themselves, ensuring object names are no longer than three words.But if a object have a relation to another object, such as 'in' and 'on', then extract that relation.Hall.To the southwest is the entrance to the Computer Site, and you can go east here as well as go up with a stair."Triples:<Hall,southwest,Computer Site>, <Hall, east, to east>, <Hall, up, to up>Note: Pay more attention to objects and directions than to objects' states or other decorative details.Now, extract the relationships for the input step by step and merge all the results into a single output enclosed within |start| * |end|, where * represents the list of extracted triples.<ENDOF INSTRUCTIONS>
Instruction:<START OF INSTRUCTIONS>You're Example:Input: "Stairwell (First Floor) You're in the north stairwell."Triple: <You, in, Stairwell (First Floor)>3. If the observation doesn't include a precise location, do not provide any <You, in, *> triple.4. Use 'have' as the relation to represent interactive objects present in the location. Focus only on the objects themselves asthe 'obj' in the triple. Ignore decorative details unless they indicate an interactive object. Limit object names to a maximumof three words.Example:Input: "There is a small mailbox here."Triple: &lt;[Location], have, mailbox&gt;5. Example:Input: "A buzzing water fountain has been moved."Triple: "&lt;[Location], have, water fountain&gt;"Input: "A sock is on th table."Triple: "&lt;[Location], have, sock&gt;, &lt;[Location], have, table&gt;, <sock, on, table>"6. If the input specifies a requirement or action needed to continue, use <location/object, need/require, something to action>.Example:Input: "Forest. You would need a machete to go further west."Triple: <Forest, need, machete to go west>7. For objects or locations mentioned with a direction (e.g., 'to the north', 'up to', 'down'), use <current location, direction,[new location]/to [direction]>.Example:Input: "</p>
<p>Table 5 :
5
Prompt template: Relation Extraction.<START OF INSTRUCTIONS> You wil receive a previous input(step) from a text-based IF game, and please split the input into two parts, action and objs, as "<verb; [objs]>".Please follow these instructions to complete the task step by step.</p>
<p>Instruction:</p>
<p>Table 6 :
6
Prompt template: Splitting Action.You are playing the classic text-based interactive fiction game.Your goal is to explore, solve puzzles, collect treasures, and reach the winning end state.Throughout the game, you will: 1. Receive a history of the game's the action you performed, the new observation representing what you see or experience after your action.The action will help you understand what led to the new observation.2.Have access only to the last 10 turns of conversation as your history.You must rely on these 10 turns and your internal reasoning to keep track of your location, inventory, puzzles, and progress.3.Receive current new observation based on the last action and the current game states as input.4.Produce all responses formatted between "|start|" and "|end|".At each turn, carefully read the provided new observation and the action you performed.-Useyourinternal chain-of-thought to determine the best possible action to advance in the game, considering your inventory, location, known puzzles, and the limited historical context you have.-Onceyou have reasoned through your options, produce exactly ONE game command.
-Always Format your command as this at the end of your response:<strong>Final Command:</strong>|start| [your chosen command] |end|<strong>Guidelines:</strong>-Avoid random or nonsensical actions. Each move should serve a purpose or help solve a puzzle.-Try to use player (human) logic to guide your decision.
Instruction:<strong>Your Task:</strong> -</p>
<p>Table 7 :
7
Prompt template: Baseline action generation.<STARTOFINSTRUCTIONS> You are a game engine summarizer.Your task is to read the current log of the game state and produce a concise, cohesive summary of the player's progress so far (This happens every time the player gets a score or loses a score).Do NOT reveal any hidden or undiscovered information.Focus only on details the player already knows or has directly experienced.A list of "Step" will be provided.Each step includes: -An observation (what the player sees), -Info about moves and current score, -The action taken just before the observation.<strong>SummaryStructure:</strong>1. "location": where the player is (or what area is described) when the score changes.If the player has died, give the location name before death.<em>1.1</em> -One Location name Only.<em>1.2</em> -Description of situation.2."puzzle_status": what puzzles or obstacles have been solved to earn/lose the points.<em>2.1</em> -ONLY related steps to solve the puzzles directly.Any requirement for solving the puzzles, such as 'player need to <step>open door<step> at Room1 to enter <loc>Room2<loc>.<em>2.2</em> -Description of the puzzle.3."scoring":how the player earned/lost points for the last step.Any action leads to earning/losing points.<em>3.1</em> -Step done to earn/lose points.<em>3.2</em> -How many points are changed?4."important_experience":The experience can be used for the future.Only the most notable and valuable clues or items the player learned about for the global game experience or any warning must be recorded through all previous logs.Only Focus on confirmed information.<em>EarnPoints</em>-ONLY when player earn points, then we only need to know what leads to earn points and ingore other unchecked information.-Forexample:'player noticed there is a rabbit on the table (unchecked)' is not experienced.'Room1.playeropen a locked door by a key (The key got from the roof)' is the experience as' player need to go to <loc>roof<loc> for a <obj>key<obj> to open the locked door in <loc>Room1<loc>.<em>LosePoints</em> -ONLY when the player loses points (died usually or lost in the game, where 'lost' here means the player earned no points for a long time ), you also need to give suggestions for the future.-Forexample: 'player died in Room2.(Player saw a rabbit on table in Room1, but player did nothing with the rabbit)' you can now give the suggestion for next time that try to check <obj>rabbit<obj> before going <loc>Room2<loc>.</p>
<p>Instruction: <strong>Remember</strong>:</p>
<p>Table 8 :
8
Prompt template: experience summarization.</p>
<p>For all RL agents, they receive completed observation and inventory at each step. However, LLMs-based agent needs to decide when to call the command 'look' or 'i' to get such information by themselves.
See section 7 Error Analysis for more details.
The backbone model is Qwen2.5-14B-Instrction.
We use DeepSeek-R1-Distill-Qwen-14B (DeepSeek-AI et al., 2025) as the CoT distillation model for this task.
The selective mechanism is choosing one action from game engines' action candidates same as RL approaches.
AcknowledgmentsThis work is supported by the Alan Turning Institute/DSO grant: Improving multimodality misinformation detection with affective analysis.
Cybertext: perspectives on ergodic literature. Espen Aarseth, 1995University of Bergen</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Hearts, clubs, diamonds, spades: Players who suit muds. Richard Bartle, Journal of MUD research. 11191996</p>
<p>Learning from experience. Wilfred Bion, Robert Hinshelwood, 2023Routledge</p>
<p>Reflective learning: Key to learning from experience. M Evelyn, Ann W Boyd, Fales, Journal of humanistic psychology. 2321983</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang, arXiv:2501.129482025Preprint</p>
<p>Learning more effectively from experience. Ioan Fazey, John A Fazey, Della M A Fazey, Ecology and Society. 1022005</p>
<p>Principles of learning, implications for teaching: A cognitive neuroscience perspective. Usha Goswami, Journal of Philosophy of education. 423-42008</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, 10.1609/aaai.v34i05.6297Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics2016</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Prompting explicit and implicit knowledge for multi-hop question answering based on human reading process. Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Theory of fun for game design. Raph Koster, 2013O'Reilly Media, Inc</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Agentboard: An analytical evaluation board of multi-turn LLM agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Gpt-4o mini: Advancing cost-efficient intelligence. 2025aOpenAI</p>
<p>Openai o3 mini. 2025b. 2025-02-08. OpenAI. November 30, 2022OpenAIIntroducing Chat-GPT</p>
<p>Giambattista Parascandolo. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Daniel Pantuliano ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, ; Song, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Yoo, arXiv:2303.08774Adam Perelman, Filipe de Avila Belbute Peres. Michelle Pokorny, Pokrass, H Vitchyr, Tolly Pong, Alethea Powell, Boris Power, Elizabeth Power, Raul Proehl, Alec Puri, Jack Radford, Aditya Rae, Cameron Ramesh, Francis Raymond, Kendra Real, Carl Rimbach, Bob Ross, Henri Rotsted, Nick Roussez, Mario Ryder, Ted Saltarelli, Shibani Sanders, Girish Santurkar, Heather Sastry, David Schmidt, Schnurr, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,; Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David; Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng; Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael; Natalie Staudacher; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason; Kevin YuJuan Felipe Cerón UribeAndrea Vallone, Arun VijayvergiyaPreprintFelipe Petroski Such. Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report</p>
<p>Inherently explainable reinforcement learning in natural language. Xiangyu Peng, Mark Riedl, Prithviraj Ammanabrolu, Advances in Neural Information Processing Systems. 202235</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.011082020Preprint</p>
<p>Tom Schaul, arXiv:1511.05952Prioritized experience replay. 2015arXiv preprint</p>
<p>Self-imitation learning for action generation in text-based games. Zijing Shi, Yunqiu Xu, Meng Fang, Ling Chen, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, arXiv:2107.08408Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems. Ishika Singh, Gargi Singh, Ashutosh Modi, the 21st International Conference on Autonomous Agents and Multiagent Systems2021. 2022arXiv preprintPre-trained language models as prior knowledge for playing text-based games</p>
<p>Partially observable markov decision processes. Matthijs Tj Spaan, Reinforcement learning: Stateof-the-art. Springer2012</p>
<p>Dawn Spring, Gaming history: computer and video games as historical scholarship. Rethinking History. 201519</p>
<p>Qwen Team. 2024. Qwen2.5: A party of foundation models. </p>
<p>Rules of play: Game design fundamentals. Katie Salen, Tekinbas , Eric Zimmerman, 2003MIT press</p>
<p>Can large language models play text games well? current state-of-the-art and open questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, arXiv:2304.028682023arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2022</p>
<p>Perceiving the world: Question-guided reinforcement learning for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, arXiv:2204.095972022arXiv preprint</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, arXiv:2407.10671Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuarXiv preprintYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, 10.18653/v1/2021.naacl-main.247Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Keep calm and explore: Language models for action generation in text-based. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 2020</p>            </div>
        </div>

    </div>
</body>
</html>
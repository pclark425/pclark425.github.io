<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9679 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9679</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9679</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-273228777</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.06965v3.pdf" target="_blank">Uncovering Factor Level Preferences to Improve Human-Model Alignment</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) often exhibit tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. While crucial for improvement, identifying the factors driving these misalignments remains challenging due to existing evaluation methods'reliance on coarse-grained comparisons and lack of explainability. To address this, we introduce PROFILE, an automated framework to uncover and measure factor-level preference alignment of humans and LLMs. Using PROFILE, we analyze preference alignment across three key tasks: summarization, instruction-following, and document-based QA. We find a significant discrepancy: while LLMs show poor factor-level alignment with human preferences when generating texts, they demonstrate strong alignment in discrimination tasks. We demonstrate how leveraging the identified generation-discrimination gap can be used to improve LLM alignment through multiple approaches, including fine-tuning with self-guidance. Our work highlights the value of factor-level analysis for identifying hidden misalignments and provides a practical framework for improving LLM-human preference alignment.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9679.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9679.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROFILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probing Factors of Influence for Explainability (PROFILE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated framework introduced in this paper to decompose overall pairwise preference judgments into factor-level influences by (1) extracting response-level preferences, (2) measuring factor manifestations per response, (3) computing a factor score (τ14) via concordant/discordant/tie counts, and (4) comparing ranked factor scores between agents using correlation coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various (GPT-4o, Gemini 1.5, LLaMA 3.1-70B, Mixtral, TULU 2.5 variants, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Set of proprietary and open-source generative LLMs evaluated in the paper, including GPT-4o (OpenAI, proprietary), Gemini 1.5 Flash (proprietary), LLaMA 3.1 70B (open-source, 70B parameters), Mixtral 8x7B Instruct (open-source mixture-of-experts), and TULU 2.5 variants (13B/70B with PPO/DPO alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / text generation evaluation (summarization, instruction-following, document-based QA)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Factor-level analysis: compute response-level pairwise preferences from human labels or score-conditioned generation, extract scalar factor measurements per response (rule-based, learned metrics, or LLM-based extraction), compute τ14 factor scores from concordance statistics across pairs, then compare factor score rankings between humans and models via Spearman ρ, Kendall's τb, and Pearson r.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Set of interpretable quality factors (Receptiveness, Off-Focus, Intent Alignment, Hallucination, Source Coverage, Formality Alignment, Novel Words, Length, Fluency, Number of Facts, Helpfulness, Misinformation, Coherence) plus statistical metrics (τ14 factor score, Spearman's ρ, Kendall's τb, Pearson r, pairwise agreement rates).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to existing human-preference datasets: Reddit TL;DR (summarization), Stanford Human Preference-2 (SHP-2; instruction/helpfulness), OpenAI WebGPT (document QA / ELI5). These datasets provide pairwise human preference labels used for PROFILE analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PROFILE reveals systematic factor-level misalignment: models consistently over-prioritize Length across tasks while humans prioritize task-dependent factors (e.g., IntentAlignment, FormalityAlignment, SourceCoverage for summarization; Receptiveness and Helpfulness for instruction/QA). Quantitatively, even the best model achieved only τ≈0.289 correlation with human factor ranks in summarization; evaluation (discrimination) alignment is substantially higher (e.g., GPT-4o evaluation τ≈0.82 vs generation τ≈0.16).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on accurate factor extraction (some factors require LLM-based extraction that has non-perfect accuracy), uses available preference datasets that may not cover all human preference diversity, score-conditioned generation is a proxy for intrinsic generative preference, and closed models may lack accessible log probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>PROFILE moves beyond coarse overall scores to show that standard binary human preference labels can mask divergent factor-level priorities; humans vary factor importance by task while models show consistent heuristics (e.g., prefer longer outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use factor-level decomposition (PROFILE) when assessing alignment; extract factors via a mix of rule-based, learned metrics (UniEval), and validated LLM-based extraction; report τ14 and rank correlations; validate LLM-based extractions against human annotations; leverage evaluation (discrimination) capabilities to guide generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9679.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9679.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>τ14</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>τ14 (tie-aware Kendall-style factor score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tie-aware Kendall correlation variant used as the per-factor 'factor score' measuring concordance between pairwise overall preferences and pairwise factor manifestations; defined as (|C_f| - |D_f|) / (|C_f| + |D_f| + |T_f|) where C_f/D_f/T_f are concordant/discordant/tie pair counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Framework-agnostic (used with generated outputs from GPT-4o, LLaMA 3.1-70B, Mixtral, TULU 2.5, Gemini 1.5, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Applied to outputs from multiple LLMs (see PROFILE entry).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation statistics for NLP preference alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute τ14 per factor across all response pairs to quantify whether the preferred response tends to show a stronger manifestation of that factor; positive values indicate a positive influence, negative a negative influence, and magnitude indicates strength.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Factor influence sign and magnitude; subsequent rank-correlation comparisons (Spearman ρ, Kendall τb, Pearson r) between agents' τ14 vectors to quantify alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Computed over pairwise-labeled datasets (Reddit TL;DR, SHP-2, WebGPT) and over model-generated score-conditioned outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used to produce ranked factor importance lists for humans and models; revealed that model factor scores (τ14) often emphasize Length strongly while human τ14 emphasize other factors depending on task; used also to measure improvement alignment when applying self-refinement/feedback (e.g., compute τ14 on improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires reliable scalar measurements m_f(r) for each factor; ties in factor manifestation or preference can reduce discriminatory power; interpretation depends on quality of factor extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides interpretable per-factor influence unlike aggregate metrics (e.g., BLEU/ROUGE) or single-score human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report τ14 per factor with confidence intervals; combine with rank-correlation metrics when comparing agents; complement with human validation of factor measures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9679.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9679.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Score-based prompting (generation proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Score-conditioned Generation via Score-based Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical proxy for a model's intrinsic generative preference where the model is prompted to generate outputs conditioned on a target quality score (1–5); the target score serves as the model's U(r) value for pairwise preference computations when log-probabilities are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-3.1-70B, Mixtral, TULU 2.5, GPT-4o (used for validation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source and proprietary models prompted to produce outputs conditioned on discrete target quality scores; LLaMA-3.1-70B and Mixtral were validated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP generation preference measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Condition models to produce outputs labeled by target score and use the target score as the model's value function U(r); validate proxy by correlating target scores with models' log probabilities on generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pearson correlation between assigned target score and model log probability of generated output; used as evidence that score-conditioned outputs map to the model's intrinsic likelihood preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Validated on 100 summarization samples (Reddit TL;DR) for LLaMA-3.1-70B and Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High Pearson correlations observed between target score and log-prob: LLaMA 3.1-70B r≈0.975 and Mixtral r≈0.82, supporting the effectiveness of score-conditioning as a proxy for intrinsic generation preference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Proxy depends on model capability to follow score-conditioning prompts; may not reflect unconstrained sampling distributions exactly; closed models may not permit log-prob validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides an operational alternative to using internal log probabilities for generation preference measurement, enabling factor-level analysis even when logits are inaccessible.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Validate score-conditioned outputs against log probabilities when possible (as done with a subset of models); use highest-score (score=5) outputs to approximate unconstrained generation behavior where validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9679.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9679.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factor extraction methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factor Manifestation Extraction: rule-based, learned (UniEval), and LLM-based extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixed extraction strategy: rule-based measures for Length and Novel Words; learned metrics (UniEval) for some quality dimensions; and LLM-based prompts (GPT-4o) for response-level and atomic-fact-level factors (Intent Alignment, Formality Alignment, Number of Facts, Hallucination, Helpfulness, Misinformation, Off-Focus, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o (used to extract many factors), UniEval (learned metric), plus rule-based scripts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4o used with few-shot prompts to label atomic facts and response-level properties; UniEval provides 0–1 scores for various aspects; rule-based code counts words/novel words.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP annotation and metric extraction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For atomic-fact factors, generate/identify atomic facts then compute ratios (e.g., hallucination ratio). For response-level factors, use LLM classification prompts (Aligned/Not Aligned/Partially). For learned metrics, take UniEval outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary/ternary decisions for response-level factors; ratios and counts for atomic-fact factors; continuous scores from UniEval; extraction validation against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on the same datasets (Reddit TL;DR, SHP-2, WebGPT) and model outputs; validation subset of 50 samples for LLM-based extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-based extraction (GPT-4o) achieved validation accuracies: Intent Alignment 86% (43/50), Formality Alignment 92% (46/50), Receptiveness 90% (50-sample WebGPT), Helpfulness: 87% response-level and 80% atomic-fact-level, Misinformation: 87% response-level and 70% atomic-fact-level precision. Cost per summarization sample for LLM-based extraction ≈ $0.018 total.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM-based extractors are not perfect (noted false positives/negatives); some misclassifications arise when content inaccuracies are misattributed to intent/formality issues; extraction cost and API dependence are non-negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM-based extraction approximates human-level factor annotation efficiently and at lower cost but requires validation; combines well with learned metrics and rule-based measures for comprehensive factor coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Perform human validation on a held-out sample to estimate extractor accuracy; use mixed methods (rule-based + UniEval + LLM-based) and report extractor accuracy and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9679.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9679.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrimination-vs-Generation (Gen-Eval) gap & LLM-as-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation-Discrimination Gap and Leveraging LLM-as-Evaluator for Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that LLMs show substantially stronger factor-level alignment with humans when performing pairwise evaluation (discrimination) than when generating outputs; the paper demonstrates two practical uses of this: (1) supervised fine-tuning (SFT) using model self-evaluation re-ranking (self-refinement), and (2) feedback-driven generation where an evaluator chooses and justifies a preferred generated output and the generator refines accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>TULU 2.5 variants (generator in SFT experiments), GPT-4o (evaluator in feedback-driven experiments), GPT-4o / Gemini / LLaMA as evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Generators: TULU 2.5 70B/13B variants; Evaluator: GPT-4o; other models used for comparative evaluation: GPT-4o, Gemini 1.5, LLaMA 3.1 70B, Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Model alignment and training strategies in NLP</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure factor-level alignment (Kendall's τ) in both generation and evaluation settings; perform SFT using model re-ranked outputs (re-ranked by model evaluations), and perform interactive feedback-driven generation with an evaluator providing selection + justification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Kendall's τ between model factor-scores and human factor-scores; response-level agreement rates with human judgments; improvements in τ after SFT or feedback-guided refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Reddit TL;DR summarization dataset used for SFT and feedback-driven experiments; generated data used for fine-tuning (4,000 examples) and held-out evaluation (500 examples for SFT; 100 samples for feedback-driven evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Models show higher factor-level alignment in evaluation than generation (example: GPT-4o evaluation τ≈0.82 vs generation τ≈0.16). Self-refinement SFT using model self-evaluations on TULU 2.5 produced significant improvements in generation alignment, reaching performance comparable to GPT-4o; naive SFT on human-preferred responses (human-SFT) performed worse than original TULU, indicating value of leveraging model evaluation. Feedback-driven generation with GPT-4o evaluator consistently improved alignment vs baselines; baselines that applied implicit self-critique without explicit feedback often showed negative correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SFT used relatively small fine-tuning set (4k) and evaluation limited to Reddit summarization; human-SFT underperformed, indicating complexity in using human-preferred data; faithfulness of model-generated feedback needs more scrutiny; evaluation improvements may not generalize across tasks without further study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Shows a practical pathway where LLM evaluation strengths (discrimination) can be used as a training signal to improve generation alignment, complementing or even outperforming direct training on human-labeled preferred outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Leverage LLM-as-evaluator signals (re-ranking + justifications) to produce training data for SFT; use explicit evaluator feedback rather than implicit self-refinement; validate improvements on held-out human-labeled data; analyze which factors the evaluator emphasizes to ensure alignment with human priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9679.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9679.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets/Benchmarks used</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference datasets: Reddit TL;DR, Stanford Human Preference-2 (SHP-2), OpenAI WebGPT, RewardMATH (example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Public pairwise human-preference datasets used as evaluation benchmarks and ground truth for factor-level alignment: Reddit TL;DR (summaries with axis evaluations), SHP-2 (helpfulness/instruction-following preferences), WebGPT (document QA/ELI5), and RewardMATH used as an example for evaluation on math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Not applicable (benchmarks used with outputs from GPT-4o, Gemini, LLaMA, Mixtral, TULU)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP preference alignment benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Provide human pairwise preference labels (used to compute human factor scores and to compare model-generated factor scores); used both for direct comparison and as training/evaluation data for SFT and feedback-driven methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise human preference labels across tasks; axis evaluations in Reddit TL;DR; helpfulness focus in SHP-2; factual accuracy/usefulness in WebGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Reddit TL;DR (Stiennon et al., 2020) — human ratings of summaries; SHP-2 (Ethayarajh et al., 2022) — instruction-response preferences emphasizing helpfulness; WebGPT (Nakano et al., 2021) — QA answers with factual/useful comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Datasets enabled PROFILE to reveal factor-level misalignment patterns and to train/evaluate self-refinement and feedback-driven approaches; the paper excluded tie-labeled pairs to focus on clear preference distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Coverage: datasets may not represent the full spectrum of human preferences or cultural diversity; human evaluation in this study was limited in scale (budget constraints) and primarily on one task (summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These pairwise-preference datasets reflect common training/evaluation data for RLHF/DPO-style alignment, but PROFILE shows they can mask factor-level misalignment when only used in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using pairwise-preference benchmarks, augment evaluations with factor-level annotations or extraction; avoid relying solely on binary preference signals when training generative models to prevent reinforcing superficial heuristics (e.g., verbosity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncovering Factor Level Preferences to Improve Human-Model Alignment', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DecipherPref: Analyzing influential factors in human preference judgments via GPT-4 <em>(Rating: 2)</em></li>
                <li>The generative AI paradox in evaluation: "what it can solve, it may not evaluate." <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
                <li>FactScore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9679",
    "paper_id": "paper-273228777",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "PROFILE",
            "name_full": "Probing Factors of Influence for Explainability (PROFILE)",
            "brief_description": "An automated framework introduced in this paper to decompose overall pairwise preference judgments into factor-level influences by (1) extracting response-level preferences, (2) measuring factor manifestations per response, (3) computing a factor score (τ14) via concordant/discordant/tie counts, and (4) comparing ranked factor scores between agents using correlation coefficients.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Various (GPT-4o, Gemini 1.5, LLaMA 3.1-70B, Mixtral, TULU 2.5 variants, GPT-3.5)",
            "llm_description": "Set of proprietary and open-source generative LLMs evaluated in the paper, including GPT-4o (OpenAI, proprietary), Gemini 1.5 Flash (proprietary), LLaMA 3.1 70B (open-source, 70B parameters), Mixtral 8x7B Instruct (open-source mixture-of-experts), and TULU 2.5 variants (13B/70B with PPO/DPO alignment).",
            "scientific_domain": "NLP / text generation evaluation (summarization, instruction-following, document-based QA)",
            "evaluation_method": "Factor-level analysis: compute response-level pairwise preferences from human labels or score-conditioned generation, extract scalar factor measurements per response (rule-based, learned metrics, or LLM-based extraction), compute τ14 factor scores from concordance statistics across pairs, then compare factor score rankings between humans and models via Spearman ρ, Kendall's τb, and Pearson r.",
            "evaluation_criteria": "Set of interpretable quality factors (Receptiveness, Off-Focus, Intent Alignment, Hallucination, Source Coverage, Formality Alignment, Novel Words, Length, Fluency, Number of Facts, Helpfulness, Misinformation, Coherence) plus statistical metrics (τ14 factor score, Spearman's ρ, Kendall's τb, Pearson r, pairwise agreement rates).",
            "benchmark_or_dataset": "Applied to existing human-preference datasets: Reddit TL;DR (summarization), Stanford Human Preference-2 (SHP-2; instruction/helpfulness), OpenAI WebGPT (document QA / ELI5). These datasets provide pairwise human preference labels used for PROFILE analysis.",
            "results_summary": "PROFILE reveals systematic factor-level misalignment: models consistently over-prioritize Length across tasks while humans prioritize task-dependent factors (e.g., IntentAlignment, FormalityAlignment, SourceCoverage for summarization; Receptiveness and Helpfulness for instruction/QA). Quantitatively, even the best model achieved only τ≈0.289 correlation with human factor ranks in summarization; evaluation (discrimination) alignment is substantially higher (e.g., GPT-4o evaluation τ≈0.82 vs generation τ≈0.16).",
            "limitations_or_challenges": "Relies on accurate factor extraction (some factors require LLM-based extraction that has non-perfect accuracy), uses available preference datasets that may not cover all human preference diversity, score-conditioned generation is a proxy for intrinsic generative preference, and closed models may lack accessible log probabilities.",
            "comparison_to_human_or_traditional": "PROFILE moves beyond coarse overall scores to show that standard binary human preference labels can mask divergent factor-level priorities; humans vary factor importance by task while models show consistent heuristics (e.g., prefer longer outputs).",
            "recommendations_or_best_practices": "Use factor-level decomposition (PROFILE) when assessing alignment; extract factors via a mix of rule-based, learned metrics (UniEval), and validated LLM-based extraction; report τ14 and rank correlations; validate LLM-based extractions against human annotations; leverage evaluation (discrimination) capabilities to guide generation.",
            "uuid": "e9679.0",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "τ14",
            "name_full": "τ14 (tie-aware Kendall-style factor score)",
            "brief_description": "A tie-aware Kendall correlation variant used as the per-factor 'factor score' measuring concordance between pairwise overall preferences and pairwise factor manifestations; defined as (|C_f| - |D_f|) / (|C_f| + |D_f| + |T_f|) where C_f/D_f/T_f are concordant/discordant/tie pair counts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Framework-agnostic (used with generated outputs from GPT-4o, LLaMA 3.1-70B, Mixtral, TULU 2.5, Gemini 1.5, etc.)",
            "llm_description": "Applied to outputs from multiple LLMs (see PROFILE entry).",
            "scientific_domain": "Evaluation statistics for NLP preference alignment",
            "evaluation_method": "Compute τ14 per factor across all response pairs to quantify whether the preferred response tends to show a stronger manifestation of that factor; positive values indicate a positive influence, negative a negative influence, and magnitude indicates strength.",
            "evaluation_criteria": "Factor influence sign and magnitude; subsequent rank-correlation comparisons (Spearman ρ, Kendall τb, Pearson r) between agents' τ14 vectors to quantify alignment.",
            "benchmark_or_dataset": "Computed over pairwise-labeled datasets (Reddit TL;DR, SHP-2, WebGPT) and over model-generated score-conditioned outputs.",
            "results_summary": "Used to produce ranked factor importance lists for humans and models; revealed that model factor scores (τ14) often emphasize Length strongly while human τ14 emphasize other factors depending on task; used also to measure improvement alignment when applying self-refinement/feedback (e.g., compute τ14 on improvements).",
            "limitations_or_challenges": "Requires reliable scalar measurements m_f(r) for each factor; ties in factor manifestation or preference can reduce discriminatory power; interpretation depends on quality of factor extraction.",
            "comparison_to_human_or_traditional": "Provides interpretable per-factor influence unlike aggregate metrics (e.g., BLEU/ROUGE) or single-score human judgments.",
            "recommendations_or_best_practices": "Report τ14 per factor with confidence intervals; combine with rank-correlation metrics when comparing agents; complement with human validation of factor measures.",
            "uuid": "e9679.1",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Score-based prompting (generation proxy)",
            "name_full": "Score-conditioned Generation via Score-based Prompting",
            "brief_description": "A practical proxy for a model's intrinsic generative preference where the model is prompted to generate outputs conditioned on a target quality score (1–5); the target score serves as the model's U(r) value for pairwise preference computations when log-probabilities are unavailable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "LLaMA-3.1-70B, Mixtral, TULU 2.5, GPT-4o (used for validation)",
            "llm_description": "Open-source and proprietary models prompted to produce outputs conditioned on discrete target quality scores; LLaMA-3.1-70B and Mixtral were validated in the paper.",
            "scientific_domain": "NLP generation preference measurement",
            "evaluation_method": "Condition models to produce outputs labeled by target score and use the target score as the model's value function U(r); validate proxy by correlating target scores with models' log probabilities on generated outputs.",
            "evaluation_criteria": "Pearson correlation between assigned target score and model log probability of generated output; used as evidence that score-conditioned outputs map to the model's intrinsic likelihood preferences.",
            "benchmark_or_dataset": "Validated on 100 summarization samples (Reddit TL;DR) for LLaMA-3.1-70B and Mixtral.",
            "results_summary": "High Pearson correlations observed between target score and log-prob: LLaMA 3.1-70B r≈0.975 and Mixtral r≈0.82, supporting the effectiveness of score-conditioning as a proxy for intrinsic generation preference.",
            "limitations_or_challenges": "Proxy depends on model capability to follow score-conditioning prompts; may not reflect unconstrained sampling distributions exactly; closed models may not permit log-prob validation.",
            "comparison_to_human_or_traditional": "Provides an operational alternative to using internal log probabilities for generation preference measurement, enabling factor-level analysis even when logits are inaccessible.",
            "recommendations_or_best_practices": "Validate score-conditioned outputs against log probabilities when possible (as done with a subset of models); use highest-score (score=5) outputs to approximate unconstrained generation behavior where validated.",
            "uuid": "e9679.2",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Factor extraction methods",
            "name_full": "Factor Manifestation Extraction: rule-based, learned (UniEval), and LLM-based extraction",
            "brief_description": "A mixed extraction strategy: rule-based measures for Length and Novel Words; learned metrics (UniEval) for some quality dimensions; and LLM-based prompts (GPT-4o) for response-level and atomic-fact-level factors (Intent Alignment, Formality Alignment, Number of Facts, Hallucination, Helpfulness, Misinformation, Off-Focus, etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o (used to extract many factors), UniEval (learned metric), plus rule-based scripts",
            "llm_description": "GPT-4o used with few-shot prompts to label atomic facts and response-level properties; UniEval provides 0–1 scores for various aspects; rule-based code counts words/novel words.",
            "scientific_domain": "NLP annotation and metric extraction",
            "evaluation_method": "For atomic-fact factors, generate/identify atomic facts then compute ratios (e.g., hallucination ratio). For response-level factors, use LLM classification prompts (Aligned/Not Aligned/Partially). For learned metrics, take UniEval outputs.",
            "evaluation_criteria": "Binary/ternary decisions for response-level factors; ratios and counts for atomic-fact factors; continuous scores from UniEval; extraction validation against human annotations.",
            "benchmark_or_dataset": "Applied on the same datasets (Reddit TL;DR, SHP-2, WebGPT) and model outputs; validation subset of 50 samples for LLM-based extraction.",
            "results_summary": "LLM-based extraction (GPT-4o) achieved validation accuracies: Intent Alignment 86% (43/50), Formality Alignment 92% (46/50), Receptiveness 90% (50-sample WebGPT), Helpfulness: 87% response-level and 80% atomic-fact-level, Misinformation: 87% response-level and 70% atomic-fact-level precision. Cost per summarization sample for LLM-based extraction ≈ $0.018 total.",
            "limitations_or_challenges": "LLM-based extractors are not perfect (noted false positives/negatives); some misclassifications arise when content inaccuracies are misattributed to intent/formality issues; extraction cost and API dependence are non-negligible.",
            "comparison_to_human_or_traditional": "LLM-based extraction approximates human-level factor annotation efficiently and at lower cost but requires validation; combines well with learned metrics and rule-based measures for comprehensive factor coverage.",
            "recommendations_or_best_practices": "Perform human validation on a held-out sample to estimate extractor accuracy; use mixed methods (rule-based + UniEval + LLM-based) and report extractor accuracy and cost.",
            "uuid": "e9679.3",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Discrimination-vs-Generation (Gen-Eval) gap & LLM-as-evaluator",
            "name_full": "Generation-Discrimination Gap and Leveraging LLM-as-Evaluator for Alignment",
            "brief_description": "Observation that LLMs show substantially stronger factor-level alignment with humans when performing pairwise evaluation (discrimination) than when generating outputs; the paper demonstrates two practical uses of this: (1) supervised fine-tuning (SFT) using model self-evaluation re-ranking (self-refinement), and (2) feedback-driven generation where an evaluator chooses and justifies a preferred generated output and the generator refines accordingly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "TULU 2.5 variants (generator in SFT experiments), GPT-4o (evaluator in feedback-driven experiments), GPT-4o / Gemini / LLaMA as evaluated models",
            "llm_description": "Generators: TULU 2.5 70B/13B variants; Evaluator: GPT-4o; other models used for comparative evaluation: GPT-4o, Gemini 1.5, LLaMA 3.1 70B, Mixtral.",
            "scientific_domain": "Model alignment and training strategies in NLP",
            "evaluation_method": "Measure factor-level alignment (Kendall's τ) in both generation and evaluation settings; perform SFT using model re-ranked outputs (re-ranked by model evaluations), and perform interactive feedback-driven generation with an evaluator providing selection + justification.",
            "evaluation_criteria": "Kendall's τ between model factor-scores and human factor-scores; response-level agreement rates with human judgments; improvements in τ after SFT or feedback-guided refinement.",
            "benchmark_or_dataset": "Reddit TL;DR summarization dataset used for SFT and feedback-driven experiments; generated data used for fine-tuning (4,000 examples) and held-out evaluation (500 examples for SFT; 100 samples for feedback-driven evaluation).",
            "results_summary": "Models show higher factor-level alignment in evaluation than generation (example: GPT-4o evaluation τ≈0.82 vs generation τ≈0.16). Self-refinement SFT using model self-evaluations on TULU 2.5 produced significant improvements in generation alignment, reaching performance comparable to GPT-4o; naive SFT on human-preferred responses (human-SFT) performed worse than original TULU, indicating value of leveraging model evaluation. Feedback-driven generation with GPT-4o evaluator consistently improved alignment vs baselines; baselines that applied implicit self-critique without explicit feedback often showed negative correlations.",
            "limitations_or_challenges": "SFT used relatively small fine-tuning set (4k) and evaluation limited to Reddit summarization; human-SFT underperformed, indicating complexity in using human-preferred data; faithfulness of model-generated feedback needs more scrutiny; evaluation improvements may not generalize across tasks without further study.",
            "comparison_to_human_or_traditional": "Shows a practical pathway where LLM evaluation strengths (discrimination) can be used as a training signal to improve generation alignment, complementing or even outperforming direct training on human-labeled preferred outputs.",
            "recommendations_or_best_practices": "Leverage LLM-as-evaluator signals (re-ranking + justifications) to produce training data for SFT; use explicit evaluator feedback rather than implicit self-refinement; validate improvements on held-out human-labeled data; analyze which factors the evaluator emphasizes to ensure alignment with human priorities.",
            "uuid": "e9679.4",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Datasets/Benchmarks used",
            "name_full": "Preference datasets: Reddit TL;DR, Stanford Human Preference-2 (SHP-2), OpenAI WebGPT, RewardMATH (example)",
            "brief_description": "Public pairwise human-preference datasets used as evaluation benchmarks and ground truth for factor-level alignment: Reddit TL;DR (summaries with axis evaluations), SHP-2 (helpfulness/instruction-following preferences), WebGPT (document QA/ELI5), and RewardMATH used as an example for evaluation on math reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Not applicable (benchmarks used with outputs from GPT-4o, Gemini, LLaMA, Mixtral, TULU)",
            "llm_description": "N/A",
            "scientific_domain": "NLP preference alignment benchmarks",
            "evaluation_method": "Provide human pairwise preference labels (used to compute human factor scores and to compare model-generated factor scores); used both for direct comparison and as training/evaluation data for SFT and feedback-driven methods.",
            "evaluation_criteria": "Pairwise human preference labels across tasks; axis evaluations in Reddit TL;DR; helpfulness focus in SHP-2; factual accuracy/usefulness in WebGPT.",
            "benchmark_or_dataset": "Reddit TL;DR (Stiennon et al., 2020) — human ratings of summaries; SHP-2 (Ethayarajh et al., 2022) — instruction-response preferences emphasizing helpfulness; WebGPT (Nakano et al., 2021) — QA answers with factual/useful comparisons.",
            "results_summary": "Datasets enabled PROFILE to reveal factor-level misalignment patterns and to train/evaluate self-refinement and feedback-driven approaches; the paper excluded tie-labeled pairs to focus on clear preference distinctions.",
            "limitations_or_challenges": "Coverage: datasets may not represent the full spectrum of human preferences or cultural diversity; human evaluation in this study was limited in scale (budget constraints) and primarily on one task (summarization).",
            "comparison_to_human_or_traditional": "These pairwise-preference datasets reflect common training/evaluation data for RLHF/DPO-style alignment, but PROFILE shows they can mask factor-level misalignment when only used in aggregate.",
            "recommendations_or_best_practices": "When using pairwise-preference benchmarks, augment evaluations with factor-level annotations or extraction; avoid relying solely on binary preference signals when training generative models to prevent reinforcing superficial heuristics (e.g., verbosity).",
            "uuid": "e9679.5",
            "source_info": {
                "paper_title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DecipherPref: Analyzing influential factors in human preference judgments via GPT-4",
            "rating": 2,
            "sanitized_title": "decipherpref_analyzing_influential_factors_in_human_preference_judgments_via_gpt4"
        },
        {
            "paper_title": "The generative AI paradox in evaluation: \"what it can solve, it may not evaluate.\"",
            "rating": 2,
            "sanitized_title": "the_generative_ai_paradox_in_evaluation_what_it_can_solve_it_may_not_evaluate"
        },
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2,
            "sanitized_title": "learning_to_summarize_with_human_feedback"
        },
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "FactScore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 1,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        }
    ],
    "cost": 0.016497,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Uncovering Factor-Level Preference to Improve Human-Model Alignment
15 Nov 2025</p>
<p>Juhyun Oh 411juhyun@kaist.ac.kr 
Eunsu Kim 
Jiseon Kim 
Wenda Xu 
University of California
Santa Barbara</p>
<p>Inha Cha 
Georgia Institute of Technology</p>
<p>William Yang Wang 
University of California
Santa Barbara</p>
<p>Alice Oh 
Kaist 
Uncovering Factor-Level Preference to Improve Human-Model Alignment
15 Nov 2025B5571AB7A45088B5AC0D30808136FD2EarXiv:2410.06965v3[cs.CL]
Large language models (LLMs) often exhibit tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs.While crucial for improvement, identifying the factors driving these misalignments remains challenging due to existing evaluation methods' reliance on coarse-grained comparisons and lack of explainability.To address this, we introduce PROFILE, an automated framework to uncover and measure factor-level preference alignment of humans and LLMs.Using PROFILE, we analyze preference alignment across three key tasks: summarization, instruction-following, and document-based QA.We find a significant discrepancy: while LLMs show poor factorlevel alignment with human preferences when generating texts, they demonstrate strong alignment in discrimination tasks.We demonstrate how leveraging the identified generation-discrimination gap can be used to improve LLM alignment through multiple approaches, including fine-tuning with self-guidance.Our work highlights the value of factor-level analysis for identifying hidden misalignments and provides a practical framework for improving LLM-human preference alignment.</p>
<p>Introduction</p>
<p>Human preference for text is inherently multifaceted, influenced by an interplay of factors such as fluency, helpfulness, and conciseness.The relative importance of these factors is not static; it shifts depending on the specific task and context.For instance, a desirable summary should be concise and to the point, while creative writing might prioritize novelty and an engaging narrative.As large language models (LLMs) generate increasingly human-like text, a critical question arises: do LLMs prioritize these quality factors in ways that align with human expectations?This question becomes particularly pressing given recent findings highlighting discrepancies between LLMs' generation and discrimination abilities (West et al., 2023;Oh et al., 2024).We extend this line of inquiry to investigate whether such discrepancies also manifest at the factor level of preference alignment: do models prioritize individual quality factors consistently with human judgment during generation, and does this prioritization differ when discriminating between responses?Despite significant advances in preference alignment (Ouyang et al., 2022;Rafailov et al., 2024;Song et al., 2024), existing approaches take a coarse-grained view, measuring overall preferences while overlooking the underlying factors that drive them.Recent work has begun examining finer-grained preference aspects (Hu et al., 2023;Kirk et al., 2024;Scheurer et al., 2023), yet systematic factor-level comparisons between human and model priorities remain To address these gaps, we introduce PRO-FILE, an automated framework designed to decompose and quantify how individual factors (e.g., fluency, helpfulness) contribute to overall preference decisions.PROFILE quantifies each factor's contribution as a factor score.By comparing how factor scores of responses correlate with overall preference decisions, PRO-FILE captures the factor-level preferences of humans and models.This enables systematic comparison between human and model priorities across both generation and discrimination settings.Using this framework, we investigate three key research questions:</p>
<ol>
<li>
<p>To what extent do LLMs, during generation, exhibit factor-level preference alignment with human expectations across various tasks?</p>
</li>
<li>
<p>How does this factor-level alignment compare when the same models perform discrimination tasks (i.e., distinguishing between good and bad responses) versus generation?</p>
</li>
<li>
<p>Can insights from observed alignment differences between these settings be leveraged to improve the less aligned setting?</p>
</li>
</ol>
<p>We conduct comprehensive experiments across three preference alignment taskssummarization, instruction-following, and document-based QA-evaluating eight prominent LLMs.Our analysis reveals systematic misalignments: models often do not prioritize quality factors in line with human expectations during generation.For instance, models frequently exhibit a strong preference for length regardless of the task, whereas human preferences for factors such as conciseness or detail vary contextually (Figure 1).Interestingly, we observe that these same LLMs demonstrate notably better factor-level alignment during discrimination tasks, specifically in evaluation settings where the model selects which of two outputs is better.This disparity between generation and discrimination alignment presents an opportunity, and we demonstrate ways to leverage the stronger alignment in discrimination to enhance the factorlevel preference alignment during generation.Our work underscores the importance of factorlevel analysis for a deeper understanding of LLM alignment and offers a pathway toward more genuinely human-aligned generative models.</p>
<p>PROFILE: Framework for Analyzing Human-Model Alignment</p>
<p>We introduce PROFILE (Probing Factors of Influence for Explainability), a framework that provides a systematic way to decompose overall response level preferences into their underlying factors.PROFILE makes explicit the notion of factor-level preference-the extent to which specific factors such as fluency, factual accuracy, or conciseness influence which response is preferred.By uncovering these factor-level preferences, PROFILE enables a more interpretable analysis of human and model choices and provides a principled basis for comparing and improving their alignment.PROFILE centers on the computation of factor scores, which quantify the influence of individual factors on overall preferences.To compute these scores, PROFILE proceeds in three steps: (1) we define overall response-level preferences for humans and models ( § 2.2); (2) we quantify the manifestation of each factor in every response based on our factor taxonomy ( §2.3); (3) we compare these pairwise factor manifestations with preference labels to measure concordance, thereby deriving factor scores that reveal the influence of each factor and the alignment of model preferences with human values ( §2.1 &amp; §2.4). Figure 2 illustrates this overall process, which is detailed below:</p>
<p>Quantifying Factor Influence</p>
<p>To quantify the influence of a given factor f , we calculate its factor score, τ (f ), by analyzing the concordance between response-level preferences and factor-level manifestations across all pairs of responses {r i , r j }.We use τ 14 , a variation of Kendall's correlation well-suited for handling ties (Macháček and Bojar, 2014), defined as:
τ (f ) = |C f | − |D f | |C f | + |D f | + |T f | (1)
Here, C f is the set of concordant pairs, where the overall preference aligns with the factor manifestation (i.e., the preferred response also has a stronger manifestation of the factor).D f is the set of discordant pairs where they do not align, and T f accounts for ties.</p>
<p>A positive score indicates a positive preference for the factor, a negative score indicates a negative preference, and a score near zero implies minimal influence.The magnitude reflects the strength of this influence.To compute this score, we must first define its two key components: the overall response-level preference (P ref ), and the pairwise factor manifestation (M f ).</p>
<p>Measuring Overall Response-level Preference</p>
<p>The overall preference function P ref (r i , r j ) captures which response is considered better in a pair.We define it as:
P ref (r i , r j ) = sign U (r i ) − U (r j ) ,
where U (r) denotes the value assigned to a response r by the agent.This yields 1 if r i is preferred, −1 if r j is preferred, and 0 for a tie.This value function is obtained as follows for humans and models.Human Preference.For humans, the value U (r) is derived directly from pairwise annotations where labelers select the preferred response, thus determining the sign of the preference.In our study, we leverage existing datasets with human preference labels to obtain these values.</p>
<p>Model Preference in Generation.A model's generation preference is traditionally defined using log likelihood (P (x) = n i=1 log P (x i |x &lt;i )).While this is a direct measure, it presents practical challenges: manipulating logits to obtain distinctive outputs can be difficult, and log probabilities are often inaccessible for closed models.</p>
<p>To overcome these issues, we use score-based prompting as a proxy measure.In this approach, we instruct the LLM to generate a response conditioned on achieving a target quality score from 1 to 5. The target score itself serves as the value for the generated response, U (r).For instance, if response r i was generated with a target score of 4 and r j with a score of 3, we define that U (r i ) &gt; U (r j ), and thus the model "prefers" r i in this generation context.This approach is inspired by methods used in constructing training datasets for LLM-as-a-judge (Kim et al., 2023), reflecting real-world applications where models are conditioned on specific quality targets.</p>
<p>To validate that this proxy effectively approximates the models' intrinsic preferences, we conducted an experiment using 100 samples from summarization tasks.Specifically, we prompted open-source models (Llama-3.1-70Band Mixtral) to generate distinct summaries Factor Description Tasks</p>
<p>Receptiveness</p>
<p>Whether the core question of the input has been answered.</p>
<p>I, Q Off Focus</p>
<p>The ratio of atomic facts that are not related to the main focus of the input.S, I, Q Intent Align.</p>
<p>Whether the intent of the source and output is the same.</p>
<p>S Hallucination</p>
<p>The ratio of atomic facts that are incorrect compared to the original source.</p>
<p>S, I, Q Source Coverage</p>
<p>The ratio of atomic facts in the source that appear in the output.S Formality Align.</p>
<p>Whether the formality of the source and output is the same.</p>
<p>S Novel Words</p>
<p>The ratio of words in the output that are not used in the source.</p>
<p>S Length</p>
<p>The number of words used in the output.</p>
<p>S, I, Q Fluency</p>
<p>The quality of individual sentences of the output.S, I, Q Number Of Facts The number of atomic facts in the output.</p>
<p>S, I, Q Helpfulness</p>
<p>The ratio of facts that provide additional helpful information.</p>
<p>I, Q Misinformation</p>
<p>The ratio of facts in the output that include potentially incorrect or misleading information.</p>
<p>I, Q</p>
<p>Coherence Whether all the sentences of the output form a coherent body.S, I, Q for target scores ranging from 1 to 5. We then computed the log probability of each generated summary and observed a strong Pearson correlation with the target scores (Llam: 0.975; Mixtral: 0.82; see Figure 4 in the Appendix).These results suggest that our scoring mechanism serves as an effective proxy for the models' intrinsic generation preferences.</p>
<p>Measuring Pairwise Factor Manifestation</p>
<p>The factor manifestation function M f (r i , r j ) determines which response in a pair exhibits a stronger presence of factor f .It is defined similarly to preference:
M f (r i , r j ) = sign m f (r i ) − m f (r j ) ,
where m f (r) is a scalar measurement of factor f in response r.For example, if m length (r) is the character count, M length (r i , r j ) = 1 when r i is longer.The measurement m f (r) is derived from our factor taxonomy.(Zhong et al., 2022).UniEval is a learned metric that provides scores of range 0-1 for various aspects of text quality.(iii) LLM-based: For factors that rely on objective criteria but require more nuanced judgment, we use GPT-4o with carefully designed prompts.This approach is further divided into "response-based" (Intent Alignment and Formality Alignment) and "atomic-fact-based" (the remaining seven) extraction depending on the level of detail needed for each factor.The specific details of the implementation of each method and validation of LLM-based extractions can be found in Appendix D.</p>
<p>Comparing Human and Model Preferences</p>
<p>Finally, we measure the factor-level preference alignment between humans and models.</p>
<p>With a factor score τ (f ) computed for each agent, we create a ranked list of factors for both humans and models.We then quantify the alignment of these rankings using standard correlation coefficients: Spearman's ρ, Kendall's τ b , and Pearson's r.This provides a clear metric for how well a model's factor priorities align with human values.Together, these steps make PROFILE a structured framework for analyzing the drivers of preference judgments.By making factor-level influences explicit, PROFILE enables 1) more interpretable comparisons between humans and models and 2) provides a consistent basis for assessing their alignment across different tasks and factors.</p>
<p>Uncovering Factor-Level Preference of LLMs</p>
<p>In this section, we analyze how models and humans differ in their factor-level preferences during text generation.Using human preference datasets across summarization, instructionfollowing, and QA tasks, we apply PROFILE to model-generated responses and compare the relative importance of each factor with human judgments.Tasks.We use human preference alignment data publicly available.Among them we choose: (i) Reddit TL;DR (Stiennon et al., 2020) ).Proprietary models include Gemini 1.5 Flash (Reid et al., 2024), GPT-4o (OpenAI, 2024), and GPT-3.5.From here on, we refer to Gemini 1.5 Flash as Gemini 1.5, Mixtral 8x7B Instruct v0.1 as Mixtral, TÜLU v2.5 models as Tulu 2.5 + {alignment training strategy}.Detailed descriptions of the datasets and models can be found in Appendix A.2.</p>
<p>Experimental Setting</p>
<p>Experimental Setup.For each task, models generate a response that would receive a score of 1-5.The specific prompts we used can be found in Appendix E. Additionally, we find that responses generated with score 5 strongly align with those from direct, unconstrained generation (see Table 16), suggesting the generalizability of our experimental setting.</p>
<p>Factor-level Alignment in Generations</p>
<p>PROFILE enables fine-grained analysis of preference alignment by breaking down overall judgments into interpretable factor-level scores.This allows us to identify not only how models and humans differ in ranking specific factors (Figure 3), but also to quantify their alignment using correlation metrics (Table 2).Through this, PROFILE reveals consistent patterns of agreement and misalignment that would be obscured by aggregate quality scores alone.Human and model preferences consistently misalign at the factor level across tasks.</p>
<p>While humans' most preferred factors vary</p>
<p>1 Our framework can also be applied to other tasks.We provide guidelines for applying it to different tasks, with an example of a mathematical reasoning task in the Appendix E.2. by task, models consistently prioritize length across all tasks, suggesting models associate better quality with longer outputs.In both instruction-following tasks (Figure 3b) and document-based QA (Figure 3c), humans prioritize Receptiveness and Helpfulness.Although these two factors are also highly ranked for the models, the models always prioritize Length as the most important factor.</p>
<p>The misalignment pattern is particularly problematic in summarization tasks.Humans prioritize IntentAlignment, FormalityAlignment, and SourceCoverage while penalizing the inclusion of words not in the original post, indicating the importance of maintaining the original content and style.In contrast, models consistently prefer longer summaries with new words (Table 7).A full list of factor scores of all models across three tasks is available in the Appendix (Table 10 -12).</p>
<p>To quantify this misalignment, we measure factor-level preference alignment (τ ).The left Generation column in Table 2 shows that even the best-performing model (Gemini 1.5) only achieves a 0.289 τ correlation with human preferences in summarization.Similar low correlations are observed in the other two tasks (Table 15).This low correlation highlights the limitations of current models in capturing the granular aspects comprising human preference.</p>
<p>Qualitative analysis demonstrates how PROFILE explains the observed misalignment.In a Reddit post below, GPT-4o's score 5 summary is longer and includes more facts than its score 3 summary, yet the shorter summary is human-preferred.The higher-scored model summary includes irrelevant details like "Midwest hometown" and "new to Reddit," demonstrating the model's tendency to prioritize information quantity over relevance.Full examples are in Appendix B.1.</p>
<p>GPT-4o Generation Sample</p>
<p>Post: Good Morning/Afternoon r/advice, Never posted on Reddit before at all, but I figured (based on the overall reliability of you nice individuals) (...)</p>
<p>Score 5 [length: 93, # facts: 10, src.coverage: 0.389]: A Reddit user recently moved back to their Midwest hometown and, while setting up utilities for their new place, discovered they owe $500 in gas bills from a college house they lived in until 2012.(...) Score 3 [length: 61, # facts: 9, src.coverage: 0.44]: A Reddit user seeks advice after discovering they owe $500 in gas bills from a college house they left in 2012.(...) (Human Preferred Output)</p>
<p>Achieving Better Alignment</p>
<p>Using the Model as an Evaluator</p>
<p>In this section, we examine factor-level alignment in discrimination tasks.Specifically, we turn to the evaluation setting, where models are used to determine which of two outputs is better.Prior work has shown that model behavior can differ substantially between generation and discrimination tasks (West et al., 2023;Oh et al., 2024).We therefore examine whether factor-level alignment also varies in evaluation ( §4.1), and whether any observed improvements can be leveraged to guide better generation ( §4.2).</p>
<p>Factor-level Alignment in Evaluation</p>
<p>We examine the same models from §2.1 in an evaluation setting where models perform pairwise comparisons to determine which response is better.We use the response pairs provided in the existing datasets, with evaluation prompts detailed in AppendixE.Our analysis reveals that models demonstrate significantly stronger factor-level alignment with human preferences during evaluation compared to generation tasks.Table 3 illustrates this pattern by comparing factorlevel preference alignment between humans and models, measured using Kendall's τ .Alignment scores are consistently higher in evaluation across all models: GPT-4o achieves the highest evaluation alignment (τ = 0.82) while showing substantially lower generation alignment (τ = 0.16).This gap between evaluation and generation capabilities suggests that models possess stronger discriminative abilities than their generative performance would indicate.This substantial difference in alignment capabilities raises a natural question: can we leverage models' superior evaluation alignment to actively improve their generation alignment?</p>
<p>Leveraging LLM-as-an-evaluator</p>
<p>Given the substantial gap between models' evaluation and generation alignment, we investigate whether LLMs' superior evaluation capabilities can be leveraged to improve generation performance.Using Reddit TL;DR summarization dataset, we explore two complementary approaches: self-refinement through supervised fine-tuning and feedback-driven generation.</p>
<p>Gen-Eval gap explains</p>
<p>self-refinement's effectiveness We investigate whether supervised fine-tuning (SFT) with self-evaluation can bridge the alignment gap in generation tasks.Using TULU 2.5 (70B RM), we generate summaries with target scores 1-5, then use the same model to perform pairwise evaluations and re-rank these summaries based on win rates.We then fine-tune the generator on 4,000 such examples, where inputs are instructions to generate summaries of different quality levels (1-5) and outputs are the re-ranked summaries.We evaluate the finetuned model on 500 unseen examples.</p>
<p>Table 4 demonstrates that this selfevaluation approach significantly improves generation alignment, achieving performance comparable to GPT-4o (see Table 2).Notably, a baseline model trained on human-preferred responses (human-SFT ) using identical training configurations actually performed worse than the original TULU model.This counterintu-itive result aligns with broader observations that even DPO-or RLHF-trained models often struggle to consistently align with human preferences.This result highlights the potential of leveraging models' evaluation capabilities for training.</p>
<p>Leveraging evaluation for better alignment in generation</p>
<p>We further explore whether explicit evaluator feedback can improve generation alignment in real-time.Our approach involves a generator producing two initial summaries per post, followed by an evaluator selecting the preferred response (or tie) and providing a detailed justification.The generator then uses this feedback to produce an improved summary.</p>
<p>Using GPT-4o as the evaluator, we compare this feedback-driven approach against two baselines: (1) Baseline A : the generator produces one improved summary from both initial summaries without external feedback; (2) Baseline B : the generator produces two improved summaries without feedback, each refined from one initial summary.These baselines represent typical self-improvement scenarios.We evaluate on 100 samples across three generators: GPT-4o, LLaMA 3.1 70B, and Tulu 2.5 + PPO.</p>
<p>Table 5 shows that incorporating evaluator feedback consistently improves alignment with both GPT-4o and human judgments across all generators.In contrast, both baselines show negative correlations, indicating that implicit self-critique without explicit feedback (i.e., regeneration) actually diverges from desired preferences.Manual analysis of 30 samples confirms that evaluator feedback effectively emphasizes factors that align with evaluation preferences (see Appendix F.2.3 for detailed analysis).</p>
<p>These findings demonstrate that leveraging models' superior evaluation capabilities-either through self-refinement during training or explicit feedback during generation-can effectively improve factor-level alignment in generation tasks.See Appendix F.2.1 for prompt and metric details.</p>
<p>Related Work</p>
<p>Human-AI Preference Alignment.Aligning LLMs with human preferences is a central focus in LLM research, leading to techniques like supervised instruction tuning (Mishra et al Fine-grained Evaluation of LLMs.Recent research has increasingly emphasized the need for more fine-grained evaluations of LLMs.</p>
<p>For instance, researchers have proposed finegrained atomic evaluation settings for tasks like fact verification and summarization (Min et al., 2023;Krishna et al., 2023), developed a benchmark for fine-grained holistic evaluation of LLMs on long-form text (Ye et al., 2024), and enhanced evaluation transparency through natural language feedback (Xu et al., 2023).</p>
<p>Building on this trend, our work shifts from evaluating individual factors in isolation to analyzing their influence on human preferences and investigating the alignment between human and model judgments regarding the relative importance of these factors.</p>
<p>Analyzing behaviors of LLM-as-a-judge.Furthermore, researchers are actively exploring the potential of LLMs as evaluators.</p>
<p>Conclusion</p>
<p>We introduce PROFILE, a framework for granular factor level analysis of LLM alignment with human preferences.Our analysis using PRO-FILE reveals that LLMs tend to over-prioritize factors like output length, misaligning human preferences during generation.However, these models exhibit stronger alignment in evaluation tasks, indicating the potential for leveraging evaluative insights to improve generative alignment.PROFILE facilitates a nuanced understanding of the alignment gaps between human and model preferences.These insights underscore the necessity for more sophisticated, factor-level alignment strategies that can guide the development of LLMs to better align with human expectations, ultimately fostering more reliable aligned AI systems.</p>
<p>Limitations</p>
<p>This study has several limitations.First, the preference datasets used may not fully represent the entire spectrum of human preferences.2024)6 , and GPT-3.57 .We set the parameters for all models to: temperature = 0.6, top_p = 0.9, and max_tokens = 1024.4 Quadro RTX 8000 48GB were used with CUDA version 12.4 when running TULU Models.We used autrotrain library8 for supervised fine-tuning TULU model in experiments in § 4. The parameters for fine-tuning are as follows: block_size: 2048, model_max_length: 4096, epochs: 2, batch_size: 1, lr: 1e-5, peft: true, quantization: int4, target_modules: all-linear, padding: right, optimizer: paged_adamw_8bit, scheduler: linear, gradient_accumulation: 8, mixed_precision: bf16, merge_adapter: true</p>
<p>B Human Evaluation of Model Generations</p>
<p>We collect human preference data via Amazon Mechanical Turk (MTurk) for 30 posts and 6 models.For each post, three summary pairs-selected from five model-generated summaries (scored 1 to 5)-are presented to three annotators.Annotators, restricted to US-based workers with a 95% and HIT approval rate and over 5,000 approved HITs are recruited.</p>
<p>The MTurk task description clearly explained the study's purpose and data usage.As shown in Figure 5, we provide detailed instructions about the experiment through MTurk, and participants who consented then participated in the study.</p>
<p>For the main experiment, we gave annotators the following instructions: "A good summary is a shorter piece of text that captures the essence of the original.It aims to accomplish the same purpose and convey the same key information as the original post.Please choose the better summary, A or B." Each annotation is compensated at $0.25.This process yields 1,620 annotations (30 posts * 6 models * 3 pairs/model/post * 3 annotations/pair).</p>
<p>For each model and corresponding summary pair, we calculate an agreement rate, which indicates the percentage of pairs where at least two out of three annotators prefer the summary with the higher score assigned by the model.Each model is evaluated on 90 summary pairs, derived from 30 posts with three pairs per post.A higher score summary generated by GPT-4o, as shown by the factor-level analysis (in magenta), is longer and includes details not necessary to understand the essence of the original post.</p>
<p>B.1 Examples of Human-Model Misalignment</p>
<p>GPT-4o misaligned examples</p>
<p>B.2 Human Evaluation Results of Model Generations</p>
<p>The agreement rates for each model are summarized in Table 6.Despite low factor-level preference alignment (τ ), overall agreement rates range from 56% to 75%.This suggests that binary evaluations, where annotators choose the better summary based on overall quality, can mask factor-level misalignments such as preferences for summary length.Since models consistently favor longer responses, continued reliance on this setup to generate humanannotated training data may reinforce the tendency towards verbose answers, aligning with human preferences superficially but missing finer-grained alignment on factors like length or conciseness.</p>
<p>C Additional Examples of Model Generations and Evaluations</p>
<p>C.1 Generation</p>
<p>Table 7 shows GPT-4o generated summaries of scores 1-5.There is a clear trend of higher score summaries being longer.The information additional to lower-scoring summaries is the ages of the characters, which is not essential in understanding the original post.This is a randomly selected example and a typical modelgenerated summary case.Additionally, we analyze cases where modelgenerated outputs receive higher scores but are not preferred by human annotators (based on majority votes).The human-model misalignment is more pronounced when comparing summaries with larger score gaps assigned by the models.Below, we provide additional examples of such divergence for GPT-4o, LLaMA 3.1 70B and Tulu 2.5 + PPO (13B RM).We find that when the length and information gap is significant, humans favor summaries that are shorter and less information-dense, as model-generated summaries with lower scores (1-2) generally convey the main intention of the post as well.When factual content is similar, humans tend to favor more informal responses (e.g., "Guy" over "An 18-year-old male", and "gotten back together" over "renewed romantic connection"), which aligns with our factor-level findings indicating higher formality-alignment for human-preferred outputs, since Reddit posts are mostly informal.</p>
<p>Warning: Examples below contain content that may be offensive or upsetting.</p>
<p>GPT-4o misaligned examples</p>
<p>GPT-4o's score 4 generation: An 18-yearold male in a long-distance relationship with his 17-year-old girlfriend told her he loves her but wants an open relationship.The girlfriend, who has been with four other boys before him, reacted very negatively, telling him to "FUCK OFF" and that she never wants to speak to him again.He is seeking advice on how to make the situation easier for her and whether he can maintain their relationship while exploring other people.</p>
<p>GPT-4o'</p>
<p>LLaMA-3.1-70b misaligned examples</p>
<p>LLaMA's score 4 generation: I'm in a relationship with an 18-year-old girl who's a self-described Tsundere.We share similar interests, but her mood swings are causing problems.She can be affectionate and loving, but also violent and aggressive, lashing out at me over minor issues.For example, she threw shoes at me for a joke and almost stabbed me with a knife over a trivial matter.I'm scared and unsure how to navigate these outbursts.</p>
<p>LLaMA's score 2 generation: I'm dating a girl who's similar to me, but she's really violent and has mood swings.She gets angry over small things and has even attacked me.I'm scared and need help.(Human Preferred Output)</p>
<p>Tulu 2.5 + PPO (13B RM) misaligned examples</p>
<p>Tulu's score 4 generation: The OP discovered a potentially incriminating text exchange between their SO and her ex's sister, suggesting a renewed romantic connection.</p>
<p>Tulu's score 1 generation: The OP read a suspicious text on their SO's phone, sent from her ex's sister, that suggested they might have gotten back together.(Human Preferred Output)</p>
<p>C.2 Evaluation</p>
<p>We provide examples where the model evaluations align with human preferences, even if the chosen option contains less facts or is shorter.In the first example, where both GPT-4o and LLaMA 3.1 70B correctly chose humanpreferred summary, while the chosen summary is shorter, it more accurately reflects the key issue in the original post by mentioning the writer's economic status.In the second example, the GPT-4o chosen summary is more clearly reflecting the content in post over the other option which analogically describes the main idea of the post.</p>
<p>GPT-4o &amp; LLaMA aligned examples</p>
<p>Post: Yesterday, I accidentally dropped my Motorola Atrix 2 and the screen cracked really badly.My phone is still fully functional, but it's a bit difficult to see what I'm doing when I'm texting or web browsing, etc. Anyway, I stupidly didn't buy insurance for my phone and I'm not eligible for an upgrade until next May!AT&amp;T offers some options as far as getting a no-commitment phone at a slight discount, but spending $300-$600 for a new phone isn't really in the budget right now.(...) I found a couple websites that will repair your phone if you send it in.[Doctor Quick Fix]</p>
<p>GPT-4o aligned &amp; LLaMA misaligned examples</p>
<p>Post: I got a letter in the mail saying I've been passed up for being hired for my dream job.I wanted this job for 10 damn years and now it's over.I've trained my body, mind, and soul for this job and just through a simple letter, I've been removed from that process.I was in good standing with getting hired.Passed everything with flying colors.(...) Now what?Am I to live with my parents the rest of my life?Am I to never get my dream car?Am I to just keep my job where I only get paid minimum wage while I make the company tens of thousands?I don't know what to do.I mean my second dream job would be to work with penguins, but I don't think that's possible for me.Anyone have any advice for me?What should I do?Summary A: I followed the yellow brick road for half my life and ended up at a complete dead end and I can't turn around to go back.</p>
<p>GPT-4o misaligned examples</p>
<p>GPT-4o's score 4 generation: An 18-yearold male in a long-distance relationship with his 17-year-old girlfriend told her he loves her but wants an open relationship.The girlfriend, who has been with four other boys before him, reacted very negatively, telling him to "FUCK OFF" and that she never wants to speak to him again.He is seeking advice on how to make the situation easier for her and whether he can maintain their relationship while exploring other people.</p>
<p>GPT-4o's score 1 generation: Guy told his girlfriend he wants an open relationship.She got mad and told him to go away.He wants advice.(Human Preferred Output)</p>
<p>Llama-3.1-70b misaligned examples</p>
<p>Llama's score 4 generation: I'm in a relationship with an 18-year-old girl who's a self-described Tsundere.We share similar interests, but her mood swings are causing problems.She can be affectionate and loving, but also violent and aggressive, lashing out at me over minor issues.For example, she threw shoes at me for a joke and almost stabbed me with a knife over a trivial matter.I'm scared and unsure how to navigate these outbursts.</p>
<p>Llama's score 2 generation: I'm dating a girl who's similar to me, but she's really violent and has mood swings.She gets angry over small things and has even attacked me.I'm scared and need help.(Human Preferred Output)</p>
<p>Tulu 2.5 + PPO (13B RM) misaligned examples</p>
<p>Tulu's score 4 generation: The OP discovered a potentially incriminating text exchange between their SO and her ex's sister, suggesting a renewed romantic connection.</p>
<p>Tulu's score 1 generation: The OP read a suspicious text on their SO's phone, sent from her ex's sister, that suggested they might have gotten back together.(Human Preferred Output)</p>
<p>D PROFILE</p>
<p>D.1 Validation</p>
<p>Figure 4 shows the distribution of Pearson correlations over 100 samples for both LLaMA-3.1-70B and Mixtral.</p>
<p>We find that the correlation of most samples are concentrated between 0.85 and 1.0, indicating a strong correlation between the target scores in our score-conditioned setting and the models' log probabilities (i.e., their preference for those responses)</p>
<p>D.2 Factor Extraction Methods</p>
<p>Rule-based Extraction We obtain the Length and Novel Words using a rule-based extraction method.First, we calculate the output's length and count the novel words by removing special characters and splitting the text into words.The total word count represents Length.For Novel Words, we stem both the source text and the model output to create unique sets of stemmed words, then determine the number and proportion of unique words in the output that differ from the source.</p>
<p>LLM-based Extraction</p>
<p>The calculations are divided into atomic-fact-level and responselevel based on the granularity of the factors.</p>
<p>Atomic-Fact-Level Factors refer to those factors that are evaluated based on the presence or absence of each factor at the atomic fact level.An atomic fact is a short, self-contained piece of information that does not require further explanation and cannot be broken down further (Min et al., 2023).These include the Number Of Facts, Source Coverage, Off Focus, Hallucination, Helpfulness, and Misinformation.The Number Of Facts is determined by counting the total atomic facts, while the remaining factors are calculated as the ratio of relevant atomic facts to the total number of atomic facts.</p>
<p>Response-Level Factors refer to those factors that are evaluated based on the presence or absence of each factor at the response level.These include Receptiveness, Intent Alignment, and Formality Alignment.Formality Alignment is classified into one of three categories: [Aligned/Misaligned/Partially-Aligned], while the other two factors are determined in a binary manner [Yes/No].</p>
<p>The prompts used are provided in D.3.The Source Coverage does not have a separate prompt since it was calculated using the output from the Hallucination (i.e., the ratio of nonhallucinated atomic facts to the total number of atomic facts in the Source Post).</p>
<p>Cost of LLM-based Extraction.</p>
<p>Here we report the average cost required for LLM-based extraction using GPT-4o.</p>
<p>D.3.2 Template for Input-Output Factors Receptiveness</p>
<p>Does the response clearly address the query from the original post?First determine the core question or purpose of the original post from the user, and evaluate whether the response clearly serves as the proper answer to the question.Provide your response in JSON format, with a 'yes' or 'no' decision regarding the response's receptiveness to the original post, along with justifications.: {FEW SHOT}</p>
<p>INPUT:</p>
<p>Post: {POST} Response : {OUTPUT}</p>
<p>Off Focus</p>
<p>You have been provided a statement.Can you determine if it is related to the main focus of the post?The main focus of a post is the core subject around which all the content revolves.Format your response in JSON, containing a 'yes' or 'no' decision for each statement in the set, along with justifications.</p>
<p>{FEW SHOT} INPUT:</p>
<p>Reddit Post: {POST}</p>
<p>D.3.3 Template for Source-Output Factors Intent Alignment</p>
<p>You have been provided a statement.Can you determine if it is related to the main focus of the post?The main focus of a post is the core subject around which all the content revolves.Format your response in JSON, containing a 'yes' or 'no' decision for each statement in the set, along with justifications.</p>
<p>{FEW SHOT}</p>
<p>INPUT: {ATOMIC FACT} Reddit Post: {POST}</p>
<p>Hallucination</p>
<p>You have been provided with a set of statements.Does the factual information within each statement accurately match the post?A statement is considered accurate if it does not introduce details that are unmentioned in the post, or contradicts the post's existing information.Provide your response in JSON format, with a 'yes' or 'no' decision for each statement in the set, along with justifications.</p>
<p>{FEW SHOT}</p>
<p>INPUT: {ATOMIC FACT} Reddit Post: {POST}</p>
<p>Formality Alignment</p>
<p>You have been provided an original post and a summary.First determine the formality (formal, informal) for both the post and the summary.Then, decide if the formalities align.If they match perfectly, return "Aligned", if they are similar in terms of formality (e.g., both informal) but have slight differences in how much formal/informal they are, return "Partially Aligned", and if they don't match, return "Not Aligned".Format your response in JSON as follows: Output Format: {"decision": , "justification": } {FEW SHOT} Reddit Post: {POST} Summary : {OUTPUT}</p>
<p>D.3.4 Template for Output-Only Factors Helpfulness</p>
<p>You have been provided a statement.Can you determine if this statement provides helpful information, although not directly necessary to answer the question?{FEW SHOT} INPUT: question: {POST} statements: {ATOMIC FACT}</p>
<p>Misinformation</p>
<p>You have been provided a statement.Can you determine if it contains potentially incorrect or misleading information?Potential misleading information include assumptions about user; medical, legal, financial advice; conspiracy theories; claims to take real world action and more.</p>
<p>{FEW SHOT} INPUT: {ATOMIC FACT}</p>
<p>D.4 Validation of LLM-based Extractions</p>
<p>We use GPT-4o to extract (1) manifestations of response-level factors-Intent Alignment and Formality Alignmentand (2) Number 0f Facts from outputs for our analysis ('atomic-factbased'). To assess the validity of GPT-4o's evaluation of each factor, we randomly selected 50 samples and found that GPT-4o accurately assessed Intent Alignment in 43 out of 50 samples (86%) and Formality Alignment in 46 out of 50 samples, resulting in an accuracy of 92%.Most misalignments occur when GPT-4o marks a response as 'Not aligned' due to content inaccuracies, even when intent or formality is not the issue.Consistent with prior works using GPT as an extractor of atomic facts (Hu et al., 2023;Min et al., 2023), we find taking atomic facts generated by GPT-4o acceptable and similar to human.We rely on GPT-4o in detecting Hallucination Off Focus, as Hu et al. ( 2023) reports the accuracy of GPT-4 in these two tasks as 89% and 83%, respectively.Source Coverage is essentially extracted in the same way as Hallucination but with the direction of fact-checking reversed (i.e., checking whether the atomic fact from the source (post) is present in the output (summary)).</p>
<p>We further validated GPT-4o's extractions for Helpfulness and Misinformation, finding them largely consistent with human assessments.</p>
<p>For Receptiveness, we randomly sample 50 instances from WebGPT dataset and find the accuracy to be 90%.For Helpfulness, we find the accuracy at a response-level to be 87% and 80% in the atomic-fact-level.The model generally made sound, context-aware judgments, for example, correctly dismissing helpful advice when it contradicted the question's premise (e.g., suggesting coffee when the question stated it didn't help).For Misinformation, we observed 87% response-level accuracy and 70% atomicfact level precision.Most inaccuracies were false positives, often triggered by exaggerated claims (e.g., "Your paper is now 100% more skimmable").</p>
<p>E Prompts</p>
<p>The details of the model response generation and evaluation prompts we used for each experimental setting are as follows.</p>
<p>E.1 Generation Prompts</p>
<p>E.1.1 Score-based Generation</p>
<p>The output generation prompts for the three tasks are as follows.</p>
<p>Task Description The following are the descriptions of the three tasks-summarization, helpful response generation, and documentbased QA-that are included in the prompt explaining the task to the model.These descriptions replace the {TASK_DESCRIPTION} part in each template below.</p>
<p>-Summary: A good summary is a shorter piece of text that captures the essence of the original.It aims to accomplish the same purpose and convey the same key information as the original post.</p>
<p>-Heplfulness: A helpful response is a concise and efficient answer that directly addresses the user's question or task.It should provide accurate and relevant information without unnecessary elaboration.</p>
<p>-WebGPT: A useful answer directly addresses the core question with accurate and relevant information.It should be coherent, free of errors or unsupported claims, and include helpful details while minimizing unnecessary or irrelevant content.</p>
<p>Generation Template</p>
<p>The following is the prompt for generating the model's output, rated from 1 to 5, for the given task.The outputs of the three models are referred to as 'summary', 'response', and 'response' respectively.For Tulu and Mixtral models, we customize the prompt by adding ", SCORE 2 SUMMARY:, SCORE 3 SUMMARY:, SCORE 4 SUMMARY:, SCORE 5 SUMMARY:".• Length -Measures the number of words in the output.</p>
<p>• Coherence -Ensures logical flow between reasoning steps.</p>
<p>• Fluency -Evaluates the readability and naturalness of sentences.</p>
<p>Defining Additional Factors</p>
<p>Considering the characteristics of mathematical problemsolving, additional critical factors include:</p>
<ol>
<li>
<p>Answer Correctness -Ensures the mathematical accuracy of the response.</p>
</li>
<li>
<p>Solution Robustness -Assesses logical consistency and handling of edge cases.</p>
</li>
<li>
<p>Solution Efficiency -Evaluates conciseness and avoidance of unnecessary steps.</p>
</li>
</ol>
<p>Establishing Definitions and Prompts</p>
<p>for Evaluating These New Factors The evaluation is conducted using structured prompts9 : Evaluation Criteria:</p>
<p>• Answer Correctness: Assesses whether the response is accurate and relevant.</p>
<p>• Solution Robustness:</p>
<p>-Score 1: The response is completely incoherent.-Score 2: The response contains major logical inconsistencies.</p>
<p>-Score 3: The response has some logical inconsistencies but remains understandable.</p>
<p>-Score 4: The response is logically sound but does not address all edge cases.</p>
<p>-Score 5: The response is logically flawless and considers all possible edge cases.</p>
<p>• Solution Efficiency:</p>
<p>-Score 1: The reasoning is significantly inefficient and requires complete restructuring.</p>
<p>-Score 2: The response lacks efficiency and conciseness, requiring major reorganization.</p>
<p>-Score 3: The logic needs improvement with significant edits.</p>
<p>-Score 4: The response is largely efficient but contains minor redundancies.</p>
<p>-Score 5: The response is optimally efficient with no unnecessary steps.</p>
<p>Feature Extraction Prompt:</p>
<p>We would like to request your feedback on the performance of the response of the assistant to the user instruction displayed below.In the feedback, I want you to rate the quality of the response in these 2 categories (Robustness, Efficiency) according to each score rubric: rubric Instruction: question Assistant's Response: answer Please give overall feedback on the assistant's responses.Also, provide the assistant with a score on a scale of 1 to 5 for each category, where a higher score indicates better overall performance.Only write the feedback corresponding to the score rubric for each category.The scores of each category should be orthogonal, indicating that 'Robustness of solution' should not be considered for 'Efficiency of solution' category, for example.Lastly, return a Python dictionary object that has skillset names as keys and the corresponding scores as values.Ex: {'Robustness': score, 'Efficiency': score'} 4. Extracting Factor-Level Preferences and Analyzing Metrics After evaluation, factor-level preferences are extracted and analyzed using outlined metrics to systematically assess model performance.As an example, we extract results of GPT-4o and Gemini using the outlined steps for 100 samples in the evaluation setting.The results are summarized in Table 9.In this experiment, we use the RewardMATH dataset (Kim et al., 2024).</p>
<p>Factor</p>
<p>Gemini GPT-4o  For a given initial response r init and the improved response r post , since the model is considered to have 'improved' the responses, r post is regarded as the model's 'preferred' response over r init .The factor scores are then calculated as follows:
τ 14 (f k ) = |C k | − |D k | |C k | + |D k | + |T k |(2)
where
C k = r init ,rpost∈R ⊮[M k (r post , r init ) = +1], D k = r init ,rpost∈R ⊮[M k (r post , r init ) = −1], T k = r init ,rpost∈R ⊮[M k (r post , r init ) = 0],
For the Length factor, if the model produces responses that are longer than the original responses r init , (i.e.M length (r post , r init ) = 1), this response pair is classified as concordant and vice versa.When evaluating all response pairs, a positive factor score suggests that the model significantly considers this factor when improving responses, while a negative score indicates a negative influence.A score near zero implies that the factor has minimal impact on the improvement process.The magnitude of the score reflects the degree of influence this factor exerts on the response enhancement.</p>
<p>Subsequently, we calculate Kendall's τ between the set of "factor scores of improvement" for each factor and the factor scores assigned by both human evaluators and automated evaluators, which we denote as ∆τ .This ∆τ quantifies how the model's improvements correlate with human and evaluator's factor-level preferences.</p>
<p>F.2.3 Feedback Validation</p>
<p>One of the authors examine 30 samples of GPT-4o evaluator's feedback to determine whether it correspond to our predefined factors.The analysis reveals that out of the 30 samples, the most frequently addressed factor in GPT-4o's feedback is Intent Alignment, appearing 20 times.This is followed by Source Coverage, which appeared 15 times, and Number of Facts with 12 occurrences.The Length and Off Focus factors are mentioned 10 and 9 times each.Less frequently addressed is Coherence, which appeared 6 times, and Fluency, which is mentioned 3 times.Factors other than these are not mentioned in the feedback at all.As shown in Table 10 (a), in the evaluation setting, GPT-4o exhibit correlations close to zero or negative for most factors except for Intent Alignment, Formality Alignment, Number of Facts Source Coverage, Length and Coherence.This observed trend aligns with our findings from the feedback, except for Formality Alignment, with the internal preference not explicitly expressed in the feedback.Future work should look more into the faithfulness of model-generated feedback and internal preference expressed through the overall evaluation outcome.</p>
<p>G Factor-Level Preference Alignment</p>
<p>G.1 Factor Scores</p>
<p>Table 10-12 present the full lists of factor scores for both generation (gen) and evaluation (eval) across all three tasks used in the study.</p>
<p>G.2 Factor-Level Alignment with</p>
<p>Human and Models.</p>
<p>Table 15 shows models' factor-level alignment (Kendall's τ ) with humans for helpful response generation tasks (SHP-2) and document-based QA tasks (WebGPT), and response-level agreement with humans in an evaluation setting.</p>
<p>G.3 Factor Correlations</p>
<p>Figure 6 presents the correlation matrix for the GPT-4o, Gemini-1.5, and Tulu 2.5 + PPO (13B RM) models across three tasks.The analysis focuses on the correlation between the distributions of feature scores for each feature within the samples generated by these models.</p>
<p>In summarization task, the patterns of feature correlation are generally consistent across the three models.Notably, there is a strong correlation between {length and number of facts} as well as {number of facts and source cover-age}.These results are intuitive: the more factual content an answer includes, the longer the response tends to be, which in turn increases the likelihood of covering information from the source material.</p>
<p>In helpfulness task, All three models consistently exhibit a high correlation among {length, number of facts, and helpfulness}.This is expected, as longer responses are more likely to include a greater number of facts, which often translates into more helpful content.Interestingly, in the GPT-4o model specifically, there is a noticeable correlation between "receptiveness" and the set of factors {helpfulness, number of facts, coherence, length}.As detailed in Table 11, these are precisely the factors that GPT-4o tends to prioritize in this task.This pattern suggests that the GPT-4o model frequently considers these factors during response generation, resulting in a higher prevalence of these features in its outputs.</p>
<p>In the WebGPT task, there was a high correlation among {length, number of facts, and helpfulness}, similar to the helpfulness task.For GPT-4o and Tulu 2.5 + PPO (13B RM), the correlation between novel word and hallucination was high, which can be explained by the tendency to use novel words when hallucinating something.</p>
<p>H Generalizability of Our Results</p>
<p>Our research deviates from the typical language model setup by using a 1-5 scoring system for response generation.To assess the validity of our approach, we compare responses generated through direct generation (without scoring) with those across the score range through all summary, helpfulness, and document-based QA tasks.In every task, we found that score 5 consistently aligns best with direct generation responses, based on the fine-grained factors we use, in models like GPT-4o, Tulu 2.5 + PPO (70B RM), and LLaMA 3.1 70B (see Table 16 in the Appendix H).This suggests that our scoring framework, specifically score 5, captures the essence of unconstrained language model outputs, implying the potential generalizability of our findings to general settings.</p>
<p>We conduct experiments by prompting the model to generate responses with scores ranging from 1 to 5.This setup allows us to verify whether the results can generalize to a typical scenario where the model generates responses directly.We compare the model's direct responses and the score-based responses for the summarization task on Reddit TL;DR using outputs from GPT-4o, Tulu 2.5 + PPO (70B RM), and LLaMA 3.1 70B.</p>
<p>Since the value ranges differ across features, we scale the data using min-max scaling before calculating cosine similarity.The results in Table 16 indicate that the model's direct responses are most similar to those with a score of 5, all showing a high similarity of over 0.85.Overall, as the scores decrease, the similarity also declines.</p>
<p>This finding suggests that the model's direct  responses align closely with its best-generated responses.Additionally, the lower the score, the less similarity there is to the direct responses, indicating that our score-based responses align well with the model's outputs.Thus, we demonstrate that our findings can generalize to typical settings where responses are generated directly by the model.</p>
<p>I Use of AI Assistant</p>
<p>We used ChatGPT web assistant (Chat-GPT Pro)10 and Gemini web application (2.0 Flash) 11 to refine the writing of the manuscript.</p>
<p>Figure 1 :
1
Figure 1: PROFILE uncovers that models exhibit misalignments with human preferences when generating texts.While humans prioritize different quality factors for different tasks, models show consistent bias towards longer output.</p>
<p>Figure 2 :
2
Figure 2: An overview of PROFILE pipeline: (1) Extracting overall Response-level Preference, (2)Comparing factor manifestation in a pairwise manner, (3) Quantifying Factor Influence, and (4) Comparing human and model preference at the factor-level.</p>
<p>Figure 3 :
3
Figure 3: PROFILE uncovers the factor-level preferences of humans and models.Figure illustrates the comparison of factor-level preference alignment between humans, GPT-4o, and Gemini-1.5 in generation across three tasks: (a) Summarization, (b) Instruction-following, and (c) Document QA task.The left bar graphs display factor scores (τ 14 ) for selected factors.The right tables show the rankings of all factors for each task.Notably, both models consistently rank 'length' as the top factor across tasks, while human preferences vary by task.</p>
<p>We examine three publicly available datasets of pairwise human judgments commonly used in preference optimization methods like RLHF and DPO training: Reddit TL;DR We analyze the dataset released by OpenAI(Stiennon et al., 2020), which includes human ratings of summaries across multiple axes (referred to as "axis evaluations").Higher scores indicate human preference across multiple evaluation dimensions.StanfordHumanPreference-2 (SHP-2) (Ethayarajh et al., 2022), focuses on capturing human preferences over responses to questions and instructions, prioritizing helpfulness.Higher scores indicate a more helpful response.For this study, we use responses from the "reddit/askacademia" domain.OpenAI WebGPT This dataset(Nakano et al., 2021), addresses the task of generating answers to questions from the ELI5 ("Explain Like I'm Five") subreddit.Human annotations compare two model-generated answers based on factual accuracy and overall usefulness.We exclude pairs with Tie ratings in all three datasets, as our analysis focuses on cases with clear preference distinctions.A.2 ModelsOur study focuses on the most advanced and widely-used generative models currently accessible, encompassing both proprietary and opensource options.For open-source models, we include LLaMA 3.1 70B(Dubey et al., 2024) 2 , Mixtral 8x7B Instruct v0.1 (Jiang et al., 2024), three TÜLU 2.5 Models(Ivison et al., 2024)-TÜLU 2.5 + PPO 13B (13B RM) 3 , TÜLU 2.5 + PPO 13B (70B RM) 4 , and TÜLU 2.5 + DPO 13B 5 .For proprietary models, we use Gemini 1.5 Flash(Reid et al., 2024),</p>
<p>Figure 4 :
4
Figure 4: Pearson correlation between target conditioning scores and log probabilities of generated summaries for Mistral-7b and LLaMA-3.1-70b.</p>
<p>Figure 5 :
5
Figure 5: A screenshot of a sample summary with preference annotations.</p>
<p>Summary B: Got passed up for a dream job.Now what the hell are I supposed to do with my life that doesn't include my dream job? (Human Preferred Output)</p>
<p>Table 1 :
1
The full taxonomy factors, definitions, and associated tasks (S: Summarization, I: Instruction-following, Q: DocumentQA).</p>
<p>Table 3 :
3
Kendall's τ correlation in generation and evaluation settings, and evaluation agreement rate (%) for the summarization task.</p>
<p>.,
GPT-4oLLaMA 3.1 70BTulu 2.5 + PPO (70B RM)τGτHτGτHτGτHBaselineA-0.24−0.07−0.20−0.29−0.29−0.29BaselineB-0.29−0.29−0.42−0.42−0.24−0.24GPT-4o feedback0.360.450.290.200.160.16Table 5: Factor-level alignment (τ ) between improvements made by different generators (GPT-4o, LLaMA3.1 70B, Tulu 2.5 + PPO (70B RM)) and factor-level preferences from GPT-4o (evaluation) and human.τ G and τ H indicate alignment with GPT-4o and human preferences respectively. Higher values showstronger alignment.2021; Wei et al., 2021), RLHF (Ouyang et al.,2022), DPO (Guo et al., 2024), and RLAIF,which utilizes AI-generated feedback (Bai et al.,2022; Lee et al., 2023). However, most studiesfocus on overall performance (e.g., a responseas a whole). While some work has explored us-ing fine-grained human feedback (Dong et al.,2023; Wu et al., 2024), a comprehensive un-derstanding of how granular factors contributeto and differentiate human and model prefer-
Hu et al. (2023)acking.Hu et al. (2023)address this gap by deciphering the factors influencing human preferences.We extend this work by analyzing factor-level preferences across multiple tasks and comparing the driving factors of both humans and model preferences.</p>
<p>Our research relies on established benchmarks and models, and does not involve the development of new data, methodologies, or models that pose significant risks of harm.The scope of our experiments is limited to analyzing existing resources, with a focus on model performance.Human studies conducted within this work adhere to relevant IRB exemptions, and we ensure fair treatment of all participants.Our work is mainly focused on performance evaluation, we recognize that it does not specifically address concerns such as bias or harmful content.
studies are required to thoroughly assess theefficacy and generalizability of these methods.While this study focuses on post-hoc correc-tion methods, future research should investigatehow to incorporate the identified preference fac-tors as signals during the training stage. Addi-tionally, exploring how to embed these signalswithin datasets used for preference optimiza-tion represents a promising direction for futurework.8 Ethics StatementSecond, due to budget constraints, human eval-uations of model outputs were conducted on alimited scale, with a restricted number of partic-ipants, and only on one task. Furthermore, thisstudy represents a preliminary exploration intomethods for achieving better alignment, high-lighting the potential of various techniques toenhance generation and evaluation. Extensive</p>
<p>Table 6 :
6
Factor-level preference alignment (τ ) and response-level agreement with human preferences in the summarization Task.</p>
<p>s score 1 generation: Guy told his girlfriend he wants an open relationship.She got mad and told him to go away.He wants advice.(Human Preferred Output)
Score 1Score 2Score 3Score 4Score 5The poster andA couple livingA19-year-oldA 19-year-old womanA young woman (19)their fiance havewith a roommatewoman and herand her 20-year-old fi-and her fiance (20)scabies and aregot scabies and20-year-old fianceance recently movedrecently moved into atreating it. Theirstarted treatment.contracted scabiesinto a new apart-new apartment with aroommate refusesTheroommate,after moving intoment with a mutualmutual friend and un-to get treated orwho has no symp-a new apartmentfriend and contractedfortunately contractedclean his things,toms, refuses towith a mutualscabies. Despite bothscabies. Both haverisking reinfesta-treat himself or hisfriend. Both are un-of them undergoingstarted treatment aftertion. The poster isbelongings.Thedergoing treatment,treatment, their room-a doctor's confirma-considering askingposter is worriedbut the roommatemate refuses to taketion. However, theirhim to move out.about reinfestationrefusestogetany preventive mea-20-year-old roommate,and may ask thetreated or clean hissures or treat himself,who has not shownroommate to movebelongings.Theeven though he hassymptoms, refuses toout since he isn'tposter is concernednot shown symptomsundergo treatment oron the lease.about reinfestationyet. The poster isclean his clothes andand is contem-worried that this willlinen. This refusal hasplating asking thelead to a reinfestationcaused concern for theroommate to moveand is considering ask-woman, as she fearsout, as he is not oning the roommate toreinfestation and isthe lease.move out since he isseriously consideringnot on the lease.asking the roommateto move out, especiallysince he is not on thelease.</p>
<p>Table 7 :
7
An example of GPT-4-generated summaries of scores 1-5.</p>
<p>will do it for $110 and I'm still waiting on a quote from <a href="So my question is, have any of you used this company, or know anyone who has used it?Should I trust these companies?Do you have any recommendations?What should I do to get my phone fixed?
Summary A: Dropped my phone, theysaid they won't repair phones that havebeen physically abused. Looking for sug-gestions on cell phone repair companies, ifany, and what I should do to get my phone fixed.Summary B: I dropped my phone,cracking the screen. I can't afford to buy afull price phone, so should I try the aboverepair companies? What should I do? (HumanPreferred Outpu">CPR</a>)</p>
<p>Table 8 shows the average cost for each factor in a single sample Your task is to extract atomic facts from the INPUT.These are self-contained units of information that are unambiguous and require no further splitting.
FactorInputOutputSumAtomic facts$0.00146 $0.00057 $0.00203Hallucination$0.00165 $0.00203 $0.00368Off-focus$0.00332 $0.00236 $0.00568Intent-alignment$0.00461 $0.00071 $0.00532Formality-alignment $0.00076 $0.00057 $0.00133Total$0.01180 $0.00624 $0.01804Table 8: Average cost per LLM-based factor in asingle summarization sample.of the summarization task, with the total costbeing $0.018 per post sample.D.3 Prompt Template For LLM-basedFactor ExtractionD.3.1 Template for Atomic FactGenerationNumber Of Fact{FEW SHOT}INPUT: inputOUTPUT:</p>
<p>Table 9 :
9
Due to the relative nature of preference, we cannot directly assess the alignment of the improved response itself.Instead, we measure the degree of the improvement resulting from the evaluator's feedback to evaluate how well the occurred improvement aligns with both human and evaluator preferences.For each factor f k and pairwise factor comparison function M k , we calculate the factor score of improvement with τ 14 .
E.3 Evaluation PromptsE.3.1 Comparison-Based EvaluationEvaluation Template We provide themodel with two responses using the evaluationprompt below and ask it to assess which outputis better. Depending on the task, we also pro-vide relevant sources (e.g., post, question, andreference) along with the responses generatedby the model to help it choose the preferredresponse.{TASK_DESCRIPTION}### Summarization &amp; Helpful Response Generation###Analyze the provided [summaries/responses] and originalpost, then select the better [summary/response] orindicate if they are equally good. Output the result inJSON format. Where "better [summary/response]" canbe "[Summary/Response] 1", "[Summary/Response] 2",or "Tie" if both [summaries/responses] are equally good.Output Format:{{"better summary": "","justification": ""}}Reddit Post: {CONTENT}[Summary/Response] 1: {RESPONSE1}[Summary/Response] 2: {RESPONSE2}### document-based QA ###Where "better answer" can be "Answer 1", "Answer 2", or"Tie" if both responses are equally good.Question: {QUESTION}Answer 1: {ANSWER1}Reference 1: {REFERENCE1}Answer 2: {ANSWER2}Reference 2: {REFERENCE2}Output the result in JSON format.Output Format:{{"better answer": "","justification": ""}}F Achieving Better AlignmentThrough ProfileF.1 Improving Alignment inEvaluation through Factor-levelGuidance.This section explains the specific experimentalsettings for the Improving Alignment in Evalu-</p>
<p>Table 10 :
10
Full lists of factor scores in generation (gen) and evaluation (eval) in Summarization task.Sorted based on the human factor score.</p>
<p>Table 11 :
11
Full lists of factor scores in generation (gen) and evaluation (eval) in SHP2 dataset.Sorted based on the human factor score.</p>
<p>Inference for LLaMA was conducted using the Together AI API. https://www.together.ai/
  3  We use huggingface allenai/tulu-v2.5-ppo-13b-ufmean-13b-uf-rm
model.4  We use huggingface allenai/tulu-v2.5-ppo-13b-ufmean-70b-uf-
rm model.5  We use huggingface allenai/tulu-v2.5-dpo-13b-ufmean model.
We use gpt-4o-2024-05-13 version for all GPT-4o inference.
We use gpt-3.5-turbo-1106 version for all GPT-3.5 inference.
https://huggingface.co/autotrain
We refer to the(Ye et al., 2024) for the criteria and prompt.
https://chatgpt.com/
AcknowledgmentsThis work was supported by Institute for Information &amp; communications Technology Promotion(IITP) grant funded by the Korea government (MSIP) (No.RS-2024-00443251, Accurate and Safe Multimodal, Multilingual Personalized AI Tutors)Factor Specific GuidanceOff Focus: Note that the summary should capture the main focus of the post, which is the core subject around which all the content revolves.Hallucination: Note that the summary should contain factual information that accurately matches the post.Coherence: Note that whether all the sentences form a coherent body or not is not the primary factor in determining the quality of a summary.Fluent: Note that the summary should be fluent.Intent Alignment: Focus on how well the summary represents the main intents of the original post.F.2 Leveraging Evaluation for BetterAlignment in Generation.F.2.1 Prompts for ImprovementThe prompts we used to enhance the model's output are as follows.We focuses on the Summary task for the experiment.Task Description For Summary task, the description is the same as the one used in the score-based generation prompt.Summary: A good summary is a shorter piece of text that captures the essence of the original.The three prompts used for improvement are as follows.Improvement Template
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Makesh Narsimhan Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. Yi Dong, Zhilin Wang, arXiv:2310.05344Steerlm: Attribute conditioned sft as an (usersteerable) alternative to rlhf. 2023arXiv preprint</p>
<p>Understanding dataset difficulty with V-usable information. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2024. 2022162arXiv preprintKawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta</p>
<p>Summeval: Reevaluating summarization evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mccann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 20219</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Improving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nat Mcaleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, arXiv:2402.04792Direct language model alignment from online ai feedback. 2024arXiv preprint</p>
<p>DecipherPref: Analyzing influential factors in human preference judgments via GPT-4. Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Fei Liu, 10.18653/v1/2023.emnlp-main.519Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A Smith, Yejin Choi, Hannaneh Hajishirzi, arXiv:2406.09279Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. 2024arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Evaluating robustness of reward models for mathematical reasoning. Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, Jinyoung Yeo, arXiv:2410.017292024Preprint</p>
<p>Adina Williams, He He, et al. 2024. The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Rose Hannah, Alexander Kirk, Paul Whitefield, Andrew Röttger, Katerina Bean, Juan Margatina, Rafael Ciro, Max Mosquera, Bartolo, arXiv:2404.16019arXiv preprint</p>
<p>LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, Kyle Lo, 10.18653/v1/2023.eacl-main.121Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi, International Conference on Machine Learning. 2023</p>
<p>G-eval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Results of the WMT14 metrics shared task. Matouš Macháček, Ondřej Bojar, 10.3115/v1/W14-3336Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2305.142512023arXiv preprint</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ; Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2104.08773arXivCross-task generalization via natural language crowdsourcing instructions. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2021. 2021arXiv preprint</p>
<p>The generative AI paradox in evaluation: "what it can solve, it may not evaluate. Juhyun Oh, Eunsu Kim, Inha Cha, Alice Oh, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research WorkshopSt. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Hello, gpt-4 turbo. 2024OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024arXiv preprint</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, 10.18653/v1/2020.acl-main.442Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Training language models with language feedback at scale. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2303.167552023arXiv preprint</p>
<p>Preference ranking optimization for human alignment. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>The generative ai paradox: "what it can create, it may not understand. Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Fine-grained human feedback gives better rewards for language model training. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi, Advances in Neural Information Processing Systems. 202436</p>
<p>INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li, 10.18653/v1/2023.emnlp-main.365Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Flask: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Towards a unified multidimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, 10.18653/v1/2022.emnlp-main.131Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7696 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7696</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7696</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-142.html">extraction-schema-142</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-110f5dc6d5bfe67138d64c261d6851c727021d1f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/110f5dc6d5bfe67138d64c261d6851c727021d1f" target="_blank">Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An experimental design is established that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent, finding LLM-generated ideas are judged as more novel than human expert ideas while being judged slightly weaker on feasibility.</p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p<0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7696.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7696.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-grounded ideation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation for Paper-Grounded Idea Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of an LLM (Claude-3.5-Sonnet) with a Semantic Scholar retrieval loop to fetch relevant papers (titles/abstracts) and prepend them as context to prompts so the model generates research ideas grounded in the scholarly literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3-5-sonnet-20240620</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Claude family LLM used as the backbone for multi-step prompting: generating API function calls, scoring/reranking retrieved documents, and producing idea text; used zero-shot and with demonstrations for generation and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>Natural Language Processing / Prompting research</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>120</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with demonstration examples</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt the LLM to generate Semantic Scholar API calls to retrieve papers (up to N=120 per topic), score and rerank retrieved papers for relevance/empirical value/inspirational potential, then prepend selected titles+abstracts (k=10) and demonstration idea examples to the idea-generation prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert blind review of generated ideas (novelty, excitement, feasibility, effectiveness, overall score); additional automatic deduplication and LLM-based ranking validation on ICLR scraped data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td>Not applicable to law extraction; RAG-grounded generation produced AI ideas judged more novel than human ideas in expert review (statistically significant).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The pipeline is for ideation rather than distilling formal laws; retrieved-paper context was used to inspire ideas but no systematic rule/law induction was performed. Authors also report limited diversity in generated ideas despite heavy overgeneration, and LLM-based ranking/evaluation is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Authors propose extending the evaluation to execution (recruiting researchers to implement ideas) and to compare AI ideas with accepted papers; methods to improve idea diversity and robust auto-evaluation (beyond current LLM rankers) are suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7696.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7696.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM review-pattern extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based extraction and clustering of review main points</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of Claude-3.5 to extract and cluster the main points from free-text expert reviews to summarize qualitative failure modes and strengths in AI- and human-generated ideas; clusters were then manually verified and labeled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude-3.5 used as an instruction-following LLM to perform extraction and clustering of free-text review rationales, followed by manual verification by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>Peer review text (expert reviews of research ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>298</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Instruction-prompted extraction + clustering</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt the LLM to extract salient points from each review and cluster them into thematic groups; authors manually verified and labeled clusters afterwards.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td>pattern extraction / thematic clustering (text-mining)</td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual verification and labeling of clusters by paper authors; qualitative corroboration with numerical review metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td>Clusters highlighted common AI failure modes (vagueness, dataset misuse, missing baselines, unrealistic assumptions, resource demands) and human strengths (grounding in prior work, feasibility focus); no formal laws distilled.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extraction performed on reviews (not scholarly papers) and relied on manual verification; LLM may introduce summarization biases and rephrasings; procedure is descriptive rather than producing formal qualitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Potential to apply similar extraction/clustering to larger corpora (papers) to surface recurring patterns, but authors caution about LLM brittleness as an evaluator and recommend human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7696.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7696.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM pairwise paper ranker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Pairwise Ranker for Estimating Paper Quality (Zero-shot Claude-3.5-Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of an LLM to perform pairwise comparisons on converted project proposals (based on scraped ICLR submissions) to predict which paper would be accepted; used as an automatic ranker for generated ideas via a Swiss-system tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot prompting of Claude-3.5-Sonnet to compare two proposals and predict which is better/accepted; applied repeatedly to produce tournament scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>Machine learning / NLP conference submissions</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>1200</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot pairwise comparison with Swiss-system aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Convert prior conference submissions into the proposal format, pair accepted and rejected papers, use the LLM to predict winners in pairwise comparisons, then run N=5 rounds of Swiss-style pairings to produce aggregate scores for ranking new proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Validation accuracy on held ICLR LLM-related submissions (Claude-3.5 achieved ~71.4% accuracy distinguishing accepted vs rejected in pairwise setting).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td>LLM pairwise ranker showed non-trivial accuracy on ICLR corpus and was used to select top AI ideas, but authors note discrepancies with human reranking (only 17/49 overlap) and consider the ranker imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pairwise judgement approach can be miscalibrated; automatic ranker disagreed substantially with human expert reranking and with downstream human review scores. Authors caution against relying solely on LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Improve calibration and robustness of automatic rankers, combine human-in-the-loop reranking, and develop better evaluation proxies for idea quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7696.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7696.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related-work mentions: pattern discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mentions of LLMs for discovering patterns in large text corpora</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references prior work that uses LLMs to discover patterns in large text corpora (cited as Lam et al., 2024 and Zhong et al., 2023) as motivating examples of LLMs applied to knowledge discovery in textual scientific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>Text corpora / scientific literature / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Citation-level mention that LLMs have been used to discover patterns in large text corpora; the paper does not describe methods or present concrete distilled laws from those works.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>This is only a literature mention; no details or replication are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Authors situate their work among literature that applies LLMs to discovery and pattern extraction and suggest extending ideation/execution evaluations to broader scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7696",
    "paper_id": "paper-110f5dc6d5bfe67138d64c261d6851c727021d1f",
    "extraction_schema_id": "extraction-schema-142",
    "extracted_data": [
        {
            "name_short": "RAG-grounded ideation",
            "name_full": "Retrieval-Augmented Generation for Paper-Grounded Idea Generation",
            "brief_description": "Use of an LLM (Claude-3.5-Sonnet) with a Semantic Scholar retrieval loop to fetch relevant papers (titles/abstracts) and prepend them as context to prompts so the model generates research ideas grounded in the scholarly literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "claude-3-5-sonnet-20240620",
            "model_description": "Proprietary Claude family LLM used as the backbone for multi-step prompting: generating API function calls, scoring/reranking retrieved documents, and producing idea text; used zero-shot and with demonstrations for generation and scoring.",
            "model_size": null,
            "corpus_domain": "Natural Language Processing / Prompting research",
            "corpus_size": 120,
            "method_name": "Retrieval-Augmented Generation (RAG) with demonstration examples",
            "method_description": "Prompt the LLM to generate Semantic Scholar API calls to retrieve papers (up to N=120 per topic), score and rerank retrieved papers for relevance/empirical value/inspirational potential, then prepend selected titles+abstracts (k=10) and demonstration idea examples to the idea-generation prompt.",
            "law_extraction_approach": null,
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": "Human expert blind review of generated ideas (novelty, excitement, feasibility, effectiveness, overall score); additional automatic deduplication and LLM-based ranking validation on ICLR scraped data.",
            "evaluation_result": "Not applicable to law extraction; RAG-grounded generation produced AI ideas judged more novel than human ideas in expert review (statistically significant).",
            "baseline_comparison": false,
            "limitations": "The pipeline is for ideation rather than distilling formal laws; retrieved-paper context was used to inspire ideas but no systematic rule/law induction was performed. Authors also report limited diversity in generated ideas despite heavy overgeneration, and LLM-based ranking/evaluation is imperfect.",
            "future_directions": "Authors propose extending the evaluation to execution (recruiting researchers to implement ideas) and to compare AI ideas with accepted papers; methods to improve idea diversity and robust auto-evaluation (beyond current LLM rankers) are suggested.",
            "uuid": "e7696.0",
            "source_info": {
                "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM review-pattern extraction",
            "name_full": "LLM-based extraction and clustering of review main points",
            "brief_description": "Application of Claude-3.5 to extract and cluster the main points from free-text expert reviews to summarize qualitative failure modes and strengths in AI- and human-generated ideas; clusters were then manually verified and labeled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "claude-3.5",
            "model_description": "Claude-3.5 used as an instruction-following LLM to perform extraction and clustering of free-text review rationales, followed by manual verification by authors.",
            "model_size": null,
            "corpus_domain": "Peer review text (expert reviews of research ideas)",
            "corpus_size": 298,
            "method_name": "Instruction-prompted extraction + clustering",
            "method_description": "Prompt the LLM to extract salient points from each review and cluster them into thematic groups; authors manually verified and labeled clusters afterwards.",
            "law_extraction_approach": "pattern extraction / thematic clustering (text-mining)",
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": "Manual verification and labeling of clusters by paper authors; qualitative corroboration with numerical review metrics.",
            "evaluation_result": "Clusters highlighted common AI failure modes (vagueness, dataset misuse, missing baselines, unrealistic assumptions, resource demands) and human strengths (grounding in prior work, feasibility focus); no formal laws distilled.",
            "baseline_comparison": false,
            "limitations": "Extraction performed on reviews (not scholarly papers) and relied on manual verification; LLM may introduce summarization biases and rephrasings; procedure is descriptive rather than producing formal qualitative laws.",
            "future_directions": "Potential to apply similar extraction/clustering to larger corpora (papers) to surface recurring patterns, but authors caution about LLM brittleness as an evaluator and recommend human verification.",
            "uuid": "e7696.1",
            "source_info": {
                "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM pairwise paper ranker",
            "name_full": "LLM Pairwise Ranker for Estimating Paper Quality (Zero-shot Claude-3.5-Sonnet)",
            "brief_description": "Use of an LLM to perform pairwise comparisons on converted project proposals (based on scraped ICLR submissions) to predict which paper would be accepted; used as an automatic ranker for generated ideas via a Swiss-system tournament.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "claude-3.5-sonnet",
            "model_description": "Zero-shot prompting of Claude-3.5-Sonnet to compare two proposals and predict which is better/accepted; applied repeatedly to produce tournament scores.",
            "model_size": null,
            "corpus_domain": "Machine learning / NLP conference submissions",
            "corpus_size": 1200,
            "method_name": "Zero-shot pairwise comparison with Swiss-system aggregation",
            "method_description": "Convert prior conference submissions into the proposal format, pair accepted and rejected papers, use the LLM to predict winners in pairwise comparisons, then run N=5 rounds of Swiss-style pairings to produce aggregate scores for ranking new proposals.",
            "law_extraction_approach": null,
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": "Validation accuracy on held ICLR LLM-related submissions (Claude-3.5 achieved ~71.4% accuracy distinguishing accepted vs rejected in pairwise setting).",
            "evaluation_result": "LLM pairwise ranker showed non-trivial accuracy on ICLR corpus and was used to select top AI ideas, but authors note discrepancies with human reranking (only 17/49 overlap) and consider the ranker imperfect.",
            "baseline_comparison": true,
            "limitations": "Pairwise judgement approach can be miscalibrated; automatic ranker disagreed substantially with human expert reranking and with downstream human review scores. Authors caution against relying solely on LLM evaluators.",
            "future_directions": "Improve calibration and robustness of automatic rankers, combine human-in-the-loop reranking, and develop better evaluation proxies for idea quality.",
            "uuid": "e7696.2",
            "source_info": {
                "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Related-work mentions: pattern discovery",
            "name_full": "Mentions of LLMs for discovering patterns in large text corpora",
            "brief_description": "The paper references prior work that uses LLMs to discover patterns in large text corpora (cited as Lam et al., 2024 and Zhong et al., 2023) as motivating examples of LLMs applied to knowledge discovery in textual scientific corpora.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "corpus_domain": "Text corpora / scientific literature / NLP",
            "corpus_size": null,
            "method_name": null,
            "method_description": "Citation-level mention that LLMs have been used to discover patterns in large text corpora; the paper does not describe methods or present concrete distilled laws from those works.",
            "law_extraction_approach": null,
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": null,
            "evaluation_result": null,
            "baseline_comparison": false,
            "limitations": "This is only a literature mention; no details or replication are provided in this paper.",
            "future_directions": "Authors situate their work among literature that applies LLMs to discovery and pattern extraction and suggest extending ideation/execution evaluations to broader scientific domains.",
            "uuid": "e7696.3",
            "source_info": {
                "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.012506249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can LLMs Generate Novel Research Ideas? <br> A Large-Scale Human Study with 100+ NLP Researchers</h1>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto<br>Stanford University<br>{clsi, diyiy, thashim}@stanford.edu</p>
<h4>Abstract</h4>
<p>Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel $(p&lt;0.05)$ than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The rapid improvement of LLMs, especially in capabilities like knowledge and reasoning, has enabled many new applications in scientific tasks, such as solving challenging mathematical problems (Trinh et al., 2024), assisting scientists in writing proofs (Collins et al., 2024), retrieving related works (Ajith et al., 2024, Press et al., 2024), generating code to solve analytical or computational tasks (Huang et al., 2024, Tian et al., 2024), and discovering patterns in large text corpora (Lam et al., 2024, Zhong et al., 2023). While these are useful applications that can potentially increase the productivity of researchers, it remains an open question whether LLMs can take on the more creative and challenging parts of the research process.
We focus on this problem of measuring the research ideation capabilities of LLMs and ask: are current LLMs capable of generating novel ideas that are comparable to expert humans? Although ideation is only one part of the research process, this is a key question to answer, as it is the very first step to the scientific research process and serves as a litmus test for the possibility of autonomous research agents that create their own ideas. Evaluating expert-level capabilities of LLM systems is challenging (Bakhtin</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our study: we recruit 79 expert researchers to perform blind review of 49 ideas from each of the three conditions: expert-written ideas, AI-generated ideas, and AI-generated ideas reranked by a human expert. We standardize the format and style of ideas from all conditions before the blind review. We find AI ideas are judged as significantly more novel than human ideas ( $p&lt;0.05$ ).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of the three experiment conditions across all review metrics. Red asterisks indicate that the condition is statistically better than the Human baseline with two-tailed Welch's t-tests and Bonferroni correction. All scores are on a 1 to 10 scale. More detailed results are in Section 5.
et al., 2022, Collins et al., 2024), and research ideation takes this to an extreme. Qualified expert researchers are difficult to recruit at scale, evaluation criteria can be highly subjective, and it is difficult for even the best experts to judge the quality of an idea (Beygelzimer et al., 2021, Simsek et al., 2024). We address these challenges directly, recognizing that for important, high-stakes tasks like research ideation, there is no substitute for a large-scale expert evaluation. We design a carefully controlled comparison of human and LLM ideas that overcomes sample size and baseline problems present in earlier small-scale evaluation studies. Our study recruited a large pool of over 100 highly qualified NLP researchers to produce human baseline ideas and perform blind reviews of human and LLM ideas. To reduce the possibility that confounding variables affect our outcome measures, we enforce strict controls that standardize the styles of human and LLM ideas and match their topic distribution. We compare our human expert baseline with a simple and effective LLM agent that incorporates retrieval augmentation and adopts recent ideas in inference-time scaling, such as overgenerating and reranking LM outputs. These measures allow us to make statistically rigorous comparisons between human experts and state-of-the-art LLMs (Figure 1).</p>
<p>Our evaluation-centric approach complements many recent methods-centric works that attempt to instantiate research agents (Baek et al., 2024, Li et al., 2024, Lu et al., 2024, Wang et al., 2024, Yang et al., 2024). The majority of these works rely on fast and lower-cost evaluation surrogates - either by decreasing the number of expert reviewers (Baek et al., 2024, Li et al., 2024, Wang et al., 2024, Yang et al., 2024), constraining the length and detailedness of the ideas (Wang et al., 2024, Yang et al., 2024), or relying on LLM-as-a-judge (Lu et al., 2024). They do not perform the large-scale human comparison studies that are needed to answer the motivating question of our work. Our work takes the opposite approach, performing a year-long and high-cost evaluation that provides human expert baselines and a standardized evaluation protocol to serve as a foundation for future follow-up studies and methods work. Through nearly 300 reviews across all our conditions, we find that AI-generated ideas are judged as more novel than human expert ideas ( $p&lt;0.05$ ), which holds robustly under multiple hypothesis correction and across different statistical tests. We find some signs that these gains are correlated with excitement and overall score, and may come at the slight expense of feasibility, but our study size did not have sufficient power to conclusively identify these effects (Figure 2).
Qualitative analysis of free-text responses in our review corroborates these findings on novelty and feasibility. Apart from evaluating the ideas, we also analyze the LLM agent, showing limitations and open problems - despite excitement about inference-time scaling of LLMs, we find that they lack idea diversity when we scale up idea generation, and they cannot currently serve as reliable evaluators.</p>
<h1>2 Problem Setup</h1>
<p>The central experiment of our work is a comparison of human- and LLM-generated ideas. While this goal is simple, there is no existing consensus on how to formulate the task of research ideation and evaluation, and we begin by defining the key aspects of our experiment design.
We think of research idea evaluation as consisting of three separate components: 1). the idea itself, generated in response to our instructions, 2). the writeup which communicates the idea, and 3). the evaluation of the writeup by experts. We outline our experiment design in each of these three parts with particular focus on potential confounders, such as the area of research, the format of a research idea, and the evaluation process.</p>
<p>Ideation Scope and Instructions Research ideas can take many different forms. They can be simple tricks to improve model performance, or they may be large-scale research programs that form the basis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and interestingness of a research idea with the practical realities of eliciting ideas from a large population. In our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments so that the resulting ideas can be executed by experts in a follow-up set of experiments.
These constraints have led us to study prompting-based NLP research as a testbed for our study. Prompting research has been popular in recent years of NLP and AI research (e.g., Chen et al., 2023, Diao et al., 2024, Madaan et al., 2023, Qin et al., 2024, Schulhoff et al., 2024, Si et al., 2023, Wang et al., 2023, Wei et al., 2022, Yao et al., 2023, Yasunaga et al., 2024, Zhou et al., 2023, inter alia). This class of projects strikes a reasonable trade-off among our constraints. The most impactful prompting projects like chain-of-thought have had a major influence on LLM performance (Wei et al., 2022), and prompting projects are executable with minimal computing hardware.
We further structure our ideation process to avoid selection-bias-based confounders in ideation. If we simply ask LLMs and humans to produce ideas on 'prompting topics', we may find that LLMs and humans differ in the types of research ideas they produce (for example, LLMs may naturally suggest more projects on safer topics, which might be judged as less exciting by humans). This would</p>
<p>lead us to simply measure misalignment in research topic preference between LLMs and humans, which is not the goal of our study. To address this possibility, we define a set of seven specific research topics extracted from the Call For Papers page of recent NLP conferences such as COLM. ${ }^{2}$ Specifically, our topics include: Bias, Coding, Safety, Multilinguality, Factuality, Math, and Uncertainty (see Appendix A for a complete description of these topics).
Each human and LLM participant of the ideation experiment receives the same set of natural language instructions including the same topic description, idea template, and demonstration example to ensure a fair comparison. For human participants, we additionally allow them to select a preferred topic from the list, and for each selected topic, we generate a corresponding LLM idea. This exactly matches the idea topic distribution between the LLM and human participants, while ensuring that human experts are able to select topics according to their expertise.</p>
<p>Idea Writeup An idea can only be evaluated if it is written up to be communicated, but this writing process introduces many additional potential confounders. Human researchers may write in ways that subtly signal quality research, such as including more examples and implementation details. The format of the writeup functions as a way to scaffold what contents should be included and the level of detailedness. Ideally, we want both human and LLM participants to provide all the necessary implementation details for their generated ideas.
We take inspiration from guidelines used in grant submissions and introduce a template to specify the structure and detailedness of idea proposals. Specifically, we construct a template that includes fields for the title, problem statement, motivation, proposed method, step-by-step experiment plan, test case examples, and the fallback plan. Both the LLM agent and the human idea writers are instructed to follow this template and our provided demonstration examples to produce a project proposal as the output (see Appendix B for the full template and Appendix C for the demo example).
Even with these templates, there may be subtle writing style cues that affect the outcome measure. For example, humans may tend to write in a more engaging and informal tone. To reduce this possibility further, we developed a style normalization module that uses an LLM to convert all ideas into the same writing and formatting style without changing the original content. Our small-scale human study shows that such a normalization approach leads to a $50 \%$ accuracy for expert human judges who are asked to distinguish AI ideas from human ideas. Finally, the use of an LLM style anonymizer has the possibility of substantively changing the content of the ideas. To rule this out, the first author of this paper manually verified each human idea proposal to ensure all contents of the original ideas were preserved. We present the full prompt used in Appendix D.</p>
<p>Review and Evaluation Reviewing research ideas is notoriously subjective, so we want to design a review form that defines all review criteria clearly to standardize and anchor the evaluations as much as possible. At the same time, we want our review criteria and measured variables to capture all the desiderata of high-quality research ideas.
We follow best practices from AI conference reviewing (e.g., ICLR and ACL) when designing the review form, where we define four breakdown metrics including novelty, excitement, feasibility, and expected effectiveness, apart from the overall score. For each metric, we ask for a numerical score on a 1-10 scale along with a free-text rationale. We provide clear definitions and grounding for each numerical scale to calibrate all reviewers' standards (see Appendix E for the full review form).
Our blind review evaluation will compare ideas from three different conditions:</p>
<ol>
<li>
<p>Human Ideas: Idea proposals written by our recruited expert researchers.
<sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>AI Ideas: Idea proposals generated by our LLM agent. We directly take the top-ranked ideas from the agent's output.</p>
</li>
<li>AI Ideas + Human Rerank: Idea proposals generated by our LLM agent. The first author of this paper manually selected the top-ranked ideas out of all the LLM agent's generations rather than relying on the LLM ranker in order to better estimate the upper-bound quality of AI ideas.</li>
</ol>
<p>In the next two sections, we instantiate how our LLM agent generates ideas and how our expert participants generate and review the ideas.</p>
<h1>3 Idea Generation Agent</h1>
<p>We build a simple but effective LLM ideation agent to compare with the human expert baseline. Rather than focusing on innovating the agent itself, we adhere to a minimalist design principle, aiming to understand the current capabilities of LLMs in idea generation. Our research ideation agent has three essential components: paper retrieval, idea generation, and idea ranking, which we will describe in detail below.</p>
<h3>3.1 Paper Retrieval for RAG</h3>
<p>To ground idea generation, the agent needs to retrieve papers related to the given research topic, so that it will be aware of related works when generating new ideas. To do so, we leverage retrieval-augmented generation (RAG), which has demonstrated effectiveness on many knowledge-intensive tasks (Lewis et al., 2020, Shi et al., 2024). Concretely, given a research topic (e.g., "novel prompting methods that can improve factuality and reduce hallucination of large language models"), we prompt an LLM to generate a sequence of function calls to the Semantic Scholar API. We use claude-3-5-sonnet-20240620 as the backbone model for our agent but the pipeline should generalize to other LLMs as well. The paper retrieval action space includes: {KeywordQuery (keywords), PaperQuery (paperId), GetReferences (paperId)}. Each action generation is grounded on the previous actions and executed results. We keep the top $k=20$ papers from each executed function call and stop the action generation when a max of $N=120$ papers have been retrieved. We then use the LLM to score and rerank all retrieved papers based on three criteria: 1) the paper should be directly relevant to the specified topic; 2) the paper should be an empirical paper involving computational experiments; ${ }^{3}$ 3) the paper is interesting and can inspire new projects. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these criteria and we use the top-ranked papers for the next step of idea generation.</p>
<h3>3.2 Idea Generation</h3>
<p>Our key insight for idea generation is to generate as many candidate ideas as possible. Our intuition is that only a small fraction of all generated ideas might be high-quality, and we should be willing to expend inference-time compute to generate more candidates so that we can later use a reranker to discover the "diamond in the rough". This aligns with existing results showing that scaling inference compute with repeated sampling can boost LLM performance on various coding and reasoning tasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed ideas on each research topic. The idea generation prompt includes the demonstration examples and the retrieved papers. We craft $k=6$ demonstration examples by manually summarizing exemplar</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>papers (Dhuliawala et al., 2023, Madaan et al., 2023, Weller et al., 2023, Weston and Sukhbaatar, 2023, Yasunaga et al., 2024, Zheng et al., 2024) into our desired idea format. For retrieval augmentation, we randomly select $k=10$ papers from the top-ranked retrieved papers and concatenate their titles and abstracts to prepend to the idea generation prompt. We also append the titles of all previously generated ideas to the prompt to explicitly ask the LLM to avoid repetitions.
To remove duplicated ideas from this large pool of candidate ideas, we first perform a round of deduplication by encoding all seed ideas with all-MiniLM-L6-v2 from Sentence-Transformers (Reimers and Gurevych, 2020) and then computing pairwise cosine similarities. We set a similarity threshold of 0.8 for the idea deduplication based on manual inspection. ${ }^{4}$ This leaves about $5 \%$ non-duplicated ideas out of all the generated seed ideas. We expand more on this duplication issue later in Section 7.1.</p>
<h1>3.3 Idea Ranking</h1>
<p>The next step is for our ideation agent to rank all the remaining ideas so that we can find the best ones among them. To build such an automatic idea ranker, we use public review data as a proxy. Specifically, we scraped 1200 ICLR 2024 submissions related to LLMs (with keyword filtering) along with their review scores and acceptance decisions. We explored multiple ways of predicting the scores and decisions of these submissions and found that LLMs are poorly calibrated when asked directly to predict the final scores or decisions, but can achieve non-trivial accuracy when asked to judge which paper is better in pairwise comparisons.
We converted the ICLR submissions into our standard project proposal format and randomly paired up accepted and rejected papers and asked LLMs to predict which one is accepted. On this task, Claude-3.5-Sonnet achieves an accuracy of $71.4 \%$ with zero-shot prompting. For comparison, GPT-40 achieves $61.1 \%$ and Claude-3-Opus achieves $63.5 \%$, and we do not observe significant gains from additional prompting techniques like few-shot or chain-of-thought prompting. We therefore choose the Claude-3.5-Sonnet zero-shot ranker.
In order to obtain reliable scores for all project proposals based on pairwise comparisons, we adopt a Swiss system tournament where all project proposals are paired with those whose accumulated scores are similar, and if the proposals are judged to be better, they gain an additional point. We repeat this for $N$ rounds so the total score of each project proposal will be within the $[0, N]$ range. As a sanity check, we use the Claude-3.5-Sonnet ranker to rank the 1.2 K ICLR LLM-related submissions and compare the average review scores of the top 10 ranked papers and the bottom 10 ranked papers in Table 1. We see a clear separation between the top and bottom ranked papers, indicating the effectiveness of the LLM ranker. We choose $N=5$ for all our experiments since it gives the best ranking result on this validation set. The top-ranked project proposals from the agent will be directly used for the AI Ideas condition of the human study.
Since our AI ranker is still far from perfect, we also introduce another experiment condition where the first author of this paper manually reranked the generated project proposals instead of relying on the LLM ranker, and we call this the AI Ideas + Human Rerank condition. As we show in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 12, 17 out of the 49 ideas in the AI Ideas + Human Rerank condition overlap with the AI Ideas condition, while the other 32 are different, indicating the discrepancy between the LLM ranker and the human expert reranking.</p>
<h1>4 Expert Idea Writing and Reviewing</h1>
<p>In this section, we shift focus to the human branch of idea generation comparison. We present the details of our human study, including information about the recruited experts, the human idea generation task, and the subsequent review process.</p>
<h3>4.1 Expert Recruitment</h3>
<p>We recruit our expert participants (including for idea writing and reviewing) by sending sign-up forms to several channels, including: 1) the OpenNLP Slack channel with 1426 NLP researchers from 71 institutions (with consent from the channel manager); 2) Twitter (X); 3) Slack channels of various NLP groups by direct communication with the group members; and 4) official chat app of the NAACL 2024 conference. We also conducted in-person recruitment by giving out name cards and wearing T-shirts ${ }^{5}$ with sign-up links at the NAACL 2024 conference as well as various other local NLP social events. Our study has been approved by the Stanford IRB (ID 74246).
We performed screening on all the US participants ${ }^{6}$ based on their provided Google Scholar profiles. We set a minimum requirement of having published at least one paper at a major AI venue. ${ }^{7}$ We reached out to all participants who satisfied this requirement with the consent form and followed up with the annotation documents for those who consented to participate.
In the end, we recruited $N=49$ experts for writing ideas, and $N=79$ experts for reviewing ideas. Note that 24 out of the 79 reviewers also participated in the idea writing, and we made sure no reviewer would review their own idea. This results in $N=104$ total participants across the two tasks. Each idea writer is asked to write one idea within 10 days and we compensate $\$ 300$ for each, with a $\$ 1000$ bonus for the top 5 ideas as scored by the expert reviewers. Each idea reviewer is assigned 2 to 7 ideas to review and we collected $N=298$ unique reviews in total. They are given one week to finish the reviews and we compensated $\$ 25$ for each review written by the idea reviewers.</p>
<h3>4.2 Expert Qualifications</h3>
<p>Our pool of participants is highly qualified and diverse. The 49 idea writers come from 26 different institutions (Table 15) and the majority of them are current PhD students (Figure 3 left). The 79 reviewers come from 32 institutions (Table 16) and are mostly PhD students and Postdocs (Figure 3 right). We use their Google Scholar profiles to extract several proxy metrics, including the number of papers, citations, h-index, and i10-index at the time of their submission. Table 2 shows that our idea writers have an average of 12 papers and 477 citations, while every reviewer has published at least two papers and has an average citation of 635 and h-index of 7 . Moreover, based on their survey
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Positions of our idea writer (left) and reviewer (right) participants.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Idea Writing Participants (N=49)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Idea Reviewing Participants (N=79)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">SD</td>
</tr>
<tr>
<td style="text-align: left;">papers</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">citations</td>
<td style="text-align: center;">477</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4553</td>
<td style="text-align: center;">861</td>
<td style="text-align: center;">635</td>
<td style="text-align: center;">327</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7276</td>
<td style="text-align: center;">989</td>
</tr>
<tr>
<td style="text-align: left;">h-index</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">i10-index</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">6</td>
</tr>
</tbody>
</table>
<p>Table 2: Research profile metrics of the idea writing and reviewing participants. Data are extracted from Google Scholar at the time of idea or review submission.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Median</th>
<th style="text-align: right;">Min</th>
<th style="text-align: right;">Max</th>
<th style="text-align: right;">SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Familiarity (1-5)</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Difficulty (1-5)</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">Time (Hours)</td>
<td style="text-align: right;">5.5</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">15.0</td>
<td style="text-align: right;">2.7</td>
</tr>
<tr>
<td style="text-align: left;">Length (Words)</td>
<td style="text-align: right;">901.7</td>
<td style="text-align: right;">876.0</td>
<td style="text-align: right;">444.0</td>
<td style="text-align: right;">1704.0</td>
<td style="text-align: right;">253.5</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Length (Words)</td>
<td style="text-align: right;">1186.3</td>
<td style="text-align: right;">1158.0</td>
<td style="text-align: right;">706.0</td>
<td style="text-align: right;">1745.0</td>
<td style="text-align: right;">233.7</td>
</tr>
<tr>
<td style="text-align: left;">AI + Human Rerank Ideas</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Length (Words)</td>
<td style="text-align: right;">1174.0</td>
<td style="text-align: right;">1166.0</td>
<td style="text-align: right;">706.0</td>
<td style="text-align: right;">1708.0</td>
<td style="text-align: right;">211.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics of the 49 ideas from each condition.
responses, 72 out of the 79 reviewers have previously reviewed for major AI conferences or journals. These statistics indicate that our participants are highly qualified and have substantial research experience. ${ }^{8}$</p>
<h1>4.3 Idea Writing</h1>
<p>We report statistics of our idea writers' ideas to measure their quality. As shown in Table 3, idea writers indicate a moderately high familiarity with their selected topic ( 3.7 on a 1 to 5 scale), and indicate the task as moderately difficult ( 3 on a 1 to 5 scale). They spent an average of 5.5 hours on the task and their ideas are 902 words long on average. These indicate that participants are putting substantial effort into this task. ${ }^{9}$ We also show the distribution of their selected topics in Table 4.</p>
<h3>4.4 Idea Reviewing</h3>
<p>Review Assignment We let all reviewer participants select their top two preferred topics as well as their preferred reviewing load (from 2 to 7). We then randomly assign them to ideas within their selected topics and all ideas are anonymized. In the assignment, we balance the number of ideas from each condition for each reviewer and ensure that each reviewer gets at least one human idea and one AI idea. Every idea is reviewed by 2 to 4 different reviewers. We also avoid assigning ideas written by authors from the same institution to avoid any potential contamination. Table 5 shows that each reviewer wrote an average of 3.8 reviews from 2 or 3 conditions, across 1 to 3 topics.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Idea topic distribution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Median</th>
<th style="text-align: right;">Min</th>
<th style="text-align: right;">Max</th>
<th style="text-align: right;">SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Familiarity (1-5)</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Confidence (1-5)</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">Time (Minutes)</td>
<td style="text-align: right;">31.7</td>
<td style="text-align: right;">30.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">120.0</td>
<td style="text-align: right;">16.8</td>
</tr>
<tr>
<td style="text-align: left;">Length (Word)</td>
<td style="text-align: right;">231.9</td>
<td style="text-align: right;">208.0</td>
<td style="text-align: right;">41.0</td>
<td style="text-align: right;">771.0</td>
<td style="text-align: right;">112.1</td>
</tr>
<tr>
<td style="text-align: left;">ICLR 2024</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Confidence (1-5)</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">0.8</td>
</tr>
<tr>
<td style="text-align: left;">Length (Word)</td>
<td style="text-align: right;">421.5</td>
<td style="text-align: right;">360.0</td>
<td style="text-align: right;">14.0</td>
<td style="text-align: right;">2426.0</td>
<td style="text-align: right;">236.4</td>
</tr>
<tr>
<td style="text-align: left;">Length (Word; Strengths \&amp; Weaknesses)</td>
<td style="text-align: right;">247.4</td>
<td style="text-align: right;">207.0</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">2010.0</td>
<td style="text-align: right;">176.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of our collected reviews, with ICLR 2024 reviews as a baseline (for the 1.2 K submissions that mentioned the keyword "language models").</p>
<p>Review Quality Check Apart from ensuring reviewer qualifications, we also compute statistics to measure the quality of the reviews in Table 6. On average, the reviewers indicated a familiarity of 3.7 (out of 5) in their selected topic and a confidence of 3.7 (out of 5) in their reviews. This is comparable with the 1.2 K ICLR 2024 submissions related to language models, where the reviewers also have an average confidence of 3.7 out of 5 . Moreover, reviewers spent an average of 32 minutes on each review, with each review being about 232 words long.
Since our review forms are different from the ICLR review forms, we compare them with the ICLR reviews where we remove the summary and question sections and only count the lengths of the strengths and weaknesses sections. This way, the ICLR reviews have an average length of 247, similar to our collected reviews. As an additional measure of review quality, out of the 298 unique reviews that we have collected, 80 of them provided links to existing papers in their rationales to justify why the proposed method is not novel. These results further validate the high quality of our review data.</p>
<h1>5 Main Result: AI Ideas Are Rated More Novel Than Expert Ideas</h1>
<p>In this section, we present our main finding on whether LLMs can generate better research ideas than experts. Consistently across three different statistical tests accounting for the possible confounders, we find that AI ideas have higher novelty scores than human ideas while being comparable on all other metrics.</p>
<h3>5.1 Test 1: Treating Each Review as an Independent Datapoint</h3>
<p>In Test 1, we treat each review as an independent datapoint and aggregate all reviews from the same condition. We treat the Human Ideas as the baseline condition and compare it with AI Ideas and AI Ideas + Human Rerank using two-tailed Welch's t-tests with Bonferroni correction. We show the barplot in Figure 2 and the detailed numerical results in Table 7. Both AI Ideas $(\mu=5.64 \pm \sigma=1.76)$ and AI Ideas + Human Rerank $(\mu=5.81 \pm \sigma=1.66)$ are significantly better than Human Ideas $(\mu=4.84 \pm \sigma=1.79)$ on the novelty score $(p&lt;0.01)$. In this particular test, the AI ideas in both conditions are also significantly better than human ideas on the excitement score ( $p&lt;0.05$ ), and the AI Ideas + Human Rerank condition is also significantly better than Human Ideas in terms of</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Condition</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Mean</th>
<th style="text-align: left;">Median</th>
<th style="text-align: left;">SD</th>
<th style="text-align: left;">SE</th>
<th style="text-align: left;">Min</th>
<th style="text-align: left;">Max</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Novelty Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: left;">119</td>
<td style="text-align: left;">4.84</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1.79</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.64</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.76</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.81</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.66</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Excitement Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: left;">119</td>
<td style="text-align: left;">4.55</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1.89</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.19</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.73</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">$\mathbf{0 . 0 4 *}$</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.46</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.82</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Feasibility Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: left;">119</td>
<td style="text-align: left;">6.61</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">1.99</td>
<td style="text-align: left;">0.18</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">6.34</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.88</td>
<td style="text-align: left;">0.18</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">6.44</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.63</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Expected Effectiveness Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: left;">119</td>
<td style="text-align: left;">5.13</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1.76</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.47</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.58</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.55</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.52</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">0.29</td>
</tr>
<tr>
<td style="text-align: left;">Overall Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: left;">119</td>
<td style="text-align: left;">4.68</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1.90</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">4.85</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">1.70</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">5.34</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">1.79</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">$\mathbf{0 . 0 4 *}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Scores across all conditions by treating each review as an independent datapoint (Test 1). Size is the number of reviews for each condition and the p-values are computed with twotailed Welch's t-tests with Bonferroni correction. We bold results that are statistically significant $\left({ }^{<em>} p&lt;0.05 ;{ }^{</em> <em>} p&lt;0.01 ;^{</em> * *} p&lt;0.001\right)$. AI ideas are judged as significantly better than human ideas in terms of novelty and excitement while being comparable on all other metrics.
the overall score ( $p&lt;0.05$ ). We do not observe significant differences between AI-generated ideas and human-written ideas on the other metrics.</p>
<h1>5.2 Test 2: Treating Each Idea as an Independent Datapoint</h1>
<p>Since we collect multiple reviews for each idea, one could argue that we should not treat each review as an independent datapoint. To account for this potential confounder, we perform Test 2 where we average the scores of each idea and treat each idea as one datapoint. This way, the sample size for every condition will be $N=49$, namely the number of ideas. We treat the Human Ideas as the baseline condition and compare it with AI Ideas and AI Ideas + Human Rerank using two-tailed Welch's t-tests with Bonferroni correction. As shown in Table 8, we still see significant results $(p&lt;0.05)$ where both AI Ideas $(\mu=5.62 \pm \sigma=1.39)$ and AI Ideas + Human Rerank $(\mu=5.78 \pm \sigma=1.07)$ have higher novelty scores than Human Ideas $(\mu=4.86 \pm \sigma=1.26)$.</p>
<h3>5.3 Test 3: Treating Each Reviewer as an Independent Datapoint</h3>
<p>Another possible confounder is that different reviewers might have different biases, for example, some reviewers may be more lenient than others. To account for such reviewer biases, we perform Test</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Condition</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;">SD</th>
<th style="text-align: center;">SE</th>
<th style="text-align: center;">Min</th>
<th style="text-align: center;">Max</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Novelty Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.62</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 *}$</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Excitement Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.18</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.45</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Feasibility Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">6.53</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">6.30</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">8.50</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">6.41</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Expected Effectiveness Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.10</td>
<td style="text-align: center;">5.33</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.48</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.57</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">Overall Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Human Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">6.67</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5.32</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">1.24</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">0.06</td>
</tr>
</tbody>
</table>
<p>Table 8: Scores across all conditions by averaging the scores for each idea and treating each idea as one data point (Test 2). Size is the number of ideas for each condition, and the p-values are computed with two-tailed Welch's t-tests with Bonferroni correction. We bold results that are statistically significant $\left({ }^{<em>} p&lt;0.05 ;{ }^{</em> *} p&lt;0.01\right)$. AI ideas are judged as significantly better than human ideas in terms of novelty while being comparable on all other metrics.</p>
<p>3 where we treat each reviewer as one datapoint and compute their average score on each condition. Then for each reviewer, we get their mean score difference between the AI Ideas condition and the Human Ideas condition, as well as the difference between the AI Ideas + Human Rerank condition and the Human Ideas condition. This way, we only analyze the differences among the different conditions. That is, if the differences are significantly higher than zero under the one-sample t-test, that indicates reviewers are giving higher scores to one condition compared to the other. The results are shown in Table 9, and we see significant results ( $p&lt;0.05$ ) that AI ideas in both the AI Ideas and AI Ideas + Human Rerank conditions are rated more novel than Human Ideas. Therefore, we conclude that AI ideas generated by our ideation agent are judged as more novel than human expert generated ideas, consistently across all three different statistical tests. ${ }^{10}$</p>
<h1>6 In-Depth Analysis of the Human Study</h1>
<p>While the above main results highlight the promise of LLMs in generating novel research ideas, there are some additional nuances. In this section, we move beyond the statistical comparisons and dive</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">N</th>
<th style="text-align: left;">Mean Diff</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Novelty Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas vs Human Ideas</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank vs Human Ideas</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Excitement Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas vs Human Ideas</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">$\mathbf{0 . 0 1 *}$</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank vs Human Ideas</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">0.87</td>
<td style="text-align: left;">$\mathbf{0 . 0 0 * *}$</td>
</tr>
<tr>
<td style="text-align: left;">Feasibility Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas vs Human Ideas</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">-0.29</td>
<td style="text-align: left;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank vs Human Ideas</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">-0.08</td>
<td style="text-align: left;">0.74</td>
</tr>
<tr>
<td style="text-align: left;">Effectiveness Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas vs Human Ideas</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.42</td>
<td style="text-align: left;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank vs Human Ideas</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">0.39</td>
<td style="text-align: left;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">Overall Score</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas vs Human Ideas</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.24</td>
<td style="text-align: left;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">AI Ideas + Human Rerank vs Human Ideas</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">$\mathbf{0 . 0 1 *}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Mean score differences between AI ideas and human ideas by treating each reviewer as a data point (Test 3). All p-values are computed with one-sample t-tests with Bonferroni correction. We bold results that are statistically significant $\left({ }^{<em>} p&lt;0.05 ;{ }^{</em> *} p&lt;0.01\right)$.
into other aspects of our collected data. Specifically, we focus on the quality of human ideas, reviewer preferences, and the extent of reviewer agreement.</p>
<h1>6.1 Human Experts May Not Be Giving Their Best Ideas</h1>
<p>We first investigate whether human experts are submitting their best ideas to us. We did a poststudy survey to understand how idea-writing participants came up with their ideas. Out of the 49 participants, 37 of them came up with the idea on the spot, while the other 12 already had the idea before the study. Furthermore, we asked the survey question: "How does this idea compare to your past research ideas (ideas that you actually worked on)? Please answer with a percentile. E.g., this idea is one of my top $10 \%$ ideas." Our participants indicated that on average their submitted ideas are about the top $43 \%$ of all their past ideas. This implies that our collected ideas are likely the median-level ideas from these expert researchers, which is reasonable given that most of them came up with the idea within the 10-day time constraint of the task.</p>
<h3>6.2 Reviewers Tend to Focus More on Novelty and Excitement</h3>
<p>To gain a deeper understanding of the dynamics between the different metrics in the review process, we explore whether reviewers focus on specific aspects when evaluating the ideas. We compute the pairwise correlation between different metrics in Table 10. The overall score mostly correlates with the novelty score $(r=0.725)$ and excitement score $(r=0.854)$, while having almost no correlation $(r&lt;0.1)$ with the feasibility score. This implies that reviewers might be paying more attention to the novelty and excitement aspects of the ideas when they are reviewing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">Novelty</th>
<th style="text-align: center;">Excitement</th>
<th style="text-align: center;">Feasibility</th>
<th style="text-align: center;">Effectiveness</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.642</td>
</tr>
<tr>
<td style="text-align: left;">Novelty</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">-0.073</td>
<td style="text-align: center;">0.357</td>
</tr>
<tr>
<td style="text-align: left;">Excitement</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-0.031</td>
<td style="text-align: center;">0.565</td>
</tr>
<tr>
<td style="text-align: left;">Feasibility</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">-0.073</td>
<td style="text-align: center;">-0.031</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.251</td>
</tr>
<tr>
<td style="text-align: left;">Effectiveness</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 10: Pairwise correlation between different metrics (symmetric matrix).</p>
<h1>6.3 Reviewing Ideas is Inherently Subjective</h1>
<p>Finally, we acknowledge that reviewing is inherently subjective, and reviewing based on ideas rather than executed papers might be even more subjective. We investigate this using inter-reviewer agreement. Specifically, we randomly split reviewers of each paper into half, use one half to rank the top and bottom $25 \%$ of all ideas, and then measure agreement with the held-out set of reviewers. ${ }^{11}$ As shown in the first block of Table 11, reviewers have a relatively low agreement ( $56.1 \%$ ) despite the fact that we have provided detailed explanations for each metric in our review form. As a baseline comparison, the NeurIPS 2021 reviewer consistency experiment found $66.0 \%$ accuracy using this reviewer agreement metric in the balanced setting (Beygelzimer et al., 2021, Lu et al., 2024). We also computed the reviewer agreement using the same metric on the 1.2 K ICLR 2024 submissions related to language models, which has a balanced accuracy of $71.9 \%$. While our reviewer agreement is higher than random ( $50 \%$ ), it is generally lower than conference reviewing, most likely due to the higher subjectivity involved when evaluating ideas without seeing the actual experiment results.</p>
<h2>7 Limitations of LLMs</h2>
<p>With our findings from the human study in mind, we now turn to LLM performance to provide insights that could inform future methods for improving idea generation systems. Our ideation agent is motivated by two potential strengths of LLMs: their ability to scale by generating a vast number of ideas - far more than any human could - and the possibility of filtering these ideas to extract the best ones from the large pool. In theory, this approach could lead to high-quality ideas by leveraging inference scaling. However, we present empirical evidence that this naive assumption about scaling idea generation has significant limitations.</p>
<h3>7.1 LLMs Lack Diversity in Idea Generation</h3>
<p>We adopted an over-generate and rank paradigm in idea generation. This raises the question: is there an upper limit to how many new ideas LLMs can generate? To answer this question, we take a closer look at 4000 generated seed ideas for each topic.
We encode all raw ideas with all-MiniLM-L6-v2 from Sentence-Transformers. For each idea, we compute its cosine similarity with all previously generated ideas on the same topic. We consider an idea as a duplicate if it has a similarity of above 0.8 with any of the previously generated ideas. In Figure 4, we show that as the agent keeps generating new batches of ideas, the percentage of non-duplicates in newly generated batches keeps decreasing, and the accumulated non-duplicate ideas eventually plateau. In fact, out of the 4000 generated seed ideas, there are only 200 non-duplicate</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Measuring duplication of AI-generated ideas: the left figure plots the percentage of non-duplicate ideas in each new bucket of generated ideas; the right figure plots the accumulated non-duplicate ideas as the agent keeps generating new ideas. All data points are averaged across all topics.</p>
<p>Unique ideas. This sets a bottleneck on our inference-time scaling since increasing the number of generated ideas simply leads to repeating duplicate ideas.</p>
<h3>7.2 LLMs Cannot Evaluate Ideas Reliably</h3>
<p>Most prior works have adopted <em>LLM-as-a-judge</em> for evaluating research ideas (Lu et al., 2024) motivated by the observation that LLMs can have a higher agreement with human evaluators than the inter-human agreement. However, we offer some empirical evidence that LLMs cannot evaluate ideas reliably yet.</p>
<p>Concretely, we use the average review score of each idea to rank the top and bottom 25% of all our collected human and AI ideas, and use this to benchmark various LLM evaluators. Specifically, we obtain the LLM predicted scores of all ideas and set the median score as the threshold to measure their accuracy on our balanced idea ranking data.</p>
<p>In the second block of Table 11, we compare several different LLM evaluators: 1) directly giving the review criteria and prompting for a final score (Baek et al., 2024; Li et al., 2024; Yang et al., 2024); 2) our pairwise ranker as described in Section 3.3; and 3) the "AI Scientist" reviewer agent (Lu et al., 2024). All of these LLM evaluators <strong>have a lower agreement than our expert reviewers' scores</strong>. Even the best LLM evaluator  our own Claude-3.5 pairwise ranker  only achieves an accuracy of 53.3%, lower than our inter-reviewer consistency of 56.1%.</p>
<p>Even if AI-human agreement eventually matches or exceeds human-human agreement, simply meeting this baseline does not imply that AI-as-a-reviewer is meaningful, since we may be trading variance for bias, where AI reviewers are more consistent but rely on spurious correlations (Durmus et al., 2022). Our findings in Table 11 are consistent with these brittleness concerns, as we find a significant drop in AI-human agreement scores under our study compared to the original studies.</p>
<p>Finally, even though Claude-3.5 pairwise agreements may seem close to human agreement, many other pieces of evidence throughout the paper leads us to be cautious about the use of LLM-as-a-judge.</p>
<p>in such a complex and subjective task. These include our findings on the significant discrepancy between the agent's top-ranked ideas and the human expert's top-ranked ideas (Appendix I) and how the AI Ideas + Human Rerank condition tends to score higher than the AI Ideas condition on all metrics in Section 5. These limitations of LLM auto-evaluation not only constrain the effectiveness of our over-generate-and-rank paradigm for idea generation but also raise concerns about trusting conclusions that are based primarily on LLM evaluators.</p>
<h1>8 Qualitative Analysis and Examples</h1>
<p>In this section, we offer some qualitative analysis of human- and AI-generated ideas based on our collected reviews and present four pairs of randomly sampled human and AI ideas as case studies.</p>
<h3>8.1 Analysis of Free-Text Reviews</h3>
<p>Following recent practices of using LLMs to extract patterns from text corpora (Zhong et al., 2022, 2023), we use Claude-3.5 to extract and cluster the main points from all reviews. We then manually verified and labeled each cluster.
Many reviews reinforce our quantitative finding that AI ideas tend to be more novel. For example, reviewers noted: "The idea of [...] is quite novel in an in-context learning setting.", "The idea of exploring [...] using an LLM-based iterative approach is novel.", "The idea of [...] when constructing prompts to improve cross-lingual transfer is one that I have not heard of before.", "I like the idea to [...], and think it will be helpful for other researchers in the community.", "Combining [...] is a unique way of attempting to preserve the gist of the information while likely losing specific identifiers.", and "Safeguarding using [...] is clearly novel. Similar ideas have not been seen in the related work.".
Next, we summarize some common failure modes of AI ideas:</p>
<ol>
<li>Being too vague on implementation details. For example, one reviewer noted: "I'm not super clear on the details of this lattice and how the model will be prompted, so I'm not super sure how well the model will complete these subtasks and how well-suited this particular structure is to completing the overall task." and another reviewer noted: ""For analyzing the effectiveness of the method, the proposal only provides a very ad-hoc + hand-wavey suggestion to compare responses across predefined questions." In another case, the AI idea is criticized for not considering practical implementation details: "I think in each of the steps, there is something hard to execute. For example, in step Constellation Formation, how do we do the weighted sum?" Similarly, other reviews noted: "It's unclear how CLIP is connected to the language model and how training a CLIP model would enable the LM to understand images.", and "There's no mentioning on how to prompt the model to generate defensive strategies and refine the model's responses using these strategies." Such vagueness often makes it difficult for reviewers to make confident judgments: "Because this idea is too general and vague, I can't really answer the previous question. An idea needs a certain level of details to be determined if it fits for a conference/journal but this one misses them."</li>
<li>Misuse of datasets. For example: "I'm not sure about the datasets picked. StereoSet is not a QA dataset; it simply contains statements. Also, I don't understand why Dialogue NLI responses require empathy.", "I'm concerned the datasets proposed are the right test cases for security of the code (since they are really just ML/programming problems, not system-level programming).", and "the choice of datasets might not be the best to show the effect of incorporating multiple perspectives, especially TruthfulQA and ScienceQA, which seems to have a single correct interpretation and answer." In another example, the benchmark datasets chosen are considered</li>
</ol>
<p>too easy by the reviewer: "none of the chosen datasets (MATH, GSM8K, and MMLU) uses complicated math concepts".
3. Missing or inappropriate baselines. For example: "The proposed method needs to be compared to simply asking the model to think of one (or several) facts about the question before answering using more turns. This could be an additional baseline to verify the scoring process is meaningful." and "Although the proposal includes some baselines that should be compared to, it does not mention some methods which seem to do quite well with LLMs." Sometimes, "the chosen baselines may not be suitable", for example, because they are not directly comparable with the proposed method.
4. Making unrealistic assumptions. For example: "The assumption that model can (mostly) accurately flag its own hallucinations is quite tricky.", "there is a presupposed assumption that hallucinations in LLMs are ungrounded and independent of the data they are trained on, which is generally not considered true", "The big issue for the effectiveness of the proposed method is that, it asserts very strong assumptions on downstream tasks, such as there must exist only two extremes.", "Some assumptions (e.g., [...]) are unlikely to be true in practice, especially when low-resource languages and less represented cultures are included in the study.", and "A major assumption in this approach is that the model is able to [...]. However, [...]".
5. Being too resource-demanding. Despite the fact that we explicitly prompted the agent to consider feasibility when generating ideas, some of the generated ideas are still too resourcedemanding. For example, one reviewer noted: "The biggest issue to feasibility I see is that the project calls for fine-tuning BLOOM (See step 5). BLOOM has 176B parameters so it's going to take quite a lot of GPUs to fine-tune. From a systems perspective, I see this as causing delays." In some other cases, manual data annotation is being criticized for feasibility: "The bottleneck seems to be the dataset collection process if there are no existing datasets that fit the requirements of the paper.", and "the manual evaluation by native speakers or cultural experts could be time-consuming and resource-intensive".
6. Not well-motivated. For example: "Not well-motivated and there is not a clear intuition that this work can work to increase the factuality.", "And in general the method is not well-motivated and needs reasons why retrieving from model itself is meaningful by use cases or specific tasks.", and "The idea simply doesn't make sense to me. Given current LLMs' ability, I'm pretty sure they can simply recite code like inserting data to a binary search tree."
7. Not adequately following existing best practices. For example: "The proposal does not seem to include awareness of what has been previously tried, or more strategic ways to evaluate success/failures..."</p>
<p>We contrast these with some of the unique strengths and weaknesses of human ideas:</p>
<ol>
<li>Human ideas are generally more grounded in existing research and practical considerations, but may be less innovative. For example, these ideas might be applying existing techniques to new problems: "Multilinguality as a debiasing method has already been considered in the literature, although not necessarily in the prompt engineering framework." Sometimes people apply incremental changes to existing techniques: "The overall idea shares quite a similar idea with program-of-thought (PoT). The only difference is that there is an additional step where an LLM is prompted to decide whether to use code or not." Some ideas try to combine existing techniques: "Query decomposition and RAG separately are well studied, if there is no existing work that combines both (which I'm not aware of), then it's reasonably novel." As some reviewers</li>
</ol>
<p>noted, human ideas tend to build on known intuitions and results: "There are already existing works on using available lexicons to improve the translation capabilities of LLMs in general."
2. Human ideas tend to be more focused on common problems or datasets in the field. For example: "The problem of models not handling negation properly is a very common problem, especially among propriety LMs such as claude-3-5-sonnet.", "The data exist. This project mainly entails plugging in these datasets to a prompt template and finetuning for a bit. There is little left unspecified, and it should be quite simple to execute on.", "I haven't found any work using this idea to solve this specific problem, but [...] is definitely not new.", and "While existing works have explored the problem of calibration in long-form answers (e.g. [...]), the specific method for calibration is different."
3. Human ideas sometimes prioritize feasibility and effectiveness rather than novelty and excitement. For example, reviewers noted: "I don't think this will be a groundbreaking finding, but it will probably work." and "while the idea is promising and could lead to significant improvements, it may not be groundbreaking enough to be considered transformative or worthy of a best paper award".</p>
<h1>8.2 Randomly Sampled Human and AI Ideas with Reviews</h1>
<p>We randomly sample four pairs of ideas from different topics to ground our numerical results with actual examples. In each pair, there is one AI idea and one human idea. To save space, we include the full project proposal of each idea along with the full set of reviews in the Appendix, but we list their titles, topics, and average scores here for quick reference (we reveal whether each idea is AI-generated or human-written in Appendix X):</p>
<ol>
<li>Modular Calibration for Long-form Answers: Appendix P Topic: Uncertainty; Average Overall Score: 5.5</li>
<li>Semantic Resonance Uncertainty Quantification: Calibrating LLM Confidence through MultiPath Reasoning: Appendix Q
Topic: Uncertainty; Average Overall Score: 6</li>
<li>Translation with LLMs through Prompting with Long-Form Context: Appendix R Topic: Multilingual; Average Overall Score: 4</li>
<li>Linguistic Pivot Constellation: Enhancing Cross-Lingual Transfer for Low-Resource Languages and Dialects: Appendix S
Topic: Multilingual; Average Overall Score: 6.7</li>
<li>LLM Directed Retrieval Querying for Improving Factuality: Appendix T Topic: Factuality; Average Overall Score: 4.7</li>
<li>Semantic Divergence Minimization: Reducing Hallucinations in Large Language Models through Iterative Concept Grounding: Appendix U Topic: Factuality; Average Overall Score: 3.3</li>
<li>Autoprompting: Generate Diverse Few-shot Examples for Any Application: Appendix V Topic: Coding; Average Overall Score: 5</li>
<li>Temporal Dependency Unfolding: Improving Code Generation for Complex Stateful Systems: Appendix W
Topic: Coding; Average Overall Score: 6.7</li>
</ol>
<h1>9 Related Work</h1>
<p>Research idea generation and execution. Several prior works explored methods to improve idea generation, such as iterative novelty boosting (Wang et al., 2024), multi-agent collaboration (Baek et al., 2024), and multi-module retrieval and revision (Yang et al., 2024). While some of them share similar components as our ideation agent, these works focus on improving the idea generation methods over vanilla prompting baselines, without comparisons to any human expert baselines. Beyond ideation, another line of work uses LLMs for executing experiments by generating code given the research problems (Huang et al., 2024, Tian et al., 2024), or combining idea generation with code generation to directly implement AI-generated ideas (Li et al., 2024, Lu et al., 2024). These works either use automatic evaluation on a pre-defined set of problems and benchmarks, setting a constrained problem space; or rely on proxy metrics like LLM evaluators, which are often unreliable.
LLM for other research-related tasks. LLMs have also been used for several other research-related tasks, such as generating code to perform data-driven discovery (Gu et al., 2024, Guo et al., 2024, Hu et al., 2024, Ifargan et al., 2024, Majumder et al., 2024), automatic review generation (D'Arcy et al., 2024, Liang et al., 2024), related work curation (Ajith et al., 2024, Kang and Xiong, 2024, Lehr et al., 2024, Press et al., 2024), experiment outcome prediction (Hewitt et al., 2024, Lehr et al., 2024, Manning et al., 2024, Zhang et al., 2024), and future work recommendation (Zhang et al., 2024). Unlike these works, we tackle the more creative and open-ended task of research ideation.
Computational creativity. Our work also connects to the line of work on examining AI's novelty and diversity in creative tasks. Chakrabarty et al. (2024) found that AI writings are less creative than professional writers, while we show LLM-generated ideas can be more novel than experts on the task of research ideation. Another line of work found that LLM generations lack collective diversity (Anderson et al., 2024, Zhou et al., 2024), which matches our findings on idea generation. Lastly, several other works conducted human evaluation to study the impact of AI exposure or humanAI collaboration on novelty and diversity (Ashkinaze et al., 2024, Liu et al., 2024, Padmakumar and He, 2024) with mixed conclusions. While we also conduct a human evaluation of idea novelty, we focus on the human-AI comparison on the challenging task of research ideation with expert participants.</p>
<h2>10 Discussion</h2>
<p>To summarize, we compared research ideas generated by our AI agent with ideas written by expert researchers, and observed the robust finding that expert reviewers rate AI ideas as statistically more novel than expert ideas. In this section, we discuss some high-level questions readers might have and suggest some ways to address them.
Question 1: Do these collected expert ideas represent their best ideas? One might argue that these ideas submitted by our idea-writing participants might not represent their best ideas as we discussed in Section 6.1, since most of them came up with the idea on the spot within a short period. In order to address this concern, we have designed an experiment where we will compare AI ideas with papers accepted at top-tier AI conferences. To avoid any possible contamination, we target the upcoming EMNLP 2024 conference, which will release the accepted papers in October 2024. We have generated AI ideas with our agent on 23 topics from the EMNLP Call For Papers page in July 2024 and cached them. We pre-registered our analysis plan which also includes the link to the cached ideas. ${ }^{12}$ Apart from comparing the quality of these ideas, we will also compute the overlap between AI-generated ideas and accepted papers on the same topics.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Question 2: Are evaluations based solely on ideas subjective? In this current study, we focused solely on evaluating the ideas themselves. Ideas that sound novel and exciting might not necessarily turn into successful projects, and our results indeed indicated some feasibility trade-offs of AI ideas. We view the current study as a preliminary evaluation of AI-generated ideas. In the next phase, we will recruit researchers to execute some AI and human-generated ideas into full projects. This will enable reviewers to assess the complete experimental outcomes, providing a more reliable basis for evaluation. Furthermore, it will allow us to analyze whether our initial idea evaluations align with the assessments of the actual project outcomes.
Question 3: Why do you focus only on prompting-based research in NLP? The scope of our study is limited to prompting research ideas within NLP. We chose this design to facilitate the next phase of our execution experiment, where we prefer research ideas that are less resource-demanding and can be executed relatively quickly. We believe that the evaluation protocols we established should be applicable to other research domains as well, although the conclusions could be different depending on the research fields. Future work should consider extending such human study to other research domains and it would be interesting to compare how the conclusions differ.
Question 4: Can you automate idea execution as well? It is tempting to envision an end-to-end automated research pipeline where AI agents can implement AI-generated ideas to directly evaluate their effectiveness. Apart from speeding up scientific discovery, one could also imagine using such execution agents to automatically verify experiment results in existing papers or new submissions. We have also explored building an LLM agent to generate code to implement the generated ideas. Specifically, we provide a template codebase that consists of: (1) loading datasets from Huggingface or generating synthetic test examples; (2) implementing baseline methods; (3) implementing the proposed method; (3) loading or implementing the evaluation metrics; (4) running experiments on the testset with the baselines and the proposed method, so that the output of the agent will be a report of the baseline performance as well as the proposed method's performance. While this agent can generate code that compiles and executes, we find that the automated experiments can be misleading because the agent often skips or modifies steps in the baselines or proposed methods. In some cases, the metric functions are also not correctly defined. This highlights the core challenge: just comparing the final experiment results is not enough; we have to verify the faithfulness of the implementations as well. Performing such implementation verification is not a trivial task, and we leave it to future work. We provide detailed description of our idea execution agent in Appendix Y.</p>
<h1>11 Ethical Considerations</h1>
<p>Publication Policy. The growing use of AI to generate research ideas raises serious concerns about the potential abuse of these technologies by students or researchers who may flood academic conferences with low-quality or poorly thought-out submissions. The availability of LLM-generated content could lead to a decline in the overall quality of academic discourse, as some individuals might take a lazy approach, relying on AI to both generate ideas and review submissions. This would undermine the credibility and integrity of the review process. The risks are real. Without proper oversight, we could see a deluge of submissions that lack depth or intellectual merit. To prevent this, it is essential to hold researchers accountable for the outputs generated through AI tools. Rigorous standards must be applied equally to both AI-assisted and human-generated research to ensure that the use of LLMs does not result in misleading, superficial, or unethical academic contributions.
Intellectual Credit. The use of LLMs to generate research ideas introduces significant ambiguity around the concept of intellectual credit. Traditional frameworks for attributing credit in research, based on human authorship and contribution, become less clear when AI plays a significant role</p>
<p>in idea generation. Questions arise around how to distribute credit between the developers of the LLM, the researchers who designed the frameworks for its use, and the researchers who integrate AI-generated ideas into their work. Furthermore, it becomes increasingly difficult to trace the origins of AI-generated contributions, especially when they draw from vast datasets composed of numerous sources. This complexity calls for a broader rethinking of how intellectual credit is assigned in AI-driven research. While a complete overhaul of legal and academic norms is beyond the scope of this project, we advocate for the adoption of transparent documentation practices. Researchers should clearly disclose the role AI played in the idea generation process, specifying which models, data sources, and frameworks were used, and outlining the level of human involvement. This could ensure that the credit distribution in AI-supported research is as transparent and fair as possible.
Potential for Misuse. AI-generated research ideas, especially those that introduce novel concepts, have the potential to be misused in ways that could lead to harmful or destabilizing outcomes. For instance, ideation agents could be leveraged to generate adversarial attack strategies or other unethical applications. This concern aligns with broader arguments from those focused on existential risk (X-risk), who argue that AI-driven innovation could be a primary route to destabilizing the status quo, posing risks at a societal or even global level. Our stance is that such discussions on safety should be evidence-based to the extent that it is possible, and careful evaluation work is an important component of keeping these discussions grounded in actual, measured capabilities of these systems. We advocate for continued safety research specifically targeting these types of concerns-such as the development of Reinforcement Learning from Human Feedback (RLHF) systems or anti-jailbreak mechanisms for research ideation agents. Additionally, we believe it would be meaningful to create safety benchmarks that assess the ethical and safe application of AI-generated ideas.
Idea Homogenization. Our analysis showed that current LLMs lack diversity in idea generation. This raises important concerns that wide adoption of LLMs can result in idea homogenization, where the generated ideas only reflect a narrow set of perspectives or have systematic biases. Over time, this could lead to a reduction in the richness and diversity of research outputs globally. Future work should develop ways to either improve LLMs themselves or refine our idea generation methods to promote idea diversity. It's also important to note that our evaluation primarily assesses the quality of the typical ideas being generated, and may not fully capture the long tail of unique or novel ideas that would be truly transformative.
Impact on Human Researchers. The integration of AI into research idea generation introduces a complex sociotechnical challenge, as research is fundamentally a community-driven, collaborative effort. By introducing AI, particularly LLMs, into this social system, we risk unforeseen consequences. Overreliance on AI could lead to a decline in original human thought, while the increasing use of LLMs for ideation might reduce opportunities for human collaboration, which is essential for refining and expanding ideas. To mitigate these risks, future works should explore new forms of human-AI collaboration, and our results on human reranking of AI ideas show that even naive human-AI collaboration approaches can be effective. Beyond reranking, humans can play a critical role in the ideation process by providing intermediate feedback, taking AI-generated ideas as inspiration for further development, and bringing their unique expertise into the process. Understanding how to integrate LLMs into this collaborative process without disrupting the social fabric of research will be an important ongoing problem, requiring careful consideration of the broader sociotechnical implications.</p>
<h1>Positionality Statement</h1>
<p>We disclose the authors' anticipated outcomes of the human study before the experiment was conducted to be transparent about experimenter biases. Among the three authors, Tatsu and Diyi were expecting a null result from the study while Chenglei expected AI to be better than humans.</p>
<h2>Acknowledgement</h2>
<p>We thank all participants who wrote and reviewed ideas for us. Many of them also provided insightful feedback on various aspects of this study. This project would not have been possible without their support. To ensure the integrity and fairness of phase II of our study, we leave our participants anonymous but will update this manuscript with a detailed acknowledgment of all participants in the project's final report.
We thank Rose Wang, Dora Zhao, Irena Gao, Isabel Gallegos, Ken Liu, Aryaman Arora, Harshit Joshi, Shi Feng, Tianyu Gao, Xinran Zhao, Yangjun Ruan, Xi Ye, Mert Yuksekgonul, and members of Tatsu Lab and SALT Lab for their helpful feedback on the early version of this draft.
We thank our undergraduate intern Isha Goswami and faculty administrator Eric Alejandro Pineda for assisting with review data collection and financial logistics.
This work was supported by gifts from Open Philanthropy, Tianqiao and Chrissy Chen Institute, Meta, IBM, and Amazon, and grants from ONR, NSF IIS-2247357, and CNS-2308994.</p>
<h2>References</h2>
<p>Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. LitSearch: A Retrieval Benchmark for Scientific Literature Search. ArXiv, abs/2407.18940, 2024.</p>
<p>Barrett R Anderson, Jash Hemant Shah, and Max Kreminski. Homogenization Effects of Large Language Models on Human Creative Ideation. In Proceedings of the 16th Conference on Creativity $\mathcal{E}$ Cognition, 2024.</p>
<p>Joshua Ashkinaze, Julia Mendelsohn, Qiwei Li, Ceren Budak, and Eric Gilbert. How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. ArXiv, abs/2401.13481, 2024.</p>
<p>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. ArXiv, abs/2404.07738, 2024.</p>
<p>Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378:1067 - 1074, 2022.</p>
<p>Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan. The neurips 2021 consistency experiment. https://blog.neurips.cc/2021/12/08/ the-neurips-2021-consistency-experiment, 2021. Neural Information Processing Systems blog post.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ https://osf.io/z6qa4&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
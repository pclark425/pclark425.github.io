<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7025 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7025</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7025</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273638030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.19494v3.pdf" target="_blank">Graph Linearization Methods for Reasoning on Graphs with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph reasoning tasks. The key question, therefore, is how to transform graphs into linear sequences of tokens, a process we term"graph linearization", so that LLMs can handle graphs naturally. We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs. To achieve this, we developed several graph linearization methods based on graph centrality and degeneracy. These methods are further enhanced using node relabeling techniques. The experimental results demonstrate the effectiveness of our methods compared to the random linearization baseline. Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multimodal processing using a unified transformer model.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7025.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7025.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EdgeList-Order</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-list linearization with centrality/degeneracy ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represents a graph as a sequence of bracketed edge pairs (v, u) where edges are ordered by a node-centric ranking (descending centrality or core number) and edges incident to each ranked node are listed; optional tie-breaking is random.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Ordered edge-list (Degree / PageRank / CoreNumber)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is serialized as a bracketed pair (v, u) and the full graph is the sequence of such pairs. Nodes are first ranked (by degree, PageRank, or core number) and the edges incident to each ranked node are appended in that node order; ties in ranking are broken randomly. Prompts append a task-specific question after the linearized edge list.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Node-centric ordering by descending centrality (Degree, PageRank) or degeneracy (CoreNumber); for each node in ranking, list its incident edges (edge pairs), ties randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphWave; GraphQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-level and node-level reasoning tasks (Node Counting, Node Degree, Max Degree, Edge Existence, Path Existence, Shortest Path, Diameter, Motif Shape Classification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 Instruct (primary: 8B); ablations with Llama 3 70B and Qwen 2.5 14B-1M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large language models used in inference-only experiments (Llama 3 Instruct 8B: main model, deterministic sampling settings; Llama 3 70B: larger-size ablation; Qwen 2.5 14B-1M: large-context ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match accuracy (percentage of exact-correct outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across GraphWave and GraphQA, ordered edge-list variants consistently outperform a fully random edge-list baseline. Reported specific gains include: Degree+node-relabeling improved maximum node degree estimation by ~35% and shortest path by ~13% on GraphQA; Degree+relabeling improved Edge Existence by ~26% on GraphWave. Exact per-task table values are reported in the paper's Tables 1–3.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not used for model training; used at inference/prompting time. The representation improved downstream reasoning accuracy in zero-shot and one-shot prompting compared to random ordering, indicating better alignment with LLM priors without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on ordering heuristics with non-deterministic tie-breaking (not fully canonical); input token cost grows with number of edges (context-window limited); global properties (e.g., diameter) remain challenging despite ordering; evaluated only on synthetic datasets in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms fully random edge-list baseline. Degree and PageRank orderings are most consistent across tasks; CoreNumber (degeneracy) performs particularly well on edge-centric tasks. Pseudo-random/default generator orderings can be competitive on some generators (see pseudo-random experiments). Linegraph-based variants (edge-as-node) can outperform node-based orderings on edge-specific tasks but have lower overall average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7025.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NodeRelabel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node relabeling by ranking index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>After ranking nodes by centrality/degeneracy, original node identifiers are replaced with their positions in the ranking (e.g., highest -> 0) to align token positions across graphs and improve global alignment for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relabeled edge-list (rank-index labels)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Same edge-list serialization as above, but node identifiers are replaced by their rank indices from the chosen ordering (so node 0 is the highest-ranked node in every graph). This is applied prior to forming the textual sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based (label-canonicalization attempt)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute ranking (Degree, PageRank, CoreNumber) then replace original labels by rank positions; serialize edges as (rank_v, rank_u) in ordered sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphWave; GraphQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same graph reasoning tasks as ordered edge-list (Node Counting, Node Degree, Max Degree, Edge/Path existence, Shortest Path, Diameter, Motif classification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 Instruct (8B primary); also tested with Llama 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLMs used in prompting experiments; node relabeling was applied to inputs prior to prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Node relabeling further improves ordered edge-list performance across nearly all tasks. Paper reports combined Degree-based ordering + node relabeling yields the largest gains (e.g., ~35% improvement on max degree estimation on GraphQA; ~26% boost on edge existence in GraphWave when combined with degree ordering).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not applied during training; at inference the relabeling improves cross-sample alignment and raises zero-/one-shot accuracy without model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relabeling depends on the chosen ranking metric and still requires tie-breaks (random) in the implementation, so it is not fully canonical; benefits depend on task and ordering choice; does not remove the context-window limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Ordering plus relabeling outperforms ordering without relabeling and the random baseline. Relabeling makes sequences more globally aligned across graphs, better matching LLM training priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7025.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LineGraph-Lin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linegraph-based linearization (edges-as-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms the original graph to its linegraph L(G) so that original edges become nodes; the same ordering and serialization pipeline is then applied to L(G) to capture edge-to-edge relationships in the textual sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linegraph linearization (LG{*})</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert G to linegraph L(G): each original edge becomes a node in L(G); two nodes in L(G) connect if their corresponding edges in G are incident. The authors then apply node-ranking and edge-list serialization to L(G) so the resulting token sequence reflects edge adjacencies.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based (edge-centric representation)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Linegraph transformation followed by the same ranking (Degree/PageRank/CoreNumber computed on L(G)) and sequential serialization of incident relations (now edge-as-node pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphWave; GraphQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Edge-centric reasoning tasks (Edge Existence, Path/Connectivity) and general graph reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 Instruct (8B) and ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLMs used with linegraph-transformed textual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Linegraph-based methods have a lower overall average accuracy than the best node-based linearizations but generally perform better in edge-based tasks such as edge existence and path reasoning (paper reports improved edge/task-specific scores for LG{*} variants; exact per-task numbers are in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Used at inference; helps capture inter-edge dependencies leading to improved performance on edge-centric tasks without model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Overall average performance lower than best node-based methods; adds a preprocessing transform and can increase serialized size/complexity; still subject to context-window token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to node-based ordered edge-lists, linegraph variants specialize for edge-oriented tasks and capture edge-to-edge relationships better, but underperform on many node-focused tasks and in overall averages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7025.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PseudoRandomDefaultOrder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generator default / pseudo-random edge ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses the edge order as produced by the graph generator (procedural construction order) as the serialized edge-list; this ordering sometimes implicitly encodes structural construction information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Default generator edge ordering (pseudo-random linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize edges in the exact order they were output by the graph generation process (no additional reordering or relabeling). The paper calls this 'pseudo-random' because many generators produce structurally informative sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Use the existing edge output order from the graph generator without reordering; optionally combined with default labels or structured node relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphWave; GraphQA (evaluated in Appendix A)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same set of graph reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 Instruct (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLM used for inference; no additional preprocessing beyond using generator ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Default generator ordering performs competitively and in some cases significantly better than fully random ordering; on GraphQA default ordering often significantly outperforms random ordering across many tasks (paper Appendix A provides detailed comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>No training impact; using generator ordering can boost inference-time accuracy when the ordering encodes construction-related structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires access to generator-specific ordering behavior which may not be available for real-world graphs; performance varies with the generator and can be dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Sometimes outperforms structured ordering; however, structured orderings with relabeling often yield consistent improvements across datasets and tasks when generator ordering is not available or meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7025.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RawEdgeList/NL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Raw edge-list or natural language graph descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple graph-to-text approaches that either list edges as pairs or describe the graph in natural language sentences (e.g., adjacency statements) prior to a task prompt; used commonly in prior LLM-for-graph work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Raw edge-list / natural language description</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two common baseline forms: (1) Raw edge-list: textual serialization of edges (e.g., '[(b,a),(c,b),(b,d)]'); (2) Natural language: sentence-based descriptions of nodes and adjacency (e.g., 'Node b is connected to a. Node c is connected to b.').</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based (textual description)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Direct listing of edges or natural-language adjacency statements without additional structural ordering or relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used broadly in LLM graph reasoning/generation prior work (e.g., as prompts for connectivity/cycle/path tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Often evaluated with exact-match accuracy in related works (e.g., GraphQA evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Baseline approach; simple to use for prompting but provides weaker inductive alignment to LLM priors compared to structured linearizations reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not attempt to align token orderings with natural language properties (local dependency, global alignment); ordering is arbitrary which can hurt LLM reasoning; context-window still limits graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper frames these as the common prior practice (used in Fatemi et al., Wang et al., Yao et al.). Structured ordering and relabeling presented in this paper outperform naive raw edge-list/natural-language descriptions in many evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7025.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-PENMAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstract Meaning Representation (AMR) linearization via PENMAN notation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AMR is a graph-based semantic representation for sentences commonly linearized into PENMAN notation for graph‑to‑text tasks and used to fine-tune sequence models (e.g., BART) in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearization of directed acyclic semantic graphs into PENMAN bracketed notation combining node IDs, concept labels, and edge role tokens; used as input/output for graph-to-text models.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical / bracketed sequential representation</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>PENMAN serialization (parenthesized, role-tagged linear sequence); preserves node/edge labels and semantic roles.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR corpora (referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text generation and fine-tuning of seq2seq models (e.g., BART)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (in referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder-decoder model fine-tuned on AMR-linearized data (as reported in related literature cited by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Standard NLG metrics in literature (BLEU/ROUGE) and graph-to-text evaluation; not evaluated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>AMR is specialized for semantic representations of sentences (DAGs) and not general graphs; PENMAN linearization uses structured bracket tokens which differ from simple edge-list tokens and may have different inductive alignment with LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Mentioned as a domain where linearization is naturally derived and successfully used to fine-tune language models; contrasts with the paper's focus on linearizing general graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7025.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GML/GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Modeling Language (GML) and GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two established textual formats for representing graphs: GML (human-readable key–value format) and GraphML (XML-based) used in graph exchange and some graph-to-text contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GML / GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Structured textual graph file formats: GML uses human-readable key-value blocks for nodes/edges; GraphML is XML-based with extensible tags for nodes, edges and attributes. They encode full graph structure and attributes in text files.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical, token‑based (markup)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>File-format serialization of nodes, edges and attributes (GML key-value blocks; GraphML XML nodes/edges).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph representation / interchange formats (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verbose (especially GraphML), not optimized for LLM prompt compactness; paper mentions them as domain-specific representations rather than LLM-optimized linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Mentioned as alternatives in domain-specific use; unlike the paper's proposed compact edge-list ordering, these are general-purpose file formats rather than prompt-focused serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7025.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7025.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphToken</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphToken (continuous learned representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed approach that trains an encoder to produce continuous (non-text) representations of graphs rather than textual tokens, enabling integration with language models via learned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphToken (continuous encoder representations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Learn a continuous embedding/tokenization mapping from graphs to vector tokens (not discrete text) that can be consumed by transformers; contrasts with textual linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>continuous / embedding-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Train an encoder to map graph structures to continuous tokens (learned), rather than serialize as text.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph representation for transformer consumption (referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not a text-based representation — requires training an encoder and is outside the paper's text-serialization focus; mentioned as an alternative direction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Provides continuous representations that avoid some limitations of textual serialization (e.g., token-budget) but requires training encoder modules; contrasted with the paper's text-only approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>GraphQA <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation for sembanking <em>(Rating: 1)</em></li>
                <li>Graph modelling language (GML) <em>(Rating: 1)</em></li>
                <li>GraphToken <em>(Rating: 1)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graph <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7025",
    "paper_id": "paper-273638030",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "EdgeList-Order",
            "name_full": "Edge-list linearization with centrality/degeneracy ordering",
            "brief_description": "Represents a graph as a sequence of bracketed edge pairs (v, u) where edges are ordered by a node-centric ranking (descending centrality or core number) and edges incident to each ranked node are listed; optional tie-breaking is random.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Ordered edge-list (Degree / PageRank / CoreNumber)",
            "representation_description": "Each edge is serialized as a bracketed pair (v, u) and the full graph is the sequence of such pairs. Nodes are first ranked (by degree, PageRank, or core number) and the edges incident to each ranked node are appended in that node order; ties in ranking are broken randomly. Prompts append a task-specific question after the linearized edge list.",
            "representation_type": "sequential, token‑based",
            "encoding_method": "Node-centric ordering by descending centrality (Degree, PageRank) or degeneracy (CoreNumber); for each node in ranking, list its incident edges (edge pairs), ties randomized.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "GraphWave; GraphQA",
            "task_name": "Graph-level and node-level reasoning tasks (Node Counting, Node Degree, Max Degree, Edge Existence, Path Existence, Shortest Path, Diameter, Motif Shape Classification)",
            "model_name": "Llama 3 Instruct (primary: 8B); ablations with Llama 3 70B and Qwen 2.5 14B-1M",
            "model_description": "Decoder-only large language models used in inference-only experiments (Llama 3 Instruct 8B: main model, deterministic sampling settings; Llama 3 70B: larger-size ablation; Qwen 2.5 14B-1M: large-context ablation).",
            "performance_metric": "Exact match accuracy (percentage of exact-correct outputs)",
            "performance_value": "Across GraphWave and GraphQA, ordered edge-list variants consistently outperform a fully random edge-list baseline. Reported specific gains include: Degree+node-relabeling improved maximum node degree estimation by ~35% and shortest path by ~13% on GraphQA; Degree+relabeling improved Edge Existence by ~26% on GraphWave. Exact per-task table values are reported in the paper's Tables 1–3.",
            "impact_on_training": "Not used for model training; used at inference/prompting time. The representation improved downstream reasoning accuracy in zero-shot and one-shot prompting compared to random ordering, indicating better alignment with LLM priors without fine-tuning.",
            "limitations": "Relies on ordering heuristics with non-deterministic tie-breaking (not fully canonical); input token cost grows with number of edges (context-window limited); global properties (e.g., diameter) remain challenging despite ordering; evaluated only on synthetic datasets in this work.",
            "comparison_with_other": "Outperforms fully random edge-list baseline. Degree and PageRank orderings are most consistent across tasks; CoreNumber (degeneracy) performs particularly well on edge-centric tasks. Pseudo-random/default generator orderings can be competitive on some generators (see pseudo-random experiments). Linegraph-based variants (edge-as-node) can outperform node-based orderings on edge-specific tasks but have lower overall average performance.",
            "uuid": "e7025.0",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "NodeRelabel",
            "name_full": "Node relabeling by ranking index",
            "brief_description": "After ranking nodes by centrality/degeneracy, original node identifiers are replaced with their positions in the ranking (e.g., highest -&gt; 0) to align token positions across graphs and improve global alignment for LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relabeled edge-list (rank-index labels)",
            "representation_description": "Same edge-list serialization as above, but node identifiers are replaced by their rank indices from the chosen ordering (so node 0 is the highest-ranked node in every graph). This is applied prior to forming the textual sequence.",
            "representation_type": "sequential, token‑based (label-canonicalization attempt)",
            "encoding_method": "Compute ranking (Degree, PageRank, CoreNumber) then replace original labels by rank positions; serialize edges as (rank_v, rank_u) in ordered sequence.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "GraphWave; GraphQA",
            "task_name": "Same graph reasoning tasks as ordered edge-list (Node Counting, Node Degree, Max Degree, Edge/Path existence, Shortest Path, Diameter, Motif classification)",
            "model_name": "Llama 3 Instruct (8B primary); also tested with Llama 3 70B",
            "model_description": "Decoder-only LLMs used in prompting experiments; node relabeling was applied to inputs prior to prompting.",
            "performance_metric": "Exact match accuracy",
            "performance_value": "Node relabeling further improves ordered edge-list performance across nearly all tasks. Paper reports combined Degree-based ordering + node relabeling yields the largest gains (e.g., ~35% improvement on max degree estimation on GraphQA; ~26% boost on edge existence in GraphWave when combined with degree ordering).",
            "impact_on_training": "Not applied during training; at inference the relabeling improves cross-sample alignment and raises zero-/one-shot accuracy without model updates.",
            "limitations": "Relabeling depends on the chosen ranking metric and still requires tie-breaks (random) in the implementation, so it is not fully canonical; benefits depend on task and ordering choice; does not remove the context-window limitation.",
            "comparison_with_other": "Ordering plus relabeling outperforms ordering without relabeling and the random baseline. Relabeling makes sequences more globally aligned across graphs, better matching LLM training priors.",
            "uuid": "e7025.1",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LineGraph-Lin",
            "name_full": "Linegraph-based linearization (edges-as-nodes)",
            "brief_description": "Transforms the original graph to its linegraph L(G) so that original edges become nodes; the same ordering and serialization pipeline is then applied to L(G) to capture edge-to-edge relationships in the textual sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linegraph linearization (LG{*})",
            "representation_description": "Convert G to linegraph L(G): each original edge becomes a node in L(G); two nodes in L(G) connect if their corresponding edges in G are incident. The authors then apply node-ranking and edge-list serialization to L(G) so the resulting token sequence reflects edge adjacencies.",
            "representation_type": "sequential, token‑based (edge-centric representation)",
            "encoding_method": "Linegraph transformation followed by the same ranking (Degree/PageRank/CoreNumber computed on L(G)) and sequential serialization of incident relations (now edge-as-node pairs).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "GraphWave; GraphQA",
            "task_name": "Edge-centric reasoning tasks (Edge Existence, Path/Connectivity) and general graph reasoning tasks",
            "model_name": "Llama 3 Instruct (8B) and ablations",
            "model_description": "Decoder-only LLMs used with linegraph-transformed textual inputs.",
            "performance_metric": "Exact match accuracy",
            "performance_value": "Linegraph-based methods have a lower overall average accuracy than the best node-based linearizations but generally perform better in edge-based tasks such as edge existence and path reasoning (paper reports improved edge/task-specific scores for LG{*} variants; exact per-task numbers are in paper tables).",
            "impact_on_training": "Used at inference; helps capture inter-edge dependencies leading to improved performance on edge-centric tasks without model fine-tuning.",
            "limitations": "Overall average performance lower than best node-based methods; adds a preprocessing transform and can increase serialized size/complexity; still subject to context-window token limits.",
            "comparison_with_other": "Compared to node-based ordered edge-lists, linegraph variants specialize for edge-oriented tasks and capture edge-to-edge relationships better, but underperform on many node-focused tasks and in overall averages.",
            "uuid": "e7025.2",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PseudoRandomDefaultOrder",
            "name_full": "Generator default / pseudo-random edge ordering",
            "brief_description": "Uses the edge order as produced by the graph generator (procedural construction order) as the serialized edge-list; this ordering sometimes implicitly encodes structural construction information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Default generator edge ordering (pseudo-random linearization)",
            "representation_description": "Serialize edges in the exact order they were output by the graph generation process (no additional reordering or relabeling). The paper calls this 'pseudo-random' because many generators produce structurally informative sequences.",
            "representation_type": "sequential, token‑based",
            "encoding_method": "Use the existing edge output order from the graph generator without reordering; optionally combined with default labels or structured node relabeling.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "GraphWave; GraphQA (evaluated in Appendix A)",
            "task_name": "Same set of graph reasoning tasks",
            "model_name": "Llama 3 Instruct (8B)",
            "model_description": "Decoder-only LLM used for inference; no additional preprocessing beyond using generator ordering.",
            "performance_metric": "Exact match accuracy",
            "performance_value": "Default generator ordering performs competitively and in some cases significantly better than fully random ordering; on GraphQA default ordering often significantly outperforms random ordering across many tasks (paper Appendix A provides detailed comparisons).",
            "impact_on_training": "No training impact; using generator ordering can boost inference-time accuracy when the ordering encodes construction-related structure.",
            "limitations": "Requires access to generator-specific ordering behavior which may not be available for real-world graphs; performance varies with the generator and can be dataset-dependent.",
            "comparison_with_other": "Sometimes outperforms structured ordering; however, structured orderings with relabeling often yield consistent improvements across datasets and tasks when generator ordering is not available or meaningful.",
            "uuid": "e7025.3",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RawEdgeList/NL",
            "name_full": "Raw edge-list or natural language graph descriptions",
            "brief_description": "Simple graph-to-text approaches that either list edges as pairs or describe the graph in natural language sentences (e.g., adjacency statements) prior to a task prompt; used commonly in prior LLM-for-graph work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Raw edge-list / natural language description",
            "representation_description": "Two common baseline forms: (1) Raw edge-list: textual serialization of edges (e.g., '[(b,a),(c,b),(b,d)]'); (2) Natural language: sentence-based descriptions of nodes and adjacency (e.g., 'Node b is connected to a. Node c is connected to b.').",
            "representation_type": "sequential, token‑based (textual description)",
            "encoding_method": "Direct listing of edges or natural-language adjacency statements without additional structural ordering or relabeling.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "",
            "task_name": "Used broadly in LLM graph reasoning/generation prior work (e.g., as prompts for connectivity/cycle/path tasks)",
            "model_name": "",
            "model_description": "",
            "performance_metric": "Often evaluated with exact-match accuracy in related works (e.g., GraphQA evaluations)",
            "performance_value": null,
            "impact_on_training": "Baseline approach; simple to use for prompting but provides weaker inductive alignment to LLM priors compared to structured linearizations reported in this paper.",
            "limitations": "Does not attempt to align token orderings with natural language properties (local dependency, global alignment); ordering is arbitrary which can hurt LLM reasoning; context-window still limits graph size.",
            "comparison_with_other": "Paper frames these as the common prior practice (used in Fatemi et al., Wang et al., Yao et al.). Structured ordering and relabeling presented in this paper outperform naive raw edge-list/natural-language descriptions in many evaluated tasks.",
            "uuid": "e7025.4",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AMR-PENMAN",
            "name_full": "Abstract Meaning Representation (AMR) linearization via PENMAN notation",
            "brief_description": "AMR is a graph-based semantic representation for sentences commonly linearized into PENMAN notation for graph‑to‑text tasks and used to fine-tune sequence models (e.g., BART) in prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "AMR PENMAN linearization",
            "representation_description": "Linearization of directed acyclic semantic graphs into PENMAN bracketed notation combining node IDs, concept labels, and edge role tokens; used as input/output for graph-to-text models.",
            "representation_type": "hierarchical / bracketed sequential representation",
            "encoding_method": "PENMAN serialization (parenthesized, role-tagged linear sequence); preserves node/edge labels and semantic roles.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR corpora (referenced works)",
            "task_name": "Graph-to-text generation and fine-tuning of seq2seq models (e.g., BART)",
            "model_name": "BART (in referenced prior work)",
            "model_description": "Pretrained encoder-decoder model fine-tuned on AMR-linearized data (as reported in related literature cited by the paper).",
            "performance_metric": "Standard NLG metrics in literature (BLEU/ROUGE) and graph-to-text evaluation; not evaluated in this paper",
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "AMR is specialized for semantic representations of sentences (DAGs) and not general graphs; PENMAN linearization uses structured bracket tokens which differ from simple edge-list tokens and may have different inductive alignment with LLM pretraining.",
            "comparison_with_other": "Mentioned as a domain where linearization is naturally derived and successfully used to fine-tune language models; contrasts with the paper's focus on linearizing general graphs.",
            "uuid": "e7025.5",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GML/GraphML",
            "name_full": "Graph Modeling Language (GML) and GraphML",
            "brief_description": "Two established textual formats for representing graphs: GML (human-readable key–value format) and GraphML (XML-based) used in graph exchange and some graph-to-text contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GML / GraphML",
            "representation_description": "Structured textual graph file formats: GML uses human-readable key-value blocks for nodes/edges; GraphML is XML-based with extensible tags for nodes, edges and attributes. They encode full graph structure and attributes in text files.",
            "representation_type": "hierarchical, token‑based (markup)",
            "encoding_method": "File-format serialization of nodes, edges and attributes (GML key-value blocks; GraphML XML nodes/edges).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "",
            "task_name": "Graph representation / interchange formats (mentioned in related work)",
            "model_name": "",
            "model_description": "",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Verbose (especially GraphML), not optimized for LLM prompt compactness; paper mentions them as domain-specific representations rather than LLM-optimized linearizations.",
            "comparison_with_other": "Mentioned as alternatives in domain-specific use; unlike the paper's proposed compact edge-list ordering, these are general-purpose file formats rather than prompt-focused serializations.",
            "uuid": "e7025.6",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GraphToken",
            "name_full": "GraphToken (continuous learned representations)",
            "brief_description": "A previously proposed approach that trains an encoder to produce continuous (non-text) representations of graphs rather than textual tokens, enabling integration with language models via learned embeddings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GraphToken (continuous encoder representations)",
            "representation_description": "Learn a continuous embedding/tokenization mapping from graphs to vector tokens (not discrete text) that can be consumed by transformers; contrasts with textual linearizations.",
            "representation_type": "continuous / embedding-based",
            "encoding_method": "Train an encoder to map graph structures to continuous tokens (learned), rather than serialize as text.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph representation for transformer consumption (referenced prior work)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Not a text-based representation — requires training an encoder and is outside the paper's text-serialization focus; mentioned as an alternative direction.",
            "comparison_with_other": "Provides continuous representations that avoid some limitations of textual serialization (e.g., token-budget) but requires training encoder modules; contrasted with the paper's text-only approach.",
            "uuid": "e7025.7",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "GraphQA",
            "rating": 2
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        },
        {
            "paper_title": "Abstract Meaning Representation for sembanking",
            "rating": 1,
            "sanitized_title": "abstract_meaning_representation_for_sembanking"
        },
        {
            "paper_title": "Graph modelling language (GML)",
            "rating": 1,
            "sanitized_title": "graph_modelling_language_gml"
        },
        {
            "paper_title": "GraphToken",
            "rating": 1,
            "sanitized_title": "graphtoken"
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graph",
            "rating": 1,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graph"
        }
    ],
    "cost": 0.01659975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Linearization Methods for Reasoning on Graphs with Large Language Models
25 Jun 2025</p>
<p>Christos Xypolopoulos 
Ecole Polytechnique</p>
<p>NTUA</p>
<p>Guokan Shang guokan.shang@mbzuai.ac.ae 
MBZUAI</p>
<p>Xiao Fei 
Ecole Polytechnique</p>
<p>Giannis Nikolentzos 
University of Peloponnese</p>
<p>Hadi Abdine 
MBZUAI</p>
<p>Iakovos Evdaimon 
Ecole Polytechnique</p>
<p>Michail Chatzianastasis 
Ecole Polytechnique</p>
<p>Giorgos Stamou 
NTUA</p>
<p>Michalis Vazirgiannis 
Ecole Polytechnique</p>
<p>MBZUAI</p>
<p>Graph Linearization Methods for Reasoning on Graphs with Large Language Models
25 Jun 20252CC22D4344FF04552A8C2B94CC202D66arXiv:2410.19494v3[cs.CL]
Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph reasoning tasks.The key question, therefore, is how to transform graphs into linear sequences of tokens-a process we term "graph linearization"-so that LLMs can handle graphs naturally.We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs.To achieve this, we developed several graph linearization methods based on graph centrality and degeneracy.These methods are further enhanced using node relabeling techniques.The experimental results demonstrate the effectiveness of our methods compared to the random linearization baseline.Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multimodal processing using a unified transformer model.</p>
<p>Introduction</p>
<p>Transformer-based large pre-trained models have revolutionized machine learning research, demonstrating unprecedented performance across diverse data modalities and even a mixture of modalities, including image, audio, and text domains (Xu et al., 2023;Yin et al., 2023).In particular, large language models (LLMs) have shown promising results in arithmetic, symbolic, and logical reasoning tasks (Hendrycks et al., 2020).Despite their success, the adaptation for processing graphs-an ubiquitous data structure that encapsulates rich structural and relational information-remains a comparably emerging and underdeveloped research direction, even if it has recently been gaining attention (Ye et al., 2023;Fatemi et al., 2023;Wang et al., 2024).This asymmetry is due in large part to the inherent challenge of representing graphs as sequential tokens, in a manner conducive to the language modeling objectives typical of transformers, a challenge not encountered when dealing with the other modalities.This unique problem has encouraged us to investigate a key question: How can we represent graphs as linear sequences of tokens for transformers in a suitable way?We refer to this research endeavor as Graph Linearization.</p>
<p>Existing methods of using LLMs for graph machine learning tasks, such as graph reasoning and graph generation, represent entire graphs as either raw edge lists or natural language descriptions that adhere to adjacency matrices without any special treatment (Fatemi et al., 2023;Wang et al., 2024;Yao et al., 2024).For example, a natural language description of the star graph S might be: "An undirected graph with nodes a, b, c, and d.Node b is connected to a. Node c is connected to b. Node b is connected to d.", its equivalent edge list representation is: "[(b, a), (c, b), (b, d)]".Either of the linearized representations is then appended with a task-specific question to form an LLM query prompt, e.g., "Is there a cycle in this graph?".Other studies focus solely on node-level tasks (Zhao et al., 2023;Ye et al., 2023), centering the linearization around an ego-subgraph for a target node (Hamilton et al., 2017), where the neighboring graph structure and node features up to k-hop are described in the prompt.However, there is a lack of studies investigating how to maintain the integrity of graph structures while efficiently transforming them into sequences suitable for LLMs.</p>
<p>Our research addresses this limitation.By relying on edge list representations as exemplified above, we study the performance impact on LLMs of various methods to order the edges in the list and rename interchangeable node labels, as shown in Figure 1.We argue that if the linearization of  {4: 3, 7: 3, 6: 3, 5: 3, 0: 2, 1: 2, 3: 2, 2: 2} (4, 5), (4,6), (4, 7), (4, 3), (5, 7), (5,6), (6,7), (3,2) ,  (3, 0),  (2, 1),  (1, 0 Given an input graph G, we rank its nodes based on their degree and then explore the edges in that order.The resulting linearized graph is then combined with a task-specific prompt and passed into a LLM.</p>
<p>graphs is conducted in a meaningful way, capturing properties similar to those found in natural language such as local dependency and global alignment, it will benefit contemporary LLMs by enhancing their ability to understand graphs, as they are trained on trillions of textual tokens.We define local dependency as the ability to predict the next (missing) token based on the previous (surrounding) context, as seen when a sequence of tokens naturally progresses in text, analogously to the distributional hypothesis of language (Joos, 1950;Harris, 1954;Firth, 1957).Global alignment involves starting sequences from tokens with similar characteristics, aligning the sequences to mimic how text samples begin or end with common words like "The" or "In conclusion" respectively.Addressing this question helps identify LLM-suitable representations for graphs that potentially align with natural languages, unlocking new insights and applications in fields where graphs naturally represent data.Graph linearization paves the way for extending transformer models' capabilities to graph data and unifying various graph tasks across domains, serving as a fundamental step toward building successful large graph models.This also facilitates the integration of graph learning with the multi-modal processing trend and the development of cohesive AI systems using a unified transformer model.</p>
<p>In this work, we developed multiple general graph linearization methods that leverage graph centrality and degeneracy, enriched by node relabeling techniques to achieve the aforementioned natural language properties.We conducted a series of inference experiments on various graph reasoning tasks, which form the basis for a deeper understanding of graph structures.Experimental results using Llama 3 models (Dubey et al., 2024) on synthetic datasets demonstrate the effectiveness of our methods compared to the random linearization baseline.Our key findings are as follows:</p>
<p>• Edge ordering matters -Structured graph linearization improves the graph reasoning capabilities of LLMs.</p>
<p>• Node labels contribute significantly -Node labels based on graph features further improve performance.</p>
<p>• Task-specific linearizations -Node-based and edge-based linearizations perform better in respective tasks.</p>
<p>Related Work</p>
<p>In this section, we first introduce previous efforts to use transformers for graph machine learning tasks.We then discuss the current trend of using modern LLMs for graph reasoning and generation.Finally, we explore linearization methods for graphs especially from specific domains.</p>
<p>Transformers for graph machine learning</p>
<p>Transformers (Vaswani et al., 2017) have been successfully applied in various domains beyond text, demonstrating both versatility and effectiveness (Devlin et al., 2018).For instance, the Vision Transformer (Dosovitskiy et al., 2020) has achieved remarkable performance in image classification tasks by treating images as sequences of patches, marking a shift from traditional CNN-based approaches.Similarly, transformers have been used in speech recognition.Models like the Speech Transformer (Dong et al., 2018) apply self-attention mechanisms to process audio data as sequences, outperforming traditional RNN-based methods.</p>
<p>These successes have spurred interest in using modified transformers to replace the de facto GNNbased approaches for graph machine learning tasks.Notably, Graphormer (Ying et al., 2021) enables transformer to effectively capture the dependencies and relationships within a graph.It achieves this by integrating node centrality encoding and attention biases that account for the spatial distance between nodes.Graph Transformer (GT) (Dwivedi and Bresson, 2020) generalizes the transformer architecture for graph representation learning.GT introduces the concept of relative positional encodings to account for the pairwise distances between nodes in a graph.This approach allows the model to learn rich node representations that capture both local and global graph structures.</p>
<p>In contrast to the above works, Kim et al. (2022) show that by treating all nodes and edges as independent tokens and inputting them into a standard Transformer encoder without any graph-specific modifications, notable outcomes can be achieved both theoretically and practically.Results on molecular graphs for quantum chemical property prediction show that this approach outperforms all GNN baselines and achieves competitive performance compared to graph Transformer variants.Despite not applying any structural alterations to the Transformer, this approach still requires sophisticated token-wise node and edge embeddings to explicitly represent the connectivity structure.</p>
<p>LLMs for graph reasoning</p>
<p>Following the recent success of LLMs in tasks beyond language processing (Hendrycks et al., 2020), several studies have explored the capacity of offthe-shelf LLMs for graph reasoning.While there is no clear consensus on the specific tasks, models are tested on understanding basic topological properties, such as graph size, node degree and connectivity, which form the foundation for a deeper understanding of graph structures (Zhang et al., 2023b).Using various prompting methods, these studies show that LLMs, even without fine-tuning, demonstrate preliminary graph reasoning abilities.</p>
<p>Several studies have evaluated LLMs for graph reasoning at both the node and graph levels.For example, NLGraph (Wang et al., 2024) covers eight graph reasoning tasks of varying complexity, ranging from simple tasks like connectivity and shortest path to complex problems like maximum flow and simulating graph neural networks.This work also proposes two graph-specific prompting methods that achieve notable performance improvements.GraphQA (Fatemi et al., 2023) focuses on relatively simple tasks to measure the performance of pre-trained LLMs in edge existence, node degree, node count, edge count, connected nodes, and cycle checks.It shows that larger models generally perform better on graph reasoning, with graphs generated synthetically using various graph generators.Similar works include various studies that explore graph reasoning using different LLMs, prompting techniques, graph tasks, domains, and evaluation approaches (Chen et al., 2023;Guo et al., 2023;Zhang et al., 2023a;Hu et al., 2023;Huang et al., 2024;Liu and Wu, 2023;Das et al., 2023;Yuan et al., 2024;Wu et al., 2024;Skianis et al., 2024).</p>
<p>Another line of research criticizes the above approach, arguing that solely using prompt engineering or in-context learning with frozen LLMs hinders achieving top performance in downstream graph tasks.Therefore, instruction-tuning or finetuning is necessary.The work of Ye et al. (2023) preliminarily confirms this on the multi-class node classification task.A prompt template is designed to describe both the neighbor graph structure and node features centered around a target node up to the 3-hop level.Similarly, Zhao et al. (2023) draw inspiration from linguistic syntax trees.For a target node, the work converts its ego-subgraph into a graph syntax tree with branches describing the neighborhood's "label" and "feature", which are then encoded as text in the prompt.Results show that instruction-tuning performs much better than in-context learning, and is on par with GNN-based models.Similar conclusions can be observed in recent studies on graph-level reasoning settings (Luo et al., 2024).Finally, Perozzi et al. (2024) introduce GraphToken, which trains an encoder to create continuous representations, rather than converting graphs into text tokens.</p>
<p>LLMs for graph generation</p>
<p>Graph generation involves creating graphs with specific properties, a process that holds significant real-world value in areas like drug discovery.This task is more challenging than graph reasoning.</p>
<p>To the best of our knowledge, Yao et al. (2024) was the first to show the preliminary abilities of LLMs in graph generation.The work experimented with graph generation in three settings: 1) Rulebased: generating graphs of basic structure types, given rules describing the desired structures, e.g., trees, cycles, etc.; 2) Distribution-based: generating graphs following a structural type distribution p, given a set of example graphs with the same distribution; 3) Property-based: generating molecule structures with specific properties, given example molecules in SMILES format.Results show that LLMs have reasonably good abilities across the three tasks, and advanced prompting techniques do not necessarily lead to better performance.comment Graph generation is critical for applications like drug discovery but presents greater challenges than graph reasoning.(Yao et al., 2024) provide early insights into LLM-driven graph generation, evaluating rule-based, distribution-based, and property-based approaches.Their findings suggest that while LLMs can generate structurally meaningful graphs, advanced prompting does not always yield better performance, highlighting the need for more robust generation strategies.</p>
<p>Linearization for specific graphs</p>
<p>In deriving an ordering for graphs, topological sorting in graph theory examines the linear ordering of directed acyclic graphs, such that for every directed edge (u, v), u precedes v in the ordering.However, such graph traversal is node-centric, making edge information not encoded.</p>
<p>In other domains involving specific types of graphs, such as discourse graphs-a directed weakly connected graph reflecting discourse structure-nodes represent utterances, and edges represent discourse relations (e.g., elaboration, clarification, completion) within a conversation (Rennard et al., 2024).The work of Chernyavskiy et al. (2024) proposes a linearization method for discourse graphs that arranges utterances chronologically, assigning unique identifiers to speakers, utterances, and addressees.It incorporates discourse relations and sentiment tokens to generate a structured sequence, using special tokens for clarity.This structured sequence is then used to train a BART (Lewis et al., 2020) for dialogue generation.Similarly, Abstract Meaning Representation (AMR) uses directed acyclic graphs to provide a structured semantic representation of language, incorporating semantic roles with annotated arguments and values where nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013).AMR corpora are usually linearized using the PENMAN-based notation (Patten, 1993) as in the work of (Ribeiro et al., 2021) and (Hoyle et al., 2021) to fine-tune pre-trained language models to perform graph-to-text generation.For citation networks, Guo et al. ( 2023) have explored the Graph Modelling Language (GML) and Graph Markup Language (GraphML) for graph representation (Himsolt, 1997;Brandes et al., 2013).GML is a simple, human-readable format, while GraphML is XML-based and offers extensibility for complex applications.</p>
<p>The scope of our work.Unlike the above linearization methods limited to specific types of graphs, where linearization can be naturally derived to some extent, we focus on general graphs.Furthermore, unlike previous works using LLMs for reasoning and generation tasks, where edge lists are directly leveraged without special treatment, we introduce various linearization methods for ordering the edges in the list and renaming interchangeable node labels to make them suitable for LLMs.Although our work involves only graph reasoning experiments, our graph linearization methods are general and applicable to various scenarios.This allows for the effective transformation of graph structures into sequences suitable for language models and has the potential to improve performance in both reasoning and generation tasks, with or without fine-tuning.</p>
<p>Graph Linearization Methods</p>
<p>This section describes the graph linearization approach, emphasizing the use of graph features to enhance graph reasoning with LLMs.</p>
<p>Generally speaking, we define graph linearization as the process of representing graphs as linear sequences of tokens.In this work, we aim to identify the linearization approaches that will benefit LLMs by enhancing their ability to understand graphs.We argue that linearized graphs, represented as sequences of tokens, should capture properties similar to those in natural language, given the fact that LLMs are pre-trained on trillions of textual tokens.Such properties should include local dependency and global alignment.</p>
<p>Local dependency refers to the ability to predict the next (missing) token based on the previous (surrounding) context, within the token sequence of a single linearized graph.This property is analogous to the fundamental distributional hypothesis of language (Joos, 1950;Harris, 1954;Firth, 1957), which states that words that occur in similar contexts tend to have similar meanings (or functions).This hypothesis suggests that given a new word, one should be able to figure out its meaning based on the contexts in which it is used.In fact, the masked and casual language modeling for training encoder-only (Devlin et al., 2019) and decoder-only (Radford et al., 2019) language models, can be seen as instantiations of this hypothesis.</p>
<p>Global alignment refers to the alignment across the token sequences of different linearized graphs, ensuring that the corresponding tokens of both sequences match across their full length.This property takes into account the overall structure of the sequences, reflecting the typical flow of text, where common words like "The" or "In conclusion" are used at the start or end of a sequence, guiding the alignment.For example, linearization should always start from the node with the highest degree, and such nodes are all relabeled as index 0 for different graphs.</p>
<p>By relying on edge list, which defines a graph in terms of its individual connections, we study the performance impact on LLMs of various methods for ordering the edges in the list and renaming interchangeable node labels, as shown in Figure 1.More specifically, we leverage the advances in graph degeneracy and centrality as detailed in below to meet the local dependency property.</p>
<p>Graph Degeneracy (Seidman, 1983).Let G(V, E) be an undirected graph with n = |V | nodes and m = |E| edges.A k-core of G is a maximal subgraph of G in which every node v has at least degree k.The k-core decomposition of G forms a hierarchy of nested subgraphs whose cohesiveness and size increase and decrease, respectively, with k.Higher-level cores can be viewed as filtered versions of the graph that capture the most significant structural information.The Core Number of a node is the highest order of a core that contains the node.</p>
<p>Graph Centrality.Graph centrality is a key concept in network analysis used to determine the influence or importance of nodes based on the structure of the graph.We consider two types of centrality, one based on the Degree of nodes and one based on PageRank.Degree centrality is one of the simplest measures of node importance (Freeman, 1978).It is defined as the number of edges incident to a node, making it a measure of local centrality that reflects the node's immediate connectivity within the graph.For an undirected graph G with n = |V | nodes and m = |E| edges, the degree centrality D(v) of a vertex v ∈ V is calculated as D(v) = u∈V A vu where A is the adjacency matrix of the graph, and A vu = 1 if there is an edge between vertices v and u, and 0 otherwise.</p>
<p>PageRank is a more sophisticated centrality measure, originally developed by Brin and Page (1998) for ranking web pages.It extends the concept of degree centrality by considering not only the number of links a node has but also the importance of the nodes linking to it.PageRank effectively captures the notion that connections from highlyranked nodes contribute more to the ranking of a given node than connections from low-ranked nodes (Page et al., 1999).PageRank, although designed for directed graphs, can also be adapted for undirected ones by treating all edges as bidirectional.The PageRank centrality P R(v) of a node v ∈ V is computed iteratively using the formula:
P R(v) = 1−α |V | + α u∈N (v) P R(u) deg + (u)
where α is a damping factor typically set to 0.85, N (v) represents the set of nodes linking to v, and deg + (u) is the out-degree of node u.</p>
<p>Graph Linearization Implementation.Our approach to capitalizing on the local dependency property involves the following steps.Given a graph G, we initially rank the nodes by the centrality and degeneracy measures described previously.Then, we begin exploring the nodes by descending order and list the edges connected to it, arranging them in a random order.Each edge is represented as a node pair.In the case where two or more nodes share an equal value, the order is selected randomly.After the ordering process has concluded, each edge list constitutes a sequence of tokens following a descending order of node importance.</p>
<p>In addition to linearization methods, node relabeling is employed as a means to attempt the attainment of the global alignment property.Specifically, node relabeling introduces an additional step to our procedure.After ranking the nodes, their original labels are replaced with their respective positions in the ranking.Consequently, the node with index 0 corresponds to the one with the highest core number, and so forth.This approach may prove advantageous for the LLM by ensuring a consistent association between node indices and their respective importance properties.</p>
<p>Finally, we conducted experiments in which the edges were ordered directly, rather than the nodes.This allows our linearization to directly capture relationships between edges, which can be essential for understanding complex graph structures.To achieve this, each graph was transformed into its corresponding linegraph representation.A linegraph L(G) of a graph G is the graph where each edge of G is replaced by a node, and where two edges of G are connected in L(G) if they are incident in G. Subsequently, the previously described processes were applied directly to L(G).</p>
<p>Experimental Setup</p>
<p>In this section, we provide a detailed overview of the experimental setup, including the methodologies, resources, and evaluation frameworks used in our experiments.</p>
<p>Datasets</p>
<p>To better explore the capabilities of LLMs on graph reasoning tasks, we utilized two synthetic datasets.</p>
<p>GraphWave.First, we constructed a synthetic graph dataset using GraphWave (Donnat et al., 2018).This graph generator was originally developed for controlled experimentation and evaluation of node embedding techniques and measurement of structural equivalence on graphs with known network motifs.Motifs, in the context of network science, are sub-graphs that repeat themselves either within the same graph or across different graphs.For our analysis, these structurally similar sub-graphs enable us to construct structurerelated tasks, that allow us to assess the ability of language models to analyze and infer structural features within these graphs.</p>
<p>The generator operates by sequentially constructing a base graph, that follows either a cycle or chain structure, and then attaching a number of motifs that follow predetermined shapes-cliques, stars, fans, diamonds, and trees.To cover a wider variety of structural complexities, we also include all combinations of two shapes, along with all combinations of three shapes with unique shapes per triplet.Example graphs can be found in Figure 2.</p>
<p>For each combination of shapes, we generated 100 graphs, leading to a total of 3000 graphs.This ensures sufficient variance in graph sizes and in the combinations of base and motif sub-graphs.The number of nodes in each graph shape is selected randomly, with the following constraints: base (3-21 nodes); clique, fan, and star (4-11 nodes); diamond (6 nodes); and tree (perfect binary trees with 3-6 levels).The generated dataset contains an average of 32.33 nodes and 43.72 edges per graph.</p>
<p>GraphQA.Furthermore, we included GraphQA (Fatemi et al., 2023), which is widely used to evaluate the graph reasoning capabilities of LLMs.Similar to GraphWave, the dataset consists of randomly generated graphs derived from various graph gen-erators, including Erdős-Rényi graphs (Erdős and Rényi, 1959), scale-free networks (SFN) (Barabási and Albert, 1999), the Barabási-Albert model (BA) (Albert and Barabási, 2002), and the stochastic block model (SBM) (Holland et al., 1983), along with star, path, and complete graph generators.A total of 500 graphs were sampled for ER, BA, SFN, and SBM models, whereas 100 graphs were sampled for path, complete, and star graphs due to their lower structural variability.All generated graphs contained between 5 and 20 nodes.</p>
<p>Tasks</p>
<p>Our experiments encompass a series of graph reasoning tasks, which can be broadly categorized into classification-based and numerical tasks.Numerical tasks, ranging in difficulty, require the model to produce a numerical output, either through structural computation or counting-based inference.This combination of tasks allows us to comprehensively evaluate LLMs' understanding of structural features and examine how different edge list orderings affect their performance.</p>
<p>A fundamental task is Node Counting, where the LLM estimates the number of nodes.In Node Degree calculation, the LLM determines the degree of a node.A more advanced variant, Maximum Degree calculation, requires the LLM to internally calculate the degree of all nodes and then identify the maximum among them.</p>
<p>Beyond node-related tasks, we assess the ability to infer relational properties.In Edge Existence and Path Existence tasks the LLM is given a randomly selected pair of nodes and must determine whether an edge or a connecting path exists between them, respectively.In the Shortest Path task, the model must compute the length of the shortest path between two given nodes, requiring a deeper understanding of graph connectivity.The Diameter Estimation task requires the model to determine the longest shortest path in the graph, showcasing global graph structure understanding.</p>
<p>Table 2: Accuracy scores for a subset of tasks on the GraphWave dataset using Llama 3 70B as an ablation study.Notations remain the same as in Table 1.</p>
<p>Finally, we evaluated Motifs' Shape classification, a dataset-specific task leveraging Graph-Wave's embedded structures, where the LLM is given definitions of the five motif types and asked is to identify which is present.</p>
<p>In every prompt, a node v is represented by an incremental integer, while an edge between nodes v and u is denoted by the bracketed pair (v, u).An edge list is expressed as a sequence of edges, sorted according to the scheme used in each linearization method.We tested both zero-shot and one-shot approaches, where a randomly selected graph from the dataset was used consistently as the one-shot example across all experiments.The prompt templates are provided in Appendix D.</p>
<p>LLMs</p>
<p>We used the 8B parameter Llama 3 Instruct (Dubey et al., 2024) with a temperature of 1e−3 and a sampling parameter of 1e−1 for more deterministic outputs to assess sensitivity to our linearization methods.Experiments were conducted on an NVIDIA A5000.Additionally, we conducted partial experiments with the 70B-parameter model to evaluate its impact on graph reasoning capabilities and to verify the consistency of our approach.Finally, to explore the impact of model family differences, we also include experiments over Qwen 2.5 14B-1M (Yang et al., 2025) in the Appendix A.</p>
<p>Baselines</p>
<p>For our comparisons, we consider only a random baseline.This baseline involves a fully random ordering of the edge list, where edges are arranged without following any inherent scheme.To further eliminate structural biases, we also randomly shuffle the node labels.This baseline is founded on the fact that we are working with general graphs, where default labels or ordering are neither predetermined nor necessarily provide meaningful information in real-world applications.In addition, to mitigate the risk of skewed results, we applied five different random orderings and averaged their performance.Our random ordering can be considered comparable to prior studies, which tend to preserve the inherent structure of the generator.</p>
<p>Evaluation</p>
<p>We use exact accuracy to compare our methods, measuring the ratio of correct predictions as 1 n n i=1 I (y i = ŷi ), where n is the number of graphs, y i the correct answer, and ŷi the LLM's response.For numerical tasks, we consider a result Table 3: Accuracy scores for all tasks on the GraphQA dataset using Llama 3 8B, , including the overall average.Notations remain the same as in Table 1.</p>
<p>accurate only if it is an exact match.For the motifs' shape classification task, accuracy reflects the total across all shapes, requiring the predicted shape to appear at least once in the graph.</p>
<p>Experimental Results Analysis</p>
<p>In this section, we review the performance of our linearization methods across various tasks.</p>
<p>Tables 1 and 3 present the performance of our methods on the GraphWave and GraphQA datasets with the Llama 3 8B model.Additionally, to evaluate model scale, we report results for selected GraphWave tasks using the 70B model in Table 2.The results are organized into three groups: one where node labels in the linearized graphs are randomly assigned, ensuring a fair comparison since, in real-world graphs, labels might be arbitrary; another where node labels are reindexed according to each method, as described in Section 3; and finally, a comparison against the random baseline.The performance related to the pseudo-random (default) node labels originally provided by synthetic graph generators is discussed in Appendix A.</p>
<p>Overall, across both datasets, our linearization methods consistently outperform the random baseline, highlighting the critical role of graph linearization, as evident in the average performance and across multiple individual tasks.Notably, on the GraphQA dataset, the combination of degreebased ordering and node relabeling improves performance by approximately 35% on the maximum node degree estimation task and by around 13% on the shortest path task.Similarly, on the GraphWave dataset, the combination of degree-based ordering and node relabeling enhances edge existence per-formance by roughly 26%.Comparing random and node relabeling reveals that ordering alone offers significant improvements over the baseline, while the additional information from structured relabeling further enhances accuracy on nearly all tasks.Among all tasks, diameter estimation is the most challenging across both datasets, with consistently low performance, indicating LLMs struggle to infer global graph properties at this level of complexity.</p>
<p>Linegraph-based methods (LG{*})-where edges are reinterpreted as nodes-highlight the importance of edge-to-edge relationships.While their overall average score is lower, they generally perform better in edge-based tasks, such as edge existence and path reasoning, by capturing interdependencies that might be less evident in traditional node-focused representations.These findings suggest that a more suitable linearization approach may be necessary to fully exploit the benefits of the linegraph transformation.</p>
<p>Similarly, CoreNumber-based methods achieve better performance in edge-centric tasks, which can be attributed to its ability to capture the structural cohesiveness of a graph.By emphasizing nodes embedded in densely connected subgraphs, core number ordering effectively preserves key connectivity patterns, making it particularly advantageous for reasoning about edge relationships.In contrast, while Degree-and PageRank-based orderings demonstrate the most consistent performance across various tasks, their strengths are more pronounced in node-related tasks.</p>
<p>When moving from zero-shot to one-shot setting, we notice a performance loss on binary classification tasks like edge and path existence.This decline may result from the model's reliance on a single graph example, which does not fully capture the complexity and diversity of the dataset.However, despite this drop, one-shot prompting remains effective for more complex tasks.</p>
<p>Finally, when comparing the results between the 8B and 70B versions of Llama, we observe a significant performance boost in the node counting and motif shape classification tasks with the larger model.In contrast, the node degree task shows only a modest improvement.Meanwhile, diameter estimation continues to exhibit very low accuracy, indicating that this task remains particularly challenging regardless of model size.</p>
<p>Conclusion</p>
<p>In this study, we investigated different graph linearization techniques for LLMs.The core challenge lies in converting graphs into linear token sequences while maintaining important structural features-like local dependencies and global coherence-akin to those in natural language, to help LLMs more effectively interpret graph-based data.To this end, we developed and evaluated several linearization methods based on graph centrality and degeneracy.Our experiments showed that graph linearization notably enhances the performance of LLMs on graph reasoning tasks, especially when paired with the node relabeling technique.Our work presents novel graph representations tailored for LLMS, paving the way for integrating graph machine learning with the growing trend of multimodal processing through a unified transformer.</p>
<p>Limitations</p>
<p>Our study has several limitations that future research could address.First, we considered only a limited set of structural features.Key graph properties, such as community structures and connected components, were not incorporated, yet they could enhance the model's ability to capture complex structural patterns.Second, the diversity of datasets is limited.We conducted our experiments exclusively on synthetic graphs, and future work should explore a broader range of graph datasets to improve real-world generalization.Third, the range of LLMs evaluated was narrow.We primarily focused on LLaMA 3, and future studies could investigate a wider variety of models, including those fine-tuned for coding or mathematical reasoning, to assess their impact on graph-based tasks.Fourth, our method inherits a fundamental limitation from the underlying LLMs: the input sequence is bounded by the model's context window.This imposes an upper limit on the number of edges that can be represented per graph.In Appendix C, we analyze how this constraint translates to a maximum graph size under different edge densities and token budgets.Lastly, our study does not investigate the impact of additional training to adapt models to our linearization methods, which could potentially enhance their graph reasoning capabilities.</p>
<p>Marinka Zitnik and Jure Leskovec.2017.Predicting multicellular function through multi-layer tissue networks.arXiv preprint arXiv:1707.04638.</p>
<p>A Additional Experiments</p>
<p>Pseudo-random linearization.We also investigated the performance of utilizing the edge ordering directly provided by the graph generator.Most non-random graph generators create graphs procedurally, inherently embedding structural information within the edge list order.For instance, in GraphWave, the process begins with a base graph and subsequently attaches motifs, making it easier to distinguish between different structures.We hypothesize that this embedded structural knowledge will enhance task performance and boost the LLM's capabilities.However, generating such structure-aware edge lists requires an understanding of the graph construction process, which may not be feasible for real-world applications involving larger and more complex graphs.The results of both datasets are presented in Table 4.For the GraphWave dataset, the default edge ordering shows mixed performance compared to the random ordering baseline.When combined with structured edge ordering (Table 1), accuracy improves consistently across tasks, except for path existence.When comparing default labeling with structured node labeling, performance generally improves, though default labeling remains competitive in certain tasks.For the GraphQA dataset, the default edge ordering performs significantly better than the random ordering across all tasks except path existence.In this case, the default ordering proves to be particularly robust, making it challenging for structured edge ordering to achieve higher accuracy.Even compared to structured edge ordering (Table 3), default ordering often maintains a performance advantage, highlighting its effectiveness in this dataset.LLM Family Variation with Extended Context.To further examine the influence of model architecture and extended context capacity, we evaluated the performance of Qwen 2.5 14B-1M (Yang et al., 2025), a large-context language model from a distinct model family capable of processing input sequences of up to 1 million tokens.This evaluation allows us to assess whether architectural differences impact performance on graph-based reasoning tasks.Table 5 reports the results obtained for the same selection of tasks previously used in Table 2, enabling a direct comparison across models with varying capacities.</p>
<p>Although scaling from LLaMA 8B to 70B yields substantial gains in tasks such as node counting and motif shape classification, Table 5 demonstrates that Qwen 2.5 14B-despite having fewer parameters than LLaMA 70B-achieves competitive, and in some cases superior, performance across several tasks.This is particularly evident in motif shape classification and diameter estimation, where Qwen's results rival or exceed those of the larger model.Nonetheless, in line with trends observed across LLaMA variants, diameter estimation remains a consistently challenging task, with overall accuracy remaining low regardless of model architecture or scale.</p>
<p>B Performance and Complexity</p>
<p>All considered graph measures, such as core number, degree, and PageRank, are computationally efficient.For graphs where the number of edges exceeds the number of nodes, the computational complexity scales linearly with the number of edges, that is, O(m), where m denotes the number of edges.Given that the graphs in the evaluated datasets are relatively small, the computation time required for these measures is negligible compared to the response generation time of the LLMs.</p>
<p>To illustrate this, we report on Table 6 the number of tokens generated per task along with the average time taken to produce a single response using LLama 3 8b.Numerical tasks, LLM responses are concise and rapidly converge to a final answer.In contrast, more complex tasks elicit longer responses that often involve intermediate reasoning steps.</p>
<p>C Graph Size Limitations</p>
<p>In our approach, the entire graph is linearized into a token sequence and embedded directly into the model's input prompt.As a result, the maximum size of the graph that can be processed in a single prompt is constrained by the model's context window.Since the sequence length is primarily determined by the number of edges, we estimate the maximum number of edges that can be encoded per prompt for each model considered in this study.</p>
<p>To compute these estimates, we assume that each edge requires approximately 5 tokens to represent, and that the accompanying task description consumes an average of 100 tokens.Under these assumptions, Table 7 reports the estimated edge ca-Table 4: Accuracy scores for all tasks on the GraphWave (top) and GraphQA (bottom) datasets using Llama 3 8B.For each task, we compare the default labeling scheme, as provided by the corresponding graph generator, against the default order the edges have been generated.Notations remain the same as in Table 1.Table 5: Accuracy scores for a subset of tasks on the GraphWave dataset using Qwen 2.5 14B -1M as an ablation study.Notations remain the same as in Table 1.</p>
<p>pacity corresponding to the context window of each model.</p>
<p>In Table 8, we present statistics for several widely used graph datasets.Many of these graphs are sufficiently small to fit within the context window of contemporary LLMs, with notable exceptions such as large-scale social and e-commerce networks (e.g., Reddit, Amazon).This indicates that a substantial portion of benchmark graph datasets can be fully serialized and input to an LLM in a single prompt.Nevertheless, in practical applications, even graph neural networks often rely on sampling strategies rather than processing entire graphs at once.A similar strategy may be necessary when using LLMs for real-world graph tasks, depending on the application and scale.</p>
<p>Although our study primarily investigates the  ability of LLMs to understand and reason over complete graph structures, we recognize that some of the tasks examined-such as node counting-are primarily diagnostic and may have limited practical relevance.These tasks are intended to serve as controlled benchmarks to assess the reasoning capabilities of LLMs, rather than to reflect typical graph processing workloads.</p>
<p>D Prompt templates</p>
<p>Node Counting</p>
<p>In an undirected graph G, (i, j) means that node i and node j are connected with an undirected edge.Q: How many nodes are in G?  (Yang et al., 2018) 12,752 491,722 OGBN-Proteins (Hu et al., 2020) 132,534 39,561,252 Reddit (Hamilton et al., 2017) 232,965 114,615,892 Amazon Products (Chiang et al., 2019) 1,569,960 264,339,468 Table 8: Node and edge counts for commonly used graph datasets.</p>
<p>G: {linearized graph}</p>
<p>Max Degree</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The degree of a node is the number of edges connected to the node.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the maximum node degree in the following graph G? G: {linearized graph}</p>
<p>Node Degree</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The degree of a node is the number of edges connected to the node.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the degree of node {node} in the following graph G? G: {linearized graph}</p>
<p>Edge Existence</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Does an undirected edge ({node1}, {node2}) exist in the following graph G?. G: {linearized graph}</p>
<p>Diameter</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The diameter of a graph is the length of the shortest path between the most distanced nodes.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the diameter of the following graph G? G: {linearized graph}</p>
<p>Shortest Path</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the length of the shortest path from node {node1} to node {node2}?If no path exists, the response is '0'.G: {linearized graph}</p>
<p>Path Existence</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Does a path that connects node {node1} and {node2} exist in the following graph G? G: {linearized graph} Motifs' Shape Classification: In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The graph contains a motif graph with strictly one of the following structures.{structure}: {definition} Q: Which of the defined structures is included in the following graph?graph: {linearized graph}</p>
<p>Figure 1 :
1
Figure 1: An example of degree-based graph linearization method.Given an input graph G, we rank its nodes based on their degree and then explore the edges in that order.The resulting linearized graph is then combined with a task-specific prompt and passed into a LLM.</p>
<p>Figure 2 :
2
Figure 2: Overview of GraphWave synthetic dataset.</p>
<p>Table 6 :
6
Tokens generated and response time for a single graph using LLaMA 3 8B across tasks.
TaskNumber of Tokens Inference Time (sec)Node Counting160.6Max Degree160.6Node Degree160.6Edge Existence1284.8Diameter1284.8Shortest Path1284.8Path Existence1284.8Motif's Shape160.6LLMContext Length (tokens) Max Number of EdgesLLama 3 8b8,1921,618LLama 3 70b8,1921,618Qwen 2.5 14B 1M1,010,000199,980</p>
<p>Table 7 :
7
Estimated Maximum Number of Edges considering Context Window Size.</p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Motifs' Shape Average Random Labeling CoreNumber. 25.97 / 34.98 17.47 / 17.37 58.53 / 56.79</p>
<p>28.81 / 39.18 24.10 / 23.57 59.4 / 56.42PageRank. </p>
<p>. 28.65 / 36.05 14.57 / 16.17 58.3 / 61.32Node Relabeling CoreNumber. </p>
<p>34.35 / 38.01 26.07 / 25.24 65.37 / 56.92 50.83 / 46.08 10.47 / 11.17 32.33 / 17.67PageRank. </p>
<p>. 32.46 / 34.6 15.13 / 12.94 39.36 / 45.99Baseline. </p>
<p>Each task compares two node labeling schemes in zero-shot / one-shot prompting against the random Baseline. The linearization names represent the node ordering methods used (CoreNumber, Degree, or PageRank), while 'LG{*}' indicates graphs were transformed to the corresponding linegraph beforehand. Underlined scores denote the best-performing linearization method for each task, labeling, and X-shot combination; bold indicates task-wise best. 76.47 / 75.29 72.47 / 63.75 3.97 / 4.07Node Counting Node Degree Diameter Motifs' Shape Random Labels CoreNumber. Table 1: Accuracy scores for all tasks on the GraphWave dataset using Llama 3 8B, including the overall average</p>
<p>84.77 / 81.13 73.93 / 67.59 8.63 / 12.84 58.1 / 47.4PageRank. </p>
<p>76.27 / 76.93 75.47 / 65.32 8.77 / 12.84LG{CoreNumber}. </p>
<p>. 83.24 / 83.04 69.69 / 59.81 4.52 / 7.72 41.57 / 42.5Baseline. </p>
<p>60.77 / 24.28 15.88 / 18.67 54.44 / 37.23 70.68 / 61.59 3.28 / 18.53 49.21 / 57.25Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Average Random Labels CoreNumber. </p>
<p>62.12 / 28.31 26.03 / 28.94 56.23 / 28.05PageRank. </p>
<p>61.14 / 23.94 10.81 / 25.89 49.09 / 36.65LG{CoreNumber}. </p>
<p>64.71 / 44.97 40.72 / 43.15 58.77 / 28.65 69.97 / 70.83 3.57 / 14.13 26.17 / 49.54PageRank. </p>
<p>. 66.28 / 27.38 9.28 / 20.37 49.78 / 35.86Baseline. </p>
<p>Statistical mechanics of complex networks. Reviews of Modern Physics. References Réka Albert and Albert-László Barabási7412002</p>
<p>Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational Linguistics2013</p>
<p>Emergence of scaling in random networks. Albert-László Barabási, Réka Albert, Science. 28654391999</p>
<p>Graph markup language (graphml). Ulrik Brandes, Markus Eiglsperger, Jürgen Lerner, Christian Pich, Handbook of graph drawing visualization, Discrete mathematics and its applications. Roberto Tamassia, Boca RatonCRC Press2013u.a.</p>
<p>The anatomy of a large-scale hypertextual web search engine. Sergey Brin, Lawrence Page, Computer Networks and ISDN Systems. Elsevier199830</p>
<p>Exploring the potential of large language models (llms) in learning on graph. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>GroundHog: Dialogue generation using multi-grained linguistic input. Alexander Chernyavskiy, Lidiia Ostyakova, Dmitry Ilvovsky, Proceedings of the 5th Workshop on Computational Approaches to Discourse (CODI 2024). the 5th Workshop on Computational Approaches to Discourse (CODI 2024)St. Julians, Malta. Association for Computational Linguistics2024</p>
<p>Clustergcn: An efficient algorithm for training deep and large graph convolutional networks. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh, arXiv:1907.049312019arXiv preprint</p>
<p>Which modality should i use-text, motif, or image?. Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang, arXiv:2311.09862Understanding graphs with large language models. 2023arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition. Linhao Dong, Shuang Xu, Bo Xu, 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE2018</p>
<p>Learning structural node embeddings via diffusion wavelets. Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec ; Alexey, Lucas Dosovitskiy, Alexander Beyer, Dirk Kolesnikov, Xiaohua Weissenborn, Thomas Zhai, Mostafa Unterthiner, Matthias Dehghani, Georg Minderer, Sylvain Heigold, Gelly, arXiv:2010.11929Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018. 2020arXiv preprintAn image is worth 16x16 words: Transformers for image recognition at scale</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>A generalization of transformer networks to graphs. Vijay Prakash, Dwivedi , Xavier Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>On random graphs. Paul Erdős, Alfréd Rényi, Publicationes Mathematicae. 61959</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>A synopsis of linguistic theory, 1930-1955. John R Firth, Studies in linguistic analysis. 1957</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Linton C Freeman ; Jiayan, Lun Guo, Hengyu Du, Mengyu Liu, Xinyi Zhou, Shi He, Han, arXiv:2305.15066Social networks. 131978. 2023arXiv preprintCentrality in social networks conceptual clarification</p>
<p>Inductive representation learning on large graphs. Advances in neural information processing systems. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730</p>
<p>S Zellig, Harris, Distributional structure. Word. 195410</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Gml: Graph modelling language. Michael Himsolt, 1997University of Passau</p>
<p>Paul W Holland, Kathryn B Laskey, Samuel Leinhardt, Stochastic blockmodels: First steps. 19835</p>
<p>Promoting graph awareness in linearized graph-to-text generation. Alexander Miserlis Hoyle, Ana Marasović, Noah A Smith, 10.18653/v1/2021.findings-acl.82Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in Neural Information Processing Systems. 202033</p>
<p>Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.04944Beyond text: A deep dive into large language models' ability on understanding graph data. 2023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information through prompts, and why?. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, Transactions on Machine Learning Research. 2024</p>
<p>Description of language design. Martin Joos, The Journal of the Acoustical Society of America. 2261950</p>
<p>Pure transformers are powerful graph learners. Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong, Advances in Neural Information Processing Systems. 202235</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1603.088612016arXiv preprint</p>
<p>. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 2020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBARTOnline. Association for Computational Linguistics</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. Chang Liu, Bo Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Graphinstruct: Empowering large language models with graph understanding and reasoning capability. Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin, arXiv:2403.044832024arXiv preprint</p>
<p>The pagerank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, 1999Stanford InfoLabTechnical report</p>
<p>Book reviews: Text generation and systemic-functional linguistics: Experiences from English and Japanese. Terry Patten, Computational Linguistics. 1911993</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Leveraging discourse structure for extractive meeting summarization. Virgile Rennard, Guokan Shang, Michalis Vazirgiannis, Julie Hunter, arXiv:2405.110552024Preprint</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021Hinrich Schütze, and Iryna Gurevych</p>
<p>Network structure and minimum degree. Stephen B Seidman, 10.1016/0378-8733(83)90028-XSocial networks. 531983</p>
<p>Graph reasoning with large language models via pseudo-code prompting. Konstantinos Skianis, Giannis Nikolentzos, Michalis Vazirgiannis, arXiv:2409.179062024arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 202436</p>
<p>Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K Singh, arXiv:2406.16176Grapheval2000: Benchmarking and improving large language models on graph datasets. 2024arXiv preprint</p>
<p>Multimodal learning with transformers: A survey. Peng Xu, Xiatian Zhu, David A Clifton, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, arXiv:2501.15383Qwen2.5-1m technical report. 2025arXiv preprint</p>
<p>Zhilin Yang, William W Cohen, Ruslan Salakhutdinov, arXiv:1811.05868Benchmarking graph neural networks. 2018arXiv preprint</p>
<p>Exploring the potential of large language models in graph generation. Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei, arXiv:2403.143582024arXiv preprint</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Gracore: Benchmarking graph comprehension and complex reasoning in large language models. Zike Yuan, Ming Liu, Hui Wang, Bing Qin, arXiv:2407.029362024arXiv preprint</p>
<p>Llm4dyg: Can large language models solve problems on dynamic graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.17110Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. 2023a. 2023barXiv preprintNeurIPS 2023 Workshop: New Frontiers in Graph Learning</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023arXiv preprint</p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Motifs' Shape Default Labels CoreNumber. 25.06 / 37.41 17.87 / 16.67 60.9 / 62.45</p>
<p>35.77 / 49.38 24.40 / 26.88 63.43 / 56.42PageRank. </p>
<p>24.6 / 32.21 23.23 / 17.84 49.33 / 41.35 56.4 / 52.72 10.67 / 11.04 29.33 / 19.47LG{Degree}. </p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Default Labels CoreNumber. 61.23 / 38.52 22.4 / 34.35 52.72 / 44.51</p>
<p>67.73 / 38.38 22.66 / 30.09 47.63 / 33.75 71.87 / 61.71 1.27 / 11.68 40.44 / 47.07LG{CoreNumber}. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-343 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-343</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-343</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-270371420</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05756v1.pdf" target="_blank">EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks.However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs.The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective.Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e343.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e343.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbSpatial-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3,640-sample multiple-choice benchmark derived from annotated 3D embodied scenes (Matterport3D, AI2-THOR, ScanNet) that evaluates egocentric spatial relationships (above, below, left, right, close, far) for LVLMs in embodied scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EmbSpatial-Bench evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation of LVLMs' ability to identify egocentric spatial relationships between objects in images sampled from 3D embodied datasets; QA pairs are multiple-choice and relationships are derived from 3D coordinates projected to 2D bounding boxes (and average depth for close/far).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial understanding / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (egocentric locations, relative horizontal/vertical positions, depth: close/far); object-relational (which object relates to which), used for embodied planning contexts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit annotations from 3D embodied datasets (MP3D, AI2-THOR, ScanNet) used to construct evaluation items; models' internal knowledge comes from their pretraining unless fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot generation and likelihood-based scoring (models given image + question, returned option or option likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Benchmark items represent spatial relations via (1) 2D bounding boxes projected from 3D coordinates, (2) depth approximated by average depth in bounding box, and (3) natural-language multiple-choice questions; the benchmark forces models to map visual input to textual relational choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage correct) on multiple-choice questions; separate reporting for generation and likelihood strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Benchmark contains 3,640 QA pairs; human accuracy 90.33%; best LVLM zero-shot accuracy (generation) reported 49.11% (Qwen-VL-Max) and lower under likelihood for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Models can sometimes identify clear, strongly separated horizontal/vertical relations when objects are large and unambiguous in the view; some models show better horizontal/vertical than depth judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Systematic failures in object localization and depth reasoning (close/far), frequent mis-localization of objects leading to wrong relation judgments, confusion on depth-based relations, and inconsistent performance across simulated vs real scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Human baseline 90.33%; LVLM best generation accuracy 49.11% (Qwen-VL-Max); many models <40% generation accuracy and slightly different likelihood results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not an ablation per se (benchmark), but the paper uses both generation and likelihood evaluation strategies to decouple instruction-following from relational competence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A dedicated egocentric, embodied-scene benchmark reveals substantial gaps between human spatial understanding and current LVLMs; depth relations are especially weak and object localization errors propagate to relation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e343.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e343.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbSpatial-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbSpatial-SFT: Instruction-tuning dataset for embodied spatial understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuning dataset constructed from training split of MP3D containing ~25k training samples for spatial-relation identification and auxiliary object localization (grounding) tasks to fine-tune LVLMs for embodied spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction tuning for spatial relation identification and object localization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised fine-tuning dataset where models learn to (1) identify egocentric spatial relations between objects in images and (2) ground localizations of queried objects (textual bounding-box output) to improve downstream spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / grounding / spatial relation learning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (grounding objects, spatial relations), supports procedural grounding as auxiliary skill</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on synthetic/annotated embodied training data (MP3D-derived QA and object localization pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning / instruction tuning (train connection module + LLM LoRA adapters), evaluated zero-shot on held-out embodied scenes</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Training pairs use natural-language questions + image input; object locations encoded textually as bounding-box coordinates; relations derived from 3D->2D geometry and depth averages provide supervision for depth relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on EmbSpatial-Bench (generation and likelihood strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Fine-tuning MiniGPT-v2 on EmbSpatial-SFT produced consistent improvements: +34.25% overall accuracy (likelihood strategy) and +9.04% overall (generation strategy) compared to pre-finetuned baseline; auxiliary object localization yields small additional gains (+0.47% generation, +0.76% likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Fine-tuned models show improved grounding and relational identification across in-domain and out-of-domain embodied scenes, especially for horizontal/vertical relations; LoRA tuning of LLM backbone yields the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Depth-related (close/far) judgments still lag behind horizontal/vertical dimensions; simulated-scene generalization (AI2-THOR) improves less than real-world (ScanNet/MP3D) possibly due to domain mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to frozen-LLM variants (only connection module tuned), models with LoRA-tuned LLM backbone perform significantly better; baseline zero-shot accuracies (various LVLMs) are much lower than fine-tuned results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablation removing LoRA tuning (i.e., freezing LLM backbone) substantially reduced gains; removing auxiliary object localization training reduced overall accuracy by ~0.47% (generation) and ~0.76% (likelihood), indicating auxiliary grounding provides modest benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervised instruction-tuning with explicit grounding signals significantly improves LVLM embodied spatial understanding; the LLM backbone must learn reasoning (LoRA fine-tuning) beyond mere visual-connector adaptation, and explicit bounding-box textual grounding aids relation identification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e343.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e343.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-v2 (baseline & fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-v2 (baseline pre-instruction, and MiniGPT-v2 fine-tuned on EmbSpatial-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prevalent LVLM architecture used as baseline: frozen vision encoder + linear connection layer + LLM; in this paper the connection module and LLM adapters (LoRA) are fine-tuned on EmbSpatial-SFT to evaluate gains in spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPT-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture with a vision encoder producing features, a linear connection layer into a large language model; originally instruction-tuned checkpoints were used and then further fine-tuned by the authors by training the connection layer and LoRA adapters on the LLM backbone (LoRA rank=64, Î±=16).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EmbSpatial-Bench evaluation (zero-shot) and EmbSpatial-SFT fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot spatial relation multiple-choice evaluation on EmbSpatial-Bench; supervised fine-tuning on EmbSpatial-SFT (spatial-relation and object localization) and re-evaluation zero-shot on held-out embodied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial relation identification, object localization (grounding), instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (grounding objects and their relations); also procedural insofar as grounding supports downstream embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained LVLM checkpoint (visual+language pretraining) and supervised fine-tuning on EmbSpatial-SFT (MP3D-derived); evaluation uses provided images</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot generation and likelihood scoring for evaluation; supervised fine-tuning (LoRA + connection layer) to inject spatial grounding knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit learned weights in LLM and connection layer encode relation reasoning after fine-tuning; object locations are trained/represented as textual bounding-box outputs during localization auxiliary task; depth supervision provided via average depth per bounding box.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) on multiple-choice EmbSpatial-Bench; reported under generation and likelihood strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline MiniGPT-v2 zero-shot: generation 23.93% / likelihood 43.85% (Table 2). After fine-tuning on EmbSpatial-SFT: overall gains reported (aggregate) +34.25% (likelihood) and +9.04% (generation) relative improvement; exact post-finetune per-environment figures reported in paper (improvements consistent across environments).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>After fine-tuning, MiniGPT-v2 better at object grounding and horizontal/vertical relationship identification; LoRA tuning enables LLM backbone to learn relational reasoning rather than only adjusting visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Depth (close/far) relations remain weaker than horizontal/vertical; some residual object localization and nearest/farthest selection errors persist.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to frozen-LLM variant (only connection module tuned), LoRA-tuned MiniGPT-v2 shows substantial improvement; compared to other LVLMs, pre-finetune MiniGPT-v2 was low-performing but fine-tuning narrows gap.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Freezing the LLM backbone (w/o LoRA) lowers performance substantially versus tuning LoRA; removing auxiliary object-localization training reduces overall accuracy by ~0.47% (generation) and ~0.76% (likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning both the visual-connector and the LLM (via parameter-efficient LoRA) is necessary for LVLMs to acquire embodied spatial reasoning; grounding supervision (textual bounding boxes) helps but depth reasoning remains a hard dimension to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e343.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e343.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (GPT-4 with Vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large vision-language model from OpenAI evaluated zero-shot on EmbSpatial-Bench; despite strong instruction following, it shows poor object localization and spatial relation judgments in embodied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal variant of GPT-4 that accepts visual inputs; used in this work in a zero-shot setting to assess embodied spatial understanding. The authors used prompting to decouple object localization from relation reasoning when probing GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EmbSpatial-Bench zero-shot evaluation & targeted prompting for error analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given images from embodied scenes and multiple-choice spatial-relation questions, GPT-4V was prompted to localize objects and determine relations; additional prompting used to inspect whether errors were due to localization or relation inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>evaluation / error analysis (spatial relation identification and object localization)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (object localization and relation judgment)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on visual-text corpora and OpenAI's proprietary multimodal training; evaluated zero-shot on provided images</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting with generation outputs; also targeted multi-step prompts to separate localization and relation stages</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Predominantly implicit in model weights; model outputs natural-language descriptions and localization responses (subject to model's internal visual grounding); no explicit symbolic spatial map was used by the paper when evaluating GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) on EmbSpatial-Bench multiple-choice (generation reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot generation accuracy reported at 36.07% (Table 2); likelihood not provided. Failure case studies reveal both localization and relation inference errors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasional correct localization and relation judgments for simple, unambiguous pairs; strong instruction-following enables structured probing to isolate errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Frequent mis-localizations (e.g., misplacing clock from top-left to top-right) that lead to wrong relation choices; even when localization succeeds, wrong nearest/farthest selections occur, indicating errors in depth/relative distance reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to human 90.33% and best LVLM 49.11% (Qwen-VL-Max), GPT-4V's 36.07% is substantially lower.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper performed prompt-based decoupling (probe localization vs relation) rather than architectural ablation; found errors in both stages, implying both perceptual grounding and relational inference are limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even top-tier multimodal LLMs with strong instruction-following struggle with embodied spatial tasks: errors arise from both (1) visual grounding/localization failures and (2) incorrect relational inference (notably depth judgments); decoupling these stages is useful diagnostically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e343.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e343.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art closed-source LVLM evaluated zero-shot on the EmbSpatial-Bench; reported the highest generation accuracy among evaluated models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A frontier LVLM (Bai et al., 2023 reference) with visual-language capabilities; used zero-shot with image+question inputs to evaluate embodied spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EmbSpatial-Bench zero-shot evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice spatial-relation QA from egocentric perspective on images drawn from 3D embodied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial relation identification / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (egocentric relations and object localization as implicitly processed by the model)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large visual-language corpora; no fine-tuning on EmbSpatial-SFT reported for this model in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot generation strategy</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in network weights; model maps image features to textual relational outputs via its multimodal encoder and language decoder; no explicit symbolic spatial maps reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) on EmbSpatial-Bench (generation strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Generation accuracy reported as 49.11% (Table 2), the highest among reported LVLMs in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Relatively better at some horizontal/vertical relation items than other LVLMs in zero-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still far below human performance; struggles remain particularly for depth relations and ambiguous localizations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Best LVLM (49.11%) vs human (90.33%); other LVLMs in paper ranged lower (many <40%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation reported for this model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even the best-performing zero-shot LVLM in the study achieves ~49% accuracy, indicating that pretraining alone is insufficient for robust embodied spatial reasoning; explicit embodied training or grounding is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What's" up" with vision-language models? investigating their struggle with spatial reasoning <em>(Rating: 2)</em></li>
                <li>Visual spatial reasoning <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 1)</em></li>
                <li>Teach: Task-driven embodied agents that chat <em>(Rating: 1)</em></li>
                <li>Multimodal large language model for visual navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-343",
    "paper_id": "paper-270371420",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "EmbSpatial-Bench",
            "name_full": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
            "brief_description": "A 3,640-sample multiple-choice benchmark derived from annotated 3D embodied scenes (Matterport3D, AI2-THOR, ScanNet) that evaluates egocentric spatial relationships (above, below, left, right, close, far) for LVLMs in embodied scenarios.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "EmbSpatial-Bench evaluation",
            "task_description": "Zero-shot evaluation of LVLMs' ability to identify egocentric spatial relationships between objects in images sampled from 3D embodied datasets; QA pairs are multiple-choice and relationships are derived from 3D coordinates projected to 2D bounding boxes (and average depth for close/far).",
            "task_type": "spatial understanding / evaluation",
            "knowledge_type": "spatial (egocentric locations, relative horizontal/vertical positions, depth: close/far); object-relational (which object relates to which), used for embodied planning contexts",
            "knowledge_source": "explicit annotations from 3D embodied datasets (MP3D, AI2-THOR, ScanNet) used to construct evaluation items; models' internal knowledge comes from their pretraining unless fine-tuned",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot generation and likelihood-based scoring (models given image + question, returned option or option likelihood)",
            "knowledge_representation": "Benchmark items represent spatial relations via (1) 2D bounding boxes projected from 3D coordinates, (2) depth approximated by average depth in bounding box, and (3) natural-language multiple-choice questions; the benchmark forces models to map visual input to textual relational choices.",
            "performance_metric": "accuracy (percentage correct) on multiple-choice questions; separate reporting for generation and likelihood strategies",
            "performance_result": "Benchmark contains 3,640 QA pairs; human accuracy 90.33%; best LVLM zero-shot accuracy (generation) reported 49.11% (Qwen-VL-Max) and lower under likelihood for many models.",
            "success_patterns": "Models can sometimes identify clear, strongly separated horizontal/vertical relations when objects are large and unambiguous in the view; some models show better horizontal/vertical than depth judgments.",
            "failure_patterns": "Systematic failures in object localization and depth reasoning (close/far), frequent mis-localization of objects leading to wrong relation judgments, confusion on depth-based relations, and inconsistent performance across simulated vs real scenes.",
            "baseline_comparison": "Human baseline 90.33%; LVLM best generation accuracy 49.11% (Qwen-VL-Max); many models &lt;40% generation accuracy and slightly different likelihood results.",
            "ablation_results": "Not an ablation per se (benchmark), but the paper uses both generation and likelihood evaluation strategies to decouple instruction-following from relational competence.",
            "key_findings": "A dedicated egocentric, embodied-scene benchmark reveals substantial gaps between human spatial understanding and current LVLMs; depth relations are especially weak and object localization errors propagate to relation errors.",
            "uuid": "e343.0",
            "source_info": {
                "paper_title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "EmbSpatial-SFT",
            "name_full": "EmbSpatial-SFT: Instruction-tuning dataset for embodied spatial understanding",
            "brief_description": "An instruction-tuning dataset constructed from training split of MP3D containing ~25k training samples for spatial-relation identification and auxiliary object localization (grounding) tasks to fine-tune LVLMs for embodied spatial reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Instruction tuning for spatial relation identification and object localization",
            "task_description": "Supervised fine-tuning dataset where models learn to (1) identify egocentric spatial relations between objects in images and (2) ground localizations of queried objects (textual bounding-box output) to improve downstream spatial reasoning.",
            "task_type": "instruction following / grounding / spatial relation learning",
            "knowledge_type": "spatial + object-relational (grounding objects, spatial relations), supports procedural grounding as auxiliary skill",
            "knowledge_source": "fine-tuning on synthetic/annotated embodied training data (MP3D-derived QA and object localization pairs)",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised fine-tuning / instruction tuning (train connection module + LLM LoRA adapters), evaluated zero-shot on held-out embodied scenes",
            "knowledge_representation": "Training pairs use natural-language questions + image input; object locations encoded textually as bounding-box coordinates; relations derived from 3D-&gt;2D geometry and depth averages provide supervision for depth relations.",
            "performance_metric": "accuracy on EmbSpatial-Bench (generation and likelihood strategies)",
            "performance_result": "Fine-tuning MiniGPT-v2 on EmbSpatial-SFT produced consistent improvements: +34.25% overall accuracy (likelihood strategy) and +9.04% overall (generation strategy) compared to pre-finetuned baseline; auxiliary object localization yields small additional gains (+0.47% generation, +0.76% likelihood).",
            "success_patterns": "Fine-tuned models show improved grounding and relational identification across in-domain and out-of-domain embodied scenes, especially for horizontal/vertical relations; LoRA tuning of LLM backbone yields the largest gains.",
            "failure_patterns": "Depth-related (close/far) judgments still lag behind horizontal/vertical dimensions; simulated-scene generalization (AI2-THOR) improves less than real-world (ScanNet/MP3D) possibly due to domain mismatch.",
            "baseline_comparison": "Compared to frozen-LLM variants (only connection module tuned), models with LoRA-tuned LLM backbone perform significantly better; baseline zero-shot accuracies (various LVLMs) are much lower than fine-tuned results.",
            "ablation_results": "Ablation removing LoRA tuning (i.e., freezing LLM backbone) substantially reduced gains; removing auxiliary object localization training reduced overall accuracy by ~0.47% (generation) and ~0.76% (likelihood), indicating auxiliary grounding provides modest benefit.",
            "key_findings": "Supervised instruction-tuning with explicit grounding signals significantly improves LVLM embodied spatial understanding; the LLM backbone must learn reasoning (LoRA fine-tuning) beyond mere visual-connector adaptation, and explicit bounding-box textual grounding aids relation identification.",
            "uuid": "e343.1",
            "source_info": {
                "paper_title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MiniGPT-v2 (baseline & fine-tuned)",
            "name_full": "MiniGPT-v2 (baseline pre-instruction, and MiniGPT-v2 fine-tuned on EmbSpatial-SFT)",
            "brief_description": "A prevalent LVLM architecture used as baseline: frozen vision encoder + linear connection layer + LLM; in this paper the connection module and LLM adapters (LoRA) are fine-tuned on EmbSpatial-SFT to evaluate gains in spatial understanding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniGPT-v2",
            "model_size": null,
            "model_description": "Architecture with a vision encoder producing features, a linear connection layer into a large language model; originally instruction-tuned checkpoints were used and then further fine-tuned by the authors by training the connection layer and LoRA adapters on the LLM backbone (LoRA rank=64, Î±=16).",
            "task_name": "EmbSpatial-Bench evaluation (zero-shot) and EmbSpatial-SFT fine-tuning",
            "task_description": "Zero-shot spatial relation multiple-choice evaluation on EmbSpatial-Bench; supervised fine-tuning on EmbSpatial-SFT (spatial-relation and object localization) and re-evaluation zero-shot on held-out embodied scenes.",
            "task_type": "spatial relation identification, object localization (grounding), instruction following",
            "knowledge_type": "spatial + object-relational (grounding objects and their relations); also procedural insofar as grounding supports downstream embodied planning",
            "knowledge_source": "pretrained LVLM checkpoint (visual+language pretraining) and supervised fine-tuning on EmbSpatial-SFT (MP3D-derived); evaluation uses provided images",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot generation and likelihood scoring for evaluation; supervised fine-tuning (LoRA + connection layer) to inject spatial grounding knowledge",
            "knowledge_representation": "Implicit learned weights in LLM and connection layer encode relation reasoning after fine-tuning; object locations are trained/represented as textual bounding-box outputs during localization auxiliary task; depth supervision provided via average depth per bounding box.",
            "performance_metric": "accuracy (%) on multiple-choice EmbSpatial-Bench; reported under generation and likelihood strategies",
            "performance_result": "Baseline MiniGPT-v2 zero-shot: generation 23.93% / likelihood 43.85% (Table 2). After fine-tuning on EmbSpatial-SFT: overall gains reported (aggregate) +34.25% (likelihood) and +9.04% (generation) relative improvement; exact post-finetune per-environment figures reported in paper (improvements consistent across environments).",
            "success_patterns": "After fine-tuning, MiniGPT-v2 better at object grounding and horizontal/vertical relationship identification; LoRA tuning enables LLM backbone to learn relational reasoning rather than only adjusting visual features.",
            "failure_patterns": "Depth (close/far) relations remain weaker than horizontal/vertical; some residual object localization and nearest/farthest selection errors persist.",
            "baseline_comparison": "Compared to frozen-LLM variant (only connection module tuned), LoRA-tuned MiniGPT-v2 shows substantial improvement; compared to other LVLMs, pre-finetune MiniGPT-v2 was low-performing but fine-tuning narrows gap.",
            "ablation_results": "Freezing the LLM backbone (w/o LoRA) lowers performance substantially versus tuning LoRA; removing auxiliary object-localization training reduces overall accuracy by ~0.47% (generation) and ~0.76% (likelihood).",
            "key_findings": "Fine-tuning both the visual-connector and the LLM (via parameter-efficient LoRA) is necessary for LVLMs to acquire embodied spatial reasoning; grounding supervision (textual bounding boxes) helps but depth reasoning remains a hard dimension to learn.",
            "uuid": "e343.2",
            "source_info": {
                "paper_title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (GPT-4 with Vision)",
            "brief_description": "A closed-source large vision-language model from OpenAI evaluated zero-shot on EmbSpatial-Bench; despite strong instruction following, it shows poor object localization and spatial relation judgments in embodied scenes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_size": null,
            "model_description": "Multimodal variant of GPT-4 that accepts visual inputs; used in this work in a zero-shot setting to assess embodied spatial understanding. The authors used prompting to decouple object localization from relation reasoning when probing GPT-4V.",
            "task_name": "EmbSpatial-Bench zero-shot evaluation & targeted prompting for error analysis",
            "task_description": "Given images from embodied scenes and multiple-choice spatial-relation questions, GPT-4V was prompted to localize objects and determine relations; additional prompting used to inspect whether errors were due to localization or relation inference.",
            "task_type": "evaluation / error analysis (spatial relation identification and object localization)",
            "knowledge_type": "spatial + object-relational (object localization and relation judgment)",
            "knowledge_source": "pretraining on visual-text corpora and OpenAI's proprietary multimodal training; evaluated zero-shot on provided images",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting with generation outputs; also targeted multi-step prompts to separate localization and relation stages",
            "knowledge_representation": "Predominantly implicit in model weights; model outputs natural-language descriptions and localization responses (subject to model's internal visual grounding); no explicit symbolic spatial map was used by the paper when evaluating GPT-4V.",
            "performance_metric": "accuracy (%) on EmbSpatial-Bench multiple-choice (generation reported)",
            "performance_result": "Zero-shot generation accuracy reported at 36.07% (Table 2); likelihood not provided. Failure case studies reveal both localization and relation inference errors.",
            "success_patterns": "Occasional correct localization and relation judgments for simple, unambiguous pairs; strong instruction-following enables structured probing to isolate errors.",
            "failure_patterns": "Frequent mis-localizations (e.g., misplacing clock from top-left to top-right) that lead to wrong relation choices; even when localization succeeds, wrong nearest/farthest selections occur, indicating errors in depth/relative distance reasoning.",
            "baseline_comparison": "Compared to human 90.33% and best LVLM 49.11% (Qwen-VL-Max), GPT-4V's 36.07% is substantially lower.",
            "ablation_results": "Paper performed prompt-based decoupling (probe localization vs relation) rather than architectural ablation; found errors in both stages, implying both perceptual grounding and relational inference are limiting factors.",
            "key_findings": "Even top-tier multimodal LLMs with strong instruction-following struggle with embodied spatial tasks: errors arise from both (1) visual grounding/localization failures and (2) incorrect relational inference (notably depth judgments); decoupling these stages is useful diagnostically.",
            "uuid": "e343.3",
            "source_info": {
                "paper_title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Qwen-VL-Max",
            "name_full": "Qwen-VL-Max",
            "brief_description": "A state-of-the-art closed-source LVLM evaluated zero-shot on the EmbSpatial-Bench; reported the highest generation accuracy among evaluated models in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Max",
            "model_size": null,
            "model_description": "A frontier LVLM (Bai et al., 2023 reference) with visual-language capabilities; used zero-shot with image+question inputs to evaluate embodied spatial understanding.",
            "task_name": "EmbSpatial-Bench zero-shot evaluation",
            "task_description": "Multiple-choice spatial-relation QA from egocentric perspective on images drawn from 3D embodied scenes.",
            "task_type": "spatial relation identification / evaluation",
            "knowledge_type": "spatial + object-relational (egocentric relations and object localization as implicitly processed by the model)",
            "knowledge_source": "pretraining on large visual-language corpora; no fine-tuning on EmbSpatial-SFT reported for this model in the paper",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot generation strategy",
            "knowledge_representation": "Implicit in network weights; model maps image features to textual relational outputs via its multimodal encoder and language decoder; no explicit symbolic spatial maps reported.",
            "performance_metric": "accuracy (%) on EmbSpatial-Bench (generation strategy)",
            "performance_result": "Generation accuracy reported as 49.11% (Table 2), the highest among reported LVLMs in the paper.",
            "success_patterns": "Relatively better at some horizontal/vertical relation items than other LVLMs in zero-shot setting.",
            "failure_patterns": "Still far below human performance; struggles remain particularly for depth relations and ambiguous localizations.",
            "baseline_comparison": "Best LVLM (49.11%) vs human (90.33%); other LVLMs in paper ranged lower (many &lt;40%).",
            "ablation_results": "No ablation reported for this model in the paper.",
            "key_findings": "Even the best-performing zero-shot LVLM in the study achieves ~49% accuracy, indicating that pretraining alone is insufficient for robust embodied spatial reasoning; explicit embodied training or grounding is needed.",
            "uuid": "e343.4",
            "source_info": {
                "paper_title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What's\" up\" with vision-language models? investigating their struggle with spatial reasoning",
            "rating": 2,
            "sanitized_title": "whats_up_with_visionlanguage_models_investigating_their_struggle_with_spatial_reasoning"
        },
        {
            "paper_title": "Visual spatial reasoning",
            "rating": 2,
            "sanitized_title": "visual_spatial_reasoning"
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Teach: Task-driven embodied agents that chat",
            "rating": 1,
            "sanitized_title": "teach_taskdriven_embodied_agents_that_chat"
        },
        {
            "paper_title": "Multimodal large language model for visual navigation",
            "rating": 1,
            "sanitized_title": "multimodal_large_language_model_for_visual_navigation"
        }
    ],
    "cost": 0.013713,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models
9 Jun 2024</p>
<p>Mengfei Du 
School of Data Science
Fudan University
China</p>
<p>Binhao Wu 
School of Data Science
Fudan University
China</p>
<p>Zejun Li zejunli20@fudan.edu.cn 
School of Data Science
Fudan University
China</p>
<p>Xuanjing Huang xjhuang@fudan.edu.cn 
School of Computer Science
Fudan University
China</p>
<p>Zhongyu Wei zywei@fudan.edu.cn 
School of Data Science
Fudan University
China</p>
<p>EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models
9 Jun 2024BFC48B789E6326E450E223131867B86DarXiv:2406.05756v1[cs.AI]
The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks.However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown.Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs.The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective.Experiments expose the insufficient capacity of current LVLMs (even GPT-4V).We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding.* Equal contribution â  Corresponding author [False] The zebra is right of the person.[True] The zebra is left of the person.Existing Benchmarks Question: How are curtain and shelves positioned in relation to each other in the image?Options: A. The curtain is left of the shelves.B. The curtain is under the shelves.C. The curtain is right of the shelves.D. The curtain is out of the shelves.</p>
<p>Introduction</p>
<p>Embodied AI is the frontier direction of generalpurpose AI systems, requiring intelligent agents to understand instructions, perceive physical environments, plan and execute actions to accomplish corresponding tasks (Anderson et al., 2018).Recently, LLM-based large vision-language models (LVLMs) have demonstrated powerful capabilities in following instructions and performing planning based on the visual contexts (Li et al., 2023b;Zhu et al., 2023;OpenAI, 2023), paving a promising path for the development of embodied AI systems.</p>
<p>However, recent studies have revealed significant deficiencies of LVLMs in understanding visual contents (Li et al., 2023c).In terms of embodied scenarios, the ability to understand spatial relationships between objects is particularly vital for agents to effectively interact with the environment (Anderson et al., 2018;Padmakumar et al., 2022;Li et al., 2022b), serving a wide range of embodied models (Du et al., 2024;Zhang et al.,</p>
<p>EmbSpatial-Bench Universal Image Source: VG, COCO</p>
<p>[True] A dog to the right of a bench.</p>
<p>[False] A dog to the left of a bench.</p>
<p>Embodied 3D Scene</p>
<p>Figure 1: Comparison between EmbSpatial-Bench and existing benchmarks for spatial understanding.Existing benchmarks may determine spatial relationships based on a coordinate system centered on the subject in the image (upper right), whereas EmbSpatial-Bench consistently determines them from an egocentric perspective.2021; Driess et al., 2023).Evaluating and enhancing such capabilities of LVLMs is essential for constructing LVLM-driven embodied agents.Yet, existing benchmarks are not suitable for accurately assessing such capabilities.</p>
<p>In this paper, we argue that two important features should be considered for excellent evaluation of spatial understanding abilities in embodied tasks.First, the spatial relationships should be described from the egocentric perspective, for the reason that agents take themselves as the center of coordinates to follow instructions and infer decisions in embodied tasks.However, previous benchmarks for spatial understanding (Liu et al., 2023a) tend to depict spatial relationships from the perspective of subject within images, as illustrated in Figure 1.Second, the visual scenes for evaluation should be consistent with that in embodied tasks.Nevertheless, existing benchmarks (Liu et al., 2023a;Kamath et al., 2023) are mainly constructed from universal image-text datasets like MSCOCO (Lin et al., 2014) and VG (Krishna et al., 2017) which are weakly related to embodied scenarios.</p>
<p>To meet aforementioned requirements, we establish EmbSpatial-Bench, a benchmark for evaluating spatial understanding abilities of LVLMs in embodied environments.As shown in Figure 1, we focus on six spatial relationships described from the egocentric perspective, including above, below, left, right, close and far, which completely covers three dimensions of the coordinates.The benchmark is organized into the format of multiple-choice questions.The images used for evaluation are directly collected from embodied 3D scenes, namely MP3D (Chang et al., 2017), AI2-THOR (Kolve et al., 2017) and ScanNet (Dai et al., 2017).</p>
<p>Based on EmbSpatial-Bench, various LVLMs have been assessed.Experimental results indicate the poor embodied spatial understanding of current LVLMs, including GPT-4V (OpenAI, 2023) and Qwen-VL-Max (Bai et al., 2023).To address the issue, we further construct an instruction-tuning dataset, EmbSpatial-SFT, to empower LVLMs with embodied spatial understanding ability.LVLMs fine-tuned on EmbSpatial-SFT consistently demonstrate improved spatial perception abilities across different scenarios.1</p>
<p>EmbSpatial-Bench</p>
<p>Unlike existing benchmarks built on 2D images (Liu et al., 2023a), EmbSpatial-Bench is constructed from 3D scenes.Figure 2 illustrates the construction pipeline.We first generate target images from 3D scenes and extract spatial relations among objects.Then, we generate QA pairs and conduct filtering.Section 2.1 provides detailed explanations of each part, while Section 2.2 offers statistics of the benchmark.</p>
<p>Dataset Construction</p>
<p>Spatial Image Sources.Current embodied 3D simulators offer comprehensive annotations for tasks such as visual navigation (Chang et al., 2017) and room rearrangement (Weihs et al., 2021), making them ideal for constructing a challenging benchmark to evaluate embodied spatial understanding.Therefore, we choose MP3D (Chang et al., 2017), ScanNet (Dai et al., 2017) and AI2-THOR (Kolve et al., 2017).Specifically, we utilize the test scenes from MP3D and validation scenes from ScanNet and A. Within each 3D scene, we randomly select viewpoints and capture the corresponding RGB-D images accordingly.In AI2-THOR, we select 7 types of household tasks from ALFRED (Shridhar et al., 2020), spanning 93 different scenes.During task execution, we identify key RGB-D images based on the dataset's PDDL (Aeronautiques et al., 1998)</p>
<p>annotations.(See Appendix A).</p>
<p>Spatial Relation Extraction.Instead of relying on object detectors (Tejas et al., 2023), we extract spatial relations directly from well-annotated 3D datasets.For each object in each image, we can utilize the camera parameters along with the corresponding 3D coordinates to obtain its 2D coordinates in the image (in the form of bounding boxes).With the 2D annotations, we extract the spatial relation triples with non-overlapping bounding boxes.We consider six spatial relationships from the viewer's perspective: above, below, left, right, close and far.For the first four types, we determine the spatial relation based on position of the entire bounding boxes.For instance, if the entire bounding box of object A is located to the left of object B, we consider the relationship between A and B as A is left of B. For the other two types, we use the average depth within the bounding box to QA Generation.The format of our benchmark is multiple-choice questions, a widely adopted approach in various LVLM benchmarks (Liu et al., 2023c;Li et al., 2023d).For the relations above, below, left and right, we design 5 templates to generate questions asking spatial relations between objects, with unrelated relations provided as false options.For the relations far and close, we aggregate the relation triples for each image and generate questions for identifying the farthest or closest one among the given objects in the image.</p>
<p>Filtering and Human Verification.To ensure the reliability of our benchmark, we initially filter out QA pairs with overly large or small bounding boxes, while maintaining a balanced distribution of spatial relations.Subsequently, we check each sample and remove the inappropriate questions that referring unclear objects or wrong spatial relationships.See appendix A.4 for more filtering and human verification details.</p>
<p>Dataset Statistics</p>
<p>As shown in Table 1, the constructed benchmark comprises a total of 3,640 QA pairs, covering 294 object categories and 6 spatial relationships.The distribution of top 30 object categories can be observed in Figure 3.The set of objects is the collection among samples from three embodied datasets.</p>
<p>Indoor objects such as "chair", "bowl" and "window" are the most frequent across different scenes.The distribution of most common spatial relation triples are depicted in Figure 4, highlighting the diversity of the combination of object spatial relations present in our benchmark.We also maintain a balanced distribution of spatial relations (details in Appendix A).The diversity and balance of the data enhance the reliability of our benchmark.</p>
<p>EmbSpatial-SFT</p>
<p>To further improve LVLMs' capacity in embodied spatial understanding, we construct an instructiontuning dataset, EmbSpatial-SFT, which provides QA data for two tasks: spatial relationship identification and object localization.The former task setting is consistent with EmbSpatial-Bench, while the latter serves as an auxiliary task to enhance the model's ability to ground target objects.The auxiliary task can be considered as the foundational skill for relationship identification.EmbSpatial-SFT is solely built on the training split of MP3D.In this way, we can still conduct zero-shot evaluations of the instruction-tuned models using data from the other two scenes in EmbSpatial-Bench.Object Localization.Based on the coordinates of objects in 2D images, we construct object localization data in the form of the object grounding task (Kazemzadeh et al., 2014).The model is supposed to answer the location of inquired objects.</p>
<p>Spatial Relation</p>
<p>The location is represented in the textual format of bounding boxes, following Chen et al. (2023a).</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Based on EmbSpatial-Bench, we conduct zeroshot evaluation of current LVLMs, using accuracy as the metric.Two evaluation strategies are employed.The first one is the generation-based strategy, which directly uses predicted options from the textual outputs of models.Considering the insufficient instruction-following ability of some LVLMs, we also employed a likelihood strategy, using the option with the highest probability generated by the model (Li et al., 2023d).Please refer to Appendix B for more evaluation details.</p>
<p>Zero-shot Performance</p>
<p>Instruction Tuning on EmbSpatial-SFT</p>
<p>Furthermore, we fine-tune MiniGPT-v2 on EmbSpatial-SFT, to explore whether the data could further enhance the model's spatial understanding capabilities.The trainable parameters include the visual connection module and LoRA (Hu et al., 2021) modules in the LLM backbone.</p>
<p>Main Results.According to Table 3, under the likelihood evaluation strategy, learning from EmbSpatial-SFT consistently improves the performance across both in-domain and out-domain environments, with an increase of 34.25% in the overall accuracy.Though not as significant as that under likelihood strategy, the evaluated results under generation strategy still demonstrate an adequate performance improvement (+9.04% overall) after instruction-tuning.The improvement in AI2-THOR is less than in ScanNet, which we attribute to AI2-THOR primarily consisting of simulated scenes, unlike the real-world scenarios in MP3D and ScanNet.</p>
<p>Ablations.We further validate the effectiveness of finetuning LLM backbone with LoRA and the auxiliary object localization data.As shown in Table 3, tuning the LLM backbone with LoRA significantly contributes to the performance across all scenarios compared to the variant with a frozen LLM backbone.This phenomenon implies the necessity for the LLM backbone to learn corresponding reasoning abilities for spatial understanding, rather than solely adjusting the input visual representations.The auxiliary data also contribute to the performance across different embodied environments, leading to an overall improvement of 0.47% and 0.76% under generation strategy and likelihood strategy, respectively.</p>
<p>Large Vision-Language Models The prevalent LVLMs (Dai et al., 2023;Zeng et al., 2023) learn visual representations from abundant image-text interleaved datasets with a lightweight connection module.Further works (Tsai et al., 2023;Zheng et al., 2023) fine-tunes LVLMs-based architecture and obtain acceptable performance on embodied tasks, which preliminarily reveal the potential of LVLMs as embodied intelligence.However, these works neither evaluate nor empower LVLMs with spatial understanding ability, which is essential for various embodied tasks.</p>
<p>Benchmarks for Spatial Understanding.While there are numerous universal benchmarks available for LVLMs (Xu et al., 2023;Fu et al., 2023;Li et al., 2023d), dedicated benchmarks for evaluating spatial understanding remain scarce.VSR (Liu et al., 2023a) typically examines spatial relationships from the perspective of the subject within the image.What'sUp (Kamath et al., 2023) addresses data bias and generates uncluttered images to eliminate interference from unrelated objects.SR 2D (Tejas et al., 2023) focuses on evaluating textto-image generative model.However, all of them are built on COCO (Veit et al., 2016) or VG (Krishna et al., 2017) which are not consistent with the embodied scenarios.This lack of specialized benchmarks leaves the spatial understanding capabilities of LVLMs in embodied tasks unexplored.</p>
<p>Conclusion</p>
<p>In this work, we propose EmbSpatial-Bench, a benchmark to evaluate embodied spatial understanding of LVLMs.The evaluation results reveal the weak spatial understanding ability of current popular LVLMs.We further propose EmbSpatial-SFT, an instruction tuning dataset to enhance the capacity of LVLMs.Extensive experiments valid the effectiveness of each data component in our EmbSpatial-SFT, with the goal of empowering the spatial understanding ability of LVLMs.</p>
<p>Limitations</p>
<p>Spatial understanding in embodied environments is a crucial aspect of LVLMs' capabilities for embodied tasks.In this study, we advance towards this goal by constructing benchmark and instructiontuning datasets from well-annotated 3D embodied datasets.These datasets are derived from three widely used indoor embodied datasets, which may restrict their suitability for outdoor environments.Additionally, our study only investigates the English language, thus limiting the generalizability of the benchmark and findings to other languages.</p>
<p>Ethical Considerations</p>
<p>The benchmark and instruction-tuning data are built from publicly available embodied datasets, which include either photorealistic scenes or generated rendered scenes without any copyright issues.Besides, our data source does not contain any personal data, uniquely identifiable individuals, or offensive content.</p>
<p>Figure 2 :
2
Figure 2: Overview of the construction pipeline for EmbSpatial-Bench based on existing annotated 3D environments.</p>
<p>Figure 4 :
4
Figure 4: The top 10 most common triples from each spatial relation in EmbSpatial-Bench.</p>
<p>Table 1 :
12EMHFW&amp;RXQWFKDLUERZOZLQGRZWDEOHFDELQHWNQLIHSLFWXUHGHVNWHOHYLVLRQVLQNVKHOYHVWRPDWRFXUWDLQVRIDODPSSDQOHWWXFHPXJSRWDWREHGOHWWXFHVOLFHPLUURUSLOORZER[EDJGRRUWRLOHWWRPDWRVOLFHFXSFRXQWHUData Source #QA Pairs #Image #Object #SceneMatterport3D1,20192813326AI2-THOR1,2396839593ScanNet1,20057035175Overall3,6402,181294277
Figure 3: Distribution of top 30 object categories.Dataset Statistics of EmbSpatial-Bench determine which object is farther or closer.</p>
<p>Table 2 :
2
Zero-shot performance (Acc%) of LVLMs in EmbSpatial-Bench.Bold indicates the best results.
ModelGeneration LikelihoodBLIP2 (2023b)37.9935.71InstructBLIP (2023)38.8533.41Cheetor (2023a)24.5632.80Lynx (2023)29.0941.62mPlugOwl (2023)24.1227.42ImagebindLLM (2023)26.4633.46Shikra (2023b)28.3834.75MiniGPT4 (2023)23.5431.70MiniGPT-v2 (2023a)23.9343.85LLaVA-1.6 (2023b)35.1938.84GPT-4V (2023)36.07-Qwen-VL-Max (Bai et al., 2023)49.11-Human90.33-
Identification.Following the automatic pipeline in Section 2, We construct 25K training samples for spatial relation identification.</p>
<p>Table 2
2presents the zero-shot performance of 10open-source LVLMs and 2 closed-source models.The results indicate that current LVLMs, includingpowerful closed-source models like GPT-4V andQwen-VL-Max, have not demonstrated satisfactoryspatial understanding abilities in embodied scenes.The best performance among all LVLMs merelyreaches an accuracy of 49.11% (Generation) or43.85% (Likelihood) which is significantly lowerthan human performance (90.33%). We presentfailure cases of GPT-4V in Appendix C, revealingits poor abilities of both object localization andspatial relation identification. The versions of thesemodels can be found in Appendix B.3.</p>
<p>Table 3 :
3
Performance (Acc%) of MiniGPT-v2 tuned on EmbSpatial-SFT.OL stands for object localization while w/o LoRA indicates that only the connection module is fine-tuned.Bold indicates the best results.</p>
<p>https://github.com/mengfeidu/ EmbSpatial-Bench
AcknowledgementsThis work is supported by National Natural Science Foundation of China (No. 62176058) and National Key R&amp;D Program of China (2023YFF1204800).The project's computational resources are supported by CFFF platform of Fudan University.Appendix A Dataset Details A.1 AI2-THOR Image SelectionDue to the significant similarity between many images in the observation sequences for each task in AI2-THOR, filtering is necessary.Based on the detailed PDDL annotations from ALFRED(Shridhar et al., 2020), we select key images that show significant content changes after each sub-goal is reached as our benchmark image resources.A.2 Dataset StatisticsThe wordcloud of object categories can be observed in Figure5.The distribution of questions for each spatial relation is illustrated in Figure6.The diversity and balance of the data enhance to the reliability of our benchmark.A.3 Data CasesThree samples of EmbSpatial-Bench constructed from MP3D(Chang et al., 2017), AI2-THOR(Kolve et al., 2017)and ScanNet(Dai et al., 2017)are shown in Fig.7, Fig.8and Fig.9.A.4 Filtering and VerificationInitially, we will implement two primary filtering processes to enhance the robustness and quality of our benchmark.First, we filter out objects with excessively large or small bounding boxes.To exclude improperly displayed objects, we filter out spatial relationship triplets where the length or width of the bounding box is less than 50 or greater than half the length of the corresponding dimension of the image.After automated construction and filtering processes, the human verification is implemented to further ensure the correctness of our benchmark.Specifically, the correctness of each sample is examined by human from several aspects: 1) the objects involved in the question can be identified in the image uniquely and clearly; 2) the target object conforms to the described spatial relationship; 3) the negative options are indeed incorrect objects or relationships.Any sample that does not meet either of these conditions is discarded.Appendix B ExperimentsB.1 Experimental Details Implementation details.We useMiniGPT-v2 (Chen et al., 2023a)as a baseline LVLM for investigation.The architecture of MiniGPT-v2 comprises three components, including a vision encoder  , a linear connection layer and a large language model.We initialize the model parameters with the official checkpoint after its instruction-tuning.We finetune the connection layer and the large language model ofMiniGPT-v2 with LoRA (Hu et al., 2021).In our implementation, we set the LoRA rank, R r = 64 and scaling factor, R Î± = 16.Training and hyper-parameters.We adopt AdamW optimizer with a cosine learning rate scheduler during the finetune process.The model is finetuned for 25,000 steps on 4xV100 GPUs with a initial learning rate of 1e-5, a minimum learning rate of 1e-6, a warmup learning rate of 1e-6 and a global batch size of 16.The finetuning stage lasts around 10 hours.B.2 Evaluation StrategyFollowing the evaluation approach(Li et al., 2023d), we evaluate LVLMs with generation and likelihood strategy.The likelihood strategy relies on LVLMs' intrinsic nature as generative models and separates their instruction-following capacity from the capacity being evaluated.Given the image v, the question q, and N options C = {c i } N i=1 , the prediction can be determined by the generation likelihood of LVLM: Ä = arg maxB.4 Main Results of Each Spatial RelationWe have analyse the models' performance before and after instruct-tuning on different spatial relations, as shown in the table 4.After instruct-tuning on EmbSpatial-SFT, MiniGPT-v2 significantly improved or maintained comparable accuracy on various spatial relationship categories across different environments.In the likelihood evaluation, compared to the horizontal and vertical dimensions, performance in the depth dimension is significantly lower.We attribute this to the training data of LVLMsAppendix C GPT-4V CasesUtilizing the strong instruction following ability of GPT-4V, we delved deeper into the possible reasons for the poor performance of current LVLMs.Inspired by the two processes decoupling from spatial understanding, we prompt GPT-4V to inspect whether object localization or spatial relationships determination becomes a bottleneck.As shown in Figure10, the GPT-4V not only makes mistakes in object positioning, but also misjudge their spatial relationship when successfully localizing the objects involved.In the first case (left part), GPT-4V mistakenly positions the clock in top left corner to the top right corner, further leading to the incorrect selection of option with the word "right".In the second case (right part), GPT-4V successfully   Table4: Performance (Acc%) of MiniGPT-v2 and fine-tuned MiniGPT-v2 across different spatial relations.locates the positions of all object referred in the question, but incorrectly choose the pillow as the nearest object rather than the bed.The case study demonstrate the potential room for improvement in both two processes.
Pddl| the planning domain definition language. Constructions Aeronautiques, Adele Howe, Craig Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Wilkins Weld, Anthony Sri, Dave Barrett, Christianson, 1998Tech. Rep</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko SÃ¼nderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matter-port3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny, arXiv:2310.09478Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 2023aarXiv preprint</p>
<p>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023barXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias NieÃner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEE2017</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Delan: Dual-level alignment for visionand-language navigation by cross-modal contrastive learning. Mengfei Du, Binhao Wu, Jiwen Zhang, Zhihao Fan, Zejun Li, Ruipu Luo, Xuanjing Huang, Zhongyu Wei, arXiv:2404.019942024arXiv preprint</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, arXiv:2306.133942023arXiv preprint</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Llama-adapter v2: Parameter-efficient visual instruction model. 2023arXiv preprint</p>
<p>Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, arXiv:2309.03905Imagebind-llm: Multi-modality instruction tuning. 2023arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>What's" up" with vision-language models? investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, arXiv:2310.197852023arXiv preprint</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017arXiv preprint</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, arXiv:2308.04152Siliang Tang, and Yueting Zhuang. 2023a. Fine-tuning multimodal llms to follow zeroshot demonstrative instructions. arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023barXiv preprint</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022a</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023carXiv preprint</p>
<p>Mvptr: Multi-level semantic alignment for vision-language pre-training via multi-stage learning. Zejun Li, Zhihao Fan, Huaixiao Tou, Jingjing Chen, Zhongyu Wei, Xuanjing Huang, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022b</p>
<p>Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks. Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, arXiv:2310.025692023darXiv preprint</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, Lawrence Zitnick, European conference on computer vision. Springer2014</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023a</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.037442023barXiv preprint</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, arXiv:2307.06281Mmbench: Is your multi-modal model an all-around player?. 2023carXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Teach: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Benchmarking spatial relationships in text-to-image generation. Hamid Tejas, Besmira Gokhale, Vibhav Palangi, Eric Nushi, Ece Vineet, Chitta Horvitz, Yezhou Kamar, Yang Baral, arXiv:2212.100152023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Yao-Hung Hubert Tsai, Vansh Dhar, Jialu Li, Bowen Zhang, Jian Zhang, arXiv:2310.08669Multimodal large language model for visual navigation. 2023arXiv preprint</p>
<p>Coco-text: Dataset and benchmark for text detection and recognition in natural images. Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, Serge Belongie, arXiv:1601.071402016arXiv preprint</p>
<p>Visual room rearrangement. Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265Lvlm-ehub: A comprehensive evaluation benchmark for large visionlanguage models. 2023arXiv preprint</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, arXiv:2307.02469What matters in training a gpt4-style language model with multimodal inputs?. 2023arXiv preprint</p>
<p>Curriculum learning for vision-and-language navigation. Jiwen Zhang, Jianqing Fan, Jiajie Peng, Advances in Neural Information Processing Systems. 202134</p>
<p>Towards learning a generalist model for embodied navigation. Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang, arXiv:2312.020102023arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8287 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8287</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8287</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277633831</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.07080v1.pdf" target="_blank">DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math when presented with novel problems outside standard benchmarks. Going beyond final accuracy, we propose a deductive consistency metric to analyze chain-of-thought output from language models (LMs).Formally, deductive reasoning involves two subtasks: understanding a set of input premises and inferring the conclusions that follow from them. The proposed metric studies LMs' performance on these subtasks, with the goal of explaining LMs' reasoning errors on novel problems: how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops? Since existing benchmarks may be memorized, we develop a pipeline to evaluate LMs' deductive consistency on novel, perturbed versions of benchmark problems. On novel grade school math problems (GSM-8k), we find that LMs are fairly robust to increasing number of input premises, but suffer significant accuracy decay as the number of reasoning hops is increased. Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy. As we increase the number of solution steps using a synthetic dataset, prediction over multiple hops still remains the major source of error compared to understanding input premises. Other factors, such as shifts in language style or natural propagation of early errors do not explain the trends. Our analysis provides a new view to characterize LM reasoning -- as computations over a window of input premises and reasoning hops -- that can provide unified evaluation across problem domains.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8287.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8287.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 72B-parameter Qwen-2.5 variant instruction-tuned for math tasks; evaluated as a high-capacity math-specialized LM in the DeduCE pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 72B-parameter Qwen-2.5 model specialized for mathematical reasoning (math-specific fine-tuning reported by Qwen authors).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'templatized chain-of-thought (tCoT) used as injected premises', 'code-generation / executable reasoning graph (used to generate ground-truth steps)', 'prefix injection (partial CoT provided in context)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are prompted to emit a chain-of-thought; the pipeline templatizes an original CoT (tCoT) and converts to executable code to generate correct intermediate steps for novel (mutated) inputs. Evaluation supplies k-step prefixes (tCoT) as premises and asks the model to continue for l hops. No alternative inference algorithms (e.g., self-consistency) were used for this model in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>DeduCE varies the amount and style of CoT provided (prefix length k and hops l) and tests paraphrase styles (original, axiomatic, vanilla, reverse). Ablations compare post-training variants (RL-tuned, SFT/distilled) of Qwen models to test effect of tuning on reasoning behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated/novel versions) and SynDeduct (synthetic DAG path dataset); both evaluate deductive consistency by providing partial correct CoT prefixes and requiring the model to complete multiple reasoning hops.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>High base deductive consistency: Table 6 reports per-hop DC on GSM8K: Hop-1: 0.937 ± 0.0108, Hop-2: 0.9037 ± 0.0085, Hop-3: 0.8841 ± 0.0073, Hop-4: 0.8573 ± 0.0148, Hop-5: 0.8321 ± 0.0328. Overall trend: DC decays with hops (but stays relatively high compared to smaller models). Coverage across paraphrase styles was high (≈0.97–0.99).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Per-model behaviour shows strong base performance but measurable decay in deductive consistency as the number of hops increases; errors are mainly arithmetic/calculation errors that propagate across hops. The model is robust to increasing input-premise length (prefix k) but sensitive to the number of output hops l. Post-training (e.g., math specialization) increases base but does not eliminate hop decay.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Qwen-2.5-Math-72B achieves high base deductive consistency on novel mutated GSM8K problems but still exhibits a non-negligible decay as the number of reasoning hops increases; providing more correct premises (longer prefix) has relatively small effect compared to increasing hops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8287.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8287.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter Llama-3.3 instruction-tuned model evaluated for deductive consistency across varying prefix/hop settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 70B Llama-3.3 family model (Instruct variant) used as a large general-purpose LM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'templatized CoT (tCoT) prefixes', 'code-derived ground-truth for prefix generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same CoT-centric experimental paradigm: an expert LM produces an executable templatized CoT, mutated inputs are generated by running code, and the subject model receives k-step prefixes (tCoT) and must complete the remaining l hops.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prefix/hop sweep across GSM8K and SynDeduct; paraphrase interventions (Original, Para-ax, Para-van, Para-rev) tested to probe style sensitivity; ablations compare distilled/RL variants of Llama family where available.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated) and SynDeduct synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K per-hop DC (Table 6): Hop-1: 0.89 ± 0.0135, Hop-2: 0.8274 ± 0.0219, Hop-3: 0.7909 ± 0.0215, Hop-4: 0.7669 ± 0.0206, Hop-5: 0.7079 ± 0.0114. Shows clear decline across hops (~0.18 absolute drop from hop1 to hop5). Coverage across paraphrases ≈0.96–0.97.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Larger Llama variant has higher base but shows substantial hop-related decay; model errors dominated by arithmetic/calculation mistakes and error propagation, with limited sensitivity to the particular linguistic paraphrase of premises (small effect sizes despite statistical significance).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Llama-3.3-70B is robust to adding more input premises but exhibits meaningful decay as the number of reasoning hops increases; paraphrase style has minor effect on deductive consistency relative to hops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8287.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8287.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phi-4 (a larger/model variant reported in the paper) evaluated as a model trained on synthetic data and included in deductions experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LM (Phi-4) used in experiments; described as having been trained on synthetic data (per paper text) and included among subject models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'tCoT prefixes', 'prefix injection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Phi-4 is evaluated with same DeduCE pipeline: given templatized CoT prefixes (k steps) the model must produce remaining steps (l hops). The model's internal reasoning remains CoT-based in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prefix/hop sweeps on GSM8K and SynDeduct; paraphrase-style interventions applied.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated) and SynDeduct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K per-hop DC (Table 6): Hop-1: 0.8911 ± 0.0238, Hop-2: 0.8365 ± 0.0278, Hop-3: 0.8103 ± 0.0285, Hop-4: 0.7929 ± 0.0228, Hop-5: 0.7612 ± 0.0117. Shows decline across hops (~0.13 absolute drop from hop1 to hop5).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Phi-4 demonstrates relatively high base consistency and moderate resilience to hops; overall errors still dominated by arithmetic/calculation mistakes and propagation. Larger token windows and synthetic pretraining help but do not eliminate hop decay.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Training on synthetic data and larger capacity improves base deductive consistency and reduces, but does not remove, hop-related decay in deductive consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8287.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8287.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller 8B-parameter Llama-3 instruct-tuned model evaluated as a lower-capacity baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 8B-parameter Llama-3 model used as a smaller-size comparator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'tCoT prefixes']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same CoT-based prompting: receive templatized CoT prefixes and continue reasoning; no alternative reasoning algorithms were used.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prefix/hop sweeps and paraphrase style tests; compared against larger models and post-training ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated) and SynDeduct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K per-hop DC (Table 6): Hop-1: 0.7629 ± 0.0217, Hop-2: 0.6572 ± 0.0227, Hop-3: 0.5777 ± 0.0165, Hop-4: 0.5254 ± 0.0154, Hop-5: 0.4988 ± 0.0264. Large absolute decay across hops (~0.26 drop from hop1 to hop5).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Smaller model exhibits much larger hop-sensitivity and lower base consistency; arithmetic errors and error propagation dominate; token/window limits noted as a factor for long chains for all models but especially harmful for smaller models with smaller context capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Smaller models suffer much larger decline in deductive consistency with increasing hops, indicating capacity and/or training-data differences matter for multi-hop deductive robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8287.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8287.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter math-specialized Qwen model evaluated as a smaller math-focused model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 7B Qwen variant specialized for math tasks; included to measure effect of math-specific fine-tuning at small scale.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'tCoT prefixes']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Operated under the same DeduCE pipeline; received templatized CoT prefixes and continued generation to complete hops.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prefix/hop sweep on GSM8K and SynDeduct; compared with its base (non-math) counterpart and RL/SFT/Distilled variants in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated) and SynDeduct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K per-hop DC (Table 6): Hop-1: 0.8843 ± 0.0179, Hop-2: 0.854 ± 0.0144, Hop-3: 0.8456 ± 0.0307, Hop-4: 0.8283 ± 0.0440, Hop-5: 0.8409 ± 0.039 (some variability across hops but overall high base and modest decay).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Math-specialized small model shows relatively high base DC compared to similarly sized general models, suggesting task-specialized fine-tuning can increase base performance but does not fully remove hop decay; ablations show RL tuning vs SFT tuning have different effects (see ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Task-specialized fine-tuning (math SFT) raises base deductive consistency on in-distribution tasks but does not prevent decay across hops and may not generalize to unseen synthetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8287.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8287.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3.5-mini-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3.5-mini-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small instruction-tuned Phi-3.5 mini model used as a low-capacity baseline in the DeduCE evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-3.5-mini-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small instruction-tuned Phi-3.5 mini model evaluated for deductive consistency as a low-capacity baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'tCoT prefixes']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same pipeline: model receives templatized CoT prefixes and is asked to continue reasoning; no diversity of inference strategies applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prefix/hop sweeps and comparisons against larger/specialized models; included in ablations exploring effect of fine-tuning and RL/SFT/distillation variants where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (mutated) and SynDeduct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GSM8K per-hop DC (Table 6 partial): Hop-1: 0.8563 ± 0.0114, Hop-2: 0.7874 ± 0.0210, Hop-3: 0.7602 ± 0.0096, Hop-4: 0.6865 ± 0.0571, Hop-5: 0.6616 ± 0.0488. Shows substantial hop decay and higher variance.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Small model more prone to calculation errors and propagation; lower base and larger decay than bigger models; behavior consistent with capacity-limited multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Lower-capacity models display worse multi-hop deductive consistency and larger degradation as hops increase, reinforcing that capacity and training data/strategy affect multi-hop reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Gsmsymbolic: Understanding the limitations of mathematical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 1)</em></li>
                <li>Testing the general deductive reasoning capacity of large language models using OOD examples <em>(Rating: 1)</em></li>
                <li>Challenging big-bench tasks and whether chain-of-thought can solve them <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8287",
    "paper_id": "paper-277633831",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Qwen-2.5-Math-72B-Instruct",
            "name_full": "Qwen-2.5-Math-72B-Instruct",
            "brief_description": "A 72B-parameter Qwen-2.5 variant instruction-tuned for math tasks; evaluated as a high-capacity math-specialized LM in the DeduCE pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-Math-72B-Instruct",
            "model_description": "Instruction-tuned 72B-parameter Qwen-2.5 model specialized for mathematical reasoning (math-specific fine-tuning reported by Qwen authors).",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "templatized chain-of-thought (tCoT) used as injected premises",
                "code-generation / executable reasoning graph (used to generate ground-truth steps)",
                "prefix injection (partial CoT provided in context)"
            ],
            "reasoning_methods_description": "Models are prompted to emit a chain-of-thought; the pipeline templatizes an original CoT (tCoT) and converts to executable code to generate correct intermediate steps for novel (mutated) inputs. Evaluation supplies k-step prefixes (tCoT) as premises and asks the model to continue for l hops. No alternative inference algorithms (e.g., self-consistency) were used for this model in these experiments.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "DeduCE varies the amount and style of CoT provided (prefix length k and hops l) and tests paraphrase styles (original, axiomatic, vanilla, reverse). Ablations compare post-training variants (RL-tuned, SFT/distilled) of Qwen models to test effect of tuning on reasoning behaviour.",
            "task_or_benchmark": "GSM8K (mutated/novel versions) and SynDeduct (synthetic DAG path dataset); both evaluate deductive consistency by providing partial correct CoT prefixes and requiring the model to complete multiple reasoning hops.",
            "performance_results": "High base deductive consistency: Table 6 reports per-hop DC on GSM8K: Hop-1: 0.937 ± 0.0108, Hop-2: 0.9037 ± 0.0085, Hop-3: 0.8841 ± 0.0073, Hop-4: 0.8573 ± 0.0148, Hop-5: 0.8321 ± 0.0328. Overall trend: DC decays with hops (but stays relatively high compared to smaller models). Coverage across paraphrase styles was high (≈0.97–0.99).",
            "qualitative_findings": "Per-model behaviour shows strong base performance but measurable decay in deductive consistency as the number of hops increases; errors are mainly arithmetic/calculation errors that propagate across hops. The model is robust to increasing input-premise length (prefix k) but sensitive to the number of output hops l. Post-training (e.g., math specialization) increases base but does not eliminate hop decay.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Qwen-2.5-Math-72B achieves high base deductive consistency on novel mutated GSM8K problems but still exhibits a non-negligible decay as the number of reasoning hops increases; providing more correct premises (longer prefix) has relatively small effect compared to increasing hops.",
            "uuid": "e8287.0",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3.3-70B-Instruct",
            "name_full": "Llama-3.3-70B-Instruct",
            "brief_description": "A 70B-parameter Llama-3.3 instruction-tuned model evaluated for deductive consistency across varying prefix/hop settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.3-70B-Instruct",
            "model_description": "Instruction-tuned 70B Llama-3.3 family model (Instruct variant) used as a large general-purpose LM in experiments.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "templatized CoT (tCoT) prefixes",
                "code-derived ground-truth for prefix generation"
            ],
            "reasoning_methods_description": "Same CoT-centric experimental paradigm: an expert LM produces an executable templatized CoT, mutated inputs are generated by running code, and the subject model receives k-step prefixes (tCoT) and must complete the remaining l hops.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Prefix/hop sweep across GSM8K and SynDeduct; paraphrase interventions (Original, Para-ax, Para-van, Para-rev) tested to probe style sensitivity; ablations compare distilled/RL variants of Llama family where available.",
            "task_or_benchmark": "GSM8K (mutated) and SynDeduct synthetic dataset.",
            "performance_results": "GSM8K per-hop DC (Table 6): Hop-1: 0.89 ± 0.0135, Hop-2: 0.8274 ± 0.0219, Hop-3: 0.7909 ± 0.0215, Hop-4: 0.7669 ± 0.0206, Hop-5: 0.7079 ± 0.0114. Shows clear decline across hops (~0.18 absolute drop from hop1 to hop5). Coverage across paraphrases ≈0.96–0.97.",
            "qualitative_findings": "Larger Llama variant has higher base but shows substantial hop-related decay; model errors dominated by arithmetic/calculation mistakes and error propagation, with limited sensitivity to the particular linguistic paraphrase of premises (small effect sizes despite statistical significance).",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Llama-3.3-70B is robust to adding more input premises but exhibits meaningful decay as the number of reasoning hops increases; paraphrase style has minor effect on deductive consistency relative to hops.",
            "uuid": "e8287.1",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Phi-4",
            "name_full": "Phi-4",
            "brief_description": "Phi-4 (a larger/model variant reported in the paper) evaluated as a model trained on synthetic data and included in deductions experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4",
            "model_description": "Large LM (Phi-4) used in experiments; described as having been trained on synthetic data (per paper text) and included among subject models.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "tCoT prefixes",
                "prefix injection"
            ],
            "reasoning_methods_description": "Phi-4 is evaluated with same DeduCE pipeline: given templatized CoT prefixes (k steps) the model must produce remaining steps (l hops). The model's internal reasoning remains CoT-based in these experiments.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Prefix/hop sweeps on GSM8K and SynDeduct; paraphrase-style interventions applied.",
            "task_or_benchmark": "GSM8K (mutated) and SynDeduct.",
            "performance_results": "GSM8K per-hop DC (Table 6): Hop-1: 0.8911 ± 0.0238, Hop-2: 0.8365 ± 0.0278, Hop-3: 0.8103 ± 0.0285, Hop-4: 0.7929 ± 0.0228, Hop-5: 0.7612 ± 0.0117. Shows decline across hops (~0.13 absolute drop from hop1 to hop5).",
            "qualitative_findings": "Phi-4 demonstrates relatively high base consistency and moderate resilience to hops; overall errors still dominated by arithmetic/calculation mistakes and propagation. Larger token windows and synthetic pretraining help but do not eliminate hop decay.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Training on synthetic data and larger capacity improves base deductive consistency and reduces, but does not remove, hop-related decay in deductive consistency.",
            "uuid": "e8287.2",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3-8B-Instruct",
            "name_full": "Llama-3-8B-Instruct",
            "brief_description": "A smaller 8B-parameter Llama-3 instruct-tuned model evaluated as a lower-capacity baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-Instruct",
            "model_description": "Instruction-tuned 8B-parameter Llama-3 model used as a smaller-size comparator in experiments.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "tCoT prefixes"
            ],
            "reasoning_methods_description": "Same CoT-based prompting: receive templatized CoT prefixes and continue reasoning; no alternative reasoning algorithms were used.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Prefix/hop sweeps and paraphrase style tests; compared against larger models and post-training ablations.",
            "task_or_benchmark": "GSM8K (mutated) and SynDeduct.",
            "performance_results": "GSM8K per-hop DC (Table 6): Hop-1: 0.7629 ± 0.0217, Hop-2: 0.6572 ± 0.0227, Hop-3: 0.5777 ± 0.0165, Hop-4: 0.5254 ± 0.0154, Hop-5: 0.4988 ± 0.0264. Large absolute decay across hops (~0.26 drop from hop1 to hop5).",
            "qualitative_findings": "Smaller model exhibits much larger hop-sensitivity and lower base consistency; arithmetic errors and error propagation dominate; token/window limits noted as a factor for long chains for all models but especially harmful for smaller models with smaller context capacity.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Smaller models suffer much larger decline in deductive consistency with increasing hops, indicating capacity and/or training-data differences matter for multi-hop deductive robustness.",
            "uuid": "e8287.3",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Qwen-2.5-Math-7B-Instruct",
            "name_full": "Qwen-2.5-Math-7B-Instruct",
            "brief_description": "A 7B-parameter math-specialized Qwen model evaluated as a smaller math-focused model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-Math-7B-Instruct",
            "model_description": "Instruction-tuned 7B Qwen variant specialized for math tasks; included to measure effect of math-specific fine-tuning at small scale.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "tCoT prefixes"
            ],
            "reasoning_methods_description": "Operated under the same DeduCE pipeline; received templatized CoT prefixes and continued generation to complete hops.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Prefix/hop sweep on GSM8K and SynDeduct; compared with its base (non-math) counterpart and RL/SFT/Distilled variants in ablations.",
            "task_or_benchmark": "GSM8K (mutated) and SynDeduct.",
            "performance_results": "GSM8K per-hop DC (Table 6): Hop-1: 0.8843 ± 0.0179, Hop-2: 0.854 ± 0.0144, Hop-3: 0.8456 ± 0.0307, Hop-4: 0.8283 ± 0.0440, Hop-5: 0.8409 ± 0.039 (some variability across hops but overall high base and modest decay).",
            "qualitative_findings": "Math-specialized small model shows relatively high base DC compared to similarly sized general models, suggesting task-specialized fine-tuning can increase base performance but does not fully remove hop decay; ablations show RL tuning vs SFT tuning have different effects (see ablations).",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Task-specialized fine-tuning (math SFT) raises base deductive consistency on in-distribution tasks but does not prevent decay across hops and may not generalize to unseen synthetic datasets.",
            "uuid": "e8287.4",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Phi-3.5-mini-instruct",
            "name_full": "Phi-3.5-mini-instruct",
            "brief_description": "A small instruction-tuned Phi-3.5 mini model used as a low-capacity baseline in the DeduCE evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-3.5-mini-instruct",
            "model_description": "Small instruction-tuned Phi-3.5 mini model evaluated for deductive consistency as a low-capacity baseline.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "tCoT prefixes"
            ],
            "reasoning_methods_description": "Same pipeline: model receives templatized CoT prefixes and is asked to continue reasoning; no diversity of inference strategies applied.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Prefix/hop sweeps and comparisons against larger/specialized models; included in ablations exploring effect of fine-tuning and RL/SFT/distillation variants where applicable.",
            "task_or_benchmark": "GSM8K (mutated) and SynDeduct.",
            "performance_results": "GSM8K per-hop DC (Table 6 partial): Hop-1: 0.8563 ± 0.0114, Hop-2: 0.7874 ± 0.0210, Hop-3: 0.7602 ± 0.0096, Hop-4: 0.6865 ± 0.0571, Hop-5: 0.6616 ± 0.0488. Shows substantial hop decay and higher variance.",
            "qualitative_findings": "Small model more prone to calculation errors and propagation; lower base and larger decay than bigger models; behavior consistent with capacity-limited multi-hop reasoning.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Lower-capacity models display worse multi-hop deductive consistency and larger degradation as hops increase, reinforcing that capacity and training data/strategy affect multi-hop reasoning robustness.",
            "uuid": "e8287.5",
            "source_info": {
                "paper_title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Gsmsymbolic: Understanding the limitations of mathematical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "gsmsymbolic_understanding_the_limitations_of_mathematical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 1,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "Testing the general deductive reasoning capacity of large language models using OOD examples",
            "rating": 1,
            "sanitized_title": "testing_the_general_deductive_reasoning_capacity_of_large_language_models_using_ood_examples"
        },
        {
            "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "rating": 1,
            "sanitized_title": "challenging_bigbench_tasks_and_whether_chainofthought_can_solve_them"
        }
    ],
    "cost": 0.016407249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEDUCE: DEDUCTIVE CONSISTENCY AS A FRAMEWORK TO EVALUATE LLM REASONING
9 Apr 2025</p>
<p>Atharva Pandey t-atpandey@microsoft.com 
Microsoft Research
India</p>
<p>Kshitij Dubey t-ksdubey@microsoft.com 
Microsoft Research
India</p>
<p>Rahul Sharma 
Microsoft Research
India</p>
<p>Amit Sharma 
Microsoft Research
India</p>
<p>DEDUCE: DEDUCTIVE CONSISTENCY AS A FRAMEWORK TO EVALUATE LLM REASONING
9 Apr 2025CE73B629DAF44D32551ACB4A02433756arXiv:2504.07080v1[cs.CL]
Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math when presented with novel problems outside standard benchmarks.Going beyond final accuracy, we propose a deductive consistency metric to analyze chain-of-thought output from language models (LMs).Formally, deductive reasoning involves two subtasks: understanding a set of input premises and inferring the conclusions that follow from them.The proposed metric studies LMs' performance on these subtasks, with the goal of explaining LMs' reasoning errors on novel problems: how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops?Since existing benchmarks may be memorized, we develop a pipeline to evaluate LMs' deductive consistency on novel, perturbed versions of benchmark problems.On novel grade school math problems (GSM-8k), we find that LMs are fairly robust to increasing number of input premises, but suffer significant accuracy decay as the number of reasoning hops is increased.Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy.As we increase the number of solution steps using a synthetic dataset, prediction over multiple hops still remains the major source of error compared to understanding input premises.Other factors, such as shifts in language style or natural propagation of early errors do not explain the trends.Our analysis provides a new view to characterize LM reasoning-as computations over a window of input premises and reasoning hops-that can provide unified evaluation across problem domains.</p>
<p>INTRODUCTION</p>
<p>Chain-of-thought prompting, the practice of instructing a language model (LM) to output its intermediate steps before the final answer, has led to significant gains on reasoning tasks such as math (Wei et al., 2024), logic (Saparov &amp; He, 2023;Saparov et al., 2023;Parmar et al., 2024b), and language tasks (Suzgun et al., 2022).Recent work shows that models can also solve Olympiad-level problems (Gao et al., 2024).However, a parallel stream of work shows that LLMs are sensitive to simple perturbations of the original question, such as changing the numeric values occurring in grade school word problems (Mirzadeh et al., 2024;Srivastava et al., 2024).Importantly, these perturbation do not change the difficulty level of a problem, yet accuracy of frontier LLMs such as GPT-4 significantly reduces.Other studies show a similar reasoning gap between original and perturbed problems covering math, logic (Wu et al., 2024) and syllogisms (Lewis &amp; Mitchell, 2024b), but why this gap arises is less explored.</p>
<p>In this work, we analyze the reasoning capabilities of language models (LMs) by assessing their consistency with an ideal deductive reasoner.To this end, we introduce deductive consistency, a metric that evaluates an AI system's reasoning validity based on its intermediate steps.We conceptualize a reasoning problem as comprising a set of premises and a target predicate or variable.Given a set of inference rules, the goal of a reasoning system is to determine whether the target predicate can be derived from the input premises (or infer the value of the target variable).An ideal deductive reasoner operates by systematically deriving the correct reasoning steps from any given set of premises, eventually reaching the target predicate if it is logically deducible.Our key insight is to consider the chain-of-thought trace of an LM as a deductive proof and construct partial traces to evaluate against an ideal reasoner.Specifically, for a given reasoning problem, we provide an LM with the initial premises (corresponding to problem definition) and append the first few correct reasoning steps before allowing it to continue its generation.This approach enables us to measure consistency with an ideal deductive reasoner across different input premises and reasoning hops, offering a more fine-grained evaluation of LM reasoning beyond final accuracy.------------------------------------------- --------------------------------------------Template Variable List ------------------------------------------ Our method requires access to a single valid solution to compute deductive consistency across multiple premises and reasoning hops.Given a benchmark problem, we use pre-trained LMs to templatize its solution and obtain an executable code solution.Then we update the variables' values in the code and generate a novel problem on which the subject LM is evaluated.For any (number of premises, number of hops) combination, we assess whether the subject LM's solution contains the correct values of the variables.</p>
<p>Template Question</p>
<p>For real world problems, however, obtaining an ideal reasoner is a key challenge.In addition, an LM may have memorized existing benchmark problems, thus making it difficult to evaluate deductive consistency on existing benchmarks.We find a solution by evaluating LM's reasoning on novel, perturbed versions of benchmark problems while utilizing the memorization capabilities of recent LMs to obtain an ideal reasoner based on the original benchmark problem.Specifically, as Figure 1 shows, the pipeline first produces a chain-of-thought solution for the original benchmark problem, then translates it into a general solution (python code).We apply multiple correctness checks to filter out original problems that lack a corresponding code representation.For each novel problem, we update the Python code to reflect the new variable values, then convert the modified code back into natural language to generate the correct solution steps, which in turn can be input as additional premises to a target LM.We refer to this framework as DeduCE-Deductive Consistency Evaluation of LMs.</p>
<p>Importantly, the DeduCE framework requires a single valid solution for a problem, not the full ideal reasoner.Following (Mirzadeh et al., 2024), we construct novel problems by altering variable values in the original benchmark problem.</p>
<p>To explain LMs' decreased accuracy on novel problems, we provide a definition for deductive consistency that is parameterized by the number of input premises and output steps.Using this definition, we study how consistency changes as 1) partial CoTs include larger input premises, and 2) the task involves higher number of output steps to predict.We formalize both observations as the average slope or decay in deductive consistency as number of input premises or output steps are increased.</p>
<p>We apply the proposed framework to evaluate multiple LMs on grade school math problems, using GSM8K (Cobbe et al., 2021) and a synthetic dataset.As shown by past work, there is a significant drop in final accuracy on GSM8K between original and mutated problems.Our main findings are:</p>
<p>• Deductive consistency of LMs is robust to the length of the input premises for GSM8K, but sensitive to the number of reasoning hops.As the number of reasoning hops increases from 1 to 5, deductive consistency falls by 15-30% (Figure 2) .Note that this effect was likely masked in original benchmark problems by memorization.</p>
<p>• Post-training methods like supervised fine-tuning and reinforcement learning enhance task-specific patterns rather than general deductive reasoning, even for the same underlying reasoning task.</p>
<p>• Other hypotheses, such as novel problems inducing a different language style in the solution or early errors that propagate, are unable to explain the difference in final accuracy between original and mutated problems.</p>
<p>The maximum deviation seen is nearly 5% (Figure 5)</p>
<p>RELATED WORK</p>
<p>Evaluating reasoning beyond memorization.To avoid confounding in experiments due to dataset memorization, novel datasets based on synthetic data have been proposed (Zhu et al., 2023).To keep real world relevance, recent work propose perturbed or novel versions of existing datasets, for math (Mirzadeh et al., 2024;Zhang et al., 2024), analogical reasoning (Lewis &amp; Mitchell, 2024b;a), and many other diverse tasks (Wu et al., 2024).</p>
<p>Metrics for reasoning beyond final answer's accuracy.(Xu et al., 2024) four different types of metrics based on answer correctness and explanation correctness.(Seals &amp; Shalin, 2024) test deductive reasoning based on logical questions.We aim to provide a general metric for any reasoning task.Another stream of work checks language models' ability to detect errors in a solution (Zeng et al., 2024) and fix any detected errors (Singh et al., 2024).</p>
<p>Deductive reasoning in LLMs.LogicBench evaluates various models on natural language problems over propositional, first order, and non-monotonic logic (Parmar et al., 2024a).Other examples include analyzing categorical syllogisms (Zong &amp; Lin, 2024) and proving theorems in intuitionistic propositional logic (An et al., 2024).</p>
<p>DEFINING DEDUCTIVE CONSISTENCY</p>
<p>Given a consistent proof system S = ⟨L, R⟩, where L is the underlying logical language and R is the set of inference rules, let P ⊂ L denote the set of premises and Th(P ) ⊆ L the deductive closure of P under R. We assume access to a dataset of problems (P j , t j ) M j=1 ∼ D where P j denotes the set of premises and t j the target predicate to be proved in each problem j.Total number of problems in the dataset being M .</p>
<p>For example, consider a system with the language L of statements of the form X → Y and transitivity as a single inference rule,
W → X, X → Y ⇒ W → Y . A sample set of premises may be "A → B; B → C; B → D; C → F ; D → F ; E → F ; F → G"
, and a target predicate to be proved be "A → G".Assume that a reasoning system A (e.g., an AI reasoning model) produces the (incorrect) proof, A → D; A → E; E → G to conclude A → G. Beyond final accuracy, to evaluate the reasoning system's steps A on such problems, we define the Deductive Consistency metric.</p>
<p>DEDUCTIVE CONSISTENCY GIVEN A COMPLETE PROOF SYSTEM</p>
<p>Consistency evaluates the extent to which a reasoning system A agrees with the reference proof system S.For each problem d ∼ D, where d = (P , t), we generate a proof A(P , t) using A. A simple way of measuring reasoning performance may be to compute per-predicate accuracy.For each X i ∈ A(P , t): Cons(d j ) = i I X i |A(Pj ,tj )| where I z is an indicator function, 1 whenever z is correctly inferred by A and zero otherwise.For each X i , we use the complete proof system to verify whether it is true or not, given P and X k : {k : 1, 2, ..i − 1}.For the example proof above, the metric will be 0.67 because the second predicate is incorrect.</p>
<p>A key part of deductive reasoning is to process multiple input premises and determine the next correct predicates.Therefore, we extend the above metric to include longer input premises than in an original problem.We do so by sampling a (correct) proof from the reference proof system and adding the first k steps of the proof to the input premises P .The input premises now become P ′ = P ∪ S k (P , t) where S k (P , t) is the first k steps of the proof.Then, as k increases, we obtain a measure of how well a reasoning system can handle larger input premises.Let X ′ i be the proof steps generated by A(P ∪ S k (P , t), t).The reasoning system's goal is to complete the proof.
Cons(d j , k) = N i=k+1 I X ′ i |A(P ′ j , t j )|
Continuing our example with k = 1, the reference proof system may add the first step, "A → C" and let the target system A complete the rest.Here, the system may produce a faulty proof as before, A → C; A → E; A → G.However, with k = 2 and adding the first two steps "A → C; A → F ", the system A may produce A → C; A → F ; A → G, which is a correct proof.</p>
<p>However, the above metric has a right censoring issue (Gijbels, 2010): this measure of deductive consistency depends trivially on the number of input premises.In general, the difficulty of a proof is associated with the number of inference rules required to complete it.If many premises are already provided, the number of inference rules to reach the target predicate decreases and the problem becomes simpler.As a result, if we see an increase in deductive consistency as the number of reference proof steps are increased (as we see for the example above), it may simply be due to the fewer steps that need to be predicted, rather than due to the reasoning system's improved consistency after access to the reference system's guidance for the first few steps.Therefore, we also introduce a hops parameter, denoting the number of inference rules (steps) until which we evaluate the reasoning system.
DedCons(k, l) = dj ∼D I X ′ k+l+1,j</p>
<p>M</p>
<p>Compared to final accuracy, a key benefit of the our formulation is that we obtain multiple premises and evaluation sets from a single problem instance.This allows us to test a diverse set of deductive tasks even from a small number of problem instances.</p>
<p>DEDUCTIVE CONSISTENCY GIVEN A REFERENCE PROOF</p>
<p>While the above metric works for a complete proof system as the reference, in practice it is more common to have access to a limited reference system that can only generate a single proof S(P , t) given a problem.Therefore, we now assume access to a reference proof system that given a set of premises P and a target predicate t, can generate a proof involving predicates S(P , t) ⊆ Th(P ), representing the predicates within the closure that were proved while proving the main result for t.We call such a proof as the reference solution.Continuing the transitivity example, it would mean that we only have access to a reference proof solution, A → C; A → F ; A → G, but cannot assess the validity of a predicate outside it such as A → D.</p>
<p>Given a reasoning system's proof A(P , t), this implies that we can only verify the predicates that are also present in S(P , t).We therefore orient the deductive consistency metric to focus on the fraction of the verifiable predicates that are proved by A. For each Z i ∈ S(P , t)
DedCons(k, l) = dj ∼D I Z ′ k+l+1 ∈A(P ′ j ,tj ) M
where the numerator is an indicator function checking whether a given predicate Z i ∈ S(P , t) is also included in the proof by A. Note that the above metric introduces a bias because the reasoning system A may generate (true) predicates that are not in the reference solution (there can be multiple ways to solve the same problem).For instance, if a reasoning system produces a valid proof, A → D; A → F ; A → G, it will not have consistency=1 because the first step A → D is not a part of the reference solution S(P , t).</p>
<p>In such cases, the consistency metric above can under-estimate the deductive consistency-the reasoning system may be penalized for a producing a valid solution because its steps are different than that of the reference system.Hence, we also introduce a metric for coverage.The Coverage metric is defined as the expected proportion of variables in S(P , t) inferred by A. Let V S(P ,t) be the variables included in the reference solution.Then coverage is
Coverage = dj ∼D |V S(P j ,t) ∩ V A(P j ,t) | dj ∼D |V A(P j ,t) |
Thus, coverage measures how reliably verification of the set of predicates V measures consistency.When the coverage is high, an ideal reasoning system's deductive consistency should be a constant close to 1, independent of the number of premises k and the number of hops l.To measure this we define decay w.r.t.hops as follows:
µ ℓ = 1 ℓ max + 1 ℓmax ℓ=0 ℓ ℓ max , µ = 1 ℓ max + 1 ℓmax ℓ=0 E k [DedCons(k, ℓ)] Decay = − ℓmax ℓ=0 ℓ ℓmax − µ ℓ (E k [DedCons(k, ℓ)] − µ) ℓmax ℓ=0 ℓ ℓmax − µ ℓ 2
where ℓ max represents maximum number of hops in dataset.</p>
<p>EVALUATING DEDUCTIVE CONSISTENCY FOR LLMS</p>
<p>As noted above, we need at least one reference solution for a reasoning problem to evaluate deductive consistency.Given a benchmark reasoning dataset, we now provide a method to obtain such solutions and evaluate deductive consistency.</p>
<p>We use auxiliary expert LMs to help with transformation tasks.Specifically, we use Code Generation LM that generates executable reasoning graphs (Python Code), a Templatization LM that defines variable templates, and a Variable Extraction LM (Parser) that extracts predicate values for evaluation.The Subject LM is the model under evaluation.The entire pipeline is shown in Figure 1.</p>
<p>GENERATING CORRECT SOLUTION FOR A BENCHMARK PROBLEM</p>
<p>While generating the correct solution for a reasoning problem is hard, an expert LM may be able to generate candidate solutions.In particular, its capability for generating good candidate solutions may be higher for the original benchmark problem.We therefore adopt a generate-then-verify approach to select problems on which we can obtain a correct solution.We employ specific LMs to produce Code and templatized COT.The templatized COT is generated first, followed by the code.To build confidence in our candidate solution, we rely on the concept of internal consistency.We represent the solution in semantically equivalent forms, such as code and a templatetized version of the CoT, and perform multiple sanity checks to ensure its correctness.</p>
<p>One key sanity check involves verifying the equivalence between the code representation and the templatized Chain of Thought (tCoT).The variables in the code and the placeholders in the COT are equivalent.We validate this by ensuring that, given the same factual input, both representations yield identical variable values at each step.Once confirmed consistent with each other, the code serves as a symbolic representation of both the problem and its correct solution.</p>
<p>GENERATING A NOVEL PROBLEM BY PERTURBING THE BENCHMARK PROBLEM</p>
<p>Given the reasoning gap between existing benchmark problems and novel problems created by perturbation, we evaluate deductive consistency on novel problems only.This avoids any memorization concerns.In the current work, we adopt a simple perturbation: changing the values of variables in the problem statement.Other perturbations, such as changing variable names and adding irrelevant info (Mirzadeh et al., 2024) can be easily added.</p>
<p>Using the template generated earlier, we sample new values for the variables in the template.These sampled values serve as inputs to the reasoning code, which computes the corresponding solution for the novel problem.We substitute the computed values back into the template Chain of Thought (tCoT) and template Question (tQ) to produce modified questions (Q') and their reasoning steps (COT').COT' provides a detailed, step-by-step chain of reasoning for evaluation.</p>
<p>EVALUATING DEDUCTIVE CONSISTENCY</p>
<p>We evaluate deductive consistency by generating a perturbed question (Q') by modifying the seed premises and deriving its reasoning steps (COT) using the templatized Chain of Thought (tCoT).We then substitute intermediate premises into placeholders in tCoT by evaluating the Code with the same premises that generated the perturbed questions.</p>
<p>Through this process, we traverse a computation graph that serves as a proof Directed Acyclic Graph (DAG).Each link in the DAG represents a reasoning step, analogous to the transitivity of premises in a formal proof.For example, the sequence A → C; A → F; A → G encompasses the intermediate steps required to reach the target premise or the variable of interest.</p>
<p>We define a "hop" as the number of edges needed to progress from a specific premise to the target premise.Conversely, we define a "prefix" as the number of premises we provide to the model in advance (k).Using the template, we supply the LLM with k steps worth of premises and then prompt it to solve the remaining steps-l hops-needed to arrive at the final conclusion.By varying k and l, we can systematically evaluate the model's performance across different levels of partial information and reasoning depth.</p>
<p>REASONING EVALUATION ON SYNTHETIC DATASETS</p>
<p>Why Use Synthetic Dataset?Synthetic datasets offer a controlled framework for evaluating deductive accuracy, as all data points are generated according to predefined rules with precisely derived ground truth.This setup enables meticulous regulation of the underlying computation graph, allowing us to specify the total number of reasoning steps (i.e., edges in the graph) needed to arrive at the final answer.</p>
<p>Furthermore, it becomes straightforward to craft questions that traverse designated nodes in a prescribed order.For example, a path can be orchestrated to move from the initial premise to an intermediate node A ("prefix k") and then from node A to the target node B ("hop l"), such that Distance(seed premise, B) = k+l.This level of control over the graph's structure proves highly valuable for generating datasets with specific properties and systematically assessing deductive performance.</p>
<p>Dataset Generation In constructing of SynDeduct dataset, we begin by sampling a set of DAGs according to parameters that define constants, variable distributions, and arithmetic operators.We then extract paths from each DAG as programmatically computed reasoning traces.The resulting ground-truth derivations are converted into Chain of Thought representations by applying a set of verbalization templates, yielding readable textual explanations.Unlike in GSM8K, our approach does not require code generation or templated Chains of Thought, as the underlying computation graph is already available.</p>
<p>We quantify a path's difficulty by counting the number of reasoning steps (graph edges) it takes to move from the initial (base) node to the final (target) node.Accordingly, we generate N sets of questions, where the n-th set contains questions that require n steps.</p>
<p>To accommodate varying input-premise lengths, we create additional questions by progressively appending segments of the ground-truth reasoning chain to converge on the same target premise.We then place these questions into bins based on how many hops are needed, intermixing different prefix lengths within each bin.This organization yields n bins, each focused on questions requiring n hops but differing in the prefixed portion of the chain.Such binning enables robust averaging of model performance for varying prefix lengths within the same number of steps.Details are present in Appendix A.7.</p>
<p>RESULTS: MATH REASONING ON GSM8K</p>
<p>5.1 EVALUATION SETUP Dataset Statistics.A subset of 1000 problems from GSM8K is randomly chosen.The responses of LMs under evaluation are filtered as described in section 4. Problem instances common across the models are collected and used as final dataset that will be used to evaluate these models.This consists of 165 problem instances.</p>
<p>LMs in DeduCE pipeline.We use LLama-3-70B-Instruct LM as the templatizer, code generation and variable extractor.We find that LMs such as LLama-3-70B-Instruct are reasonably capable at templatization, obtaining a failure rate (unable to generate json) close to 30%, which we filter out.The additional sanity checks ensures that we have high quality dataset for evaluation.Models under evaluation.We evaluate the following LMs: Phi-3.5-mini-instruct,Phi-4, Qwen2.5-Math-7B-Instruct,Qwen2.5-Math-72B-Instruct,Llama-3.3-70B-Instruct,Llama-3-8B-Instruct.All models are Instruct tuned.Model suffixes will be truncated in plots.</p>
<p>HYPOTHESES</p>
<p>Based on the deductive consistency metric, we can formulate some hypotheses on why final accuracy decreases for novel math problems, as reported in past work (Mirzadeh et al., 2024;Srivastava et al., 2024).We also test the hypothesis on the effects of language style on reasoning (Han et al., 2024).H0: Novel Problems induce error in chain of thought reasoning.H1: Novel problems induce early errors in the math computation, which propagate to lead to an incorrect solution.H2: LMs have a significant decay in reasoning ability as the number of premises or hops increase.H3: Novel problems induce a style change in the CoT answers, which may lead to faulty reasoning and hence incorrect final answer.</p>
<p>To decide between hypotheses, we create four kinds of premises that can be added.The first is the original (eg: Yasna has 60 + 12 = 72 pages to read.), sourced from subject LMs answers on the original benchmark problem.Others are different paraphrases of the original style.They are explained in Appendix A.2</p>
<p>DEDUCTIVE REASONING DECAYS WITH NUMBER OF HOPS</p>
<p>Coverage Table 7 shows that the coverage is high across all premises.Given a LM, this implies that the intermediate variables inferred in the solution for the novel problem are almost the same as the variables inferred in the solution for the original benchmark problem.Therefore the code obtained from reasoning code generator is reliable as a reference proof.So we can go ahead with interpreting the consistency results.H0: We find that the deductive consistency as a function of hops on the original benchmark achieves a constant value of 1 across all models.When deductive consistency is computed on perturbed problem a, we find it to be significantly lower (see Figure 2).This indicates Memorization Effect of the benchmark, validating H0.H1: Mean deductive consistency is computed by averaging predicate consistency across prefixes for a given hop.We include only hops where the ratio of single-premise samples for the given hop to those with premise-length of 1 hop-1 exceeds 20%, ensuring sufficient data for reliable estimates.Our findings reveal that consistency remains high for the first hop, contradicting hypothesis by demonstrating that models correctly answer the first step.While novel problems do not induce early errors, we observe frequent computational errors in model responses.These errors propagate, providing evidence for part of hypothesis.Detailed error analysis, is in Appendix A.4.</p>
<p>H2:</p>
<p>The second key result is decay in deductive consistency as hops increase, which was masked due to memorization of the original benchmark.We characterize this decay as negative of the slope of the best fit line in Figure 2 with hops normalized between 0 and 1 and base refers to the deductive consistency of the reasoning model with 1 Hop.An ideal model must achieve zero decay and a base value of one.</p>
<p>We find that larger models (Qwen-Math-72B-Instruct, and Llama-3.3-70B-Instruct),models trained on synthetic data (Phi-4) as well as math-specific models (Qwen-2.5-Math-72B-Instruct,and Qwen-2.5-Math-72B-Instruct)do achieve greater base values.However, even these models show significant decay in the deductive consistency as the number of hops increases.Smaller models like Llama-3-8B-Instruct and Phi-3.5-mini-instructperform poorly with lower base values and Llama-3-8B-Instruct exhibits a high decay value compared to other models (also see Figure 2).Importantly, deductive consistency does not vary much as the length of input premises are changed.We observe lower variance in mean deductive consistency as a function of prefix as seen in Figure 7 .So we have partial evidence of hypothesis: it depends on hops, but not on the premises.A caveat is that due to the simplicity of the GSM8K problems, the maximum premise length we could evaluate on is 7.</p>
<p>H3:</p>
<p>We observe slight decrease in base values across models due to impact of language style.While it is expected that the original benchmark's style should have highest accuracy, the variation across paraphrases is not high.Even though, on performing t-test p-values values were significant (at 0.05 significance level), Cohens' effects sizes were too small to consider (&lt;0.1).We find weak evidence for H3 referring to Figure 4 in Appendix.</p>
<p>RESULTS: EVALUATION ON A SYNTHETIC DATASET</p>
<p>To validate the conclusions from GSM8K, we now evaluate deductive consistency on a synthetic dataset.In particular, the problems are designed such that the solutions involve a large number of hops spread across prefixes.All models are Instruct tuned.The general trend of decreasing deductive consistency over hops supports our results in GSM8K.Our findings in Figure 3 suggests larger models demonstrate greater resilience to increases in the number of hops, while smaller models-such as Llama-3-8B-Instruct-experience a substantial drop in performance.One contributing factor may be the larger maximum token limit, which allows these models to accommodate more extensive reasoning chains.However, for queries with increasingly longer chains, the input size expands significantly, risking the approach of token length limits and thereby degrading performance.This trend becomes evident when examining accuracy versus prefix length across multiple hops: as prefix segments grow, the overall task accuracy declines.We observe that larger instruction-tuned and SFT-tuned models exhibit lower decay with respect to prefix length.While Qwen-2.5-Math-72Bshows improved deductive consistency at shorter prefixes, these gains diminish rapidly with increasing prefix length-eventually falling below the performance of its base model.(AppendixTable 20, Table 21)</p>
<p>RESULTS ON SYNDEDUCT
Qwen-2.5-Math-7B Qwen-2.5-7B Qwen-2.5-Math-72B Qwen-2.5-72B Llama-3-8B DeepSeek-R1-Llama-70B Llama-3.3-70B DeepSeek-R1-Qwen-7B</p>
<p>ABLATIONS</p>
<p>DISTILLATION AND RL TUNED MODELS</p>
<p>To better understand the impact of different fine-tuning strategies on deductive reasoning, we focus on two primary fine-tuning approaches: iterative fine-tuning with reinforcement learning (RL-training) and supervised fine-tuning (SFT).</p>
<p>For RL-based and iterative fine-tuning models, we analyze Qwen-2.5-Math-Instruct in both its 7B and 72B variants (Yang et al., 2024), comparing them against their respective base models.Similarly, for SFT-based tuning, we compare R1 distilled models to their base counterparts (DeepSeek-AI et al., 2025), namely DeepSeek-R1-Distill-Llama-70B and DeepSeek-R1-Distill-Qwen-7B.</p>
<p>RL fine tuning is more effective (Table 1, Table 3) in reducing the decay of deductive consistency.RL fine-tuning shows minimal change in base deductive consistency on in-distribution datasets, as a drastic reduction on unseen datasets such as SynDeduct.SFT after training causes a decrease in deductive consistency and worsens decay for both data sets (Table 2, Table 4).More work is required to study the extent of generalization that such post-training methods provide.In general, these findings emphasize that neither of the two post-training techniques is successful in improving deductive consistency between models and datasets.</p>
<p>Model</p>
<p>Base Decay</p>
<p>Qwen-2.5-72B-Instruct 0.6868 0.0602 Qwen-2.5-Math-72B-Instruct0.5674 0.0273 Qwen-2.5-7B-Instruct0.5458 0.0432 Qwen-2.5-Math-7B-Instruct0.2083 0.0211</p>
<p>Model</p>
<p>Base Decay</p>
<p>Qwen-2.5-72B-Instruct 0.9149 0.2339 Qwen-2.5-Math-72B-Instruct0.9164 0.1725 Qwen-2.5-7B-Instruct0.8881 0.1618 Qwen-2.5-Math-7B-Instruct0.8427 0.1189</p>
<p>ERROR ANALYSIS</p>
<p>Evaluation method.Model responses are grouped into five groups based on final accuracy on the mutated dataset.Accuracy here is computed over the set of mutated problems for each problem in the original benchmark.The groups are; Group-1 : Accuracy = 1 ; Group-2 : 1 &lt; Accuracy ≤ 0.7 ; Group-3 : 0.7 &lt; Accuracy ≤ 0.4 ; Group-4 : 0.4 &lt; Accuracy &lt; 0 ; Group-5 : Accuracy = 0 Error Categories.We use GPT-4o as an evaluator.Calculation errors like arithmetic mistakes, as well as errors in rounding, along with error propagation Logic errors are wrong application of logic/rule/formula.Understanding errors are wrong assumption or contradiction of a given fact.These errors are seen in cases where the problem mentions scenarios that are far from real world such as there being 97 days in a week.In Table 5 we report the frequency of error normalized by number of error responses in that group.Observation and Findings.A higher proportion of calculation errors is observed relative to logical and comprehension errors.These calculation errors predominantly emerge during arithmetic operations within the chain-of-thought, and they propagate through subsequent reasoning steps.Furthermore, models exhibit (pre-training) bias.They reproduce the original reasoning graph from the vanilla solution.This shows weak robustness from changes in reasoning structure.Logical errors stem from ambiguous natural language.For instance, the sentence "My brother is twice more older than me" should ideally be represented as: myBrotherAge = myAge + myAge * 2. However, models typically interpret it as: myBrotherAge = 2 * myAge which correctly corresponds to the unambiguous phrasing "My brother is twice as old as me.".Refer to Appendix A.4 for details.</p>
<p>A APPENDIX A.1 DETAILS FOR GSM8K PIPELINE</p>
<p>Inference on original dataset We sample a subset of GSM8K of size 1000.We prompt the LM under investigation to solve the question using the prompt template provided in subsection A.6.</p>
<p>Templatization and Code Generation</p>
<p>We templatize the question and LM CoT response using Llama-3-70B as Template Builder Agent.The model is prompted (as shown in subsection A.6) to generate templatized question, templatized CoT answer (as well as chunk it into steps), explanation of variables of templates along with assignment of variables in question.</p>
<p>Sanity Checks</p>
<p>We check that the code produced is an executable code, if the format of template generate is consistent with our reference template format, if the all variables in factual assignment are present in code.The generated code is executed with factual assignment as inputs for variables in question template and the value of other variables in code are checked to be consistent with the factual assignment in template.Further we have check if the final answer in response matches the ground truth answer in original dataset.If any of these checks fail then we remove that question from pipeline.For each model we now have a reduces set of questions that has passed sanity checks.We take intersection of such questions over multiple models to get a dataset on which we can evaluate all the models under consideration.This support set depends on the set of models being used in the experiments.</p>
<p>Mutation DetailsWe create mutated dataset by sampling the values of variables in question and executing the code with these newly sampled values to obtain assignment corresponding to other variables.Parameters for the sampler are (min-value,max-value,max-iter).If the factual assignment of a variable is integer, we sample from integers in the range (min-value,max-value), if factual assignment of a variable is decimal between 0 and 1, we uniformly sample from this range, else if it is any other decimal we sample a float from (min-valu,max-value). We try to make sure that all the variable assignments after positive.If not we rerun until we get a all positive assignment or we reach maximum iterations of the sampler.We substitute these values into template question and template CoT answer.We sample 10 mutated questions per question in original dataset.We create dataset with mutated question and varying length of mutated CoT answer present in LMs context.The number of steps from mutated template CoT answer is defined as prefix length.We collect the sampled variable assignments,mutated Question and Prefix into the mutated dataset.</p>
<p>Inference on mutated datasetWe run inference of LMs on this mutated dataset.Since all LMs we evaluate are Instruction tuned, we use chat template.Mutated question is passed as user-content where as prefix is passed as assistant-content.We remove the &lt;∥eot∥ &gt;token and let the generation continue as if the model were completing the generation.</p>
<p>Computation of Deductive consistency</p>
<p>The response of the model to mutated dataset is passed into a variable extraction LM which extracts value if the variables under consideration (ones in template) if present in response(see subsection A.6). Hops are decided by the relative positions of variables under consideration in the template CoT.We check if these extracted values are consistent with code-generated values.We collect this data for every variable across mutated question and then group it by prefix and hop.We filter instances where the responses gives From this data structure we derive Deductive Consistency as function of hops or prefixes (as required).</p>
<p>A.2 PARA PHRASING STYLES</p>
<p>• Vanilla (Para-van): In this approach, the text is rephrased using conventional linguistic variations without incorporating any specialized semantic constraints.For example: Yasna's task involves 60 pages plus an additional 12 pages, which totals to 72 pages</p>
<p>• Axiomatic (Para-ax): This method reformulates the statement as a set of axioms that articulate the underlying numerical relationships.An example is : Axiom-1 (Addition): Given two numerical values x and y, the operation ADD(x, y) yields their arithmetic sum, thus ADD(60, 12) yields 72, which represents the total number of pages Yasna has to read.</p>
<p>• Reverse (Para-rev): the sentence is restructured by inverting the typical cause-effect relationship-presenting the effect before providing the explanation for its cause.For instance, in the example: Yasna has to read the 72 pages, which is the sum of 60 and 12.</p>
<p>A.3 RESULTS -GSM8K</p>
<p>Here we report the full results presented in the main paper.Qwen-2.5-7B0.8881 ± 0.0176 0.8453 ± 0.0246 0.8101 ± 0.0222 0.7738 ± 0.0281 0.7620 ± 0.0352 Qwen-2.5-Math-7B0.8427 ± 0.0156 0.8021 ± 0.0131 0.7739 ± 0.0142 0.7499 ± 0.0251 0.7499 ± 0.0306   In this section, we perform a qualitative and quantitative analysis of errors that arise in the response models.Errors:</p>
<p>• Wrong Assumption: The response assumes there are enough people to turn into vampires.Justification: The village only has 33 people initially, so the vampires cannot turn more people into vampires than the village's population allows.The response fails to consider this limitation.</p>
<p>• Wrong Logic: The response calculates the number of new vampires incorrectly on the second night.Justification: On the second night, the vampires cannot turn more than the remaining population into vampires.Given the initial population, they cannot create 18,240 new vampires.[This is logic error !!]</p>
<p>Understanding Error</p>
<p>Question</p>
<p>James writes a 36-page letter to 71 different friends 32 times a week.How many pages does he write in a year?</p>
<p>Answer (Phi-4)</p>
<p>Step-by-step solution:</p>
<ol>
<li>Calculate the total number of pages written per week: -James writes a 36-page letter to each of his 71 friends.-Total pages per week:</li>
</ol>
<p>Total pages per week = 36 × 71 = 2, 556 pages 2. Calculate the total number of pages written in a year: -There are 52 weeks in a year.</p>
<p>Total pages per year = 2, 556 × 52 = 132, 912 pages Final Answer: 132, 912 pages Evaluation (GPT-4o)</p>
<p>Errors:</p>
<p>• Wrong Assumption: The response assumes James writes a 36-page letter to each friend only once per week.Justification: The problem states he writes 32 times per week, so the calculation of total pages per week is incorrect.</p>
<p>• Calculation Error: The response calculates the total number of pages per year using an incorrect assumption.Justification: Since the number of times he writes per week is not factored in, the yearly calculation does not reflect the correct total.</p>
<p>Calculation Error</p>
<p>Question</p>
<p>Max likes to collect model trains.He asks for 53 for every birthday of his, and asks for 18 each Christmas.Max always gets the gifts he asks for, and asks for these same gifts every year for 16 years.At the end of the 16 years, his parents give him double the number of trains he already has.How many trains does Max have now?</p>
<p>Answer (Llama-3-8B)</p>
<p>Step-by-step solution:</p>
<ol>
<li>Max gets 53 trains for each birthday, and he has 16 birthdays:    What is its average speed?","templatized_answer": [ "The train travels a distance of {distance} kilometers in {time} hours.","Average speed is calculated as distance divided by time.","Average speed = {distance} / {time} = {average_speed} kilometers per hour."], "factual_assignment": { "distance": 60, "time": 2, "average_speed": 30 }, "node_explanation":{ "distance": "The distance traveled by the train", "time": "time taken by the train to travel the distance", "average_speed": "The average speed of the train" } } { "add": { "function": "lambda x, y: x + y", "verbalization": "{child} is the sum of {parent1} and {parent2}."}, "subtract": { "function": "lambda x, y: x -y", "verbalization": "{child} is the difference between {parent1} and {parent2}."} }
53 × 16 =</li>
</ol>
<p>Data Generation Steps</p>
<p>Initially, Directed Acyclic Graphs are generated.A computation graph is then constructed by selecting a fixed-length path within each DAG and randomly assigning values and operators to its nodes.A rule set, in conjunction with a predetermined collection of nouns, is employed to generate verbalization.Additionally, Chain-of-Thought solutions along with final answer are produced and later used to create prefixes.It is important to note that the questions generated in this process consist exclusively of "n" hops and do not include any prefix.</p>
<p>python3 init.py--num_graphs 99000 --m 60 --unary_ratio 0.0 --logic_mode bodmas --naming_mode noun --nouns.json--operators_file ruleset.json--output_file output.json--max_hops 24 --max_graphs 4000</p>
<p>Steps kept and undersampled to 4000:  A single Data-point of SynDeduct</p>
<p>Prompt Part A: Graph Structure and Question (will be given as user) The graph structure encompasses the complete verbalization of the entire graph, whereas the question is derived solely from a specific path within that graph.Consequently, a considerable amount of the information contained in the graph structure is not necessary for generating a solution.This design serves to assess the model's capability to extract and utilize only the relevant information from a broader context.</p>
<p>=== Graph Structure === Inputs:</p>
<p>-Masako (value = 8) -Nalca (value = 2) -Gassman (value = 5) Derived Nodes:</p>
<p>-Certain is the sum of Nalca and Masako.</p>
<p>-Irtysh is the sum of Certain and Gassman.</p>
<p>-Horstman is the difference between Masako and Certain.</p>
<p>-Pellicano is the difference between Horstman and Gassman.</p>
<p>-Taoiseach is the difference between Masako and Gassman.</p>
<p>-Vanvalkenburg is the difference between Gassman and Certain.</p>
<p>-Nourse is the sum of Irtysh and Nalca.</p>
<p>-Clapham is the sum of Pellicano and Taoiseach.</p>
<p>-Nuncio is the difference between Nalca and Horstman.</p>
<p>-Foxbat is the difference between Nalca and Gassman.</p>
<p>-Kenyon is the sum of Nuncio and Masako.</p>
<p>-Riva is the sum of Kenyon and Nourse.</p>
<p>-Claymore is the difference between Irtysh and Riva.</p>
<p>-Ballville is the sum of Masako and Riva.</p>
<p>-Lai is the difference between Kenyon and Clapham.</p>
<p>-Smolik is the sum of Vanvalkenburg and Riva.</p>
<p>-Bushi is the sum of Horstman and Claymore.</p>
<p>-Batiste is the sum of Riva and Kenyon.</p>
<p>-Criner is the sum of Riva and Certain.</p>
<p>-Begnaud is the difference between Nourse and Foxbat.</p>
<p>-SEPA is the sum of Certain and Irtysh.</p>
<p>-Wentling is the sum of Nalca and Smolik.</p>
<p>-Troon is the sum of Lai and Begnaud.</p>
<p>-Sanderson is the sum of Wentling and Begnaud.</p>
<p>-Ferozepore is the difference between Horstman and Sanderson.</p>
<p>-Sibiu is the sum of Ballville and Riva.</p>
<p>-Bootle is the sum of Irtysh and Nalca.</p>
<p>-Climategate is the sum of Vanvalkenburg and Taoiseach.</p>
<p>-Maland is the difference between Certain and Vanvalkenburg.</p>
<p>-Hobby is the difference between Sanderson and Kenyon.</p>
<p>-Tikrit is the difference between Nourse and Bootle.</p>
<p>-Lamarca is the sum of Maland and Criner.</p>
<p>-Dnipr is the sum of Irtysh and Nourse.</p>
<p>-Arvid is the difference between SEPA and Horstman.</p>
<p>-Plath is the sum of SEPA and Criner.</p>
<p>-Gulliver is the difference between Kenyon and Sibiu.</p>
<p>-Helatrobus is the difference between Plath and Sanderson.</p>
<p>-Tulu is the sum of Nalca and Kenyon.</p>
<p>-Shuka is the sum of Nourse and Vanvalkenburg.</p>
<p>-Hemsley is the difference between Bootle and Pellicano.</p>
<p>-Creasman is the sum of Nourse and Troon.</p>
<p>-Falcon is the sum of Clapham and Irtysh.</p>
<p>-Border is the difference between Gassman and Tikrit.</p>
<p>-Noyola is the difference between Lamarca Hobby.</p>
<p>-Tommie is the sum of Taoiseach and Helatrobus.</p>
<p>-Hines is the sum of Masako and Batiste.</p>
<p>-Adney is the difference between Wentling and Bushi.</p>
<p>-Winsford is the sum of Ballville and Shuka.</p>
<p>-Iga is the sum of Plath and Riva.</p>
<p>-Jacqueline is the sum of Winsford and Vanvalkenburg.</p>
<p>-Wheatley is the sum of Border and Irtysh.</p>
<p>-Lyndon is the sum of Noyola and Shuka.</p>
<p>-Arvelo is the difference between Pellicano and SEPA.</p>
<p>-Belphegor is the difference between Helatrobus and Wheatley.</p>
<p>-Kassandra is the difference between Bootle and Tulu.</p>
<p>-Garth is the difference between Wentling and Plath.</p>
<p>-Yucatec is the sum of Pellicano and Hines.</p>
<p>What is the value of Arvelo?</p>
<p>Prompt PartB: Partial COT (prefix) This assistant prompt-response pair will be provided to the language model with the eos token removed from the end.This ensures that the model continues generating text seamlessly from where the given prefix ends, thereby guiding its output to align with the intended structure and constraints.</p>
<p>Template System Prompt</p>
<p>You are a computation graph reasoning assistant designed to evaluate mathematical expressions described in any style of verbalizations.Your task is to process graph structure, interpret the relationships between nodes based on the provided verbalizations, and answer questions about specific nodes.</p>
<p>Here are the rules and expectations for your behavior: Subsequently, the output generated by the language model is processed using a Variable Extractor analogous to that employed in the GSM8K dataset.The parsed response is then normalized-massaged into the correct format (for instance, converting fractional representations to floating-point numbers)-and subsequently compared to the final expected answer, allowing for a tolerance of up to 5 per-cent deviation from the original value.</p>
<p>Figure 1 :
1
Figure1: Pipeline for Deductive Consistency Evaluation.Our method requires access to a single valid solution to compute deductive consistency across multiple premises and reasoning hops.Given a benchmark problem, we use pre-trained LMs to templatize its solution and obtain an executable code solution.Then we update the variables' values in the code and generate a novel problem on which the subject LM is evaluated.For any (number of premises, number of hops) combination, we assess whether the subject LM's solution contains the correct values of the variables.</p>
<p>Figure 2 :
2
Figure 2: Left: Deductive Consistency vs. Reasoning Hops across models.Right: Base deductive consistency vs. Decay.The premises are set using the Original paraphrase setting.</p>
<p>Figure 3 :
3
Figure 3: Accuracy v/s Hops.Each Hop bin has roughly same variation of Prefixes.Y-Ticks on each data-point is Standard Error for Accuracy.For Coverage refer to Appendix Figure 11.</p>
<p>Figure 4 :
4
Figure 4: Comparison of Base vs. Decay Across Different Styles.Model name suffixes have been dropped for readability.</p>
<p>Figure 8 :
8
Figure 5: Coverage Comparison Across Interventions (Adjusted Y-Limits)</p>
<p>Figure 9 :
9
Figure 9: Comparison of Decay across Models for Ablations 1-4</p>
<p>Figure</p>
<p>Figure 11: Coverage Metric in SynDeDeduct for Different Models</p>
<p>Answer: -Given value of Masako = 8 .-Given value of Nalca = 2 .Certain is the sum of Nalca and Masako.The Computed value of Certain = 10 Response From LLM -Given value of Gassman = 5 .Irtysh is the sum of Certain and Gassman.The Computed value of Irtysh = 15 Horstman is the difference between Masako and Certain.The Computed value of Horstman = -2 Pellicano is the difference between Horstman and Gassman.The Computed value of Pellicano = -7 Taoiseach is the difference between Masako and Gassman.The Computed value of Taoiseach = 3 Vanvalkenburg is the difference between Gassman and Certain.The Computed value of Vanvalkenburg = -5 Nourse is the sum of Irtysh and Nalca.The Computed value of Nourse = 15 + 10 = 15 The Computed value of Nourse = 100 + 10 = 100 The Computed value of Nourse = 100 + 10 = 110 + 15 = 115 The Computed value of Nourse = 15 + 10 = 110 + 15 = 115 The Computed value of Nourse = 100 + 15 = 115 The Computed value of Nourse = 15 The Computed value of Nourse = 15 The Computed value of Nourse = 15 + 10 = 25 The Compu ted value of Nourse = 100 The Computed value of Nourse = 100 + 15 = 115 The Computed value of Nourse = 15 + 15 = 30 The Computed value of Nourse = 100 T he Computed value of Nourse = 100 The value of Septembr = 100 The value of Rondeau = 10 The value of Rondeau = 10 The value of Rondeau = 10 The value o f Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalc a = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Nalca = 10 The value of Malca = 10 The value of Malca = 10 The value of Malca = 10 The value of Maland = 10 The value of Maland = 10 The value of Mand = 100 The value of Mand = 100 The value of Nalca = 100 The value of Nalca = 100 The value of Nalca = 10 The va lue of Salca = 10 The value of Valca = 10 The value of Calca = 10 The value of Galca = 10 The value of Palca = 10 The value of Balca = 10 The value of Talca = 10 The value of Falca = 10 SEPA is the sum of Certain and Irtysh.The Computed value of SEPA = 10 + 15 = 25 Pellicano is the difference between Horstman and Gassman.The Computed value of Pellicano = -2 -5 = -7 Arvelo is the difference between Pellicano and SEPA.The Computed value of Arvelo = -7 -25 = -32 \n#### -3</p>
<p>---### Rules: {RuleSet.json is filled here} ** Graph Structure Processing ** : -Nodes are defined as inputs or derived nodes.-Inputs have predefined values.-Derived nodes depend on other nodes and their relationships as defined by verbalizations.<strong> Step-by-Step Reasoning ** : -Interpret the graph structure line-by-line.-Calculate the value of each derived node based on its dependencies, ensuring that the verbalization is correctly mapped to its mathematical function.-Use previously calculated or input values as required.</strong> Answer Presentation ** :-Provide the value of the requested node only after completing all necessary computations and make sure the value is a integer or a float.-SHOW THE REASONING STEP-BY-STEP AND PROVIDE THE FINAL ANSWER CLEARLY, PREFIXED BY '####' and NOTHING AFTER IT. -Suppose answer is 56.You must output '#### 56' at the end of each step-byis an input with value 10.-Septembr is the square of Rondeau..The value of Septembr = 100 ####100</p>
<p>Table 1 :
1
Ablation for RL post training on SynDeduct.
ModelBaseDecayQwen-2.5-Math-7B-Instruct0.2083 0.0211DeepSeek-R1-Distill-Qwen-7B0.5424 0.0381Llama-3.3-70B-Instruct0.8465 0.0212DeepSeek-R1-Distill-Llama-70B 0.7389 0.0314</p>
<p>Table 2 :
2
Ablation for SFT post training on SynDeduct.</p>
<p>Table 3 :
3
Ablation for RL post training on GSM8k.
ModelBaseDecayQwen-2.5-Math-7B-Instruct0.8509 0.0613DeepSeek-R1-Distill-Qwen-7B0.8468 0.1957Llama-3.3-70B-Instruct0.8532 0.20065DeepSeek-R1-Distill-Llama-70B 0.8366 0.24895</p>
<p>Table 4 :
4
Ablation for SFT post training on GSM8k.</p>
<p>Table 6 :
6
Model Performance and Deductive Consistency Across Hops GSM8K
ModelsHop-1Hop-2Hop-3Hop-4Hop-5Llama-3.3-70B0.89 ± 0.01350.8274 ± 0.0219 0.7909 ± 0.0215 0.7669 ± 0.0206 0.7079 ± 0.0114Llama-3-8B0.7629 ± 0.0217 0.6572 ± 0.0227 0.5777 ± 0.0165 0.5254 ± 0.0154 0.4988 ± 0.0264Phi-40.8911 ± 0.0238 0.8365 ± 0.0278 0.8103 ± 0.0285 0.7929 ± 0.0228 0.7612 ± 0.0117Phi-3.50.8563 ± 0.0114 0.7874 ± 0.0210 0.7602 ± 0.0096 0.6865 ± 0.0571 0.6616 ± 0.0488Qwen-2.5-Math-72B 0.937 ± 0.0108 0.9037 ± 0.0085 0.8841 ± 0.0073 0.8573 ± 0.0148 0.8321 ± 0.0328Qwen-2.5-Math-7B0.8843 ± 0.0179 0.854 ± 0.0144 0.8456 ± 0.0307 0.8283 ± 0.0440.8409 ± 0.039</p>
<p>Table 7 :
7
Coverage across different language styles for the premises.
ModelsOriginal Para-ax Para-van Para-revLlama-3.3-70B0.97250.96390.96160.966Llama-3-8B0.96690.94310.95430.9486Phi-40.98490.96940.97590.9747Phi-3.50.96840.95230.96490.9624Qwen-2.5-Math-72B 0.98880.97450.98620.985Qwen-2.5-Math-7B0.97010.94420.96560.9648</p>
<p>Table 8 :
8
Para-ax: Model Performance and Deductive Consistency Across Hops
ModelsHop-1Hop-2Hop-3Hop-4Hop-5Llama-3.3-70B0.8875 ± 0.0158 0.8083 ± 0.0271 0.7653 ± 0.0282 0.752 ± 0.01890.7328 ± 0.004Llama-3-8B0.7309 ± 0.0244 0.6177 ± 0.034 0.5096 ± 0.0305 0.4799 ± 0.0328 0.4798 ± 0.0173Phi-40.8703 ± 0.0178 0.8245 ± 0.0177 0.8055 ± 0.0172 0.7525 ± 0.0275 0.7263 ± 0.0164Phi-3.50.8146 ± 0.0270.694 ± 0.0279 0.6106 ± 0.0426 0.5795 ± 0.0658 0.5715 ± 0.0424Qwen-2.5-Math-72B 0.9196 ± 0.0143 0.851 ± 0.0168 0.8149 ± 0.0194 0.8038 ± 0.0154 0.8019 ± 0.026Qwen-2.5-Math-7B0.8627 ± 0.0192 0.8001 ± 0.0233 0.7672 ± 0.0216 0.7418 ± 0.0165 0.7483 ± 0.0148Table 9: Para-van: Model Performance and Deductive Consistency Across HopsModelsHop-1Hop-2Hop-3Hop-4Hop-5Llama-3.3-70B0.863 ± 0.02760.79 ± 0.03180.7561 ± 0.0277 0.7242 ± 0.0323 0.7203 ± 0.0103Llama-3-8B0.7538 ± 0.0212 0.612 ± 0.0352 0.5503 ± 0.0207 0.4997 ± 0.0348 0.4961 ± 0.0275Phi-40.8505 ± 0.0248 0.8062 ± 0.0271 0.7856 ± 0.0273 0.7655 ± 0.0356 0.7408 ± 0.0153Phi-3.50.8397 ± 0.0189 0.739 ± 0.0203 0.7178 ± 0.0132 0.6679 ± 0.0366 0.6279 ± 0.0355Qwen-2.5-Math-72B 0.9175 ± 0.0144 0.8758 ± 0.0143 0.8569 ± 0.0138 0.8427 ± 0.0182 0.8063 ± 0.0284Qwen-2.5-Math-7B0.8671 ± 0.0207 0.8206 ± 0.0192 0.7841 ± 0.0271 0.7854 ± 0.0246 0.8267 ± 0.0299</p>
<p>Table 10 :
10
Para-rev: Model Performance and Deductive Consistency Across Hops
ModelsHop-1Hop-2Hop-3Hop-4Hop-5Llama-3.3-70B0.8481 ± 0.0222 0.7828 ± 0.0306 0.7508 ± 0.0313 0.7148 ± 0.0334 0.7211 ± 0.0097Llama-3-8B0.7163 ± 0.0288 0.5509 ± 0.0504 0.4612 ± 0.0591 0.4609 ± 0.0371 0.4617 ± 0.025Phi-40.8532 ± 0.0282 0.8102 ± 0.0289 0.7874 ± 0.0326 0.7724 ± 0.025 0.7442 ± 0.0122Phi-3.50.7958 ± 0.0241 0.7308 ± 0.024 0.6917 ± 0.0165 0.6351 ± 0.0483 0.6355 ± 0.0431Qwen-2.5-Math-72B 0.9051 ± 0.017 0.8636 ± 0.0166 0.8279 ± 0.0172 0.8105 ± 0.0176 0.7979 ± 0.0258Qwen-2.5-Math-7B0.8463 ± 0.0219 0.7914 ± 0.0227 0.7298 ± 0.0364 0.7543 ± 0.0124 0.7952 ± 0.0201</p>
<p>Table 11 :
11
Decay and Base Values Across Interventions
ModelsOriginalPara-axPara-vanPara-revDecayBaseDecayBaseDecayBaseDecayBaseLlama-3.3-70B0.212350.890.18285 0.8875 0.17560.8630.1610.8481Llama-3-8B0.330.76290.320.7309 0.31385 0.7538 0.2996 0.7163Phi-40.1517 0.89110.180.8703 0.13005 0.8505 0.1279 0.8532Phi-3.50.24515 0.8563 0.30035 0.8146 0.24735 0.8397 0.20815 0.7958Qwen-2.5-Math-72B 0.12810.9370.1413 0.9196 0.12775 0.9175 0.13375 0.9051Qwen-2.5-Math-7B0.05625 0.8843 0.14355 0.86270.0580.8671 0.06965 0.8463</p>
<p>Table 12 :
12
Deductive Consistency ± Standard Error vs. Prefix Length for Different Models
ModelPrefix 1Prefix 2Prefix 3Prefix 4Prefix 5Phi-3.50.7664 ± 0.0304 0.7772 ± 0.0255 0.7767 ± 0.0224 0.8153 ± 0.0247 0.7707 ± 0.0215Qwen-2.5-Math-7B 0.8549 ± 0.0268 0.8366 ± 0.0195 0.8510 ± 0.0243 0.8641 ± 0.0329 0.8922 ± 0.0383Qwen-2.5-Math0.8802 ± 0.0200 0.8704 ± 0.0223 0.8803 ± 0.0166 0.9067 ± 0.0117 0.8967 ± 0.0161Llama-3-8B0.5884 ± 0.0580 0.5826 ± 0.0523 0.5932 ± 0.0466 0.5984 ± 0.0306 0.6637 ± 0.0330Llama-3.3-70B0.7981 ± 0.0342 0.7929 ± 0.0322 0.7937 ± 0.0369 0.7952 ± 0.0344 0.7905 ± 0.0322Phi-40.8254 ± 0.0303 0.8277 ± 0.0274 0.8309 ± 0.0311 0.8199 ± 0.0440 0.7996 ± 0.0444</p>
<p>Table 14 :
14
Ablation 2: Deductive Consistency vs. Hops
ModelHop 1Hop 2Hop 3Hop 4Hop 5</p>
<p>Table 15 :
15
Ablation 3: Deductive Consistency vs. Hops Distill-Qwen-7B 0.8468 ± 0.0271 0.7989 ± 0.0391 0.7451 ± 0.0414 0.7309 ± 0.0566 0.6851 ± 0.0531
Hops12345Qwen-2.5-Math-7B0.8509 ± 0.018 0.8093 ± 0.0168 0.8002 ± 0.0198 0.7957 ± 0.0382 0.7964 ± 0.0458DeepSeek-R1-</p>
<p>Table 16 :
16
Ablation 4: Deductive Consistency vs. Hops Distill-Llama-70B 0.8366 ± 0.0238 0.7726 ± 0.0333 0.7093 ± 0.0389 0.6741 ± 0.0385 0.6369 ± 0.0307
Hops12345Llama-3.3-70B0.8532 ± 0.0134 0.7876 ± 0.0158 0.7515 ± 0.0127 0.7075 ± 0.0107 0.6926 ± 0.0076DeepSeek-R1-</p>
<p>Table 17 :
17
The response states that at the end of the 16 years, Max's parents give him double the number of trains he already has, calculated as 1, 136 × 2 = 2, 268.Justification: Number of instances for different models across dataset groups.
848 trains</p>
<p>Table 21 :
21
DC ± Standard Error for Prefix 7-12
ModelPrefix 7Prefix 8PrefixPrefix 10Prefix 11Prefix 12Qwen-2.5-Math-7B0.0383 ± 0.0157 0.0407 ± 0.0106 0.0191 ± 0.0080 0.0151 ± 0.0061 0.0176 ± 0.0100 0.0124 ± 0.0061Qwen-2.5-7B0.1651 ± 0.0386 0.1544 ± 0.0383 0.1918 ± 0.0329 0.1376 ± 0.0362 0.1277 ± 0.0238 0.1217 ± 0.0227Qwen-2.5-Math-72B0.2979 ± 0.0410 0.3003 ± 0.0274 0.2063 ± 0.0336 0.1619 ± 0.0301 0.1676 ± 0.0401 0.2034 ± 0.0359Qwen-2.5-72B0.2892 ± 0.0583 0.2505 ± 0.0558 0.2376 ± 0.0490 0.2286 ± 0.0419 0.2007 ± 0.0436 0.2138 ± 0.0478Llama-3-8B0.0775 ± 0.0240 0.0903 ± 0.0147 0.0821 ± 0.0163 0.0362 ± 0.0085 0.0599 ± 0.0091 0.0452 ± 0.0108DeepSeek-R1-Llama-70B 0.6311 ± 0.0498 0.6519 ± 0.0377 0.6133 ± 0.0316 0.6845 ± 0.0149 0.6875 ± 0.0352 0.6426 ± 0.0473Llama-3.3-70B0.6164 ± 0.0176 0.5876 ± 0.0226 0.6538 ± 0.0195 0.5721 ± 0.0317 0.5625 ± 0.0206 0.5844 ± 0.0122DeepSeek-R1-Qwen-7B0.1470 ± 0.0350 0.1923 ± 0.0339 0.1927 ± 0.0377 0.1070 ± 0.0292 0.1302 ± 0.0344 0.1321 ± 0.0300
CONCLUSIONWe propose a metric for evaluating AI deductive reasoning using only text-based reasoning traces, enabling broad domain applicability.Our experiments reveal contrasting fine-tuning effects on synthetic (SynDeduct) and real-world (GSM8K) benchmarks, highlighting key trade-offs.IMPACT STATEMENTThis paper presents work whose goal is to advance the reliability of AI reasoning.We expect that insights from our work can be used to improve reliability of AI reasoning, leading to a positive impact on downstream applications by avoiding reasoning bugs.L la m a -3 .3-7 0 B L la m a -3 -8 B P h i-4 P h i-3 .5Q w e n -2 .5 -M a t h -7 2 B Q w e n -2 .5 -M a t h -7 B Error RateGroup 4Error Type Logical Understanding CalculationInstructions:You are an expert in comprehension and variable extraction.Your task is to analyze a question, a stepby-step solution, and a dictionary of variables and return a JSON object that adheres to the following rules: -Guidelines:1. Inputs:• Question: The problem description.•Step-by-step solution:The solution text, where variables may be explicitly stated or calculated.• Dictionary of variables: Contains variable names and their descriptions.Not all variables may appear in the question or solution.2. Output Format:• Return a JSON object with:-Keys: Variable names from the dictionary.-Values: Numeric values extracted from the solution or question.-If a value is explicitly mentioned in the step-by-step solution, extract it without recalculating.-If the variable is not present in the solution or question, return "None".-Values must preserve their original format (e.g., fractions, decimals, or expressions).3. Output Structure:• Enclose the output JSON object within <JSON> and </JSON> tags.• All numeric values must be string representations (e.g., "3/2", "25.5", or "12+8").Restrictions:• Do not solve the problem yourself or calculate missing values.• Extract only the values as they appear in the solution.Example 1:Question: At a flea market, Hillary sells handmade crafts for 15 dollars per craft.Today, Hillary sells 6 crafts and is given an extra 5 dollars from an appreciative customer.Later on, Hillary deposits 12 dollars from today's profits into her bank account.How many dollars is Hillary left with after making the deposit?Step-by-step solution: Hillary earns (15 \times 6 = 90) dollars from selling crafts.Adding the extra 5 dollars, she has 90 + 5 = 95 dollars.After depositing 12 dollars, she has 95 -12 = 83 dollars left. Dictionary of variables: { "price_per_craft": "The price of each craft", "number_of_crafts": "The number of crafts sold", "extra_dollars": "The extra amount given by the customer", "deposit_amount": "The amount deposited into the bank account", "total_earnings": "The total amount earned from selling crafts", "total_amount": "The total amount after receiving the extra dollars", "amount_left": "The amount left after depositing" } Output: <JSON> { "price_per_craft": "15", "number_of_crafts": "6", "extra_dollars": "5",
Learn from failure: Fine-tuning LLMs with trial-and-error data for intuitionistic propositional logic proving. Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational Linguistics12024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Ziyang Song2025</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang, 2024</p>
<p>. Irène Gijbels. Censored data. WIREs Computational Statistics. 222010</p>
<p>Beyond surface structure: A causal assessment of llms' comprehension ability. Yujin Han, Lei Xu, Sirui Chen, Difan Zou, Chaochao Lu, 2024</p>
<p>Evaluating the robustness of analogical reasoning in large language models. Martha Lewis, Melanie Mitchell, arXiv:2411.142152024aarXiv preprint</p>
<p>Using counterfactual tasks to evaluate the generality of analogical reasoning in large language models. Martha Lewis, Melanie Mitchell, arXiv:2402.089552024barXiv preprint</p>
<p>Gsmsymbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, 2024</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational Linguistics12024</p>
<p>Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, arXiv:2404.155222024barXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Testing the general deductive reasoning capacity of large language models using OOD examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, 10.48550/arXiv.2305.15269Proceedings of the 2024 Conference of the North American Chapter. the 2024 Conference of the North American ChapterHuman Language Technologies20241Seals and Valerie Shalin. Evaluating the deductive competence of large language models</p>
<p>Exposing the achilles' heel: Evaluating llms ability to handle mistakes in mathematical reasoning. Joykirat Singh, Akshay Nambi, Vibhav Vineet, 2024</p>
<p>Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. Saurabh Srivastava, P V Anto, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, arXiv:2402.194502024arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Jason Zhou, Wei, arXiv:2210.092612022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2024ISBN 9781713871088</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation and beyond. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria, 2024</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, arXiv:2409.12122Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv preprint</p>
<p>Mr-gsm8k: A meta-reasoning benchmark for large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, 2024</p>
<p>Darg: Dynamic evaluation of large language models via adaptive reasoning graph. Zhehao Zhang, Jiaao Chen, Diyi Yang, 2024</p>
<p>Dyval: Dynamic evaluation of large language models for reasoning tasks. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Categorical syllogisms revisited: A review of the logical reasoning abilities of LLMs for analyzing categorical syllogisms. Shi Zong, Jimmy Lin, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). Lotem Peled-Cohen, Nitay Calderon, Shir Lissak, Roi Reichart, the 1st Workshop on NLP for Science (NLP4Science)2024deposit_amount": "12", "total_earnings": "90", "total_amount": "95", "amount_left": "83"</p>
<p>John takes away 7 pink hard hats and twice as many green hard hats as the number of pink hard hats he removed. Calculate the total number of hard hats that remained in the truck. Question: In a truck, there are 5 pink hard hats, 16 green hard hats, and 15 yellow hard hats. Carl takes away 10 pink hard hats. 2The total number of hats remaining. Output: <JSON> { "pink": "5", "green": "16", "yellow": "15", "carl_pink": "10", "john_pink": "7", "total_initial": "36", "total_after_carl": "26", "total_after_john_pink": "19", "john_green": "14", "total_final": "5"</p>
<p>This configuration implies that each such question originated from a 24-hop question, wherein the first 12 hops, serving as the prefix of the chain-of-thought, are provided, and the language model is required to resolve the remaining 12 hops. python transformer.py --max_hops 12 --max_items 120 --max_prefixes 12 --max_prefix_length 10 output.json Prefix Length Distribution Per Hop Category (After Undersampling): Hop 1. Prefix11: 10120To create a balanced dataset, the maximum number of hops is limited to 12, half the total hops, and the total number of items is capped at 120. For instance, in the case of Hop12, there are 10 questions featuring a 12-hop prefix. Prefix12: 10 Total Prefix Length Distribution Across Hops: Prefix1: 120 Prefix2: 120 Prefix3: 120 Prefix4: 120 Prefix5: 120 Prefix6: 120 Prefix7: 120 Prefix8: 120 Prefix9: 120 Prefix10: 120 Prefix11</p>            </div>
        </div>

    </div>
</body>
</html>
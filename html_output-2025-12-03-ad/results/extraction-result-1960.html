<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1960 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1960</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1960</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280421906</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.01184v1.pdf" target="_blank">Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> A core problem of Embodied AI is to learn object manipulation from observation, as humans do. To achieve this, it is important to localize 3D object affordance areas through observation such as images (3D affordance grounding) and understand their functionalities (affordance classification). Previous attempts usually tackle these two tasks separately, leading to inconsistent predictions due to lacking proper modeling of their dependency. In addition, these methods typically only ground the incomplete affordance areas depicted in images, failing to predict the full potential affordance areas, and operate at a fixed scale, resulting in difficulty in coping with affordances significantly varying in scale with respect to the whole object. To address these issues, we propose a novel approach that learns an affordance-aware 3D representation and employs a stage-wise inference strategy leveraging the dependency between grounding and classification tasks. Specifically, we first develop a cross-modal 3D representation through efficient fusion and multi-scale geometric feature propagation, enabling inference of full potential affordance areas at a suitable regional scale. Moreover, we adopt a simple two-stage prediction mechanism, effectively coupling grounding and classification for better affordance understanding. Experiments demonstrate the effectiveness of our method, showing improved performance in both affordance grounding and classification.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1960.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1960.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ours</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-scale Cross-modal Affordance Representation (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>End-to-end cross-modal framework that grounds image-depicted affordances onto 3D point clouds via ROI-based 2D affordance features, multi-scale 3D region features, multi-head cross-attention fusion, GCN-based geometric propagation, and a cascaded grounding→classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-scale Cross-modal Affordance Representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extracts context-aware 2D affordance features via a pretrained Swin image backbone + ROI-align for object/subject/scene, applies a scene gating network to selectively integrate context, extracts multi-scale 3D region features with PointNet++ downsampling, computes region similarity graphs, fuses 2D and 3D modalities with multi-head cross-attention (image→3D and 3D→image variants), refines region features with a two-layer GCN using similarity graphs (geometric feature propagation), performs soft multi-scale selection via a learned gate, decodes a point-wise probabilistic affordance mask from upsampled regional features, and finally combines global (pooled) and mask-weighted local features to predict the affordance class in a cascaded grounding→classification design.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Swin transformer (pretrained Swin model used in experiments; also compared using ResNet in an ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained Swin model (paper states pretrained weights are loaded; dataset(s) for pretraining not explicitly specified in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-modal multi-head attention: (1) use 2D context-aware affordance feature as key/value and 3D region features as queries (and vice versa) for mutual enhancement; (2) predict region-wise probabilistic mask via an MLP on upsampled region features; (3) use predicted mask to weight local region features and combine with global 2D features for classification. Geometric propagation (GCN on region similarity graphs) extrapolates image-observed affordance to full object regions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: 2D global/context-aware features (ROI-level), 3D region-level at two scales (multi-scale regions), and final upsampled per-point probabilistic mask (point-level after upsampling).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D point cloud coordinates and derived region partitions; ROI bounding boxes for 2D subject/object; similarity graphs among 3D regions (normalized adjacency); multi-scale region indices used for up/down-sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation affordance grounding and affordance classification (affordance understanding for manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIAD image↔point-cloud affordance grounding benchmark (seen / unseen splits on PIAD)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Third-person images paired with 3D point clouds (exocentric interaction images depicting a person interacting with an object) — synthetic/collected real object scans as point clouds (PIAD dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Affordance grounding: AUC, aIOU, SIM, MAE; Affordance classification: Accuracy (ACC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On PIAD (Seen split) reported best overall: AUC 87.20%, aIOU 22.75%, SIM 0.604, MAE 0.081, ACC 90.91%; (Unseen split) reported: AUC 74.40%, aIOU 8.50%, SIM 0.363, MAE 0.117, ACC 45.14% (values as reported in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation entries in Table 2: 'Blind' (all key contributions removed / baseline) shows AUC 82.92%, aIOU 17.45% (Seen). Removing MSI: AUC drops to 85.96% (Seen); removing GFPM: AUC 85.83% (Seen); removing CGC: AUC 86.49% (Seen); removing SG: AUC 86.30% (Seen). This indicates measurable drops when modules tied to grounding are ablated.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to baseline/blind variants the full model gains ≈+4.28 AUC points vs blind on Seen (87.20% vs 82.92%), and shows largest single-module contributions from MSI and GFPM (each producing ~1–2 AUC points and larger aIOU/SIM gains).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper compares using the Swin backbone and also reports results using ResNet (Table 5): even when using ResNet (to match IAG's original backbone), the proposed model still outperforms IAG on both grounding and classification; exact numbers reported in Table 5 (e.g., Ours with ResNet: AUC 85.76%, aIOU 21.33%, SIM 0.578, MAE 0.086, ACC ~86.36%).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify two perception-grounding challenges: (1) image observations often show only a partial affordance region, so naive grounding anchors only to visible parts and fails to recover full potential affordance area; (2) multi-scale spatial extent of affordances (some are small regions like bag-lifting, others large like 'contain')—fixed-scale methods struggle. They also note noisy/low-quality point clouds (e.g., keyboard example) can cause over-extension of predicted affordance areas after geometric propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative + some quantitative ablations: (i) Without GFPM (geometric feature propagation) grounding predictions fail to cover full potential affordance areas (visualized; w/o GFPM reduces aIOU and SIM); (ii) without MSI (multi-scale information) precision on fine interaction regions degrades; (iii) without CGC (coupling grounding and classification) the model confuses affordance areas with incorrect functionalities (affordance-class mismatch). The paper documents the frequency of drops in metrics via ablation (Table 2) but does not provide per-failure-type percentages beyond metric deltas; they also show an example where GFPM overextends the 'press' affordance beyond the keyboard due to poor point cloud quality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Evaluated cross-dataset generalization qualitatively: model applied to ShapeNet point clouds (category not present in PIAD, e.g., sofa) with favorable grounding visualizations; no explicit domain-adaptation layers used — generalization is demonstrated empirically but no systematic domain-adaptation experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Paper reports Seen vs Unseen splits on PIAD: large performance drop in Unseen (example: Ours Unseen ACC 45.14% vs Seen ACC 90.91%), indicating substantially degraded affordance classification and grounding when object classes are unseen in training.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Not directly compared: paper uses a pretrained Swin backbone with loaded pretrained weights (no explicit frozen vs fine-tuned ablation reported). PointNet++ encoder used untrained (random init) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not studied — paper does not analyze effects of pretraining dataset scale on grounding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention (multi-head) in two directions: image→3D (image features as key/value to enhance 3D region queries) and 3D→image (refining global image-aware affordance features), plus gating (scene gate) and soft-weighted multi-scale gating for selection; final fusion concatenates pooled global and mask-weighted local features for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No few-shot / demonstration-count study; training setup: trained end-to-end for 150 epochs with batch size 16 on PIAD; no direct comparison of sample efficiency with/without grounding reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Effective affordance grounding for embodied manipulation requires (1) coupling grounding and classification (cascaded grounding→classification reduces mismatches between local region and functionality), (2) multi-scale 3D region representations to handle widely varying spatial extents of affordances, (3) geometric propagation across similar regions (GCN on region-similarity graphs) to extrapolate full potential affordance areas from partial image observations, and (4) selective scene gating to avoid context distractions. Ablations show MSI and GFPM are the most critical components for improved grounding coverage and precision.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1960.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1960.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline cross-attention fusion baseline (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simplified baseline used in experiments: extract context-aware image features, extract PointNet++ point cloud features, fuse via multi-head attention, then feed fused representation into two MLP heads for grounding and classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baseline cross-attention fusion</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image ROI features (same extractor as paper) + PointNet++ point-level features concatenated via cross-attention and passed to separate MLP heads for grounding mask prediction and affordance classification. No multi-scale region propagation, no scene gating, and no cascaded grounding→classification coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Swin transformer (same image backbone as main model in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained Swin model (same as main model; exact pretraining dataset unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Direct cross-attention / concatenation fusion of 2D and 3D features followed by MLP heads — no geometric propagation or multi-scale selection.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point-level (PointNet++ features) fused with ROI-level 2D features.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D point coordinates (via PointNet++), 2D ROIs for image regions; no region similarity graphs or GCN propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D affordance grounding and classification</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIAD benchmark (used as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Third-person images paired with 3D point clouds (PIAD)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC, aIOU, SIM, MAE, ACC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Table 1 as a 'Blind' / baseline: Seen AUC 82.92%, aIOU 17.45% (other metrics reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Baseline tends to ground only the visible/incomplete regions and lacks mechanisms to extrapolate full potential affordance areas; demonstrated by worse aIOU/SIM compared to full model in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention / concatenation followed by MLP heads (simple fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Simpler direct fusion baselines underperform vs approaches that add multi-scale regions, geometric propagation, and task coupling — baseline shows lower AUC / aIOU and visual incompleteness in grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1960.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1960.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IAG (referenced comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that aligns 2D images with 3D point clouds via a 2D–3D alignment network and performs affordance grounding and classification, but (according to this paper) treats the two tasks independently leading to inconsistent predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IAG (2D-3D alignment network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a 2D–3D alignment network to align image features with point cloud features and then performs grounding and classification (reported in the original IAG work); in this paper IAG is re-run with the same image and point-cloud backbones as the proposed method for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Originally reported with ResNet (paper also reports IAG re-run using the shared Swin backbone for fairness)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in this paper for IAG (original IAG uses ResNet pretrained as per its own paper); in this work IAG is re-evaluated both with ResNet and with the Swin backbone for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>2D–3D alignment network (explicit alignment module) followed by separate heads for grounding and classification; no task-coupled cascaded inference as in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>2D↔3D aligned representations (image-level and point-cloud-level alignment), region/point-level grounding in 3D</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Image features + point cloud coordinates via IAG's alignment pipeline (details in IAG original work; this paper does not alter alignment design except for using same backbones for fair comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D affordance grounding and classification</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIAD benchmark (compared in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Third-person images paired with 3D point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC, aIOU, SIM, MAE, ACC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Table 1 (this paper's re-evaluation): Seen AUC ~85.96%, aIOU ~20.13%, SIM ~0.564, MAE ~0.092, ACC ~83.50% (values as presented in Table 1); with ResNet backbone (Table 5) IAG: AUC 84.84%, aIOU 20.23% (paper's re-evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>This paper reports that IAG underperforms on classification relative to the proposed cascaded coupling design, suggesting coupling grounding→classification improves consistency and classification accuracy (quantified by higher ACC for Ours vs IAG in Table 1/5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>This paper re-runs comparisons using ResNet (IAG's original) and Swin; Ours still outperforms IAG under matched backbones (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper argues IAG fails to effectively couple grounding and classification which can lead to inconsistent affordance-function predictions (qualitative failure mode discussed).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>IAG sometimes predicts multiple spatial regions corresponding to different affordances simultaneously (e.g., grasp vs stab on a knife) due to weak coupling between region and functionality, as shown in visual comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>2D–3D alignment network (explicit alignment) followed by independent heads (original IAG design); in this paper IAG was kept unchanged except backbone substitution for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Independent alignment + separate heads can produce reasonable grounding but may cause affordance–function inconsistencies; coupling grounding and classification in a cascaded manner improves classification consistency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1960.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1960.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Laso</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LASO (language-guided affordance segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language-guided 3D affordance segmentation approach originally using textual inputs to guide 3D affordance segmentation; in this paper LASO is adapted to use image input and compared for image-guided 3D affordance grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Laso: Language-guided affordance segmentation on 3d object</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LASO (adapted to image input for this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Originally a language-conditioned segmentation model for 3D affordance; in this work authors replace LASO's text backbone with an image backbone and retain LASO's cross-modal fusion design to serve as a comparison method for image-guided 3D affordance grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Adapted with same image backbone as Ours in experiments (pretrained Swin in main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>As used in this work: pretrained Swin (paper does not detail LASO's original pretraining in this re-evaluation beyond backbone substitution)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Language-conditioned (original) or image-conditioned (adapted here) cross-modal fusion; original LASO uses textual conditioning for affordance segmentation; adapted variant uses image features as guidance with LASO fusion mechanisms retained.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D region/point-level affordance segmentation conditioned on textual or image cues</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D point cloud coordinates; conditioning from external modality (text or image)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D affordance grounding/segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIAD benchmark (adapted to image-conditioned scenario in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Third-person images paired with 3D point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC, aIOU, SIM, MAE, ACC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Table 1 (adapted to image input): Seen AUC ~84.26%, aIOU ~19.83% (other metrics in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Original LASO is language-driven, so its adaptation to image input in this paper yields acceptable but inferior performance to the proposed multi-scale geometric propagation and coupling approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Original LASO's cross-modal fusion design (language-to-3D or adapted image-to-3D conditioning); retained in comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Language-conditioned segmentation methods can be adapted to image conditioning but may not capture multi-scale geometric propagation and grounding–classification coupling benefits demonstrated by the proposed model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1960.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1960.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XMF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XMF (cross-modal fusion for image→point-cloud tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-modal fusion model originally for image-guided point cloud completion that is adapted in this paper as a baseline for affordance grounding by replacing output heads with grounding/classification modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XMF (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original XMF fuses image and point cloud features to complete point clouds; in this paper XMF's cross-modal fusion is retained but its outputs are replaced with 3D affordance grounding and classification heads for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Image backbone substituted to match this paper's experiments (pretrained Swin for fairness)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained Swin used in comparisons (paper does not detail XMF's original pretraining here)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Image↔point cloud fusion via XMF's cross-modal learning design; adapted to produce region-wise grounding masks and class labels.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point/region-level via fused representations</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D point cloud coordinates; image features fused (no explicit region similarity GCN propagation as in Ours)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D affordance grounding (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>PIAD benchmark (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Third-person images paired with 3D point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC, aIOU, SIM, MAE, ACC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Table 1 (adapted): Seen AUC ~80.39%, aIOU ~14.42% (other metrics shown in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>As an adapted point-completion fusion method, XMF lacks the multi-scale region propagation and task coupling of the proposed approach and underperforms on affordance grounding metrics in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>XMF cross-modal fusion (adapted to this task)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Cross-modal fusion designed for other tasks (e.g., completion) can be repurposed for affordance grounding but performs worse than a targeted design with multi-scale regions, geometric propagation, and task coupling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounding 3d object affordance from 2d interactions in images <em>(Rating: 2)</em></li>
                <li>Laso: Language-guided affordance segmentation on 3d object <em>(Rating: 2)</em></li>
                <li>Crossmodal learning for image-guided point cloud shape completion <em>(Rating: 1)</em></li>
                <li>3D AffordanceNet: A benchmark for visual object affordance understanding <em>(Rating: 2)</em></li>
                <li>PartAfford: Part-level affordance discovery from 3d objects <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1960",
    "paper_id": "paper-280421906",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "Ours",
            "name_full": "Multi-scale Cross-modal Affordance Representation (this paper)",
            "brief_description": "End-to-end cross-modal framework that grounds image-depicted affordances onto 3D point clouds via ROI-based 2D affordance features, multi-scale 3D region features, multi-head cross-attention fusion, GCN-based geometric propagation, and a cascaded grounding→classification head.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-scale Cross-modal Affordance Representation",
            "model_description": "Extracts context-aware 2D affordance features via a pretrained Swin image backbone + ROI-align for object/subject/scene, applies a scene gating network to selectively integrate context, extracts multi-scale 3D region features with PointNet++ downsampling, computes region similarity graphs, fuses 2D and 3D modalities with multi-head cross-attention (image→3D and 3D→image variants), refines region features with a two-layer GCN using similarity graphs (geometric feature propagation), performs soft multi-scale selection via a learned gate, decodes a point-wise probabilistic affordance mask from upsampled regional features, and finally combines global (pooled) and mask-weighted local features to predict the affordance class in a cascaded grounding→classification design.",
            "visual_encoder_type": "Swin transformer (pretrained Swin model used in experiments; also compared using ResNet in an ablation)",
            "visual_encoder_pretraining": "Pretrained Swin model (paper states pretrained weights are loaded; dataset(s) for pretraining not explicitly specified in the paper)",
            "grounding_mechanism": "Cross-modal multi-head attention: (1) use 2D context-aware affordance feature as key/value and 3D region features as queries (and vice versa) for mutual enhancement; (2) predict region-wise probabilistic mask via an MLP on upsampled region features; (3) use predicted mask to weight local region features and combine with global 2D features for classification. Geometric propagation (GCN on region similarity graphs) extrapolates image-observed affordance to full object regions.",
            "representation_level": "Multi-level: 2D global/context-aware features (ROI-level), 3D region-level at two scales (multi-scale regions), and final upsampled per-point probabilistic mask (point-level after upsampling).",
            "spatial_representation": "Explicit 3D point cloud coordinates and derived region partitions; ROI bounding boxes for 2D subject/object; similarity graphs among 3D regions (normalized adjacency); multi-scale region indices used for up/down-sampling.",
            "embodied_task_type": "Object manipulation affordance grounding and affordance classification (affordance understanding for manipulation)",
            "embodied_task_name": "PIAD image↔point-cloud affordance grounding benchmark (seen / unseen splits on PIAD)",
            "visual_domain": "Third-person images paired with 3D point clouds (exocentric interaction images depicting a person interacting with an object) — synthetic/collected real object scans as point clouds (PIAD dataset).",
            "performance_metric": "Affordance grounding: AUC, aIOU, SIM, MAE; Affordance classification: Accuracy (ACC)",
            "performance_value": "On PIAD (Seen split) reported best overall: AUC 87.20%, aIOU 22.75%, SIM 0.604, MAE 0.081, ACC 90.91%; (Unseen split) reported: AUC 74.40%, aIOU 8.50%, SIM 0.363, MAE 0.117, ACC 45.14% (values as reported in Table 1 of the paper).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation entries in Table 2: 'Blind' (all key contributions removed / baseline) shows AUC 82.92%, aIOU 17.45% (Seen). Removing MSI: AUC drops to 85.96% (Seen); removing GFPM: AUC 85.83% (Seen); removing CGC: AUC 86.49% (Seen); removing SG: AUC 86.30% (Seen). This indicates measurable drops when modules tied to grounding are ablated.",
            "grounding_improvement": "Compared to baseline/blind variants the full model gains ≈+4.28 AUC points vs blind on Seen (87.20% vs 82.92%), and shows largest single-module contributions from MSI and GFPM (each producing ~1–2 AUC points and larger aIOU/SIM gains).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper compares using the Swin backbone and also reports results using ResNet (Table 5): even when using ResNet (to match IAG's original backbone), the proposed model still outperforms IAG on both grounding and classification; exact numbers reported in Table 5 (e.g., Ours with ResNet: AUC 85.76%, aIOU 21.33%, SIM 0.578, MAE 0.086, ACC ~86.36%).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify two perception-grounding challenges: (1) image observations often show only a partial affordance region, so naive grounding anchors only to visible parts and fails to recover full potential affordance area; (2) multi-scale spatial extent of affordances (some are small regions like bag-lifting, others large like 'contain')—fixed-scale methods struggle. They also note noisy/low-quality point clouds (e.g., keyboard example) can cause over-extension of predicted affordance areas after geometric propagation.",
            "failure_mode_analysis": "Qualitative + some quantitative ablations: (i) Without GFPM (geometric feature propagation) grounding predictions fail to cover full potential affordance areas (visualized; w/o GFPM reduces aIOU and SIM); (ii) without MSI (multi-scale information) precision on fine interaction regions degrades; (iii) without CGC (coupling grounding and classification) the model confuses affordance areas with incorrect functionalities (affordance-class mismatch). The paper documents the frequency of drops in metrics via ablation (Table 2) but does not provide per-failure-type percentages beyond metric deltas; they also show an example where GFPM overextends the 'press' affordance beyond the keyboard due to poor point cloud quality.",
            "domain_shift_handling": "Evaluated cross-dataset generalization qualitatively: model applied to ShapeNet point clouds (category not present in PIAD, e.g., sofa) with favorable grounding visualizations; no explicit domain-adaptation layers used — generalization is demonstrated empirically but no systematic domain-adaptation experiments reported.",
            "novel_object_performance": "Paper reports Seen vs Unseen splits on PIAD: large performance drop in Unseen (example: Ours Unseen ACC 45.14% vs Seen ACC 90.91%), indicating substantially degraded affordance classification and grounding when object classes are unseen in training.",
            "frozen_vs_finetuned": "Not directly compared: paper uses a pretrained Swin backbone with loaded pretrained weights (no explicit frozen vs fine-tuned ablation reported). PointNet++ encoder used untrained (random init) in experiments.",
            "pretraining_scale_effect": "Not studied — paper does not analyze effects of pretraining dataset scale on grounding quality.",
            "fusion_mechanism": "Cross-attention (multi-head) in two directions: image→3D (image features as key/value to enhance 3D region queries) and 3D→image (refining global image-aware affordance features), plus gating (scene gate) and soft-weighted multi-scale gating for selection; final fusion concatenates pooled global and mask-weighted local features for classification.",
            "sample_efficiency": "No few-shot / demonstration-count study; training setup: trained end-to-end for 150 epochs with batch size 16 on PIAD; no direct comparison of sample efficiency with/without grounding reported.",
            "key_findings_grounding": "Effective affordance grounding for embodied manipulation requires (1) coupling grounding and classification (cascaded grounding→classification reduces mismatches between local region and functionality), (2) multi-scale 3D region representations to handle widely varying spatial extents of affordances, (3) geometric propagation across similar regions (GCN on region-similarity graphs) to extrapolate full potential affordance areas from partial image observations, and (4) selective scene gating to avoid context distractions. Ablations show MSI and GFPM are the most critical components for improved grounding coverage and precision.",
            "uuid": "e1960.0"
        },
        {
            "name_short": "Baseline (this paper)",
            "name_full": "Baseline cross-attention fusion baseline (this paper)",
            "brief_description": "Simplified baseline used in experiments: extract context-aware image features, extract PointNet++ point cloud features, fuse via multi-head attention, then feed fused representation into two MLP heads for grounding and classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Baseline cross-attention fusion",
            "model_description": "Image ROI features (same extractor as paper) + PointNet++ point-level features concatenated via cross-attention and passed to separate MLP heads for grounding mask prediction and affordance classification. No multi-scale region propagation, no scene gating, and no cascaded grounding→classification coupling.",
            "visual_encoder_type": "Swin transformer (same image backbone as main model in experiments)",
            "visual_encoder_pretraining": "Pretrained Swin model (same as main model; exact pretraining dataset unspecified)",
            "grounding_mechanism": "Direct cross-attention / concatenation fusion of 2D and 3D features followed by MLP heads — no geometric propagation or multi-scale selection.",
            "representation_level": "Point-level (PointNet++ features) fused with ROI-level 2D features.",
            "spatial_representation": "3D point coordinates (via PointNet++), 2D ROIs for image regions; no region similarity graphs or GCN propagation.",
            "embodied_task_type": "3D affordance grounding and classification",
            "embodied_task_name": "PIAD benchmark (used as baseline in this paper)",
            "visual_domain": "Third-person images paired with 3D point clouds (PIAD)",
            "performance_metric": "AUC, aIOU, SIM, MAE, ACC",
            "performance_value": "Reported in Table 1 as a 'Blind' / baseline: Seen AUC 82.92%, aIOU 17.45% (other metrics reported in Table 1).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Baseline tends to ground only the visible/incomplete regions and lacks mechanisms to extrapolate full potential affordance areas; demonstrated by worse aIOU/SIM compared to full model in Table 1.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention / concatenation followed by MLP heads (simple fusion)",
            "sample_efficiency": null,
            "key_findings_grounding": "Simpler direct fusion baselines underperform vs approaches that add multi-scale regions, geometric propagation, and task coupling — baseline shows lower AUC / aIOU and visual incompleteness in grounding.",
            "uuid": "e1960.1"
        },
        {
            "name_short": "IAG",
            "name_full": "IAG (referenced comparator)",
            "brief_description": "Prior work that aligns 2D images with 3D point clouds via a 2D–3D alignment network and performs affordance grounding and classification, but (according to this paper) treats the two tasks independently leading to inconsistent predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "IAG (2D-3D alignment network)",
            "model_description": "Uses a 2D–3D alignment network to align image features with point cloud features and then performs grounding and classification (reported in the original IAG work); in this paper IAG is re-run with the same image and point-cloud backbones as the proposed method for fair comparison.",
            "visual_encoder_type": "Originally reported with ResNet (paper also reports IAG re-run using the shared Swin backbone for fairness)",
            "visual_encoder_pretraining": "Not specified in this paper for IAG (original IAG uses ResNet pretrained as per its own paper); in this work IAG is re-evaluated both with ResNet and with the Swin backbone for comparison.",
            "grounding_mechanism": "2D–3D alignment network (explicit alignment module) followed by separate heads for grounding and classification; no task-coupled cascaded inference as in this paper.",
            "representation_level": "2D↔3D aligned representations (image-level and point-cloud-level alignment), region/point-level grounding in 3D",
            "spatial_representation": "Image features + point cloud coordinates via IAG's alignment pipeline (details in IAG original work; this paper does not alter alignment design except for using same backbones for fair comparison)",
            "embodied_task_type": "3D affordance grounding and classification",
            "embodied_task_name": "PIAD benchmark (compared in this paper)",
            "visual_domain": "Third-person images paired with 3D point clouds",
            "performance_metric": "AUC, aIOU, SIM, MAE, ACC",
            "performance_value": "Reported in Table 1 (this paper's re-evaluation): Seen AUC ~85.96%, aIOU ~20.13%, SIM ~0.564, MAE ~0.092, ACC ~83.50% (values as presented in Table 1); with ResNet backbone (Table 5) IAG: AUC 84.84%, aIOU 20.23% (paper's re-evaluation).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "This paper reports that IAG underperforms on classification relative to the proposed cascaded coupling design, suggesting coupling grounding→classification improves consistency and classification accuracy (quantified by higher ACC for Ours vs IAG in Table 1/5).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "This paper re-runs comparisons using ResNet (IAG's original) and Swin; Ours still outperforms IAG under matched backbones (Table 5).",
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Paper argues IAG fails to effectively couple grounding and classification which can lead to inconsistent affordance-function predictions (qualitative failure mode discussed).",
            "failure_mode_analysis": "IAG sometimes predicts multiple spatial regions corresponding to different affordances simultaneously (e.g., grasp vs stab on a knife) due to weak coupling between region and functionality, as shown in visual comparisons.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "2D–3D alignment network (explicit alignment) followed by independent heads (original IAG design); in this paper IAG was kept unchanged except backbone substitution for fair comparison.",
            "sample_efficiency": null,
            "key_findings_grounding": "Independent alignment + separate heads can produce reasonable grounding but may cause affordance–function inconsistencies; coupling grounding and classification in a cascaded manner improves classification consistency.",
            "uuid": "e1960.2"
        },
        {
            "name_short": "Laso",
            "name_full": "LASO (language-guided affordance segmentation)",
            "brief_description": "Language-guided 3D affordance segmentation approach originally using textual inputs to guide 3D affordance segmentation; in this paper LASO is adapted to use image input and compared for image-guided 3D affordance grounding.",
            "citation_title": "Laso: Language-guided affordance segmentation on 3d object",
            "mention_or_use": "use",
            "model_name": "LASO (adapted to image input for this paper)",
            "model_description": "Originally a language-conditioned segmentation model for 3D affordance; in this work authors replace LASO's text backbone with an image backbone and retain LASO's cross-modal fusion design to serve as a comparison method for image-guided 3D affordance grounding.",
            "visual_encoder_type": "Adapted with same image backbone as Ours in experiments (pretrained Swin in main experiments)",
            "visual_encoder_pretraining": "As used in this work: pretrained Swin (paper does not detail LASO's original pretraining in this re-evaluation beyond backbone substitution)",
            "grounding_mechanism": "Language-conditioned (original) or image-conditioned (adapted here) cross-modal fusion; original LASO uses textual conditioning for affordance segmentation; adapted variant uses image features as guidance with LASO fusion mechanisms retained.",
            "representation_level": "3D region/point-level affordance segmentation conditioned on textual or image cues",
            "spatial_representation": "3D point cloud coordinates; conditioning from external modality (text or image)",
            "embodied_task_type": "3D affordance grounding/segmentation",
            "embodied_task_name": "PIAD benchmark (adapted to image-conditioned scenario in this paper)",
            "visual_domain": "Third-person images paired with 3D point clouds",
            "performance_metric": "AUC, aIOU, SIM, MAE, ACC",
            "performance_value": "Reported in Table 1 (adapted to image input): Seen AUC ~84.26%, aIOU ~19.83% (other metrics in Table 1).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Original LASO is language-driven, so its adaptation to image input in this paper yields acceptable but inferior performance to the proposed multi-scale geometric propagation and coupling approach.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Original LASO's cross-modal fusion design (language-to-3D or adapted image-to-3D conditioning); retained in comparisons here.",
            "sample_efficiency": null,
            "key_findings_grounding": "Language-conditioned segmentation methods can be adapted to image conditioning but may not capture multi-scale geometric propagation and grounding–classification coupling benefits demonstrated by the proposed model.",
            "uuid": "e1960.3"
        },
        {
            "name_short": "XMF",
            "name_full": "XMF (cross-modal fusion for image→point-cloud tasks)",
            "brief_description": "A cross-modal fusion model originally for image-guided point cloud completion that is adapted in this paper as a baseline for affordance grounding by replacing output heads with grounding/classification modules.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "XMF (adapted)",
            "model_description": "Original XMF fuses image and point cloud features to complete point clouds; in this paper XMF's cross-modal fusion is retained but its outputs are replaced with 3D affordance grounding and classification heads for comparison.",
            "visual_encoder_type": "Image backbone substituted to match this paper's experiments (pretrained Swin for fairness)",
            "visual_encoder_pretraining": "Pretrained Swin used in comparisons (paper does not detail XMF's original pretraining here)",
            "grounding_mechanism": "Image↔point cloud fusion via XMF's cross-modal learning design; adapted to produce region-wise grounding masks and class labels.",
            "representation_level": "Point/region-level via fused representations",
            "spatial_representation": "3D point cloud coordinates; image features fused (no explicit region similarity GCN propagation as in Ours)",
            "embodied_task_type": "3D affordance grounding (adapted)",
            "embodied_task_name": "PIAD benchmark (adapted)",
            "visual_domain": "Third-person images paired with 3D point clouds",
            "performance_metric": "AUC, aIOU, SIM, MAE, ACC",
            "performance_value": "Reported in Table 1 (adapted): Seen AUC ~80.39%, aIOU ~14.42% (other metrics shown in Table 1).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "As an adapted point-completion fusion method, XMF lacks the multi-scale region propagation and task coupling of the proposed approach and underperforms on affordance grounding metrics in this evaluation.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "XMF cross-modal fusion (adapted to this task)",
            "sample_efficiency": null,
            "key_findings_grounding": "Cross-modal fusion designed for other tasks (e.g., completion) can be repurposed for affordance grounding but performs worse than a targeted design with multi-scale regions, geometric propagation, and task coupling.",
            "uuid": "e1960.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounding 3d object affordance from 2d interactions in images",
            "rating": 2
        },
        {
            "paper_title": "Laso: Language-guided affordance segmentation on 3d object",
            "rating": 2
        },
        {
            "paper_title": "Crossmodal learning for image-guided point cloud shape completion",
            "rating": 1
        },
        {
            "paper_title": "3D AffordanceNet: A benchmark for visual object affordance understanding",
            "rating": 2
        },
        {
            "paper_title": "PartAfford: Part-level affordance discovery from 3d objects",
            "rating": 1
        }
    ],
    "cost": 0.02038125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning
2 Aug 2025</p>
<p>Xinhang Wan wanxinhang@nudt.edu 
National University of Defense Technology
ChangshaChina</p>
<p>Dongqiang Gou 
ShanghaiTech University
ShanghaiChina</p>
<p>Xinwang Liu xinwangliu@nudt.edu 
National University of Defense Technology
ChangshaChina</p>
<p>En Zhu enzhu@nudt.edu 
National University of Defense Technology
ChangshaChina</p>
<p>Xuming He 
ShanghaiTech University
ShanghaiChina</p>
<p>Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning
2 Aug 2025897DB32986D69E66AE4DD26327349C79arXiv:2508.01184v1[cs.CV]
A core problem of Embodied AI is to learn object manipulation from observation, as humans do.To achieve this, it is important to localize 3D object affordance areas through observation such as images (3D affordance grounding) and understand their functionalities (affordance classification).Previous attempts usually tackle these two tasks separately, leading to inconsistent predictions due to lacking proper modeling of their dependency.In addition, these methods typically only ground the incomplete affordance areas depicted in images, failing to predict the full potential affordance areas, and operate at a fixed scale, resulting in difficulty in coping with affordances significantly varying in scale w.r.t the whole object.To address these issues, we propose a novel approach that learns an affordance-aware 3D representation and employs a stage-wise inference strategy leveraging the dependency between grounding and classification tasks.Specifically, we first develop a cross-modal 3D representation through efficient fusion and multi-scale geometric feature propagation, enabling inference of full potential affordance areas at a suitable regional scale.Moreover, we adopt a simple two-stage prediction mechanism, effectively coupling grounding and classification for better affordance understanding.Experiments demonstrate the effectiveness of our method, showing improved performance in both affordance grounding and classification.</p>
<p>Introduction</p>
<p>As robotics and computer vision advance, the significance of embodied AI becomes increasingly evident [34,52,59].A core problem of embodied AI is learning object manipulation from observation, like humans do [17,45].A fundamental prerequisite for object manipulation is the ability to localize the interactive areas of 3D objects through observation (3D affordance grounding) and to understand their functionalities (affordance classification), as these aspects * Both authors contributed equally to this research.define where and how an agent can effectively interact with an object [9,38,56,58,60].In this paper, we focus on both grounding and classifying 3D object affordances from images, enabling robots to understand how to manipulate 3D objects under the guidance of images.</p>
<p>Early efforts often address those two problems [7,21] separately.For instance, [13,57] focus on affordance classification but primarily at the object level, lacking a finegrained understanding of affordances.In contrast, other approaches [51,55] align 2D images with 3D point clouds, using the aligned representation to guide grounding and locate affordance areas.Recently, several works have attempted to address the two tasks simultaneously.In particular, IAG [53] employs a 2D-3D alignment network to align images with point clouds, subsequently using this alignment representation to ground and classify 3D affordances independently.However, such a design is unable to fully capture the dependency between two tasks, often leading to inconsis-tent predictions (See Fig. 1.(a)).Moreover, for the grounding task, existing methods often suffer from two main limitations: 1) a tendency to ground the incomplete regions depicted from the image, failing to predict the full potential affordance area (See Fig. 1.(b)).2) generating grounding at a fixed scale, whereas different affordance regions can vary greatly in their scope relative to the object (See Fig. 1.(c)).</p>
<p>To address these challenges, we propose a novel crossmodal fusion framework for 3D affordance grounding and classification.Our method learns an effective affordanceaware 3D representation and employs a stage-wise inference strategy that leverages the dependency between grounding and classification tasks.Specifically, we develop a cross-modal affordance representation through efficient fusion and multi-scale geometric feature propagation.This approach facilitates the inference of the full scope of affordance areas, accommodating regions of varying sizes (See Fig. 1(b)&amp;(c)).Additionally, we adopt a simple cascaded scheme for the grounding and classification tasks, effectively coupling them and alleviating inconsistencies between their predictions.</p>
<p>In detail, given a single image depicting a person interacting with an object and the corresponding 3D point cloud, we first employ an image encoder to extract context-aware affordance features from the 2D image.Recognizing that scene context plays a varying role in affordance prediction, we design a novel gating network that selectively integrates scene information.Concurrently, we utilize a point cloud encoder to extract multi-scale features from the 3D point cloud.We then propose a simple yet efficient cross-modal fusion module, consisting of two multi-head cross-attention mechanisms, to integrate features from both modalities.To better cover the full potential affordance area beyond the incomplete affordance region reflected in the image (See Fig. 1.(b)), we exploit the geometric information of the point cloud to propagate regional features across multiple scales, ensuring that geometrically similar regions share similar representations.Subsequently, we leverage a multi-scale selector to fuse these multi-scale features.Finally, we predict the grounding probabilistic mask and then fuse the global and masked local features to predict the affordance class.</p>
<p>We conduct comprehensive experiments and both the statistical and visual results show that the effectiveness of our method, and validate the effectiveness of geometric feature propagation and multi-scale feature extraction.In summary, our contributions are as follows: 1.We propose a novel cross-modal multiscale affordance representation that can better handle large variations in affordance regions and improve the coverage of affordance grounding via efficient fusion and propagation.2. We employ a simple yet effective cascaded strategy for coupling the affordance grounding and classification tasks, which mitigates the inconsistencies between their predictions.3. We comprehensively evaluate both grounding and classification performance on the standard benchmark.Extensive numerical and visual results demonstrate the effectiveness of our method.</p>
<p>Related Work</p>
<p>Affordance leanring</p>
<p>Existing works on affordance learning focuses on humanmade objects, as these objects are designed to fulfill specific functional or interactive needs [11,26,31,42].Early research originated in the image domain, with the primary task being to recognize interactive objects in images and classify their affordance types [5,6,28].However, they are often limited to the object level, lacking fine-grained affordance understanding.</p>
<p>With the introduction of embodied AI, the fine-grained manipulation and labeling of 3D objects has become an area of growing interest [8,32,44].3D AffordanceNet [10] introduced the first fine-grained labeled dataset for 3D objects, where each point in the point cloud is annotated with interaction probabilities.Inspired by this dataset, a series of researches have been proposed.For instance, Jact et al. [27], combined natural language processing (NLP) with affordance learning, demonstrating that 3D information is more suitable for affordance learning than 2D data.Additionally, PartAfford [50] segments 3D point clouds into affordance-specific components.</p>
<p>However, existing approaches often overlook a deeper understanding of the specific operations associated with affordances, typically treating affordance as a mere label.For instance, PartAfford can segment a chair into components labeled as "sit", but it does not provide an understanding of the specific actions that correspond to the "sit" affordance.Humans learn affordances by observing interactions, whereas 3D point clouds alone fail to capture interaction dynamics.</p>
<p>Image-Point Cloud Cross-Modal Learning</p>
<p>Cross-modal learning between image and point cloud data has been widely applied in various computer vision tasks, including point cloud registration [18,20,40] and object recognition [4,24,37].Images provide rich semantic information such as object categories, textures, and colors, while point clouds capture precise 3D geometric structures of objects.Fusing these modalities enhances scene reconstruction [15] and improves object localization [2,46].Despite these advancements, leveraging semantic information from images to enhance the understanding of interactions in point clouds remains a critical challenge.Several attempts have been made to address this issue [16,35,48].Existing approaches typically rely on datasets like 3D Af- fordanceNet [10] to pair images with point clouds for identifying interactive regions of objects [12].For instance, Luo et al. [25] capture affordance knowledge from exocentric human-object interactions and utilize a cross-view knowledge transfer framework for grounding.Additionally, LEMON [54] exploits the correlation between interaction counterparts to jointly anticipate human contact, object affordance, and human-object spatial relations in 3D space.Although these studies have advanced geometric understanding and the detection of interactive regions, they primarily focus on identifying and localizing these areas, often neglecting a deeper functional understanding of their roles.</p>
<p>Method</p>
<p>Overview</p>
<p>Our task is to ground the affordance depicted in an image onto a point cloud representation of the same object category and predict the affordance class.Formally, the input to our method consists of a 3D point cloud P ∈ R N ×3 of the object, where N is the number of points, an RGB image I ∈ R 3×H×W with height H and width W which reflects an affordance category and a pair of bounding boxes B = {B sub , B obj } for the affordance-related subject and object entities in the image I, respectively.The scene context outside these bounding boxes is denoted by a scene mask M sce .Our goal is to predict the affordance category y and generate a point-wise probabilistic mask ϕ ∈ [0, 1] N , where the probability value at each point represents the likelihood of that point being associated with the affordance.</p>
<p>To address this task, we propose a multiscale crossmodal fusion framework that aims to generate better coverage for the affordance grounding and consistent prediction on the affordance category.To achieve this, our framework learns an effective affordance-aware 3D representation and a stage-wise inference strategy that exploits the dependency of the grounding and classification subtasks.Specifically, we design an end-to-end network architecture consisting of four main modules, as summarized in Fig.</p>
<p>Multi-modal Feature Module</p>
<p>2D Context-Aware Affordance Feature</p>
<p>Given the input image I and the bounding boxes B = {B sub , B obj }, we first extract an affordance feature encoding the functionality of the target object (indicated by B obj ) and its image context.To this end, we first use a vision backbone network to compute an image embedding, followed by ROI-align [14] to generate object entity feature map F ob , subject entity feature map F sb , and scene context feature map F sc .We then introduce an adaptive fusion method to integrate those features into a context-aware affordance representation.Specifically, we adopt a crossattention operator to fuse the object entity with the subject entity and the scene context as follows [53],
F e = MHA(W q F ob , W k F sb , W v F sb ), F s = MHA(W q F ob , W k F sc , W v F sc ),(1)
where F e and F s indicate fused entity and scene features, respectively, MHA(Q, K, V ) represents the standard attention operator with input query Q, key K and value V , and W q , W k and W v are weight matrices.We observe that the scene context plays a varying role in affordance prediction and may introduce distraction in certain cases.Therefore, we design a gate operator to selectively integrate scene context into the fused entity features, which generates the final affordance feature F I .Formally,
F I = LP (F e + Sigmoid (LN (F s )) ⊙ F s ) ,(2)
where Sigmoid (•) denotes the sigmoid activation function, LN (•) is a linear layer, and ⊙ represents element-wise multiplication.Here LP (•) is a linear layer with ReLU activation for the feature fusion.</p>
<p>Multi-Scale 3D Geometric Feature</p>
<p>Leveraging multi-scale features is essential for affordance learning.Consider the example of a bag, which exhibits distinct affordances such as lifting and containing.The spatial extent of the affordance associated with lifting is notably smaller compared to that of containing.In light of this, we utilize a point cloud backbone network to extract region features at two different scales.Specifically, we first form two types of regions by a 3D backbone network and downsampling operations, and then perform feature pooling in each region, which outputs F l P ∈ R C×N l p and F s P ∈ R C×N s p , where N l p and N s p are the number of regions at a large and a small scale, respectively.</p>
<p>Moreover, we construct a similarity graph among regions of each scale based on the point cloud features, and the resulting graph weight matrices, denoted as S l and S s , are computed as follows:
S l i,j = ⟨(F l P (i), F l P (j)⟩, S s i,j = ⟨(F s P (i), F s P (j)⟩,(3)
where ⟨•, •⟩ denotes the inner product, and
F l/s P (i) indicate the i-th column of F l/s P .</p>
<p>Cross-modal Fusion Module</p>
<p>One of the key challenges in affordance grounding is to effectively align the image-based affordance cue to the point cloud representation of target objects.To tackle this, we adopt a simple yet efficient fusion mechanism to integrate features from two modalities.Specifically, we use the multi-head cross-attention to capture various aspects of semantic and geometric information contained in the image and point cloud [33].Given the context-aware affordance feature F I , we first enhance the multi-scale geometric features of point cloud F l P and F s P using the affordance cues as follows:
Fl P = MHA(W l q F l P , W l k F I , W l v F I ), Fs P = MHA(W s q F s P , W s k F I , W s v F I ),(4)
where MHA is the multi-head attention operator, and W l/s q,k,v are the weight matrices for its query, key and value.</p>
<p>After obtaining the local region representations Fl P and Fs P , we further compute a global affordance representation at each scale by fusing the 3D features into F I through a similar cross-attention process, which are denoted as Fl</p>
<p>Propagation and Selection Module</p>
<p>The affordance regions within an image often represent only a fraction of the object's full potential affordance area.For instance, the "open" affordance associated with a microwave handle typically exhibits only limited interaction points within the image context, as users generally engage with a specific portion of it.To extrapolate the complete affordance regions within the point cloud, we leverage geometric similarities across various regions of the object to encourage similar regions sharing similar affordance representations, thereby facilitating more accurate grounding.Formally, we introduce two-layer GCN networks to refine the multiscale regional features Fl P and Fs P .Specifically, at each scale, we take the similarity graph S l (or S s ) and the corresponding features Fl P (or Fs P ) as input and iteratively update the features based on message propagation:
R (t+1) = Sigmoid ÃR (t) W (t) , t = 0, 1, 2, (5) where Ã = D − 1 2 AD − 1 2
is the normalized adjacency matrix and A is S l P or S s P , R (t) is the updated regional feature and W (t) is the learnable weight matrix at layer t.The initial regional feature R (0) = Fl/s P ⊙ Γ(F l/s P ), where Γ(•) is the pooling and expansion process for feature reweighting.We take the GCN outputs, denoted as Rl/s P , as the refined regional affordance features.Subsequently, we select a suitable scale in a soft-weighting manner via a gate fusion network as below
RP = αU Rl P + (1 − α)U Rs P ,(6)
where U(•) is the up-sample operation, α = α1 α1+α2 , α 1 and α 2 is a gate network score using Rl P and Rs P as inputs.Similarly, we use the same scores to compute the global affordance features as
FI = α Fl I + (1 − α) Fs I .(7)</p>
<p>Affordance Prediction Module</p>
<p>Finally, we use the multiscale affordance features to decode a probabilistic mask via a mask head and its category via a classifier head.As the affordance area is strongly correlated to its category, we adopt a two-stage strategy to generate the final prediction.Specifically, we first predict the grounding based on the local regional features as follows,
φ = f ϕ RP . (8)
where f ϕ is an MLP taking each local region's feature as input and predicts affordance confidence for that region.</p>
<p>Given the grounding output, we then combine the global context-aware affordance feature FI with the local affordance features to predict the final affordance class, which is formulated as:
ŷ = f y [P ( FI ), P ( φ • RP )] ,(9)
where P (•) denotes the average pooling operation and f y is an MLP classifier.</p>
<p>Loss Function</p>
<p>While our method employs a sequential prediction strategy, all four main modules in the proposed network are differentiable and hence allow us to train the full model in an end-to-end manner.Our loss function is a combination of grounding loss and classification loss:
L = L g + λ c L c ,(10)
where L g is the grounding loss that measures the discrepancy between the ground truth ϕ and the predicted grounding result φ, computed using a focal loss [41] combined with a Dice loss [29].The term L c denotes the crossentropy loss for classification and λ c is a hyperparameter that balances the two terms.Given that few studies simultaneously address both affordance grounding and classification, public datasets on this topic are limited.In our paper, we utilize the PIAD dataset collected in [53] for comparison.This dataset contains 7,012 point cloud instances and 5,162 images across 23 object categories and 17 affordance classes.Each point cloud in the image-point cloud pairs consists of 2,048 points.Notably, compared to existing affordance datasets, PIAD offers a similar breadth of affordance categories, object classes, and instances while standing out as the dataset with the largest number of images, as detailed in Table 3.We adopt two experimental settings: seen and unseen.In the seen setting, the training and test sets share similar class categories, while in the unseen setting, some object classes in the test set may not appear in the training set.We compare our method with XMF [1], Laso [19], and IAG [53].As a baseline, we extract features directly and fuse them using multi-head attention, then concatenate the multi-modal features to serve as input to two MLP networks for grounding and classification results.To ensure fairness, all compared methods use the same image backbone, Swin [22], and the point cloud backbone, PointNet++ [36].Details of these baselines are provided in the Appendix Sec A.1.Additionally, considering that IAG originally utilizes ResNet to extract image features, we also compare our method with it via ResNet in the Appendix Sec B.1.</p>
<p>Experiment</p>
<p>Evaluation metrics</p>
<p>Most methods for locating affordance areas utilize four metrics for comparison [10,30,57], specifically AUC, aIOU, SIM, and MAE.For affordance classification, we report the accuracy (ACC) results.Except for MAE, higher values for all other metrics indicate better performance.Additionally, detailed calculations of the evaluation metrics are provided in Appendix Sec.A.3.</p>
<p>Experimental results</p>
<p>The experimental results of our method and those of the competitors are reported in Table 1.The table demonstrates that our method outperforms all compared methods across all metrics.For instance, it surpasses the second-best algorithm by 1.44%, 13.02%, 7.09%, 11.96%, and 7.98% on the five metrics in the Seen setting, respectively.Significant improvements are also observed in the Unseen setting, which verifies the effectiveness of our proposed method.</p>
<p>Additionally, both IAG and our method integrate affordance classification and grounding simultaneously, which may explain their superior performance and inspire future research.However, IAG performs worse in affordance classification.We attribute this to its failure to effectively couple affordance classification with affordance grounding.Furthermore, while Laso originally utilized text information to guide grounding, we replace its text backbone with an image backbone to address the image-guided 3D affordance grounding problem, yielding acceptable results.</p>
<p>Ablation study</p>
<p>To verify the effectiveness of different modules in our model, we conduct an ablation study by removing one module at a time.The main contributions of our model can be summarized in four parts: multi-scale information (MSI), geometric feature propagation module (GFPM), coupling grounding and classification (CGC), and scene gate (SG).</p>
<p>For MSI, we extract region features at a fixed scale and remove the multi-scale fusion module to assess its effectiveness.We perform similar operations to evaluate the impact of GFPM and SG.For CGC, we couple 3D affordance grounding and affordance classification using a grounding-classification pipeline, and we examine the effects of removing this module by simultaneously outputting the results.The term 'Blind' refers to the results obtained by removing all key contributions.</p>
<p>The results are documented in Table 2. From the table, we observe that performance declines when any of the main contributions are removed from our model, particularly for MSI and GFPM.</p>
<p>Additionally, to further illustrate the effectiveness of these components, we present visualizations of ablation study in Section 4.4.2 and Fig. 4 below.These visual results provide an intuitive understanding of how each module impacts our tasks.</p>
<p>Visualization</p>
<p>In previous sections, we demonstrated the superiority of our model through statistical analysis.In this section, we provide visualization results to further illustrate the effectiveness of our method.Specifically, we compare our model with competing approaches and present visualization of ablation study to validate key components.Additionally, we analyze the effect of varying object instances for a given image and different affordance depictions for a given object, with detailed results provided in Appendix Sec B.4.2 and Sec B.4.3 for completeness.</p>
<p>Comparison results</p>
<p>We present the visualization results of our method and the competing approaches in Fig. 3.The figure clearly illustrates that our proposed method delivers superior per- formance in terms of visualization quality.Moreover, the experimental results highlight the importance of coupling affordance grounding with affordance classification, region propagation, and multi-scale information.1)In the Seen setting, a knife has three affordances: cut, grasp, and stab.When reasoning with images depicting grasping, our competitors mistakenly identify both the grasping and stabbing regions simultaneously, leading to confusion.This highlights their failure to effectively couple affordance areas with their corresponding functionalities.In contrast, our approach successfully couples these elements, addressing this issue.2)For the lay affordance of a bed, existing methods tend to predict incomplete areas.Our geometric feature propagation technique effectively addresses this issue, enabling the prediction of the complete affordance area.</p>
<p>3)Regarding the lifting affordance of a bag, which corresponds to regions occupying a relatively small part of the object, using larger-scale segmentation unavoidably affects surrounding areas.Our multi-scale selection method effec-tively overcomes this challenge, resulting in a more accurate prediction of the affordance area.Additional extensive comparison results are provided in Appendix Sec.B.4.1.</p>
<p>Visualization of Ablation Study</p>
<p>To further illustrate the effectiveness of GFPM, MSI and CGC in our method, we visualize ablation results in Fig. 4, showing that: 1) GFPM (Fig. 4(a)): Without GFPM, grounding predictions fail to cover the full potential affordance area, indicating the importance of geometric feature propagation; 2) MSI (Fig. 4(b)): Removing multi-scale information impairs the model's precision in grounding finegrained interaction regions; 3) CGC (Fig. 4(c)): Without CGC, the model confuses affordance areas and their functionalities, highlighting the necessity of coupling grounding with classification.</p>
<p>These visualizations validate the significant contribution of each component to accurate image-guided 3D affordance grounding and classification.</p>
<p>Cross-Dataset Generalization</p>
<p>To further validate the generalization capability of our model, we collected additional point clouds from other datasets and evaluated our approach on these samples.Fig. 5 visualizes our grounding results on point cloud samples from the ShapeNet dataset [3].As illustrated, our model achieves favorable grounding results, demonstrating strong generalization across datasets.Notably, the sofa category does not appear in PIAD, yet our model achieves strong grounding performance on such objects, further demonstrating its robustness and generalization across datasets.</p>
<p>Limitations</p>
<p>In our method, we leverage the inherent geometric similarity within point clouds to promote consistent grounding results for geometrically similar regions.However, this approach may occasionally result in predicted areas that extend beyond the true potential affordance areas.As shown in Fig. 6, for instance, in the case of the 'press' affordance of a computer keyboard, the predicted area may erroneously extend to regions outside the keyboard.While the geometric feature propagation technique has generally been effective in most experiments, this issue could be due to the poor quality of the point cloud data for the keyboard, making it difficult to accurately distinguish the keyboard from its surrounding areas.To address this, we can collect higherquality point cloud data for further verification.</p>
<p>Conclusion</p>
<p>In this work, we propose a novel approach that integrates context-aware affordance information from images to enhance functional region learning in 3D point clouds of the same object category.By combining localized functional regions with global context from 2D images, our method improves 3D affordance understanding.To address fixedscale learning limitations and incomplete affordance area predictions, we introduce multi-scale feature selection and geometric feature propagation.Our approach not only enhances performance but also paves the way for future advancements in affordance grounding across diverse contexts.</p>
<p>We selected two 3D Affordance Grounding works, including IAG [53] and Laso [19], for comparison.IAG and Laso utilize 2D image and textual information respectively to guide 3D affordance grounding.The key difference is that IAG performs both 3D Affordance Grounding and Affordance Classification, while Laso only performs 3D Affordance Grounding.Additionally, we chose a related work, XMF [1], for comparison.Furthermore, we designed a baseline model without any innovative modules.All comparison methods share the same backbone network as our method.We retained the cross-modal fusion strategies of these methods but adapted their output heads to perform both 3D Affordance Grounding and Affordance Classification tasks.The specific designs are as follows:</p>
<p>• Baseline: In the baseline design, we use the same method as our framework to extract context-aware affordance information from the image.For point cloud feature extraction, we adopt the standard PointNet++ method.Then, the features of the image and point cloud are fused using a cross-attention mechanism, and the fused representation is used as input for two output modules: 3D Affordance Grounding and Affordance Classification.• IAG [53]: We replace IAG's image backbone with the same backbone as our method while keeping the rest of IAG's original design unchanged.• Laso [19]: We replace Laso's textual input with image input and adopt the same backbone as our method.Meanwhile, we retain Laso's original design for cross-modal learning mechanisms and cross-modal fusion.To meet the new task requirements, we add an Affordance Classification output module to Laso's original output modules.• XMF [1]: XMF is used for fusing images and point clouds to complete the point cloud completion task.We replace XMF's image backbone with the same backbone as our method while retaining XMF's cross-modal learning design.To meet the new task requirements, we replace XMF's original output modules with the 3D Affordance Grounding and Affordance Classification output modules.</p>
<p>A.2. Method Details</p>
<p>We use a pretrained Swin model to extract image features.Since the input images possess varying sizes, inspired by IAG, we first resize the images to 224 × 224 and then use the backbone to extract the initial affordance feature F ini ∈ R 512×7×7 .Next, we generate a scene mask M sce using the bounding boxes of the subject and object entities in the image.The three components are then located in the corresponding positions in F ini , and their features are extracted using ROI alignment to attain object entity feature map F ob ∈ R 512×16 , subject entity feature map F sb ∈ R 512×16 , and scene context feature map F sc ∈ R 512×16 .Subsequently, we employ a multi-head attention mechanism (MHA) to fuse the object with both the subject and the scene features to attain F e ∈ R 512×16 and F s ∈ R 512×16 .A scene gating operation is then applied to selectively integrate the scene information, as show in Fig. 7.After which the two components are fused to obtain 2D context-aware affordance feature F I ∈ R 512×16 .Following this, we use the Pointnet++ and downsampling operations on the point cloud to extract multi-scale features F l P ∈ R 512×64 and F s P ∈ R 512×128 , and construct graphs for different scales among regions to attain S l ∈ R 64×64 and S s ∈ R 128×128 .</p>
<p>Gating Network</p>
<p>Details of Scene Gate</p>
<p>Next, in the cross-modal fusion module, we also use MHA to obtain cross-modal representations.Specifically, we first enhance the point cloud features using the contextaware affordance feature, resulting in Fl P ∈ R 512×64 and Fs P ∈ R 512×128 .Then, the updated multi-scale geometric region features are used to refine the multi-scale global contextual features Fl I ∈ R 512×16 and Fs I ∈ R 512×16 .After the cross-modal fusion, we perform multi-scale decoding operations.We propagate regional information using a two-layer GCN network.The input to the GCN is Fl/s P ⊙ Γ(F l/s P ) and S l/s , and the learnable weight matrix of each layer has the same dimensions R 512×512 .After obtaining the outputs Rl P ∈ R 512×64 and Rs P ∈ R 512×128 , as the refined regional affordance features, we combine these with the earlier downsampling parameters to perform upsampling, then a gating network is then used for soft selection of the multi-scale features as RP ∈ R 512×2048 , which are fed into an MLP network to produce the grounding result φ ∈ R 2048×1 .At the same time, we share the selection weights of semantic and geometric features and transfer these weights to the semantic features, obtaining FI ∈ R 512×16 , the details are summarized in Fig. 8.</p>
<p>Finally, we use the grounding result as a weight on RP .Then conduct the average pooling on it and FI , we finally concatenate them as the input of our affordance class head to predict the affordance category ŷ.The notation table is summarized in Table 4.</p>
<p>A.3. Evaluation Metrics</p>
<p>Below are five used evaluation metrics and their calculation methods:</p>
<ol>
<li>AUC (Area Under Curve):</li>
</ol>
<p>AUC [23]  SIM [43] measures the similarity between the predicted saliency map and the ground truth saliency map.For a predicted map P and a ground truth map Q D , SIM is computed by taking the minimum value between them at each pixel position and summing these minimum values.</p>
<p>Both the predicted map and the ground truth map need to be normalized so that their pixel sums equal 1.The formula for SIM is:
SIM(P, Q D ) = i min(P i , Q Di ).(12)
A higher SIM value indicates greater similarity between the prediction and the ground truth.</p>
<p>MAE (Mean Absolute Error):</p>
<p>MAE [49] evaluates the point-wise error between the predicted saliency map and the ground truth saliency map.It is calculated by taking the sum of the absolute values of all prediction errors and dividing it by the total number of points n:
MAE = 1 n n i=1 |e i |,(13)
where e i is the error for each point.A smaller MAE value indicates a smaller difference between the prediction and the ground truth.</p>
<p>ACC (Accuracy):</p>
<p>ACC evaluates the proportion of correctly classified points in the entire dataset.It is defined as the ratio of the sum of true positives (TP) and true negatives (TN) to the total number of points.The formula is:
ACC = TP + TN TP + FP + FN + TN .(14)
A higher ACC value indicates a better overall classification performance.</p>
<p>A.4. Training Details</p>
<p>Our model is implemented in PyTorch and is trained using the Adam optimizer.We set the number of epochs to 150 and the batch size to 16.To ensure fairness, we run comparative algorithms with both 150 epochs and the recommended values from their respective papers, reporting</p>
<p>B. Experiments B.1. Backbones</p>
<p>Considering that IAG uses ResNet as the backbone for image feature extraction, in this section, we compare both IAG and our algorithm using the feature extractor specified in the original IAG paper, while keeping other components unchanged.The results are presented in Table 5.From the table, it can be observed that our model still outperforms IAG in both affordance grounding and classification tasks, especially in affordance classification.Therefore, our algorithm demonstrates superior performance across different feature extractors, further validating the effectiveness of our model.</p>
<p>B.2. Model Size</p>
<p>We introduce multi-scale features and a geometric feature propagation module to address the issue of incomplete affordance area prediction and the lack of multi-scale information utilization in existing methods.However, their inclusion may increase model size.To demonstrate the scalability of our approach, we compare model sizes with existing methods, as shown in Table 6.Despite incorporating geometric similarity modeling, our model remains smaller than Laso and XMF and is comparable to IAG, highlighting its efficiency and ability to achieve superior performance without excessive computational cost.</p>
<p>B.3. Hyper-Parameter Analysis</p>
<p>In our model, we use a hyperparameter to balance the relative importance of classification loss and grounding loss.To investigate the impact of this hyperparameter on model performance, we performed a sensitivity analysis, as shown in Table 7.The results indicate that when the hyper-parameter λ c is set to 0.3, our model achieves the best performance on both the seen and unseen settings.Therefore, we choose 0.3 as the optimal value for this hyper-parameter.</p>
<p>B.4. More Visualization Results</p>
<p>B.4.1. More comparison results</p>
<p>In the main text, we have provided several visualization results to demonstrate the effectiveness of our algorithm.</p>
<p>In this section, we present additional visualization results to showcase the superior performance of our method, as shown in Fig. 9 and 10.From these figures, it can be observed that our algorithm achieves satisfactory affordance grounding results across different affordances and various objects.Furthermore, the visualizations also demonstrate that our method successfully addresses the issues present in existing methods, such as ignoring multi-scale information and failing to incorporate the structural guidance of the point cloud for affordance grounding.</p>
<p>B.4.2. Different instances</p>
<p>Unlike existing methods using a complex alignment module, we propose a simple yet effective cross-modal fusion module.Since we do not utilize a complex 2D-3D alignment module, we might still achieve satisfactory results when the the object in image and the point cloud share similar affordances.To investigate it, we conduct experiments by pairing the same image with: 1) objects of the same cate- gory, 2) objects of different categories but with similar geometric structures, and 3) different object categories with different geometric structures.The results are plotted in Fig. 11.We observe that as the geometric discrepancy increases, the grounding performance gradually decreases.However, even for objects with vastly different geometric structures, our method can still achieve satisfactory results.</p>
<p>B.4.3. Different affordances</p>
<p>Our method first extract the context-aware affordance information of an image and utilize it to guide 3D affordance grounding.To verify the effectiveness of our image context learning, we pair the same point cloud with images that reflect different affordances, and we utilize our method to generate the grounding results.As shown in Fig. 12, we can observe that our method successfully grounds the affordance depicted in the image onto the 3D point cloud.Therefore, the 2D Context-Aware Affordance Extractor of our method is effective.</p>
<p>Figure 1 .
1
Figure 1.Motivation of Our Method: a) The cascaded model effectively couples grounding and classification; b) The inherent geometric similarity among different parts of an object allows for deriving the full potential interactive areas from a specific interaction; c) Learning affordance areas from a single scale is inefficient due to the wide span of scales, while multi-scale geometric features facilitate effective learning of affordance areas across this range of scales.</p>
<p>Figure 2 .
2
Figure 2. Overview of our framework.It identifies the affordance region and category in four steps: 1) Extract 2D context-aware affordance feature FI and multi-scale 3D geometric features F l P and F s P (Sec.3.2); 2) Fuse the two modalities, obtain multiscale features Fl P , Fs P , Fl I and Fs I (Sec.3.3); 3) Propogate the regional features at each scale and conduct scale selection to generate final representations RP and FI (Sec.3.4); 4) Predict the probabilistic mask φ and affordance category ŷ (Sec.3.5).</p>
<p>2 : 1 )
21
Multi-modal Feature Module, which extracts contextaware affordance cue from the image and multi-scale geometric features from the point cloud (Sec.3.2); 2) Crossmodal Fusion Module, which fuses those two modalities to obtain multi-scale affordance-aware 3D representations at the global and local region levels (Sec.3.3); 3) Propagation and Selection Module, which then propagates the local features at each scale via a graph neural network and selects a relevant scale to generate final global and local representations (Sec.3.4); 4) Affordance Prediction Module, which first predicts the grounding probabilistic mask and then fuses the global contextual feature and masked local feature to predict the affordance class (Sec.3.5).Below, we will introduce the design of each module and the model training (Sec.3.6) in detail.Additionally, we provide further details on model implementation and training in Ap-pendix Sec.A.2 and Sec.A.4.</p>
<ol>
<li>1 .
1
Experimental Setting 4.1.1.Dataset and baselines</li>
</ol>
<p>Figure 3 .
3
Figure 3.The visualization results.The first row displays the interactive images that reflect object affordances.The last row shows the ground truth of the affordance regions in the 3D point cloud.The left four columns correspond to the Seen setting, while the right four columns represent the Unseen setting.</p>
<p>Figure 4 .
4
Figure 4. Visualization of ablation studies: (a) Without GFPM, predictions fail to cover the full affordance area.(b) Removing MSI reduces precision for fine-grained regions.(c) Without CGC, grounding areas are confused with incorrect functionalities.</p>
<p>Figure 5 .Figure 6 .
56
Figure 5. Visualization of cross-dataset generalization results on ShapeNet point clouds</p>
<p>Figure 7 .Figure 8 .
78
Figure 7.The details of our scene gata.</p>
<p>Figure 9 .Figure 10 .Figure 11 .Figure 12 .
9101112
Figure 9.The visualization results of our method with IAG and GT.</p>
<p>Table 1 .
1
The experimental results of our proposed method and the competitors on PIAD.The best results are marked in bold.Seen and Unseen are two partitions of the dataset.AUC, aIOU, and ACC are shown in percentage.SIM ↑ MAE ↓ ACC ↑ AUC ↑ aIOU ↑ SIM ↑ MAE ↓ ACC ↑
Method AUC ↑ aIOU ↑ Baseline 82.92 17.45Seen 0.5220.10184.1966.096.76Unseen 0.3120.13829.65XMF80.3914.420.4970.11475.4067.684.900.3030.12820.14Laso84.2619.830.5470.09373.4270.626.000.3460.12638.33IAG85.9620.130.5640.09283.5072.207.390.3490.12542.36Ours87.2022.750.6040.08190.9174.408.500.3630.11745.14</p>
<p>Table 2 .
2
The ablation study to investigate the effectiveness of main modules of our method.The best results are marked in bold.SIM ↑ MAE ↓ ACC ↑ AUC ↑ aIOU ↑ SIM ↑ MAE ↓ ACC ↑
Variants AUC ↑ aIOU ↑ Blind 82.92 17.45Seen 0.5220.10184.1966.096.76Unseen 0.3120.13829.65w/o MSI85.9620.390.5790.08788.9370.987.130.3590.13034.40w/o GFPM85.8319.930.5810.09290.3270.696.790.3480.13938.43w/o CGC86.4922.060.5930.08490.0270.616.250.3370.14838.33w/o SG86.3021.580.5990.09089.9271.426.540.3550.11941.74Ours87.2022.750.6040.08190.9174.408.500.3630.11745.14</p>
<p>Table 3 .
3
Comparison of different affordance datasets in terms of object class, affordance class, point cloud count, text annotations, and image samples.
MethodObject CLS Affordance CLS Instance Text ImageAdaAfford [47]152972--Where2Explore [32]142942--LASO [19]23188434870-Lemon [54]21175143-5001Ours23177012-5162Therefore, in the future, we could propose a method ca-pable of handling both text-guided and image-guided 3Daffordance tasks. Furthermore, we compare model sizes todemonstrate the scalability of our approach. The results areprovided in Appendix Sec.B.2. Similarly, further details onhyperparameter analysis experiments are provided in Ap-pendix Sec. B.3.</p>
<p>Table 4 .
4
The dimension and meaning of the tensors of our method.
TensorDimensionMeaningF ini512 × 7 × 7image extractor outputF ob,sb,sc 512 × 16features output by roi-alignF I512 × 16context-aware affordance featureF S l/s l/s P512 × 64/128 64/128 × 64/128 similarity graph of large/small-scale regional feature large/small-scale regional featureFl/s P512 × 64/128large/small-scale regional feature after cross-modal fusionFl/s I512 × 16context-aware affordance feature with large/small-scale regional informationRl/s P RP512 × 64/128 512 × 2048large/small-scale regional feature after message propagation regional feature after scale selectionFI512 × 16context-aware affordance feature after scale selectionφ2048 × 1predicted grounding resultŷ1predicted affordance category resultthe highest results. All experiments are conducted on a sin-gle A100 GPU with an initial learning rate of 0.0005. Ourmodel includes a hyperparameter to balance the weights oftwo loss components, set to 0.3. We use Swin as the back-bone for images, loading pretrained weights, while the pointcloud feature extractor uses an untrained PointNet++. LikeIAG, each point cloud is paired with two images duringtraining.</p>
<p>Table 5 .
5
Comparison of Experimental Results between IAG and Our Proposed Method using ResNet.SIM ↑ MAE ↓ ACC ↑ AUC ↑ aIOU ↑ SIM ↑ MAE ↓ ACC ↑
Method AUC ↑ aIOU ↑ IAG 84.84 20.23Seen 0.5610.09682.3171.677.93Unseen 0.3520.13033.62Ours85.7621.330.5780.08686.3672.758.890.3640.12739.77</p>
<p>Table 6 .
6
Comparison of model sizes (in MB).
ModelXMFLASOIAGOursSize (MB) 392.23 391.14 366.53 370.55</p>
<p>Table 7 .
7
The experimental results with different hyperparameters.λSeen Unseen AUC ↑ aIOU ↑ SIM ↑ MAE ↓ ACC ↑ AUC ↑ aIOU ↑ SIM ↑ MAE ↓ ACC ↑
0.387.2022.750.6040.08190.9174.408.500.3630.11745.140.186.8822.340.5910.08390.7173.407.860.3550.12241.940.586.8422.470.5970.08390.7173.397.990.3560.12543.700.786.5221.690.5890.08490.1272.807.550.3430.12942.25
Supplementary MaterialA. Implementation Details A.1.Baselines
Crossmodal learning for image-guided point cloud shape completion. Emanuele Aiello, Diego Valsesia, Enrico Magli, Advances in Neural Information Processing Systems. 2022351</p>
<p>Objectfusion: Multi-modal 3d object detection with object-centric fusion. Qi Cai, Yingwei Pan, Ting Yao, Chong-Wah Ngo, Tao Mei, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Shapenet: An information-rich 3d model repository. X Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Jianxiong Su, Li Xiao, Fisher Yi, Yu, 2015</p>
<p>Pimae: Point cloud and image interactive masked autoencoders for 3d object detection. Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang, Yuheng Lu, Yandong Guo, Shanghang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>A survey of visual affordance recognition based on deep learning. Dongpan Chen, Dehui Kong, Jinghua Li, Shaofan Wang, Baocai Yin, IEEE Transactions on Big Data. 22023</p>
<p>Learning to act properly: Predicting and explaining affordances from images. Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Learning to act properly: Predicting and explaining affordances from images. Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Scenefun3d: Fine-grained functionality and affordance understanding in 3d scenes. Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, Francis Engelmann, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Transvg: End-to-end visual grounding with transformers. Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, Houqiang Li, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021</p>
<p>3d affordancenet: A benchmark for visual object affordance understanding. Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia, proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202135</p>
<p>Affordancenet: An end-to-end deep learning approach for object affordance detection. Thanh-Toan Do, Anh Nguyen, Ian Reid, 2018 IEEE International Conference on Robotics and Automation (ICRA). 2018</p>
<p>Learning 2d invariant affordance knowledge for 3d affordance grounding. Xianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan Ding, Bin Zhao, Xuelong Li, 2024</p>
<p>Visual affordance and function understanding: A survey. Mohammed Hassanin, Salman Khan, Murat Tahtali, ACM Comput. Surv. 543</p>
<p>Piotr Dollár, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Crosssource point cloud registration: Challenges, progress and prospects. Xiaoshui Huang, Guofeng Mei, Jian Zhang, Neurocomputing. 54821263832023</p>
<p>Affordpose: A large-scale dataset of hand-object interactions with affordance-driven hand pose. Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, Jian Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Origins of human intelligence: The chain of tool-making and brain evolution. Hyun Kwang, Ko, Anthropological Notebooks. 2212016</p>
<p>Image-to-point cloud registration via deep classification. Jiaxin Li, Gim Hee, Lee , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20212</p>
<p>Laso: Language-guided affordance segmentation on 3d object. Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-Seng Chua, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202461</p>
<p>A novel point cloud registration using 2d image features. Chien-Chou Lin, Yen-Chou Tai, Jhong-Jin Lee, Yong-Sheng Chen, EURASIP Journal on Advances in Signal Processing. 201722017</p>
<p>Shang-Ching Liu, Wenkai Van Nhiem Tran, Wei-Lun Chen, Yen-Lin Cheng, I-Bin Huang, Yung-Hui Liao, Jianwei Li, Zhang, arXiv:2410.11564Pavlm: Advancing point cloud based affordance understanding via vision-language model. 2024arXiv preprint</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Auc: a misleading measure of the performance of predictive distribution models. Jorge M Lobo, Alberto Jiménez-Valverde, Raimundo Real, Global Ecology and Biogeography. 22008</p>
<p>Open-vocabulary point-cloud object detection without 3d annotation. Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Learning affordance grounding from exocentric images. Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Context-based affordance segmentation from 2d images for robot actions. Timo Lüddecke, Tomas Kulvicius, Florentin Wörgötter, Robotics and Autonomous Systems. 11922019</p>
<p>Pretraining on interactions for learning grounded affordance representations. Jack Merullo, Dylan Ebert, Carsten Eickhoff, Ellie Pavlick, arXiv:2207.022722022arXiv preprint</p>
<p>Intention-related natural language grounding via object affordance detection and intention semantic extraction. Jinpeng Mi, Hongzhuo Liang, Nikolaos Katsakis, Song Tang, Qingdu Li, Changshui Zhang, Jianwei Zhang, Frontiers in Neurorobotics. 202014</p>
<p>V-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 2016 fourth international conference on 3D vision (3DV). Ieee2016</p>
<p>Grounded human-object interaction hotspots from video. Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Detecting object affordances with convolutional neural networks. Anh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, Nikos G Tsagarakis, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2016</p>
<p>Where2explore: Few-shot affordance learning for unseen novel categories of articulated objects. Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, Hao Dong, Advances in Neural Information Processing Systems. Curran Associates, Inc202326</p>
<p>Mhsan: multi-head self-attention network for visual semantic embedding. Geondo Park, Chihye Han, Wonjun Yoon, Daeshik Kim, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2020</p>
<p>What makes an ai device human-like? the role of interaction quality, empathy and perceived psychological anthropomorphic characteristics in the acceptance of artificial intelligence in the service industry. Corina Pelau, Dan-Cristian Dabija, Irina Ene, Computers in Human Behavior. 12211068552021</p>
<p>Learning spatial affordances from 3d point clouds for mapping unseen human actions in indoor environments. Lasitha Piyathilaka, Sarath Kodagoda, Karthick Thiyagarajan, Massimo Piccardi, D M G Preethichandra, Umer Izhar, IEEE Access. 1222024</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Charles Ruizhongtai, Qi , Li Yi, Hao Su, Leonidas J Guibas, Advances in neural information processing systems. 3052017</p>
<p>Imvotenet: Boosting 3d object detection in point clouds with image votes. Charles R Qi, Xinlei Chen, Or Litany, Leonidas J Guibas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Affordancellm: Grounding affordance from vision language models. Weifeng Shengyi Qian, Min Chen, Xiong Bai, Zhuowen Zhou, Li Erran Tu, Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation. Atiqur Md, Yang Rahman, Wang, 2016</p>
<p>Corri2p: Deep image-to-point cloud registration via dense correspondence. Yiming Siyu Ren, Junhui Zeng, Xiaodong Hou, Chen, IEEE Transactions on Circuits and Systems for Video Technology. 202233</p>
<p>Focal loss for dense object detection. T-Ylpg Ross, Dollár, proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>A multi-scale cnn for affordance segmentation in rgb images. Anirban Roy, Sinisa Todorovic, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part IV 14</p>
<p>Color indexing. J Michael, Dana H Swain, Ballard, International Journal of Computer Vision. 21991</p>
<p>Lgafford-net: A local geometry aware affordance detection network for 3d point clouds. Ramesh Ashok Tabib, Dikshit Hegde, Uma Mudenagudi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>The oldowan: the tool making of early hominins and chimpanzees compared. Annual Review of Anthropology. Nicholas Toth, Kathy Schick, 200938</p>
<p>Da-net: Density-aware 3d object detection network for point clouds. Shuhua Wang, Ke Lu, Jian Xue, Yang Zhao, IEEE Transactions on Multimedia. 22023</p>
<p>Adaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot interactions. Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, Hao Dong, European conference on computer vision. Springer2022</p>
<p>Object pose estimation from rgb-d images with affordance-instance segmentation constraint for semantic robot manipulation. Zhongli Wang, Guohui Tian, IEEE Robotics and Automation Letters. 912024</p>
<p>Advantages of the mean absolute error (mae) over the root mean square error (rmse) in assessing average model performance. C J Willmott, Matsuura, Climate Research. 22005</p>
<p>Chao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, Siyuan Huang, arXiv:2202.13519Partafford: Part-level affordance discovery from 3d objects. 2022arXiv preprint</p>
<p>Weakly-supervised 3d visual grounding based on visual linguistic alignment. Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wenhui Wu, Zequn Jie, Lin Ma, Xu Wang, 2024</p>
<p>Artificial intelligence: A powerful paradigm for scientific research. Yongjun Xu, Xin Liu, Xin Cao, Changping Huang, Enke Liu, Sen Qian, Xingchen Liu, Yanjun Wu, Fengliang Dong, Cheng-Wei Qiu, Junjun Qiu, Keqin Hua, Wentao Su, Jian Wu, Huiyu Xu, Yong Han, Chenguang Fu, Zhigang Yin, Miao Liu, Ronald Roepman, Sabine Dietmann, Marko Virta, Fredrick Kengara, Ze Zhang, Lifu Zhang, Taolan Zhao, Ji Dai, Jialiang Yang, Liang Lan, Ming Luo, Zhaofeng Liu, Tao An, Bin Zhang, Xiao He, Shan Cong, Xiaohong Liu, Wei Zhang, James P Lewis, James M Tiedje, Qi Wang, Zhulin An, Fei Wang, Libo Zhang, Tao Huang, Chuan Lu, Zhipeng Cai, Fang Wang, Jiabao Zhang, The Innovation. 241001792021</p>
<p>Grounding 3d object affordance from 2d interactions in images. Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha, 2023 IEEE/CVF International Conference on Computer Vision (ICCV). 2023. 1, 4, 5</p>
<p>Lemon: Learning 3d human-object interaction relation from 2d images. Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202436</p>
<p>Sat: 2d semantics assisted training for 3d visual grounding. Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021</p>
<p>Human intention understanding based on object affordance and action classification. Zhibin Yu, Sangwook Kim, Rammohan Mallipeddi, Minho Lee, 2015 International Joint Conference on Neural Networks (IJCNN). 2015</p>
<p>One-shot object affordance detection in the wild. Wei Zhai, Hongchen Luo, Jing Zhang, Yang Cao, Dacheng Tao, International Journal of Computer Vision. 1301052022</p>
<p>Mono3dvg: 3d visual grounding in monocular images. Yang Zhan, Yuan Yuan, Zhitong Xiong, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Study on artificial intelligence: The state of the art and future prospects. Caiming Zhang, Yang Lu, Journal of Industrial Information Integration. 2311002242021</p>
<p>Multi3drefer: Grounding text description to multiple 3d objects. Yiming Zhang, Zeming Gong, Angel X Chang, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>            </div>
        </div>

    </div>
</body>
</html>
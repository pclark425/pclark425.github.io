<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2152 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2152</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2152</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-279251781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07591v1.pdf" target="_blank">Automating Exploratory Multiomics Research via Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces PROTEUS, a fully automated system that produces data-driven hypotheses from raw data files. We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries. PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal. It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes. We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses. Results were evaluated through external data validation and automatic open-ended scoring. Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty. In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2152.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2152.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROTEUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROTeogenomics Exploration and Understanding System (PROTEUS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully automated, LLM-orchestrated system that performs end-to-end exploratory analysis of clinical multiomics data (proteomics-centric) to generate, decompose, validate, and integrate data-driven biological hypotheses using a graph-structured memory and a suite of predefined bioinformatics tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PROTEUS</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid system (large language model orchestrator + predefined bioinformatics tools)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>clinical proteogenomics / multiomics data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>scientific hypotheses and research directions (open-ended, multi-step mechanistic hypotheses across omics layers)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Two complementary automated validation pipelines: (1) evidence-based cohort validation using external CPTAC cohorts (statistical tests run via predefined CPTAC analysis functions called automatically by the LLM) which labels each analysis result as support/weak support/none/weak contradict/contradict based on p-value thresholds and effect directions; (2) open-ended literature-informed scoring using GPT-4o to score each hypothesis along five metrics (Literature Alignment, Logical Coherence, Scientific Novelty, Biological Significance, General Quality) with PubMed references provided as context.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Assessed by LLM-based scoring (Scientific Novelty metric in 0-5 scale) with referenced PubMed articles and by inspecting low Literature Alignment scores (indicating originality) together with external cohort corroboration; no explicit 'distance-from-training-data' metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generated 360 hypotheses across 10 datasets (36 per dataset). PROTEUS produced more complex hypotheses (involving more relationships) and, compared to two LLM baselines using the same toolset, produced a larger total number of CPTAC-validation results and a larger absolute number of supporting validation results. Qualitatively, PROTEUS balanced novelty and reliability better than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>On 5 datasets with matching CPTAC cohorts, CPTAC-based assessments (support + weak support) comprised a majority (>70%) of effective assessments for all datasets except BRCA; for ccRCC, LSCC, LUAD the strongest assessment ('support') approached ~70% on those datasets. LLM-scoring (GPT-4o) showed higher average scores on five quality metrics compared to baselines. No single numerical accuracy/precision/recall reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Observed: hypotheses that were more verifiable by CPTAC data typically had higher Literature Alignment (i.e., resembled existing literature). Scientific Novelty scores did not monotonically increase with CPTAC-verifiability thresholds, indicating that highly novel hypotheses are not necessarily verified by cohort statistics and require additional non-statistical evidence/interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — PROTEUS generates complex, multi-edge hypotheses that can be novel and creative, but statistical validation is limited by available cohort data; some novel hypotheses lack external cohort support despite high LLM-scoring novelty/biological-significance. Conversely, hypotheses that are easy to validate statistically often align more with existing literature (less novel).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Designed for open-ended exploration (aims to handle out-of-distribution / novel directions), but empirical evaluation indicates variable performance: good statistical validation on more recent, higher-quality datasets (ccRCC, LSCC, LUAD) and poorer performance on older/smaller/dataset-quality-limited BRCA cohort. No numeric OOD metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation required multiple automated statistical tool calls against external CPTAC cohorts plus LLM-driven selection and retry loops for parameter tuning; cost higher than single-step generation but exact runtime/costs not reported. Tool-calling allowed up to 3 attempts per tool; after retries each dataset had >=50 successful tool executions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combines LLM planning with predefined statistical tools, iterative tool-call retry loops, structured relationship/conclusion graphs to track context and compose multi-step validations, and dual evaluation combining cohort statistics and literature-informed LLM scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PROTEUS can autonomously generate diverse, multi-step biological hypotheses from raw multiomics data and validate many of them automatically via external cohort statistics and LLM-based literature scoring, showing a practical balance between novelty and statistical reliability across multiple datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2152.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (GPT-4o used as base LLM in PROTEUS and evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used as PROTEUS's central decision-maker and also used for automatic hypothesis scoring and CPTAC function selection; employed for generation, planning, tool orchestration, and scoring tasks in the system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning and domain-specific bioinformatics orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>natural-language research directions, detailed hypotheses, planning and decomposition of research objectives, tool parameter selection, interpretation and integration of statistical results</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used to propose CPTAC analysis function calls and parameters, to retry failed calls based on errors, and to interpret statistical outputs into support/contradict assessments; also used to score hypotheses against literature on five metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly measured for the LLM itself; used within PROTEUS's novelty scoring pipeline as the evaluator for Scientific Novelty and Literature Alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>As the base model, GPT-4o enabled PROTEUS to achieve the highest number of supporting CPTAC validation results and highest LLM-scoring across multiple metrics compared to alternative base models (GPT-4o-mini, DeepSeek-R1).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Enabled automated, multi-step validation via CPTAC tools with higher percentages of supporting results and greater total number of successful validations than alternative base LLMs in ablations; also produced higher average LLM-scores (General Quality, Literature Alignment, Logical Coherence).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>As evaluator, GPT-4o's literature-alignment scoring correlated with CPTAC verifiability (higher CPTAC support associated with higher Literature Alignment), indicating the model's scoring aligns statistical support with existing literature, but it does not guarantee novel hypotheses receive high validation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Observed: GPT-4o favors producing hypotheses that are more verifiable and logically coherent (higher validation success) compared to models tuned for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>High-capability LLM used to orchestrate retries, parameter adjustments, and interpretation; combining LLM judgments with external cohort statistics aims to reduce hallucination and increase empirical rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o, when used as PROTEUS's base LLM and as an evaluator, provided superior empirical validation success and higher-quality scoring compared to smaller/alternative models, indicating strong utility as an orchestrator and assessor in automated scientific workflows.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2152.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (smaller variant used in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller variant of GPT-4o evaluated in ablation studies as an alternative base LLM for PROTEUS; compared on CPTAC validation rates and LLM-scoring metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (smaller variant)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning and bioinformatics orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>similar generation tasks as GPT-4o but with reduced capacity</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as a base model in PROTEUS ablation to orchestrate tool-calls and score outputs with the same automated evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Assessed indirectly via LLM-scoring metrics; inferior to GPT-4o in several metrics per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Inferior to GPT-4o in CPTAC evaluation (lower percent supporting results and fewer total successful results) and LLM-scoring metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Lower validation success and lower average LLM-scoring compared to GPT-4o; specific numeric gaps not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not separately characterized; overall inferior validation performance suggests less robustness than GPT-4o for both familiar and novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows the trend that reduced model capacity leads to lower validation effectiveness and lower scoring quality.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Same framework as GPT-4o but with less capacity; no special mechanisms reported to close gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller base LLM (GPT-4o-mini) underperformed GPT-4o in both CPTAC validation success and LLM-scoring, indicating base model capability materially affects automated hypothesis generation and validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2152.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (R1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative base reasoning model evaluated in ablations; characterized as producing hypotheses with higher Scientific Novelty and Biological Significance at the cost of lower reliability in validation and lower scores on other quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / reasoning-focused LLM</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific hypothesis generation and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates novel hypotheses, reasoning chains, and mechanistic conjectures</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Applied within PROTEUS for hypothesis generation and subjected to the same CPTAC statistical validation and LLM-scoring evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Scored slightly higher on Scientific Novelty and Biological Significance in LLM-scoring relative to GPT-4o, indicating stronger tendency toward novel, high-impact hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Produced more novel hypotheses but with lower empirical validation success; lowest scores for Literature Alignment, General Quality, and Logical Coherence among compared base models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Lowest overall validation success and lower LLM-evaluation on several metrics, implying more false or less statistically supported hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Demonstrates a trade-off: higher novelty but lower reliability/validation success compared to GPT-4o, indicating novelty correlates with lower cohort-verifiability for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — stronger at generation/novelty but weaker at producing hypotheses that survive statistical validation, highlighting a generation-vs-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>No special mechanisms reported; authors note the trade-off and interpret R1 as producing novel but less reliable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepSeek-R1 produces more novel and biologically significant-seeming hypotheses but at a measurable cost to statistical rigor and coherency compared to GPT-4o, exemplifying the generation-vs-validation trade-off.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2152.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-centered baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-centered baseline (gpt-4o baseline for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline system built around GPT-4o that generates Python analysis code using the same bioinformatics packages as PROTEUS to analyze the same datasets; operates with up to 5 code-refinement iterations and requires a preset research direction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-centered baseline (GPT-4o-driven code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model + code-generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>bioinformatics data analysis and hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates analysis code and then produces a fixed number of hypotheses based on execution results (goal-directed generation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation limited to results of generated code execution on the source dataset; compared against PROTEUS using the same CPTAC and LLM-scoring evaluations for downstream assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly optimized for novelty; relies on code-executed statistical signals from the source dataset; novelty inferred to be lower than PROTEUS based on LLM-scoring results.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Produced hypotheses with similar distributions of CPTAC support statuses but fewer total supporting results and less complexity (i.e., fewer multi-edge hypotheses) compared to PROTEUS.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Lower absolute number of supporting CPTAC results and lower average LLM-scoring metrics relative to PROTEUS.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not specifically characterized; tends to produce more constrained hypotheses that are less exploratory, yielding fewer novel hypotheses but possibly more straightforwardly validated ones.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Less pronounced asymmetry than PROTEUS; more conservative generation yields fewer complex but more easily testable hypotheses, yet overall validation counts were lower.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Relies on iterative code refinement; lacks PROTEUS's relationship/conclusion graph scaffolding and iterative exploration heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A vanilla LLM-code baseline yields reasonable but fewer and less complex hypotheses than PROTEUS and achieves lower downstream validation counts, indicating the value of PROTEUS's structured exploration and tool orchestration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2152.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POPPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POPPER (prior work applying LLMs to validate preset hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system (cited) that uses LLMs to analyze data in order to validate a specific, predefined hypothesis rather than perform open-ended exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>POPPER</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + data-analysis tooling</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>bioinformatics / data-driven hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>validates specific, user-specified hypotheses (not open-ended discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Guides model with a predetermined goal/hypothesis and applies analysis to attempt validation; emphasizes directed validation rather than exploratory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not designed for novelty; focused on assessing pre-specified hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Directed hypothesis validation (constrains generation to a single target) reduces exploration but simplifies validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior art that validates preset hypotheses via LLM-driven analysis, contrasting with PROTEUS's open-ended exploratory approach.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2152.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DREAM (autonomous research system)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DREAM: a biomedical data-driven self-evolving autonomous research system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced autonomous research system that eliminates human input but evaluates outputs primarily by whether the initial research question was resolved rather than performing deeper verification of reliability or depth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DREAM: a biomedical data-driven self-evolving autonomous research system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DREAM</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>autonomous research system (LLM-based / agentic)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data-driven autonomous research</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>automated research outputs / decisions aimed at resolving a predefined research question</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated by whether the initial question was resolved; does not perform thorough external verification of result reliability per paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not emphasized; focus on automated resolution of specific research goals.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Self-evolving autonomous pipelines but evaluation limited to task resolution, highlighting potential gaps in rigorous validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an autonomous system that removes human inputs but whose evaluation does not address reliability or depth, in contrast to PROTEUS's dual-evaluation approach.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2152.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical research (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work demonstrating autonomous experimental optimization (chemistry reaction optimization) using LLMs and lab automation, typically in well-defined, repetitive tasks with established quantitative evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous chemical research agent (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + lab automation (agentic)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>chemistry / experimental optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>experimental designs, parameter optimization, predictions of reaction outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experimental testing (lab-in-the-loop) with quantitative optimization metrics (e.g., yield), enabling clear, repeatable evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by improvement in experimental metrics and optimization performance; typically less open-ended novelty than exploratory biology hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tight integration with experimental execution allows direct empirical validation, reducing generation-validation gap in well-defined task spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate autonomous systems succeeding in narrowly defined, quantitatively-evaluable tasks (contrast to open-ended multiomics discovery).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2152.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2152.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialAgent / CellAgent / other LLM agent frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialAgent, CellAgent and related LLM-driven multi-agent bioinformatics frameworks (cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent LLM-driven agent frameworks for domain-specific automated analyses (spatial biology, single-cell analysis) that automate aspects of experimental design or data processing in specialized domains with clearer task definitions than open-ended hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SpatialAgent / CellAgent / similar</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM multi-agent frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>spatial biology, single-cell omics, specialized bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>automated analyses, experimental design suggestions, pipeline orchestration for domain-specific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Typically validated on domain-specific tasks with established benchmarks or via lab-in-the-loop experiments; methods vary by paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Often evaluated by task-specific performance and usefulness rather than open-ended novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Domain specialization and task-constrained agent design enable clearer validation and reduce generation-validation mismatch compared to fully open-ended systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work in domain-specialized autonomous analyses; contrasts with PROTEUS's broader exploratory scope and dual evaluation strategy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>POPPER <em>(Rating: 1)</em></li>
                <li>DREAM: a biomedical data-driven self-evolving autonomous research system <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>SpatialAgent: An autonomous AI agent for spatial biology <em>(Rating: 2)</em></li>
                <li>CellAgent: An LLM-driven multi-agent framework for automated single-cell data analysis <em>(Rating: 2)</em></li>
                <li>Automated hypothesis validation with agentic sequential falsifications <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2152",
    "paper_id": "paper-279251781",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "PROTEUS",
            "name_full": "PROTeogenomics Exploration and Understanding System (PROTEUS)",
            "brief_description": "A fully automated, LLM-orchestrated system that performs end-to-end exploratory analysis of clinical multiomics data (proteomics-centric) to generate, decompose, validate, and integrate data-driven biological hypotheses using a graph-structured memory and a suite of predefined bioinformatics tools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PROTEUS",
            "system_type": "hybrid system (large language model orchestrator + predefined bioinformatics tools)",
            "domain": "clinical proteogenomics / multiomics data analysis",
            "generation_capability": "scientific hypotheses and research directions (open-ended, multi-step mechanistic hypotheses across omics layers)",
            "validation_method": "Two complementary automated validation pipelines: (1) evidence-based cohort validation using external CPTAC cohorts (statistical tests run via predefined CPTAC analysis functions called automatically by the LLM) which labels each analysis result as support/weak support/none/weak contradict/contradict based on p-value thresholds and effect directions; (2) open-ended literature-informed scoring using GPT-4o to score each hypothesis along five metrics (Literature Alignment, Logical Coherence, Scientific Novelty, Biological Significance, General Quality) with PubMed references provided as context.",
            "novelty_measure": "Assessed by LLM-based scoring (Scientific Novelty metric in 0-5 scale) with referenced PubMed articles and by inspecting low Literature Alignment scores (indicating originality) together with external cohort corroboration; no explicit 'distance-from-training-data' metric reported.",
            "generation_performance": "Generated 360 hypotheses across 10 datasets (36 per dataset). PROTEUS produced more complex hypotheses (involving more relationships) and, compared to two LLM baselines using the same toolset, produced a larger total number of CPTAC-validation results and a larger absolute number of supporting validation results. Qualitatively, PROTEUS balanced novelty and reliability better than baselines.",
            "validation_performance": "On 5 datasets with matching CPTAC cohorts, CPTAC-based assessments (support + weak support) comprised a majority (&gt;70%) of effective assessments for all datasets except BRCA; for ccRCC, LSCC, LUAD the strongest assessment ('support') approached ~70% on those datasets. LLM-scoring (GPT-4o) showed higher average scores on five quality metrics compared to baselines. No single numerical accuracy/precision/recall reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Observed: hypotheses that were more verifiable by CPTAC data typically had higher Literature Alignment (i.e., resembled existing literature). Scientific Novelty scores did not monotonically increase with CPTAC-verifiability thresholds, indicating that highly novel hypotheses are not necessarily verified by cohort statistics and require additional non-statistical evidence/interpretation.",
            "generation_validation_asymmetry": "Yes — PROTEUS generates complex, multi-edge hypotheses that can be novel and creative, but statistical validation is limited by available cohort data; some novel hypotheses lack external cohort support despite high LLM-scoring novelty/biological-significance. Conversely, hypotheses that are easy to validate statistically often align more with existing literature (less novel).",
            "out_of_distribution_performance": "Designed for open-ended exploration (aims to handle out-of-distribution / novel directions), but empirical evaluation indicates variable performance: good statistical validation on more recent, higher-quality datasets (ccRCC, LSCC, LUAD) and poorer performance on older/smaller/dataset-quality-limited BRCA cohort. No numeric OOD metric provided.",
            "calibration_quality": null,
            "validation_computational_cost": "Validation required multiple automated statistical tool calls against external CPTAC cohorts plus LLM-driven selection and retry loops for parameter tuning; cost higher than single-step generation but exact runtime/costs not reported. Tool-calling allowed up to 3 attempts per tool; after retries each dataset had &gt;=50 successful tool executions.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combines LLM planning with predefined statistical tools, iterative tool-call retry loops, structured relationship/conclusion graphs to track context and compose multi-step validations, and dual evaluation combining cohort statistics and literature-informed LLM scoring.",
            "evidence_type": "supports",
            "key_findings": "PROTEUS can autonomously generate diverse, multi-step biological hypotheses from raw multiomics data and validate many of them automatically via external cohort statistics and LLM-based literature scoring, showing a practical balance between novelty and statistical reliability across multiple datasets.",
            "uuid": "e2152.0"
        },
        {
            "name_short": "GPT-4o (as used)",
            "name_full": "GPT-4o (GPT-4o used as base LLM in PROTEUS and evaluations)",
            "brief_description": "A state-of-the-art large language model used as PROTEUS's central decision-maker and also used for automatic hypothesis scoring and CPTAC function selection; employed for generation, planning, tool orchestration, and scoring tasks in the system.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o",
            "system_type": "large language model",
            "domain": "general scientific reasoning and domain-specific bioinformatics orchestration",
            "generation_capability": "natural-language research directions, detailed hypotheses, planning and decomposition of research objectives, tool parameter selection, interpretation and integration of statistical results",
            "validation_method": "Used to propose CPTAC analysis function calls and parameters, to retry failed calls based on errors, and to interpret statistical outputs into support/contradict assessments; also used to score hypotheses against literature on five metrics.",
            "novelty_measure": "Not explicitly measured for the LLM itself; used within PROTEUS's novelty scoring pipeline as the evaluator for Scientific Novelty and Literature Alignment metrics.",
            "generation_performance": "As the base model, GPT-4o enabled PROTEUS to achieve the highest number of supporting CPTAC validation results and highest LLM-scoring across multiple metrics compared to alternative base models (GPT-4o-mini, DeepSeek-R1).",
            "validation_performance": "Enabled automated, multi-step validation via CPTAC tools with higher percentages of supporting results and greater total number of successful validations than alternative base LLMs in ablations; also produced higher average LLM-scores (General Quality, Literature Alignment, Logical Coherence).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "As evaluator, GPT-4o's literature-alignment scoring correlated with CPTAC verifiability (higher CPTAC support associated with higher Literature Alignment), indicating the model's scoring aligns statistical support with existing literature, but it does not guarantee novel hypotheses receive high validation.",
            "generation_validation_asymmetry": "Observed: GPT-4o favors producing hypotheses that are more verifiable and logically coherent (higher validation success) compared to models tuned for novelty.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "High-capability LLM used to orchestrate retries, parameter adjustments, and interpretation; combining LLM judgments with external cohort statistics aims to reduce hallucination and increase empirical rigor.",
            "evidence_type": "supports",
            "key_findings": "GPT-4o, when used as PROTEUS's base LLM and as an evaluator, provided superior empirical validation success and higher-quality scoring compared to smaller/alternative models, indicating strong utility as an orchestrator and assessor in automated scientific workflows.",
            "uuid": "e2152.1"
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (smaller variant used in ablation)",
            "brief_description": "A smaller variant of GPT-4o evaluated in ablation studies as an alternative base LLM for PROTEUS; compared on CPTAC validation rates and LLM-scoring metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o-mini",
            "system_type": "large language model (smaller variant)",
            "domain": "general scientific reasoning and bioinformatics orchestration",
            "generation_capability": "similar generation tasks as GPT-4o but with reduced capacity",
            "validation_method": "Used as a base model in PROTEUS ablation to orchestrate tool-calls and score outputs with the same automated evaluation pipelines.",
            "novelty_measure": "Assessed indirectly via LLM-scoring metrics; inferior to GPT-4o in several metrics per paper.",
            "generation_performance": "Inferior to GPT-4o in CPTAC evaluation (lower percent supporting results and fewer total successful results) and LLM-scoring metrics.",
            "validation_performance": "Lower validation success and lower average LLM-scoring compared to GPT-4o; specific numeric gaps not provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not separately characterized; overall inferior validation performance suggests less robustness than GPT-4o for both familiar and novel tasks.",
            "generation_validation_asymmetry": "Shows the trend that reduced model capacity leads to lower validation effectiveness and lower scoring quality.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Same framework as GPT-4o but with less capacity; no special mechanisms reported to close gaps.",
            "evidence_type": "supports",
            "key_findings": "Smaller base LLM (GPT-4o-mini) underperformed GPT-4o in both CPTAC validation success and LLM-scoring, indicating base model capability materially affects automated hypothesis generation and validation.",
            "uuid": "e2152.2"
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1 (R1)",
            "brief_description": "An alternative base reasoning model evaluated in ablations; characterized as producing hypotheses with higher Scientific Novelty and Biological Significance at the cost of lower reliability in validation and lower scores on other quality metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepSeek-R1",
            "system_type": "large language model / reasoning-focused LLM",
            "domain": "scientific hypothesis generation and reasoning",
            "generation_capability": "generates novel hypotheses, reasoning chains, and mechanistic conjectures",
            "validation_method": "Applied within PROTEUS for hypothesis generation and subjected to the same CPTAC statistical validation and LLM-scoring evaluation pipelines.",
            "novelty_measure": "Scored slightly higher on Scientific Novelty and Biological Significance in LLM-scoring relative to GPT-4o, indicating stronger tendency toward novel, high-impact hypotheses.",
            "generation_performance": "Produced more novel hypotheses but with lower empirical validation success; lowest scores for Literature Alignment, General Quality, and Logical Coherence among compared base models.",
            "validation_performance": "Lowest overall validation success and lower LLM-evaluation on several metrics, implying more false or less statistically supported hypotheses.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Demonstrates a trade-off: higher novelty but lower reliability/validation success compared to GPT-4o, indicating novelty correlates with lower cohort-verifiability for this model.",
            "generation_validation_asymmetry": "Yes — stronger at generation/novelty but weaker at producing hypotheses that survive statistical validation, highlighting a generation-vs-validation gap.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "No special mechanisms reported; authors note the trade-off and interpret R1 as producing novel but less reliable outputs.",
            "evidence_type": "supports",
            "key_findings": "DeepSeek-R1 produces more novel and biologically significant-seeming hypotheses but at a measurable cost to statistical rigor and coherency compared to GPT-4o, exemplifying the generation-vs-validation trade-off.",
            "uuid": "e2152.3"
        },
        {
            "name_short": "LLM-centered baseline",
            "name_full": "LLM-centered baseline (gpt-4o baseline for comparison)",
            "brief_description": "A baseline system built around GPT-4o that generates Python analysis code using the same bioinformatics packages as PROTEUS to analyze the same datasets; operates with up to 5 code-refinement iterations and requires a preset research direction.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM-centered baseline (GPT-4o-driven code generation)",
            "system_type": "large language model + code-generation pipeline",
            "domain": "bioinformatics data analysis and hypothesis generation",
            "generation_capability": "generates analysis code and then produces a fixed number of hypotheses based on execution results (goal-directed generation)",
            "validation_method": "Validation limited to results of generated code execution on the source dataset; compared against PROTEUS using the same CPTAC and LLM-scoring evaluations for downstream assessment.",
            "novelty_measure": "Not explicitly optimized for novelty; relies on code-executed statistical signals from the source dataset; novelty inferred to be lower than PROTEUS based on LLM-scoring results.",
            "generation_performance": "Produced hypotheses with similar distributions of CPTAC support statuses but fewer total supporting results and less complexity (i.e., fewer multi-edge hypotheses) compared to PROTEUS.",
            "validation_performance": "Lower absolute number of supporting CPTAC results and lower average LLM-scoring metrics relative to PROTEUS.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not specifically characterized; tends to produce more constrained hypotheses that are less exploratory, yielding fewer novel hypotheses but possibly more straightforwardly validated ones.",
            "generation_validation_asymmetry": "Less pronounced asymmetry than PROTEUS; more conservative generation yields fewer complex but more easily testable hypotheses, yet overall validation counts were lower.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Relies on iterative code refinement; lacks PROTEUS's relationship/conclusion graph scaffolding and iterative exploration heuristics.",
            "evidence_type": "supports",
            "key_findings": "A vanilla LLM-code baseline yields reasonable but fewer and less complex hypotheses than PROTEUS and achieves lower downstream validation counts, indicating the value of PROTEUS's structured exploration and tool orchestration.",
            "uuid": "e2152.4"
        },
        {
            "name_short": "POPPER",
            "name_full": "POPPER (prior work applying LLMs to validate preset hypotheses)",
            "brief_description": "A prior system (cited) that uses LLMs to analyze data in order to validate a specific, predefined hypothesis rather than perform open-ended exploration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "POPPER",
            "system_type": "LLM + data-analysis tooling",
            "domain": "bioinformatics / data-driven hypothesis validation",
            "generation_capability": "validates specific, user-specified hypotheses (not open-ended discovery)",
            "validation_method": "Guides model with a predetermined goal/hypothesis and applies analysis to attempt validation; emphasizes directed validation rather than exploratory generation.",
            "novelty_measure": "Not designed for novelty; focused on assessing pre-specified hypotheses.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Directed hypothesis validation (constrains generation to a single target) reduces exploration but simplifies validation.",
            "evidence_type": "neutral",
            "key_findings": "Cited as prior art that validates preset hypotheses via LLM-driven analysis, contrasting with PROTEUS's open-ended exploratory approach.",
            "uuid": "e2152.5"
        },
        {
            "name_short": "DREAM (autonomous research system)",
            "name_full": "DREAM: a biomedical data-driven self-evolving autonomous research system",
            "brief_description": "A referenced autonomous research system that eliminates human input but evaluates outputs primarily by whether the initial research question was resolved rather than performing deeper verification of reliability or depth.",
            "citation_title": "DREAM: a biomedical data-driven self-evolving autonomous research system",
            "mention_or_use": "mention",
            "system_name": "DREAM",
            "system_type": "autonomous research system (LLM-based / agentic)",
            "domain": "biomedical data-driven autonomous research",
            "generation_capability": "automated research outputs / decisions aimed at resolving a predefined research question",
            "validation_method": "Evaluated by whether the initial question was resolved; does not perform thorough external verification of result reliability per paper's discussion.",
            "novelty_measure": "Not emphasized; focus on automated resolution of specific research goals.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Self-evolving autonomous pipelines but evaluation limited to task resolution, highlighting potential gaps in rigorous validation.",
            "evidence_type": "mixed",
            "key_findings": "Mentioned as an autonomous system that removes human inputs but whose evaluation does not address reliability or depth, in contrast to PROTEUS's dual-evaluation approach.",
            "uuid": "e2152.6"
        },
        {
            "name_short": "Autonomous chemical research (Boiko et al.)",
            "name_full": "Autonomous chemical research with large language models",
            "brief_description": "Cited prior work demonstrating autonomous experimental optimization (chemistry reaction optimization) using LLMs and lab automation, typically in well-defined, repetitive tasks with established quantitative evaluation metrics.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_name": "Autonomous chemical research agent (Boiko et al.)",
            "system_type": "LLM + lab automation (agentic)",
            "domain": "chemistry / experimental optimization",
            "generation_capability": "experimental designs, parameter optimization, predictions of reaction outcomes",
            "validation_method": "Experimental testing (lab-in-the-loop) with quantitative optimization metrics (e.g., yield), enabling clear, repeatable evaluation.",
            "novelty_measure": "Measured by improvement in experimental metrics and optimization performance; typically less open-ended novelty than exploratory biology hypotheses.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Tight integration with experimental execution allows direct empirical validation, reducing generation-validation gap in well-defined task spaces.",
            "evidence_type": "supports",
            "key_findings": "Cited to illustrate autonomous systems succeeding in narrowly defined, quantitatively-evaluable tasks (contrast to open-ended multiomics discovery).",
            "uuid": "e2152.7"
        },
        {
            "name_short": "SpatialAgent / CellAgent / other LLM agent frameworks",
            "name_full": "SpatialAgent, CellAgent and related LLM-driven multi-agent bioinformatics frameworks (cited works)",
            "brief_description": "Recent LLM-driven agent frameworks for domain-specific automated analyses (spatial biology, single-cell analysis) that automate aspects of experimental design or data processing in specialized domains with clearer task definitions than open-ended hypothesis generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SpatialAgent / CellAgent / similar",
            "system_type": "LLM multi-agent frameworks",
            "domain": "spatial biology, single-cell omics, specialized bioinformatics",
            "generation_capability": "automated analyses, experimental design suggestions, pipeline orchestration for domain-specific workflows",
            "validation_method": "Typically validated on domain-specific tasks with established benchmarks or via lab-in-the-loop experiments; methods vary by paper.",
            "novelty_measure": "Often evaluated by task-specific performance and usefulness rather than open-ended novelty.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Domain specialization and task-constrained agent design enable clearer validation and reduce generation-validation mismatch compared to fully open-ended systems.",
            "evidence_type": "neutral",
            "key_findings": "Cited as related work in domain-specialized autonomous analyses; contrasts with PROTEUS's broader exploratory scope and dual evaluation strategy.",
            "uuid": "e2152.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "POPPER",
            "rating": 1
        },
        {
            "paper_title": "DREAM: a biomedical data-driven self-evolving autonomous research system",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "SpatialAgent: An autonomous AI agent for spatial biology",
            "rating": 2
        },
        {
            "paper_title": "CellAgent: An LLM-driven multi-agent framework for automated single-cell data analysis",
            "rating": 2
        },
        {
            "paper_title": "Automated hypothesis validation with agentic sequential falsifications",
            "rating": 2
        }
    ],
    "cost": 0.018376,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automating Exploratory Multiomics Research via Language Models</p>
<p>Shang Qu 
Tsinghua University</p>
<p>Ning Ding 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Linhai Xie 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>International Academy of Phronesis Medicine (Guangdong)</p>
<p>Yifei Li 
Tsinghua University</p>
<p>Zaoqu Liu 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Kaiyan Zhang 
Tsinghua University</p>
<p>FrontisAI</p>
<p>National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Yibai Xiong 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Yuxin Zuo 
Tsinghua University</p>
<p>Zhangren Chen 
FrontisAI</p>
<p>Ermo Hua 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Xingtai Lv 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Youbang Sun 
Tsinghua University</p>
<p>Yang Li 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Dong Li 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Fuchu He 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>International Academy of Phronesis Medicine (Guangdong)</p>
<p>Bowen Zhou 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Automating Exploratory Multiomics Research via Language Models
A6EC06CEB03EE7780627E9A9581C8AFD
This paper introduces PROTEUS , a fully automated system that produces data-driven hypotheses from raw data files.We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries.PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal.It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes.We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses.Results were evaluated through external data validation and automatic open-ended scoring.Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty.In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.and proteome), necessary for holistic investigations of complex biological processes.Through investigating rich relationships between biological molecules and clinical features, researchers can progress beyond surface-level statistical trends to arrive at deeper mechanistic insights.However, manual analysis of such high-dimensional and heterogeneous datasets are often time-consuming, and sufficient investigation of possible research directions is hard to achieve.Such challenges are amplified by biology's shift from strictly hypothesis-driven to highly open-ended data collection and analysis[6,7], especially in high-throughput omics.Scientific discovery in omics often progresses through the bidirectional interaction between data and hypotheses [8, 9]: rough hypotheses can motivate data collection, but the resulting data typically contains information along diverse directions, catalyzing discoveries that surpass initial anticipations[10].Consequently, downstream data analysis increasingly benefits from open-ended explorations exceeding a predetermined goal.This trend necessitates relevant research tools and systems to evolve in tandem.We propose that autonomous systems bridging general large language models (LLMs) and domain-specific analysis tools can enable unprecedented extents of automation in clinical multiomics research.LLMs[11][12][13][14]possess powerful instructionfollowing abilities and extensive general knowledge, which have expanded their use cases from simple language tasks to wideranging professional scenarios[15,16].Their competence and flexibility in planning complex tasks and calling diverse tools[17,18]further supports their applications in specialized domains.In bioinformatics specifically, data analysis tools are essential for discovering statistically significant trends, which in turn guide subsequent research.Accessing these tools allows LLMs to obtain insights from raw data directly, enabling more rigorous and specific scientific discovery than possible through natural language alone.Moreover, scientific discovery is not a single task but rather a multi-step, open-ended process.Regarding this challenge, the versatility of LLMs, which is their primary advantage over previous machine learning approaches, makes them well-suited for simulating a broad range of tasks involved in scientific research.Current methods using LLMs for automatic data analysis via tool-calling typically guide the model with a predefined goal or hypothesis to validate.POPPER[19], for instance, analyzes data to attempt to validate a specific hypothesis, and was applied to a b c d LLM Hypothesis</p>
<p>Introduction</p>
<p>Clinical multiomics [1][2][3] links genotype to phenotype and helps uncover fundamental biological insights.High-throughput sequencing technologies, notably recent advances in protein sequencing [4,5], have allowed researchers to simultaneously measure thousands of molecules and obtain omics datasets that contain copious biological information.Further integrating these datasets facilitates a more comprehensive understanding of different levels of biology (e.g.genome, transcriptome, the bioinformatics domain.For omics specifically, common tasks include batch effect correction [20], cell type annotation [20], and differential gene selection [21].On the other hand, most existing methods that attempt to encompass the full bioinformatics research pipeline still rely heavily on human intervention, either requiring a pre-determined procedure to link single steps [22], or relying on frequent user inputs to guide the analysis [23] [24].DREAM [25], while eliminating human inputs, evaluates the system's outputs solely by judging whether the initial research question was resolved, lacking verification of the reliability and depth of the results.Another line of work additionally employs LLMs for experimental design in chemistry [18,26] and spatial biology [27], enhancing the efficiency of lab-in-the-loop research procedures.These methods commonly focus on strictly defined task settings where quantitative evaluation methods are well-established, for instance chemical reaction optimization or gene panel design.Correspondingly, the analysis processes of LLMs within these systems are often repetitive or even follow fixed templates.Therefore, automating end-to-end exploratory research processes remains challenging.</p>
<p>Addressing limitations in existing research, we develop a fully automated PROTeogenomics Exploration and Understanding System (PROTEUS) that targets scientific analysis and discovery in clinical proteogenomics.Instead of requiring a predetermined analysis goal, PROTEUS freely explores data characteristics to autonomously pinpoint notable trends and promising research directions.In addition, the system is designed to initiate and organize complex bioinformatics tool-use, analyzing omics data along diverse directions that are pertinent to the research direction.Through iteratively deepening its inquiries, the system investigates possible biological mechanisms behind initial discoveries of surface-level trends.This exploratory and iterative discovery process of PROTEUS is its key distinguishing feature compared with previous approaches, making it more suitable for open-ended exploration of high-throughput omics datasets.</p>
<p>PROTEUS achieves this through simulating different stages of scientific research -data exploration, hypothesis proposal, hypothesis decomposition, statistical validation, and finally result integration.It takes iterative loops where preliminary results guide future analyses.All elements throughout this process, including research directions, bioinformatics tools, and analysis results, are linked to relevant biological relationships or relationship types, for instance the relationship between a protein and a clinical feature.This unified structured formulation allows PROTEUS to use biologically motivated graph structures as its scaffold when managing the complexities of bioinformatics analysis, from organizing analysis tools to keeping track of newly obtained results.These designs fully exploit the versatility of language models while tailoring the system to domainspecific characteristics of multiomics research.We present the results of PROTEUS on 10 multiomics datasets from existing publications, totaling 360 hypotheses proposed fully automatically.</p>
<p>We additionally design scalable and quantitative methods to evaluate the quality of individual hypotheses.We introduce a dual evaluation procedure that combines rigorous evidence-seeking with the ability to assess open-ended, entirely novel hypotheses.First, hypotheses were validated using external cohorts corresponding to the same cancer types from the Clinical Protein Tumor Analysis Consortium (CPTAC) [28].We develop an automatic evaluation system that obtains multiple statistical results relevant to each hypothesis from the CPTAC cohort, then assesses whether each result supports or contradicts the original hypothesis.Second, we search PubMed for related literature and, based on these references, employ LLMs to score each hypothesis along five dimensions.Results demonstrate the reliability, novelty, and general quality of PROTEUS 's hypotheses.</p>
<p>Results</p>
<p>PROTEUS Framework Design</p>
<p>PROTEUS takes as input a clinical multiomics dataset and produces data-driven hypotheses along diverse research directions without any human intervention.It automates the full process of multiomics downstream research, spanning data exploration, hypothesis proposal, hypothesis decomposition, statistical validation, and finally result integration.Throughout this pipeline, it formulates omics research as investigating the relationships between biological entities and clinical features.These designs aid PROTEUS in coordinating complex research, enabling exploratory and iterative hypothesis generation from data.</p>
<p>Omics data and bioinformatics analysis methods Integrative analysis of multiomics data is conducive towards achieving a comprehensive understanding of biological processes.The input data of PROTEUS consists of sample-level clinical feature metadata and omics data.The latter can include expression levels of proteins, phosphosites, and mRNA transcripts, as well as the existence of genomic variants.Omic subtypes, biological pathway levels, and cell type abundances can be either directly provided or computed by the system based on omics expression data.Accordingly, PROTEUS incorporates 41 bioinformatics tools to support direct statistical analysis across diverse data types, including both single and multiomic methods.We detail these methods in Section 4.2.Relationship and conclusion graphs PROTEUS keeps track of a fixed relationship graph and a dynamic conclusion graph.The relationship graph represents all supported entity types and their relationships, where each entity type (e.g.proteins, transcripts, clinical features) corresponds to sample-level data covering many specific entities (e.g.different proteins).The conclusion graph records specific entities and relationships as they are uncovered by the system's analysis.</p>
<p>Along this line, all components of PROTEUS , simulating human bioinformatics research processes, can be viewed from the lens of relationships between biological entities.In clinical multiomics specifically, research directions, hypotheses, and conclusions revolve around connections between biological molecules and clinical phenotypes, as well as connections between different molecules.Bioinformatics analysis tools can also be classified according to the type of relationship they aim to uncover.This means that the relationship and conclusion graphs can help structure each step in PROTEUS , from research direction planning to tool organization and memory management, as discussed in the following section.Importantly, the conclusion graph serves as the system's long-term record of its analysis progress, and relevant results can be conveniently extracted and provided as context for the LLM based on each step's core entities.Simulating stages of research in PROTEUS We next provide an overview of the full pipeline of PROTEUS , which consists of 5 LLM-driven modules that simulate different stages of research.To begin, two modules, the Explorer and Hypothesizer, formulate a research direction to guide further analysis.The Explorer takes the description of the raw dataset to designate two biological entity types (e.g. protein and clinic) whose relationship will be investigated.In the first iteration, this result is directly taken as a general research direction to guide initial data exploration.In each subsequent iteration, the Hypothesizer narrows down the research direction by selecting specific entities within the entity types (e.g.protein_STAT3 and clinic_survival).</p>
<p>PROTEUS Pipeline</p>
<p>Proteomics</p>
<p>Each iteration is executed via the following three modules: Decomposer, Validator, and Integrator.Given a research direction, the Decomposer references the full relationship graph to select several edges (representing individual relationships) whose analysis may contribute to the general research direction.For instance, the direction "protein_STAT3, clinic_survival", may be decomposed into "protein_STAT3-transcript" and "transcript-clinic_survival" to investigate whether STAT3 acts as a transcription factor for any gene, whose mRNA transcript levels in turn correlate with patient survival.For each edge, from the full set of bioinformatics tools, PROTEUS filters out those that support the edge's relationship type.Based on the predefined documentations of these tools describing tool functionalities and parameters, the Validator then selects an appropriate tool and automatically assigns parameters.PROTEUS additionally implements a tool retry loop in which the Validator adjusts its parameter selection using results or error messages from previous tool executions.Finally, the Integrator takes all relevant results along with the initial research direction to provide a final summary of generated hypotheses.PROTEUS next loops back to the Explorer or Hypothesizer to restart the process along a new research direction.All of the above stages are context dependent, allowing PROTEUS to deepen or diversify its analysis based on previous results instead of performing repetitive analysis.Context provided to the LLM modules includes both the previous iteration's Integrator output, representing detailed short-term information, and edges on the conclusion graph, representing long-term analysis history.Conclusion graph edges are extracted based on a combined assessment of their relevance, significance, and recency.Each module's prompts, including specific inputs, are covered in Section 4.3.In all main experiments, we use gpt-4o as the base LLM of PROTEUS .(b) Comparison of support status distributions over all hypotheses between PROTEUS and two baselines.(c) Comparison of the total number of support type validation results over all hypotheses.Each hypothesis leads to a variable number of CPTAC statistical analysis steps, thus a variable number of final support status results.(d) Frequencies of CPTAC analysis methods used during evaluation for PROTEUS .Full explanations of the analysis methods can be found at Section 4.4.(e) Frequencies that the three main omics data types were assigned during parameter selection for CPTAC analysis.(f) Visualization of the evaluation process for an individual hypothesis.The LLM calls multiple analysis functions with flexible parameter assignments, then interprets the support status of each result based on strictly defined criteria.</p>
<p>PROTEUS produces hypotheses that are verifiable in external cohorts.</p>
<p>To systematically evaluate PROTEUS , we collected 10 clinical multiomics datasets, each representing a different cancer type and including at least three of the following data types: proteomics, transcriptomics, genomic variants, phosphoproteomics, and clinical features.In addition to the raw data files, we provided the system with minimal textual data descriptions listing the types of available data and clinical features.For each dataset, PROTEUS generated 36 scientific hypotheses (3 hypotheses for each of 12 research directions), totaling 360.We provide detailed statistics of the datasets in Section 4.1, and an example data description in Appendix A.</p>
<p>Using external datasets to corroborate key analysis results is a widely adopted method in bioinformatics.It leads to more reliable and generalizable conclusions and mitigates the impact of dataset-specific biases.We mirror this method and use clinical cohort data from CPTAC to enable statistics-based verification of each individual hypothesis generated by PROTEUS .Baselines We implemented an LLM-centered baseline using gpt-4o, same as the base LLM of PROTEUS .For each dataset, the model's input was a research direction, the dataset's description, and a list of bioinformatics packages included in PRO-TEUS paired with their usage examples.The LLM interacts with the source data through generating Python code and refining the code for up to 5 iterations to fix errors.The model finally outputs a fixed number of hypotheses based on code execution results.We reuse research directions generated by PROTEUS on the same dataset to ensure the diversity of the baseline's hypotheses.The two baseline experimental settings, general and specific refer to using research directions generated by the Explorer and Hypothesizer, respectively.Prompts and design details are covered in Section 4.3.Evaluation Method 5 of the 10 total datasets had corresponding CPTAC cohorts with the same cancer type and were all used in the following evaluation.The 5 cancer types covered are: BRCA, ccRCC, GBM, LSCC, and LUAD.The evaluation pipeline consisted of three fully automatic phases.First, an LLM (gpt-4o) was given the target hypothesis and outputted a list of CPTAC data analysis methods and parameters that may uncover statistical trends pertinent to the original hypothesis.Second, the analysis methods were executed, and upon encountering errors, the LLM adjusted parameters and reran analysis.Third, for each analysis result, the model assessed to what extent it supports the hypothesis, selecting between the following: support, weak support, none, weak contradict, contradict.In these choices, "weak" refers to when a valid trend is found but does not reach statistical significance.Frequencies of these support statuses can therefore reflect the quality of PROTEUS 's hypotheses from the perspective of statistical rigor.Results Figure 2 shows quantitative evaluation results based on CPTAC data.On all datasets excluding BRCA, support and weak support comprise an evident majority (over 70%) of effective assessments.Datasets ccRCC, LCSS, and LUAD show particularly solid results, with the strongest assessment result, support, nearing 70% on all three datasets.Results for BRCA are near evenly split between supporting and contradicting assessments, indicating subpar statistical validity.A possible explanation is that the BRCA dataset [29] used is a relatively dated reanalysis of a small number of tumor samples, with inherent problems such as not having uniformly passed proteomics quality assessment [30].</p>
<p>Comparing result distributions from PROTEUS and the two baselines, the three settings produced similar distributions, which was expected since they use the same set of packages to analyze identical datasets.Directly comparing the absolute number of total results and strong supports found, PROTEUS surpassed both baselines, indicating more complex hypotheses that hinge on larger numbers of biological relationships.Despite this complexity and its inherent challenges, the system's proposed hypotheses maintained statistical rigor, with substantially more supporting results than contradicting ones overall, especially on more recent datasets.</p>
<p>PROTEUS produces reliable and novel open-ended hypotheses.</p>
<p>While verification using external cohorts has the advantage of being evidence-based, it is insufficient for evaluating open-ended hypotheses.Results produced by PROTEUS are not simply compilations of statistical results, but additionally involve flexible interpretation and reasoning to link multiple results and form higher quality final hypotheses.Therefore, we introduce a second automatic evaluation method that better accommodates open-ended results.Evaluation Method We used GPT-4o to conduct automatic hypothesis scoring according to 5 distinct metrics.For each separate hypothesis and metric, we prompted the LLM with detailed scoring criteria, the full hypothesis text, and reference information consisting of relevant PubMed articles.Prompts instructed GPT-4o to provide free-form analysis, followed by an integer score between 0 and 5.The 5 evaluation metrics were: Literature Alignment, Logical Coherence, Scientific Novelty, Biological Significance, and General Quality.These metrics are informative indicators of the plausibility, novelty, and potential for further exploration of a scientific hypothesis.Refer to Section 4.5 for full evaluation prompts.Results As shown in Fig 3 , PROTEUS surpassed both baseline settings on all 5 metrics while having lower variance.Its had the most substantial advantage on General Quality, leading by over 1 point.Notably, although we previously found the hypotheses of PROTEUS to be more complex than those of either baseline, PROTEUS still yielded the highest Logical Coherence among the three, again demonstrating balance between complexity and reliability.The relatively low absolute score along this dimension may be attributed to the large number of statistical relationships each hypothesis depended on, which led to a higher chance of logical errors.We expected the relatively low scores in Literature Alignment, since PROTEUS is expected to produce original, unreported hypotheses instead of parroting known facts.On the remaining two metrics, PROTEUS 's hypotheses consistently reached high scores.Examining the separate score distributions of each dataset, we see that on all datasets except mCRC, the majority of hypotheses scored 3 on literature alignment and coherence and 4 on novelty, significance, and general quality, demonstrating high quality as well as consistency.</p>
<p>Additional Analysis and Ablations</p>
<p>In this section, we present quantitative analysis of several key system components, explore the relationship between our two evaluation methods, and present examples of hypotheses generated by PROTEUS .Base Models In our main experiments, we used GPT-4o as the base LLM for PROTEUS .Here we compare the system's performance when using GPT-4o, GPT-4o-mini, and DeepSeek-R1 as its base model.As shown in Fig 4 , GPT-4o had a marked advantage over the other two base models on CPTAC evaluation, with higher percentages of supporting results and a larger number of total results.On LLM scoring evaluation, GPT-4o also achieved the highest General Quality, Literature Alignment, and Logical Coherence scores.Notably, DeepSeek-R1 scored slightly higher on Scientific Novelty and Biological Significance, but lowest for the other three metrics.This indicates that R1, as a reasoning model, produces novel hypotheses at the cost of result reliability, likely due to the model's heavy reliance on its own biological knowledge and reasoning.In contrast, GPT-4o arrives at more rigorous and logical hypotheses while maintaining relatively high novelty and significance scores.</p>
<p>Bioinformatics Tool-Use and Conclusion Graph Statistics</p>
<p>We next analyze how PROTEUS calls bioinformatics analysis tools to establish statistical relationships.PROTEUS was allowed at most three attempts for each tool-call.During retries, the LLM adjusted tool parameters based on previous results to fix errors and optimize any thresholds for result significance.In Fig 4(c), we see that each additional attempt increased the total number of successful tool executions substantially.After the total three attempts, each dataset had at least 50 successful tool executions, providing rich statistical results that supported further hypothesis proposal.Result success rates similarly increased monotonously for most datasets and dropped slightly at attempt three for two datasets (mCRC and HCC).This is expected since tool retries can either increase success rates through error handling or loosening significance thresholds, or decrease rates through tightening thresholds to focus analysis on fewer highly significant results.In Fig 4 (d) and (e) we display tool selection frequencies and conclusion edge type frequencies.Frequencies in both plots were averaged over the number of datasets that support each tool or edge.Tools and conclusion edges related to biological pathways appeared most frequently, possibly due to their informativeness towards both disease mechanisms and molecular functions.Generally, the diversity of tools and edge types supports PROTEUS 's multifaceted investigation of complex biological networks and mechanisms.CPTAC Evaluation and LLM Scoring are Complementary Hypothesis Evaluation Methods Figures (e), (f), and (g) explore the relationship between CPTAC evaluation and LLM scoring.We separately considered three metrics derived from CPTAC evaluation results: percentage of supporting results (both support and weak support), total number of supporting results, and total number of results.The plots show changes in LLM scoring results when using different thresholds to filter hypotheses Colors represent different entity types of the source node.(f, g, h) Changes in LLM scoring results over all datasets following preliminary hypothesis filtering based on CPTAC evaluation results.Filtering was performed based on success result rates, total success numbers, and total valid result numbers, respectively, and the plots show the influence of different filtering thresholds.</p>
<p>based on these metrics.For all three metrics, Literature Alignment showed the most steady increase alongside CPTAC-based thresholds, while other metrics did not show consistent increase.In other words, hypotheses that were highly verifiable by CPTAC data typically also had more parallels with existing biological research, while more open-ended aspects such as Scientific Novelty required more than simple statistical corroboration to assess.This further highlights the necessity for combining these two complementary evaluation methods for well-rounded hypothesis assessment.</p>
<p>Case Studies Finally, we use two representative hypotheses proposed by PROTEUS to demonstrate features of its research process.On the LSCC dataset, PROTEUS noted a jointly acting groups of four proteins (PRICKLE2, RAC2, SELL, and PECAM1) that recruits gamma delta T cells.However, concurrently, MAP2K2 variants upregulate the CDH5 protein, which positively correlates with the TRIOBP protein to form apical cytoskeleton structures that may physically block T cells from taking effect.This explains why CDH5+ tumor samples correlated with poor patient prognosis despite universal immune recruitment.PROTEUS arrived at this complex hypothesis through two separate lines of analysis.First, it pinpointed the four proteins promoting T cell activity through correlation analyses between protein and transcript expression levels.Second, it investigated poor Subtype II survival: starting from the discovery of CDH5 upregulation in MAP2K2-mutated tumors, it further uncovered a correlation between CDH5 and TRIOBP.Finally, the LLM involved its knowledge on how these two groups of proteins may impact T Cell activity, linking RAC2 and PECAM1 to leukocyte adhesion and migration, and TRIOBP to cytoskeletal formation.Both the ability of the LLM itself and the PROTEUS framework enable the system to keep track of relevant results among thousands and arrive at a cohesive final hypothesis spanning multiple omics layers.</p>
<p>On the ccRCC dataset, PROTEUS conjectured that HFM1 variants reduce PFDN6 protein levels, impairing prefoldin activity, thereby destabilizing DNA repair complexes and causing increased phosphorylation of HMGXB4.The observation that prompted this line of exploration was a correlation between HFM1 variant rates and HMGXB4 phosphorylation.PRO-TEUS then extended its analysis across transcript and protein layers, considering the relationships: HFM1 variant -transcripts / proteins, transcripts / proteins -HMGXB4 phosphorylation.Among implicated proteins, PROTEUS focused on PFDN6 in particular due to its role in prefoldin activity, which implies potential impact on DNA repair complexes.This example demonstrates how PROTEUS uses iterative planning and biologically plausible deductions to elucidate initial statistical observations with increasingly detailed biological context, gradually progressing towards mechanistic insights.We provide the two full hypotheses directly generated by PROTEUS in Appendix B.</p>
<p>Discussion</p>
<p>We introduced PROTEUS , a fully automatic system for scientific discovery from clinical multiomics data.A key challenge of achieving such extents of automation in data-driven discovery is combining the advantages of versatile LLMs and domainspecific research paradigms and analysis tools.Towards this goal, PROTEUS employs a state-of-the-art LLM as the central orchestrator of five stages in the full research pipeline: data exploration, hypothesis proposal, hypothesis decomposition, individual validation, and final result integration.To further tailor the framework to characteristics of multiomics, we introduced a static relationship graph and a dynamic conclusion graph that structures the analysis of PROTEUS throughout, from pinpointing research directions to maintaining a long-term record of complex analysis results.Combining these features, PROTEUS is well-suited for navigating high-throughput, multifaceted multiomics datasets and complex, dynamic research processes.It represents an exploratory, iterative, and domain-specific approach to automating hypothesis formulation in clinical omics.</p>
<p>We applied PROTEUS to 10 clinical multiomics datasets, each covering 3 − 6 data types, and arrived at a total of 360 biological hypotheses.We devised two complementary approaches to evaluate these statistics-based, yet diverse and openended hypotheses.First, for each hypothesis, we automatically analyzed external cohorts from CPTAC and judged whether each relevant statistical result supported the original hypothesis.This procedure assessed the statistical foundations of proposed hypotheses and effectively identified unsupported claims that stemmed from data biases or model hallucinations.Second, we used LLMs to score each hypothesis, with reference to relevant literature, along 5 metrics.This method accounts for the openended nature of scientific hypotheses and evaluates aspects such as novelty and significance that cannot be directly derived from statistical results.Combining these two approaches, we demonstrate how PROTEUS 's hypotheses balance statistical reliability and scientific novelty.Through simulating the full scientific inquiry process, PROTEUS arrives at diverse and complex results containing rich mechanistic insights, advancing towards new paradigms of bioinformatics research.</p>
<p>We identify several directions for extending and improving PROTEUS .First, the system's research scope can be expanded through incorporating additional biological entity types, relationship types, and analysis methods.Examples include other posttranslational modifications such as acetylation, copy number alterations, and metabolites.Due to the structure of PROTEUS 's research process, these extensions would not substantially complicate individual steps for the LLM.Furthermore, predefined tools are currently the only avenue for the LLM to interact with omics data.This limits the flexibility of PROTEUS , as it cannot perform data operations that exceed these existing tools.Combining predefined tools and automatic code generation is a clear approach for mitigating this problem.Finally, we currently use external information solely for result evaluation, not for the main hypothesis proposal process within PROTEUS .However, analysis insights from external data may be informative for improving hypothesis proposal and selecting directions for further analysis.Integrating external data is thus a feasible direction for improvement, whether the data is in the form of external cohorts, knowledge graphs, or existing literature.</p>
<p>We believe that PROTEUS charts a promising path towards more efficient and comprehensive research in bioinformatics.Its features of relationship-guided research, exploratory and iterative discovery, as well as scalable integration of new entity and data types, are highly generalizable to diverse realms of scientific discovery.Therefore, we anticipate that the design principles of PROTEUS will be informative for developing autonomous systems across diverse directions of scientific research.</p>
<p>Method</p>
<p>Datasets</p>
<p>We collected a total of 10 multiomics datasets which include clinical features and omics matrices, among which 5 have corresponding CPTAC cohorts focusing on the same disease.Since we emphasize proteomics-centric multiomics analysis, all datasets contain proteome expression data, along with at least one other data type, including transcriptome, phosphosite, and genomic variant data.In the table below, we provide detailed statistics for each dataset, demonstrating their high-throughput nature.</p>
<p>Bioinformatics Tools and Supported Relationship Types</p>
<p>We detail the bioinformatics tools incorporated in PROTEUS organized according to their main statistical method.The same method may be applied to different entity types, thus form distinct tools in the system.PROTEUS includes a total of (36) tools, corresponding to analysis of (18) biological relationship types.Tools typically support either analyzing general biological entity types (e.g.differential analysis over all proteins) or narrowing down to specific entities (e.g.calculating the correlation between proteins STAT1 and TP53).All tools are implemented in Python and take omics-related dataframes and a set of customizable parameters as input.</p>
<p>Consensus Clustering Clustering tissue samples based on their omics expression profiles allows researchers to identify clinically relevant molecular subtypes that facilitate further analysis.Clustering can be based on the sample-level expressions of any omics type, including proteomics, transcriptomics, phosphoproteomics, and biological pathways.PROTEUS employs consensus clustering [40] for result stability and uses silhouette scores to automatically select the optimal number of clusters.The maximum number of clusters considered is an adjustable parameter.PROTEUS first runs clustering on all available data types using fixed parameters as part of its initial data preparation phase.The model can also automatically call tools using customized parameters to redo clustering during its subsequent main analysis steps.</p>
<p>Cell Type Deconvolution The cell type deconvolution tool uses omics expression profiles to infer the cell type composition of each tissue sample.It is a necessary preparatory step for further analyses such as differential cell type abundance calculation.</p>
<p>We use the TumorDecon [41] package and include the algorithms DeconRNAseq [42] and CIBERSORT [43].PROTEUS selects the algorithm name and the omics data type to use for deconvolution (proteomics or transcriptomics).In addition to being a standalone tool, the cell type deconvolution tool is also automatically called when PROTEUS selects another tool that requires cell type data.</p>
<p>Single-Sample Enrichment Analysis Single-sample gene set enrichment analysis (ssGSEA) is a variant of classic GSEA that, instead of using features to group samples for comparison, directly compares each single sample against all others to obtain sample-level pathway signatures.Further downstream analysis, such as clustering or survival analysis, can be performed based on these pathway profiles.The ssGSEA tool is implemented based on the GSEApy [44]  Correlation Analysis We implement both Pearson and Spearman correlation using the scipy package.Tools support correlation calculation between any pair of omics expression data type provided to the system.We also include tools for correlations between omics expressions and clinical feature values or genomic variant statuses.Parameters include the analysis method name and correlation coefficient threshold for significance.</p>
<p>Survival Analysis Survival analysis directly links patient molecular profiles to clinical survival outcomes.PROTEUS incorporates survival analysis tools that analyze proteomics, transcriptomics, biological pathway, and cell type abundance data, respectively.The statistical analysis is implemented based on the lifelines [47] package.Parameters include the analysis method (discrete or continuous), column names in the clinical data corresponding to survival status and time, and the p value threshold for statistical significance.For discrete survival analysis, the tool experiments with different thresholds for classifying expression levels as "high" or "low", then uses the Kaplan-Meier method to fit the data and finally selects the threshold that yields the most significant results.For continuous, we use Cox proportional hazard analysis directly on the continuous molecule or pathway levels.</p>
<p>Protein-Protein Interaction Prediction</p>
<p>The STRING [48] database contains comprehensive information on protein-protein interaction (PPI) networks, providing insights into functional relationships between proteins.The PPI tool in PROTEUS first performs differential protein expression analysis to get subgroups of up-and down-regulated proteins, the queries the STRING API to obtain pairwise interaction scores for each subgroup.It therefore includes all parameters from differential expression analysis, with the addition of a minimum STRING interaction score for a interaction to be considered significant.</p>
<p>Transcription Factor Activity Inference Transcription factor (TF) activity analysis first performs differential expression analysis, then, based on the results, infers TF activity differences between sample groups.This tool is implemented using the decoupler [49] package, combined with the CollecTRI database for information on TF -target gene relationships.It requires basic parameters for differential analysis, as well as parameters denoting the number of top-ranking TFs and the number of top target genes for each TF to return to PROTEUS .</p>
<p>The following table provides all tools in PROTEUS organized according to the type of biological relationship they analyze, along with their adjustable parameters.The first column covers all supported relationship types in the full relationship graph provided to PROTEUS .</p>
<p>Framework Details and Prompts of PROTEUS Prompt for PROTEUS Explorer</p>
<p>You are a bioinformatics researcher analyzing multiomics data.First, you will be given a data description of the data available for you to analyze.You will then be given a relationship graph outlining the biological entities and relationships you may choose to investigate.You may be given a summary of previous analysis results for reference.You may be provided with a list of previous exploration directions that have already been investigated.You will then be given a query in natural language to use as your goal for exploration.Finally, you will be given a list of available entities to choose from.Your task is to choose a research direction, which consists of a pair of nodes in the relationship graph for further research into their relationship.You can select any pair of nodes regardless of whether they are directly connected in the relationship graph.You do not need to provide the relationship between the nodes.Ensure that the entities you choose are among those provided.As a biology researcher, you should consider the biological significance and potential novelty of your choice to select the most promising research direction.Aim to diversify your exploration directions to cover different aspects of the biological system.Both the previous exploration directions and the analysis results can help you with this.It is critical that you avoid repeating directions that have been previously explored.Review the provided exploration history and choose a different pair of nodes.Provide your output in the format of a JSON object: "source_node": "<node1>", "target_node": "<node2>".Do not output any additional words or explanations.</p>
<p>Prompt for PROTEUS Hypothesizer</p>
<p>You are a bioinformatics researcher analyzing multiomics data.First, you will be given a data description of the omics data available for you to analyze.You will then be given a description of a relationship graph, followed by a research direction consisting of a source node and a target node defined on the relationship graph.You will be investigating the relationship between these two nodes.You will also be provided with a list of previously investigated hypotheses -you should avoid repeating these exact combinations.You may also be provided with a series of conclusions related to these two nodes as context for reference.You may be given a summary of previous analysis results for reference.Finally, you may be given a query in natural language to use as your goal.Your task is to adjust and / or narrow down the research direction into a new hypothesis, where you specify specific entities within the provided entity types of the nodes.You can refer to the example values in the ontology graph description to get an idea of what the specific entities should look like.As a biology researcher, you should consider the biological significance, potential novelty, and relevance to query of your choice to select the most promising hypothesis for further validation.You must avoid generating hypotheses that have already been investigated (listed in Past Hypotheses).Considering any existing research results provided in the context, aim to diversify your hypothesis to explore different aspects of the data.To do so, you are encouraged to be daring and creative, combining your own knowledge and insights with the specific data analysis results.Provide your output in the format of a JSON object: "source_node": "<entity_type><em><specific_entity>", "target_node": "<entity_type></em><specific_entity>".You must fill in <entity_type>, each of which should strictly correspond to a node in the relationship graph.They should be the same as the research direction.<specific_entity> is optional.If and only if you have an idea of which specific entities should be further investigated, suggest specific values using your general knowledge and example values listed in the relationship graph description.Try to provide at least one specific entity.Do not output any additional words or explanations.</p>
<p>Prompt for PROTEUS Decomposer</p>
<p>You are a bioinformatics researcher analyzing multiomics data.First, you will be given a description of an relationship graph of biological entity types, followed by research objective consisting of a source node and a target node defined on the relationship graph.You will be investigating the relationship between these two nodes.The nodes may either be general entity types (e.g.protein, RNA) or be specific entities (e.g.APP protein, SOAT1 gene).You may also be provided with a series of either general relationships or specific conclusions related to these two nodes as context for reference.You may be given a summary of previous analysis results.Finally, you may be given a query in natural language to use as your goal.Your task is to decompose the research objective into a series of no more than 5 single entity relationships whose investigation would facilitate the research objective.As a biology researcher, you should consider the biological significance, potential novelty, and relevance of your choice to select the most promising entity relationships for further validation.You should use your general knowledge, reasoning, and creativity to suggest edges.You should base your selection on the provided context (if any) and consider how you can better build upon existing progress.Be selective and thoughtful on your selections.In cases where more than 5 edges are possible, choose the ones that best build upon the latest analysis to provide further insights into directions that seem promising but are underexplored.This means refraining from repeating research directions that have already been covered.You are encouraged to be daring and creative, combining your own knowledge and insights with the specific data analysis results.For instance, if the objective is (transcript, clinic), you may want to suggest edges like (transcript, protein) and (protein, clinic) to bridge the gap between "gene" and "clinic".Provide your output in the format of a JSON object, whose format is of a list, in which each term defines one edge: ["source_node": "<entity_type><em><specific_entity>", "target_node": "<entity_type></em><specific_entity>", "relationship": "<relation-ship>", ...].You must fill in <entity_type>, each of which should strictly correspond to a node in the ontology graph.<specific_entity> is optional.At the beginning of your investigation, you should only suggest general entity types, since this will help you perform general exploration of the data.After you have an idea of which specific entities should be further investigated, you may suggest specific values using your general knowledge and example values listed in the ontology graph description.Feel free to suggest the same node for the source and target if it's connected to itself in the relationship graph.You can directly include the research objective if the nodes are connected.Output no more than 5 edges.Do not output any additional words or explanations.### Relationship Graph: [Insert relationship graph description] ### General Objective: [Insert the current research objective (will be the output of Explorer for the first iteration and Hypothesizer for the remaining iterations of this exploration)] ### Context: [Insert relevant context from the current conclusion graph] ### Latest Analysis Summary: [Insert the output of the previous iteration's Integrator (when applicable)] ### Query: [Insert user query] ### Output:</p>
<p>Prompt for PROTEUS Validator</p>
<p>You are a bioinformatics researcher analyzing multiomics data.You are attempting to validate a relationship between two biological entities using bioinformatics analysis tools.First, you will be given a description of the omics data you're working with.Then, you will be given a specific conclusion to attempt to validate.It will be in the structure of a source node, a target node, and a relationship.The nodes may either be general entity types (e.g.protein, RNA) or be specific entities (e.g.APP protein, SOAT1 gene).You may additionally be given a query in natural language to use as your goal.Finally, you will be provided with a list of available tools and their corresponding tool documents, which will contain descriptions of their functionalities and parameters.Your task is to devise strategies to investigate the given relationship using existing tools.You should select a list of tools and also select their parameters.As a biology researcher, you should consider the biological significance, potential novelty, and relevance of your validation strategy to decide on the most productive path forward.For instance, if the objective is (gene, clinic), you may want to perform differential expression analysis based on gene expressions of normal and tumor samples.Provide your output in the format of a JSON object, whose format is of a list, in which each term defines your strategy for one tool: ["tool_name": "<selected_tool>", "parameters": "<name_of_parameter1>": "<value_of_parameter1>", "<name_of_parameter2>": "<value_of_parameter2>", .Please analyze the validation results and decide whether to retry with modified parameters.Consider the success rate of the latest attempt (success_count/total_results) and previous attempts.First provide a brief analysis of the results of the previous attempt.Consider whether a retry is necessary, and if so, what needs to be changed.Then provide your decision on whether to retry, and if so, what new parameters to use.When providing new parameters, ensure that you provide all necessary parameters according to the tool documentation.If not retrying, provide an empty dictionary for the parameters.Retry if and only if it is obviously necessary.If the tool was run successfully with a reasonable number of successful results, do not retry.Since we are searching for the most notable relationships, low success rates are normal, and you should not specifically seek to increase success rates.It is also normal for some tools to always have success rates of 1.0.If no successful results have been found across all attempts, always adjust parameters and retry.Output format: "analysis": <analysis>, "retry": <true/false>, "parameters": "param1": <value1>, "param2": <value2>, ...</p>
<p>Prompt for PROTEUS Integrator</p>
<p>You are an AI research assistant helping to analyze and summarize scientific validation results.You will be given previous summaries (when available), a research objective, a user query, and a list of new validation results.Your task is to provide a clear, concise summary of biological hypotheses that can be derived from the validation results.Keep your summary factual and evidence-based.Discuss both the implications regarding the research objective and any other notable directions.Strive to connect different validation results to each other or to your biological knowledge to arrive at multi-step, complex hypotheses, instead of simply focusing on interpreting single results.Be concise and specific and focus on hypotheses that are novel, promising, and most warrant further investigation.In other words, there are often a large number of reasonable hypotheses, among which you can freely select the most interesting ones, but focus on highly specific hypotheses and refrain from general overviews.You are also encouraged to focus on using your understanding of the statistical results to further propose biologically significant and novel hypotheses, instead of simply reporting the results.Previous summaries (if provided) are intended to help you focus your hypotheses on under-reported directions and refrain from repeating content already covered in previous summaries.You can use their information and build upon them, but never directly repeat hypotheses.In contrast, new validation results should be the main support for your summary.Do try to address the user query to the best of your ability.Provide your output in the format of a numbered list (e.g., 1. <hypothesis 1> 2. <hypothesis 2> ...).The list should have at most 3 hypotheses.When there are more, only include the 3 best hypotheses.Feel free to output less than 3 hypotheses if there is not enough new information to work with.When there are large amounts of new information, try to combine relevant ones to form more complex hypotheses.Feel free to ignore the ones that are not useful or relevant.</p>
<p>Automatic Evaluation via External Cohorts Evaluation Method Details and Prompts</p>
<p>We used the cptac package to access and analyze datasets in CPTAC.For each of the 5 datasets (BRCA, ccRCC, GBM, LSCC, LUAD) involved, we used the following data types: proteomics (source: bcm), transcriptomics (source: bcm), phosphoproteomics (source: umich), somatic_mutation (source: harmonized), clinical (source: mssm), follow-up (source: mssm).</p>
<p>We used GPT-4o to conduct automatic evaluation via the following three steps.First, the LLM takes the hypothesis and a description of all analysis functions, then outputs a list of functions and parameters to call that would assist hypothesis evaluation.Second, the corresponding functions are executed, and in the case of an error, it adjusts the function's parameters based on the error message.Finally, given textual results provided by each function, the LLM outputs freeform analysis of the relationship between the results and the original hypothesis, before classifying each individual result based on whether it supports the hypothesis.Detailed prompts are provided below.</p>
<p>Prompt for CPTAC Function Calling</p>
<p>You are a bioinformatics researcher analyzing cancer proteomics data from the CPTAC dataset.You will be given a hypothesis about cancer biology that needs to be validated using available CPTAC analysis tools.Your task is to select appropriate analysis functions and their parameters to test this hypothesis.</p>
<p>Provide your output as a JSON array of objects, where each object represents a function call with its parameters:</p>
<p>Choose functions that are most relevant to the hypothesis and will provide meaningful insights.Be precise with parameter values, ensuring they match the expected data types and available options.Particularly, when passing phosphosites as molecule parameters, only provide the gene name, and do not include the specific modification site.Hypothesis to validate: [Insert hypothesis to be validated] Available CPTAC analysis functions: [Insert descriptions of available analysis functions and their parameters] Output:</p>
<p>Prompt for CPTAC Function Retrying</p>
<p>You are a bioinformatics researcher troubleshooting failed function calls in cancer multiomics data analysis.You will be given a hypothesis, a function call that failed, and the error message received.Your task is to determine what went wrong with the parameters and provide corrected parameters for the same function.</p>
<p>Provide your output as a single JSON object containing only the corrected parameters: "<param1>": "<value1>", "<param2>": "<value2>", .... Do not include the function name in your output.Be precise with parameter values, ensuring they match the expected data types and available options.Most common errors include: incorrect parameter names, invalid data types, missing parameters, or specifying molecules that don't exist in the dataset.Particularly, when passing phosphosites as molecule parameters, only provide the gene name, and do not include the specific modification site.</p>
<p>Prompt for CPTAC Result Assessment</p>
<p>You are a bioinformatics researcher analyzing cancer multiomics data from the CPTAC dataset.You will be given a hypothesis and the results of several CPTAC analysis functions that were run to test this hypothesis.Your task is to analyze each result and determine whether it supports, weakly supports, contradicts, weakly contradicts, or doesn't provide significant information about the hypothesis.</p>
<p>First, provide a detailed analysis of each result, explaining your reasoning.Then, provide a structured summary as a comma-separated list of strings, where each string is one of: "support", "weak support", "contradict", "weak contradict", or "none", corresponding to each result in the order they were presented.Base your analysis on the following criteria: -"support": The result is statistically significant and the direction of effect aligns with what the hypothesis predicts -"weak support": The result is not statistically significant but the trend direction aligns with what the hypothesis predicts -"contradict": The result is statistically significant and the direction of effect is opposite to what the hypothesis predicts -"weak contradict": The result is not statistically significant but the trend direction is opposite to what the hypothesis predicts -"none": The result shows no detectable pattern, no valid results were returned, or the investigation was irrelevant to the hypothesis Statistical significance should be judged by p-values less than 0.05 and/or confidence intervals that don't include zero/one (depending on the test).Hypothesis to validate: [Insert hypothesis to be validated] Analysis Results: [Insert textual results from each analysis function called] Output:</p>
<p>CPTAC Analysis Functions We include the following analysis functions for CPTAC-based evaluation, all centered on concrete statistical analysis.Molecule Correlation Analysis calculates the Pearson correlation coefficient and p value of the correlation between the expression levels of two biological molecules.The omics types of the two molecules can be either same or different.The parameters to be set by the LLM are the two molecule names and their corresponding two omics types (proteomics, transcriptomics, or phosphoproteomics).Clinical Feature Differential Analysis performs differential expression analysis on a certain biological molecule, grouping samples based on a selected clinical feature.The LLM selects the name of the clinical feature, the name of the biological molecule, and the molecule's omics type.The function iterates through all possible ways to form two sample groups based on the provided clinical feature, and returns each grouping's results separately, including the p value and the direction of regulation.It uses the t-test for statistical analysis.Variant Status Differential Analysis performs differential expression analysis on a certain biological molecule, grouping samples based on whether a selected gene is a variant or wildtype in the sample.Parameters are the name of the gene whose variant statuses will be compared, the name of the biological molecule, and the molecule's omics type.The function uses the t-test and returns the p value and the direction of regulation.</p>
<p>Survival Analysis performs survival analysis on a certain biological molecule based on its expression values.It fits a Cox Proportional Hazards model to the data and returns the hazard ratio and p value.Parameters are the name of the biological molecule to analyze and its corresponding omics type.</p>
<p>Automatic Evaluation via LLM Scoring</p>
<p>Here we provide the full prompts we used for performing automatic scoring on each of the 5 metrics.</p>
<p>Prompt for</p>
<p>A Dataset Description Example</p>
<p>Below we provide the data description provided as input to PROTEUS for the ccRCC dataset as an example.All dataset descriptions were written by humans and follow the same format, containing minimal necessary information to outline the dataset's contents.</p>
<p>Input Description for the ccRCC Dataset</p>
<p>The files provided give information on the proteome, phosphoproteome, transcriptome, genome variants, and clinical features of clear cell renal carcinomca (ccRCC).The data was obtained from tumor and normal tissue samples.</p>
<p>Protein.csv provides protein expression levels in different tissue samples.Phospho.csvcontains phosphorylation site measurements across samples, with site identifiers in the format "S/T/YXXX_ProteinName" where S/T/Y represents the modified AA and XXX is the phosphorylation site position.</p>
<p>Transcript.csv contains RNA expression levels for genes across samples.Variant.csvindicates variant status ("Variant" or "WildType") for genes across samples.Clinic.csvincludes clinical features, with each available field and example values as follows:</p>
<p>B Case Study Hypotheses</p>
<p>Table B1 lists the original hypotheses produced by PROTEUS that were analyzed as case studies.</p>
<p>Table B1: Full hypotheses proposed by the system, corresponding to the two case studies.</p>
<p>Dataset Name Hypothesis ccRCC HFM1-driven prefoldin complex disruption impairs chaperone-mediated folding of DNA repair effectors, compensatory HMGXB4_S502 phosphorylation: HFM1 variants reduce PFDN6 protein levels, a prefoldin subunit critical for cytoskeletal protein folding.Loss of prefoldin activity may destabilize DNA repair complexes, prompting phosphorylation of HMGXB4_S502-a site near its HMG-box DNA-binding domain-to enhance chromatin structural plasticity.This hypothesis connects protein folding defects to epigenetic stress responses, with HMGXB4_S502 acting as a sensor of proteostatic imbalance.</p>
<p>LSCC PRICKLE2-SELL-RAC2 axis recruits gamma delta T cells via PECAM1-mediated transendothelial migration, but MAP2K2-mutant CDH5 upregulation spatially excludes them via TRIOBP cytoskeletal compartmentalization, driving OS disparity: PRICKLE2 correlates with SELL , which strongly associates with RAC2 and PECAM1 -proteins critical for leukocyte adhesion and endothelial transmigration.Gamma delta T cells rely on PECAM1 for tissue infiltration.However, MAP2K2-mutant tumors show extreme CDH5 upregulation , which correlates with TRIOBP to form apical cytoskeletal structures that may physically block T cell entry despite high SELL.This could explain why CDH5+ subtype II tumors exhibit worse OS despite universal PRICKLE2-SELL immune recruitment signals, as structural barriers negate gamma delta T cell antitumor activity.</p>
<p>Fig. 1 :
1
Fig. 1: (a) An overview of PROTEUS .The LLM-based system coordinates its research with biologically motivated relationship and conclusion graphs, and automatically analyzes data using bioinformatics tools.It directly takes clinical multiomics datasets and outputs a list of scientific hypotheses.(b) The separate roles of the 5 core modules of PROTEUS that simulate different research stages: Explorer, Hypothesizer, Decomposer, Validator, and Integrator.Below each module is an example of the module's output, illustrated in the context of the relationship and conclusion graphs.</p>
<p>Fig. 2 :
2
Fig. 2: (a) Distributions of CPTAC data support statuses (support, weak support, weak contradict, contradict) for hypotheses produced by PROTEUS over the 5 datasets (36 hypotheses for each dataset, multiple validation results for each hypothesis).(b)Comparison of support status distributions over all hypotheses between PROTEUS and two baselines.(c) Comparison of the total number of support type validation results over all hypotheses.Each hypothesis leads to a variable number of CPTAC statistical analysis steps, thus a variable number of final support status results.(d) Frequencies of CPTAC analysis methods used during evaluation for PROTEUS .Full explanations of the analysis methods can be found at Section 4.4.(e) Frequencies that the three main omics data types were assigned during parameter selection for CPTAC analysis.(f) Visualization of the evaluation process for an individual hypothesis.The LLM calls multiple analysis functions with flexible parameter assignments, then interprets the support status of each result based on strictly defined criteria.</p>
<p>Fig. 3 :
3
Fig. 3: (a) The pipeline of LLM automatic scoring evaluation.The LLM evaluates each hypothesis with reference to related literature, separately scoring it according to 5 metrics.(b) Average metric scores on all 360 hypotheses from 10 datasets, comparing PROTEUS and two baselines.(c) Detailed score distributions of PROTEUS 's results over all datasets combined.(d) Score distributions on each individual dataset.</p>
<p>Fig. 4 :
4
Fig. 4: (a) Comparison of CPTAC evaluation results obtained from using three different base LLMs for PROTEUS .(b) Comparison of LLM scoring results corresponding to the three different base models.(c) Comparison of tool execution result success rate and total number of successful tools after 1, 2, and 3 tool-calling attempts, respectively.(d) Average tool use frequencies per dataset.Colors represent different analysis types.(e) Average conclusion edge type frequencies per dataset.Colors represent different entity types of the source node.(f, g, h) Changes in LLM scoring results over all datasets following preliminary hypothesis filtering based on CPTAC evaluation results.Filtering was performed based on success result rates, total success numbers, and total valid result numbers, respectively, and the plots show the influence of different filtering thresholds.</p>
<h3>Data Description: [Insert pre-defined data description] ### Relationship Graph: [Insert relationship graph description] ### Latest Analysis Summary: [Insert the output of the previous iteration's Integrator (when applicable)] ### Previous Exploration Directions: [Insert a list of previous Explorer outputs (when applicable)] ### Query: [Insert user query] ### Available Entities: [Insert the full list of nodes on the relationship graph] ### Output:</h3>
<h3>Data Description: [Insert pre-defined data description] ### Relationship Graph: [Insert relationship graph description] ### Research Direction: [Insert general research direction provided by the previous Explorer] ### Context: [Insert relevant context from the current conclusion graph] ### Latest Analysis Summary: [Insert the output of the previous iteration's Integrator (when applicable)] ### Past Hypotheses (Avoid repeating these hypotheses): [Insert a list of previous Hypothesizer outputs (when applicable)] ### Query: [Insert user query] ### Output:</h3>
<p>..].You can omit parameter assignments to use the default values.You must assign a parameter if a default value is not provided.Do not output any additional words or explanations.### Data description: [Insert pre-defined data description] ### Relationship edge to validate: [Insert the current relationship edge to be validated] ### Query: [Insert the user query] ### List of available tools and descriptions: [Insert a list of descriptions of available tools that can provide relevant results for the current relationship type] ### Output: Prompt for PROTEUS Validator (Tool Retrying) ### Previous validation attempts: [Insert of list of previous validation attempts (tool parameters)] ### Latest attempt details: Tool: [Insert tool name of the latest attempt] Parameters: [Insert tool parameters selected in the latest attempt] Total Results: [Insert the total number of results returned] Successful Results: [Insert the total number of successful results returned] ### Example Results (up to 5 from latest attempt): [Insert up to 5 specific results from the latest attempt] ### Data description: [Insert pre-defined data description] ### Edge to validate: [Insert the current relationship edge to be validated] ### Full tool documentation: [Insert the description of the current tool]</p>
<h3>Previous Summaries: [Insert list of outputs from previous Integrators (when applicable)] ### Research Objective: [Insert original research objective from the Explorer or Hypothesizer] ### User Query: [Insert user query] ### Relevant New Results: [Insert relevant new results extracted from the conclusion graph] ### Output:</h3>
<p>Hypothesis to validate: [Insert hypothesis to be validated] Available CPTAC analysis functions: [Insert descriptions of available analysis functions and their parameters] Previous function call that failed: [Insert failed analysis function name] with parameters: [Insert parameters of the failed function call] Error message received: [Insert error message from the failed function call] Output:</p>
<p>2 :
2
General assessment: Score (0-5): &lt;0/1/2/3/4/5&gt; Prompt for Auto-evaluation: Biological Significance Use the provided similar PubMed articles (titles, abstracts, PMIDs) to evaluate the biological significance of the AI-generated conclusion in the context of clinical multiomics research.Assess the scope and extent to which the conclusion, if correct, would influence biological understanding and clinical practice.Consider: a) Impact on fundamental biological understanding of cellular mechanisms, pathways, or protein functions b) Potential clinical applications or translational implications for diagnostics, therapeutics, or patient stratification c) Relevance to current major challenges or knowledge gaps in the field d) Breadth of impact across multiple diseases, biological systems, or research domains e) Potential to guide future research directions or experimental designs If the provided articles are irrelevant to the conclusion, please disregard them and evaluate the significance based on your general knowledge of the field.Score on a scale of 0-5: 0: Minimal significance; conclusion addresses trivial aspects with little to no impact on biological understanding or clinical practice 1: Low significance; marginally advances understanding of specific proteins or cellular processes with limited broader implications Moderate significance; provides useful insights that modestly extend current knowledge in a specific area of clinical multiomics research 3: Notable significance; advances understanding of important biological mechanisms or offers potential clinical applications in a specific disease context 4: High significance; substantially advances understanding of critical biological processes or has clear translational potential across multiple conditions 5: Exceptional significance; fundamentally transforms understanding of major biological systems or presents breakthrough implications for clinical practice with broad applications PubMed Articles: [Insert relevant PubMed article information here] AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: Biological impact analysis: Clinical relevance: General assessment: Score (0-5): &lt;0/1/2/3/4/5&gt; Strictly adhere to the format and do not output any additional words or explanations.Prompt for Auto-evaluation: General Quality Use the provided similar PubMed articles (titles, abstracts, PMIDs) to evaluate the general quality of the following AI-generated hypothesis in the context of proteogenomics research.Assess the hypothesis based on: a) Clarity and specificity of the stated relationship between proteins/cell types and biological conditions b) Biological plausibility of the proposed mechanism or relationship c) Logical structure and internal consistency d) Potential scientific significance if validated e) Testability using current clinical multiomics methodologies Score on a scale of 0-5: 0: Poor quality; vague, implausible, or fundamentally flawed hypothesis 1: Below average; lacks specificity or contains significant logical inconsistencies 2: Average; reasonably clear but with notable weaknesses in plausibility or significance 3: Above average; clear, plausible hypothesis with moderate scientific significance 4: Good; well-formulated, highly plausible hypothesis with clear scientific significance 5: Excellent; exceptionally clear, biologically sound hypothesis with potential for high impact PubMed Articles: [Insert relevant PubMed article information here] AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: General assessment: Score (0-5): &lt;0/1/2/3/4/5&gt; Strictly adhere to the format and do not output any additional words or explanations.</p>
<p>1 .
1
<strong>Tissue type</strong>: Whether the sample is from tumor tissue or tumor-adjacent normal tissue (tumor, normal) 2. <strong>Metastases</strong>: Metastases situation of the tumor (M0, M1) 3. <strong>Status at diagnosis</strong>: Tumor status at the time of diagnosis (Localized, Advanced) 4. <strong>Stage at diagnosis</strong>: Tumor stage at the time of diagnosis as classified by the TNM system (I, II, III, IV) 5. <strong>RECIST</strong>: Tumor RECIST classification (SD, PR, PD, CR) 6. <strong>Smoking</strong>: Whether the patient smokes (Yes, No) 7. <strong>Live Status</strong>: Survival event indicator (1 = dead, 0 = alive) 8. <strong>OS</strong>: Overall survival time in months</p>
<p>Table 1 :
1
Information on the 10 clinical multiomics datasets used.
Dataset NameDiseaseCPTAC#Samples #Proteins #RNAs#Phospho #VariantsProteogenomics of clear cell renal cell carcinomaccRCCYes23112, 29118, 5786, 19919, 115response to tyrosine kinase inhibitor [31]Proteogenomics connects somatic mutations to signallingBRCAYes10613, 155-54, 601-in breast cancer [29]Proteogenomics of glioblastoma associates molecularGBMYes884, 56723, 011--patterns with survival [32]Proteogenomic landscape of squamous cell lung can-LSCCYes1098, 28119, 559-784cer [33]Proteogenomic characterization reveals tumorigenesisLUADYes14510, 255-27, 283-and progression of lung cancer manifested as subsolidnodules [34]Proteogenomic characterization of small cell lung cancerSCLCNo2259, 55917, 23526, 979136, 378identifies biological insights and subtype-specific thera-peutic strategies [35]Integrated Proteogenomic Characterization of HBV-HCCNo3196, 478-26, 41821, 158Related Hepatocellular Carcinoma [36]Proteogenomic characterization of cholangiocarci-CCANo4356, 71212, 4509, 109434noma [37]Proteogenomic characterization identifies clinically rele-iCCANo2638, 31418, 78018, 32916, 628vant subgroups of intrahepatic cholangiocarcinoma [38]Integrated Omics of Metastatic Colorectal Cancer [39]mCRCNo1476, 408-22, 00040, 462</p>
<p>[46]age and uses the KEGG_2016[45]gene set by default.PROTEUS selects the omics data type to use (proteomics or transcriptomics.Similar to deconvolution, the ssGSEA tool is called prior to any tool that requires sample-level pathway data, if this data does not already exist.Differential Expression Analysis For differential expression analysis, we use the scipy package to implement the following algorithms: t-test, ANOVA, and limma.Parameters include: algorithm name, grouping feature values, and statistical significance thresholds (maximum p value, maximum adjusted p value, and minimum logFC value).Tools all support either comparing two sample groups or comparing one group against all other samples.PROTEUS incorporates a series of differential analysis tools that differ by the omics data type analyzed (proteomics, transcriptomics, or phosphoproteomics) and the data type used for grouping samples (clinical features, genomic variant status, or molecular subtype).Grouping by genomic variant status means selecting a gene and comparing samples where the gene is mutated or unmutated.Grouping by molecular subtype means comparing samples classified as different subtypes following consensus clustering.Enrichment Analysis For enrichment analysis, we implement gene set enrichment analysis (GSEA) using the GSEApy package and the KEGG_2016 gene set.Since GSEA is based on gene sets obtained from differential expression analysis, enrichment analysis tools carry over all parameters of differential expression analysis tools.We support conducting GSEA based on either proteomics or transcriptomics data.Kinase-Substrate Enrichment Analysis Kinase-substrate enrichment analysis (KSEA) refers to first identifying sets of upand down-regulated phosphosites, then inferring the differential expression of protein kinases based on kinase-substrate relationships.For this functionality, PROTEUS directly calls the KEA3[46]API with identified gene sets.We only implement KSEA following clinical feature-based sample grouping and differential analysis, and include the same set of parameters as differential analysis.</p>
<p>Table 2 :
2
Information on the 41 bioinformatics tools and 22 relationship types supported by PROTEUS .
Relationship Tool NameParametersDescriptionprotein -ProteinInteractionmethod, min_correlation, pro-Queries the STRING database toproteinteinobtain pairwise protein-protein inter-action scores on protein subsetsContinued on next page</p>
<p>Auto-evaluation: Literature Alignment</p>
<p>Moderate support; generally aligns with literature, but some notable discrepancies or gaps.Conclusions whose specific cell types overlap with those in existing papers but exhibit notable differences should be considered to have notable gaps.4:Strong support; aligns well with multiple studies, only minor inconsistencies.Conclusions whose specific cell types are close to those in existing papers, with only minor differences, for instance in specificity, should be considered to have minor inconsistencies.Use the provided similar PubMed articles (titles, abstracts, PMIDs) to assess the logical coherence and biological plausibility of a provided AI-generated multiomics hypotheses based on fundamental principles of molecular biology, biochemistry, bioinformatics and multiomics.Evaluate: a) Consistency with known protein or cell type functions b) Adherence to established biological mechanisms and characteristics existent in the emphasized disease(s)
Prompt for Auto-evaluation: Logical Coherencec) Plausibility of proposed molecular mechanismsd) General logical coherence and consistencyScore on a scale of 0-5:Perform a comprehensive literature review using PubMed to evaluate a provided AI-generated conclusion's 0: Fundamentally flawed; violates basic principles of molecular biology or biochemistryalignment with existing multiomics research. Consider: 1: Major logical inconsistencies; proposed mechanisms highly unlikely based on current biological knowledgea) Consistency with established trends in protein or cell type abundances with respect to similar biological 2: Some logical gaps; parts of the conclusion are biologically plausible, but significant aspects arecondition comparisons questionableb) Concordance with previously reported quantitative statistical results 3: Generally sound; mostly adheres to biological principles with a few minor logical leapsc) Consistency with systems biology perspectives and further general implications in the field 4: Logically robust; aligns well with biological principles, only very minor questionable pointsScore on a scale of 0-5: 5: Exemplary logical coherence; fully adheres to all relevant biological principles and considers potential0: Contradicts well-established omics findings; multiple studies refute the conclusion complexities in omics data interpretation1: Limited support; mostly contradicts current literature with only minor points of agreement PubMed Articles: [Insert relevant PubMed article information here]2: Mixed support; some aspects align with literature but significant contradictions exist AI-generated conclusion: [Insert AI conclusion here]Provide your output in the following format: Strengths in biological reasoning: Weaknesses or questionable aspects: 3: Gaps in current literature relevant to the conclusion: Suggestions for improving biological plausibility:General assessment:Score (0-5): &lt;0/1/2/3/4/5&gt;Prompt for Auto-evaluation: Scientific NoveltyConduct a thorough PubMed search to evaluate the novelty of the AI-generated conclusion in the context ofmultiomics research. Consider:a) Identification of previously unknown disease biomarkers or immune signaturesb) Novel insights into protein functions, cell type functions, biological pathways or mechanismsc) Unique integration of multiomics data analysis results with general omics and biological knowledged) Innovative approaches to data interpretation in multiomicse) Potential for opening new avenues of research in the fieldScore on a scale of 0-5:0: Entirely unoriginal; all aspects have been extensively reported in multiple studies1: Minimal novelty; mostly reiterates known findings with only trivial new aspects2: Modest novelty; combines known concepts in a somewhat new way, but no significant new insights3: Moderate novelty; presents a fresh perspective on well-studied multiomics concepts or ideas4: High novelty; uncovers a previously unreported trend, idea, or interpretation in multiomics research5: Groundbreaking; presents an entirely new concept or approach that could significantly advance the fieldPubMed Articles: [Insert relevant PubMed article information here]AI-generated conclusion: [Insert AI conclusion here]Provide your output in the following format:Most closely related existing research (with PMIDs):Aspects that distinguish this conclusion from existing work:General assessment:Score (0-5): &lt;0/1/2/3/4/5&gt;
5: Excellent support; perfectly aligns with well-established findings across multiple studies and reviews PubMed Articles: [Insert relevant PubMed article information here] AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: Key supporting studies (with PMIDs): Key contradicting studies (if any, with PMIDs):</p>
<p>An overview of technologies for MS-based proteomics-centric multi-omics. A T Rajczewski, P D Jagtap, T J Griffin, 10.1080/14789450.2022.20704762025-02-0819</p>
<p>C Chen, J Wang, D Pan, X Wang, Y Xu, J Yan, L Wang, X Yang, M Yang, G.-P Liu, 10.1002/mco2.3152025-05-01Applications of multi-omics analysis in human diseases. 4</p>
<p>M Krassowski, V Das, S K Sahu, B B Misra, 10.3389/fgene.2020.6107982025-05-01State of the field in multi-omics research: From computational needs to data mining and sharing 11. </p>
<p>High-throughput proteomics: a methodological mini-review. M Cui, C Cheng, L Zhang, 10.1038/s41374-022-00830-72025-05-01Nature Publishing Group102</p>
<p>Mass spectrometry-based high-throughput proteomics and its role in biomedical studies and systems biology. C B Messner, V Demichev, Z Wang, J Hartl, G Kustatscher, M Mülleder, M Ralser, 10.1002/pmic.2022000132025-05-0123</p>
<p>A hypothesis is a liability. I Yanai, M Lercher, 10.1186/s13059-020-02133-w21</p>
<p>Big data biology: Between eliminative inferences and exploratory experiments. E Ratti, 10.1086/6803322025-05-0182</p>
<p>The data-hypothesis conversation. I Yanai, M Lercher, 10.1186/s13059-021-02277-322</p>
<p>T Felin, J Koenderink, J I Krueger, D Noble, G F R Ellis, 10.1186/s13059-021-02276-42025-05-01The data-hypothesis relationship. 22</p>
<p>Could big data be the end of theory in science?. F Mazzocchi, 10.15252/embr.2015410012025-05-01</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Pre-trained models: Past, present and future. X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu, Y Yao, A Zhang, L Zhang, AI Open. 22021</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>K Saab, T Tu, W.-H Weng, R Tanno, D Stutz, E Wulczyn, F Zhang, T Strother, C Park, E Vedadi, arXiv:2404.18416Capabilities of gemini models in medicine. 2024arXiv preprint</p>
<p>K Zhang, S Zeng, E Hua, N Ding, Z.-R Chen, Z Ma, H Li, G Cui, B Qi, X Zhu, arXiv:2406.03949Ultramedical: Building specialized generalists in biomedicine. 2024arXiv preprint</p>
<p>Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, Tool Learning with Foundation Models. </p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O Baldassari, C White, A D Schwaller, P , Nature Machine Intelligence. </p>
<p>Automated hypothesis validation with agentic sequential falsifications. K Huang, Y Jin, R Li, M Y Li, E Candès, J Leskovec, arXiv:2502.098582025arXiv preprint</p>
<p>CellAgent: An LLM-driven multi-agent framework for automated single-cell data analysis. Y Xiao, J Liu, Y Zheng, X Xie, J Hao, M Li, R Wang, F Ni, Y Li, J Luo, S Jiao, J Peng, bioRxiv 2024.05.13.5938612024</p>
<p>. J Zhou, B Zhang, X Chen, H Li, X Xu, S Chen, W He, C Xu, An AI Agent for Fully Automated. </p>
<p>A data-intelligence-intensive bioinformatics copilot system for large-scale omics researches and scientific insights. Y Liu, R Shen, L Zhou, Q Xiao, J Yuan, Y Li, bioRxiv 2024.05.19.5948952024</p>
<p>BioInformatics agent (BIA): Unleashing the power of large language models to reshape bioinformatics workflow. Q Xin, Q Kong, H Ji, bioRxiv 2024.05.22.5952402024</p>
<p>scChat: A Large Language Model-Powered Co-Pilot for Contextualized Single. Y.-C Lu, A Varghese, R Nahar, H Chen, K Shao, X Bao, C Li, 10.1101/2024.10.01.616063Cell RNA Sequencing Analysis. 2024</p>
<p>DREAM: a biomedical data-driven self-evolving autonomous research system. L Deng, Y Wu, Y Ren, H Lu, arXiv:2407.136372024arXiv preprint</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0624</p>
<p>SpatialAgent: An autonomous AI agent for spatial biology. H Wang, Y He, P P Coelho, M Bucci, A Nazir, B Chen, L Trinh, S Zhang, K Huang, V Chandrasekar, D C Chung, M Hao, A C Leote, Y Lee, B Li, T Liu, J Liu, R Lopez, T Lucas, M Ma, N Makarov, L Mcginnis, L Peng, S Ra, G Scalia, A Singh, L Tao, M Uehara, C Wang, R Wei, R Copping, O Rozenblatt-Rosen, J Leskovec, A Regev, 10.1101/2025.04.03.646459v1Section: New Results. </p>
<p>N J Edwards, M Oberti, R R Thangudu, S Cai, P B Mcgarvey, S Jacob, S Madhavan, K A Ketchum, 10.1021/pr501254j2025-05-02The CPTAC data portal: A resource for cancer proteomics research. American Chemical Society14</p>
<p>P Mertins, D R Mani, K V Ruggles, M A Gillette, K R Clauser, P Wang, X Wang, J W Qiao, S Cao, F Petralia, E Kawaler, F Mundt, K Krug, Z Tu, J T Lei, M L Gatza, M Wilkerson, C M Perou, V Yellapantula, K.-L Huang, C Lin, M D Mclellan, P Yan, S R Davies, R R Townsend, S J Skates, J Wang, B Zhang, C R Kinsinger, M Mesri, H Rodriguez, L Ding, A G Paulovich, D Fenyö, M J Ellis, S A Carr, 10.1038/nature18003Proteogenomics connects somatic mutations to signalling in breast cancer. Nature Publishing Group534</p>
<p>K Krug, E J Jaehnig, S Satpathy, L Blumenberg, A Karpova, M Anurag, G Miles, P Mertins, Y Geffen, L C Tang, D I Heiman, S Cao, Y E Maruvka, J T Lei, C Huang, R B Kothadia, A Colaprico, C Birger, J Wang, Y Dou, B Wen, Z Shi, Y Liao, M Wiznerowicz, M A Wyczalkowski, X S Chen, J J Kennedy, A G Paulovich, M Thiagarajan, C R Kinsinger, T Hiltke, E S Boja, M Mesri, A I Robles, H Rodriguez, T F Westbrook, L Ding, G Getz, K R Clauser, D Fenyö, K V Ruggles, B Zhang, D R Mani, S A Carr, M J Ellis, M A Gillette, S C Avanessian, S Cai, D Chan, X Chen, N J Edwards, A N Hoofnagle, M H Kane, K A Ketchum, E Kuhn, D A Levine, S Li, D C Liebler, T Liu, J Luo, S Madhavan, C Maher, J E Mcdermott, P B Mcgarvey, M Oberti, A Pandey, S H Payne, D F Ransohoff, R C Rivers, K D Rodland, P Rudnick, M E Sanders, K M Shaw, I.-M Shih, R J C Slebos, R D Smith, M Snyder, S E Stein, D L Tabb, R R Thangudu, S Thomas, Y Wang, F M White, J R Whiteaker, G A Whiteley, H Zhang, Z Zhang, Y Zhao, H Zhu, L J Zimmerman, 10.1016/j.cell.2020.10.036Proteogenomic landscape of breast cancer tumorigenesis and targeted therapy. 183</p>
<p>Proteogenomics of clear cell renal cell carcinoma response to tyrosine kinase inhibitor. H Zhang, L Bai, X.-Q Wu, X Tian, J Feng, X Wu, G.-H Shi, X Pei, J Lyu, G Yang, Y Liu, W Xu, A Anwaier, Y Zhu, D.-L Cao, F Xu, Y Wang, H.-L Gan, M.-H Sun, J.-Y Zhao, Y Qu, D Ye, C Ding, 10.1038/s41467-023-39981-62025-03-2014</p>
<p>G Yanovich-Arad, P Ofek, E Yeini, M Mardamshina, A Danilevsky, N Shomron, R Grossman, R Satchi-Fainaro, T Geiger, 10.1016/j.celrep.2021.108787Proteogenomics of glioblastoma associates molecular patterns with survival. 2025-03-2034</p>
<p>. P A Stewart, E A Welsh, R J C Slebos, B Fang, V Izumi, M Chambers, G Zhang, L Cen, F Pettersson, Y Zhang, Z Chen, C.-H Cheng, R Thapa, Z Thompson, K M Fellows, J M Francis, J J Saller, T Mesa, C Zhang, S Yoder, G M Denicola, A A Beg, T A Boyle, J K Teer, Ann Chen, Y Koomen, J M Eschrich, S A Haura, E B , 10.1038/s41467-019-11452-xProteogenomic landscape of squamous cell lung cancer. 101Nature Publishing Group</p>
<p>Proteogenomic characterization reveals tumorigenesis and progression of lung cancer manifested as subsolid nodules. H Su, L Chen, J Wu, Z Cheng, J Li, Y Ren, J Xu, Y Dang, M Zheng, Y Cao, J Gao, C Dai, X Hu, H Xie, J Chen, T Luo, J Zhu, C Wu, W Sha, C Chen, H Liu, 10.1038/s41467-025-57364-x2025-03-2016</p>
<p>Proteogenomic characterization of small cell lung cancer identifies biological insights and subtype-specific therapeutic strategies. Q Liu, J Zhang, C Guo, M Wang, C Wang, Y Yan, L Sun, D Wang, L Zhang, H Yu, L Hou, C Wu, Y Zhu, G Jiang, H Zhu, Y Zhou, S Fang, T Zhang, L Hu, J Li, Y Liu, H Zhang, B Zhang, L Ding, A I Robles, H Rodriguez, D Gao, H Ji, H Zhou, P Zhang, 10.1016/j.cell.2023.12.0042025-03-20187</p>
<p>Integrated proteogenomic characterization of HBV-related hepatocellular carcinoma. Q Gao, H Zhu, L Dong, W Shi, R Chen, Z Song, C Huang, J Li, X Dong, Y Zhou, Q Liu, L Ma, X Wang, J Zhou, Y Liu, E Boja, A I Robles, W Ma, P Wang, Y Li, L Ding, B Wen, B Zhang, H Rodriguez, D Gao, H Zhou, J Fan, 10.1016/j.cell.2019.08.0522025-03-20179</p>
<p>M Deng, P Ran, L Chen, Y Wang, Z Yu, K Cai, J Feng, Z Qin, Y Yin, S Tan, Y Liu, C Xu, G Shi, Y Ji, J Zhao, J Zhou, J Fan, Y Hou, C Ding, 10.1002/hep.32624Proteogenomic characterization of cholangiocarcinoma. 77</p>
<p>Proteogenomic characterization identifies clinically relevant subgroups of intrahepatic cholangiocarcinoma. L Dong, D Lu, R Chen, Y Lin, H Zhu, Z Zhang, S Cai, P Cui, G Song, D Rao, X Yi, Y Wu, N Song, F Liu, Y Zou, S Zhang, X Zhang, X Wang, S Qiu, J Zhou, S Wang, X Zhang, Y Shi, D Figeys, L Ding, P Wang, B Zhang, H Rodriguez, Q Gao, D Gao, H Zhou, J Fan, 10.1016/j.ccell.2021.12.0062025-03-2040</p>
<p>. C Li, Y.-D Sun, G.-Y Yu, J.-R Cui, Z Lou, H Zhang, Y Huang, C.-G Bai, L.-L Deng, P Liu, K Zheng, Y.-H Wang, Q.-Q Wang, Q.-R Li, Q.-Q Wu, Q Liu, Y Shyr, Y.-X Li, L.-N Chen, J.-R Wu, W Zhang, R Zeng, 10.1016/j.ccell.2020.08.002Integrated omics of metastatic colorectal cancer. 3852025-03-20</p>
<p>Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data. S Monti, </p>
<p>R A Aronow, S Akbarinejad, T Le, S Su, L Shahriyari, 10.1016/j.softx.2022.1010722025-05-02TumorDecon: A digital cytometry software 18. Elsevier101072</p>
<p>DeconRNASeq: a statistical framework for deconvolution of heterogeneous tissue samples based on mRNA-seq data. T Gong, J D Szustakowski, 10.1093/bioinformatics/btt0902025-05-02Publisher: Oxford Academic. 298</p>
<p>Robust enumeration of cell subsets from tissue expression profiles. A M Newman, C L Liu, M R Green, A J Gentles, W Feng, Y Xu, C D Hoang, M Diehn, A A Alizadeh, 10.1038/nmeth.33372025-05-02Nature Publishing Group12</p>
<p>GSEApy: a comprehensive package for performing gene set enrichment analysis in python. Z Fang, X Liu, G Peltz, 10.1093/bioinformatics/btac7572025-05-02Publisher: Oxford Academic. 391</p>
<p>KEGG: Kyoto encyclopedia of genes and genomes. H Ogata, S Goto, K Sato, W Fujibuchi, H Bono, M Kanehisa, 10.1093/nar/27.1.292025-05-02Oxford Academic27</p>
<p>KEA3: improved kinase enrichment analysis via data integration. M V Kuleshov, Z Xie, A B K London, J Yang, J E Evangelista, A Lachmann, I Shu, D Torre, A Ma'ayan, 10.1093/nar/gkab3592025-05-0249</p>
<p>C Davidson-Pilon, 10.5281/zenodo.12549337Lifelines, Survival Analysis in Python. </p>
<p>The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest 51. D Szklarczyk, R Kirsch, M Koutrouli, K Nastou, F Mehryary, R Hachilif, A L Gable, T Fang, N T Doncheva, S Pyysalo, P Bork, L J Jensen, C Mering, 10.1093/nar/gkac10002025-05-02Publisher: Oxford Academic. </p>
<p>P Badia-I-Mompel, J Vélez Santiago, J Braunger, C Geiss, D Dimitrov, S Müller-Dott, P Taus, A Dugourd, C H Holland, R O Ramirez Flores, J Saez-Rodriguez, 10.1093/bioadv/vbac0162025-05-02decoupleR: ensemble of computational methods to infer biological activities from omics data. 2</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7607 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7607</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7607</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-0ff8c04c8bdbf93b39b49582c9195cf3fc894d03</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0ff8c04c8bdbf93b39b49582c9195cf3fc894d03" target="_blank">Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Software Engineering</p>
                <p><strong>Paper TL;DR:</strong> It is found that adding semantic facts to the code in the prompt actually does help and this approach improves performance in several different settings suggested by prior work, including for three different Large Language Models.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLM) are a new class of computation engines, “programmed” via prompt engineering. Researchers are still learning how to best “program” these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of “code analysis” and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly. actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU 11Scores of 30–40 BLEU are considered “Good” to “Understandable” for natural language translation; see https://cloud.google.com/translate/automl/docs/evaluate.. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7607.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7607.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASAP (prompt augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Semantic Augmentation of Prompts (ASAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering method that augments few-shot exemplars and the target input with automatically extracted semantic facts (repository info, tagged identifiers, data-flow graph) presented as separate labeled fields in the prompt to improve LLM performance on code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only autoregressive large language model (Codex variant) pre-trained on large corpora of code and natural language; used in a few-shot completion setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization (CodeSearchNet / CodeXGLUE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a natural-language function/method summary (docstring/comment) given a function's source code.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt where each exemplar is: [function source code || labeled analysis products (repo info, tagged identifiers, DFG) || gold comment], followed by the target function and its analysis product.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three-shot few-shot prompting (3 exemplars) chosen by BM25 retrieval; semantic facts embedded as separately labeled fields; temperature set to 0; ~4k token window; exemplars and target include code + analysis products; final prompt: e1 || e2 || e3 || target_code || target_analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-CN primary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>25.32 BLEU (overall, ASAP aggregated across 6 languages)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>22.66 BLEU (vanilla BM25 few-shot baseline, overall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+2.66 BLEU absolute (+11.74% relative)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002 via OpenAI API; 1000 random test samples per language (6 languages aggregated = 6000); temperature=0; context window ~4k tokens; 3 BM25-picked exemplars by default; BLEU-CN smoothing used.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>p < 0.01 (one-sided pairwise Wilcoxon signed-rank test, B-H corrected)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7607.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Repository-info augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding repository name and path to prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including repository-level metadata (repo name and file path tokens) as a labeled field in each exemplar and the target, to provide domain context that helps the model generate more accurate summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Codex variant used in few-shot completion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate function summaries from code, with added repository-level context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt with BM25-selected exemplars; each exemplar includes a labeled 'Repo info' field (repo name and path) alongside code and comment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt content / prompt field</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>BM25 retrieval for exemplars; repo info inserted as a distinct labeled block; experiments compare BM25 baseline vs BM25+repo vs other single-component augmentations; 3-shot default; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-CN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BM25+repo: 25.12 BLEU (overall across languages, Table 3 'BM25+repo' aggregated shown as 25.12)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>BM25: 22.66 BLEU (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+2.46 BLEU absolute (+10.86% relative overall); example per-language: Java 22.87 → 25.23 (+2.36 BLEU), Python 21.78 → 24.22 (+2.44 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002, 3-shot BM25 exemplars; repo info presented verbatim or tokenized (they also tested tokenized path to check memorization risk); temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>p < 0.01 reported for full ASAP vs BM25; component-level per-language improvements reported in Table 3 (Wilcoxon test used for comparisons against BM25).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7607.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tagged-identifiers augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding tagged identifiers (identifier roles) to prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting prompts with a list of identifiers extracted from the AST and explicitly tagging them with roles (function name, parameters, local identifiers, globals) to help the model focus on semantically important tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Codex variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarize functions; experiments compare vanilla BM25 vs BM25 augmented with tagged identifier lists.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt with a labeled 'Tagged Identifiers' field per exemplar and target; identifiers extracted using tree-sitter AST traversal and labeled.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt content / prompt field</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot BM25 exemplars; explicit role labels on identifiers; compared to an untagged list of identifiers (ablation showed tagged > untagged by 0.41 BLEU Java, 1.22 BLEU Python).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-CN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BM25+id: Java 23.39 BLEU; Python 22.54 BLEU (from Table 3 component columns)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>BM25: Java 22.87 BLEU; Python 21.78 BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Java: +0.52 BLEU absolute (+2.28%); Python: +0.76 BLEU absolute (+3.49%); tagged identifiers performed better than a plain tag-free list (improvement 0.41 BLEU Java, 1.22 BLEU Python).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002; tree-sitter AST used for extraction; 3-shot BM25 selection; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Component-level differences reported, overall ASAP significance p < 0.01 (Wilcoxon); specific small-component p-values not always individually reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7607.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFG augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding Data-Flow Graph (DFG) information to prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including a representation of the function's data-flow graph (identifier indices and flow edges) as a labeled field in exemplars and target to provide semantic relationships between identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Codex variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarize functions using prompts augmented with data-flow information.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt where each exemplar includes a labeled 'DFG' block; long DFGs are truncated to first 30 lines to fit in prompt window.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt content / prompt field</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot BM25 exemplars; DFG lines presented as identifier indices and edges; truncated for long graphs; compared BM25 vs BM25+DFG vs other components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-CN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BM25+DFG: Java 23.13 BLEU; Python 21.82 BLEU (Table 3 component columns)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>BM25: Java 22.87; Python 21.78</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Java: +0.26 BLEU (+1.14%); Python: +0.04 BLEU (+0.18%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002; DFG computed and truncated to first 30 lines when long; 3-shot; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>DFG alone gives modest gains; overall ASAP combining components showed p < 0.01 vs BM25. Individual DFG-only p-values not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7607.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought / step-by-step prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought / step-by-step style prompting (Kojima-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Formatting prompts to encourage the model to produce intermediate reasoning steps (lemmas) before giving the final summary; authors tested both few-shot versions containing intermediate steps and zero-shot 'let's think step by step' variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (experimented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Codex variant; tested with chain-of-thought style prompts that included intermediate steps in exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce natural-language summary; chain-of-thought formatting appends or interleaves intermediate 'lemmas' before final output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot and zero-shot chain-of-thought prompts: exemplars (or zero-shot instruction) ask model to produce intermediate steps and then a final summary.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (reasoning elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot exemplars included intermediate steps and final comments; zero-shot used 'let's think step by step' style instruction; intention was to induce intermediate lemmas generated by the model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-CN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Chain-of-thought (few-shot enhanced): ≈20.25 BLEU (reported), Chain-of-thought zero-shot performed worse (value not specified but 'even worse').</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla BM25 few-shot baseline: 24.97 BLEU (specific baseline quoted adjacent to chain-of-thought result in text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>≈ -4.72 BLEU absolute (few-shot chain-of-thought ≈20.25 vs baseline ≈24.97), i.e., worse performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Model prompted to generate intermediate 'lemmas' in exemplars or prompted zero-shot with 'step-by-step' instruction; attempted to frame problem as chain-of-thought; temperature unspecified here but main experiments used temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7607.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 retrieval for exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25-based retrieval for few-shot exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using BM25 information-retrieval to select semantically-relevant few-shot exemplars (input-output pairs) from the pool rather than using random exemplars, which improves performance of few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (few-shot experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex variant used in few-shot completion; BM25 used as retrieval engine external to the model to pick exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate function summaries using few-shot exemplars selected either randomly or via BM25 retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt where exemplar selection is either random or BM25-retrieved; exemplars are plain input-output pairs (no ASAP augmentation for baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Compared 'Few-shot (random)' vs 'Few-shot with BM25'. Typically 3 exemplars; BM25 queries constructed from the target function code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (BLEU-CN reported elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot with BM25: Java 22.87 BLEU; Python 21.78 BLEU (Table 2 values under 'Few-shot with BM25').</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Few-shot (random): Java 19.87 BLEU; Python 20.66 BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Java: +3.00 BLEU absolute (+15.10% relative); Python: +1.12 BLEU (+5.42% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>BM25 retrieval over the training pool to pick top-3 exemplars most similar to the target; models evaluated on 1000 samples per language (subset); temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7607.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shots vs ASAP (context budget)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt shot-count trade-offs (more exemplars vs semantic augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing that using more vanilla exemplars (4 or 5 shots) without semantic augmentation does not outperform the ASAP approach using fewer (3) augmented shots, indicating that prompt content/format matters more than sheer exemplar count within the model's context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex variant tested with varying numbers of exemplars in prompt (3, 4, 5) with/without ASAP augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare BLEU between (a) ASAP with 3 augmented shots and (b) vanilla BM25 with 3/4/5 shots.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts: varying number of exemplars (3,4,5); ASAP uses 3 augmented exemplars (analysis products included); vanilla baseline uses 3/4/5 plain exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / exemplar count</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>3-shot ASAP vs BM25 with 3,4,5 shots; model context window ~4k tokens limits number/size of shots; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASAP (3 shots): Java 25.41 BLEU; Python 24.26 BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla BM25: Java 3-shot 22.87 / 4-shot 23.13 / 5-shot 23.20; Python 3-shot 21.78 / 4-shot 21.89 / 5-shot 21.74</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>ASAP(3-shot) vs BM25(5-shot) Java: 25.41 vs 23.20 = +2.21 BLEU absolute; Python: 24.26 vs 21.74 = +2.52 BLEU absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002; experiments run with differing numbers of BM25 exemplars; ASAP uses 3 augmented exemplars; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7607.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-type sensitivity (completion vs chat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASAP effect across model families (completion models vs chat models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ASAP-augmented prompts yield stronger relative improvements on completion-style models (code-davinci-002, text-davinci-003) than on the chat-optimized gpt-3.5-turbo, which produced less stylistically-aligned comments and smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002, text-davinci-003, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three OpenAI models tested: code-davinci-002 (Codex, code-focused completion), text-davinci-003 (completion), gpt-3.5-turbo (chat-optimized model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (for Davinci/Codex variants and GPT-3.5 family approximated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate function summaries with ASAP-augmented prompts and compare effects across different LLM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot (BM25) exemplars augmented with ASAP facts; same prompt structure fed to different model endpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model family / prompt compatibility</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>500-sample evaluations for Java, Python, PHP; temperature=0; identical ASAP prompt used across models; measured BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: Java: code-davinci-002 BM25→ASAP 23.90→25.78 (+7.87%); text-davinci-003 18.98→22.31 (+17.54%); gpt-3.5-turbo 16.68→16.96 (+1.68%, p=0.95). Python: code-davinci-002 22.00→24.78 (+12.64%); text-davinci-003 16.74→18.93 (+13.08%); gpt-3.5-turbo 15.01→16.38 (+9.13%). PHP similar pattern with larger gains for completion models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Model-specific BM25 values as in Table 6 (e.g., code-davinci-002 Java 23.90 BLEU; text-davinci-003 Java 18.98 BLEU; turbo Java 16.68 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Varies by model and language; improvements larger and statistically significant (p < 0.01) for completion models; small and not significant for some turbo comparisons (e.g., Java turbo p=0.95).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ASAP prompts applied unchanged to each model; 500 samples per language for these cross-model tests; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>p < 0.01 for most comparisons (except Java on gpt-3.5-turbo where p = 0.95 as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7607.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7607.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot ASAP for line completion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASAP augmentation applied in zero-shot line-completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending semantic facts (repo info, tagged identifiers, DFG derived only from preceding lines) in a zero-shot prompt improved line-completion Exact Match and Edit Similarity for Java and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex variant used in zero-shot completion experiments (no exemplars shown).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Line completion (code completion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given preceding lines of a function, generate a target (selected) line. ASAP appends semantic facts to the prefix in zero-shot fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot completion where prompt = [Repo info || tagged identifiers || DFG (from preceding lines) || preceding lines], model must generate the next/target line.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Randomly selected target line per sample; semantic facts computed only from preceding lines to avoid leakage; metrics: Exact Match (EM) and Edit Similarity (ES); temperature presumably 0 as elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and Edit Similarity (ES)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Overall (combined Java+Python): EM 19.01% (ASAP) and ES 55.72% (ASAP). Java: EM 22.12% (ASAP), ES 59.66%; Python: EM 14.58%, ES 50.12%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Overall baseline EM 17.97% and ES 53.01%. Java baseline EM 20.75% / ES 55.35%; Python baseline EM 14.05% / ES 49.71%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Overall EM +1.04 percentage points (+5.79% relative); overall ES +2.71 percentage points (+5.11% relative). Java EM +1.37 pp (+6.6%), ES +4.31 pp (+7.79%). Python EM +0.53 pp (+3.77%, p=0.13 not significant), ES +0.41 pp (+0.82%, p<0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>code-davinci-002; Java N=9292, Python N=6550 samples; random target line selection; semantic facts appended; tests used McNemar for EM and Wilcoxon for ES; some statistical tests reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall: p < 0.01 for EM and ES; Java: p < 0.01 both EM and ES; Python: EM p = 0.13 (not significant), ES p < 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Repository-level prompt generation for large language models of code <em>(Rating: 2)</em></li>
                <li>Few-shot training LLMs for project-specific code-summarization <em>(Rating: 2)</em></li>
                <li>ChatGPT: Jack of all trades, master of none <em>(Rating: 1)</em></li>
                <li>GraphCodeBERT: Pre-training Code Representations with Data Flow <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7607",
    "paper_id": "paper-0ff8c04c8bdbf93b39b49582c9195cf3fc894d03",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "ASAP (prompt augmentation)",
            "name_full": "Automatic Semantic Augmentation of Prompts (ASAP)",
            "brief_description": "A prompt-engineering method that augments few-shot exemplars and the target input with automatically extracted semantic facts (repository info, tagged identifiers, data-flow graph) presented as separate labeled fields in the prompt to improve LLM performance on code tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Decoder-only autoregressive large language model (Codex variant) pre-trained on large corpora of code and natural language; used in a few-shot completion setup.",
            "model_size": "≈175B",
            "task_name": "Code summarization (CodeSearchNet / CodeXGLUE)",
            "task_description": "Generate a natural-language function/method summary (docstring/comment) given a function's source code.",
            "problem_format": "Few-shot prompt where each exemplar is: [function source code || labeled analysis products (repo info, tagged identifiers, DFG) || gold comment], followed by the target function and its analysis product.",
            "format_category": "prompt style / input modality",
            "format_details": "Three-shot few-shot prompting (3 exemplars) chosen by BM25 retrieval; semantic facts embedded as separately labeled fields; temperature set to 0; ~4k token window; exemplars and target include code + analysis products; final prompt: e1 || e2 || e3 || target_code || target_analysis.",
            "performance_metric": "BLEU (BLEU-CN primary)",
            "performance_value": "25.32 BLEU (overall, ASAP aggregated across 6 languages)",
            "baseline_performance": "22.66 BLEU (vanilla BM25 few-shot baseline, overall)",
            "performance_change": "+2.66 BLEU absolute (+11.74% relative)",
            "experimental_setting": "code-davinci-002 via OpenAI API; 1000 random test samples per language (6 languages aggregated = 6000); temperature=0; context window ~4k tokens; 3 BM25-picked exemplars by default; BLEU-CN smoothing used.",
            "statistical_significance": "p &lt; 0.01 (one-sided pairwise Wilcoxon signed-rank test, B-H corrected)",
            "uuid": "e7607.0",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Repository-info augmentation",
            "name_full": "Adding repository name and path to prompt",
            "brief_description": "Including repository-level metadata (repo name and file path tokens) as a labeled field in each exemplar and the target, to provide domain context that helps the model generate more accurate summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Decoder-only Codex variant used in few-shot completion.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Generate function summaries from code, with added repository-level context.",
            "problem_format": "Few-shot prompt with BM25-selected exemplars; each exemplar includes a labeled 'Repo info' field (repo name and path) alongside code and comment.",
            "format_category": "prompt content / prompt field",
            "format_details": "BM25 retrieval for exemplars; repo info inserted as a distinct labeled block; experiments compare BM25 baseline vs BM25+repo vs other single-component augmentations; 3-shot default; temperature=0.",
            "performance_metric": "BLEU (BLEU-CN)",
            "performance_value": "BM25+repo: 25.12 BLEU (overall across languages, Table 3 'BM25+repo' aggregated shown as 25.12)",
            "baseline_performance": "BM25: 22.66 BLEU (overall)",
            "performance_change": "+2.46 BLEU absolute (+10.86% relative overall); example per-language: Java 22.87 → 25.23 (+2.36 BLEU), Python 21.78 → 24.22 (+2.44 BLEU).",
            "experimental_setting": "code-davinci-002, 3-shot BM25 exemplars; repo info presented verbatim or tokenized (they also tested tokenized path to check memorization risk); temperature=0.",
            "statistical_significance": "p &lt; 0.01 reported for full ASAP vs BM25; component-level per-language improvements reported in Table 3 (Wilcoxon test used for comparisons against BM25).",
            "uuid": "e7607.1",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Tagged-identifiers augmentation",
            "name_full": "Adding tagged identifiers (identifier roles) to prompt",
            "brief_description": "Augmenting prompts with a list of identifiers extracted from the AST and explicitly tagging them with roles (function name, parameters, local identifiers, globals) to help the model focus on semantically important tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Decoder-only Codex variant.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Summarize functions; experiments compare vanilla BM25 vs BM25 augmented with tagged identifier lists.",
            "problem_format": "Few-shot prompt with a labeled 'Tagged Identifiers' field per exemplar and target; identifiers extracted using tree-sitter AST traversal and labeled.",
            "format_category": "prompt content / prompt field",
            "format_details": "3-shot BM25 exemplars; explicit role labels on identifiers; compared to an untagged list of identifiers (ablation showed tagged &gt; untagged by 0.41 BLEU Java, 1.22 BLEU Python).",
            "performance_metric": "BLEU (BLEU-CN)",
            "performance_value": "BM25+id: Java 23.39 BLEU; Python 22.54 BLEU (from Table 3 component columns)",
            "baseline_performance": "BM25: Java 22.87 BLEU; Python 21.78 BLEU",
            "performance_change": "Java: +0.52 BLEU absolute (+2.28%); Python: +0.76 BLEU absolute (+3.49%); tagged identifiers performed better than a plain tag-free list (improvement 0.41 BLEU Java, 1.22 BLEU Python).",
            "experimental_setting": "code-davinci-002; tree-sitter AST used for extraction; 3-shot BM25 selection; temperature=0.",
            "statistical_significance": "Component-level differences reported, overall ASAP significance p &lt; 0.01 (Wilcoxon); specific small-component p-values not always individually reported in the text.",
            "uuid": "e7607.2",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "DFG augmentation",
            "name_full": "Adding Data-Flow Graph (DFG) information to prompt",
            "brief_description": "Including a representation of the function's data-flow graph (identifier indices and flow edges) as a labeled field in exemplars and target to provide semantic relationships between identifiers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Decoder-only Codex variant.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Summarize functions using prompts augmented with data-flow information.",
            "problem_format": "Few-shot prompt where each exemplar includes a labeled 'DFG' block; long DFGs are truncated to first 30 lines to fit in prompt window.",
            "format_category": "prompt content / prompt field",
            "format_details": "3-shot BM25 exemplars; DFG lines presented as identifier indices and edges; truncated for long graphs; compared BM25 vs BM25+DFG vs other components.",
            "performance_metric": "BLEU (BLEU-CN)",
            "performance_value": "BM25+DFG: Java 23.13 BLEU; Python 21.82 BLEU (Table 3 component columns)",
            "baseline_performance": "BM25: Java 22.87; Python 21.78",
            "performance_change": "Java: +0.26 BLEU (+1.14%); Python: +0.04 BLEU (+0.18%).",
            "experimental_setting": "code-davinci-002; DFG computed and truncated to first 30 lines when long; 3-shot; temperature=0.",
            "statistical_significance": "DFG alone gives modest gains; overall ASAP combining components showed p &lt; 0.01 vs BM25. Individual DFG-only p-values not explicitly reported.",
            "uuid": "e7607.3",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Chain-of-thought / step-by-step prompt",
            "name_full": "Chain-of-thought / step-by-step style prompting (Kojima-style)",
            "brief_description": "Formatting prompts to encourage the model to produce intermediate reasoning steps (lemmas) before giving the final summary; authors tested both few-shot versions containing intermediate steps and zero-shot 'let's think step by step' variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (experimented)",
            "model_description": "Decoder-only Codex variant; tested with chain-of-thought style prompts that included intermediate steps in exemplars.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Produce natural-language summary; chain-of-thought formatting appends or interleaves intermediate 'lemmas' before final output.",
            "problem_format": "Few-shot and zero-shot chain-of-thought prompts: exemplars (or zero-shot instruction) ask model to produce intermediate steps and then a final summary.",
            "format_category": "prompt style (reasoning elicitation)",
            "format_details": "Few-shot exemplars included intermediate steps and final comments; zero-shot used 'let's think step by step' style instruction; intention was to induce intermediate lemmas generated by the model itself.",
            "performance_metric": "BLEU (BLEU-CN)",
            "performance_value": "Chain-of-thought (few-shot enhanced): ≈20.25 BLEU (reported), Chain-of-thought zero-shot performed worse (value not specified but 'even worse').",
            "baseline_performance": "Vanilla BM25 few-shot baseline: 24.97 BLEU (specific baseline quoted adjacent to chain-of-thought result in text)",
            "performance_change": "≈ -4.72 BLEU absolute (few-shot chain-of-thought ≈20.25 vs baseline ≈24.97), i.e., worse performance.",
            "experimental_setting": "Model prompted to generate intermediate 'lemmas' in exemplars or prompted zero-shot with 'step-by-step' instruction; attempted to frame problem as chain-of-thought; temperature unspecified here but main experiments used temperature=0.",
            "statistical_significance": null,
            "uuid": "e7607.4",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "BM25 retrieval for exemplar selection",
            "name_full": "BM25-based retrieval for few-shot exemplar selection",
            "brief_description": "Using BM25 information-retrieval to select semantically-relevant few-shot exemplars (input-output pairs) from the pool rather than using random exemplars, which improves performance of few-shot prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (few-shot experiments)",
            "model_description": "Codex variant used in few-shot completion; BM25 used as retrieval engine external to the model to pick exemplars.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Generate function summaries using few-shot exemplars selected either randomly or via BM25 retrieval.",
            "problem_format": "Few-shot prompt where exemplar selection is either random or BM25-retrieved; exemplars are plain input-output pairs (no ASAP augmentation for baseline).",
            "format_category": "prompt style / exemplar selection",
            "format_details": "Compared 'Few-shot (random)' vs 'Few-shot with BM25'. Typically 3 exemplars; BM25 queries constructed from the target function code.",
            "performance_metric": "BLEU-4 (BLEU-CN reported elsewhere)",
            "performance_value": "Few-shot with BM25: Java 22.87 BLEU; Python 21.78 BLEU (Table 2 values under 'Few-shot with BM25').",
            "baseline_performance": "Few-shot (random): Java 19.87 BLEU; Python 20.66 BLEU",
            "performance_change": "Java: +3.00 BLEU absolute (+15.10% relative); Python: +1.12 BLEU (+5.42% relative).",
            "experimental_setting": "BM25 retrieval over the training pool to pick top-3 exemplars most similar to the target; models evaluated on 1000 samples per language (subset); temperature=0.",
            "statistical_significance": null,
            "uuid": "e7607.5",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Shots vs ASAP (context budget)",
            "name_full": "Prompt shot-count trade-offs (more exemplars vs semantic augmentation)",
            "brief_description": "Evaluation showing that using more vanilla exemplars (4 or 5 shots) without semantic augmentation does not outperform the ASAP approach using fewer (3) augmented shots, indicating that prompt content/format matters more than sheer exemplar count within the model's context limits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Codex variant tested with varying numbers of exemplars in prompt (3, 4, 5) with/without ASAP augmentation.",
            "model_size": "≈175B",
            "task_name": "Code summarization",
            "task_description": "Compare BLEU between (a) ASAP with 3 augmented shots and (b) vanilla BM25 with 3/4/5 shots.",
            "problem_format": "Few-shot prompts: varying number of exemplars (3,4,5); ASAP uses 3 augmented exemplars (analysis products included); vanilla baseline uses 3/4/5 plain exemplars.",
            "format_category": "prompt style / exemplar count",
            "format_details": "3-shot ASAP vs BM25 with 3,4,5 shots; model context window ~4k tokens limits number/size of shots; temperature=0.",
            "performance_metric": "BLEU-4",
            "performance_value": "ASAP (3 shots): Java 25.41 BLEU; Python 24.26 BLEU",
            "baseline_performance": "Vanilla BM25: Java 3-shot 22.87 / 4-shot 23.13 / 5-shot 23.20; Python 3-shot 21.78 / 4-shot 21.89 / 5-shot 21.74",
            "performance_change": "ASAP(3-shot) vs BM25(5-shot) Java: 25.41 vs 23.20 = +2.21 BLEU absolute; Python: 24.26 vs 21.74 = +2.52 BLEU absolute.",
            "experimental_setting": "code-davinci-002; experiments run with differing numbers of BM25 exemplars; ASAP uses 3 augmented exemplars; temperature=0.",
            "statistical_significance": null,
            "uuid": "e7607.6",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Model-type sensitivity (completion vs chat)",
            "name_full": "ASAP effect across model families (completion models vs chat models)",
            "brief_description": "ASAP-augmented prompts yield stronger relative improvements on completion-style models (code-davinci-002, text-davinci-003) than on the chat-optimized gpt-3.5-turbo, which produced less stylistically-aligned comments and smaller gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002, text-davinci-003, gpt-3.5-turbo",
            "model_description": "Three OpenAI models tested: code-davinci-002 (Codex, code-focused completion), text-davinci-003 (completion), gpt-3.5-turbo (chat-optimized model).",
            "model_size": "≈175B (for Davinci/Codex variants and GPT-3.5 family approximated)",
            "task_name": "Code summarization",
            "task_description": "Generate function summaries with ASAP-augmented prompts and compare effects across different LLM variants.",
            "problem_format": "Few-shot (BM25) exemplars augmented with ASAP facts; same prompt structure fed to different model endpoints.",
            "format_category": "model family / prompt compatibility",
            "format_details": "500-sample evaluations for Java, Python, PHP; temperature=0; identical ASAP prompt used across models; measured BLEU.",
            "performance_metric": "BLEU",
            "performance_value": "Examples: Java: code-davinci-002 BM25→ASAP 23.90→25.78 (+7.87%); text-davinci-003 18.98→22.31 (+17.54%); gpt-3.5-turbo 16.68→16.96 (+1.68%, p=0.95). Python: code-davinci-002 22.00→24.78 (+12.64%); text-davinci-003 16.74→18.93 (+13.08%); gpt-3.5-turbo 15.01→16.38 (+9.13%). PHP similar pattern with larger gains for completion models.",
            "baseline_performance": "Model-specific BM25 values as in Table 6 (e.g., code-davinci-002 Java 23.90 BLEU; text-davinci-003 Java 18.98 BLEU; turbo Java 16.68 BLEU).",
            "performance_change": "Varies by model and language; improvements larger and statistically significant (p &lt; 0.01) for completion models; small and not significant for some turbo comparisons (e.g., Java turbo p=0.95).",
            "experimental_setting": "ASAP prompts applied unchanged to each model; 500 samples per language for these cross-model tests; temperature=0.",
            "statistical_significance": "p &lt; 0.01 for most comparisons (except Java on gpt-3.5-turbo where p = 0.95 as reported).",
            "uuid": "e7607.7",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Zero-shot ASAP for line completion",
            "name_full": "ASAP augmentation applied in zero-shot line-completion",
            "brief_description": "Appending semantic facts (repo info, tagged identifiers, DFG derived only from preceding lines) in a zero-shot prompt improved line-completion Exact Match and Edit Similarity for Java and overall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Codex variant used in zero-shot completion experiments (no exemplars shown).",
            "model_size": "≈175B",
            "task_name": "Line completion (code completion)",
            "task_description": "Given preceding lines of a function, generate a target (selected) line. ASAP appends semantic facts to the prefix in zero-shot fashion.",
            "problem_format": "Zero-shot completion where prompt = [Repo info || tagged identifiers || DFG (from preceding lines) || preceding lines], model must generate the next/target line.",
            "format_category": "prompt style / input modality",
            "format_details": "Randomly selected target line per sample; semantic facts computed only from preceding lines to avoid leakage; metrics: Exact Match (EM) and Edit Similarity (ES); temperature presumably 0 as elsewhere.",
            "performance_metric": "Exact Match (EM) and Edit Similarity (ES)",
            "performance_value": "Overall (combined Java+Python): EM 19.01% (ASAP) and ES 55.72% (ASAP). Java: EM 22.12% (ASAP), ES 59.66%; Python: EM 14.58%, ES 50.12%.",
            "baseline_performance": "Overall baseline EM 17.97% and ES 53.01%. Java baseline EM 20.75% / ES 55.35%; Python baseline EM 14.05% / ES 49.71%.",
            "performance_change": "Overall EM +1.04 percentage points (+5.79% relative); overall ES +2.71 percentage points (+5.11% relative). Java EM +1.37 pp (+6.6%), ES +4.31 pp (+7.79%). Python EM +0.53 pp (+3.77%, p=0.13 not significant), ES +0.41 pp (+0.82%, p&lt;0.01).",
            "experimental_setting": "code-davinci-002; Java N=9292, Python N=6550 samples; random target line selection; semantic facts appended; tests used McNemar for EM and Wilcoxon for ES; some statistical tests reported.",
            "statistical_significance": "Overall: p &lt; 0.01 for EM and ES; Java: p &lt; 0.01 both EM and ES; Python: EM p = 0.13 (not significant), ES p &lt; 0.01.",
            "uuid": "e7607.8",
            "source_info": {
                "paper_title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Repository-level prompt generation for large language models of code",
            "rating": 2
        },
        {
            "paper_title": "Few-shot training LLMs for project-specific code-summarization",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT: Jack of all trades, master of none",
            "rating": 1
        },
        {
            "paper_title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
            "rating": 2
        }
    ],
    "cost": 0.019997749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)</h1>
<p>Toufique Ahmed<br>University of California, Davis<br>Davis, California, USA<br>tfahmed@ucdavis.edu<br>Premkumar Devanbu<br>University of California, Davis<br>Davis, California, USA<br>ptdevanbu@ucdavis.edu</p>
<h2>ABSTRACT</h2>
<p>Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. Researchers are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.</p>
<p>One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of "code analysis" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.</p>
<p>Prior work shows that LLM performance on code summarization benefits from embedding a few code \&amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.</p>
<p>We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU $^{1}$. In addition, we</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Kunal Suresh Pai <br> University of California, Davis <br> Davis, California, USA <br> kunpai@ucdavis.edu</h2>
<p>Earl T. Barr<br>University College London \&amp; Google Brain<br>London, UK<br>e.barr@ucl.ac.uk</p>
<p>have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.</p>
<h2>KEYWORDS</h2>
<p>LLM, Code Summarization, Program Analysis, Prompt Engineering</p>
<h2>ACM Reference Format:</h2>
<p>Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl T. Barr. 2024. Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization). In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE '24), April 14-20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3597503.3639183</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) often outperform smaller, customtrained models on tasks, especially when prompted with a "fewshot" set of exemplars. LLMs are pre-trained on a self-supervised (masking or de-noising) task, using vast amounts of data, and exhibit surprising emergent behaviour as training data and parameter counts are scaled up. They excel at many tasks with few-shot (or even zero-shot) learning: with just a few exemplar input-output pairs inserted first in the prompt, the models can generate very good outputs for a given input! Few-shot learning works so well with LLMs that it is unclear whether sufficient task-specific data can ever be gathered to train a customized model to rival their performance [3, 12]. LLMs are ushering in a new era, where prompt engineering, to carefully condition the input to an LLM to tailor its massive, but generic capacity, to specific tasks, will become a new style of programming, placing new demands on software engineers.</p>
<p>We propose Automatic Semantic Augmentation of Prompts ( $\mathcal{A S A P}$ ), a new method for constructing prompts for software engineering tasks. The $\mathcal{A S A P}$ method rests on an analogy: an effective prompt for an LLM, for a task, relates to the facts a developer thinks about when manually performing that task. In other words, we hypothesize that prompting an LLM with the syntactic and semantic facts a developer considers when manually performing a task will improve LLM performance on that task. To realise this hypothesis, $\mathcal{A S A P}$ augments prompts with semantic facts automatically extracted from the source code using semantic code analysis.</p>
<p>We illustrate the $\mathcal{A S A P}$ methodology first on code summarization. This task takes code, usually a function, and summarizes it using natural language; such summaries can support code understanding to facilitate requirements traceability and maintenance.</p>
<p>$\mathcal{A S A P}$ uses a few-shot prompting because its effectiveness. $\mathcal{A S A P}$ finds relevant shots using $B M 25$, the current state of the art in finding few-shot exemplars that are "semantically close" to the target function [48], in our case, the function-to-summarize, by querying the LLM's training data. When instantiating $\mathcal{A S A P}$ for the summarization task, we equipped it to extract the following semantic facts: the repository name, the fully qualified name of the name of the target function, its signature, the AST tags of its identifiers, and its data flow graph (Section 3.4). These facts are presented to the LLM as separate, labelled, fields ${ }^{2}$. The model is then provided with the function-to-summarize, exemplars (along with facts extracted from each), and asked to emit a summary. We confirm our hypothesis that augmenting prompts with semantic facts can improve LLM performance on the code completion task. We evaluated $\mathcal{A S A P}$ 's benefits on the high-quality (carefully de-duplicated, multi-project) CodeSearchNet [32] dataset.</p>
<p>In summary, we find that in all cases, our approach of automatic semantic augmentation improves average performance on several commonly-used metrics. For almost all languages, the average improvement comfortably surpasses the 2-BLEU threshold noted by Roy et al. [57], below which BLEU results are unreliable predictors of human preference. For Go, gains are still significant, and just slightly less than 2; for PHP, we see an improvement of 4.6 BLEU, reaching a SOTA high-point of 32.73 on the well-curated, de-duplicated, CodeSearchNet dataset.</p>
<p>Our principal contributions follow:</p>
<ul>
<li>The $\mathcal{A S A P}$ approach for software engineering tasks using facts derived from code.</li>
<li>We evaluate $\mathcal{A S A P}$ on the code summarization task on the code-davinci-002, text-davinci-003. and GPT-3.5-turbo models against a few-shot prompting baseline built using vanilla BM25 (Section 4.1).</li>
<li>We find that the $\mathcal{A S A P}$ approach statistically significantly improves LLM performance on the code summarization task. In almost all cases, we observe statistically significant improvements of almost, or in excess of, 2 BLEU; and, for PHP, we break 30 BLEU for the first time (to our knowledge) on this challenging dataset.</li>
<li>We find that $\mathcal{A S A P}$ also leads to improved performance on the code-completion task.</li>
</ul>
<p>All the data, evaluation scripts, and code needed to reproduce this work will be available at https://doi.org/10.5281/zenodo.7779196, and can be reproduced on any available language models. Our experiments suggest that $\mathcal{A S A P}$ works well with any language model powerful enough to leverage few-shot prompting.</p>
<h2>2 BACKGROUND \&amp; MOTIVATION</h2>
<p>Large Language Models (LLM) are a transformative technology: they are essentially a new kind of computation engine, requiring a new form of programming, called prompt engineering. We first contextualise $\mathcal{A S A P}$, our contribution to prompt engineering. Finally, we discuss code summarization as a sample problem to demonstrate $\mathcal{A S A P}$ 's effectiveness.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1 Few-shot Learning in Software Engineering</h3>
<p>LLMs are now widely used in Software Engineering for many different problems: code generation [14, 34], testing [38, 42], mutation generation [10], program repair [18, 35, 36, 48], incident management [6], and even code summarization [3]. Clearly, tools built on top of pre-trained LLM are advancing the state of the art. Beyond their raw performance at many tasks, two key factors govern the growing dominance of pretrained LLM, both centered on cost. First, training one's own large model, or even extensively fine-tuning a pre-trained LLM, requires expensive hardware. Second, generating a supervised dataset for many important software engineering tasks is difficult and time-consuming, often beyond the sources of all but the largest organizations.</p>
<p>In contrast to overall LLM trends, there are some smaller models, specialized for code, that have gained popularity, e.g., Polycoder [67] or Codegen [49]. Despite these counterpoints, we focus on LLM rather than small models, because, while small models can be finetuned, they don't do very well at few-shotting, and thus are not helpful when only small amounts of data are available. The few-shot approach is key because it brings into reach many problems, like code summarization, for which collecting sufficient, high-quality, project- or domain-specific training data to train even small models from scratch is challenging.</p>
<p>With few-shot learning, the actual model parameters remain unchanged. Instead, we present a few problem instances along with solutions (i.e., problem-solution pairs as "the exemplars") to a model and ask it to complete the answer for the last instance ("the test input"), for which we do not provide a solution. Thus with each exemplar consisting of an 〈input, output〉 pair, and just a test-input input ${ }<em t="t">{t}$ (without the corresponding, desired output ${ }</em>$ ), the final prompt looks like:</p>
<p>$$
\text { prompt } \leftarrow \text { exemplar }<em 2="2">{1}\left|\operatorname{exemplar}</em>}\right| \text { exemplar <em t="t">{3} \mid \text { input }</em>
$$</p>
<p>With this prompt, the LLM generates output $t_{t}$, mimicking the inputoutput behavior illustrated by the exemplars in the prompt. In practice, this approach performs quite well.</p>
<p>When it works, few-shotting allows us to automate even purely manual problems, since generating a few exemplar samples is relatively easy. In this paper, we experiment with the code-davinci-002 model. We discuss models in more detail in Section 3.2.</p>
<h3>2.2 Prompting LLMs to Reason</h3>
<p>Human Reasoning involves using evidence, logical thinking, and arguments to make judgments or arrive at conclusions [31, 51]. Natural language processing (NLP) researchers have developed approaches to reason about specific scenarios and improve performance. Approaches like "Chain of thought" [66] and "step-bystep" [40] require generating intermediate results ("lemmas") and utilizing them in the task at hand. Such approaches appear to work on simpler problems like school math problems even without providing them with "lemmas", because, for these problems, models are powerful enough to generate their own "lemmas"; in some cases just adding "let's think step by step" seems sufficient (Kojima et al. [40]).</p>
<p>We tried an enhanced version of the "step-by-step" prompt, with few-shots, on code summarization. We found that the model underperformed (getting about 20.25 BLEU), lower even than our vanilla BM25 baseline (24.97 BLEU). With zero-shot Kojima-style "step by step" prompt, the models perform even worse. To induce the model to generate steps, and finally a summary, we framed the problem as chain of thought, and included few-shot samples containing both intermediate steps ("lemmas") and final comments. The reasoning is that, on the (usually challenging) code-related tasks, models need to explicitly be given intermediate "lemmas", derived from code, to be able to reason effectively about most software engineering tasks, which tend to be more complex and varied than school maths.</p>
<p>Fortunately, mature tools for code analysis are available. We can readily derive "lemmas", viz., analysis products, using code analysis tools, rather than expecting the models to (perhaps implicitly) derive them, during on-task performance. We directly embed analysis products into the prompt we give the language model, and evaluate the benefits of such analysis products. The information we derive and add are based on our own intuitions about the kinds of "lemmas" that developers consciously or unconsciously consider as they seek to understand and summarize code.</p>
<p>We find that providing such information improves LLM performance. We remind the reader that most work involving large language models (LLMs) usually uses some form of prompt engineering to boost performance. In this paper, we show that the $\mathcal{A} S A P$ approach, which augments prompts with code analysis products, improves on previous prompting approaches.</p>
<h3>2.3 Summarizing Code</h3>
<p>Well-documented code is much easier to maintain; thus, experienced developers usually add, e.g., function summary headers. However, summary comments may become outdated, as projects evolve [11, 22]. Automated code summarization is thus a wellmotivated task, which has attracted a great deal of attention; and considerable progress (albeit incremental, over many years) has been made. Initially, template-based approaches were popular [17, $26,27,56,61]$; however, creating a list of templates with good coverage is very challenging. Later, researchers focused on the retrievalbased (IR) approach [17, 26, 27, 56], where existing code (with a summary) is retrieved based on similarity-based metrics. However, this promising approach only worked if a similar code-comment pair could be found in the available pool.</p>
<p>Meanwhile, the similarity of code summarization to Neural Machine Translation (NMT), (one can think of generating an English summary of code as producing a representation of "the same meaning in a different language") led to research that adapted Neural Machine Translation (NMT) to code summarization. Numerous studies have been conducted in this area [1, 30, 33, 41]. Some have combined previous approaches, such as template-based and retrieval-based approaches, using neural models [69], and have reported promising results. Such neural methods for NLP have vastly improved, due to the Transformer architectural style.</p>
<p>Until recently, pre-trained language models such as CodeBERT, CodeT5, and CodeT5+ performed best for code summarization.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different steps of $\mathcal{A} S A P$. (1) Input code and (2) Pool of samples are given the BM25 engine, which matches the given input code against the pool and (3) retrieves bestmatching samples, viz. 3 input+output pairs. These examples are processed by $\mathcal{A} S A P$ to produce a prompt (4) including 3 exemplars. Each exemplar includes a function definition, the results of analyzing that definition, and its associated comment; the input code is finally appended, along with its analysis product. Exemplar details are in Figure 2. The final prompt is sent via API call (5) to the GPT-3.x model; the returned output, e.g., summary (6) is returned by GPT-3x.</p>
<p>However, Large Language Models (LLMs) now typically outperform smaller pre-trained models on many problems. Ahmed \&amp; Devanbu [3] report that LLMs can outperform pre-trained language models with a simple prompt consisting of just a few samples already in the same project; this work illustrates the promise of careful construction of prompt structures (c.f. "prompt engineering"). We present $\mathcal{A} S A P$ here as another general principle of prompt engineering. We emphasize, again, that progress in code summarization (and other applications of AI to SE, such as code patching, defect detection, testing etc) has been incremental, as in the field of NMT, where practical, usable translation systems took decades to emerge. Thus incremental advances are still needed, and helpful, and we contribute our work to this long-term enterprise.</p>
<h2>3 DATASET \&amp; METHODOLOGY</h2>
<p>We now discuss our dataset, models, and methodology.</p>
<h3>3.1 Dataset</h3>
<p>Our experiments use the widely used CodeSearchNet [32] dataset; CodeSearchNet was constructed by extracting the first paragraph of the function prefix documentation, subject to some restrictions (e.g. length). It is a carefully de-duplicated, multi-project dataset, which allows (more demanding) cross-project testing. De-duplication is key: Code duplication in machine learning models can deceptively inflate performance metrics a lot, when compared to de-duplicated datasets [7, 46, 59].</p>
<p>It is part of the CodeXGLUE [47] benchmark, which comprises 14 datasets for 10 software engineering tasks. Many models have been evaluated on this dataset. CodeSearchNet contains thousands of samples from six different programming languages (i.e., Java, Python, JavaScript, Ruby, Go, PHP). However, we did not use the entire test dataset, which would have been prohibitively expensive</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Components of an $\mathcal{A} S A P$ Exemplar. Source Code and Output Comment are extracted from the retrieved pool sample. The Repo info is derived from the source code using GitHub; the Dataflow Info and tagged Identifiers with labels is obtained from an analysis using Treesitter.</p>
<table>
<thead>
<tr>
<th>Language</th>
<th>#of Training Samples</th>
<th>#of Test Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Java</td>
<td>164,923</td>
<td>1000</td>
</tr>
<tr>
<td>Python</td>
<td>251,820</td>
<td>1000</td>
</tr>
<tr>
<td>Ruby</td>
<td>24,927</td>
<td>1000</td>
</tr>
<tr>
<td>JavaScript</td>
<td>58,025</td>
<td>1000</td>
</tr>
<tr>
<td>Go</td>
<td>167,288</td>
<td>1000</td>
</tr>
<tr>
<td>PHP</td>
<td>241,241</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of training and test samples. and slow using ours models API endpoints; instead, we selected 1000 samples $^{3}$ uniformly at random from each language. Since the original dataset is cross-project and we sampled it uniformly, our subsample includes cross-project data. In addition, we subsetted this dataset for same-project few-shotting, following Ahmed and Devanbu [3]: we sort same-project data by creation date (using git blame). Now, we use the temporal order to make sure that only temporally earlier samples are used the few-shot exemplars; this is realistic, since only older, already existing data is available for use. We will delve deeper into this same-project dataset in Section 4.3.</p>
<p>As mentioned earlier, we don't use any parameter-changing training on the model; we just insert a few exemplars selected from the training subset into the few-shot prompt. Table 1 lists the count of training \&amp; test samples used in our experiments.</p>
<h3>3.2 The Models</h3>
<p>In earlier work, transformer-based pre-trained language models offered significant gains, in both NLP and software engineering. Pre-trained language models can be divided into three categories: encoder-only, encoder-decoder, and decoder-only models. While encoder-decoder models have initially shown success on many tasks, decoder-only LLMs are now more scaleable and effective for numerous tasks.</p>
<p>Encoder-Decoder model. BERT is one of the earliest pre-trained language models [15]; it was pre-trained on two self-supervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). Later, RoBERTa [45] was introduced with some minor</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>modifications to BERT. Using only MLM training, it outperforms BERT. CodeBERT [21] and GraphCodeBERT [25] introduced these ideas to Software Engineering. Although CodeBERT and GraphCodeBERT are encoder-only models, they can be applied to code summarization after fine-tuning, cascaded to a decoder trained during fine-tuning. Ahmed \&amp; Devanbu report that polyglot models, which are fine-tuned with multilingual data, outperform their monolingual counterparts [4]. They also report that identifiers play a critical role in code summarization tasks. PLBART [2], CodeT5 [64], and CodeT5+ [63] also include pre-trained decoders and are reported to work well for code summarization tasks. More recently, very large scale (decoder-only) auto-regressive LLMs (with 175B+ parameters) have been found to be successful at code summarization with few-shot learning, without any explicit training. In the next section, we will briefly introduce the three OpenAI models we considered for our experiments.</p>
<p>Decoder-only model. In generative pre-training, the task is to auto-regressively predict the next token given the previous tokens moving from earlier to later. This unidirectional auto-regressive training prevents the model from pooling information from future tokens. The newer generative models such as GPT [52], GPT-2 [53] and GPT-3 [12], are also trained in this way, but they have more parameters, and are trained on much larger datasets. Current large language models, such as GPT-3, have around (or more than) 175B parameters. These powerful models perform so well, with few-shot prompting, that interest on task-specific parameter-adjustment via fine-tuning has reduced.</p>
<p>Codex is a GPT-3 variant, intensively trained on code and natural language comments. The Codex family consists of two versions: Codex-Cushman, which is smaller, with 12B parameters, and CodexDavinci, the largest, with 175B parameters. The Codex model is widely used, for various tasks. Our experiments mostly target the Code-Davinci model, particularly Code-Davinci-002, which excels at translating natural language to code [14] and supports code completion as well as code insertion ${ }^{4}$. Some new variants, Text-Davinci-003 \&amp; GPT-3.5-turbo, are also available; unlike the Codex variants, these models understand and generate both natural language and code. Although optimized for chat, GPT-3.5-turbo also performs well on traditional completion tasks. Text-Davinci-003 is a completion model like Code-Davinci-002. We study how our prompt enhancement works using the Text-Davinci-003 \&amp; GPT-3.5turbo models.</p>
<h3>3.3 Retrieving Exemplars from Training Data</h3>
<p>As noted earlier, few-shot learning works quite well, when used with very large models. We prompt the model with a small number of (problem, solution) exemplars, and ask it to solve a new problem. However, carefully selecting exemplars for few-shot learning is helpful. Nashid et al. discovered that retrieval-based exemplar selection is helpful for problems such as assertion generation and program repair [48]. Following their recommendation, we use the BM25 IR algorithm to select relevant few-shot exemplars from the training set. BM25 [55] is a frequency-based retrieval method which improves upon TF-IDF [54]. We noted a substantial improvement</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{3}$ Please see experimental power discussion in Section 7.</p>
<p>over the same fixed exemplars in few-shot learning, as detailed in Section 4.1. Nashid et al. compare several retrieval methods, and found $B M 25$ works best; we therefore use it, as well.</p>
<h3>3.4 Automatic Semantic Prompt Augmentation</h3>
<p>This section presents the three semantic facts we selected to enhance $\mathcal{A S A P}$ 's prompts and the $\mathcal{A S A P}$ pipeline (See Figure 2). The choice of these facts comes from applying our central hypothesis, viz. that augmenting prompts with what developers think about when working on a task, to the code summarization task. $\mathcal{A S A P}$ is not tied to any specific semantic facts or static analysis; it can easily incorporate others, as discussed later.</p>
<p>Repository Name \&amp; Path. Augmenting prompts with domainspecific information can improve LLM performance on various tasks. Prior work suggests that augmenting prompts with code from the same repository improves performance in code generation tasks [60]. We argue that basic repository-level meta-information, such as the repository name and the complete path to the repository, provides additional context. For example, repository names like "tony19/logback--android", "apache/parquet--mr", and "ngageoint/ geo-package-android" all connect a function to a specific domain (e.g., android, apache, geo-location), which can enhance the understanding of the target code to be summarized. Figure 2 (yellow part) presents an example of how we enhance the prompt with repository-level information. Similar to the repository name, the path to the function can also contribute to the model.</p>
<p>Tagged Identifiers. Prior work suggests that language models find more value in identifiers, rather than code structure, when generating code summaries [4]. However, identifiers play different roles in code. Local variables, function names, parameters, global variables etc., play different parts in the functioning of the method in which they occur; a developer reading the code is certainly aware of the roles of identifier, simply by identifying the scope and use. Thus, augmenting prompts with the specific roles of identifiers could help the model better "understand" the function. We use tree-sitter to traverse the function's AST and gather identifiers, along with their roles. Figure 2 (blue part) presents a sample example showing how we enhanced the prompt of the function with tagged identifiers. Although the model has access to the token sequence of the code, and thus also all the identifiers, them to the model in a tagged form might a) save the model some compute effort, and b) better condition the model's output.</p>
<p>Data Flow Graph (DFG). Guo et al. introduced the GraphcodeBERT model, which uses data flow graphs (DFG) instead of syntacticlevel structures like abstract syntax trees (ASTs) in the pre-training stage [25]. GraphcodeBERT outperformed CodeBERT [21] on various software engineering (SE) tasks. We incorporate this DFG information into the few-shot exemplars; we conjecture that this provides the model a better semantic understanding of each exemplar, and the target example. Figure 2 (orange) presents a sample showing the Data Flow Graph (DFG) we used for our experiments. Each line contains an identifier with its index and the index of the identifiers to which that particular data flows. Unlike repo and tagged identifiers, the data flow graph can be very long, making it inconvenient to add the complete data flow to the prompt. In the
case of long prompts, we only kept the first 30 lines of the DFG in the prompt. In addition to identifiers, the DFG also provides a better understanding of the importance of identifiers in the function.</p>
<p>Use Case \&amp; Completion Pipeline. $\mathcal{A S A P}$ has 3 components: an LLM, a pool of available exemplars (labeled input-output pairs, e.g., code with comments), and a static analysis tool for deriving facts from code (See Figures 1 and 2).</p>
<p>A configuration file specifies these components. Once configured, a developer invokes $\mathcal{A S A P}$ on a function body $C_{\text {in }}$ (Figure 1), for which an output (e.g.,, code summary) is desired. $\mathcal{A S A P}$ uses $C_{\text {in }}$ as a BM25 query over the its sample pool to get a result set of exemplar candidates $e_{1}, e_{2}, \ldots$, where each $e_{i j}$ is a pair of the form $\left\langle\right.$ input $<em i="i">{i}$, output $\left.</em>\right\rangle$; in our context, input $<em i="i">{i}$ is the function definition and output $</em>$ is the function header comment. BM25 chooses the input $<em _in="{in" _text="\text">{i}$ s that match best with the given $C</em>$ and the several exemplar inputs input $}}$. $\mathcal{A S A P}$ then applies program analyses to both the input $C_{\text {in }<em _in="{in" _text="\text">{i}$ s, yielding analysis products $a p</em>$ s.}}$ and several $a p_{i</p>
<p>Each exemplar $e_{i}$ (Figure 2) is the triple: $\left\langle\right.$ input $<em i="i">{i}$, ap $</em>$, output $\left.<em i="i">{i}\right\rangle$, where each triple illustrates, for the LLM, how input source code input $</em>$, to the output output $}$ relates, via the analysis product $a p_{i<em 1="1">{i}$. The final prompt is then " $e</em>$ queries an LLM with that prompt, and returns the completion (e.g., natural language summary).}\left|e_{2}\right| e_{3}\left|C_{\text {in }}\right| a p_{\text {in }}$ ". $\mathcal{A S A P</p>
<p>By default, $\mathcal{A S A P}$ is configured with analyses to extract repository info, tag identifiers, construct DFGs. These analyses are independent and are their outputs are separately labeled in the prompt. For example, Figure 2 shows the output of the DFG analysis in $\mathcal{A S A P}$ 's constructed prompt. These few shot examples, are augmented and inserted into the prompt: the code, repository info, tagged identifiers, the DFG, and the desired (Gold) summary are all included in each few-shot. The target example includes just analysis product, and the LLM is prompted to produce the desired output.</p>
<p>In prior work using "chain of thought" [66] or "step by step" [40] reasoning, no such information is given to the model; instead, the prompt simply helps it organize its reasoning about the sample into a sequence of instructions. Here, rather than having the model do its own reasoning, we shape its reasoning externally by using simple program analyses, since we can get very precise information from very efficient analysis tools. Each few-shot example includes source code, derived information, and conclusion (summary), thus providing exemplary "chains of thought" for the model to implicitly use when generating the desired target summary. Figure 1 presents the overall pipeline of our approach that we apply to each sample. The BM25 engine matches input code against a sample pool, $\mathcal{A S A P}$ processes resulting examples to create a prompt, and the final prompt is sent to the GPT-3.x model via API, yielding a summary as output.</p>
<p>Next, we describe how we evaluate this pipeline.</p>
<h3>3.5 Metrics</h3>
<p>BLEU [50] is the most widely-used, similarity-based measure for code summarization [57] and commit log generation [16]. BLEU counts the fraction of $n$-grams (usually for $n \in[1 . .4]$ ), that occur in both generated candidates and one or more reference translations; the geometric mean of these fractions is the BLEU, usually normalized to the range $0-100$. At sentence granularity, BLEU tends to overly penalize candidate translations when few (or none) of the</p>
<p>longer n-grams co-occur, so "Sentence BLEU" has been criticized for correlating poorly with human judgment. Various smoothing techniques [13, 23, 44] have been used, to reduce Sentence BLEU's sensitivity to sparse $n$-gram matches, and better align it with human quality assessment. We report data on two variants: BLEU-CN, which uses a kind of Laplacian smoothing [2, 3, 8, 21, 33, 47, 64] and BLEU-DC, which uses newer smoothing methods [29, 65]. Other proposed metrics such as BERTScore [28, 70], BLEURT [58], NUBIA [37], are computationally expensive, not widely used and thus not readily comparable with prior work for benchmarking.</p>
<p>Given all these options, metrics for code summarization and, independently, for commit-log generation [16], have been debated [24, 28, 57]. In this paper, we follow prior work and primarily use BLEUCN ; this facilitates the comparison of our results with prior work. The CodeXGLUE benchmark recommends BLEU-CN, and most newer models [3, 21, 64] use this metric. We, however, have not neglected other measures. Besides BLEU-CN, and BLEU-DC, we also report results using ROUGE-L [43] and METEOR [9].</p>
<p>In all cases, $\mathcal{A S A P}$ achieves significant overall improvements: we observe gains greater than 2.0 BLEU for all programming languages except for Go (Table 3). We contend that gains greater than 2.0 BLEU are important for two reasons. Roy et al. [57] provide arguments, grounded on human subject study that for code summarization (our central task), that a gain of 2.0 or more BLEU is more likely to correspond with human perception of improvement. Second, we argue that even smaller gains matter (especially if repeatable and statistically significant) since incremental progress on such tasks accumulates, towards strong practical impact, as evidenced by decades-long work in natural language translation.</p>
<p>In addition to code summarization, we evaluated $\mathcal{A S A P}$ approach on the code completion task. The standard metrics used for this task are exact match (did the completion match exactly) and edit similarity (how close is the completion to the expected sequence). Here, too, $\mathcal{A S A P}$ achieves significant overall improvements.</p>
<h3>3.6 Experimental Setup \&amp; Evaluation Criteria</h3>
<p>Our primary model is OpenAI's code-davinci-002. We use the beta version, via its web service API. To balance computational constraints like rate limits and our desire for robust estimates of performance, we chose to use 1000 samples $^{2}$ per experimental treatment (one treatment for each language, each few-shot selection approach, with $\mathcal{A S A P}$, without $\mathcal{A S A P}$ etc.).</p>
<p>Our experiments yielded statistically significant, interpretable results in most cases. Each 1000-sample trial still took 5 to 8 hours, varying (presumbly) with OpenAI's load factors. We include waiting periods between attempts, following OpenAI's recommendations. To obtain well-defined answers from the model, we found it necessary to set the temperature to 0 , for all our experiments. The model is designed to allow a window of approximately 4 K tokens; this limits the number of few-shot samples. For our experiments, we used 3 shots. $\mathcal{A S A P}$ defaults to three shots because related work [3, 12] has shown, and our own experiments with $\mathcal{A S A P}$ confirmed, that more shots did not significantly improve performance. However, for up to $2 \%$ of the randomly chosen samples in each experiment, we didn't get good results; either the prompt didn't fit into the model's</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>window, or the model mysteriously generated an empty string. In cases where the prompt as constructed with 3 samples was too long, we automatically reduce the number of shots. When empty summaries were emitted, we resolved this by increasing the number of shots. This simple, repeatable, modest-overhead procedure can be incorporated into automated summarization tools.</p>
<h2>4 RESULTS</h2>
<p>We evaluate the benefits of $\mathcal{A S A P}$-enhanced prompts, for code summarization, in different settings and using various metrics. We find evidence of overall performance gain, in studies on six languages. However, for other detailed analyses, we focused primarily on Java and Python, because of OpenAI API rate limits.</p>
<h3>4.1 Encoder-decoders \&amp; Few-shot Learning</h3>
<p>Our baseline results on CodeSearchNet [47], using IR-based fewshotting, come first. Prior work reports that IR methods can find better samples for few-shot prompting, for tasks such as program repair [48] and code generation [34]. In Table 2, we observe that this is also true for code summarization; we note improvements of 3.00 (15.10\%) and 1.12 (5.42\%) in BLEU-4 score for Java and Python, respectively, simply by using BM25 as a few-shot sample selection mechanism. Since BM25 was already used in prior paper (albeit for other tasks) [48], we consider this BM25-based few-shot learning for code summarization as just a baseline (not a contribution per se) of this paper.</p>
<h3>4.2 $\mathcal{A S A P}$ Prompt Enhancement</h3>
<p>We now focus on the central result of our paper: the effect of $\mathcal{A S A P}$ prompt enhancement. Table 3 shows the component-wise and overall improvements achieved after combining all the prompting components for all six programming languages. BLEU improvements range from $1.84(8.12 \%)$ to $4.58(16.27 \%)$. In most cases, we see improvements of over 2.0 BLEU, the required threshold for human perception noted by Roy et al. [57].</p>
<p>We also noticed that all three components (i.e., Repository Information., DFG Data Flow Graph, Identifiers) help the model achieve better performance in all six languages, as we combined these components individually with BM25. However, for Ruby, the best performing combination includes just the Repo. information. In most cases, the Repo. helps a lot, relative to other components.</p>
<p>To ascertain improvement significance, we used the pairwise one-sided Wilcoxon signed-rank test, finding statistical significance in all cases for our final prompt when compared with vanilla BM25 few-shot learning, even after adjusting for false discovery risk.</p>
<h3>4.3 Same Project Code Summarization</h3>
<p>We now examine the benefits of $\mathcal{A S A P}$ in the context of some earlier work on few-shot selection. Prior work has shown that selecting few-shots from the same projects substantially improves performance [3]. To see if our prompt enhancement idea further helps in project-specific code summarization, we evaluated our approach on the dataset from Ahmed and Devanbu [3]. Due to rate limits, we reduced the number of test samples to 100 for each of the four Java and Python projects. Since we have too few samples for a per-project test, we combined all the samples to perform the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">CodeBERT</th>
<th style="text-align: center;">GraphCodeBERT</th>
<th style="text-align: center;">Polyglot CodeBERT</th>
<th style="text-align: center;">Polyglot GraphcodeBERT</th>
<th style="text-align: center;">CodeT5</th>
<th style="text-align: center;">CodeT5+</th>
<th style="text-align: center;">Few-shot (random)</th>
<th style="text-align: center;">Few-shot with BM25</th>
<th style="text-align: center;">Gain (\%) over random few-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">18.52</td>
<td style="text-align: center;">20.22</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">19.78</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">22.87</td>
<td style="text-align: center;">$+15.10 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">17.73</td>
<td style="text-align: center;">17.35</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">18.33</td>
<td style="text-align: center;">19.98</td>
<td style="text-align: center;">18.85</td>
<td style="text-align: center;">20.66</td>
<td style="text-align: center;">21.78</td>
<td style="text-align: center;">$+5.42 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of encoder-decoder and few-shot models on Java and Python code summarization, measured using BLEU.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">BM25+repo</th>
<th style="text-align: center;">BM25+id</th>
<th style="text-align: center;">BM25+DFG</th>
<th style="text-align: center;">$\mathcal{A S A P}$</th>
<th style="text-align: center;">Comparing with BM25</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gain (\%) over BM25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">22.87</td>
<td style="text-align: center;">25.23</td>
<td style="text-align: center;">23.39</td>
<td style="text-align: center;">23.13</td>
<td style="text-align: center;">25.41</td>
<td style="text-align: center;">$+11.11 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">21.78</td>
<td style="text-align: center;">24.22</td>
<td style="text-align: center;">22.54</td>
<td style="text-align: center;">21.82</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">$+11.39 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ruby</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">19.67</td>
<td style="text-align: center;">19.19</td>
<td style="text-align: center;">17.55</td>
<td style="text-align: center;">19.62</td>
<td style="text-align: center;">$+14.00 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">JavaScript</td>
<td style="text-align: center;">23.27</td>
<td style="text-align: center;">25.11</td>
<td style="text-align: center;">24.21</td>
<td style="text-align: center;">24.04</td>
<td style="text-align: center;">25.36</td>
<td style="text-align: center;">$+8.98 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">24.41</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">23.42</td>
<td style="text-align: center;">24.51</td>
<td style="text-align: center;">$+8.12 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">28.15</td>
<td style="text-align: center;">32.07</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">28.92</td>
<td style="text-align: center;">32.73</td>
<td style="text-align: center;">$+16.27 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">22.66</td>
<td style="text-align: center;">25.12</td>
<td style="text-align: center;">23.72</td>
<td style="text-align: center;">23.15</td>
<td style="text-align: center;">25.32</td>
<td style="text-align: center;">$+11.74 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of prompt enhanced comment generation with code-davinci-002 model, measured using BLEU. p-values are calculated applying one-sided pair-wise Wilcoxon signed-rank test and B-H corrected.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Project Name</th>
<th style="text-align: center;">#of training sample</th>
<th style="text-align: center;">#of test sample</th>
<th style="text-align: center;">Cross-project</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Same-project</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\mathcal{A S A P}$</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\mathcal{A S A P}$</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">wildfly/wildfly</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">24.05</td>
<td style="text-align: center;">24.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.86</td>
<td style="text-align: center;">18.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">orientechnologies/orientdb</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">25.54</td>
<td style="text-align: center;">27.23</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.43</td>
<td style="text-align: center;">20.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ngageoint/geopackage-android</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">29.33</td>
<td style="text-align: center;">42.84</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.48</td>
<td style="text-align: center;">46.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RestComm/jain-slec</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">17.04</td>
<td style="text-align: center;">19.06</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">17.99</td>
<td style="text-align: center;">19.61</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">apache/airflow</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">20.39</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.36</td>
<td style="text-align: center;">20.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">tensorflow/probability</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">21.36</td>
<td style="text-align: center;">21.18</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20.30</td>
<td style="text-align: center;">20.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">h2oai/h2o-3</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">19.50</td>
<td style="text-align: center;">20.72</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.75</td>
<td style="text-align: center;">19.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">chaoss/grimoirelab-perceval</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">25.23</td>
<td style="text-align: center;">29.23</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.75</td>
<td style="text-align: center;">38.23</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of prompt enhanced comment generation with code-davinci-002 model on same project data (measured using BLEU) and p-values are calculated applying one-sided pair-wise Wilcoxon signed-rank test after combining the data from all projects.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;"># of Samples</th>
<th style="text-align: center;">Exact Match (EM)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Edit Similarity (ES)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$\begin{gathered} \mathcal{A S A P} \ \text { (Zero-shot) } \end{gathered}$</td>
<td style="text-align: center;">Gain (\%)</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$\begin{gathered} \mathcal{A S A P} \ \text { (Zero-shot) } \end{gathered}$</td>
<td style="text-align: center;">Gain (\%)</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">9292</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">22.12</td>
<td style="text-align: center;">$+6.6 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">55.35</td>
<td style="text-align: center;">59.66</td>
<td style="text-align: center;">$+7.79 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">6550</td>
<td style="text-align: center;">14.05</td>
<td style="text-align: center;">14.58</td>
<td style="text-align: center;">$+3.77 \%$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">49.71</td>
<td style="text-align: center;">50.12</td>
<td style="text-align: center;">$+0.82 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">15842</td>
<td style="text-align: center;">17.97</td>
<td style="text-align: center;">19.01</td>
<td style="text-align: center;">$+5.79 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">53.01</td>
<td style="text-align: center;">55.72</td>
<td style="text-align: center;">$+5.11 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of $\mathcal{A S A P}$ enhanced prompts with code-davinci-002 model on line completion task.
statistical test. Note that our total sample size for the statistical test exceeds the number of required samples determined through the analysis mentioned in Section 7. When working with the same project, one must split data with care, to avoid leakage from future samples (where desired outputs may already exist) to past ones. Therefore, we sorted the samples by creation dates in this dataset. After generating the dataset, we applied our approach to evaulate the performance in same project setting. We also compared our results with a cross-project setup, where we retrieved samples from the complete cross-project training set, similar to the setting used in Section 4.2.</p>
<p>Table 4 shows the results project-based code summarization. Note that this is a project-specific scenario where data is not available at all. The training data for each project is very limited. We found that, for 4 projects, cross-project few-shot learning yielded the best performance; while, for 4 others, same-project few-shot learning was most effective. We note that Ahmed \&amp; Devanbu didn't
use IR to select few-shot samples and consistently achieved better results with same-project few-shot learning [3]. IR does find relevant examples in the large samples available for Java \&amp; Python, and we get good results. We analyzed 16 pairs of average BLEU from 8 projects, considering both cross-project and same-project scenarios. Our prompt-enhanced few-shot learning outperformed vanilla BM25 retrieved few-shot learning in 14 cases ( $87.5 \%$ ). This suggests that $\mathcal{A S A P}$ prompt enhancement is helpful across projects. $\mathcal{A S A P}$ statistically improves performance in both cross-project and same-project settings.</p>
<h3>4.4 Is $\mathcal{A S A P}$ Model-agnostic?</h3>
<p>Our results so far pertain to the code-davinci-002 models. We also fed $\mathcal{A S A P}$-augmented prompts to the other two models, text-davinci-003 \&amp; gpt-3.5-turbo (chat model). Our findings are in Table 6. Our prompt-enhanced few-shot learning approach improved the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">BM25</th>
<th style="text-align: center;">$\mathcal{A S A P}$</th>
<th style="text-align: center;">Gain</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Java</td>
<td style="text-align: left;">Code-davinci-002</td>
<td style="text-align: left;">23.90</td>
<td style="text-align: center;">$\mathbf{2 5 . 7 8}$</td>
<td style="text-align: center;">$+7.87 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text-davinci-003</td>
<td style="text-align: left;">18.98</td>
<td style="text-align: center;">$\mathbf{2 2 . 3 1}$</td>
<td style="text-align: center;">$+17.54 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Turbo-GPT-3.5</td>
<td style="text-align: left;">16.68</td>
<td style="text-align: center;">$\mathbf{1 6 . 9 6}$</td>
<td style="text-align: center;">$+1.68 \%$</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Python</td>
<td style="text-align: left;">Code-davinci-002</td>
<td style="text-align: left;">22.00</td>
<td style="text-align: center;">$\mathbf{2 4 . 7 8}$</td>
<td style="text-align: center;">$+12.64 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text-davinci-003</td>
<td style="text-align: left;">16.74</td>
<td style="text-align: center;">$\mathbf{1 8 . 9 3}$</td>
<td style="text-align: center;">$+13.08 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Turbo-GPT-3.5</td>
<td style="text-align: left;">15.01</td>
<td style="text-align: center;">$\mathbf{1 6 . 3 8}$</td>
<td style="text-align: center;">$+9.13 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;">PHP</td>
<td style="text-align: left;">Code-davinci-002</td>
<td style="text-align: left;">28.42</td>
<td style="text-align: center;">$\mathbf{3 3 . 5 2}$</td>
<td style="text-align: center;">$+17.95 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text-davinci-003</td>
<td style="text-align: left;">21.67</td>
<td style="text-align: center;">$\mathbf{2 5 . 7 2}$</td>
<td style="text-align: center;">$+18.69 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Turbo-GPT-3.5</td>
<td style="text-align: left;">18.48</td>
<td style="text-align: center;">$\mathbf{1 9 . 9 9}$</td>
<td style="text-align: center;">$+8.17 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance on code summarization, measured using BLEU. p-values are calculated applying one-sided pairwise Wilcoxon signed-rank test and B-H corrected.
performance of the gpt-3.5-turbo model by $1.68 \%$ to $9.13 \%$ and test-davinci-003 model by $13.08 \%$ to $18.69 \%$ on 500 samples each from Java, Python, PHP.</p>
<p>Gpt-3.5-turbo does worse than the code-davinci-002 and text-davinci-003 models at code summarization. The Turbo version is verbose and produces comments stylistically different from those written by developers, and also from the few-shot exemplars in the prompt. Careful prompt-engineering might improve the turbo model and enable it to generate more natural, brief comments; this is left for future work. This underperformance by the chat model is consistent with the findings by Kocon et al. [39]. Text-davinci-003 model showed the maximum performance increase (albeit still outdone by code-davinci-002). Note that text-davinci-003 is a completion model, like code-davinci-002. Our findings suggest that $\mathcal{A S A P}$ is more effective with completion models than chat models. We also conducted pairwise one-sided Wilcoxon signed rank tests, and the statistical significance of our findings (except java with gpt-3.5-turbo) suggests that $\mathcal{A S A P}$ will apply beyond just the original code-davinci-002 model.</p>
<h3>4.5 ASAP for Completion</h3>
<p>Our primary focus so far has been on code-summarization, in a few-shot setting. Here, we explore if $\mathcal{A S A P}$ works on another task: code completion, in a zero-shot setting where no example is shown or presented to the model. We assessed the value of including semantic facts for the line completion task, where the model generates the next line given the prior line. We uniformly and randomly collected 9292 Java and 6550 Python samples from the CodeSearchNet dataset to conduct our evaluation. We randomly selected a line for each sample and tasked the model with generating that line, given just all the preceding lines. While applying $\mathcal{A S A P}$, we append the repository information and other semantic facts (i.e., tagged identifiers, DFG) before the preceding lines. Importantly, when generating tagged identifiers and DFG, we only used partial information from preceding lines to avoid information leakage from later lines to the target lines.</p>
<p>We used two metrics, Exact Match (EM) and Edit Similarity (ES), in line with the CodeXGLUE benchmark, to measure the model's performance. We conducted a McNemar test for EM and a pairwise Wilcoxon sign-rank test to evaluate the model's performance, similar to what we performed for code summarization. Table 5 summarizes our findings. We observe an overall $5.79 \%$ gain in Exact Match (EM) and a $5.11 \%$ gain in Edit Similarity (ES), highlighting</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">Prompt Component</th>
<th style="text-align: left;">BLEU-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Java</td>
<td style="text-align: left;">ALL</td>
<td style="text-align: left;">$\mathbf{2 5 . 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-Repo.</td>
<td style="text-align: left;">23.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-Id</td>
<td style="text-align: left;">25.27</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-DFG</td>
<td style="text-align: left;">24.86</td>
</tr>
<tr>
<td style="text-align: left;">Python</td>
<td style="text-align: left;">ALL</td>
<td style="text-align: left;">$\mathbf{2 4 . 2 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-Repo.</td>
<td style="text-align: left;">22.80</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-Id</td>
<td style="text-align: left;">23.93</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">-DFG</td>
<td style="text-align: left;">23.31</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study.
the effectiveness of incorporating semantic facts. For Python, we find statistical significance only for ES improvement, not for EM.</p>
<h3>4.6 Performance on Other Metrics</h3>
<p>In addition to BLEU-CN, we measured performance with 3 other metrics; BLEU-DC, ROUGE-L and METEOR. Our results, in Table 10, shows average gains with $\mathcal{A S A P}$ on all three metrics. We conducted pairwise one-sided Wilcoxon signed-rank tests and found significant performance improvements with BLEU-DC and ROUGE-L for all the languages. However, we did not observe significant differences with METEOR for 4 out of 6 languages, though sample averages do improve with $\mathcal{A S A P}$ in all 6 comparisons. It's worth noting that we had only 1000 language samples (due to cost) for each language, so it's not unexpected to see some cases where we didn't observe significance. To evaluate the overall impact of $\mathcal{A S A P}$, we combined the dataset from all languages for code-davinci-002 model ( 6000 samples) and performed the same test; we then get statistical significance ( $p$-value $&lt;0.01$ ) for all three metrics, suggesting that $\mathcal{A S A P}$ does provide value.</p>
<h2>5 DISCUSSION AND ABLATION STUDY</h2>
<p>We now present an ablation study of $\mathcal{A S A P}$ 's design and the particular semantic facts our instantiation of $\mathcal{A S A P}$ uses before comparing $\mathcal{A S A P}$ 's output to our vanilla BM25 baseline. The primary aim of an ablation study is to gauge the contribute of each aspect of a model to the final observed performance In our study, we removed each semantic component of the enhanced prompt and observed performance. We found that the Repo. component contributes most to the model's performance (Table 7) both for Java and Python. However, tagged identifier and DFG are also helpful, and the best results were obtained when we combined all three components in the prompt.
Two Illustrative Examples When manually examining results, we observed that in several samples, the $\mathcal{A S A P}$ prompt contained information that was crucial for the summary. Table 8 shows two example results that illustrate this point. In the first example, the baseline model failed to generate the term "element-wise". However, our prompted enhanced version capture this important concept, yielding a higher BLEU-4 score of 74.0 compared to the baseline score of 39.0. Similarly, in the second example, the baseline model did not recognize the function as a standalone process, leading to a low BLEU score of 10.0. However, our proposed approach did identify the function as a standalone process, resulting in a higher BLEU score of 33.0 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Example 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">def round(input_a, name: nil) <br> check_allowed_types(input_a, TensorStream::Ops::FLOATING_POINT_TYPES) <br> _op(round, input_a, name: name)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">end</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gold \&amp; model output</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comment</td>
<td style="text-align: center;">BLEU</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rounds the values of a tensor to the nearest integer element - wise</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Round a tensor to the nearest integer</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{A S A P}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rounds the values of a tensor to the nearest integer, element-wise.</td>
<td style="text-align: center;">74</td>
</tr>
<tr>
<td style="text-align: center;">Example 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">public static void main(final String[] args) { <br> loadPropertiesFiles(args); <br> final ShutdownSignalBarrier barrier = new ShutdownSignalBarrier(); <br> final MediaDriver.Context ctx = new MediaDriver.Context(); <br> ctx.termination(Fook(barrier::signal)); <br> try (MediaDriver ignore = MediaDriver.launch(ctx)) <br> } <br> barrier.await(); <br> System.out.println("Shutdown Driver...");</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gold \&amp; model output</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comment</td>
<td style="text-align: center;">BLEU</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Start Media Driver as a stand - alone process .</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Main method that starts the CLR Bridge from Java .</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{A S A P}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Main method for running Media Driver as a standalone process.</td>
<td style="text-align: center;">33</td>
</tr>
</tbody>
</table>
<p>Table 8: Selected examples, illustrating the effectiveness of $\mathcal{A S A P}$ enhancement.</p>
<p>Does the Model Memorize the Path? Of the three semantic facts $\mathcal{A S A P}$ adds to a prompt, repo. information impacts the model's performance most. This may be due to the fact that Code-Davinci002 had memorized the specific file paths in our data during pretraining; when we provide the path to the function, perhaps the model just recalls memorized information? To investigate this question, we change the path representation: we took the repository name and path, split the tokens at "/" , and presented the model with a list of tokens. The main idea behind this approach is to diffuse the original representation, and present the model with something not encountered during pre-training. If the model isn't literally memorizing, its performance should not be impacted. We observed that the differences between both versions were very small. For Java, we gained 0.24 BLEU but, for Python, we lost 0.04 with tokenized paths. This suggests a lower risk that the model memorized the path to the function.
Is the Identifier Tag Necessary? In this paper, we assign roles to the identifiers and tag them as Function Name, Parameters, Identifier etc. in the prompt (See Figure 2). Does this explicit tagging actually help performance? To investigate this question, we compare the model's performance when provided with a plain, "tag-free" list of identifiers. We observed that the tagged identifiers lead to better performance for both Java and Python than a simple tag-free list of identifiers. Our performance metric BLEU increased by 0.41 and 1.22 for Java and Python, respectively, suggesting that explicit semantic information does indeed contribute to better model performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Prompt Enhanced</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Vanilla BM25</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#of shots</td>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;">#of shots</td>
<td style="text-align: center;">BLEU-4</td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">25.41</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">22.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">23.13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">23.20</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">21.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">21.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">21.74</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparing with higher-shots Vanilla BM25.
What's Better: More Shots or ASAP? Despite having billions of parameters, LLMs have limited prompt sizes. For example, codedavinci-002 and gpt-3.5-turbo support allow prompt-lengths of just 4 k tokens. $\mathcal{A S A P}$ augmentation does consume some of the available prompt length budget! Thus we have two design options: 1) use fewer, $\mathcal{A S A P}$-Augmented samples in the prompt or 2) use more few-shot samples sans augmentation. To investigate this, we also tried using 4 and 5 shots (instead of 3) for Java and Python with the code-davinci-002 model. However, Table 9 shows that higher shots using BM25 does not necessarily lead to better performance. With higher shots, there is a chance of introducing unrelated samples, which can hurt the model instead of helping it.</p>
<p>Only for Java did we observe better performance with both 4 and 5 shots compared to our baseline model. However, our proposed technique with just 3-shots still outperforms using BM25 with 5 shots. It's worth noting that the context window of the model is increasing day by day, and the upcoming GPT-4 model will allow us to have up to 32 K tokens ${ }^{8}$. Therefore, the length limit might not be an issue in the near future. However, our study suggests that Automated Semantic Augmentation will still be a beneficial way to use available prompt length budget; moreover, it stands to reason that constructing more signal-rich, informative prompts will beneficial regardless of length.
What's New in $\mathcal{A S A P}$ 's Output? We add a pro forma analysis of a few hand-picked examples, to be consistent with peer-reviewrequired community rituals; however, these analyses are highly anecdotal must be interpreted cautiously. We manually examine several samples to discuss our results in greater detail; specifically, to answer three questions: to specify 1) the new types of information $\mathcal{A S A P}$ presents to the LLM and 2) how $\mathcal{A S A P}$ 's summaries differ from those created by existing techniques, and 3) to analyze the errors that $\mathcal{A S A P}$ introduces. Table 11 presents some samples where, for the first three, $\mathcal{A S A P}$ performed very well compared to our retrieval-based baselines, and for the second three, the baseline performed better than $\mathcal{A S A P}$. While we discuss our findings in the context of the provided samples, our observations generalise to other samples.
The new types of information $\mathcal{A S A P}$ presents to the LLM: As discussed in the paper, our primary contribution involves augmenting retrieved samples (retrieved using BM25, as per Nashid et al. [48]) with semantic facts, resulting in improved performance compared to the base retrieval approach. We add semantic facts related to repository details, identifiers, and data flow graphs to both retrieved samples and input code. As anticipated, the added semantic facts transfer into, and enhance, the model output.</p>
<p>In the first sample, the baseline retrieval-only method fails to capture the term "gradient" entirely. However, by incorporating</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">BLEU-DC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">ASAP</td>
<td style="text-align: center;">Gain (\%)</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">ASAP</td>
<td style="text-align: center;">Gain (\%)</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">ASAP</td>
<td style="text-align: center;">Gain (\%)</td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">14.09</td>
<td style="text-align: center;">15.94</td>
<td style="text-align: center;">$+13.13 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">36.85</td>
<td style="text-align: center;">38.41</td>
<td style="text-align: center;">$+4.23 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">35.66</td>
<td style="text-align: center;">36.10</td>
<td style="text-align: center;">$+1.23 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">12.63</td>
<td style="text-align: center;">14.49</td>
<td style="text-align: center;">$+14.73 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">35.32</td>
<td style="text-align: center;">37.74</td>
<td style="text-align: center;">$+6.85 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">33.05</td>
<td style="text-align: center;">35.63</td>
<td style="text-align: center;">$+7.81 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Ruby</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">11.01</td>
<td style="text-align: center;">$+20.2 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">28.19</td>
<td style="text-align: center;">30.55</td>
<td style="text-align: center;">$+8.37 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">27.65</td>
<td style="text-align: center;">29.20</td>
<td style="text-align: center;">$+5.61 \%$</td>
</tr>
<tr>
<td style="text-align: center;">JavaScript</td>
<td style="text-align: center;">14.89</td>
<td style="text-align: center;">16.71</td>
<td style="text-align: center;">$+12.22 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">32.28</td>
<td style="text-align: center;">33.88</td>
<td style="text-align: center;">$+4.96 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">32.08</td>
<td style="text-align: center;">33.02</td>
<td style="text-align: center;">$+2.93 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">17.10</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">$+8.60 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">41.04</td>
<td style="text-align: center;">42.43</td>
<td style="text-align: center;">$+3.39 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">36.78</td>
<td style="text-align: center;">37.26</td>
<td style="text-align: center;">$+1.31 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">16.97</td>
<td style="text-align: center;">20.63</td>
<td style="text-align: center;">$+21.57 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">40.48</td>
<td style="text-align: center;">44.90</td>
<td style="text-align: center;">$+10.92 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">40.14</td>
<td style="text-align: center;">43.35</td>
<td style="text-align: center;">$+8.00 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">14.14</td>
<td style="text-align: center;">16.23</td>
<td style="text-align: center;">$+14.78 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">35.69</td>
<td style="text-align: center;">37.99</td>
<td style="text-align: center;">$+6.44 \%$</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">34.23</td>
<td style="text-align: center;">35.76</td>
<td style="text-align: center;">$+4.47 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: The effectiveness of ASAP in popular code summarization metrics. p-values are calculated applying one-sided pair-wise Wilcoxon signed-rank test and B-H corrected.</p>
<p>| Change (BLEU-4) | Reference | BM25 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<h3>6.2 Other Datasets</h3>
<p>There are several datasets available for code summarization, in addition to CodeXGLUE [47]. TL-CodeSum [30] is a relatively smaller dataset, with around 87 K samples, but it does include duplicates, which may result in high performance estimates that may not generalize. Funcom [41] is a dedicated dataset with 2.1 million Java functions, but contains duplicates. We chose CodeXGLUE (derived from CodeSearchNet) because it is a diverse, multilingual dataset that presents a challenge for models. Even well-trained models like CodeBERT struggle on this benchmark; its performance is particularly poor on languages with fewer training samples.</p>
<p>There has been a lot of work on code summarization, ranging from template matching to few-shot learning. These models use different representations and sources of information to perform well in code summarization. Comparing or discussing all of these models is beyond the scope of this work. We note, however, that our numbers represent a new high-point on the widely used CodeXGlue benchmark for code summarization and code-completion; we refer the reader to https://microsoft.github.io/CodeXGLUE/ for a quick look at the leader-board. Our samples are smaller ( $\mathrm{N}=1000$ ), but the estimates, and estimated improvements, are statistically robust (See the sample size discussion in Section 7).</p>
<h3>6.3 LLMs in Software Engineering</h3>
<p>Although LLMs are not yet so widely used for code summarization, they are extensively used for code generation [14, 49, 67] and program repair [5, 18, 35, 36]. Models like Codex aim to reduce the burden on developers by automatically generating code or completing lines. Several models such as Polycoder [67] and Codegen [49] perform reasonably well, and due to their few-shot learning or prompting, they can be applied to a wide set of problems. However, Code-davinci-002 model generally performs well than those models and allows us to fit our augmented prompts into a bigger window.</p>
<p>Jain et al. proposed supplementing LLM operation with subsequent processing steps based on program analysis and synthesis techniques to improve performance in program snippet generation [34]. Bareiß et al. showed the effectiveness of few-shot learning in code mutation, test oracle generation from natural language documentation, and test case generation tasks [10]. CODAMOSA [42], an LLM-based approach, conducts search-based software testing until its coverage improvements stall, then asks the LLM to provide example test cases for functions that are not covered. By using these examples, CODAMOSA helps redirect search-based software testing to more useful areas of the search space. Jiang et al. evaluated the effectiveness of LLMs for the program repair problem [35].</p>
<p>Retrieving and appending a set of training samples has been found to be beneficial for multiple semantic parsing tasks in NLP, even without using LLM [68]. One limitation of this approach is that performance can be constrained by the availability of similar examples. Nashid et al. used a similar approach and gained improved performance in code repair and assertion generation with the help of LLM [48]. However, none of the above works has attempted to automatically semantically augment the prompt. Note that it is still too early to comment on the full capabilities of these large language models. Our findings so far suggest that augmenting the exemplars in the prompt with semantic hints helps on the code summarization
and code completion tasks; judging the value of $\mathcal{A} S A P$ in other tasks is left for future work.</p>
<h2>7 THREATS \&amp; LIMITATIONS</h2>
<p>A major concern when working with large language models is the potential for test data exposure during training. Sadly, one can't directly check this since the training dataset is not accessible. The model's lower performance with random few-shotting suggests that memorization may not be a major factor. As we incorporate relevant information, the model's performance improves with the amount and quality of information. Had the model already memorized the summaries, it could have scored much higher, even without the benefit of relevant exemplars and semantic augmentation.</p>
<p>Sample Size Analysis: We used the observed means and standard deviations to calculate (using G*power [19, 20]) the required sample sizes, using commonly used values: $\sigma$ of 0.01 (desired p-value) and a $\beta$ of 0.20 ( viz, a $20 \%$ chance of NOT discovering an effect, should one exist). For the tests that we used (Wilcoxon Signed-rank test), we found that the needed sample size was always below the sample size we used for our primary studies, viz., 1000.
User Study: We did not conduct a user study for $\mathcal{A S A P}$. Thus, the enhancements in metrics presented here may not necessarily translate into improved developer performance. This aspect is left to future work.</p>
<p>Finally: fine-tuning large LMs to use derived semantic facts may improve on our augmented prompting approach, but would be costly. We will leave its consideration to future research.</p>
<h2>8 CONCLUSION</h2>
<p>In this paper, we explored the idea of Automatic Semantic Augmentation of Prompts, whereby we propose to enhance few-shot samples in LLM prompts, with tagged facts automatically derived by semantic analysis. This based on an intuition that human developers often scan the code to implicitly extract such facts in the process of code comprehension leading to writing a good summary. While it is conceivable that LLMs can implicitly infer such facts for themselves, we conjectured that adding these facts in a formatted style to the exemplars and the target, within the prompt, will help the LLM organize it's "chain of thought" as it seeks to construct a summary. We evaluated this idea a challenging, de-duplicated, wellcurated CodeSearchNet dataset, on two tasks: code summarization and code completion. Our findings indicate that Automated Semantic Augmentation of Prompts is generally helpful. Our estimates suggest it helps surpass state-of-the-art.</p>
<p>Acknowledgements: We would like to acknowledge National Science Foundation under Grant NSF CCF (SHF-MEDIUM) No. 2107592 and the Intelligence Advanced Research Projects Agency (IARPA) under contract W911NF20C0038 for partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors and no official endorsement should be inferred.</p>
<h2>REFERENCES</h2>
<p>[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 49985007 .</p>
<p>[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655-2668.
[3] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. In 37th IEEE/ACM International Conference on Automated Software Engineering. 1-5.
[4] Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for software engineering. In Proceedings of the 44th International Conference on Software Engineering. 1443-1455.
[5] Toufique Ahmed and Premkumar Devanbu. 2023. Better patching using LLM prompting, via Self-Consistency. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1742-1746.
[6] Toufique Ahmed, Sqintyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, and Saravan Rajmohan. 2023. Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models. ICSE (2023).
[7] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. 143-153.
[8] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400 (2018).
[9] Salanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65-72.
[10] Patrick Bureill, Beatrix Souza, Marcelo d'Amorim, and Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022).
[11] Lionel C Briand. 2003. Software documentation: how much is enough?. In Seventh European Conference onSoftware Maintenance and Reengineering, 2003. Proceedings, IEEE, 13-15.
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[13] Boxing Chen and Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentence-level BLEU. In Proceedings of the ninth workshop on statistical machine translation. 362-367.
[14] Mark Chen, Jerry Tworek, Horwoo Jun, Qining Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[16] Samanta Dey, Venkatesh Vinayakarao, Monika Gupta, and Sampath Dechu. 2022. Evaluating commit message generation: to BLEU or not to BLEU?. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 31-35.
[17] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. 2013. Evaluating source code summarization techniques: Replication and expansion. In 2013 21st International Conference on Program Comprehension (ICPC). IEEE, $13-22$.
[18] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, and Shin Hwei Tan. 2022. Automated Repair of Programs from Large Language Models. ICSE.
[19] Franz Faul, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. Statistical power analyses using G<em> Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 1149-1160.
[20] Franz Faul, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007. G</em> Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods 39, 2 (2007), 175-191.
[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A PreTrained Model for Programming and Natural Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. $1536-1547$.
[22] Andrew Forward and Timothy C Lethbridge. 2002. The relevance of software documentation, tools and technologies: a survey. In Proceedings of the 2002 ACM symposium on Document engineering. 26-33.
[23] Jianfeng Gao and Xiaodong He. 2013. Training MRF-based phrase translation models using gradient ascent. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 430-459.
[24] David Gros, Hariharan Seshiyan, Prem Devanbu, and Zhou Yu. 2020. Code to Comment ?Translation?: Data, Metrics, Baselining \&amp; Evaluation. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE).</p>
<p>IEEE, 746-757.
[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. In International Conference on Learning Representations.
[26] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering Volume 2. 223-226.
[27] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. In 2010 17th Working Conference on Reverse Engineering. IEEE, 35-44.
[28] Sakib Haque, Zachary Eberhart, Aakash Bansal, and Collin McMillan. 2022. Semantic similarity metrics for evaluating source code summarization. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. $36-47$.
[29] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th conference on program comprehension. 200210 .
[30] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing source code with transferred API knowledge. In Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2269-2275.
[31] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large Language Models: A Survey. arXiv preprint arXiv:2212.10403 (2022).
[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearcher's fullenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).
[33] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073-2083.
[34] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. In Proceedings, 44th ICSE. 1219-1231.
[35] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of Code Language Models on Automated Program Repair. ICSE (2023).
[36] Harshit Joshi, José Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, and Gust Verbruggen. 2022. Repair is nearly generation: Multilingual program repair with llms. arXiv preprint arXiv:2208.11640 (2022).
[37] Hassan Kase, Mohammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, and Mohamed Coulibali. 2020. NUBIA: NeUral Based Interchangeability Assessor for Text Generation. arXiv:2004.14667 [ca.CL].
[38] Sungmin Kang, Juyoon Yoon, and Shin Yoo. 2023. Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction. ICSE (2023).
[39] Jan Kocon, Igor Cichecki, Olivier Karayca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.
[40] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11956 (2022).
[41] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, $795-806$.
[42] Caroline Lemieux, Jeevana Priya Inala, Shorenshi K Lahiri, and Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained Large Language Models. In 45th International Conference on Software Engineering, ser. ICSE.
[43] Chin-Tew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74-81.
[44] Chin-Tew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. 501-507.
[45] Yinhan Liu, Myle Ott, Naman Goyal, Jinglei Du, Mandar Joshi, Dansj Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[46] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajuani, and Jan Vitek. 2017. DéjàVu: a map of code duplicates on GitHub. Proceedings of the ACM on Programming Languages 1, OOPALA (2017), 1-28.
[47] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).
[48] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. In Proceedings, 45th ICSE.
[49] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language</p>
<p>model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).
[50] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.
[51] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with Language Model Prompting: A Survey. arXiv preprint arXiv:2212.09597 (2022).
[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).
[53] AlecRadford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1,8 (2019), 9 .
[54] Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, Vol. 242. Citeseer, 29-48.
[55] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333-389.
[56] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, and Sidney D'Mello. 2014. Improving automated source code summarization via an eyetracking study of programmers. In Proceedings of the 36th international conference on Software engineering. 390-401.
[57] Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing automatic evaluation metrics for code summarization tasks. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1105-1116.
[58] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020).
[59] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, and Hongbin Sun. 2023. On the evaluation of neural code summarization. In Proceedings of the 44th International Conference on Software Engineering. $1597-1608$.
[60] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2022. Repositorylevel prompt generation for large language models of code. arXiv preprint
arXiv:2206.12839 (2022).
[61] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K VijayShanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM international conference on Automated software engineering. 43-52.
[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.
[63] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codef5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).
[64] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifieraware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696-8708.
[65] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual task of code summarization. Advances in neural information processing systems 32 (2019).
[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).
[67] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, 1-10.
[68] Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Panapat, Peter Shaw, Linlu Qiu, Sumit Sanghai, and Fei Sha. 2022. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing. arXiv preprint arXiv:2209.14899 (2022).
[69] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1385-1397.
[70] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://platform.openai.com/docs/models/gpt-4&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://openai.com/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-3dd61d97827e3f380bf9304101149a3f865051fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3dd61d97827e3f380bf9304101149a3f865051fc" target="_blank">Injecting Numerical Reasoning Skills into Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work shows that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.</p>
                <p><strong>Paper Abstract:</strong> Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GENBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GENBERT (Generative BERT for Numerical Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-initialized encoder-decoder Transformer that generates answers token-by-token (including numbers) and is pre-trained on synthetic numerical (ND) and textual (TD) data to inject numerical reasoning skills into a general-purpose LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer initialized from BERT-base-uncased with tied encoder/decoder weights, added encoder/decoder-specific FFNs, digit-level tokenization for numbers, and a generative decoder producing answers token-by-token. Pre-trained on 1M synthetic numerical (ND) and 2.5M synthetic textual (TD) examples in a multi-task setup including the BERT MLM objective.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Signed float combinations (up to 4 terms), min/max/avg, argmin/argmax, date min/max, date difference, percentage computations; arithmetic expressed both in short expressions (ND) and within word-problem style passages (TD); evaluated on DROP (reading-comprehension numeric QA) and MAWPS subsets (ADDSub, SOP, SEQ).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns to perform arithmetic internally by generating numeric tokens sequentially from a decoder that attends to encoded contextual representations; digit tokenization reduces the tokenization barrier to learn arithmetic; numerical skills are acquired via supervised pre-training on synthetic numeric/textual examples (learning algorithmic-like patterns rather than an external symbolic evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GENBERT achieves >96% accuracy on the synthetic ND and TD pretraining tasks; pretraining ND and/or TD raises DROP performance substantially (example: F1 from 49.3 -> 72.3 reported in abstract), and enables zero-shot transfer to filtered MWP datasets (e.g., ADDSub EM 22.8, SOP EM up to 28.3); ablations show digit tokenization is crucial (wordpiece tokenization fails ND task), and removing MLM loss or random-shift harms generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite learning numeric skills, GENBERT still struggles on expressions with >3 terms and complex numeric reasoning (performance drops sharply beyond 3-term arithmetic); error analysis shows many failures are due to skills not covered by synthetic pretraining (sorting, complex semantics) or imprecise numeric predictions, indicating the learned mechanism is not fully algorithmic nor universally generalizable.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pre-training on synthetic numerical data (ND) and textual numerical problems (TD); digit-level tokenization; random positional shift augmentation (RS); multi-tasking with original MLM objective to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Pre-training on ND and TD dramatically improved arithmetic performance on downstream tasks: DROP F1 from 49.3 to 72.3 (abstract); GENBERT+ND and GENBERT+TD both improved EM and F1 on DROP (e.g., ND+TD yields EM ~68.8 and high F1); ND pretraining speeds convergence on TD; digit tokenization drastically reduces sample complexity and is required to learn ND tasks; MLM multi-tasking prevents language understanding degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On DROP: abstract reports F1 improvement 49.3 -> 72.3 after pretraining; in-text results show GENBERT baseline EM 46.1 / F1 49.3, GENBERT+ND EM ~64.7, GENBERT+TD EM ~64.4, GENBERT+ND+TD EM ~68.8; breakdown: number-answer F1 up to 75.2 (GENBERT+ND); zero-shot MWP EM: ADDSub 22.8, SOP 28.3 (best), SEQ 22.3; SQuAD (non-numeric RC) remains similar to BERT: EM ~81.3, F1 ~88.6. Pretraining achieves >96% accuracy on synthetic ND/TD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on arithmetic expressions involving more than ~3 terms (near-zero generalization beyond this); misses complex reasoning types not covered by synthetic tasks (sorting, certain counting variants); produces imprecise numeric outputs (partial-digit matches); struggles with multi-span list answers (model outputs single-span/generative answers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared to symbolic or specialized modules (e.g., models that enumerate/extract arithmetic expressions and evaluate externally), GENBERT performs competitively on DROP but lags on some MWP benchmarks; authors argue symbolic approaches require non-differentiable search and are less flexible for integrated text+numeric outputs, while GENBERT performs computations internally albeit with limited generalization to arbitrarily complex arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid extractive QA w/ numeric heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid extractive QA models augmented with specialized count and arithmetic heads (e.g., count head, arithmetic head)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing NRoT approach that uses a pre-trained LM encoder (e.g., BERT) and multiple answer heads for spans, counts (0-9), and arithmetic combinations of text-extracted numbers; a type head marginalizes over head choices during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hybrid extractive QA (count + arithmetic heads)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture: pretrained LM computes contextual representations which feed into several small FFN 'heads' that predict context-span answers, question-span answers, a categorical count (0..9), or signed combinations of pre-identified numbers in the context; a type head predicts head probabilities and training marginalizes over heads.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting up to small fixed bound (e.g., up to 9), addition/subtraction of a few numbers drawn from context (signed combinations), extraction of arithmetic expressions that correspond to answers in RC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Performs arithmetic symbolically by restricting output types and delegating computation to specialized heads (sometimes combined with external evaluation of enumerated expressions); uses discrete representations of numbers (pre-identified tokens) rather than generating arbitrary numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>These models achieve state-of-the-art results on DROP for comparable sizes (e.g., MTMSN), showing that constrained symbolic heads are effective for datasets whose operations fall within supported types; they improve performance especially on multi-span answers and limited arithmetic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limited flexibility: can only handle pre-defined numeric ranges (counts up to 9) and restricted arithmetic (2-3 numbers), scaling up requires non-differentiable search over exponential expression spaces and bespoke architecture changes; cannot internally combine computation with subsequent span extraction if computation was delegated externally.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Specialized architectural modules (count head, arithmetic head) and marginalization over expression decompositions during training; in some related works, an external symbolic calculator is used.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves accuracy on benchmarks like DROP where supported operations match dataset needs (e.g., MTMSN base achieves higher EM on certain MWP datasets than GENBERT), but at expense of generality and requiring architecture changes to support more operations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper as comparative baselines: MTMSN_BASE EM on some MWP testsets: ADDSub 32.2, SOP 28.0, SEQ 32.5 (higher than GENBERT variants on those filtered MWP sets); MTMSN_BASE EM on DROP (as reported) ~68.2 EM (comparable to GENBERT+ND+TD EM ~68.8).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Unable to scale to arbitrary computations (exponential expression space) without non-differentiable search; limited count range and limited number of operands; difficulty integrating internal arithmetic with textual span extraction when the final answer requires mapping a computed numeric value back to text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>These approaches are closer to symbolic computation than distributed generation: they explicitly enumerate or represent arithmetic expressions and (sometimes) use external evaluation, making them more algorithmic for supported classes but brittle beyond them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic/semantic-parsing approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic program / semantic parsing approach for numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that maps text to discrete symbolic programs or expressions which are then executed (often by an external calculator) to produce numeric answers; common in earlier arithmetic word-problem work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Symbolic semantic parser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Maps question+context to a discrete symbolic expression/program (e.g., arithmetic expression) which is executed to obtain a numeric result; training typically involves search over discrete program/expression space and sometimes uses syntactic supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Full symbolic arithmetic expressed as arithmetic programs (addition, subtraction, multiplication, division), structured equation solving for algebraic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Performs arithmetic by constructing explicit symbolic expressions and executing them externally—i.e., true algorithmic/symbolic computation rather than distributed numeric generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prior literature and cited works demonstrate success on certain classes of math word problems when correct program induction is possible; symbolic outputs are exact when program is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Training/search is difficult because program space grows combinatorially, requiring non-differentiable search and making end-to-end optimization hard; not well-suited when final answer is a text span requiring integration between numeric computation and text extraction (the paper argues this as limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Use of external symbolic calculator/executor and program induction techniques (semantic parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables exact arithmetic and complex operations when programs can be correctly induced, but complicates training and reduces model flexibility to jointly produce text and computed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Search and optimization difficulties due to huge discrete space; inability to jointly model computations that must be combined with text-span extraction without special handling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Closest to symbolic human-like algorithmic computation (exact arithmetics) but operationally distinct from distributed neural generation; contrasted in paper with GENBERT's end-to-end differentiable approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3014.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3014.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit Tokenization (DT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-level tokenization for numeric tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that splits numbers into individual digit tokens (e.g., '517' -> '5','1','7') rather than treating entire numbers as single wordpiece tokens, to ease learning of numeric value composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT (applied intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Digit Tokenization is applied on top of GENBERT/BERT tokenization: numeric wordpiece tokens composed only of digits are further split into digit-level pieces and represented independently.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Facilitates learning of numeric composition for signed float combinations and other numeric outputs that must be produced token-by-token.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Reduces lexical sparsity of numeric tokens and makes value composition (learning mapping from digit sequences to numeric values and operations on them) easier for the model; effectively provides a more convenient subtoken alphabet for numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation shows ND-LM-DT (wordpiece tokenization without digit tokenization) fails to learn the ND tasks (no convergence), while using digit tokenization substantially improves sample complexity and enables the model to learn numeric operations; highlighted in §3 and §5.1.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None presented in paper; however, digit tokenization requires careful handling for very large numeric ranges or formatted numbers (not discussed here).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tokenization change (preprocessing): splitting numeric tokens into digit-level tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatically improved learning on ND pretraining (enabled convergence), reduced sample complexity for numeric tasks, and is necessary for GENBERT to learn synthetic numeric templates effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/ablation: models without DT (ND-LM-DT) did not learn ND tasks; models with DT achieved >96% accuracy on ND/TD pretraining and downstream gains on DROP. No single numeric-only accuracy number for DT alone beyond these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not discussed in detail; digit tokenization alone does not solve higher-level reasoning or multi-term generalization limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>DT is an engineering intervention to make distributed models' input representation numerically friendly, unlike symbolic approaches which operate on numeric semantics explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3014.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3014.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic Pretraining (ND and TD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining on Synthetic Numerical Data (ND) and Textual Data (TD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task pretraining regimen using large synthetic corpora: ND (1M short numeric expressions covering signed floats, min/max/avg, dates, percentages) and TD (2.5M generated passages + questions based on templated math-word-problem grammar) to teach numeric operations and how they are expressed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT (pretraining intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two synthetic corpora were automatically generated: ND consisting of numeric expressions and their solutions (up to 4-term combos, floats, dates, percentages), and TD consisting of templated passages instantiated to maintain world state plus numeric questions; GENBERT is pre-trained on both datasets jointly with the MLM objective.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Short explicit arithmetic expressions (signed float combinations), statistical ops (min/max/avg), argmax/argmin, date arithmetic (differences, min/max), percentage complements, and passage-level numeric reasoning (tracking counts, differences, sums) analogous to DROP-style questions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Supplies supervised examples of numeric operations in isolation (ND) and embedded in textual contexts (TD), allowing the model to internalize mappings from token patterns and context to numeric computation outputs and to learn to generate numeric tokens end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GENBERT trained on ND and TD achieves >96% accuracy on the synthetic tasks and large gains on DROP (e.g., abstract F1 49.3 -> 72.3), faster convergence on TD when ND pretraining is included, and improved zero-shot transfer to filtered MWP datasets; ablations show both ND and TD are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite success, pretraining on these synthetic tasks does not cover all numeric reasoning types; models still fail frequently on tasks outside the synthetic curricula (sorting, >3-term expressions), showing limits to generalization from the synthetic distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Large-scale supervised pretraining on automatically-generated synthetic numeric expressions (ND) and templated textual numeric passages/questions (TD), combined with MLM multi-tasking.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatic improvement in arithmetic-capable downstream performance: DROP F1 increases substantially, EM increases from ~46 to mid/upper-60s; improved zero-shot performance on select MWP datasets; preserved SQuAD performance when MLM included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pretraining results: >96% accuracy on synthetic ND and TD tasks; downstream DROP: reported abstract F1 increase 49.3 -> 72.3, GENBERT+ND EM ~64.7, GENBERT+TD EM ~64.4, GENBERT+ND+TD EM ~68.8; MWP zero-shot EMs (ADDSub 22.8, SOP 28.3, SEQ 22.3); SQuAD remains comparable to BERT (EM ~81.3, F1 ~88.6).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Pretraining coverage gaps lead to failures on reasoning types not included in the templates (sorting, some complex semantics), limited generalization to expressions with more operands/complex algebraic structure, and occasional imprecise numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>This is a data-driven approach to teach models arithmetic-like skills, trading off the exactness and algorithmic guarantees of symbolic methods for end-to-end differentiability and integration with natural language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3014.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3014.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Shift (RS) & MLM multi-tasking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random positional shift augmentation and inclusion of Masked LM loss during pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two training interventions: RS randomly offsets absolute positional embeddings to prevent position-overfitting on short synthetic inputs; MLM multi-tasking retains original BERT masked-language-objective during synthetic pretraining to avoid catastrophic forgetting of language knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT (training interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Random Shift: when input shorter than maximum sequence length, randomly shift position IDs to discourage reliance on absolute positions. MLM multi-tasking: include BERT masked language modeling loss alongside generative ND/TD losses with weighting lambda.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Supports robustness of learning arithmetic on short explicit expressions (ND) and textual passages (TD) by preventing overfitting to artifact positions and preserving general language representations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>RS forces the model to learn content-based (not position-based) numeric computations; MLM prevents catastrophic forgetting and preserves linguistic capabilities necessary to interpret textual contexts that express numeric operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablations: removing MLM (ND-LM) reduces DROP performance and harms SQuAD performance; removing RS (ND-LM-RS) also lowers downstream EM, indicating RS contributes to robustness for short numeric inputs. ND-LM-DT (no DT) fails, underscoring importance of combined interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct contradiction presented, but interventions do not eliminate remaining failure modes (complex arithmetic >3 terms etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Data augmentation for positional IDs (RS) and multi-task objective combining MLM with synthetic-task losses (MLM).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Including MLM during synthetic pretraining preserves SQuAD RC accuracy and improves downstream numeric QA; RS improves generalization on short-expression ND tasks and final DROP EM/F1 relative to variants without RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation notes: GENBERT+ND-LM and GENBERT+ND-LM-RS perform worse than GENBERT+ND (with MLM and RS); specific EM/F1 differences reported across tables (e.g., lower EM when MLM omitted; exact numeric deltas in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even with RS and MLM, model generalization is limited by the synthetic curricula's coverage; RS does not address complexity-generalization limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>These are pragmatic training augmentations to improve learned distributed arithmetic behavior, rather than algorithmic changes akin to symbolic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Analysing mathematical reasoning abilities of neural models. <em>(Rating: 2)</em></li>
                <li>Do nlp models know numbers? probing numeracy in embeddings. <em>(Rating: 2)</em></li>
                <li>A multi-type multi-span network for reading comprehension that requires discrete reasoning. <em>(Rating: 2)</em></li>
                <li>Giving bert a calculator: Finding operations and arguments with reading comprehension. <em>(Rating: 2)</em></li>
                <li>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. <em>(Rating: 2)</em></li>
                <li>Deep learning for symbolic mathematics. <em>(Rating: 1)</em></li>
                <li>MathQA: Towards interpretable math word problem solving with operation-based formalisms. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3014",
    "paper_id": "paper-3dd61d97827e3f380bf9304101149a3f865051fc",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GENBERT",
            "name_full": "GENBERT (Generative BERT for Numerical Reasoning)",
            "brief_description": "A BERT-initialized encoder-decoder Transformer that generates answers token-by-token (including numbers) and is pre-trained on synthetic numerical (ND) and textual (TD) data to inject numerical reasoning skills into a general-purpose LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GENBERT",
            "model_description": "Encoder-decoder Transformer initialized from BERT-base-uncased with tied encoder/decoder weights, added encoder/decoder-specific FFNs, digit-level tokenization for numbers, and a generative decoder producing answers token-by-token. Pre-trained on 1M synthetic numerical (ND) and 2.5M synthetic textual (TD) examples in a multi-task setup including the BERT MLM objective.",
            "arithmetic_task_type": "Signed float combinations (up to 4 terms), min/max/avg, argmin/argmax, date min/max, date difference, percentage computations; arithmetic expressed both in short expressions (ND) and within word-problem style passages (TD); evaluated on DROP (reading-comprehension numeric QA) and MAWPS subsets (ADDSub, SOP, SEQ).",
            "reported_mechanism": "Learns to perform arithmetic internally by generating numeric tokens sequentially from a decoder that attends to encoded contextual representations; digit tokenization reduces the tokenization barrier to learn arithmetic; numerical skills are acquired via supervised pre-training on synthetic numeric/textual examples (learning algorithmic-like patterns rather than an external symbolic evaluator).",
            "evidence_for_mechanism": "GENBERT achieves &gt;96% accuracy on the synthetic ND and TD pretraining tasks; pretraining ND and/or TD raises DROP performance substantially (example: F1 from 49.3 -&gt; 72.3 reported in abstract), and enables zero-shot transfer to filtered MWP datasets (e.g., ADDSub EM 22.8, SOP EM up to 28.3); ablations show digit tokenization is crucial (wordpiece tokenization fails ND task), and removing MLM loss or random-shift harms generalization.",
            "evidence_against_mechanism": "Despite learning numeric skills, GENBERT still struggles on expressions with &gt;3 terms and complex numeric reasoning (performance drops sharply beyond 3-term arithmetic); error analysis shows many failures are due to skills not covered by synthetic pretraining (sorting, complex semantics) or imprecise numeric predictions, indicating the learned mechanism is not fully algorithmic nor universally generalizable.",
            "intervention_type": "Pre-training on synthetic numerical data (ND) and textual numerical problems (TD); digit-level tokenization; random positional shift augmentation (RS); multi-tasking with original MLM objective to avoid catastrophic forgetting.",
            "effect_of_intervention": "Pre-training on ND and TD dramatically improved arithmetic performance on downstream tasks: DROP F1 from 49.3 to 72.3 (abstract); GENBERT+ND and GENBERT+TD both improved EM and F1 on DROP (e.g., ND+TD yields EM ~68.8 and high F1); ND pretraining speeds convergence on TD; digit tokenization drastically reduces sample complexity and is required to learn ND tasks; MLM multi-tasking prevents language understanding degradation.",
            "performance_metrics": "On DROP: abstract reports F1 improvement 49.3 -&gt; 72.3 after pretraining; in-text results show GENBERT baseline EM 46.1 / F1 49.3, GENBERT+ND EM ~64.7, GENBERT+TD EM ~64.4, GENBERT+ND+TD EM ~68.8; breakdown: number-answer F1 up to 75.2 (GENBERT+ND); zero-shot MWP EM: ADDSub 22.8, SOP 28.3 (best), SEQ 22.3; SQuAD (non-numeric RC) remains similar to BERT: EM ~81.3, F1 ~88.6. Pretraining achieves &gt;96% accuracy on synthetic ND/TD tasks.",
            "notable_failure_modes": "Fails on arithmetic expressions involving more than ~3 terms (near-zero generalization beyond this); misses complex reasoning types not covered by synthetic tasks (sorting, certain counting variants); produces imprecise numeric outputs (partial-digit matches); struggles with multi-span list answers (model outputs single-span/generative answers).",
            "comparison_to_humans_or_symbolic": "Compared to symbolic or specialized modules (e.g., models that enumerate/extract arithmetic expressions and evaluate externally), GENBERT performs competitively on DROP but lags on some MWP benchmarks; authors argue symbolic approaches require non-differentiable search and are less flexible for integrated text+numeric outputs, while GENBERT performs computations internally albeit with limited generalization to arbitrarily complex arithmetic.",
            "uuid": "e3014.0",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Hybrid extractive QA w/ numeric heads",
            "name_full": "Hybrid extractive QA models augmented with specialized count and arithmetic heads (e.g., count head, arithmetic head)",
            "brief_description": "Existing NRoT approach that uses a pre-trained LM encoder (e.g., BERT) and multiple answer heads for spans, counts (0-9), and arithmetic combinations of text-extracted numbers; a type head marginalizes over head choices during training.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Hybrid extractive QA (count + arithmetic heads)",
            "model_description": "Architecture: pretrained LM computes contextual representations which feed into several small FFN 'heads' that predict context-span answers, question-span answers, a categorical count (0..9), or signed combinations of pre-identified numbers in the context; a type head predicts head probabilities and training marginalizes over heads.",
            "arithmetic_task_type": "Counting up to small fixed bound (e.g., up to 9), addition/subtraction of a few numbers drawn from context (signed combinations), extraction of arithmetic expressions that correspond to answers in RC tasks.",
            "reported_mechanism": "Performs arithmetic symbolically by restricting output types and delegating computation to specialized heads (sometimes combined with external evaluation of enumerated expressions); uses discrete representations of numbers (pre-identified tokens) rather than generating arbitrary numeric tokens.",
            "evidence_for_mechanism": "These models achieve state-of-the-art results on DROP for comparable sizes (e.g., MTMSN), showing that constrained symbolic heads are effective for datasets whose operations fall within supported types; they improve performance especially on multi-span answers and limited arithmetic operations.",
            "evidence_against_mechanism": "Limited flexibility: can only handle pre-defined numeric ranges (counts up to 9) and restricted arithmetic (2-3 numbers), scaling up requires non-differentiable search over exponential expression spaces and bespoke architecture changes; cannot internally combine computation with subsequent span extraction if computation was delegated externally.",
            "intervention_type": "Specialized architectural modules (count head, arithmetic head) and marginalization over expression decompositions during training; in some related works, an external symbolic calculator is used.",
            "effect_of_intervention": "Improves accuracy on benchmarks like DROP where supported operations match dataset needs (e.g., MTMSN base achieves higher EM on certain MWP datasets than GENBERT), but at expense of generality and requiring architecture changes to support more operations.",
            "performance_metrics": "Reported in paper as comparative baselines: MTMSN_BASE EM on some MWP testsets: ADDSub 32.2, SOP 28.0, SEQ 32.5 (higher than GENBERT variants on those filtered MWP sets); MTMSN_BASE EM on DROP (as reported) ~68.2 EM (comparable to GENBERT+ND+TD EM ~68.8).",
            "notable_failure_modes": "Unable to scale to arbitrary computations (exponential expression space) without non-differentiable search; limited count range and limited number of operands; difficulty integrating internal arithmetic with textual span extraction when the final answer requires mapping a computed numeric value back to text.",
            "comparison_to_humans_or_symbolic": "These approaches are closer to symbolic computation than distributed generation: they explicitly enumerate or represent arithmetic expressions and (sometimes) use external evaluation, making them more algorithmic for supported classes but brittle beyond them.",
            "uuid": "e3014.1",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Symbolic/semantic-parsing approach",
            "name_full": "Symbolic program / semantic parsing approach for numerical reasoning",
            "brief_description": "An approach that maps text to discrete symbolic programs or expressions which are then executed (often by an external calculator) to produce numeric answers; common in earlier arithmetic word-problem work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Symbolic semantic parser",
            "model_description": "Maps question+context to a discrete symbolic expression/program (e.g., arithmetic expression) which is executed to obtain a numeric result; training typically involves search over discrete program/expression space and sometimes uses syntactic supervision.",
            "arithmetic_task_type": "Full symbolic arithmetic expressed as arithmetic programs (addition, subtraction, multiplication, division), structured equation solving for algebraic word problems.",
            "reported_mechanism": "Performs arithmetic by constructing explicit symbolic expressions and executing them externally—i.e., true algorithmic/symbolic computation rather than distributed numeric generation.",
            "evidence_for_mechanism": "Prior literature and cited works demonstrate success on certain classes of math word problems when correct program induction is possible; symbolic outputs are exact when program is correct.",
            "evidence_against_mechanism": "Training/search is difficult because program space grows combinatorially, requiring non-differentiable search and making end-to-end optimization hard; not well-suited when final answer is a text span requiring integration between numeric computation and text extraction (the paper argues this as limitation).",
            "intervention_type": "Use of external symbolic calculator/executor and program induction techniques (semantic parsing).",
            "effect_of_intervention": "Enables exact arithmetic and complex operations when programs can be correctly induced, but complicates training and reduces model flexibility to jointly produce text and computed outputs.",
            "performance_metrics": null,
            "notable_failure_modes": "Search and optimization difficulties due to huge discrete space; inability to jointly model computations that must be combined with text-span extraction without special handling.",
            "comparison_to_humans_or_symbolic": "Closest to symbolic human-like algorithmic computation (exact arithmetics) but operationally distinct from distributed neural generation; contrasted in paper with GENBERT's end-to-end differentiable approach.",
            "uuid": "e3014.2",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Digit Tokenization (DT)",
            "name_full": "Digit-level tokenization for numeric tokens",
            "brief_description": "An intervention that splits numbers into individual digit tokens (e.g., '517' -&gt; '5','1','7') rather than treating entire numbers as single wordpiece tokens, to ease learning of numeric value composition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GENBERT (applied intervention)",
            "model_description": "Digit Tokenization is applied on top of GENBERT/BERT tokenization: numeric wordpiece tokens composed only of digits are further split into digit-level pieces and represented independently.",
            "arithmetic_task_type": "Facilitates learning of numeric composition for signed float combinations and other numeric outputs that must be produced token-by-token.",
            "reported_mechanism": "Reduces lexical sparsity of numeric tokens and makes value composition (learning mapping from digit sequences to numeric values and operations on them) easier for the model; effectively provides a more convenient subtoken alphabet for numbers.",
            "evidence_for_mechanism": "Ablation shows ND-LM-DT (wordpiece tokenization without digit tokenization) fails to learn the ND tasks (no convergence), while using digit tokenization substantially improves sample complexity and enables the model to learn numeric operations; highlighted in §3 and §5.1.",
            "evidence_against_mechanism": "None presented in paper; however, digit tokenization requires careful handling for very large numeric ranges or formatted numbers (not discussed here).",
            "intervention_type": "Tokenization change (preprocessing): splitting numeric tokens into digit-level tokens.",
            "effect_of_intervention": "Dramatically improved learning on ND pretraining (enabled convergence), reduced sample complexity for numeric tasks, and is necessary for GENBERT to learn synthetic numeric templates effectively.",
            "performance_metrics": "Qualitative/ablation: models without DT (ND-LM-DT) did not learn ND tasks; models with DT achieved &gt;96% accuracy on ND/TD pretraining and downstream gains on DROP. No single numeric-only accuracy number for DT alone beyond these comparisons.",
            "notable_failure_modes": "Not discussed in detail; digit tokenization alone does not solve higher-level reasoning or multi-term generalization limits.",
            "comparison_to_humans_or_symbolic": "DT is an engineering intervention to make distributed models' input representation numerically friendly, unlike symbolic approaches which operate on numeric semantics explicitly.",
            "uuid": "e3014.3",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Synthetic Pretraining (ND and TD)",
            "name_full": "Pretraining on Synthetic Numerical Data (ND) and Textual Data (TD)",
            "brief_description": "A multi-task pretraining regimen using large synthetic corpora: ND (1M short numeric expressions covering signed floats, min/max/avg, dates, percentages) and TD (2.5M generated passages + questions based on templated math-word-problem grammar) to teach numeric operations and how they are expressed in text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GENBERT (pretraining intervention)",
            "model_description": "Two synthetic corpora were automatically generated: ND consisting of numeric expressions and their solutions (up to 4-term combos, floats, dates, percentages), and TD consisting of templated passages instantiated to maintain world state plus numeric questions; GENBERT is pre-trained on both datasets jointly with the MLM objective.",
            "arithmetic_task_type": "Short explicit arithmetic expressions (signed float combinations), statistical ops (min/max/avg), argmax/argmin, date arithmetic (differences, min/max), percentage complements, and passage-level numeric reasoning (tracking counts, differences, sums) analogous to DROP-style questions.",
            "reported_mechanism": "Supplies supervised examples of numeric operations in isolation (ND) and embedded in textual contexts (TD), allowing the model to internalize mappings from token patterns and context to numeric computation outputs and to learn to generate numeric tokens end-to-end.",
            "evidence_for_mechanism": "GENBERT trained on ND and TD achieves &gt;96% accuracy on the synthetic tasks and large gains on DROP (e.g., abstract F1 49.3 -&gt; 72.3), faster convergence on TD when ND pretraining is included, and improved zero-shot transfer to filtered MWP datasets; ablations show both ND and TD are complementary.",
            "evidence_against_mechanism": "Despite success, pretraining on these synthetic tasks does not cover all numeric reasoning types; models still fail frequently on tasks outside the synthetic curricula (sorting, &gt;3-term expressions), showing limits to generalization from the synthetic distributions.",
            "intervention_type": "Large-scale supervised pretraining on automatically-generated synthetic numeric expressions (ND) and templated textual numeric passages/questions (TD), combined with MLM multi-tasking.",
            "effect_of_intervention": "Dramatic improvement in arithmetic-capable downstream performance: DROP F1 increases substantially, EM increases from ~46 to mid/upper-60s; improved zero-shot performance on select MWP datasets; preserved SQuAD performance when MLM included.",
            "performance_metrics": "Pretraining results: &gt;96% accuracy on synthetic ND and TD tasks; downstream DROP: reported abstract F1 increase 49.3 -&gt; 72.3, GENBERT+ND EM ~64.7, GENBERT+TD EM ~64.4, GENBERT+ND+TD EM ~68.8; MWP zero-shot EMs (ADDSub 22.8, SOP 28.3, SEQ 22.3); SQuAD remains comparable to BERT (EM ~81.3, F1 ~88.6).",
            "notable_failure_modes": "Pretraining coverage gaps lead to failures on reasoning types not included in the templates (sorting, some complex semantics), limited generalization to expressions with more operands/complex algebraic structure, and occasional imprecise numeric outputs.",
            "comparison_to_humans_or_symbolic": "This is a data-driven approach to teach models arithmetic-like skills, trading off the exactness and algorithmic guarantees of symbolic methods for end-to-end differentiability and integration with natural language understanding.",
            "uuid": "e3014.4",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Random Shift (RS) & MLM multi-tasking",
            "name_full": "Random positional shift augmentation and inclusion of Masked LM loss during pretraining",
            "brief_description": "Two training interventions: RS randomly offsets absolute positional embeddings to prevent position-overfitting on short synthetic inputs; MLM multi-tasking retains original BERT masked-language-objective during synthetic pretraining to avoid catastrophic forgetting of language knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GENBERT (training interventions)",
            "model_description": "Random Shift: when input shorter than maximum sequence length, randomly shift position IDs to discourage reliance on absolute positions. MLM multi-tasking: include BERT masked language modeling loss alongside generative ND/TD losses with weighting lambda.",
            "arithmetic_task_type": "Supports robustness of learning arithmetic on short explicit expressions (ND) and textual passages (TD) by preventing overfitting to artifact positions and preserving general language representations.",
            "reported_mechanism": "RS forces the model to learn content-based (not position-based) numeric computations; MLM prevents catastrophic forgetting and preserves linguistic capabilities necessary to interpret textual contexts that express numeric operations.",
            "evidence_for_mechanism": "Ablations: removing MLM (ND-LM) reduces DROP performance and harms SQuAD performance; removing RS (ND-LM-RS) also lowers downstream EM, indicating RS contributes to robustness for short numeric inputs. ND-LM-DT (no DT) fails, underscoring importance of combined interventions.",
            "evidence_against_mechanism": "No direct contradiction presented, but interventions do not eliminate remaining failure modes (complex arithmetic &gt;3 terms etc.).",
            "intervention_type": "Data augmentation for positional IDs (RS) and multi-task objective combining MLM with synthetic-task losses (MLM).",
            "effect_of_intervention": "Including MLM during synthetic pretraining preserves SQuAD RC accuracy and improves downstream numeric QA; RS improves generalization on short-expression ND tasks and final DROP EM/F1 relative to variants without RS.",
            "performance_metrics": "Ablation notes: GENBERT+ND-LM and GENBERT+ND-LM-RS perform worse than GENBERT+ND (with MLM and RS); specific EM/F1 differences reported across tables (e.g., lower EM when MLM omitted; exact numeric deltas in paper tables).",
            "notable_failure_modes": "Even with RS and MLM, model generalization is limited by the synthetic curricula's coverage; RS does not address complexity-generalization limits.",
            "comparison_to_humans_or_symbolic": "These are pragmatic training augmentations to improve learned distributed arithmetic behavior, rather than algorithmic changes akin to symbolic systems.",
            "uuid": "e3014.5",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models.",
            "rating": 2,
            "sanitized_title": "analysing_mathematical_reasoning_abilities_of_neural_models"
        },
        {
            "paper_title": "Do nlp models know numbers? probing numeracy in embeddings.",
            "rating": 2,
            "sanitized_title": "do_nlp_models_know_numbers_probing_numeracy_in_embeddings"
        },
        {
            "paper_title": "A multi-type multi-span network for reading comprehension that requires discrete reasoning.",
            "rating": 2,
            "sanitized_title": "a_multitype_multispan_network_for_reading_comprehension_that_requires_discrete_reasoning"
        },
        {
            "paper_title": "Giving bert a calculator: Finding operations and arguments with reading comprehension.",
            "rating": 2,
            "sanitized_title": "giving_bert_a_calculator_finding_operations_and_arguments_with_reading_comprehension"
        },
        {
            "paper_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs.",
            "rating": 2,
            "sanitized_title": "drop_a_reading_comprehension_benchmark_requiring_discrete_reasoning_over_paragraphs"
        },
        {
            "paper_title": "Deep learning for symbolic mathematics.",
            "rating": 1,
            "sanitized_title": "deep_learning_for_symbolic_mathematics"
        },
        {
            "paper_title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms.",
            "rating": 1,
            "sanitized_title": "mathqa_towards_interpretable_math_word_problem_solving_with_operationbased_formalisms"
        }
    ],
    "cost": 0.0159985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Injecting Numerical Reasoning Skills into Language Models</h1>
<p>Mor Geva*<br>Tel Aviv University, Allen Institute for AI<br>morgeva@mail.tau.ac.il</p>
<p>Ankit Gupta*<br>Tel Aviv University<br>ankitgupta.iitkanpur@gmail.com</p>
<p>Jonathan Berant<br>Tel Aviv University, Allen Institute for AI<br>joberant@cs.tau.ac.il</p>
<h4>Abstract</h4>
<p>Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP ( $49.3 \rightarrow 72.3 \mathrm{~F}_{1}$ ), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.</p>
<h2>1 Introduction</h2>
<p>Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our approach for injecting numerical skills into a pre-trained LM. (a) We add two pre-training steps over large amounts of synthetic numerical data (ND) and textual data (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQUAD).
text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage.</p>
<p>To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were designed for counting (but only until ' 9 ') and for addition and subtraction (but of 2-3 numbers only). Such models perform well on existing datasets, such as DROP (Dua et al., 2019), but do not generalize to unsupported computations, which will require modifying the model architecture. Moreover, current models marginalize at training time over all numerical expressions that evaluate to the correct answer. Since the number of such expressions grows exponentially, scaling these approaches to arbitrary computations entails using non-differentiable operations (sampling or computing top- $K$ numerical expressions), which can lead to training difficulties.</p>
<p>Passage: Taunton has four art galleries... Hughes/ Donahue Gallery founded in 2007, a local community gallery serving local Taunton artists... Art Euphoric founded in 2008 has both visual and craft exhibits...
Q1: How many years after founding of Hughes/ Donahue was Art Euphoric founded?
A1: 1 (number)
Q2: Which gallery was founded later, Hughes/ Donahue or Art Euphoric?
A2: Art Euphoric (span)
Table 1: Example passage from DROP, and two questions with different answer types.</p>
<p>In this work, we propose that reasoning skills, such as numerical reasoning, are amenable to automatic data generation. Hence, one can inject that skill directly into the model by adding additional pre-training steps, allowing the model to learn the skill in an end-to-end fashion. This results in a fully-differentiable training procedure over a standard and general-purpose architecture, where the output space can be easily controlled through the data generation procedure.</p>
<p>Specifically (Figure 1), we add to a large pre-trained LM two pre-training steps over automatically-generated synthetic data. First, we generate numerical data of the form $3+4+11=$ 18. Training on this data teaches the model to compute the value of numbers from their tokens and to perform numerical operations. Second, we automatically generate question-passage pairs that require numerical reasoning using a compact grammar (textual data). Training on this data endows the model with the ability to understand computations expressed in pseudo-natural language.</p>
<p>In both pre-training steps, the model, GENBERT, generates output numbers token-by-token. Thus, the model has a standard architecture, where an answer can either be extracted from the input question and passage or generated from a decoder. Pre-training is done in a multi-task setup with a standard LM objective, in order to avoid "catastrophic forgetting" (Kirkpatrick et al., 2017) of the linguistic information in the original LM. After pre-training, the model has sufficient language and numerical skills to be directly fine-tuned on a target numerical reasoning dataset, without resorting to specialized architectures. Augmenting more numerical skills does not require changing the model, only generating additional data.</p>
<p>We demonstrate the validity of our approach by a series of experiments showing that:
(a) GENBERT is able to solve pre-training tasks for numerical reasoning.
(b) Pre-training on these tasks provides GENBERT with 1) skills to reach performance that matches state-of-the-art models of comparable size on DROP (Dua et al., 2019), a standard numerical reasoning dataset, as well as 2) the ability to generalize to math word problem (MWP) datasets (Koncel-Kedziorski et al., 2016).
(c) GENBERT learns these numerical skills while maintaining high performance on SQuAD (Rajpurkar et al., 2016), a standard reading comprehension dataset.
(d) Initializing models for numerical reasoning with GENBERT's weights improves their original performance.
To conclude, in this work we address the problem of injecting LMs with numerical reasoning skills. Our contributions are:</p>
<ul>
<li>A method for injecting skills into pre-trained LMs, given that automatic data generation is possible.</li>
<li>GENBERT, an architecture for pre-trained LM with generative and extractive abilities.</li>
<li>A framework for generating numerical and textual synthetic data for numerical reasoning.
Our code and data can be downloaded from https://github.com/ag1988/ injecting_numeracy.</li>
</ul>
<h2>2 Numerical Reasoning Over Text</h2>
<p>Numerical reasoning over text (NRoT) is commonly set up as a reading comprehension (RC) task. Given a training set of question-context-answer triples $\left{\left(\mathbf{q}<em i="i">{i}, \mathbf{c}</em>$, or (b) a number that is the result of some computation (see examples in Table 1).}, a_{i}\right)\right}_{i=1}^{N}$, the goal is to learn a function that returns the answer $a$ to a question $\mathbf{q}$ given a context $\mathbf{c}$. However, in NRoT the answer generally requires to internally perform some numerical computation using the entities and numbers in the context. Specifically, the answer is either: (a) a span (or list of spans) from the context $\mathbf{c}$ or question $\mathbf{q</p>
<p>Two natural, yet opposing, approaches lend themselves to tackling NRoT: (a) A symbolic approach: a model can read the question and context, output a numerical expression and evaluate the answer with an external symbolic calculator. This approach is a particular case of semantic parsing (Kamath and Das, 2019), and was common in early NRoT datasets (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Hosseini et al., 2014). How-</p>
<p>ever, it suffers from several drawbacks. First, because numerical expressions are discrete and their space grows combinatorially, the model must learn to search in this space using non-differentiable operations, which are usually difficult to optimize. Second, numerical expressions are limited to numerical answers, while in DROP often a numerical computation is required but the final answer is a text span. (b) A distributed approach: have a model directly generate the answer given $(\mathbf{q}, \mathbf{c})$. When the answer is a text span, the model can extract it from the input, and when the answer is a number that is not in $\mathbf{q}$ or $\mathbf{c}$, the model must generate it. While this makes training straightforward, the model must learn to perform numerical computations from the relatively small target dataset. We empirically show in $\S 3$ that this leads to low performance in general.</p>
<p>As a compromise, most NRoT models (Dua et al., 2019; Kinley and Lin, 2019; Hu et al., 2019; Efrat et al., 2019) have taken a hybrid approach: they augment standard extractive QA models with specialized modules for handling a limited set of numerical computations. We briefly describe this architecture, as it is the basis for our model in $\S 3$.</p>
<p>Given a question with $n_{1}$ tokens $\mathbf{q}=$ $\left(q_{1}, \ldots, q_{n_{1}}\right)$ and a context with $n_{2}$ tokens $\mathbf{c}=$ $\left(c_{1}, \ldots, c_{n_{2}}\right)$, the hybrid model first computes contextualized representations for the $n_{1}+n_{2}+3$ tokens $\langle[\mathrm{CLS}] \mathbf{q}[\mathrm{SEP}] \mathbf{c}[\mathrm{SEP}]\rangle$ using a pretrained LM, such as BERT (Devlin et al., 2019):</p>
<p>$$
\mathbf{L}=\mathbf{L M}(\mathbf{q}, \mathbf{c})
$$</p>
<p>The representations $\mathbf{L}$ are then passed to multiple heads, which are small neural networks that estimate $p(a \mid \mathbf{q}, \mathbf{c}, h)$, that is, the probability of the answer given the input and conditioned on a head $h$, corresponding to a particular answer type:</p>
<ul>
<li>Context span head: computes a distribution over all spans in the context using a feed-forward network (FFN) $\mathbf{F F}_{\mathbf{c}}(\mathbf{L})$.</li>
<li>Question span head: computes a distribution over spans in the question using a FFN $\mathbf{F F}_{\mathbf{q}}(\mathbf{L})$.</li>
<li>Count head: computes a distribution over the numbers ${0, \ldots, 9}$ using a FFN $\mathbf{F F}_{\mathbf{c n t}}(\mathbf{L})$.</li>
<li>Arithmetic head: computes a distribution over all signed combinations of numbers in the context using a FFN $\mathbf{F F}_{\mathbf{c m b}}(\mathbf{L})$ (the numbers in the context are identified in a pre-processing step).
While the first two heads are standard in extractive QA, the latter two heads are specialized and meant to handle answers that do not appear in the input.</li>
</ul>
<p>Finally, for deciding which answer head to use for a given input, a type head $\mathbf{F F}<em _head="{head" _text="\text">{\text {typ }}(\mathbf{L})$ outputs a probability distribution $p</em>, h)$.}}(h \mid \mathbf{q}, \mathbf{c})$ (using a FFN). Thus the model probability for an answer is $p(a \mid \mathbf{q}, \mathbf{c})=\sum_{h \in \text { heads }} p_{\text {head }}(\mathbf{h} \mid \mathbf{c}, \mathbf{q}) \cdot p(a \mid \mathbf{c}, \mathbf{q</p>
<p>Training is done by enumerating all of the ways in which the answer can be obtained using all of the heads, and maximizing this marginal probability.</p>
<p>While existing models perform well on DROP, the aforementioned architecture is not flexible. First, the output space is severely constrained the model can only count up to ' 9 ', and numerical computations are restricted to signed combinations of a few numbers. Second, expanding the space of supported numerical computations is non-trivial, because training involves marginalizing over all expressions that lead to the correct answer. Since the space of numerical expressions grows exponentially, expanding this space quickly leads to a difficult search problem. Third, delegating numerical computations to an external symbolic calculator leads to modeling challenges, since there could be interactions between text and numerical computation: Consider the DROP question "How many total yards did Phil Dawson throw for touchdowns?". Current models handle such questions by computing a sum from numbers in the text and returning the result. However, if the question was "Who threw 45 total yards for touchdowns?", the model would have to compute the sum internally, and then find the relevant span in the text. This is impossible when the computation itself is delegated to an external calculator. Thus, training models to handle such numerical questions is desirable.</p>
<p>Motivated by the above arguments, we wish to push the frontier of end-to-end differentiable models for numerical reasoning. Thus, we will automatically generate large amounts of data that endow a pre-trained LM with numerical skills.</p>
<h2>3 GENBERT: A BERT-based Model for Generating Arbitrary Outputs</h2>
<p>We now describe a simple BERT-based generative model that performs numerical computations internally, termed GENBERT. The model combines the Transformer encoder-decoder architecture (Vaswani et al., 2017) with a pre-trained LM, specifically, BERT.</p>
<p>Our architecture is illustrated in Figure 2. Our encoder is a standard Transformer, initialized with</p>
<p>BERT weights. To also enjoy BERT's representations at decoding time, we tie the weights of the decoder and the encoder. Because the Transformer decoder has source attention weights (weights for attending to the encoder representations at decoding time) that are not present in BERT, we tie these source-attention weights to the self-attention weights of the encoder (which are tied to the selfattention weights of the decoder). This fully initializes the Transformer model with BERT weights.</p>
<p>Since the encoder and decoder weights are tied, we make them learn distinct representations by adding a FFN $\mathbf{F F}<em _enc="{enc" _text="\text">{\text {enc }}$ that transforms the encoder contextualized representations $\mathbf{L}</em>$ as}</p>
<p>$$
\mathbf{H}<em _enc="{enc" _text="\text">{\text {enc }}=\text { layer-norm }\left(\operatorname{gelu}\left(W \cdot \mathbf{L}</em>\right)\right)
$$}</p>
<p>where $W$ is a parameter matrix (Hendrycks and Gimpel, 2016; Ba et al., 2016). Analogously, we add $\mathbf{F F}<em 1="1">{\text {dec }}$ to the decoder. To further distinguish the encoder and decoder, we use distinct start and end tokens for input and output sequences. Given $m$ answer tokens $\mathbf{a}=$ $\left(a</em>}, \ldots, a_{m}\right)$, we form an output sequence with $m+2$ tokens: $\langle[$ SOS] a [EOS] $\rangle$. The output tokens are passed through the decoder and $\mathbf{F F<em _dec="{dec" _text="\text">{\text {dec }}$ to obtain $\mathbf{H}</em>$.}</p>
<p>Finally, the probability of an answer is defined in the usual manner: Let $\langle\mathbf{a}\rangle=\left(a_{0} \cdots a_{m+1}\right)$ be the output sequence. The decoder outputs the probability $p_{\text {dec }}\left(a_{i+1} \mid a_{0}, . . a_{i}, \mathbf{c}, \mathbf{q}\right)$, and the probability of an answer is:</p>
<p>$$
p_{\mathrm{dec}}(\langle\mathbf{a}\rangle \mid \mathbf{c}, \mathbf{q})=\prod_{i=0}^{m} p_{\mathrm{dec}}\left(a_{i+1} \mid a_{0}, . . a_{i}, \mathbf{c}, \mathbf{q}\right)
$$</p>
<p>As we have a generative model, we can remove the specialized count and arithmetic heads from $\S 2$. Thus, the type head $\mathbf{F F}<em _enc="{enc" _text="\text">{\text {typ }}\left(\mathbf{H}</em>\right)$ over the context span, question span, and decoder heads.}}\right)$ outputs a distribution $\left(p_{\mathbf{q}}, p_{\mathbf{c}}, p_{\text {dec }</p>
<p>To improve pre-training on the numeric data (§4), we make two additional modifications.</p>
<p>Digit Tokenization (DT) Conventional wordpiece tokenization treats numbers no differently than any other token. However, computing the value of numbers should be simpler when using digits directly (Wallace et al., 2019). Hence, we tokenize numbers digit-by-digit. For example, a wordpiece ## $d_{1} \cdots d_{k}$ where $d_{i} \in{0, \ldots, 9}$ is further split into ## $d_{1}, \ldots, # # d_{k}$. We show in $\S 5.1$ that this substantially improves sample complexity when training to perform numerical operations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GenBERT's network architecture: (a) a high-level overview of the network, including a generative head (red), two span-extraction heads (yellow), and an answer type head. (b) a closer overview of GENBERT's generative head.</p>
<p>Random Shift (RS) The original Transformer uses absolute positional embeddings for each token. However, in $\S 4$, we train on short inputs such as "1086.1 - $2.54+343.8$ ". Thus, the model can potentially over-fit and learn to perform numerical reasoning only when numbers are at the beginning of an input. To prevent this, when the input length $n_{1}+n_{2}+3&lt;512$, we shift all position IDs by a random integer in $(0,1, \ldots, 512-\left(n_{1}+n_{2}+3\right))$.</p>
<p>Training For each span $(i, j)$, a span extraction head $h$ outputs its probability $p_{h}((i, j) \mid \mathbf{c}, \mathbf{q}, h)$ of being the answer. Let $S$ be the set of spans in the input corresponding to the gold answer. The model loss $\mathbf{L}<em _dec="{dec" _text="\text">{\text {model }}$ marginalizes over all ways in which the answer can be predicted:
$-\log \left(p</em>(i, j)\right)$,
where conditionals have been dropped for brevity.
To evaluate the ability of GENBERT to perform numerical reasoning, we initialize it with BERT and fine-tune it on DROP. GENBERT obtains 46.1 EM and $49.3 \mathrm{~F}_{1}$, roughly 20 points lower than prior models. Thus, we conclude that acquiring numerical reasoning skills from DROP data only is difficult. To remedy this, we will automatically generate training data that will endow GENBERT with numerical skills before training it on DROP.}} \cdot p_{\text {dec }}(\langle\mathbf{a}\rangle)+\sum_{\mathbf{h} \in \mathbf{q}, \mathbf{c}} p_{\mathbf{h}} \cdot \sum_{(i, j) \in S} p_{h</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Template extraction and instantiation. A template (in red) is extracted from a MWP sentence, using categories for containers, entities, verbs, attributes and numbers, according to Hosseini et al. (2014). For generation, the categories are instantiated with a domain-specific vocabulary.</p>
<h2>4 Pre-training Tasks for Numerical Skills</h2>
<p>We now describe two automatically-generated datasets and the multi-task training procedure.</p>
<h3>4.1 Generating Numerical Data (ND)</h3>
<p>Our first dataset focuses on learning numerical values expressed by tokens and computing numerical operations, i.e., it does not involve textual content. As such, it is easy to craft templates that correspond to various numeric operations. We designed six such templates, described in Table 2. Each template consists of an expression to evaluate and its solution. Further details on their instantiation are provided in $\S$ A.1. While the numerical operations were chosen based on DROP, it is trivial to extend them to other domains (Saxton et al., 2019) with different numerical operations.</p>
<h3>4.2 Generating Textual Data (TD)</h3>
<p>Numeric data is easy to generate, since it does not contain any textual context. However, to tackle NRoT, a model needs to comprehend how numerical operations are expressed in text that refers to events, entities and quantities. This primes us to generate textual data from a simple grammar.</p>
<p>While text generation is hard in the general case, we are specifically interested in text that focuses on number manipulations. Therefore, we use the framework of Hosseini et al. (2014), who proposed to model math word problems with a simple structure. In their framework a world state consists of entities, which are objects that are being counted, and containers, which are objects that own entities. Sentences use verb categories to describe how the number of entities in a container changes, and thus a world state can be updated given a sentence.</p>
<p>Consider the textual example in Figure 1. the entities are soldiers and citizens, and the containers are the king and the commander. The verbs ("had" and "received") describe the entities the king holds, and how many were passed to the commander.</p>
<p>In this work, we use this framework to automatically generate examples. We extract templates that describe changes in the number of entities owned by containers, and automatically generate questioncontext pairs from these templates.</p>
<p>Template extraction To extract templates, we go over sentences from the corpus provided by Hosseini et al. (2014). For each sentence, we use a procedure described by Hosseini et al. (2014) to abstract its tokens to the following categories: numbers (NUM), entities (ENT), containers (CONT) and attributes (ATTR). In addition, verbs are abstracted to six categories, each corresponding to a different change in the number of entities owned by containers. Thus, each template fully specifies how to update a world state, i.e., the number of entities each container owns. The top part of Figure 3 illustrates the abstraction process. Finally, we count for each extracted template its frequency in the data, and use the top-12 templates for passage generation. Details on the abstraction process, categories used, and extracted templates are in $\S$ A.2.</p>
<p>Passage generation Using the extracted templates, we can generate sentences and maintain a world state of all containers and the number of entities they own. We construct a small vocabulary ( $&lt;100$ words) that maps categories to domainspecific words, and use the following procedure to generate passages.</p>
<p>We sample 3-6 templates with replacement, and instantiate them one-by-one (the bottom part of Figure 3 illustrates instantiation). Each template is instantiated by uniformly sampling values from the vocabulary with probability $1-p$ and from previously generated sentences with probability $p$. To avoid a collection of unrelated sentences, we set the probability of using previously used values to $p=0.7$. An example passage is shown in Table 3.</p>
<p>Question generation After generating a passage, the world state holds information about all containers in the passage and the number of entities they hold. In Table 3, the state will include the number of families and rebels of different nationalities in each container (the commander, the householder, and the countries). Based on this world state, numerical reasoning questions can be asked.</p>
<p>To create questions, we craft 13 question templates that are instantiated with objects from the world state. The questions teach the model to track events and perform numeric and discrete operations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation</th>
<th style="text-align: right;">Template</th>
<th style="text-align: left;">Example instantiation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">signed float combination</td>
<td style="text-align: right;">$s_{1} f_{1} s_{2} f_{2} s_{3} f_{3} s_{4} f_{4}$</td>
<td style="text-align: left;">$517.4-17484-10071.75*1013.21$</td>
</tr>
<tr>
<td style="text-align: left;">min/max/avg</td>
<td style="text-align: right;">$o\left(f_{1}, f_{2}, f_{3}, f_{4}\right)$</td>
<td style="text-align: left;">largest(13.42, 115.5, 72.76)</td>
</tr>
<tr>
<td style="text-align: left;">arg max, arg min</td>
<td style="text-align: right;">$\arg \left(w_{1} f_{1}, w_{2} f_{2}, w_{3} f_{3}, w_{4} f_{4}\right)$</td>
<td style="text-align: left;">arg min(highish 137.1, sightliness 43.2)</td>
</tr>
<tr>
<td style="text-align: left;">date min/max</td>
<td style="text-align: right;">$d s u p\left(d_{1}, d_{2}, d_{3}, d_{4}\right)$</td>
<td style="text-align: left;">oldest(June 04, 959; 01 May 959)</td>
</tr>
<tr>
<td style="text-align: left;">date difference</td>
<td style="text-align: right;">diff in $\operatorname{prd}\left(d_{1}, d_{2}\right)$</td>
<td style="text-align: left;">diff in days(05 April 112; June 01, 112)</td>
</tr>
<tr>
<td style="text-align: left;">percentage</td>
<td style="text-align: right;">$\operatorname{peent} w:: w_{1} p_{1} \%, w_{2} p_{2} \%, w_{3} p_{3} \%, w_{4} p_{4} \%$</td>
<td style="text-align: left;">percent not sunbed :: sunbird $33.2 \%$, defector</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;">$60.77 \%$, molehill $6.03 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Templates for generating synthetic numerical examples and the numerical operations required to answer them. Domains (defined in App. A.1): $s_{i} \in{-,+}, f_{i} \in \mathbb{R}^{+}, o \in \mathcal{O}$ : superlative words like "longest", arg $\in{\arg \min , \arg \max }$, $w_{i} \in \mathcal{W}$ : words from NTLK Words Corpus, $d_{i} \in \mathcal{D}$ : dates until Sep 2019, $d s u p \in \mathcal{D S U P}$ : superlative words like "latest", $\operatorname{prd} \in{$ "days", "months", "years" $}, p_{i} \in(0,100)$, $\operatorname{peent} \in{$ "percent", "percent not" $}$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">P</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">commander</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="mi">1949</span><span class="w"> </span><span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">householder</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="mi">1996</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span>
<span class="n">There</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="mi">10913</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">rebels</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="mi">77</span><span class="w"> </span><span class="n">Chinese</span><span class="w"> </span><span class="n">families</span>
<span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span><span class="w"> </span><span class="mi">6641</span><span class="w"> </span><span class="n">British</span><span class="w"> </span><span class="n">soldiers</span><span class="o">,</span><span class="w"> </span><span class="mi">476</span><span class="w"> </span><span class="n">asian</span><span class="w"> </span><span class="n">rebels</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="mi">338</span>
<span class="n">Germans</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Russia</span><span class="o">.</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">1996</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="w"> </span><span class="n">than</span>
<span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">47</span><span class="w"> </span><span class="o">(</span><span class="mi">1996</span><span class="o">-</span><span class="mi">1949</span><span class="o">)</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Spain</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">2073</span><span class="w"> </span><span class="o">(</span><span class="mi">4022</span><span class="o">-</span><span class="mi">1949</span><span class="o">)</span>
</code></pre></div>

<p>Table 3: An example synthetic passage (P) and questions. Questions $(\mathrm{Q})$ were generated from templates and answers $(\mathrm{A})$ were calculated based on the world state.</p>
<p>Examples for generated questions are shown in Table 3, where answers are computed from the world state. Overall, we create 13 question templates for 7 different "skills", provided in §A.2.</p>
<h3>4.3 Training GeNBERt on Synthetic Data</h3>
<p>For pre-training on ND, we generated 1M examples for training and 10 K for validation. For TD, we generated 2.5 M examples for training and 10 K for validation. For both synthetic datasets, we used the GENBERt model loss, $\mathbf{L}<em _enc="{enc" _text="\text">{\text {model }}$, from $\S 3$. To ensure that the model does not lose its language understanding abilities, we employ a multi-task setup, and include a standard masked $L M$ objective from BERT. Specifically, given a masked token sequence $\langle\mathbf{m}\rangle$, we compute the contextualized representations, $\mathbf{L}</em>}}$ and pass them through a feedforward network $\mathbf{F} \mathbf{F<em i="i">{\text {mlm }}$. For each masked index $i$, it outputs the probability $p\left(a</em>$. The MLM loss is computed as
$\mathbf{L}} \mid i,\langle\mathbf{m}\rangle\right)$ of the original token $a_{i<em _="{" _in="\in" _text="\text" i="i" masked="masked">{\text {mlm }}\left(\langle\mathbf{m}\rangle\right)=\operatorname{mean}</em>\rangle\right)\right)$.
Details about the MLM data are in §A.3.
During training, we sample mini-batches from the respective datasets, and minimize the weighted sum of the losses. Concretely, while pre-training on ND and TD, we sample mini-batches $X_{\mathrm{ND}}, X_{\mathrm{TD}}$ and $X_{\text {MLM }}$ and optimize the objective
$\mathbf{L}}}-\log \left(p\left(a_{i} \mid i,\langle\mathbf{m<em _mathrm_ND="\mathrm{ND">{\text {model }}\left(X</em>}}\right)+\mathbf{L<em _mathrm_TD="\mathrm{TD">{\text {model }}\left(X</em>}}\right)+\lambda \cdot \mathbf{L<em _MLM="{MLM" _text="\text">{\text {mlm }}\left(X</em>\right)$.
}<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Progression of eval accuracy (EM) of GENBERt, for different pre-training settings listed in $\S 5.1$.</p>
<h2>5 Experimental Evaluation</h2>
<p>We now evaluate our two pre-training steps and their applicability for numerical reasoning tasks. We consider the following variants, aiming to investigate the contributions of ND and TD, the importance of MLM loss, and techniques like DT and RS. In all cases, we initialize GENBERt with BERT-base-uncased, use DT and RS, and include the MLM loss, except where noted:</p>
<ul>
<li>GENBER $\mathrm{T}_{+ \text {ND }}$ : trained on numerical data.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM }}$ : trained on ND without the additional MLM loss.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM-DT }}$ : trained on ND using wordpiece tokenization, without the MLM loss.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM-RS }}$ : trained on ND without MLM loss and random shift (RS).</li>
<li>GENBER $\mathrm{T}_{+ \text {TD }}$ : trained on textual data (TD).</li>
<li>GENBER $\mathrm{T}<em _="+" _ND="{ND" _text="\text">{+ \text {ND+TD }}$ : GENBER $\mathrm{T}</em>$ trained on both ND and TD.}</li>
</ul>
<h3>5.1 Pre-training Performance</h3>
<p>We first ask whether the pre-training procedure allows GENBERt to absorb the intended numerical skills. We observe that across various settings (ND, TD, ND+TD), GENBERt consistently achieves more than $96 \%$ accuracy in predicting the correct solution for both ND and TD. Thus, we conclude that indeed a pre-trained LM can learn the designed skills from generated data.</p>
<p>Figure 4 shows the learning curves of GENBERT for the different variants. Note that in ND-LM-DT the model does not learn to solve the numerical data task. This demonstrates the utility of using DT over conventional wordpieces. The lower sample complexity in the case of ND+TD compared to the only-TD can be attributed to the fact that ND and TD share some numeric skills and hence a model already trained on ND converges faster on TD compared to GENBERT.</p>
<h3>5.2 Numerical Reasoning Performance</h3>
<p>After successfully injecting GENBERT with numeric skills, we test GENBERT guided by the following questions:
(a) Are the injected skills robust and generalize to NRoT datasets like DROP?
(b) Are the new skills learned at the expense of the model's ability to understand language?
(c) Can the pre-trained weights be used with architectures other than GENBERT?
For (a), we fine-tune GENBERT on DROP and further evaluate on MWP in a zero-shot setup. For (b), we evaluate GENBERT on a RC task that does not involve numerical reasoning, namely, SQuAD (Rajpurkar et al., 2016). For (c), we use GENBERT encoder as a drop-in replacement for BERT on two other architectures.</p>
<p>Results on DROP We report results of GENBERT initialized by BERT-base and leave pretraining a larger model for future work. We compare GENBERT to MTMSN (Hu et al., 2019) initialized with BERT-base, as MTMSN initialized with BERT-large is a state-of-the-art model on DROP. ${ }^{1}$</p>
<p>Table 4 presents fine-tuning results on DROP. Without pre-training, GENBERT performs poorly compared to current state of the art models like MTMSN, reporting an EM of only 46.1. Pretraining on each of the numerical data (ND) and textual data (TD) improves performance dramatically to 64.7 EM and 64.4 EM, respectively. Moreover, pre-training on both ND and TD leads to a performance of 68.8 EM , on par with MTMSN's 68.2 EM. This demonstrates that the skills that GENBERT learns from ND and TD are complementary. In addition, the lower performance of GENBERT ${ }<em D-L="D-L" M-R="M-R" S="S" _N="+N">{+N D-L M}$ and GENBERT ${ }</em>$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Performance of GENBERT and comparable models on the development and test sets of DROP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">number</th>
<th style="text-align: center;">span</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">spans</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GENBERT</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D-L M-R S}$</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D-L M}$</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 4}$</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT $_{+T D}$</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT $_{+N D+T D}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 0}$</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">$\mathbf{5 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: $\mathrm{F}_{1}$ scores on DROP development per answer type.
shows the importance of including the MLM loss and the utility of RS for short inputs.</p>
<p>Breaking down performance by answer type (Table 5) highlights several points. First, pre-training on ND and TD improves performance mostly due to number answer types, as expected. Second, GENBERT ${ }<em _BASE="{BASE" _text="\text">{+N D+T D}$ outperforms MTMSN ${ }</em>$ substantially outperforms GENBERT on questions whose answer is a list of non-contiguous spans. This is expected, as MTMSN has a specialized head and procedure for handling such questions, while build on a simpler and more standard RC architecture.}}$ on questions whose answer is a span. We argue a probable cause for this are span questions that require performing a numerical computation internally, as explained in §2. Third, MTMSN ${ }_{\text {BASE }</p>
<p>Generalization to MWP (zero-shot) The MAWPS repository is a collection of math word problem (MWP) datasets (Koncel-Kedziorski et al., 2016). To test the models on skills they were trained on, we picked datasets with addition and subtraction problems, and filtered out examples with other operations (e.g., multiplication and division). All models that were fine-tuned on DROP were evaluated in a zero-shot setup on 395 examples from ADDSub (Hosseini et al., 2014), 321 from SOP (Roy et al., 2015), and 305 from SEQ (Koncel-Kedziorski et al., 2015).</p>
<p>Results are shown in Table 6. Overall, GENBERT $<em D="D" _N="+N">{+N D+T D}$ dramatically improves performance compared to GENBERT. GENBERT $</em>$, demonstrating the utility of ND when the context is short.}$ performs much better than GENBERT $_{+T D</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ADDSub</th>
<th style="text-align: center;">SOp</th>
<th style="text-align: center;">SEQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GENBERT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +TD</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND+TD</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">$\mathbf{2 8 . 3}$</td>
<td style="text-align: center;">22.3</td>
</tr>
<tr>
<td style="text-align: left;">NABERT+</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">17.4</td>
</tr>
<tr>
<td style="text-align: left;">MTMSN BASE</td>
<td style="text-align: center;">$\mathbf{3 2 . 2}$</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">$\mathbf{3 2 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 6: EM on MWP datasets.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Breakdown of model accuracy (EM) by the number of terms in the arithmetic expression, for the MWP datasets ADDSub, SOp and SEQ.</p>
<p>Last, MTMSN outperforms GENBERT $+_{\text {ND+TD }}$. However, MTMSN uses a specialized architecture for addition and subtraction, suitable when calculations are done outside of the model. GENBERT, on the other hand, is a general-purpose generative model, that can also return span answers when the computation is done internally.</p>
<p>Next, we break down performance by the number of terms in the arithmetic expression (Figure 5). The plot shows that all models struggle to generalize to more complex problems, and completely fail when the calculation involves more than 3 terms. Interestingly, the drop in performance of GENBERT +ND +TD between 2 and 3 terms is significantly smaller than that of GENBERT +ND and GENBERT +TD . This suggests that both ND and TD are useful for improving robustness.</p>
<p>Error analysis To understand the limitations of our method, we analyze the errors of GENBERT +ND +TD on the development set of DROP, excluding questions with a multi-span answer which are not supported by the model. We sample 100 random examples for which GENBERT +ND +TD fails to predict the correct answer and manually analyze the types of questions and mistakes done by the model.</p>
<p>We find that in almost half of the cases (43\%), the example requires reasoning skills that are either not covered by the pre-training tasks (e.g. sorting), or not numerical. Another common case (23\%) is inaccurate predictions, such as spans that are too</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">EM</th>
<th style="text-align: left;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: left;">$\mathbf{8 8 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND-LM</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">85.8</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">88.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +TD</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">88.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND +TD</td>
<td style="text-align: left;">$\mathbf{8 1 . 3}$</td>
<td style="text-align: left;">$\mathbf{8 8 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance on SQuAD v1 development set. Scores for BERT are using wordpiece tokenization.
long and numbers with partial digit match to the gold answer. We note that many of these errors can be addressed by extending the pre-training tasks to cover additional numerical skills and a larger number range. We leave such extensions for future work. Further details and example failure cases are provided in §A.5.</p>
<h3>5.3 Reading Comprehension Performance</h3>
<p>Having shown that our models successfully learned to perform NRoT, we investigate if this improvement comes at the expense of performance on RC datasets. We initialize the RC model from Devlin et al. (2019) with GENBERT weights (encoder only) and fine-tune it on SQUAD v1. As shown in Table 7, the performance of GENBERT +ND +TD is almost identical to the original BERT. Moreover, GENBERT +ND-LM reported a loss of 3 EM points highlighting the importance of using the MLM loss.</p>
<h3>5.4 GENBERT With Other Architectures</h3>
<p>To further establish the utility of GENBERT, we used the weights of GENBERT +ND +TD to initialize the encoder of NABERT+ and MS-TAG, a recent multi-span tagging model of Efrat et al. (2019). Fine-tuning on DROP shows an improvement of $\sim 2$ EM points compared to the originally reported performance: $63.0 \rightarrow 65.1$ EM for NABERT+, and $67.3 \rightarrow 69.3$ EM for MS-TAG. This shows that GENBERT can be used as a drop-in replacement for BERT, when numerical reasoning is needed.</p>
<p>To summarize, we have empirically shown that one can inject numerical reasoning skills into a pre-trained LM, resulting in good performance on DROP, generalization to MWP, while maintaining high performance on standard RC datasets. Moreover, the resulting weights can be used for initializing numerical reasoning models.</p>
<h2>6 Related Work</h2>
<p>Most NRoT models designed for DROP are extractive QA models augmented with specialized modules (§2). Two recent work (Andor et al., 2019;</p>
<p>Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model.</p>
<p>A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020).</p>
<p>Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever.</p>
<h2>7 Conclusions</h2>
<p>Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limited flexibility. In this work, we propose a general method for injecting additional skills into LMs, assuming automatic data generation is possible. We apply our approach to the task of numerical reasoning over text, using a general-purpose model called GENBERT, and a simple framework for generating large amounts of synthetic examples. Our experiments demonstrate the effectiveness of our method, showing that GENBERT successfully learns the numerical skills, and performs on par with state-of-the-art NRoT models of the same size.</p>
<h2>Acknowledgments</h2>
<p>We thank Daniel Andor and Thang Luong for helpful discussions, and Shimi Salant for constructive suggestions. This research was partially supported by The Israel Science Foundation grant 942/16, The Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800).</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In North American Association for Com-
putational Linguistics (NAACL), pages 2357-2367, Minneapolis, Minnesota.</p>
<p>Daniel Andor, Emily Pitler, Kenton Lee, and Luheng He. 2019. Giving bert a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations (ICLR).</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL), pages 4171-4186, Minneapolis, Minnesota.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Association for Computational Linguistics (NAACL).</p>
<p>Avia Efrat, Elad Segal, and Mor Shoham. 2019. Tagbased multi-span extraction in reading comprehension. arXiv preprint arXiv:1909.13375.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.</p>
<p>John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word representations. In North American Association for Computational Linguistics (NAACL), pages 4129-4138.
M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Empirical Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. Automated Knowledge Base Construction (AKBC).</p>
<p>Jambay Kinley and Raymond Lin. 2019. NABERT+: Improving Numerical Reasoning in Reading Comprehension.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. In Transactions of the Association for Computational Linguistics (TACL), volume 3, pages 585597. MIT Press.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Association for Computational Linguistics (ACL), pages 1152-1157, San Diego, California.</p>
<p>Guillaume Lample and FranÃğois Charton. 2020. Deep learning for symbolic mathematics. In International Conference on Learning Representations (ICLR).</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Computational Linguistics (ACL).</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361, Hong Kong, China. Association for Computational Linguistics.
S. Roy, T. Vieira, and D. Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics (TACL), 1 .</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Empirical Methods in Natural Language Processing (EMNLP), pages 17431752.</p>
<p>Ohad Rozen, Vered Shwartz, Roee Aharoni, and Ido Dagan. 2019. Diversify your datasets: Analyzing generalization via controlled variance in adversarial datasets. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 196-205, Hong Kong, China. Association for Computational Linguistics.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations (ICLR).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), pages 5998-6008.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? probing numeracy in embeddings. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.</p>
<h2>A Supplemental Material</h2>
<h2>A. 1 Synthetic Numerical Data Generation</h2>
<p>We briefly describe the numerical templates, providing the details missing from Table 2. In all cases, integers are sampled from ${0, \ldots, 20 K}$, and split into disjoint train and development sets to assure generalization.</p>
<ul>
<li>signed float combination : Random signed combinations of up to 4 floats. Floats are sampled from the set of floats with two decimal places.</li>
<li>min/max/avg : We sample 2-4 floats and apply a min, max, avg, operation by sampling a word from the set $\mathcal{O}=$ {"longest", "last", "highest", "largest", "most", "shortest", "first", "smallest", "lowest", "least", "average"}.</li>
<li>$\arg \max , \arg \min$ : We sample word-float pairs, where words are sampled from $\mathcal{W}$ : words in the NLTK Words Corpus ${ }^{2}$ having at most 2 wordpieces, and floats are sampled as above.</li>
<li>date max/min : Same as min/max/avg above, but for dates. Dates are sampled from $\mathcal{D}$ : the set of dates until Sep 2019. The operator word is sampled from $\mathcal{D} \mathcal{S} \mathcal{U} \mathcal{P}=$ {"last", "latest", "most recent", "youngest", "first", "earliest", "oldest", "least recent"} and mapped to min or max.</li>
<li>date difference : This teaches our model to perform date arithmetic in days, months and years.</li>
<li>percentage : We teach our model to perform $100-x$ operations in the context of percentages. Given a number of arguments, we sample a percentage split using a flat Dirichlet distribution.</li>
</ul>
<h2>A. 2 Synthetic Textual Data Generation</h2>
<h2>A.2.1 Sentence template extraction</h2>
<p>To extract sentence templates, we abstract the text of math word problems from the corpus published by Hosseini et al. (2014). Going over examples, we split the problem text into sentences ${ }^{3}$, and abstract the tokens of each sentence independently. Tokens are abstracted according to the framework into numbers (NUM), verb categories (VERB), entities (ENT), containers (CONT) and attributes (ATTR).</p>
<p>To have a better control over the generation process, we extend the framework of Hosseini et al. (2014) to support two container types - agent (AGT) and environment (ENV). Agents are objects which actively collect and drop entities, for example a person or an organization. Environments are passive containers, such as places or time periods. In addi-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion, we introduce two-level containers to express inclusion relation between containers. For instance, if 3 submarines anchor near the city of Devonport, then they also anchor near the country of England.</p>
<p>The 12 most common extracted sentence templates, which were used for generating synthetic data, are provided in Table 8.</p>
<h2>A.2.2 Template instantiation</h2>
<p>Sentence templates are instantiated with a small vocabulary, that map categories into words. In this work, we construct two domain-specific smallworld vocabularies, about history and the National Football League. The vocabularies are available in a json format in https://github.com/ ag1988/injecting_numeracy.</p>
<h2>A.2.3 Question templates</h2>
<p>The 13 question templates for 7 different skills are provided in Table 9.</p>
<h2>A. 3 Data for Masked LM task</h2>
<p>For creating the training data for the masked LM task (§5.1) we took the pages from English Wikipedia whose lowercased title containing a string in {season, economy, demographics, conquest, war, battle, uprising, rebellion, insurgency, conflict, crisis, revolution, military history, mutiny, regiment, revolt, geography, raids, insurrection, invasion, feud, siege, campaign, expedition, succession, coup, university}. This resulted in 156 K full pages. In the remaining pages, paras with $&lt;15$ numbers were discarded. Pages were tokenized using DT (§ 3) and chunked into 512-token sequences. Following Devlin et al. (2019), each token was masked with probability 0.15 with no more than 65 masks per sample. This gave us 0.7 M samples.</p>
<h2>A. 4 Experimental Setup</h2>
<p>For all our experiments, we used an older version of Hugging Face's Transformers library (Wolf et al., 2019) and provide our training hyperparameters in Table 10.</p>
<h2>A. 5 GENBERT $_{\text {+ND+TD }}$ Error Analysis</h2>
<p>Table 11 summarizes the main failure types of GEN$\mathrm{BERT}_{+ \mathrm{ND}+ \mathrm{TD}}$ on 100 random examples from the development set of DROP, excluding questions with a multi-span answer.</p>
<h1>Template</h1>
<p>CONT-1-AGT VERB-1- NUM-1 ATTR-1 ENT-1 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 and CONT-2-AGT VERB-1-POS NUM-2 ATTR-1 ENT-1 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 and NUM-2 ATTR-2 ENT-2 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1, but VERB-2-NEG NUM-2 ATTR-2 ENT-2 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 in ATTR-2 CONT-2-ENV .
CONT-1-AGT VERB-1-NEG NUM-1 of the ATTR-1 ENT-1 .
CONT-1-AGT had NUM-1 ATTR-1 ENT-1, CONT-2-AGT had NUM-2 ATTR-1 ENT-1, and CONT-3-AGT had NUM-3 ATTR-1 ENT-1 .
NUM-1 ATTR-1 ENT-1, NUM-2 ATTR-2 ENT-2, and NUM-3 ATTR-3 ENT-3 were VERB-1-POS in ATTR-4 CONT-1-ENV.
There were NUM-1 ATTR-1 ENT-1 and NUM-2 ATTR-2 ENT-2 in ATTR-3 CONT-1-ENV .
There were NUM-1 ATTR-1 ENT-1 in ATTR-2 CONT-1-ENV .
CONT-1-AGT VERB-1-NEGTRN NUM-1 ATTR-1 ENT-1 to CONT-2-AGT .
CONT-1-AGT VERB-1-POSTRN NUM-1 ATTR-1 ENT-1 from CONT-2-AGT .
Table 8: Sentence templates for synthetic textual examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reasoning</th>
<th style="text-align: left;">Templates</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Selection</td>
<td style="text-align: left;">How many ATTR-1 ENT-1 were in CONT-1-ENV? <br> How many ATTR-1 ENT-1 did CONT-1-AGT VERB-POS?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity difference</td>
<td style="text-align: left;">How many more ATTR-1 ENT-1 were in CONT-1-ENV than ATTR-2 ENT-2 ? <br> How many more ATTR-1 ENT-1 did CONT-1-AGT have than ATTR-2 ENT-2 ?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity subset</td>
<td style="text-align: left;">How many ENT-1 of CONT-1 were ATTR-1 ENT-1? <br> How many ENT-1 of CONT-1 were not ATTR-1 ENT-1?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity comparison</td>
<td style="text-align: left;">Were there {more I less} ATTR-1 ENT-1 in CONT-1-ENV or in CONT-2-ENV? <br> Who had {more I less} ATTR-1 ENT-1, CONT-1-AGT or CONT-2-AGT?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity superlative</td>
<td style="text-align: left;">Who had the {highest I lowest} number of ATTR-1 ENT-1 in total?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity superlative</td>
<td style="text-align: left;">What was the {highest I lowest} number of ATTR-1 ENT-1 VERB-POS in CONT-1-ENV? <br> What is the {highest I lowest} number of ATTR-1 ENT-1 CONT-1-AGT VERB-POS?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity sum</td>
<td style="text-align: left;">How many ATTR-1 ENT-1 were in CONT-1-ENV (, CONT-<em>-ENV) and CONT-2-ENV {in total I combined}? <br> How many ATTR-1 ENT-1 did CONT-1-ENV (, CONT-</em>-ENV) and CONT-2-ENV have {in total I combined}?</td>
</tr>
</tbody>
</table>
<p>Table 9: Templates for questions about generated synthetic passages, testing for numerical reasoning. The template placeholders are filled-in with values from the world state obtained after generating the synthetic passage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">pre-training</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">finetuning</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">b</td>
<td style="text-align: center;">bsz</td>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">b</td>
<td style="text-align: center;">bsz</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3e-5</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID }}$</td>
<td style="text-align: center;">$6 e-5$</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM-DT }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM-RS }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID }}$</td>
<td style="text-align: center;">$1 e-5$</td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID+TD }}$</td>
<td style="text-align: center;">$1 e-5$</td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">14</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyperparameters used for pre-training GENBERT and finetuning it on DROP. $\mathrm{l} \mathrm{t}=$ leaning rate, bsz=train batch size. Common params: seed=42, optimizer=Bert-Adam, linear-lr-warm-up=0.1, num epochs for finetuning=30, weightdecay $=0.01$, max-grad-norm $=1.0$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error category</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Counting <br> Sorting <br> Complex calculation <br> Complex semantics <br> Not numerical</td>
<td style="text-align: center;">q: How many people were the heads of the business? <br> q: Which nationality was the fourth largest? <br> q: How many percent of people were either Black Hispanic, of Sub-Saharan African origin, or of West Indian or Afro-Caribbean American origin? <br> q: By how many points did the Pistons lose their closest game? <br> q: Who defeated the Kievan Rus at the Battle of the Alta River?</td>
</tr>
<tr>
<td style="text-align: center;">Longer span</td>
<td style="text-align: center;">q: Where there more people in the peninsula pre-war or at the time of the first census? <br> a: pre-war <br> p: pre-war population</td>
</tr>
<tr>
<td style="text-align: center;">Shorter span</td>
<td style="text-align: center;">q: Was the life expectancy in 2015 higher for males or females? <br> a: females <br> p: female</td>
</tr>
<tr>
<td style="text-align: center;">Imprecise number prediction</td>
<td style="text-align: center;">q: How many more estimated Chinese Americans lived in California compared to Massachusetts? <br> a: 1130100 <br> p: 110100</td>
</tr>
</tbody>
</table>
<p>Table 11: Error categories of GENBERT ${ }_{\text {AND }+ \text { TD }}$ on the development set of DROP, based on a manual error analysis of 85 random examples. The upper part shows categories which are not not covered by our pre-training tasks or do not require numerical skills. The lower part shows categories of inaccurate model predictions. The letters $\mathbf{q}, \mathbf{a}$ and $\mathbf{p}$ denote the question, gold answer and model prediction, respectively.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.nltk.org/
${ }^{3}$ Using the Spacy library http://spacy.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8986 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8986</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8986</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-604764133befe7a0aaa692919545846197e6e065</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/604764133befe7a0aaa692919545846197e6e065" target="_blank">Neural Text Generation from Structured Data with Application to the Biography Domain</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A neural model for concept-to-text generation that scales to large, rich domains and significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU is introduced.</p>
                <p><strong>Paper Abstract:</strong> This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8986.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8986.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Table NLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table-conditioned Neural Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feed-forward neural language model that conditions generation on Wikipedia infobox tables by embedding table structure (field/type and token positions) both locally and globally, and mixing vocabulary scores with table-token copy scores to allow generation of OOV table values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>table-conditioned linearized field-token representation (field/position triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent each infobox as a set of field/value sequences. Each token occurrence is described by triplets z_w = {(field f_j, start position p^+, end position p^-)} and embedded via learned position-specific matrices (Z^+, Z^-). Global table information is summarized via embeddings for present fields (G^f) and present words (G^w). Context embedding is the concatenation of word embeddings, local (z) embeddings and global (g_f,g_w) embeddings; multiple occurrences are aggregated with component-wise max-pooling. Output scoring mixes a standard vocabulary score (phi^W) and a table-token score (phi^Q) enabling copy actions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Infobox fact tables (treated as sets of field/value sequences) — analogous to person subgraphs in knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize field values and sentences (lowercased; numbers mapped to special tokens; years to distinct tokens); for each table token produce a field/position triplet and a corresponding special table-token id in Q (e.g. name_1); embed tokens and table features, aggregate multiple occurrences with max-pooling, compute context vector, score both vocabulary and table tokens and decode with beam search; copying implemented by selecting Q tokens which are replaced by the original table string.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Concept-to-text generation (first-sentence biography generation from Wikipedia infoboxes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best reported (Table NLM + Local + Global(field & word)): perplexity 4.40 ± 0.02; BLEU 34.7 ± 0.36; ROUGE 25.8 ± 0.36; NIST 7.98 ± 0.07 (reporting mean ± std over 5 runs). Improvement described as nearly +15 BLEU over Template Kneser-Ney baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms interpolated Kneser-Ney (KN) and a Template KN baseline: KN BLEU=2.21, Template KN BLEU=19.8; Table NLM variants: with local+copy BLEU=26.0, +global(fields) BLEU=33.4, +global(field&word) BLEU=34.7. Also shown to be several times faster at decoding than Template KN for comparable beam sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Scales to large vocabularies and many field types via embedding factorization; handles OOV/table-specific tokens via copy actions; local conditioning captures field-specific token transitions; global conditioning captures high-level themes (e.g. scientist vs artist); matrix operations allow efficient GPU decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Field-value encoding is relatively simple (only field type + token position with max-pooling). Maximum token position l is capped (l=10), losing fine-grained position info for longer fields. Perplexity comparisons are complicated by variable output vocabularies (W ∪ Q). Training loss does not explicitly penalize factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reports factual errors (e.g., incorrect occupation or nationality) when table information is insufficient or not properly encoded; example where training data had inconsistent facts between table and sentence (birth month mismatch). Model without global conditioning picks common occupations incorrectly; even full model can still produce incorrect facts because loss does not penalize factual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Text Generation from Structured Data with Application to the Biography Domain', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8986.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8986.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template KN (delexicalized KN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template Kneser-Ney with delexicalization (special field-position tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline language model that replaces words occurring in both table and sentence with special descriptors (table-position tokens like name_1) during training, then constrains decoding to emit either vocabulary words or these special tokens which are copied from the table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalized template representation (field-position descriptor tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert training sentences by replacing every word that appears in the input table with a special token encoding its table descriptor z_w (e.g., name_1, birthdate_2). Train a standard Kneser-Ney n-gram model on these template sentences. At inference, the decoder may emit template tokens which are then substituted/copied with the original table values.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Infobox fact tables (field/value pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Delexicalization: map table-occurring words in training sentences to descriptor tokens representing (field, position); train an n-gram Kneser-Ney language model on these templates; during decoding, when a template token is generated copy the corresponding table value into the output.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Concept-to-text generation (first-sentence biography generation) — used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Template KN: perplexity 7.46 (not directly comparable due to differing vocabularies), BLEU 19.8, ROUGE 10.7, NIST 5.19.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Substantially outperforms vanilla KN LM (BLEU 2.21) but is substantially outperformed by Table NLM (Table NLM + global(field&word) BLEU=34.7). Template KN is also slower in decoding due to many n-gram lookups required during beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; enables copying of table tokens (including OOV) via template tokens; improves over standard n-gram LM without table awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Less flexible than neural approaches (hard-coded descriptors), relies on exact matches and templates, slower decoding due to expensive n-gram lookups, and lower generation quality compared to Table NLM.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Example generation showed incorrect occupation (produced 'cricketer' for Frederick Parker-Rhodes) — baseline can assign wrong facts when table lacks explicit cues; template approach cannot infer missing relations beyond direct table token substitution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Text Generation from Structured Data with Application to the Biography Domain', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8986.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8986.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy actions / Q tokens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy actions via table-token set Q and mixed scoring (phi^W + phi^Q)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that augments the output domain with a dynamic set Q of tokens derived from the input table so the model can output/copy table-specific (possibly OOV) tokens; scoring mixes standard vocabulary logits with table-token logits computed from field-position embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>dynamic table-token copy representation (W ∪ Q) with mixed output scoring</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Define output domain as union of fixed vocabulary W and table-specific tokens Q (each table token given a Q id like field_position). For each table token compute a vector q(w) from field/position embeddings projected into the context space; compute phi^Q(x,w)=h(x)·q(w) and add to the vocabulary score phi^W(x,w); softmax over W ∪ Q yields probabilities. Selecting a Q token copies the corresponding original table string into the generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Infobox tables / table-derived token sets (analogous to nodes/values in a KB subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For every table token create a Q id (e.g. name_2). During scoring, compute both vocabulary scores and table-token scores and sum them; at decode time, emitted Q tokens are substituted by the original table strings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Biography sentence generation (handles OOV and table-specific strings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enabling copy actions gave large gains: Table NLM w/ Local (field,start) + copy BLEU=26.0; +Local(start,end) BLEU=26.6; +Global(field) BLEU=33.4; +Global(field & word) BLEU=34.7. Authors report these copy-enabled variants produce the large improvements over the Template KN baseline (~+15 BLEU total).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Inspired by attention-based copying in NMT (Luong et al. 2015) and delexicalization approaches in dialog systems (Wen et al. 2015). Compared to models without copy actions, models with copy actions massively improve BLEU/ROUGE/NIST.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Handles OOVs and table-specific strings, enables faithful copying of names/dates; integrates cleanly into the softmax and training; demonstrably improves automatic generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Expands the effective output domain per-example which complicates perplexity comparisons; requires bookkeeping of per-example Q tokens; does not by itself prevent factual errors beyond copying available tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>While copy actions ensure correct copying of tokens present in the table (e.g., names, dates), the model can still produce incorrect facts not directly copied (e.g., wrong occupation) because selection/inference of which field tokens to generate can be wrong and training loss does not penalize factual inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Text Generation from Structured Data with Application to the Biography Domain', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8986.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8986.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local/Global conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local conditioning (z_c_t) and Global conditioning (g_f, g_w) via embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary conditioning schemes: local conditioning describes the relation between previously generated tokens and their occurrences in the table via (field, start, end) triplets; global conditioning summarizes the set of fields present (g_f) and the set of field tokens present (g_w) using binary indicators and pooled embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>local (field-position triplets) and global (field/word indicator embeddings) conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Local conditioning: for each context token use embeddings for its field and token start/end positions (Z^+, Z^-) aggregated per-token via component-wise max to form psi_Z(z_{c_t}). Global conditioning: represent presence of fields (g_f ∈ {0,1}^|F|) and presence of words (g_w ∈ {0,1}^{|W|}) mapped through G^f and G^w and aggregated with max to form table summary vectors. Concatenate local and global vectors with word embeddings to produce the context representation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Infobox tables (field/value sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map occurrences to (field, start, end) triplets for local features; map field presence and word presence to binary indicators then to embeddings for global features; aggregate multiple instances with component-wise max-pooling; concatenate with word-context embeddings to feed the neural LM.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Biography first-sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Local conditioning alone improved performance: NLM → +Local (field,start,end) reduced perplexity from 9.40 to 8.61 and increased BLEU from 2.41 to 4.17 (no copy). With copy actions and local conditioning: BLEU=26.0; adding global(field) improved BLEU to 33.4 and adding global(field & word) to 34.7 (final).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors show local and global conditioning are complementary: local helps learn intra-field transitions and linking previous predictions to the table, global helps determine high-level themes (e.g. profession) and disambiguate similar field sets.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Local conditioning captures field-specific sequential patterns (e.g., dates), global conditioning provides context about which fields/words exist in the table enabling better content selection and disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Aggregation with max-pooling is lossy (loses ordering and multiplicity), position encoding is capped (max position l=10), and current encoding may be insufficient to infer certain facts without explicit global word conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Without global conditioning the model may predict generic/common occupations (wrong inference). The model still produces factual errors when the table lacks explicit cues or when the representation loses critical order/relational information; authors suggest recurrent or convolutional encoders as future work to mitigate these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Text Generation from Structured Data with Application to the Biography Domain', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8986.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8986.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Max-pooling aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component-wise max-pooling aggregation for multiple occurrences and global pooling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A permutation-invariant aggregation method used to combine embeddings of multiple occurrences of the same token (or multiple present fields/words) into a single fixed-size vector by taking the component-wise maximum across occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>component-wise max-pooling aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>When a word/token occurs multiple times in a table, take the component-wise maximum over its start-position embeddings and over its end-position embeddings to produce a single per-token local embedding. For global conditioning, take component-wise max over embeddings of present fields and present words to yield fixed-size global vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Infobox tables (aggregating multiple field occurrences)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute embeddings for each occurrence (position-specific), then aggregate across occurrences with component-wise max to obtain a single representation per word/field used in the model's context vector.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Supports biography sentence generation by producing fixed-size representations regardless of variable numbers of occurrences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported as a standalone metric; used as part of the Table NLM whose overall best metrics are: perplexity 4.40 ± 0.02, BLEU 34.7 ± 0.36, ROUGE 25.8 ± 0.36, NIST 7.98 ± 0.07.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>No explicit empirical comparison to other pooling schemes in this paper; authors note max pooling is simple and permutation invariant but suggest richer encoders (RNN/CNN with other pooling) as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, efficient, permutation-invariant, and selects the strongest signal across multiple occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lossy: discards order information, counts, and finer sequence structure; position capping and pooling may discard useful context for long fields.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail to capture field transitions or ordered relations that matter for factual generation; authors cite this as a limitation and propose exploring recurrent or convolutional encoders in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Text Generation from Structured Data with Application to the Biography Domain', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What to talk about and how? selective generation using lstms with coarse-to-fine alignment <em>(Rating: 2)</em></li>
                <li>Addressing the rare word problem in neural machine translation <em>(Rating: 2)</em></li>
                <li>Semantically conditioned lstm-based natural language generation for spoken dialogue systems <em>(Rating: 2)</em></li>
                <li>Learning semantic correspondences with less supervision <em>(Rating: 2)</em></li>
                <li>Generative alignment and semantic parsing for learning from ambiguous supervision <em>(Rating: 2)</em></li>
                <li>A global model for concept-to-text generation <em>(Rating: 2)</em></li>
                <li>A simple domain-independent probabilistic approach to generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8986",
    "paper_id": "paper-604764133befe7a0aaa692919545846197e6e065",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Table NLM",
            "name_full": "Table-conditioned Neural Language Model",
            "brief_description": "A feed-forward neural language model that conditions generation on Wikipedia infobox tables by embedding table structure (field/type and token positions) both locally and globally, and mixing vocabulary scores with table-token copy scores to allow generation of OOV table values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "table-conditioned linearized field-token representation (field/position triplets)",
            "representation_description": "Represent each infobox as a set of field/value sequences. Each token occurrence is described by triplets z_w = {(field f_j, start position p^+, end position p^-)} and embedded via learned position-specific matrices (Z^+, Z^-). Global table information is summarized via embeddings for present fields (G^f) and present words (G^w). Context embedding is the concatenation of word embeddings, local (z) embeddings and global (g_f,g_w) embeddings; multiple occurrences are aggregated with component-wise max-pooling. Output scoring mixes a standard vocabulary score (phi^W) and a table-token score (phi^Q) enabling copy actions.",
            "graph_type": "Infobox fact tables (treated as sets of field/value sequences) — analogous to person subgraphs in knowledge bases",
            "conversion_method": "Tokenize field values and sentences (lowercased; numbers mapped to special tokens; years to distinct tokens); for each table token produce a field/position triplet and a corresponding special table-token id in Q (e.g. name_1); embed tokens and table features, aggregate multiple occurrences with max-pooling, compute context vector, score both vocabulary and table tokens and decode with beam search; copying implemented by selecting Q tokens which are replaced by the original table string.",
            "downstream_task": "Concept-to-text generation (first-sentence biography generation from Wikipedia infoboxes)",
            "performance_metrics": "Best reported (Table NLM + Local + Global(field & word)): perplexity 4.40 ± 0.02; BLEU 34.7 ± 0.36; ROUGE 25.8 ± 0.36; NIST 7.98 ± 0.07 (reporting mean ± std over 5 runs). Improvement described as nearly +15 BLEU over Template Kneser-Ney baseline.",
            "comparison_to_others": "Outperforms interpolated Kneser-Ney (KN) and a Template KN baseline: KN BLEU=2.21, Template KN BLEU=19.8; Table NLM variants: with local+copy BLEU=26.0, +global(fields) BLEU=33.4, +global(field&word) BLEU=34.7. Also shown to be several times faster at decoding than Template KN for comparable beam sizes.",
            "advantages": "Scales to large vocabularies and many field types via embedding factorization; handles OOV/table-specific tokens via copy actions; local conditioning captures field-specific token transitions; global conditioning captures high-level themes (e.g. scientist vs artist); matrix operations allow efficient GPU decoding.",
            "disadvantages": "Field-value encoding is relatively simple (only field type + token position with max-pooling). Maximum token position l is capped (l=10), losing fine-grained position info for longer fields. Perplexity comparisons are complicated by variable output vocabularies (W ∪ Q). Training loss does not explicitly penalize factual errors.",
            "failure_cases": "Reports factual errors (e.g., incorrect occupation or nationality) when table information is insufficient or not properly encoded; example where training data had inconsistent facts between table and sentence (birth month mismatch). Model without global conditioning picks common occupations incorrectly; even full model can still produce incorrect facts because loss does not penalize factual consistency.",
            "uuid": "e8986.0",
            "source_info": {
                "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Template KN (delexicalized KN)",
            "name_full": "Template Kneser-Ney with delexicalization (special field-position tokens)",
            "brief_description": "A baseline language model that replaces words occurring in both table and sentence with special descriptors (table-position tokens like name_1) during training, then constrains decoding to emit either vocabulary words or these special tokens which are copied from the table.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "delexicalized template representation (field-position descriptor tokens)",
            "representation_description": "Convert training sentences by replacing every word that appears in the input table with a special token encoding its table descriptor z_w (e.g., name_1, birthdate_2). Train a standard Kneser-Ney n-gram model on these template sentences. At inference, the decoder may emit template tokens which are then substituted/copied with the original table values.",
            "graph_type": "Infobox fact tables (field/value pairs)",
            "conversion_method": "Delexicalization: map table-occurring words in training sentences to descriptor tokens representing (field, position); train an n-gram Kneser-Ney language model on these templates; during decoding, when a template token is generated copy the corresponding table value into the output.",
            "downstream_task": "Concept-to-text generation (first-sentence biography generation) — used as baseline",
            "performance_metrics": "Reported Template KN: perplexity 7.46 (not directly comparable due to differing vocabularies), BLEU 19.8, ROUGE 10.7, NIST 5.19.",
            "comparison_to_others": "Substantially outperforms vanilla KN LM (BLEU 2.21) but is substantially outperformed by Table NLM (Table NLM + global(field&word) BLEU=34.7). Template KN is also slower in decoding due to many n-gram lookups required during beam search.",
            "advantages": "Simple to implement; enables copying of table tokens (including OOV) via template tokens; improves over standard n-gram LM without table awareness.",
            "disadvantages": "Less flexible than neural approaches (hard-coded descriptors), relies on exact matches and templates, slower decoding due to expensive n-gram lookups, and lower generation quality compared to Table NLM.",
            "failure_cases": "Example generation showed incorrect occupation (produced 'cricketer' for Frederick Parker-Rhodes) — baseline can assign wrong facts when table lacks explicit cues; template approach cannot infer missing relations beyond direct table token substitution.",
            "uuid": "e8986.1",
            "source_info": {
                "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Copy actions / Q tokens",
            "name_full": "Copy actions via table-token set Q and mixed scoring (phi^W + phi^Q)",
            "brief_description": "A mechanism that augments the output domain with a dynamic set Q of tokens derived from the input table so the model can output/copy table-specific (possibly OOV) tokens; scoring mixes standard vocabulary logits with table-token logits computed from field-position embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "dynamic table-token copy representation (W ∪ Q) with mixed output scoring",
            "representation_description": "Define output domain as union of fixed vocabulary W and table-specific tokens Q (each table token given a Q id like field_position). For each table token compute a vector q(w) from field/position embeddings projected into the context space; compute phi^Q(x,w)=h(x)·q(w) and add to the vocabulary score phi^W(x,w); softmax over W ∪ Q yields probabilities. Selecting a Q token copies the corresponding original table string into the generated text.",
            "graph_type": "Infobox tables / table-derived token sets (analogous to nodes/values in a KB subgraph)",
            "conversion_method": "For every table token create a Q id (e.g. name_2). During scoring, compute both vocabulary scores and table-token scores and sum them; at decode time, emitted Q tokens are substituted by the original table strings.",
            "downstream_task": "Biography sentence generation (handles OOV and table-specific strings)",
            "performance_metrics": "Enabling copy actions gave large gains: Table NLM w/ Local (field,start) + copy BLEU=26.0; +Local(start,end) BLEU=26.6; +Global(field) BLEU=33.4; +Global(field & word) BLEU=34.7. Authors report these copy-enabled variants produce the large improvements over the Template KN baseline (~+15 BLEU total).",
            "comparison_to_others": "Inspired by attention-based copying in NMT (Luong et al. 2015) and delexicalization approaches in dialog systems (Wen et al. 2015). Compared to models without copy actions, models with copy actions massively improve BLEU/ROUGE/NIST.",
            "advantages": "Handles OOVs and table-specific strings, enables faithful copying of names/dates; integrates cleanly into the softmax and training; demonstrably improves automatic generation metrics.",
            "disadvantages": "Expands the effective output domain per-example which complicates perplexity comparisons; requires bookkeeping of per-example Q tokens; does not by itself prevent factual errors beyond copying available tokens.",
            "failure_cases": "While copy actions ensure correct copying of tokens present in the table (e.g., names, dates), the model can still produce incorrect facts not directly copied (e.g., wrong occupation) because selection/inference of which field tokens to generate can be wrong and training loss does not penalize factual inconsistency.",
            "uuid": "e8986.2",
            "source_info": {
                "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Local/Global conditioning",
            "name_full": "Local conditioning (z_c_t) and Global conditioning (g_f, g_w) via embeddings",
            "brief_description": "Two complementary conditioning schemes: local conditioning describes the relation between previously generated tokens and their occurrences in the table via (field, start, end) triplets; global conditioning summarizes the set of fields present (g_f) and the set of field tokens present (g_w) using binary indicators and pooled embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "local (field-position triplets) and global (field/word indicator embeddings) conditioning",
            "representation_description": "Local conditioning: for each context token use embeddings for its field and token start/end positions (Z^+, Z^-) aggregated per-token via component-wise max to form psi_Z(z_{c_t}). Global conditioning: represent presence of fields (g_f ∈ {0,1}^|F|) and presence of words (g_w ∈ {0,1}^{|W|}) mapped through G^f and G^w and aggregated with max to form table summary vectors. Concatenate local and global vectors with word embeddings to produce the context representation.",
            "graph_type": "Infobox tables (field/value sequences)",
            "conversion_method": "Map occurrences to (field, start, end) triplets for local features; map field presence and word presence to binary indicators then to embeddings for global features; aggregate multiple instances with component-wise max-pooling; concatenate with word-context embeddings to feed the neural LM.",
            "downstream_task": "Biography first-sentence generation",
            "performance_metrics": "Local conditioning alone improved performance: NLM → +Local (field,start,end) reduced perplexity from 9.40 to 8.61 and increased BLEU from 2.41 to 4.17 (no copy). With copy actions and local conditioning: BLEU=26.0; adding global(field) improved BLEU to 33.4 and adding global(field & word) to 34.7 (final).",
            "comparison_to_others": "Authors show local and global conditioning are complementary: local helps learn intra-field transitions and linking previous predictions to the table, global helps determine high-level themes (e.g. profession) and disambiguate similar field sets.",
            "advantages": "Local conditioning captures field-specific sequential patterns (e.g., dates), global conditioning provides context about which fields/words exist in the table enabling better content selection and disambiguation.",
            "disadvantages": "Aggregation with max-pooling is lossy (loses ordering and multiplicity), position encoding is capped (max position l=10), and current encoding may be insufficient to infer certain facts without explicit global word conditioning.",
            "failure_cases": "Without global conditioning the model may predict generic/common occupations (wrong inference). The model still produces factual errors when the table lacks explicit cues or when the representation loses critical order/relational information; authors suggest recurrent or convolutional encoders as future work to mitigate these issues.",
            "uuid": "e8986.3",
            "source_info": {
                "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Max-pooling aggregation",
            "name_full": "Component-wise max-pooling aggregation for multiple occurrences and global pooling",
            "brief_description": "A permutation-invariant aggregation method used to combine embeddings of multiple occurrences of the same token (or multiple present fields/words) into a single fixed-size vector by taking the component-wise maximum across occurrences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "component-wise max-pooling aggregation",
            "representation_description": "When a word/token occurs multiple times in a table, take the component-wise maximum over its start-position embeddings and over its end-position embeddings to produce a single per-token local embedding. For global conditioning, take component-wise max over embeddings of present fields and present words to yield fixed-size global vectors.",
            "graph_type": "Infobox tables (aggregating multiple field occurrences)",
            "conversion_method": "Compute embeddings for each occurrence (position-specific), then aggregate across occurrences with component-wise max to obtain a single representation per word/field used in the model's context vector.",
            "downstream_task": "Supports biography sentence generation by producing fixed-size representations regardless of variable numbers of occurrences",
            "performance_metrics": "Not reported as a standalone metric; used as part of the Table NLM whose overall best metrics are: perplexity 4.40 ± 0.02, BLEU 34.7 ± 0.36, ROUGE 25.8 ± 0.36, NIST 7.98 ± 0.07.",
            "comparison_to_others": "No explicit empirical comparison to other pooling schemes in this paper; authors note max pooling is simple and permutation invariant but suggest richer encoders (RNN/CNN with other pooling) as future work.",
            "advantages": "Simple, efficient, permutation-invariant, and selects the strongest signal across multiple occurrences.",
            "disadvantages": "Lossy: discards order information, counts, and finer sequence structure; position capping and pooling may discard useful context for long fields.",
            "failure_cases": "May fail to capture field transitions or ordered relations that matter for factual generation; authors cite this as a limitation and propose exploring recurrent or convolutional encoders in future work.",
            "uuid": "e8986.4",
            "source_info": {
                "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
                "publication_date_yy_mm": "2016-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment",
            "rating": 2
        },
        {
            "paper_title": "Addressing the rare word problem in neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
            "rating": 2
        },
        {
            "paper_title": "Learning semantic correspondences with less supervision",
            "rating": 2
        },
        {
            "paper_title": "Generative alignment and semantic parsing for learning from ambiguous supervision",
            "rating": 2
        },
        {
            "paper_title": "A global model for concept-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "A simple domain-independent probabilistic approach to generation",
            "rating": 1
        }
    ],
    "cost": 0.017943499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Text Generation from Structured Data with Application to the Biography Domain</h1>
<p>Rémi Lebret*<br>EPFL, Switzerland<br>David Grangier<br>Facebook AI Research<br>Michael Auli<br>Facebook AI Research</p>
<h4>Abstract</h4>
<p>This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. It generates biographical sentences from fact tables on a new dataset of biographies from Wikipedia. This set is an order of magnitude larger than existing resources with over 700 k samples and a 400 k vocabulary. Our model builds on conditional neural language models for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. To deal with structured data, we allow the model to embed words differently depending on the data fields in which they occur. Our neural model significantly outperforms a Templated Kneser-Ney language model by nearly 15 BLEU.</p>
<h2>1 Introduction</h2>
<p>Concept-to-text generation renders structured records into natural language (Reiter et al., 2000). A typical application is to generate a weather forecast based on a set of structured meteorological measurements. In contrast to previous work, we scale to the large and very diverse problem of generating biographies based on Wikipedia infoboxes. An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012). Similar generation applications include the generation of product descriptions based on a catalog of millions of items with dozens of attributes each.</p>
<p>Previous work experimented with datasets that contain only a few tens of thousands of records such as Weathergov or the Robocup dataset, while our dataset contains over 700k biographies from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Wikipedia. Furthermore, these datasets have a limited vocabulary of only about 350 words each, compared to over 400 k words in our dataset.</p>
<p>To tackle this problem we introduce a statistical generation model conditioned on a Wikipedia infobox. We focus on the generation of the first sentence of a biography which requires the model to select among a large number of possible fields to generate an adequate output. Such diversity makes it difficult for classical count-based models to estimate probabilities of rare events due to data sparsity. We address this issue by parameterizing words and fields as embeddings, along with a neural language model operating on them (Bengio et al., 2003). This factorization allows us to scale to a larger number of words and fields than Liang et al. (2009), or Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.</p>
<p>Moreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).</p>
<p>Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality to understand highlevel themes such as that the biography is about a scientist or an artist, while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox. We analyze the effectiveness of each and demonstrate their complementarity.</p>
<h2>2 Related Work</h2>
<p>Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androut-</p>
<p>sopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences.</p>
<p>Data-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).</p>
<p>At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011).</p>
<p>Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).</p>
<p>Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the WEATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frederick Parker-Rhodes</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Born</td>
<td style="text-align: center;">21 November 1914 <br> Newington, Yorkshire</td>
</tr>
<tr>
<td style="text-align: center;">Died</td>
<td style="text-align: center;">2 March 1987 (aged 72)</td>
</tr>
<tr>
<td style="text-align: center;">Residence</td>
<td style="text-align: center;">UK</td>
</tr>
<tr>
<td style="text-align: center;">Nationality</td>
<td style="text-align: center;">British</td>
</tr>
<tr>
<td style="text-align: center;">Fields</td>
<td style="text-align: center;">Mycology, Plant Pathology, <br> Mathematics, Linguistics, <br> Computer Science</td>
</tr>
<tr>
<td style="text-align: center;">Known for</td>
<td style="text-align: center;">Contributions to <br> computational linguistics, <br> combinatorial physics, bit- <br> string physics, plant <br> pathology, and mycology</td>
</tr>
<tr>
<td style="text-align: center;">Author abbrev. <br> (botany)</td>
<td style="text-align: center;">Park-Rhodes</td>
</tr>
</tbody>
</table>
<p>Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: "Frederick Parker-Rhodes (21 March 1914 - 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.".</p>
<h2>3 Language Modeling for Constrained Sentence generation</h2>
<p>Conditional language models are a popular choice to generate sentences. We introduce a tableconditioned language model for constraining text generation to include elements from fact tables.</p>
<h3>3.1 Language model</h3>
<p>Given a sentence $s=w_{1}, \ldots, w_{T}$ with $T$ words from vocabulary $\mathcal{W}$, a language model estimates:</p>
<p>$$
P(s)=\prod_{t=1}^{T} P\left(w_{t} \mid w_{1}, \ldots, w_{t-1}\right)
$$</p>
<p>Let $c_{t}=w_{t-(n-1)}, \ldots, w_{t-1}$ be the sequence of $n-1$ context words preceding $w_{t}$. An $n$-gram language model makes an order $n$ Markov assumption,</p>
<p>$$
P(s) \approx \prod_{t=1}^{T} P\left(w_{t} \mid c_{t}\right)
$$</p>
<h3>3.2 Language model conditioned on tables</h3>
<p>A table is a set of field/value pairs, where values are sequences of words. We therefore propose language models that are conditioned on these pairs.</p>
<p>Local conditioning refers to the information from the table that is applied to the description of the words which have already generated, i.e. the previous words that constitute the context of the language</p>
<p>Table $\left(g_{f}, g_{w}\right)$
name
birthdate
birthplace
occupation
spouse
children</p>
<table>
<thead>
<tr>
<th style="text-align: left;">John</th>
<th style="text-align: center;">Doe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">13944</td>
<td style="text-align: center;">unk</td>
</tr>
<tr>
<td style="text-align: left;">(name,1,2)</td>
<td style="text-align: center;">(name,2,1)</td>
</tr>
<tr>
<td style="text-align: left;">(spouse,2,1)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(children,2,1)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">output candidates $(w \in \mathcal{W} \cup \mathcal{Q})$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">april</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">placeholder</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">john</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">doe</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$w$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">5302</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">13944</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">unk</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\emptyset$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(birthd.,2,2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(occupation,1,1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(name,1,2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(name,2,1)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$z_{w}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(spouse,2,1) <br> (children,2,1)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Table features (right) for an example table (left); $\mathcal{W} \cup \mathcal{Q}$ is the set of all output words as defined in Section 3.3.
model. The table allows us to describe each word not only by its string (or index in the vocabulary) but also by a descriptor of its occurrence in the table. Let $\mathcal{F}$ define the set of all possible fields $f$. The occurrence of a word $w$ in the table is described by a set of (field, position) pairs.</p>
<p>$$
z_{w}=\left{\left(f_{i}, p_{i}\right)\right}_{i=1}^{m}
$$</p>
<p>where $m$ is the number of occurrences of $w$. Each pair $(f, p)$ indicates that $w$ occurs in field $f$ at position $p$. In this scheme, most words are described by the empty set as they do not occur in the table. For example, the word linguistics in the table of Figure 1 is described as follows:</p>
<p>$$
z_{\text {linguistics }}={\text { (fields, } 8) ; \text { (known for, } 4)}
$$</p>
<p>assuming words are lower-cased and commas are treated as separate tokens.</p>
<p>Conditioning both on the field type and the position within the field allows the model to encode field-specific regularities, e.g., a number token in a date field is likely followed by a month token; knowing that the number is the first token in the date field makes this even more likely.</p>
<p>The (field, position) description scheme of the table does not allow to express that a token terminates a field which can be useful to capture field transitions. For biographies, the last token of the name field is often followed by an introduction of the birth date like '(' or 'was born'. We hence extend our descriptor to a triplet that includes the position of the
token counted from the end of the field:</p>
<p>$$
z_{w}=\left{\left(f_{i}, p_{i}^{+}, p_{i}^{-}\right)\right}_{i=1}^{m}
$$</p>
<p>where our example becomes:</p>
<p>$$
z_{\text {linguistics }}={\text { (fields, } 8,4) ; \text { (known for, } 4,13)}
$$</p>
<p>We extend Equation 2 to use the above information as additional conditioning context when generating a sentence $s$ :</p>
<p>$$
P(s \mid z)=\prod_{t=1}^{T} P\left(w_{t} \mid c_{t}, z_{c_{t}}\right)
$$</p>
<p>where $z_{c_{t}}=z_{w_{t-(n-1)}}, \ldots, z_{w_{t-1}}$ are referred to as the local conditioning variables since they describe the local context (previous word) relations with the table.</p>
<p>Global conditioning refers to information from all tokens and fields of the table, regardless whether they appear in the previous generated words or not. The set of fields available in a table often impacts the structure of the generation. For biographies, the fields used to describe a politician are different from the ones for an actor or an athlete. We introduce global conditioning on the available fields $g_{f}$ as</p>
<p>$$
P\left(s \mid z, g_{f}\right)=\prod_{t=1}^{T} P\left(w_{t} \mid c_{t}, z_{c_{t}}, g_{f}\right)
$$</p>
<p>Similarly, global conditioning $g_{w}$ on the available</p>
<p>words occurring in the table is introduced:</p>
<p>$$
P\left(s \mid z, g_{f}, g_{w}\right)=\prod_{t=1}^{T} P\left(w_{t} \mid c_{t}, z_{c_{t}}, g_{f}, g_{w}\right)
$$</p>
<p>Tokens provide information complementary to fields. For example, it may be hard to distinguish a basketball player from a hockey player by looking only at the field names, e.g. teams, league, position, weight and height, etc. However the actual field tokens such as team names, league name, player's position can help the model to give a better prediction. Here, $g_{f} \in{0,1}^{Z}$ and $g_{w} \in{0,1}^{\mathcal{W}}$ are binary indicators over fixed field and word vocabularies.</p>
<p>Figure 2 illustrates the model with a schematic example. For predicting the next word $w_{t}$ after a given context $c_{t}$, the language model is conditioned on sets of triplets for each word occurring in the table $z_{c_{t}}$, along with all fields and words from this table.</p>
<h3>3.3 Copy actions</h3>
<p>So far we extended the model conditioning with features derived from the fact table. We now turn to using table information when scoring output words. In particular, sentences which express facts from a given table often copy words from the table. We therefore extend our model to also score special field tokens such as name_1 or name_2 which are subsequently added to the score of the corresponding words from the field value.</p>
<p>Our model reads a table and defines an output domain $\mathcal{W} \cup \mathcal{Q} . \mathcal{Q}$ defines all tokens in the table, which might include out of vocabulary words ( $\notin \mathcal{W}$ ). For instance Park-Rhodes in Figure 1 is not in $\mathcal{W}$. However, Park-Rhodes will be included in $\mathcal{Q}$ as name_2 (since it is the second token of the name field) which allows our model to generate it. This mechanism is inspired by recent work on attention based word copying for neural machine translation (Luong et al., 2015) as well as delexicalization for neural dialog systems (Wen et al., 2015). It also builds upon older work such as class-based language models for dialog systems (Oh and Rudnicky, 2000).</p>
<h2>4 A Neural Language Model Approach</h2>
<p>A feed-forward neural language model (NLM) estimates $P\left(w_{t} \mid c_{t}\right)$ with a parametric function $\boldsymbol{\phi}_{\theta}$
(Equation 1), where $\theta$ refers to all learnable parameters of the network. This function is a composition of simple differentiable functions or layers.</p>
<h3>4.1 Mathematical notations and layers</h3>
<p>We denote matrices as bold upper case letters ( $\mathbf{X}$, $\mathbf{Y}, \mathbf{Z}$ ), and vectors as bold lower-case letters ( $\mathbf{a}, \mathbf{b}$, c). $\mathbf{A}<em i_="i," j="j">{i}$ represents the $i^{\text {th }}$ row of matrix $\mathbf{A}$. When $\mathbf{A}$ is a 3-d matrix, then $\mathbf{A}</em>}$ represents the vector of the $i^{\text {th }}$ first dimension and $j^{\text {th }}$ second dimension. Unless otherwise stated, vectors are assumed to be column vectors. We use $\left[\mathbf{v<em 2="2">{1} ; \mathbf{v}</em>\right]$ to denote vector concatenation. Next, we introduce the notation for the different layers used in our approach.</p>
<p>Embedding layer. Given a parameter matrix $\mathbf{X} \in \mathbb{R}^{N \times d}$, the embedding layer is a lookup table that performs an array indexing operation:</p>
<p>$$
\psi_{\mathbf{X}}\left(x_{i}\right)=\mathbf{X}_{i} \in \mathbb{R}^{d}
$$</p>
<p>where $\mathbf{X}<em i="i">{i}$ corresponds to the embedding of the element $x</em>$ is a 3-d matrix, the lookup table takes two arguments:}$ at row $i$. When $\mathbf{X</p>
<p>$$
\psi_{\mathbf{X}}\left(x_{i}, x_{j}\right)=\mathbf{X}_{i, j} \in \mathbb{R}^{d}
$$</p>
<p>where $\mathbf{X}<em i="i">{i, j}$ corresponds to the embedding of the pair $\left(x</em>$. A common approach is to concatenate all resulting embeddings:}, x_{j}\right)$ at index $(i, j)$. The lookup table operation can be applied for a sequence of elements $s=x_{1}, \ldots, x_{T</p>
<p>$$
\psi_{\mathbf{X}}(s)=\left[\boldsymbol{\psi}<em 1="1">{\mathbf{X}}\left(x</em>}\right) ; \ldots ; \boldsymbol{\psi<em T="T">{\mathbf{X}}\left(x</em>
$$}\right)\right] \in \mathbb{R}^{T \times d</p>
<p>Linear layer. This layer applies a linear transformation to its inputs $\mathbf{x} \in \mathbb{R}^{n}$ :</p>
<p>$$
\gamma_{\theta}(\mathbf{x})=\mathbf{W} \mathbf{x}+\mathbf{b}
$$</p>
<p>where $\theta={\mathbf{W}, \mathbf{b}}$ are the trainable parameters with $\mathbf{W} \in \mathbb{R}^{m \times n}$ being the weight matrix, and $\mathbf{b} \in \mathbb{R}^{m}$ is the bias term.</p>
<p>Softmax layer. Given a context input $c_{t}$, the final layer outputs a score for each word $w_{t} \in \mathcal{W}$, $\boldsymbol{\phi}<em t="t">{\theta}\left(c</em>$. The probability distribution is obtained by applying the softmax activation function:}\right) \in \mathbb{R}^{|\mathcal{W}|</p>
<p>$$
P\left(w_{t}=w \mid c_{t}\right)=\frac{\exp \left(\phi_{\theta}\left(c_{t}, w\right)\right)}{\sum_{i=1}^{|\mathcal{W}|} \exp \left(\phi_{\theta}\left(c_{t}, w_{i}\right)\right)}
$$</p>
<h3>4.2 Embeddings as inputs</h3>
<p>A key aspect of neural language models is the use of word embeddings. Similar words tend to have similar embeddings and thus share latent features. The probability estimates of those models are smooth functions of these embeddings, and a small change in the features results in a small change in the probability estimates (Bengio et al., 2003). Therefore, neural language models can achieve better generalization for unseen n-grams. Next, we show how we map fact tables to continuous space in similar spirit.</p>
<p>Word embeddings. Formally, the embedding layer maps each context word index to a continuous $d$-dimensional vector. It relies on a parameter matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{W}| \times d}$ to convert the input $c_{t}$ into $n-1$ vectors of dimension $d$ :</p>
<p>$$
\boldsymbol{\psi}<em t="t">{\mathbf{E}}\left(c</em>}\right)=\left[\boldsymbol{\psi<em t-_n-1_="t-(n-1)">{\mathbf{E}}\left(w</em>}\right) ; \ldots ; \boldsymbol{\psi<em t-1="t-1">{\mathbf{E}}\left(w</em>\right)\right]
$$</p>
<p>E can be initialized randomly or with pre-trained word embeddings.</p>
<p>Table embeddings. As described in Section 3.2, the language model is conditioned on elements from the table. Embedding matrices are therefore defined to model both local and global conditioning information. For local conditioning, we denote the maximum length of a sequence of words as $l$. Each field $f_{j} \in \mathcal{F}$ is associated with $2 \times l$ vectors of $d$ dimensions, the first $l$ of those vectors embed all possible starting positions $1, \ldots, l$, and the remaining $l$ vectors embed ending positions. This results in two parameter matrices $\mathbf{Z}=\left{\mathbf{Z}^{+}, \mathbf{Z}^{-}\right} \in \mathbb{R}^{|\mathcal{F}| \times l \times d}$. For a given triplet $\left(f_{j}, p_{i}^{+}, p_{i}^{-}\right), \boldsymbol{\psi}<em j="j">{\mathbf{Z}^{+}}\left(f</em>}, p_{i}^{+}\right)$and $\boldsymbol{\psi<em j="j">{\mathbf{Z}^{-}}\left(f</em>$, respectively.}, p_{i}^{-}\right)$refer to the embedding vectors of the start and end position for field $f_{j</p>
<p>Finally, global conditioning uses two parameter matrices $\mathbf{G}^{f} \in \mathbb{R}^{|\mathcal{F}| \times g}$ and $\mathbf{G}^{w} \in \mathbb{R}^{|\mathcal{W}| \times g}$. $\boldsymbol{\psi}<em j="j">{\mathbf{G}^{f}}\left(f</em>}\right)$ maps a table field $f_{j}$ into a vector of dimension $g$, while $\boldsymbol{\psi<em t="t">{\mathbf{G}^{w}}\left(w</em>$, provided $d=g$.}\right)$ maps a word $w_{t}$ into a vector of the same dimension. In general, $\mathbf{G}^{w}$ shares its parameters with $\mathbf{E</p>
<p>Aggregating embeddings. We represent each occurence of a word $w$ as a triplet (field, start, end) where we have embeddings for the start and end position as described above. Often times a particular word $w$ occurs multiple times in a table, e.g., 'lin-
guistics' has two instances in Figure 1. In this case, we perform a component-wise max over the start embeddings of all instances of $w$ to obtain the best features across all occurrences of $w$. We do the same for end position embeddings:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\psi}<em w__t="w_{t">{\mathbf{Z}}\left(z</em>\right)= \
&amp; \quad\left[\max \left{\boldsymbol{\psi}}<em j="j">{\mathbf{Z}^{+}}\left(f</em>\right} ;\right. \
&amp; \left.\max \left{\boldsymbol{\psi}}, p_{i}^{+}\right), \forall\left(f_{j}, p_{i}^{+}, p_{i}^{-}\right) \in z_{w_{t}<em j="j">{\mathbf{Z}^{-}}\left(f</em>\right}\right]
\end{aligned}
$$}, p_{i}^{-}\right), \forall\left(f_{j}, p_{i}^{+}, p_{i}^{-}\right) \in z_{w_{t}</p>
<p>A special no-field embedding is assigned to $w_{t}$ when the word is not associated to any fields. An embedding $\boldsymbol{\psi}<em c__t="c_{t">{\mathbf{Z}}\left(z</em>$ is obtained by concatenation.}}\right)$ for encoding the local conditioning of the input $c_{t</p>
<p>For global conditioning, we define $\mathcal{F}^{q} \subset \mathcal{F}$ as the set of all the fields in a given table $q$, and $\mathcal{Q}$ as the set of all words in $q$. We also perform max aggregation. This yields the vectors</p>
<p>$$
\boldsymbol{\psi}<em f="f">{\mathbf{G}^{f}}\left(g</em>}\right)=\max \left{\boldsymbol{\psi<em j="j">{\mathbf{G}^{f}}\left(f</em>\right}
$$}\right), \forall f_{j} \in \mathcal{F}^{q</p>
<p>and</p>
<p>$$
\boldsymbol{\psi}<em w="w">{\mathbf{G}^{w}}\left(g</em>}\right)=\max \left{\boldsymbol{\psi<em t="t">{\mathbf{G}^{w}}\left(w</em>\right}
$$}\right), \forall w_{t} \in \mathcal{Q</p>
<p>The final embedding which encodes the context input with conditioning is then the concatenation of these vectors:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\psi}<em 1="1">{\alpha</em>}}\left(c_{t}, z_{c_{t}}, g_{f}, g_{w}\right)=\left[\boldsymbol{\psi<em t="t">{\mathbf{E}}\left(c</em>}\right) ; \boldsymbol{\psi<em c__t="c_{t">{\mathbf{Z}}\left(z</em>\right)\right. \
&amp; \left.\boldsymbol{\psi}}<em f="f">{\mathbf{G}^{f}}\left(g</em>}\right) ; \boldsymbol{\psi<em w="w">{\mathbf{G}^{w}}\left(g</em>
\end{aligned}
$$}\right)\right] \in \mathbb{R}^{d^{1}</p>
<p>with $\alpha_{1}=\left{\mathbf{E}, \mathbf{Z}^{+}, \mathbf{Z}^{-}, \mathbf{G}^{f}, \mathbf{G}^{w}\right}$ and $d^{1}=(n-$ $1) \times(3 \times d)+(2 \times g)$. For simplification purpose, we define the context input $x=\left{c_{t}, z_{c_{t}}, g_{f}, g_{w}\right}$ in the following equations. This context embedding is mapped to a latent context representation using a linear operation followed by a hyperbolic tangent:</p>
<p>$$
\mathbf{h}(x)=\tanh \left(\gamma_{\alpha_{2}}\left(\boldsymbol{\psi}<em 1="1">{\alpha</em>
$$}}(x)\right)\right) \in \mathbb{R}^{\mathrm{nhu}</p>
<p>where $\alpha_{2}=\left{\mathbf{W}<em 2="2">{2}, \mathbf{b}</em>}\right}$, with $\mathbf{W<em 2="2">{2} \in \mathbb{R}^{\mathrm{nhu} \times d^{1}}$ and $\mathbf{b}</em>$.} \in \mathbb{R}^{\text {nhu }</p>
<h3>4.3 In-vocabulary outputs</h3>
<p>The hidden representation of the context then goes to another linear layer to produce a real value score for each word in the vocabulary:</p>
<p>$$
\phi_{\alpha}^{\mathcal{W}}(x)=\gamma_{\alpha_{2}}(\mathbf{h}(x)) \in \mathbb{R}^{|\mathcal{W}|}
$$</p>
<p>where $\alpha_{3}=\left{\mathbf{W}<em 3="3">{3}, \mathbf{b}</em>}\right}$, with $\mathbf{W<em 3="3">{3} \in \mathbb{R}^{|\mathcal{W}| \times \text { nhu }}$ and $\mathbf{b}</em>\right}$.} \in \mathbb{R}^{|\mathcal{W}|}$, and $\alpha=\left{\alpha_{1}, \alpha_{2}, \alpha_{3</p>
<h3>4.4 Mixing outputs for better copying</h3>
<p>Section 3.3 explains that each word $w$ from the table is also associated with $z_{w}$, the set of fields in which it occurs, along with the position in that field. Similar to local conditioning, we represent each field and position pair $\left(f_{j}, p_{i}\right)$ with an embedding $\boldsymbol{\psi}<em j="j">{\mathbf{F}}\left(f</em>$. Using the max operation over the embedding dimension, each word is finally embedded into a unique vector:}, p_{i}\right)$, where $\mathbf{F} \in \mathbb{R}^{|\mathcal{F}| \times l \times d}$. These embeddings are then projected into the same space as the latent representation of context input $\mathbf{h}(x) \in \mathbb{R}^{\text {nhu }</p>
<p>$$
\begin{aligned}
&amp; \mathbf{q}(w)=\max \{ \
&amp; \tanh \left(\gamma_{\beta}\left(\boldsymbol{\psi}<em j="j">{\mathbf{F}}\left(f</em>}
\end{aligned}
$$}, p_{i}\right)\right)\right), \forall\left(f_{j}, p_{i}\right) \in z_{w</p>
<p>where $\beta=\left{\mathbf{W}<em 4="4">{4}, \mathbf{b}</em>}\right}$ with $\mathbf{W<em 4="4">{4} \in \mathbb{R}^{\text {nhu } \times d}$, and $\mathbf{b}</em>$. A dot product with the context vector produces a score for each word $w$ in the table,} \in \mathbb{R}^{\text {nhu }</p>
<p>$$
\phi_{\beta}^{\mathcal{Q}}(x, w)=\mathbf{h}(x) \cdot \mathbf{q}(w)
$$</p>
<p>Each word $w \in \mathcal{W} \cup \mathcal{Q}$ receives a final score by summing the vocabulary score and the field score:</p>
<p>$$
\phi_{\theta}(x, w)=\phi_{\alpha}^{\mathcal{W}}(x, w)+\phi_{\beta}^{\mathcal{Q}}(x, w)
$$</p>
<p>with $\theta={\alpha, \beta}$, and where $\phi_{\beta}^{\mathcal{Q}}(x, w)=0$ when $w \notin \mathcal{Q}$. The softmax function then maps the scores to a distribution over $\mathcal{W} \cup \mathcal{Q}$,
$\log P(w \mid x)=\phi_{\theta}(x, w)-\log \sum_{w^{\prime} \in \mathcal{W} \cup \mathcal{Q}} \exp \phi_{\theta}\left(x, w^{\prime}\right)$.</p>
<h3>4.5 Training</h3>
<p>The neural language model is trained to minimize the negative log-likelihood of a training sentence $s$ with stochastic gradient descent (SGD; LeCun et al. 2012) :</p>
<p>$$
L_{\theta}(s)=-\sum_{t=1}^{T} \log P\left(w_{t} \mid c_{t}, z_{c_{t}}, g_{f}, g_{w}\right)
$$</p>
<h2>5 Experiments</h2>
<p>Our neural network model (Section 4) is designed to generate sentences from tables for large-scale problems, where a diverse set of sentence types need to be generated. Biographies are therefore a good
framework to evaluate our model, with Wikipedia offering a large and diverse dataset.</p>
<h3>5.1 Biography dataset</h3>
<p>We introduce a new dataset for text generation, WIKIBIo, a corpus of 728,321 articles from English Wikipedia (Sep 2015). It comprises all biography articles listed by WikiProject Biography ${ }^{1}$ which also have a table (infobox). We extract and tokenize the first sentence of each article with Stanford CoreNLP (Manning et al., 2014). All numbers are mapped to a special token, except for years which are mapped to different special token. Field values from tables are similarly tokenized. All tokens are lower-cased. Table 2 summarizes the dataset statistics: on average, the first sentence is twice as short as the table ( 26.1 vs 53.1 tokens), about a third of the sentence tokens (9.5) also occur in the table. The final corpus has been divided into three sub-parts to provide training ( $80 \%$ ), validation ( $10 \%$ ) and test sets ( $10 \%$ ). The dataset is available for download ${ }^{2}$.</p>
<h3>5.2 Baseline</h3>
<p>Our baseline is an interpolated Kneser-Ney (KN) language model and we use the KenLM toolkit to train 5-gram models without pruning (Heafield et al., 2013). We also learn a KN language model over templates. For that purpose, we replace the words occurring in both the table and the training sentences with a special token reflecting its table descriptor $z_{w}$ (Equation 3). The introduction section of the table in Figure 1 looks as follows under this scheme: "name_1 name_2 ( birthdate_1 birthdate_2 birthdate_3 deathdate_1 deathdate_2 deathdate_3 ) was an english linguist, fields_3 pathologist, fields_10 scientist, mathematician, mystic and mycologist ." During inference, the decoder is constrained to emit words from the regular vocabulary or special tokens occurring in the input table. When picking a special token we copy the corresponding word from the table.</p>
<h3>5.3 Training setup</h3>
<p>For our neural models, we train 11-gram language models $(n=11)$ with a learning rate set to 0.0025 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Perplexity</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE</th>
<th style="text-align: center;">NIST</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KN</td>
<td style="text-align: center;">10.51</td>
<td style="text-align: center;">2.21</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">NLM</td>
<td style="text-align: center;">$9.40 \pm 0.01$</td>
<td style="text-align: center;">$2.41 \pm 0.33$</td>
<td style="text-align: center;">$0.52 \pm 0.08$</td>
<td style="text-align: center;">$1.27 \pm 0.26$</td>
</tr>
<tr>
<td style="text-align: left;">+ Local (field, start, end)</td>
<td style="text-align: center;">$8.61 \pm 0.01$</td>
<td style="text-align: center;">$4.17 \pm 0.54$</td>
<td style="text-align: center;">$1.48 \pm 0.23$</td>
<td style="text-align: center;">$1.41 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;">Template KN</td>
<td style="text-align: center;">$7.46^{*}$</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">5.19</td>
</tr>
<tr>
<td style="text-align: left;">Table NLM w/ Local (field, start)</td>
<td style="text-align: center;">$4.60 \pm 0.01^{\dagger}$</td>
<td style="text-align: center;">$26.0 \pm 0.39$</td>
<td style="text-align: center;">$19.2 \pm 0.23$</td>
<td style="text-align: center;">$6.08 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">+ Local (field, start, end)</td>
<td style="text-align: center;">$4.60 \pm 0.01^{\dagger}$</td>
<td style="text-align: center;">$26.6 \pm 0.42$</td>
<td style="text-align: center;">$19.7 \pm 0.25$</td>
<td style="text-align: center;">$6.20 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: left;">+ Global (field)</td>
<td style="text-align: center;">$\mathbf{4 . 3 0} \pm \mathbf{0 . 0 1}^{\dagger}$</td>
<td style="text-align: center;">$33.4 \pm 0.18$</td>
<td style="text-align: center;">$23.9 \pm 0.12$</td>
<td style="text-align: center;">$7.52 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">+ Global (field \&amp; word)</td>
<td style="text-align: center;">$4.40 \pm 0.02^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 7} \pm \mathbf{0 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 8} \pm \mathbf{0 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{7 . 9 8} \pm \mathbf{0 . 0 7}$</td>
</tr>
</tbody>
</table>
<p>Table 1: BLEU, ROUGE, NIST and perplexity without copy actions (first three rows) and with copy actions (last five rows). For neural models we report "mean $\pm$ standard deviation" for five training runs with different initialization. Decoding beam width is 5 . Perplexities marked with * and ${ }^{\dagger}$ are not directly comparable as the output vocabularies differ slightly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Percentile</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$5 \%$</td>
<td style="text-align: right;">$95 \%$</td>
</tr>
<tr>
<td style="text-align: left;"># tokens per sentence</td>
<td style="text-align: right;">26.1</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">46</td>
</tr>
<tr>
<td style="text-align: left;"># tokens per table</td>
<td style="text-align: right;">53.1</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">108</td>
</tr>
<tr>
<td style="text-align: left;"># table tokens per sent.</td>
<td style="text-align: right;">9.5</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;"># fields per table</td>
<td style="text-align: right;">19.7</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">36</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset statistics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: right;">Value</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># word types</td>
<td style="text-align: right;">$</td>
<td style="text-align: right;">\mathcal{W}</td>
<td style="text-align: right;">$</td>
</tr>
<tr>
<td style="text-align: left;"># field types</td>
<td style="text-align: right;">$</td>
<td style="text-align: right;">\mathcal{F}</td>
<td style="text-align: right;">$</td>
</tr>
<tr>
<td style="text-align: left;">Max. # tokens in a field</td>
<td style="text-align: right;">$l$</td>
<td style="text-align: right;">$=$</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">word/field embedding size</td>
<td style="text-align: right;">$d$</td>
<td style="text-align: right;">$=$</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: left;">global embedding size</td>
<td style="text-align: right;">$g$</td>
<td style="text-align: right;">$=$</td>
<td style="text-align: right;">128</td>
</tr>
<tr>
<td style="text-align: left;"># hidden units</td>
<td style="text-align: right;">nhu</td>
<td style="text-align: right;">$=$</td>
<td style="text-align: right;">256</td>
</tr>
</tbody>
</table>
<p>Table 3: Model Hyperparameters</p>
<p>Table 3 describes the other hyper-parameters. We include all fields occurring at least 100 times in the training data in $\mathcal{F}$, the set of fields. We include the 20,000 most frequent words in the vocabulary. The other hyperparameters are set through validation, maximizing BLEU over a validation subset of 1,000 sentences. Similarly, early stopping is applied: training ends when BLEU stops improving on the same validation subset. One should note that the maximum number of tokens in a field $l=10$ means that we encode only 10 positions: for longer field values the final tokens are not dropped but their position is capped to 10 . We initialize the word embeddings $W$ from Hellinger PCA computed over the set of training biographies. This representation has
shown to be helpful for various applications (Lebret and Collobert, 2014).</p>
<h3>5.4 Evaluation metrics</h3>
<p>We use different metrics to evaluate our models. Performance is first evaluated in terms of perplexity which is the standard metric for language modeling. Generation quality is assessed automatically with BLEU-4, ROUGE-4 (F-measure) and NIST$4^{3}$ (Belz and Reiter, 2006).</p>
<h2>6 Results</h2>
<p>This section describes our results and discusses the impact of the different conditioning variables.</p>
<h3>6.1 The more, the better</h3>
<p>The results (Table 1) show that more conditioning information helps to improve the performance of our models. The generation metrics BLEU, ROUGE and NIST all gives the same performance ordering over models. We first discuss models without copy actions (the first three results) and then discuss models with copy actions (the remaining results). Note that the factorization of our models results in three different output domains which makes perplexity comparisons less straightforward: models without copy actions operate over a fixed vocabulary. Template KN adds a fixed set of field/position pairs to this vocabulary while Table NLM models a variable set $\mathcal{Q}$ depending on the input table, see Section 3.3.</p>
<p>Without copy actions. In terms of perplexity the (i) neural language model (NLM) is slightly better</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>than an interpolated KN language model, and (ii) adding local conditioning on the field start and end position further improves accuracy. Generation metrics are generally very low but there is a clear improvement when using local conditioning since it allows to learn transitions between fields by linking previous predictions to the table unlike KN or plain NLM.</p>
<p>With copy actions. For experiments with copy actions we use the full local conditioning (Equation 4) in the neural language models. BLEU, ROUGE and NIST all improves when moving from Template KN to Table NLM and more features successively improve accuracy. Global conditioning on the fields improves the model by over 7 BLEU and adding words gives an additional 1.3 BLEU. This is a total improvement of nearly 15 BLEU over the Template Kneser-Ney baseline. Similar observations are made for ROUGE +15 and NIST +2.8 .
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: Comparison between our best model (Table NLM) and the baseline (Template KN) for different beam sizes. The x -axis is the average timing (in milliseconds) for generating one sentence. The y-axis is the BLEU score. All results are measured on a subset of 1,000 samples of the validation set.</p>
<h3>6.2 Attention mechanism</h3>
<p>Our model implements attention over input table fields. For each word $w$ in the table, Equation (23) takes the language model score $\phi_{e_{3}}^{W}$ and adds a bias $\phi_{e_{3}}^{z 3}$. The bias is the dot-product between a representation of the table field in which $w$ occurs and a representation of the context, Equation (22) that summarizes the previously generated fields and words.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Visualization of attention scores for Nellie Wong's Wikipedia infobox. Each row represents the probability distribution over (field, position) pairs given the previous words (i.e. the words heading the preceding rows as well as the current row). Darker colors depict higher probabilities.</p>
<p>Figure 4 shows that this mechanism adds a large bias to continue a field if it has not generated all tokens from the table, e.g., it emits the word occurring in name_2 after generating name_1. It also nicely handles transitions between field types, e.g., the model adds a large bias to the words occurring in the occupation field after emitting the birthdate.</p>
<h3>6.3 Sentence decoding</h3>
<p>We use a standard beam search to explore a larger set of sentences compared to simple greedy search. This allows us to explore $K$ times more paths which comes at a linear increase in the number of forward computation steps for our language model. We compare various beam settings for the baseline Template KN and our Table NLM (Figure 3). The best validation BLEU can be obtained with a beam size of $K=5$. Our model is also several times faster than the baseline, requiring only about 200 ms per sentence with $K=5$. Beam search generates many ngram lookups for Kneser-Ney which requires many</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Generated Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">frederick parker-rhodes (21 march 1914 - 21 november 1987) was an english linguist, plant <br> pathologist, computer scientist, mathematician, mystic, and mycologist.</td>
</tr>
<tr>
<td style="text-align: left;">Baseline <br> (Template KN)</td>
<td style="text-align: left;">frederick parker-rhodes ( born november 21 , 1914 - march 2 , 1987 ) was an english cricketer <br> .</td>
</tr>
<tr>
<td style="text-align: left;">Table NLM <br> +Local (field, start)</td>
<td style="text-align: left;">frederick parker-rhodes ( 21 november 1914 - 2 march 1987 ) was an australian rules foot- <br> baller who played with carlton in the victorian football league ( vfl ) during the XXXXs and <br> XXXXs .</td>
</tr>
<tr>
<td style="text-align: left;">+ Global (field)</td>
<td style="text-align: left;">frederick parker-rhodes ( 21 november 1914 - 2 march 1987 ) was an english mycology and <br> plant pathology, mathematics at the university of uk .</td>
</tr>
<tr>
<td style="text-align: left;">+ Global <br> (field, word)</td>
<td style="text-align: left;">frederick parker-rhodes ( 21 november 1914 - 2 march 1987 ) was a british computer scientist <br> , best known for his contributions to computational linguistics .</td>
</tr>
</tbody>
</table>
<p>Table 4: First sentence from the current Wikipedia article about Frederick Parker-Rhodes and the sentences generated from the three versions of our table-conditioned neural language model (Table NLM) using the Wikipedia infobox seen in Figure 1.
random memory accesses; while neural models perform scoring through matrix-matrix products, an operation which is more local and can be performed in a block parallel manner where modern graphic processors shine (Kindratenko, 2014).</p>
<h3>6.4 Qualitative analysis</h3>
<p>Table 4 shows generations for different variants of our model based on the Wikipedia table in Figure 1. First of all, comparing the reference to the fact table reveals that our training data is not perfect. The birth month mentioned in the fact table and the first sentence of the Wikipedia article are different; this may have been introduced by one contributor editing the article and not keeping the information consistent.</p>
<p>All three versions of our model correctly generate the beginning of the sentence by copying the name, the birth date and the death date from the table. The model correctly uses the past tense since the death date in the table indicates that the person has passed away. Frederick Parker-Rhodes was a scientist, but this occupation is not directly mentioned in the table. The model without global conditioning can therefore not predict the right occupation, and it continues the generation with the most common occupation (in Wikipedia) for a person who has died. In contrast, the global conditioning over the fields helps the model to understand that this person was indeed a scientist. However, it is only with the global conditioning on the words that the model can infer the correct occupation, i.e., computer scientist.</p>
<h2>7 Conclusions</h2>
<p>We have shown that our model can generate fluent descriptions of arbitrary people based on structured data. Local and global conditioning improves our model by a large margin and we outperform a Kneser-Ney language model by nearly 15 BLEU. Our task uses an order of magnitude more data than previous work and has a vocabulary that is three orders of magnitude larger.</p>
<p>In this paper, we have only focused on generating the first sentence and we will tackle the generation of longer biographies in future work. Also, the encoding of field values can be improved. Currently, we only attach the field type and token position to each word type and perform a max-pooling for local conditioning. One could leverage a richer representation by learning an encoder conditioned on the field type, e.g. a recurrent encoder or a convolutional encoder with different pooling strategies.</p>
<p>Furthermore, the current training loss function does not explicitly penalize the model for generating incorrect facts, e.g. predicting an incorrect nationality or occupation is currently not considered worse than choosing an incorrect determiner. A loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such mistakes. Also it will be important to define a strategy for evaluating the factual accuracy of a generation, beyond BLEU, ROUGE or NIST.</p>
<h2>References</h2>
<p>G. Angeli, P. Liang, and D. Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502-512. Association for Computational Linguistics.
D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations.
R. Barzilay and M. Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331-338.
R. Barzilay and M. Lapata. 2006. Aggregation via set partitioning for natural language generation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 359-366. Association for Computational Linguistics.
A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of nlg systems. In In Proc. EACL06, pages 313-320.
A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generationspace models. Natural Language Engineering, 14(04):431-455.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In International Conference on Management of Data, pages 1247-1250. ACM.
R. Dale, S. Geldof, and J.-P. Prost. 2003. Coral: Using natural language generation for navigational assistance. In Proceedings of the 26th Australasian computer science conference-Volume 16, pages 35-44. Australian Computer Society, Inc.
J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1370-1380.
P. A. Duboue and K. R. McKeown. 2002. Content planner construction via evolutionary algorithms and a corpus-based fitness function. In Proceedings of INLG 2002, pages 89-96.
H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, L. C.</p>
<p>Zitnick, and G. Zweig. 2015. From captions to visual concepts and back. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.
D. Ferrucci. 2012. Introduction to this is watson. IBM Journal of Research and Development, 56(3.4):1-1.
D. Galanis and I. Androutsopoulos. 2007. Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 143-146. Association for Computational Linguistics.
N. Green. 2006. Generation of biomedical arguments for lay readers. In Proceedings of the Fourth International Natural Language Generation Conference, pages 114121. Association for Computational Linguistics.
B. Gyawali and C. Gardent. 2014. Surface realisation from knowledge-bases. In Proc. of ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690-696, Sofia, Bulgaria, August.
A. Karpathy and L. Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.
J. Kim and R. J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543-551. Association for Computational Linguistics.
V. Kindratenko. 2014. Numerical Computations with GPUs. Springer.
R. Kiros, R. Salakhutdinov, and R. S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539.
I. Konstas and M. Lapata. 2013. A global model for concept-to-text generation. J. Artif. Int. Res., 48(1):305-346, October.
I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. $A C L$, pages 704-710.
R. Lebret and R. Collobert. 2014. Word embeddings through hellinger pca. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482-490, Gothenburg, Sweden, April. Association for Computational Linguistics.
Y. A LeCun, L. Bottou, G. B. Orr, and K.-R. Müller. 2012. Efficient backprop. In Neural networks: Tricks of the trade, pages 9-48. Springer.</p>
<p>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 91-99. Association for Computational Linguistics.
W. Lu and H. T. Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1611-1622. Association for Computational Linguistics.
M.-T. Luong, I. Sutskever, Q. V Le, O. Vinyals, and W. Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proc. ACL, pages $11-19$.
F. Mairesse and M. Walker. 2011. Controlling user perceptions of linguistic style: Trainable generation of personality traits. Comput. Linguist., 37(3):455-488.
C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations, pages 55-60.
H. Mei, M. Bansal, and M. R. Walter. 2016. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. In Proceedings of Human Language Technologies: The 2016 Annual Conference of the North American Chapter of the Association for Computational Linguistics.
A. Oh and A. Rudnicky. 2000. Stochastic language generation for spoken dialogue systems. In ANLP/NAACL Workshop on Conversational Systems, pages 27-32.
A. Ratnaparkhi. 2002. Trainable approaches to surface natural language generation and their application to conversational dialog systems. Computer Speech \&amp; Language, 16(3):435-455.
E. Reiter, R. Dale, and Z. Feng. 2000. Building natural language generation systems, volume 33. MIT Press.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167(1):137-169.
L. Shang, Z. Lu, and H. Li. 2015. Neural responding machine for short-text conversation. arXiv preprint arXiv:1503.02364.
Radu Soricut and Daniel Marcu. 2006. Stochastic language generation using widl-expressions and its application in machine translation and summarization. In Proc. ACL, pages 1105-1112.
R. Turner, S. Sripada, and E. Reiter. 2010. Generating approximate geographic descriptions. In Empirical methods in natural language generation, pages 121140. Springer.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015. Show and tell: A neural image caption generator. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.
T. Wen, M. Gasic, N. Mrkšić, P. Su, D. Vandyke, and S. Young. 2015. Semantically conditioned lstmbased natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1711-1721, Lisbon, Portugal, September. Association for Computational Linguistics.
Y. W. Wong and R. J. Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In HLT-NAACL, pages 172-179.
K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of The 32nd International Conference on Machine Learning, volume 37, July.
K. Yao, G. Zweig, and B. Peng. 2015. Attention with intention for a neural network conversation model. arXiv preprint arXiv:1510.08565.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We rely on standard software, NIST mteval-v13a.pl (for NIST, BLEU), and MSR rouge-1.5.5 (for ROUGE).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8858 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8858</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8858</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-0244aeb7c6927e2fb0c2e668687e160a00737dbe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe" target="_blank">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Orca is developed, a 13-billion parameter model that learns to imitate the reasoning process of LFMs, indicating that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.</p>
                <p><strong>Paper Abstract:</strong> Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8858.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8858.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orca (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter instruction-tuned LLM trained in this work via 'explanation tuning' on large-scale GPT-3.5 (ChatGPT) and GPT-4 explanation traces (FLAN-5M and FLAN-1M), aiming to imitate reasoning processes of larger LFMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B parameter transformer-based language model derived from a LLaMA-family base; trained with explanation-tuning: 5M ChatGPT-augmented examples (FLAN-5M) then 1M GPT-4-augmented examples (FLAN-1M), using 16 system messages to elicit step-by-step explanations and a loss computed only on teacher response tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval (multiple-choice professional & academic exams)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Human-centric suite of multiple-choice questions drawn from standardized exams (GRE, GMAT, SAT, LSAT, LogiQA, AQuA-RAT, etc.) testing analytical, mathematical, reading-comprehension and strict logical reasoning under zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Explanation tuning (fine-tuning on GPT-3.5/GPT-4 long explanation traces with system messages encouraging step-by-step reasoning); progressive two-stage training: first on ChatGPT outputs (FLAN-5M) then GPT-4 (FLAN-1M); zero-shot evaluation without Chain-of-Thought exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 41.7% across AGIEval multiple-choice English tasks (zero-shot, no CoT). Per-task examples: AQuA-RAT 27.9%, LogiQA 35.2%, LSAT-LR 43.9%, LSAT-RC 57.3%, SAT-English 76.7%, SAT-Math 32.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Per Table 8: Orca outperforms Vicuna-13B substantially (Vicuna average 29.3% → Orca +42% relative improvement). Orca retains ~88% of ChatGPT quality (ChatGPT average 47.2%) and lags GPT-4 (62.0%) by a large margin. Orca roughly matches Text-Davinci-003 aggregate (TD-003 average 41.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still substantially behind GPT-4; weaker on math-heavy categories and some tasks requiring long-context comprehension; performance sensitive to system message used at inference; zero-shot parsing rules penalize non-template responses. Error analysis highlights remaining gaps in domain-specific knowledge, long-context reasoning, geometric and LaTeX-encoded questions.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Training on large numbers of LFM-generated step-by-step explanations and using progressive learning from ChatGPT then GPT-4 improves strict reasoning vs. vanilla instruction-tuned models; system messages and scale of explanation data meaningfully affect reasoning performance (5x ChatGPT data + GPT-4 fine-tuning improved aggregate AGIEval by ~4.5 points).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8858.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8858.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orca (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>As above: the 13B model trained with explanation tuning, evaluated on BBH reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B transformer trained via explanation-tuning on FLAN-5M (ChatGPT) and FLAN-1M (GPT-4) with system messages designed to elicit explanations and CoT-like traces; packing, LLaMA BPE tokenizer, loss computed on teacher tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Big-Bench Hard (BBH) - multiple-choice subset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A suite of 23 challenging BIG-Bench tasks (multiple-choice subset used) designed to probe hard reasoning capabilities including Boolean expressions, formal fallacies, logical deduction with multiple objects, temporal sequences, tracking/shuffling objects, semantic disambiguation, spatial and color reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation (no exemplars, no CoT) of explanation-tuned Orca; training uses rich GPT-4/ChatGPT explanation traces elicited via 16 system messages and progressive fine-tuning (ChatGPT then GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy 49.7% on BBH multiple-choice tasks (zero-shot, no CoT). Selected per-task: Boolean Expressions 72.0%, Formal Fallacies 56.0%, Logical Deduction (5 objects) 39.6%, Logical Deduction (7 objects) 36.0%, Temporal Sequences 72.0%, Disambiguation QA 63.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Orca marginally outperforms ChatGPT (ChatGPT average 48.9%) on BBH and vastly outperforms Vicuna-13B (Vicuna avg 23.3% → Orca reported +113.7% relative improvement). Orca trails GPT-4 by ~17.7 percentage points on BBH average (GPT-4 67.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite parity/marginal advantage over ChatGPT on BBH aggregate, Orca still lags GPT-4 considerably; some BBH subtasks (e.g., geometric shapes) remain challenging; BBH may contain GPT-4 training contamination (noted in paper) which complicates comparison to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Explanation-tuning with scaled, diverse system-message-driven explanation traces helps closed-source-like reasoning performance in a 13B model, enabling parity with ChatGPT on BBH; specific strengths include entailment/semantic understanding and temporal/spatial reasoning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8858.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8858.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13B instruction-tuned chatbot derived from LLaMA, trained on ShareGPT user conversations; used as a baseline in this paper and shown to underperform on strict reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B parameter chat model fine-tuned from a LLaMA base on user-shared ChatGPT conversations (ShareGPT); focuses on human-like dialogue and instruction-following but with limited explicit explanation traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval (multiple-choice) and Big-Bench Hard (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Standardized exam-style and BBH tasks probing analytical and strict logical reasoning (see AGIEval and BBH descriptions above).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated as an instruction-tuned baseline (no explanation-tuning in this paper's experiments); training approach originally based on user conversations (ShareGPT) rather than large-scale explanation traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AGIEval average 29.3% (zero-shot); per-task examples: AQuA-RAT 20.1%, LogiQA 29.8%, LSAT-RC 32.7%, SAT-English 44.2%. BBH average 23.3% with many subtask failures (e.g., Logical Deduction (7 objects) 1.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially outperformed by Orca (Orca reported +42.1% relative on AGIEval and +113.7% relative on BBH). Vicuna retains far less of ChatGPT/GPT-4 reasoning ability compared to Orca according to the reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Per paper, Vicuna tends to capture style of LFMs but not underlying reasoning; performs poorly on complex analytical tasks, multi-step logical deduction, and BBH hard reasoning tasks; poor performance attributed to training data limited to conversational pairs lacking step-by-step explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction tuning on natural conversations (ShareGPT) yields strong open-ended generation but limited strict logical reasoning; paper argues richer explanation traces are necessary to close the reasoning gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8858.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8858.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 chat-optimized model used both as a teacher for data collection and as a baseline for evaluation on strict reasoning tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-series chat-optimized LLM (API: gpt-3.5-turbo) used to generate 5M explanation-augmented responses (FLAN-5M) for Orca's training; used as an evaluation baseline on AGIEval and BBH.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval and Big-Bench Hard (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same standardized exam and BBH tasks testing strict logical, analytical, temporal, spatial and semantic reasoning under zero-shot conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used as an intermediate 'teacher assistant' to generate large-scale explanation traces (cheaper/faster than GPT-4) and evaluated as a baseline; zero-shot evaluation without CoT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AGIEval average 47.2% (zero-shot). Selected AGIEval values: AQuA-RAT 31.9%, LogiQA 35.0%, LSAT-RC 65.4%, SAT-English 81.1%. BBH average 48.9%; selected BBH tasks: Boolean Expressions 82.8%, Formal Fallacies 53.6%, Logical Deduction (5 objects) 38.8%, Temporal Sequences 35.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Orca retains ~88% of ChatGPT quality on AGIEval and Orca marginally outperforms ChatGPT on BBH (Orca 49.7% vs ChatGPT 48.9%); GPT-4 substantially outperforms ChatGPT across reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ChatGPT still beaten by Orca on many BBH subtasks (e.g., temporal sequences), and in AGIEval some tasks (math-heavy, long-context) ChatGPT outperforms Orca; ChatGPT was used to generate training data so some teacher-student transfer effects exist.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Using ChatGPT as an intermediate teacher to scale explanation traces is an effective curriculum strategy for improving smaller models' reasoning abilities, though highest performance remains with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8858.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8858.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art large foundation model used both as a teacher (for 1M explanation traces) and as the strongest baseline on strict reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latest publicly referenced OpenAI LFM (size undisclosed); used to produce high-quality, longer explanation traces (FLAN-1M) and used as an evaluation baseline (strongest performer in reported benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval and Big-Bench Hard (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same standardized exam and BBH tasks probing strict logical and multi-step reasoning; evaluation performed zero-shot without CoT exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used as a teacher to produce detailed step-by-step explanations for a sampled 1M examples (FLAN-1M) and as an evaluation baseline; prompts augmented with system messages to elicit explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AGIEval average 62.0% (zero-shot); selected AGIEval scores: AQuA-RAT 40.6%, LogiQA 49.3%, LSAT-RC 85.9%, SAT-English 88.8%. BBH average 67.4%; selected BBH tasks: Boolean Expressions 77.6%, Formal Fallacies 64.4%, Logical Deduction (5 objects) 66.8%, Temporal Sequences 98.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4 outperforms all other evaluated models by large margins on both AGIEval and BBH (e.g., ~20+ points above ChatGPT on many tasks and ~17.7 points above Orca on BBH average).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes potential data contamination of GPT-4 with BBH tasks (GPT-4 may have seen BBH content), which complicates comparisons; also GPT-4 still not perfect (benchmarks show room below human top performance).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-quality, longer explanation traces from GPT-4 materially improve student-model learning; GPT-4 remains the strongest benchmark target for reasoning capability, but data contamination caveats apply for some datasets (BBH).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8858.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8858.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Davinci-003 (TD-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5-series text-completion model used as an additional baseline for AGIEval multiple-choice reasoning comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-series completion model (text-davinci-003) optimized for text completion; included as baseline in AGIEval evaluations reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>AGIEval (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Standardized exam multiple-choice tasks (AGIEval) used to measure analytical and logical reasoning under zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated zero-shot as a baseline (no CoT); not used as teacher for explanation traces in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AGIEval average 41.9% (zero-shot). Example per-task scores: AQuA-RAT 29.9%, LogiQA 22.7%, LSAT-RC 64.7%, SAT-English 74.8%, SAT-Math 35.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Orca matches or slightly exceeds TD-003 aggregate (Orca 41.7% vs TD-003 41.9% reported) on AGIEval; both are significantly below GPT-4 and somewhat below ChatGPT averages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower performance on many reasoning-focused tasks (e.g., LogiQA) relative to ChatGPT/GPT-4 and to explanation-tuned Orca in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Standard text-completion models without large-scale explanation fine-tuning lag behind models trained with explanation traces for strict reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Orca: Progressive Learning from Complex Explanation Traces of GPT-4', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AGIEval <em>(Rating: 2)</em></li>
                <li>Big-Bench Hard <em>(Rating: 2)</em></li>
                <li>Vicuna <em>(Rating: 2)</em></li>
                <li>WizardLM <em>(Rating: 1)</em></li>
                <li>FLAN-v2 (Flan 2022 Collection) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8858",
    "paper_id": "paper-0244aeb7c6927e2fb0c2e668687e160a00737dbe",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Orca",
            "name_full": "Orca (13B)",
            "brief_description": "A 13-billion-parameter instruction-tuned LLM trained in this work via 'explanation tuning' on large-scale GPT-3.5 (ChatGPT) and GPT-4 explanation traces (FLAN-5M and FLAN-1M), aiming to imitate reasoning processes of larger LFMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca (13B)",
            "model_description": "13B parameter transformer-based language model derived from a LLaMA-family base; trained with explanation-tuning: 5M ChatGPT-augmented examples (FLAN-5M) then 1M GPT-4-augmented examples (FLAN-1M), using 16 system messages to elicit step-by-step explanations and a loss computed only on teacher response tokens.",
            "model_size": "13B",
            "reasoning_task_name": "AGIEval (multiple-choice professional & academic exams)",
            "reasoning_task_description": "Human-centric suite of multiple-choice questions drawn from standardized exams (GRE, GMAT, SAT, LSAT, LogiQA, AQuA-RAT, etc.) testing analytical, mathematical, reading-comprehension and strict logical reasoning under zero-shot settings.",
            "method_or_approach": "Explanation tuning (fine-tuning on GPT-3.5/GPT-4 long explanation traces with system messages encouraging step-by-step reasoning); progressive two-stage training: first on ChatGPT outputs (FLAN-5M) then GPT-4 (FLAN-1M); zero-shot evaluation without Chain-of-Thought exemplars.",
            "performance": "Average accuracy 41.7% across AGIEval multiple-choice English tasks (zero-shot, no CoT). Per-task examples: AQuA-RAT 27.9%, LogiQA 35.2%, LSAT-LR 43.9%, LSAT-RC 57.3%, SAT-English 76.7%, SAT-Math 32.3%.",
            "baseline_comparison": "Per Table 8: Orca outperforms Vicuna-13B substantially (Vicuna average 29.3% → Orca +42% relative improvement). Orca retains ~88% of ChatGPT quality (ChatGPT average 47.2%) and lags GPT-4 (62.0%) by a large margin. Orca roughly matches Text-Davinci-003 aggregate (TD-003 average 41.9%).",
            "limitations_or_failures": "Still substantially behind GPT-4; weaker on math-heavy categories and some tasks requiring long-context comprehension; performance sensitive to system message used at inference; zero-shot parsing rules penalize non-template responses. Error analysis highlights remaining gaps in domain-specific knowledge, long-context reasoning, geometric and LaTeX-encoded questions.",
            "insights_or_conclusions": "Training on large numbers of LFM-generated step-by-step explanations and using progressive learning from ChatGPT then GPT-4 improves strict reasoning vs. vanilla instruction-tuned models; system messages and scale of explanation data meaningfully affect reasoning performance (5x ChatGPT data + GPT-4 fine-tuning improved aggregate AGIEval by ~4.5 points).",
            "uuid": "e8858.0",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Orca",
            "name_full": "Orca (13B)",
            "brief_description": "As above: the 13B model trained with explanation tuning, evaluated on BBH reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca (13B)",
            "model_description": "13B transformer trained via explanation-tuning on FLAN-5M (ChatGPT) and FLAN-1M (GPT-4) with system messages designed to elicit explanations and CoT-like traces; packing, LLaMA BPE tokenizer, loss computed on teacher tokens.",
            "model_size": "13B",
            "reasoning_task_name": "Big-Bench Hard (BBH) - multiple-choice subset",
            "reasoning_task_description": "A suite of 23 challenging BIG-Bench tasks (multiple-choice subset used) designed to probe hard reasoning capabilities including Boolean expressions, formal fallacies, logical deduction with multiple objects, temporal sequences, tracking/shuffling objects, semantic disambiguation, spatial and color reasoning.",
            "method_or_approach": "Zero-shot evaluation (no exemplars, no CoT) of explanation-tuned Orca; training uses rich GPT-4/ChatGPT explanation traces elicited via 16 system messages and progressive fine-tuning (ChatGPT then GPT-4).",
            "performance": "Average accuracy 49.7% on BBH multiple-choice tasks (zero-shot, no CoT). Selected per-task: Boolean Expressions 72.0%, Formal Fallacies 56.0%, Logical Deduction (5 objects) 39.6%, Logical Deduction (7 objects) 36.0%, Temporal Sequences 72.0%, Disambiguation QA 63.6%.",
            "baseline_comparison": "Orca marginally outperforms ChatGPT (ChatGPT average 48.9%) on BBH and vastly outperforms Vicuna-13B (Vicuna avg 23.3% → Orca reported +113.7% relative improvement). Orca trails GPT-4 by ~17.7 percentage points on BBH average (GPT-4 67.4%).",
            "limitations_or_failures": "Despite parity/marginal advantage over ChatGPT on BBH aggregate, Orca still lags GPT-4 considerably; some BBH subtasks (e.g., geometric shapes) remain challenging; BBH may contain GPT-4 training contamination (noted in paper) which complicates comparison to GPT-4.",
            "insights_or_conclusions": "Explanation-tuning with scaled, diverse system-message-driven explanation traces helps closed-source-like reasoning performance in a 13B model, enabling parity with ChatGPT on BBH; specific strengths include entailment/semantic understanding and temporal/spatial reasoning improvements.",
            "uuid": "e8858.1",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (13B)",
            "brief_description": "An open-source 13B instruction-tuned chatbot derived from LLaMA, trained on ShareGPT user conversations; used as a baseline in this paper and shown to underperform on strict reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B",
            "model_description": "13B parameter chat model fine-tuned from a LLaMA base on user-shared ChatGPT conversations (ShareGPT); focuses on human-like dialogue and instruction-following but with limited explicit explanation traces.",
            "model_size": "13B",
            "reasoning_task_name": "AGIEval (multiple-choice) and Big-Bench Hard (multiple-choice)",
            "reasoning_task_description": "Standardized exam-style and BBH tasks probing analytical and strict logical reasoning (see AGIEval and BBH descriptions above).",
            "method_or_approach": "Evaluated as an instruction-tuned baseline (no explanation-tuning in this paper's experiments); training approach originally based on user conversations (ShareGPT) rather than large-scale explanation traces.",
            "performance": "AGIEval average 29.3% (zero-shot); per-task examples: AQuA-RAT 20.1%, LogiQA 29.8%, LSAT-RC 32.7%, SAT-English 44.2%. BBH average 23.3% with many subtask failures (e.g., Logical Deduction (7 objects) 1.2%).",
            "baseline_comparison": "Substantially outperformed by Orca (Orca reported +42.1% relative on AGIEval and +113.7% relative on BBH). Vicuna retains far less of ChatGPT/GPT-4 reasoning ability compared to Orca according to the reported metrics.",
            "limitations_or_failures": "Per paper, Vicuna tends to capture style of LFMs but not underlying reasoning; performs poorly on complex analytical tasks, multi-step logical deduction, and BBH hard reasoning tasks; poor performance attributed to training data limited to conversational pairs lacking step-by-step explanations.",
            "insights_or_conclusions": "Instruction tuning on natural conversations (ShareGPT) yields strong open-ended generation but limited strict logical reasoning; paper argues richer explanation traces are necessary to close the reasoning gap.",
            "uuid": "e8858.2",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT-3.5-turbo)",
            "brief_description": "OpenAI's GPT-3.5 chat-optimized model used both as a teacher for data collection and as a baseline for evaluation on strict reasoning tasks in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-turbo)",
            "model_description": "GPT-3.5-series chat-optimized LLM (API: gpt-3.5-turbo) used to generate 5M explanation-augmented responses (FLAN-5M) for Orca's training; used as an evaluation baseline on AGIEval and BBH.",
            "model_size": null,
            "reasoning_task_name": "AGIEval and Big-Bench Hard (multiple-choice)",
            "reasoning_task_description": "Same standardized exam and BBH tasks testing strict logical, analytical, temporal, spatial and semantic reasoning under zero-shot conditions.",
            "method_or_approach": "Used as an intermediate 'teacher assistant' to generate large-scale explanation traces (cheaper/faster than GPT-4) and evaluated as a baseline; zero-shot evaluation without CoT in experiments.",
            "performance": "AGIEval average 47.2% (zero-shot). Selected AGIEval values: AQuA-RAT 31.9%, LogiQA 35.0%, LSAT-RC 65.4%, SAT-English 81.1%. BBH average 48.9%; selected BBH tasks: Boolean Expressions 82.8%, Formal Fallacies 53.6%, Logical Deduction (5 objects) 38.8%, Temporal Sequences 35.6%.",
            "baseline_comparison": "Orca retains ~88% of ChatGPT quality on AGIEval and Orca marginally outperforms ChatGPT on BBH (Orca 49.7% vs ChatGPT 48.9%); GPT-4 substantially outperforms ChatGPT across reasoning tasks.",
            "limitations_or_failures": "ChatGPT still beaten by Orca on many BBH subtasks (e.g., temporal sequences), and in AGIEval some tasks (math-heavy, long-context) ChatGPT outperforms Orca; ChatGPT was used to generate training data so some teacher-student transfer effects exist.",
            "insights_or_conclusions": "Using ChatGPT as an intermediate teacher to scale explanation traces is an effective curriculum strategy for improving smaller models' reasoning abilities, though highest performance remains with GPT-4.",
            "uuid": "e8858.3",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's state-of-the-art large foundation model used both as a teacher (for 1M explanation traces) and as the strongest baseline on strict reasoning benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Latest publicly referenced OpenAI LFM (size undisclosed); used to produce high-quality, longer explanation traces (FLAN-1M) and used as an evaluation baseline (strongest performer in reported benchmarks).",
            "model_size": null,
            "reasoning_task_name": "AGIEval and Big-Bench Hard (multiple-choice)",
            "reasoning_task_description": "Same standardized exam and BBH tasks probing strict logical and multi-step reasoning; evaluation performed zero-shot without CoT exemplars.",
            "method_or_approach": "Used as a teacher to produce detailed step-by-step explanations for a sampled 1M examples (FLAN-1M) and as an evaluation baseline; prompts augmented with system messages to elicit explanations.",
            "performance": "AGIEval average 62.0% (zero-shot); selected AGIEval scores: AQuA-RAT 40.6%, LogiQA 49.3%, LSAT-RC 85.9%, SAT-English 88.8%. BBH average 67.4%; selected BBH tasks: Boolean Expressions 77.6%, Formal Fallacies 64.4%, Logical Deduction (5 objects) 66.8%, Temporal Sequences 98.0%.",
            "baseline_comparison": "GPT-4 outperforms all other evaluated models by large margins on both AGIEval and BBH (e.g., ~20+ points above ChatGPT on many tasks and ~17.7 points above Orca on BBH average).",
            "limitations_or_failures": "Paper notes potential data contamination of GPT-4 with BBH tasks (GPT-4 may have seen BBH content), which complicates comparisons; also GPT-4 still not perfect (benchmarks show room below human top performance).",
            "insights_or_conclusions": "High-quality, longer explanation traces from GPT-4 materially improve student-model learning; GPT-4 remains the strongest benchmark target for reasoning capability, but data contamination caveats apply for some datasets (BBH).",
            "uuid": "e8858.4",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Text-Davinci-003",
            "name_full": "Text-Davinci-003 (TD-003)",
            "brief_description": "A GPT-3.5-series text-completion model used as an additional baseline for AGIEval multiple-choice reasoning comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Text-Davinci-003",
            "model_description": "GPT-3.5-series completion model (text-davinci-003) optimized for text completion; included as baseline in AGIEval evaluations reported in the paper.",
            "model_size": null,
            "reasoning_task_name": "AGIEval (multiple-choice)",
            "reasoning_task_description": "Standardized exam multiple-choice tasks (AGIEval) used to measure analytical and logical reasoning under zero-shot settings.",
            "method_or_approach": "Evaluated zero-shot as a baseline (no CoT); not used as teacher for explanation traces in this work.",
            "performance": "AGIEval average 41.9% (zero-shot). Example per-task scores: AQuA-RAT 29.9%, LogiQA 22.7%, LSAT-RC 64.7%, SAT-English 74.8%, SAT-Math 35.5%.",
            "baseline_comparison": "Orca matches or slightly exceeds TD-003 aggregate (Orca 41.7% vs TD-003 41.9% reported) on AGIEval; both are significantly below GPT-4 and somewhat below ChatGPT averages.",
            "limitations_or_failures": "Lower performance on many reasoning-focused tasks (e.g., LogiQA) relative to ChatGPT/GPT-4 and to explanation-tuned Orca in some categories.",
            "insights_or_conclusions": "Standard text-completion models without large-scale explanation fine-tuning lag behind models trained with explanation traces for strict reasoning tasks.",
            "uuid": "e8858.5",
            "source_info": {
                "paper_title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AGIEval",
            "rating": 2
        },
        {
            "paper_title": "Big-Bench Hard",
            "rating": 2
        },
        {
            "paper_title": "Vicuna",
            "rating": 2
        },
        {
            "paper_title": "WizardLM",
            "rating": 1
        },
        {
            "paper_title": "FLAN-v2 (Flan 2022 Collection)",
            "rating": 2
        }
    ],
    "cost": 0.01604825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</h1>
<p>Subhabrata Mukherjee ${ }^{<em> \dagger}$, Arindam Mitra</em><br>Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah<br>Microsoft Research</p>
<h4>Abstract</h4>
<p>Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than $100 \%$ in complex zero-shot reasoning benchmarks like BigBench Hard (BBH) and $42 \%$ on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance ( 4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm.</p>
<p>Work in progress.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
1.1 Challenges with Existing Methods ..... 5
1.2 Key Contributions ..... 6
2 Preliminaries ..... 7
2.1 Instruction Tuning ..... 7
2.2 Role of System Instructions ..... 7
3 Explanation Tuning ..... 8
3.1 Dataset Construction ..... 8
3.1.1 System Messages ..... 9
3.1.2 Dataset Description and Sampling from the FLAN-v2 Collection ..... 9
3.1.3 ChatGPT as Teaching Assistant ..... 12
3.2 Training ..... 13
4 Experiment Setup ..... 14
4.1 Baselines ..... 14
4.2 Tasks ..... 15
4.2.1 Open-ended Generation Capabilities ..... 15
4.2.2 Reasoning Capabilities ..... 16
5 Evaluation for Open-ended Generation ..... 17
6 Evaluation for Reasoning ..... 17
6.1 AGIEval Results ..... 17
6.2 Big-Bench Hard Results ..... 20
7 Evaluation for Safety ..... 23
7.1 Truthful Question Answering ..... 23
7.2 Toxic Content Generation ..... 26
7.3 Note on Hallucination and Tool Augmented LFMs ..... 27
8 Limitations ..... 28
9 Conclusions ..... 29
10 Author Contributions ..... 29
11 Case Studies ..... 30
11.1 Trigonometric Problem Solving ..... 30
11.2 Temporal Reasoning ..... 32
11.3 Multiple-choice Question-Answering ..... 33</p>
<p>11.4 Bio Olympiad ..... 34
11.5 Forming Inequalities ..... 35
11.6 Counterfactual Question Answering ..... 38
11.7 Compound Interest Problems ..... 38
11.8 Question from Vicuna-Eval ..... 39
11.9 Spatial Reasoning ..... 41
11.10Commonsense Question Answering ..... 42
11.11Hallucination ..... 44
11.12Quadratic Equation Solving ..... 45
11.13Meeting Transcript Processing ..... 46</p>
<h1>1 Introduction</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Orca (13B params) outperforms a wide range of foundation models including OpenAI ChatGPT as evaluated by GPT-4 in the Vicuna evaluation set. We further demonstrate similar results against a wide range of evaluation sets from other works in experiments.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Explanation tuning with Orca (13B params) bridges gap with OpenAI foundation models like Text-da-Vinci-003 with 5 pts gap (the gap further reduces with optimized system messages) against ChatGPT across a wide range of professional and academic exams including GRE, GMAT, LSAT, SAT from the AGIEval benchmark [1] in zero-shot settings (without any exemplar or CoT). Topical performances shown in Figure 11.</p>
<p>Large Foundation Models (LFMs) such as ChatGPT and GPT-4 [2] exhibit remarkable zeroshot performances across a broad spectrum of tasks. Alongside academic benchmarks like Human Eval [3] and Big Bench [4], GPT-4 has also demonstrated human-level performance on various professional exams, including the bar exam, SAT, GRE, and USMLE. These advancements can be credited to the scaling of both model and dataset sizes, as well as the incorporation of a second layer of training to better align the models with user intent. This alignment is accomplished by fine-tuning the models via supervised learning on demonstrations of prompts and desired model behavior, and through reinforcement learning from human preferences [5].
As these models continue to evolve and become more powerful, an intriguing question arises: Can we use the model itself to supervise its own behavior or that of other AI models? Bai et al. [6] have shown that by sampling output from an initial model, generating revisions, and then fine-tuning the original model based on these revised responses, model behavior can be controlled more effectively and can be made more harmless, with significantly fewer human labels.</p>
<p>Recently, there has been an influx of studies using LFMs like ChatGPT and GPT-4 as teachers to generate large datasets, for instruction tuning, and to train smaller models, such as Alpaca [7], WizardLM [8] and Vicuna [9]. While these models can produce content that matches the style of their teachers, they often fall short in terms of the reasoning and comprehension skills displayed by the larger foundation models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: For complex zero-shot reasoning tasks in BigBench-Hard, Orca achieves parity with ChatGPT (without any exemplar or CoT) with task performances shown in Figure 12.</p>
<p>Take, for example, the 13-billion parameter instruction-tuned model, Vicuna [9] (with LLAMA-13B [10] as the base), which is widely regarded as one of the best models in its family, as evidenced by its performance on leaderboards like OpenLLM ${ }^{3}$ and ChatArena ${ }^{4}$.</p>
<p>As illustrated in Figure 1, the widely-used evaluation method of using GPT-4 as the judge suggests that Vicuna retains $92 \%$ of ChatGPT's quality. However, a more meticulous evaluation on reasoning benchmarks against human labels finds Vicuna to retain only $64 \%$ of ChatGPT's quality on professional and academic exams (see Figure 2), and only $48 \%$ of ChatGPT's quality on complex benchmarks like BigBench-hard [11] (see Figure 3) ${ }^{5}$. This discrepancy not only underscores the limitations of existing evaluation protocols with smaller LLMs, but it also reveals their significant lag in reasoning and comprehension capabilities. In essence, these models may be articulate, but they may not necessarily possess robust reasoning skills. In this study, we discuss some of the reasons behind these gaps and propose strategies for addressing them.</p>
<h1>1.1 Challenges with Existing Methods</h1>
<p>Current research on instruction-tuning to mimic the output of LFM's like ChatGPT exhibits notable limitation in task diversity, query complexity, and data scaling. These observations are corroborated in a recent study by Gudibande et al. [12], where the authors assert that "model imitation is a false promise" since "broadly matching ChatGPT using purely imitation would require (1) a concerted effort to collect enormous imitation datasets and (2) far more diverse and higher quality imitation data than is currently available.". Contrary to this assertion, we demonstrate that both conditions (1) and (2) are attainable and that it is possible to reduce the gap with proprietary LLM's on multiple zero-shot benchmarks that require sophisticated reasoning. We elaborate on these challenges below:
Simple instructions with limited diversity. The Self-Instruct [13] process involves using an initial set of prompts to incite the LFM to produce new instructions. Any low-quality or overly similar responses are then removed, and the remaining instructions are reintegrated into the task pool for further iterations. Nonetheless, the resulting queries generated through Self-Instruct, such as "what are the three primary colors?", "what is the capital of France?", etc., can exhibit limitations in diversity and complexity. Both Alpaca [7] and WizardLM [8] employ a variant of self-instruct. WizardLM introduces the concept of Evol-Instruct, which gradually rewrites the initial set of instructions into more complex versions, attempting to overcome some of the method's inherent shortcomings. On the other hand, recent works like Vicuna [9] and Koala [14] demonstrate remarkable performance due to more human-like conversations and natural instructions in community-contributed conversations like those in ShareGPT ${ }^{6}$ that provided a forum for users to share their conversations with ChatGPT.
Task diversity and data scaling. Human-contributed conversations in ShareGPT are a valuable source of data, but they also have some limitations. They tend to favor creative</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>content generation and information-seeking queries over other types of tasks. Therefore, models trained on such natural conversations may capture the style but not the reasoning process of the LFMs - demonstrated in the performance of Vicuna in Figures 2 and 3. Additionally, such mode of data collection is also limited in scale. Table 1 shows an overview of the size of data and tuning methods employed in recent popular instruction tuning works.
Limited imitation signals. Existing methods rely on immitation learning from〈query, response〉 pairs generated by the teacher model. However, this provides limited signals to trace the reasoning process of the teacher. Prior works [15, 16] on open-box model show that richer signals such as logits, intermediate representations and attention states can significantly improve distillation performance. While they are not accessible for closed-box LFM's ${ }^{7}$, recent work [17] demonstrates that richer signals like LFM rationales can help close the gap for task-specific distillation.
Evaluation: Previous studies on instruction tuning of small models with LFMs are severely limited in their evaluation protocol. They often rely on GPT-4 for auto-evaluation by asking it to compare the outputs of two systems with a prompt like "given responses from system 1 (reference) and system 2 (target), which one is better?". However, this approach has several drawbacks, such as the small size of test sets (e.g., 80 instructions in Vicuna and 218 instructions in WizardLM) and the biases of GPT-4 as the judge [18]. For example, we notice that models that are instruction-tuned with GPT-4 responses tend to generate longer texts that GPT-4 prefers over shorter ones; as well as GPT-4 has a bias in the order of the candidate responses. We will show that such auto-evaluation measures overestimate the abilities of smaller models compared to LFMs, as the former are much weaker in comprehension and reasoning skills.</p>
<h1>1.2 Key Contributions</h1>
<p>In this research, our focus is on addressing the challenges mentioned above, specifically with:
Explanation tuning: We augment 〈query, response〉 pairs with detailed responses from GPT-4 that explain the reasoning process of the teacher as it generates the response. These provide the student with additional signals for learning. We leverage system instructions (e.g.., explain like I'm five, think step-by-step and justify your response, etc.) to elicit such explanations. This is in contrast to vanilla instruction tuning, which only uses the prompt and the LFM response for learning, providing little opportunity for mimicking the LFM's "thought" process.
Scaling tasks and instructions: We utilize the Flan 2022 Collection [19] as it provides an extensive public assortment of tasks and instructions. Particularly, we use FLANv2, supplemented with high-quality templates, advanced formatting patterns, and data augmentations. Even though FLAN holds tens of millions of instructions, we selectively sample from the task collection to form a diverse mixture of tasks, which we then further sub-sample to generate complex prompts. These prompts are used to query LFMs like ChatGPT and GPT-4, thus creating a rich and diverse training set. We collect 5 million ChatGPT responses, from which 1 million is further sampled to acquire GPT-4 responses. We demonstrate how ChatGPT as a teacher assistant helps in progressive learning.
Evaluation: We assess the generative, reasoning, and comprehension abilities of Orca, under a range of settings: (i) AutoEvaluation with GPT-4 on existing evaluation sets from Vicuna, WizardLM and the awesome prompts collection ${ }^{8}$; (ii) Academic benchmarks like Big-Bench Hard [4] and TruthfulQA [20]; (iii) Professional and Academic exams like SAT, LSAT, GRE, GMAT from AGIEval [1]; (iv) Safety evaluation with ToxiGen [21] to test toxic language generation and hate speech detection across different minority groups. Finally, we provide case-studies to compare the generation and reasoning abilities of Orca against OpenAI LFMs like ChatGPT and GPT-4, and instruction-tuned smaller model like Vicuna.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tuning Method</th>
<th>Data Size</th>
<th>Teacher</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alpaca</td>
<td>Simple Instructions / Self-instruct</td>
<td>52K</td>
<td>text-da-vinci-003</td>
</tr>
<tr>
<td>Vicuna</td>
<td>User Instructions / Natural</td>
<td>70K</td>
<td>ChatGPT</td>
</tr>
<tr>
<td>Dolly</td>
<td>User Instructions / Natural</td>
<td>15K</td>
<td>Human</td>
</tr>
<tr>
<td>WizardLM</td>
<td>Complex Instructions / Evol-instruct</td>
<td>250K</td>
<td>ChatGPT</td>
</tr>
<tr>
<td>Orca</td>
<td>Complex Instructions / Explanations</td>
<td>5M</td>
<td>ChatGPT (5M)</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$\cap$ GPT-4 (1M)</td>
</tr>
</tbody>
</table>
<p>Table 1: Overview of popular models instruction tuned with OpenAI large foundation models (LFMs). Orca leverages complex instructions and explanations for progressive learning.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Instruction-tuning with GPT-4 ${ }^{9}$. Given user instructions for a task and an input, the system generates a response. Existing works like Alpaca [7], Vicuna [9] and variants follow a similar template to train small models with $\langle$ {user instruction, input $}$, output $\rangle$.</p>
<h1>2 Preliminaries</h1>
<h3>2.1 Instruction Tuning</h3>
<p>Instruction tuning [22] is a technique that allows pre-trained language models to learn from input (natural language descriptions of the task) and response pairs, for example, {"instruction": "Arrange the words in the given sentence to form a grammatically correct sentence.", "input": "the quickly brown fox jumped", "output": "the brown fox jumped quickly"}. Instruction tuning has been applied to both language-only and multimodal tasks. For language-only tasks, instruction tuning has been shown to improve the zero-shot and few-shot performance of models such as FLAN [22] and InstructGPT [5] on various benchmarks. For multimodal tasks, instruction tuning has been used to generate synthetic instruction-following data for language-image tasks, such as image captioning [23] and visual question answering [24].
A wide range of works in recent times, including Alpaca [7], Vicuna [9], WizardLM [8] and Koala [14], have adopted instruction-tuning to train smaller language models with outputs generated from large foundation models from the GPT family. As outlined in Section 1.1, a significant drawback with all these works has been both limited task diversity, query complexity and small-scale training data in addition to limited evaluation overstating the benefits of such approach.</p>
<h3>2.2 Role of System Instructions</h3>
<p>Vanilla instruction-tuning (refer to Figure 4 for examples) often uses input, response pairs with short and terse responses. Such responses when used to train smaller models, as in existing works, give them limited ability to trace the reasoning process of the LFM. In constrast, system instructions ${ }^{10}$ in recent LFMs like GPT-4 can be used to provide guidance</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Explanation-tuning with GPT-4. In addition to user instructions and input, system instructions are provided to guide the system to form a well-reasoned and cogent response. System instructions are sampled from a diverse instruction set including chain-of-thought reasoning steps, explain like I'm five, being helpful and informative, etc. Such rich and well-structured response allows tuning small models to mimic the thinking process of GPT-4 on $\langle$ {system instruction, user instruction, input}, output $\rangle$ pairs.
to the model on how to behave and respond. They are written in natural language and separated from the user messages by using the role of "system" in the JSON request. System instructions can specify the tone, task, format, and limitations of the model's responses. System instructions are also a way of improving the safety of model responses. For example, a set of system instructions designed for safety harness could be:</p>
<ul>
<li>The assistant must not generate harmful or offensive content.</li>
<li>The assistant must respect the privacy and consent of the user.</li>
<li>The assistant must acknowledge its limitations and uncertainties.</li>
</ul>
<h1>3 Explanation Tuning</h1>
<p>To address the shortcomings of existing works, we tap into large-scale training data with diverse tasks augmented with complex instructions and rich signals. Specifically, our data contains human and augmented system instructions for a large collection of tasks sampled from FLAN-v2 (aka Flan 2022) [19]. Given the large size of the FLAN-v2 collection and varying number of examples for constituent datasets and tasks, we sample from a mixture of tasks from different categories (described in the next section) to create our training data.</p>
<h3>3.1 Dataset Construction</h3>
<p>Each instance in our training data consists of the following triple: $\langle$ System message, User query, LFM response $\rangle$. The system message, placed at the start of the prompt, provides the LFM with essential context, guidelines, and other pertinent details. We leverage the system message to vary the length of the response; outline the assistant's character; establish acceptable and non-acceptable LFM behavior; and determine the structure of the agent's response. The user query defines the actual task we want the LFM to perform. To obtain a large and diverse set of user queries we utilize the FLAN-v2 collection [19]. We sample 5 million user queries from FLAN-v2 for which we collect ChatGPT responses. We further sample 1 million instructions from the 5 million set for which we collect GPT-4 responses. All the queries to the agents are augmented with system instructions, as outlined below.</p>
<h1>3.1.1 System Messages</h1>
<p>We hand-craft a total of 16 system messages designed to evoke different kinds of responses from the LFM. This allows us to train Orca to generate long and short answers; follow guidelines, instructions, and format; generate creative content as well as address informationseeking queries; and most importantly, generate explanations and step-by-step reasoning for the responses, as prompted.</p>
<h2>Id. System Message</h2>
<p>$1 \quad$ <empty system message>
2 You are an AI assistant. Provide a detailed answer so user don't need to search outside to understand the answer.
3 You are an AI assistant. You will be given a task. You must generate a detailed and long answer.
4 You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.
5 You are an AI assistant that follows instruction extremely well. Help as much as you can.
6 You are an AI assistant that helps people find information. Provide a detailed answer so user don't need to search outside to understand the answer.
7 You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.
8 You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.
9 Explain how you used the definition to come up with the answer.
10 You are an AI assistant. You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. You might need to use additional knowledge to answer the question.
11 You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-bystep and justify your answer.
12 User will you give you a task with some instruction. Your job is follow the instructions as faithfully as you can. While answering think step-by-step and justify your answer.
13 You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.
14 You are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.
15 Given a definition of a task and a sample input, break the definition into small parts. Each of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format:
Part #: a key part of the definition.
Usage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.
16 You are an AI assistant that helps people find information.
Table 2: System instructions used to augment user instructions and task descriptions to query large foundation models for explanation tuning. System messages are designed to preserve the ability of the model to generate both short and long answers.</p>
<p>We have crafted different system messages for different sub-collections of the FLAN-v2 collection. Table 2 lists all the system instructions used to generate our training data. Figure 6 shows the distribution of system messages across different sub-collections. Note that system message #8 and system message#10 are sampled only for multiple-choice questions; thus they are less in number.</p>
<h3>3.1.2 Dataset Description and Sampling from the FLAN-v2 Collection</h3>
<p>The FLAN-v2 Collection [19] consists of five sub-collections, namely, CoT, NiV2, T0, Flan 2021, Dialogue. Each sub-collection contains multiple tasks, where each task is a collection</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Relative frequency of system messages in different collections of our training data.</p>
<p>of queries. Each sub-collection is associated with multiple academic datasets. One or more tasks are created from each dataset, focusing on zero shot and few-shot queries. In this work, we sample only zero-shot queries for training Orca. We have not sampled from the Dialogue sub-collection as the queries often lack context to elicit useful response from ChatGPT.</p>
<table>
<thead>
<tr>
<th>Mixture Name</th>
<th>Sampling Algorithm</th>
<th>Original Size</th>
<th>Sampled</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT</td>
<td>No Sampling</td>
<td>150K</td>
<td>150K</td>
</tr>
<tr>
<td>NIV2</td>
<td>Stratified Sampling</td>
<td>5M</td>
<td>440K</td>
</tr>
<tr>
<td>FLAN2021</td>
<td>Stratified Sampling</td>
<td>&gt;28.9M</td>
<td>2.5M</td>
</tr>
<tr>
<td>T0</td>
<td>Stratified Sampling</td>
<td>85.7M</td>
<td>2M</td>
</tr>
<tr>
<td>Dialog</td>
<td>Skipped</td>
<td>22.5M</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Table 3: Construction of our training data with 5 million samples.</p>
<p>Zero-Shot CoT: The zero-shot Chain-Of-Thought (CoT) collection contains a total of 18 tasks involving math word problem solving, natural language inference, common-sense reasoning, science question answering, odd-one-out reasoning. Each query in this dataset is geared for chain-of-thought response. Figure 7 shows a demonstrative example from the CoT collection. It also shows the human written response that is part of the original dataset, ChatGPT generated response and GPT-4 response. <em>While all the sources agree on the final answer, ChatGPT and GPT-4 augmented with system instructions generate quite detailed responses as opposed to the human authored answer.</em> The 18 tasks contain approximately 150 thousand queries in total. We include all of them in our 5 million set.</p>
<p>NiV2: NiV2 contains a total of 1560 tasks and roughly 5 million queries. The number of queries vary in each task. Figure 8 shows the distribution of the tasks along with task types.</p>
<p>We randomly sample 300 queries from each task, and the entire task data containing lesser number of queries. This gives us a total of 440 thousand queries.</p>
<p>Flan 2021: Flan 2021 collection contains a total of 142 tasks that are created from 62 datasets. The entire Flan 2021 collection from the published script is quite large and expensive to augment. In order to sample a diverse and representative subset, we generate at most 1 million queries from each task, which gives us roughly 28.9 million queries in total. Each task in this collection contains varying number of queries. We sample a total of 2.5 million queries from this collection of 85.7 million queries. The sampling process is outlined in Algorithm 1.</p>
<h1>### System:</h1>
<p>You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.
### Human:
Pick which sentence is not logical.
Options:</p>
<ul>
<li>Sentence A: "people in the desert often look forward to flood"</li>
<li>Sentence B: "people in the desert often look forward to rain"</li>
</ul>
<p>Hmmm, let me think. I want to lay out the solution in details.
### Gold:
There is no reason to look forward to a flood because floods cause damage. The answer is Sentence A.</p>
<h2>### ChatGPT:</h2>
<p>Sure, let's break it down step by step.</p>
<ol>
<li>Read the sentences carefully.</li>
<li>Identify the difference between the two sentences.</li>
<li>Determine which sentence is not logical based on the difference.</li>
</ol>
<p>Now let's apply these steps to the given sentences:</p>
<ol>
<li>
<p>Sentence A: "people in the desert often look forward to flood"</p>
</li>
<li>
<p>This sentence suggests that people in the desert are eagerly anticipating a flood. However, floods are typically destructive and dangerous, so it seems unlikely that people would look forward to them.</p>
</li>
<li>
<p>Sentence B: "people in the desert often look forward to rain"</p>
</li>
<li>
<p>This sentence suggests that people in the desert are eagerly anticipating rain. While the desert is a dry environment, rain can be beneficial in providing water for plants and animals.</p>
</li>
<li>
<p>Based on the above analysis, it can be concluded that Sentence A is not logical as it suggests that people in the desert look forward to something that is typically harmful and dangerous. Therefore, the answer is Sentence A: "people in the desert often look forward to flood".</p>
</li>
</ol>
<h2>### GPT4:</h2>
<p>To determine which sentence is not logical, let's examine each sentence and its meaning in the context of people living in a desert.</p>
<p>Sentence A: "people in the desert often look forward to flood"</p>
<ul>
<li>This sentence implies that people living in a desert are eagerly awaiting a flood. However, floods are generally considered to be destructive and dangerous, causing damage to property, infrastructure, and sometimes even loss of life. It is not logical for people to look forward to such a disastrous event.</li>
</ul>
<p>Sentence B: "people in the desert often look forward to rain"</p>
<ul>
<li>This sentence implies that people living in a desert are eagerly awaiting rain. Rain is typically beneficial in a desert environment, as it provides water for drinking, agriculture, and replenishing natural resources. It is logical for people living in a desert to look forward to rain, as it can bring relief from the harsh, dry conditions.</li>
</ul>
<p>Based on the analysis of both sentences, it can be concluded that Sentence A ("people in the desert often look forward to flood") is not logical, as it suggests that people are eagerly anticipating a destructive event, which is not a reasonable expectation. In contrast, Sentence B is logical, as it reflects the beneficial nature of rain in a desert environment.</p>
<p>Figure 7: Demonstrative example from Flan-CoT Collection.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: NiV2 task distribution reproduced from [25].</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Sampling Algorithm for Flan 2021 and T0 collection.
Input: tasks \(T=\left\{t_{1}, t_{2}, \ldots, t_{m}\right\}\) , number of queries to sample \(n\)
Output: sampled queries \(Q=\left\{q_{1}, q_{2}, \ldots, q_{n}\right\}\)
\(Q \leftarrow\) empty list
while \(|Q|&lt;n\) do
    \(t \leftarrow\) randomly sample a task from \(T\)
    \(q \leftarrow\) randomly sample a query without replacement from \(t\)
    add \(q\) to \(Q\)
    if \(t\) is empty then
        remove \(t\) from \(T\)
    end
end
return \(Q\)
</code></pre></div>

<p>T0: T0 collection contains a total of 193 tasks that are associated with 35 training datasets. We only incorporate the tasks that are associated with the training split T0, which excludes Big-Bench. This is important as we include Big-Bench-Hard in our evaluation benchmark. T0 collection contains roughly 85.7 million queries with the number of queries varying in each task. We sample a total of 2 million queries from this collection using the sampling process in Algorithm 1.</p>
<h1>3.1.3 ChatGPT as Teaching Assistant</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Comparing GPT-4 and ChatGPT response length distribution for different system messages. The system message ids ${1,2, \ldots, 16}$ correspond to the row numbers in Table 2. We observe GPT-4 to elicit longer responses compared to ChatGPT.</p>
<table>
<thead>
<tr>
<th>Teacher</th>
<th>Cost per 1000 token</th>
<th>Requests per minute</th>
<th>Tokens per minute</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>$\$ 0.002$</td>
<td>300</td>
<td>120,000</td>
</tr>
<tr>
<td>GPT-4 (8K)</td>
<td>$\$ 0.03$ (prompt),</td>
<td>18</td>
<td>10,000</td>
</tr>
<tr>
<td></td>
<td>$\$ 0.06$ (token completion)</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Pricing and quota limit for data collection from ChatGPT (GPT-3.5-turbo) and GPT-4 endpoints using Azure OpenAI service.</p>
<p>We generate 5 million instructions (queries augmented with system messages) referred as FLAN-5M following sampling techniques outlined in the previous section. We further randomly sample 1 million queries from FLAN-5M to create another split, referred as FLAN-1M. We use Azure OpenAI API to collect ChatGPT (GPT-3.5-turbo) responses to FLAN-5M, and GPT-4 responses to FLAN-1M.</p>
<p>We first train Orca on FLAN-5M (ChatGPT augmentations), followed by second stage of training on FLAN-1M (GPT-4 augmentations). Essentially, we leverage ChatGPT as intermediate teacher assistant for two reasons.</p>
<ul>
<li>Capacity gap: Orca with $13 B$ parameters is many times smaller than GPT-4 (size undisclosed). Leveraging an intermediate teacher with reduced gap in capabilities, in this case ChatGPT, has been shown to improve imitation learning performance for smaller students in knowledge distillation [15]. This can be viewed as a form of progressive learning or curriculum learning, where the student first learns from easier examples, followed by harder ones: with the assumption that longer responses are difficult to mimic than shorter ones, along with improved reasoning and step-by-step explanation from a larger teacher.</li>
<li>Cost ${ }^{12}$ and Time ${ }^{13}$ : Large-scale data collection from Azure OpenAI API's are constrained by, (a) rate limit in terms of allowed requests per minute to prevent throttling the endpoints, (b) available tokens per minute due to serving latency, and (c) the dollar cost for length of prompt and token completion (demonstrated in Table 4) with the ChatGPT API being much faster and cheaper than the GPT-4 endpoint. To this end, we collect $5 \times$ as much data from ChatGPT compared to GPT-4.</li>
</ul>
<p>Figure 9 shows the response length distribution for ChatGPT and GPT-4 corresponing to different system messages. We observe that GPT-4 responses are on an average $1.5 \times$ longer than that of ChatGPT. This allows Orca to progressively learn from increasing complexity of teacher explanations. We demonstrate the impact of teacher assistance via ablation experiments.</p>
<h1>3.2 Training</h1>
<p>This section provides an overview of the training process for Orca, covering different aspects of tokenization, sequencing, and loss computation.
Tokenization: We utilize the LLaMA Byte Pair Encoding (BPE) tokenizer for processing the input examples. Notably, the LLaMA tokenizer splits all numbers into individual digits, and fallbacks to bytes to decompose unknown UTF-8 characters. To deal with variable length sequences we add a padding token "[[PAD]]" into the LLaMA tokenizer vocabulary. The resulting vocabulary contains 32,001 tokens.</p>
<p>Packing: To optimize the training process and utilize the available computational resources efficiently, we employ the packing technique [26]. This method involves concatenating</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>multiple input examples into a single sequence, which is then used for training the model. The packing is performed such that the total length of the concatenated sequence does not exceed max_len=2,048 tokens. Particularly, we shuffle the input examples and then partition the examples into groups such that length of the concatenated sequence in each group is at most max_len. Padding tokens are then added to the concatenated sequence to achieve a uniform input sequence length of max_len with a packing factor of 2.7 examples per sequence given the length distribution of augmented instructions in our training data.
Loss: For the purpose of training Orca, we compute the loss only on the tokens generated by the teacher model, i.e., it learns to generate responses conditioned on the system message and task instructions. This approach ensures that the model focuses on learning from the most relevant and informative tokens, improving the overall efficiency and effectiveness of the training process.</p>
<p>Compute: We trained Orca on 20 NVIDIA A100 GPUs with 80GB memory. It took 160 hours to train Orca on FLAN-5M (ChatGPT augmentations) for 4 epochs, and 40 hours to continue training on FLAN-1M (GPT-4 augmentations) for the same number of epochs.</p>
<p>It took 2 weeks and 3 weeks respectively to collect data from GPT-3.5-turbo (ChatGPT) and GPT-4 from multiple endpoints accounting for the throttling limit, endpoint load, and length distribution of query and response pairs.</p>
<h1>4 Experiment Setup</h1>
<p>We setup a rigorous evaluation protocol that considers a host of different abilities including writing, comprehension, analytical, mathematical and logical reasoning.</p>
<h3>4.1 Baselines</h3>
<p>We compare Orca ${ }^{14}$ against the following baselines:</p>
<ul>
<li>Text-Davinci-003 (TD-003): Text-Davinci-003 belong to the GPT-3.5 ${ }^{15}$ series of generation model that is optimized for text completion. It is a powerful model designed to do language tasks with better quality, longer output, and consistent instruction-following in multiple languages.</li>
<li>ChatGPT: ChatGPT (GPT-3.5-turbo) is the most capable GPT-3.5 model and an improvement on text-davinci-003. It is optimized for chat and trained using conversations with humans. OpenAI released this chatbot in November 2022.</li>
<li>GPT-4: GPT-4 is the latest model in the GPT family and exhibits human-level performance on various professional and academic benchmarks. Like ChatGPT, GPT-4 is optimized for chat and can perform more complex tasks than its predecessors. It typically shows substantially higher performance than GPT-3.5 models, especially on tasks that require complex reasoning. For both ChatGPT and GPT-4, we use the OpenAI API version "2023-03-15-preview".</li>
<li>Vicuna: Vicuna [9] is an open-source chatbot that was trained by fine-tuning LLaMA[10] on user-shared conversations collected from ShareGPT. In this work, we use the Vicuna model consisting of 13B parameters. Vicuna has been the leading open-source language model in multiple leaderboards including Chatbot Arena ${ }^{16}$ and Open LLM Leaderboard ${ }^{17}$. We used Vicuna model checkpoint current as of April 21, 2023.</li>
</ul>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: left;">Task Type</th>
<th style="text-align: center;"># Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vicuna Prompts [9]</td>
<td style="text-align: left;">Open-ended questions and generation</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Awesome Prompts [27]</td>
<td style="text-align: left;">Open-ended questions and generation</td>
<td style="text-align: center;">164</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM Prompts [8]</td>
<td style="text-align: left;">Open-ended questions and generation</td>
<td style="text-align: center;">218</td>
</tr>
<tr>
<td style="text-align: center;">AGIEval [1]</td>
<td style="text-align: left;">Suite of professional and academic exams /</td>
<td style="text-align: center;">3546</td>
</tr>
<tr>
<td style="text-align: center;">Big-Bench Hard [11]</td>
<td style="text-align: left;">multiple-choice questions <br> Suite of complex reasoning tasks / multiple- <br> choice questions</td>
<td style="text-align: center;">5511</td>
</tr>
</tbody>
</table>
<p>Table 5: Orca evaluation benchmarks. Dataset statistics.
### System: You are a helpful and precise assistant for checking the quality of the answer.
### Human:
[Question]
Question
[The Start of Assistant 1's Answer]
Answer 1
[The Start of Assistant 2's Answer]
Answer 2
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10 , where a higher score indicates better overall performance.
Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
### Assistant:</p>
<p>Figure 10: Prompt template from Vicuna [9] to rate the writing quality of the candidate assistant model against the reference model (e.g., ChatGPT, GPT-4).</p>
<h1>4.2 Tasks</h1>
<p>We provide a detailed account of the tasks used to evaluate Orca's capability in terms of open-ended generation and its ability to reason and comprehend complex reasoning tasks in this section. Table 5 shows the statistics of different datasets used for evaluation.</p>
<h3>4.2.1 Open-ended Generation Capabilities</h3>
<p>Vicuna [9] used an evaluation framework based on GPT-4 to automate chatbot performance assessment. They originally devised eight question categories to test various aspects of chatbot performance and found that GPT-4 can produce relatively consistent scores and detailed explanations of those scores. In this setup, GPT-4 rates the quality of generation from a model on a scale of 0 to 10 . We leverage the same setup and experiment with three different prompt collections, which cover a wide range of open-ended answering tasks:</p>
<ul>
<li>Vicuna Prompts: These are the original prompts proposed in Vicuna. These 80 prompts are divided into nine skills including generic, knowledge, roleplay, common-sense, fermi, counterfactual, coding, math, and writing abilities.</li>
<li>Awesome Prompts: Awesome ChatGPT prompts [27] is a collection of prompt examples that are primarily used with the ChatGPT model as reference.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Exams</th>
<th style="text-align: left;">#Participants</th>
<th style="text-align: left;">Tasks</th>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;"># Exam- <br> ples</th>
<th style="text-align: center;">Avg. <br> Token</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GRE</td>
<td style="text-align: left;">340 K</td>
<td style="text-align: left;">AQuA-RAT</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GMAT</td>
<td style="text-align: left;">150 K</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Civil Service <br> Examination</td>
<td style="text-align: left;">2 M</td>
<td style="text-align: left;">LogiQA</td>
<td style="text-align: center;">Logic</td>
<td style="text-align: center;">651</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Law School</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSAT-AR</td>
<td style="text-align: center;">Law-Analytics</td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Admission</td>
<td style="text-align: left;">170 K</td>
<td style="text-align: left;">LSAT-LR</td>
<td style="text-align: center;">Law-Logic</td>
<td style="text-align: center;">510</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Test (LSAT)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">LSAT-RC</td>
<td style="text-align: center;">Law-Reading</td>
<td style="text-align: center;">260</td>
<td style="text-align: center;">581</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SAT</td>
<td style="text-align: left;">1.7 M</td>
<td style="text-align: left;">SAT-English</td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">206</td>
<td style="text-align: center;">656</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">SAT-Math</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">220</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6: Breakdown of tasks in AGIEval benchmark as reported in [1]. We show the statistics of individual tasks in terms of exams, number of human participants taking these exams annually, subject involved, number of examples and average tokens per example.</p>
<p>The prompts offer an efficient way to automate numerous tasks, including writing, translating, summarizing, analyzing, and beyond. These prompts are based on 164 roles such as life coach, startup tech lawyer, astrologer, chess player, statistician, and note-taking assistant.</p>
<ul>
<li>WizardLM Prompts: WizardLM prompts [8] are a collection of prompt examples based on real-world tasks. These prompts are sourced from open-source projects, platforms, and forums. They are divided into 29 distinct skills along with the difficulty level of each prompt. These skills cover some of the main requirements of human-level intelligence including math, academic writing, debugging, code generation, and reasoning abilities.</li>
</ul>
<h1>4.2.2 Reasoning Capabilities</h1>
<ul>
<li>AGIEval: AGIEval [1] is a human-centric benchmark that evaluates the general abilities of foundation models in tasks related to human cognition and problem-solving. The benchmark is derived from official and standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., GRE, GMAT, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and national civil service exams. The benchmark assesses foundation models in the context of human-centric standardized exams. The statistics of individual tasks in terms of exams, number of human participants taking these exams annually, subject involved, number of examples, and average token number is shown in Table 6. In this work, we only consider the datasets that correspond to multiple-choice questions from English language.</li>
<li>Big-Bench Hard (BBH): BIG-Bench Hard is a suite of 23 challenging BIG-Bench [4] tasks that were introduced to measure the capabilities and limitations of large language models. These are the tasks for which prior language model evaluations did not outperform the average human-rater. In this work, we only use the datasets that correspond to multiple choice questions. We perform evaluation with standard zero-shot prompting and do not use any labeled examples.</li>
</ul>
<p>Prompt template and parsing of the model response: We evaluate reasoning capabilities under zero-shot setting without any exemplars and without CoT. Given the free-form response from the generative models, it is difficult to parse the answer to the MCQ questions in these benchmarks. For all the MCQ tasks, we use the prompt format and parsing from AGIEval [1] (see Figure 13 for prompt template) with the question, followed by answer choices, and a prompt completion sequence like "Among 0 through 3, the answer is". We only consider the first capital character in the response to compare with the gold answer-id (exact match). Since models do not always follow this template in zero-shot setting, they are penalized if the expected answer-id appears later in the response. We employ the same parsing logic to all the models' responses for consistency.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Reference</th>
<th style="text-align: center;">Vicuna-13B</th>
<th style="text-align: center;">Orca-13B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vicuna Prompts</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">$101.5(10.4 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">$87.7(18.9 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Awesome Prompts</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">$98.1(13.5 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">$89.3(14.9 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM Prompts</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">$84.9(10.1 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">$78.4(13.5 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">$94.8(11.3 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">$85.1(13.5 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 7: GPT-4 is used as a judge / rater to compare the performance of candidate models (e.g., Vicuna, Orca) against ChatGPT (GPT-3.5-turbo) and GPT-4 as reference models. We report the percentage improvement in the overall score obtained by the candidate compared to that of the reference model. Percentage improvement of Orca over Vicuna is shown in parenthesis. Overall, Orca retains $95 \%$ of ChatGPT quality and $85 \%$ of GPT-4 quality aggregated across all datasets as assessed by GPT-4, a 10-point improvement over Vicuna. Figure 1 shows performance of other models including LLaMA-13B, Alpaca-13B and Bard.</p>
<h1>5 Evaluation for Open-ended Generation</h1>
<p>Table 7 shows the performance of candidate models (e.g., Vicuna, Orca) against ChatGPT (GPT-3.5-turbo) and GPT-4 as reference models on the three datasets, where GPT-4 is used as a judge / rater. The performance of a candidate model is measured as the percentage improvement in the overall score obtained by the candidate model compared to that of the reference model.</p>
<ul>
<li>Orca retains $95 \%$ of ChatGPT quality and $85 \%$ of GPT-4 quality aggregated across all datasets as assessed by GPT-4. Orca shows a 10-point improvement over Vicuna on an aggregate.</li>
<li>Orca performs on par with ChatGPT on Vicuna's original evaluation setting. In this setting, the candidate model is compared to ChatGPT with GPT-4 as rater on the Vicuna prompts dataset.</li>
<li>Orca exhibits strong performance for prompts that span across a wide range of generation roles. For the Awesome prompts dataset that spans 164 open-ended generation roles, Orca shows strong performance by retaining $98 \%$ of ChatGPT quality and $89 \%$ of GPT-4 quality.</li>
</ul>
<p>Replication Note: We observe that there is a positive bias in GPT-4 evaluation towards the response of the first model in the comparison set. This has also been reported in a recent work [18] on analyzing the bias of GPT-4 as an evaluator. In all the above evaluations, the first model is considered as the reference model consistent with Vicuna setup ${ }^{18}$.</p>
<h2>6 Evaluation for Reasoning</h2>
<h3>6.1 AGIEval Results</h3>
<p>Table 8 shows the standard zero-shot (no exemplars, no CoT) performance comparison of Orca against baseline models on the AGIEval benchmark on multiple-choice English questions. The performance on each of these tasks is measured using accuracy metric, following exactly the same evaluation setup as proposed in AGIEval [1].</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Human <br> -Avg</th>
<th style="text-align: left;">Human <br> -Top</th>
<th style="text-align: left;">TD- <br> 003</th>
<th style="text-align: left;">Chat <br> GPT</th>
<th style="text-align: left;">GPT- <br> 4</th>
<th style="text-align: left;">Vicuna- <br> 13B</th>
<th style="text-align: left;">Orca- <br> 13B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AQuA-RAT</td>
<td style="text-align: left;">85</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">29.9</td>
<td style="text-align: left;">31.9</td>
<td style="text-align: left;">40.6</td>
<td style="text-align: left;">20.1</td>
<td style="text-align: left;">$\mathbf{2 7 . 9}$ <br> $(39.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LogiQA</td>
<td style="text-align: left;">86</td>
<td style="text-align: left;">95</td>
<td style="text-align: left;">22.7</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">49.3</td>
<td style="text-align: left;">29.8</td>
<td style="text-align: left;">$\mathbf{3 5 . 2}$ <br> $(18.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LSAT-AR</td>
<td style="text-align: left;">56</td>
<td style="text-align: left;">91</td>
<td style="text-align: left;">21.7</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;">35.2</td>
<td style="text-align: left;">20.4</td>
<td style="text-align: left;">$\mathbf{2 1 . 3}$ <br> $(4.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LSAT-LR</td>
<td style="text-align: left;">56</td>
<td style="text-align: left;">91</td>
<td style="text-align: left;">47.5</td>
<td style="text-align: left;">52.6</td>
<td style="text-align: left;">80.6</td>
<td style="text-align: left;">32.6</td>
<td style="text-align: left;">$\mathbf{4 3 . 9}$ <br> $(34.9 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LSAT-RC</td>
<td style="text-align: left;">56</td>
<td style="text-align: left;">91</td>
<td style="text-align: left;">64.7</td>
<td style="text-align: left;">65.4</td>
<td style="text-align: left;">85.9</td>
<td style="text-align: left;">32.7</td>
<td style="text-align: left;">$\mathbf{5 7 . 3}$ <br> $(75.0 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SAT-Math</td>
<td style="text-align: left;">66</td>
<td style="text-align: left;">94</td>
<td style="text-align: left;">35.5</td>
<td style="text-align: left;">42.7</td>
<td style="text-align: left;">64.6</td>
<td style="text-align: left;">28.6</td>
<td style="text-align: left;">$\mathbf{3 2 . 3}$ <br> $(12.7 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SAT-English</td>
<td style="text-align: left;">66</td>
<td style="text-align: left;">94</td>
<td style="text-align: left;">74.8</td>
<td style="text-align: left;">81.1</td>
<td style="text-align: left;">88.8</td>
<td style="text-align: left;">44.2</td>
<td style="text-align: left;">$\mathbf{7 6 . 7}$ <br> $(73.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SAT-English <br> (w/o Psg.)</td>
<td style="text-align: left;">66</td>
<td style="text-align: left;">94</td>
<td style="text-align: left;">38.4</td>
<td style="text-align: left;">44.2</td>
<td style="text-align: left;">51</td>
<td style="text-align: left;">26.2</td>
<td style="text-align: left;">$\mathbf{3 8 . 8}$ <br> $(48.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">67.1</td>
<td style="text-align: left;">93.8</td>
<td style="text-align: left;">41.9</td>
<td style="text-align: left;">47.2</td>
<td style="text-align: left;">62</td>
<td style="text-align: left;">29.3</td>
<td style="text-align: left;">$\mathbf{4 1 . 7}$ <br> $(42.1 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 8: Zero-shot performance comparison of Text-da-vinci-003, ChatGPT, GPT-4, Vicuna, and Orca in AGIEval benchmark [1] on multiple-choice English questions. We report performance of Human, TD-003, ChatGPT and GPT-4 from [1]. Human performance is broken down into average and top performance. 'Average' corresponds to the average performance of all test takers, while 'top' corresponds to the performance of the top $1 \%$ of the test takers. Percentage improvement obtained by Orca over Vicuna is shown in parenthesis. Overall, Orca performs at par with Text-da-vinci-003; retaining $88 \%$ of ChatGPT quality; significantly lagging GPT-4; and outperforming Vicuna by $42 \%$.</p>
<ul>
<li>Orca performs at par with Text-da-Vinci-003, on an aggregate across all tasks, and retains $88 \%$ of ChatGPT quality. However, Orca significantly lags GPT-4.</li>
<li>We observe that for such analytical and reasoning tasks, Vicuna performs significantly worse, retaining only $62 \%$ of ChatGPT quality as opposed to $85 \%$ for open-ended generation (Table 7 vs Table 8 ). This demonstrates the poor reasoning abilities of such open-source language models.</li>
<li>While performing on par with Text-da-Vinci-003 and 5 points below ChatGPT, Orca demonstrates bigger gaps with ChatGPT on math-related tasks (in SAT, GRE, GMAT).</li>
<li>Compared to Vicuna, Orca shows much stronger performance outperforming it on every category with $42 \%$ relative improvment on average.</li>
<li>GPT-4's performance surpasses all other models by far, but there is still a lot of headroom in this benchmark as the performance of all the models are significantly lower than human performance across all tasks.</li>
<li>The performance of Orca vary significantly based on the type of system message (see Table 9). For our trained model, the empty system message often works well.</li>
<li>ChatGPT dominates Orca in 450 examples across different tasks (ChatGPT-beats-Orca examples). Majority of those examples come from LSAT-LR (22\%) and LogiQA (21\%) tasks, while other LSAT tasks and SAT-English tasks contribute less than $10 \%$ each.</li>
<li>Orca beats ChatGPT in 325 examples across different tasks (Orca-beats-ChatGPT examples). Out of those examples, majority come from LogiQA (29\%), while other LSAT tasks and SAT-English tasks contribute less than $10 \%$ each.</li>
</ul>
<p>Scaling and Teacher Assistance: To analyze the impact of progressive learning, we train Orca with only GPT-4 augmentations (FLAN-1M), and contrast with the full version trained on both ChatGPT (FLAN-5M) and GPT-4 (FLAN-1M) augmentations with results in Table 10. We observe that scaling the amount of explanation data by $5 \times$ with intermediate ChatGPT assistance significantly improves the model performance by 4.5 points on aggregate.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task / System Message</th>
<th style="text-align: center;">Empty</th>
<th style="text-align: center;">Follow Well</th>
<th style="text-align: center;">Detailed Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AQuA-RAT</td>
<td style="text-align: center;">$\mathbf{2 7 . 9}$</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">LogiQA</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">$\mathbf{3 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-AR</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-LR</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-RC</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">SAT-Math</td>
<td style="text-align: center;">$\mathbf{3 2 . 3}$</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">SAT-English</td>
<td style="text-align: center;">$\mathbf{7 6 . 7}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">SAT-English (w/o Psg.)</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">$\mathbf{3 9 . 3}$</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$\mathbf{4 1 . 7}$</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">41.6</td>
</tr>
</tbody>
</table>
<p>Table 9: Zero-shot performance comparison of Orca with different system messages in AGIEval benchmark on multiple-choice English questions. The system messages and their identifiers from Table 2 correspond to <empty system message> (Id. 1), follow well (Id. 5) and detailed answer (Id. 2). Considering the performance with the best system instruction for each task, Orca has a performance gap of 4.4 pts against ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task / Model</th>
<th style="text-align: center;">Orca</th>
<th style="text-align: center;">Orca-FLAN-1M (GPT-4 only)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AQuA-RAT</td>
<td style="text-align: center;">$\mathbf{2 7 . 9}$</td>
<td style="text-align: center;">21.65</td>
</tr>
<tr>
<td style="text-align: center;">LogiQA</td>
<td style="text-align: center;">$\mathbf{3 5 . 2}$</td>
<td style="text-align: center;">31.95</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-AR</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-LR</td>
<td style="text-align: center;">$\mathbf{4 3 . 9}$</td>
<td style="text-align: center;">41.76</td>
</tr>
<tr>
<td style="text-align: center;">LSAT-RC</td>
<td style="text-align: center;">$\mathbf{5 7 . 3}$</td>
<td style="text-align: center;">51.67</td>
</tr>
<tr>
<td style="text-align: center;">SAT-Math</td>
<td style="text-align: center;">$\mathbf{3 2 . 3}$</td>
<td style="text-align: center;">26.82</td>
</tr>
<tr>
<td style="text-align: center;">SAT-English</td>
<td style="text-align: center;">$\mathbf{7 6 . 7}$</td>
<td style="text-align: center;">68.45</td>
</tr>
<tr>
<td style="text-align: center;">SAT-English (w/o Psg.)</td>
<td style="text-align: center;">$\mathbf{3 8 . 8}$</td>
<td style="text-align: center;">36.41</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$\mathbf{4 1 . 7}$</td>
<td style="text-align: center;">37.18</td>
</tr>
</tbody>
</table>
<p>Table 10: Zero-shot performance comparison of Orca trained on FLAM-5M (ChatGPT) and FLAN-1M (GPT-4), vs Orca trained only on FLAN-1M (GPT-4) in AGIEval benchmark on multiple-choice English questions.</p>
<h1>Analysis of 100 random ChatGPT-beats-Orca and Orca-beats-ChatGPT samples:</h1>
<ul>
<li>Domain knowledge: Models require specialized domain knowledge to solve some of the problems such as Tesla batteries, concepts from Chemistry, etc. $15 \%$ and $21 \%$ of the ChatGPT-beats-Orca and Orca-beats-ChatGPT examples respectively fall under this category.</li>
<li>Complex reasoning: Some examples require complex reasoning such as reasoning about more than five objects/persons. For example, the logical reasoning question that starts with "There are 6 rectangular vegetable ponds of the same size in a plastic shed, arranged in order from left to right?" requires the model to capture spatial relationships of six ponds and perform spatial reasoning. $14 \%$ and $18 \%$ of the ChatGPT-beats-Orca and Orca-beats-ChatGPT examples respectively fall under complex reasoning category.</li>
<li>Long context: Some examples have long context (e.g., passage containing several paragraphs of text), which require reasoning over long spans of text. $16 \%$ of ChatGPTbeats-Orca examples have long context, while context of only $8 \%$ of Orca-beats-ChatGPT examples are long. This result highlights that ChatGPT has an edge over Orca in modeling long contexts.</li>
<li>Geometric reasoning: Examples such as "The ratio of the volumes of a cube to that of the sphere which will fit inside the cube is?" require reasoning about geometric objects. $2 \%$ and $5 \%$ of the ChatGPT-beats-Orca and Orca-beats-ChatGPT examples respectively fall under this category, indicating the performance gap in geometric reasoning between the two models.</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Topical breakdown in performance of GPT-4, ChatGPT and Orca in the AGIEval benchmark on professional and academic exams.</p>
<ul>
<li>LaTeX reasoning: Some examples have LaTeX typesetting in the question, which requires understanding of LaTeX symbols for solving these examples. For example, " $A$ line in the $\$ x y \$$-plane passes through the origin and has a slope of $\$ \backslash f r a c{1}{7} \$$. Which of the following points lies on the line?" requires processing the fraction operator. $2 \%$ and $10 \%$ of the ChatGPT-beats-Orca and Orca-beats-ChatGPT examples respectively fall under this category.</li>
</ul>
<h1>6.2 Big-Bench Hard Results</h1>
<p>Table 11 shows the zero-shot performance comparison of Orca against baseline models on Big-Bench Hard with standard zero-shot prompting (no exemplars, no CoT). Orca performs marginally better than ChatGPT on aggregate across all tasks; significantly lags GPT-4; and outperforms Vicuna by $113 \%$. Similar to AGIEval, Vicuna performs poorly on sophisticated reasoning tasks in this benchmark.
While significantly better than Vicuna and marginally better than ChatGPT, Orca's average performance of $49.7 \%$, lags GPT-4 by $26 \%$. Note that GPT-4 has reported a data contamination issue with Big-Bench and that we are not aware of such issues with either LLaMA's training data (the base model used by both Vicuna and Orca) or the Flan-V2 collection or Vicuna's training data (ShareGPT).
Given the close performance on average on BigBench-Hard, we take a deeper look at differences in performance between Orca and ChatGPT:</p>
<h2>Entailment and Semantic Understanding:</h2>
<ul>
<li>Orca performs better at entailment (formal fallacies) and semantic understanding (Disambiguation QA and Snarks).</li>
<li>In the formal fallacies task, a model has to determine whether a given argument can be logically deduced from a set of statements, Orca achieves $4.5 \%$ improvement over ChatGPT on this task.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">Vicuna-13B</th>
<th style="text-align: center;">Orca-13B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Boolean Expressions</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">72.0 (76.5\%)</td>
</tr>
<tr>
<td style="text-align: center;">Causal Judgement</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">59.9 (41.8\%)</td>
</tr>
<tr>
<td style="text-align: center;">Date Understanding</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">50.0 (400.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Disambiguation QA</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">63.6 (245.7\%)</td>
</tr>
<tr>
<td style="text-align: center;">Formal Fallacies</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">56.0 (18.6\%)</td>
</tr>
<tr>
<td style="text-align: center;">Geometric Shapes</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">20.8 (477.8\%)</td>
</tr>
<tr>
<td style="text-align: center;">Hyperbaton</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">64.0 (45.5\%)</td>
</tr>
<tr>
<td style="text-align: center;">Logical Deduction (5 objects)</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">39.6 (725.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Logical Deduction (7 objects)</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">36.0 (2900.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Logical Deduction (3 objects)</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">57.6 (242.9\%)</td>
</tr>
<tr>
<td style="text-align: center;">Movie Recommendation</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">78.3 (80.6\%)</td>
</tr>
<tr>
<td style="text-align: center;">Navigate</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">57.6 (24.1\%)</td>
</tr>
<tr>
<td style="text-align: center;">Penguins in a Table</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">42.5 (181.8\%)</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning about Colored Objects</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">48.4 (303.3\%)</td>
</tr>
<tr>
<td style="text-align: center;">Ruin Names</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">39.5 (151.2\%)</td>
</tr>
<tr>
<td style="text-align: center;">Salient Translation Error Detection</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">40.8 (1940.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Snarks</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">62.4 (122.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Sports Understanding</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">67.2 (38.8\%)</td>
</tr>
<tr>
<td style="text-align: center;">Temporal Sequences</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">72.0 (350.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Tracking Shuffled Objects (5 objects)</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">15.6 (69.6\%)</td>
</tr>
<tr>
<td style="text-align: center;">Tracking Shuffled Objects (7 objects)</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">14.0 (150.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Tracking Shuffled Objects (3 objects)</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">34.8 (50.0\%)</td>
</tr>
<tr>
<td style="text-align: center;">Web of Lies</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">51.2 (24.3\%)</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">49.7 (113.7\%)</td>
</tr>
</tbody>
</table>
<p>Table 11: Zero-shot performance comparison of ChatGPT, GPT-4, Vicuna, and Orca on BigBench Hard (multiple-choice questions) without CoT. Performance improvements obtained by Orca over Vicuna is shown in parenthesis.</p>
<ul>
<li>BBH benchmark has two semantic understanding tasks: Disambiguation QA, where a sentence is given with an ambiguous pronoun and the model needs to determine whether the pronoun can be deduced implicitly or the sentence is inherently ambiguous; and Snarks, where the goal is to identify sarcastic sentence from two nearly-identical sentences. Orca exceeds ChatGPT by $11.1 \%$ and $5.8 \%$ on Disambiguation QA and Snarks respectively.</li>
</ul>
<h1>Temporal and Spatial Reasoning:</h1>
<ul>
<li>Orca shows substantially better reasoning capabilities in terms of temporal reasoning, spatial reasoning and color based reasoning compared to ChatGPT.</li>
<li>Orca outperforms ChatGPT on Temporal Sequences (temporal reasoning), Navigate (following navigation instructions), Colored Objects (identify color of objects given context) by $102 \%, 3.6 \%$, and $1.7 \%$ respectively.</li>
</ul>
<h2>Causal Judgment:</h2>
<ul>
<li>Orca shows good performance on the causal judgement task, which measures the capability of the model to answer a causal question about a short story.</li>
<li>Orca performs on par with GPT-4, while exceeding ChatGPT by $4.7 \%$.</li>
</ul>
<h2>Multilingual Understanding:</h2>
<ul>
<li>Orca and ChatGPT achieve parity on the salient translation error detection task (determining the type of translation error in the translated sentence).</li>
<li>While this shows promising potential, we note that BBH offers limited evaluation of multilingual abilities and more evaluation is needed for better assessment.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{18}$ The performance of Vicuna-13B on WizardLM prompts is 84.87 (close to WizardLM's reported numbers), when Vicuna's response (candidate model) is presented first, before ChatGPT's response (reference model) in the prompt. This indicates that GPT-4 ratings can be significantly biased.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
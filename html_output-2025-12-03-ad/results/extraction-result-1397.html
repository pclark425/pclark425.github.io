<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1397 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1397</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1397</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-518b827e340c26582b5093401283a4f5cff605b9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/518b827e340c26582b5093401283a4f5cff605b9" target="_blank">Learning Invariant Representations for Reinforcement Learning without Reconstruction</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work studies how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction, and proposes a method to learn robust latent representations which encode only the task-relevant information from observations.</p>
                <p><strong>Paper Abstract:</strong> We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1397.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1397.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Bisimulation for Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation-learning + RL method that trains an encoder so L1 distances in latent space equal a learned on-policy bisimulation metric; uses a probabilistic latent dynamics model (Gaussian) only to ground the bisimulation objective and combines the representation with SAC for control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DBC latent world model (probabilistic Gaussian dynamics used for training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder (convolutional trunk -> 50-dim tanh output) maps pixels to a latent Z where L1 distance is trained to match a bisimulation metric; a probabilistic dynamics model (MLP) predicts next-latent Gaussians (mean and covariance) given stop-gradient latent inputs and actions; training loss matches latent L1 distance to reward difference plus discounted W2 distance between predicted Gaussian transition distributions. The dynamics model is used only to provide the W2 term in the bisimulation loss (not used for planning in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (task-grounded, learned latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous control from pixels (DeepMind Control suite) and autonomous driving (CARLA)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No direct pixel reconstruction error; fidelity of the dynamics model is measured inside the bisimulation loss via 2-Wasserstein (W2) between predicted Gaussian next-state distributions; downstream fidelity assessed by RL task performance (reward / success metrics) and value-bound theorems linking latent distances to V*.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric W2 or prediction-MSE values reported; empirical fidelity is demonstrated via downstream task performance (DBC is SOTA on distractor-robust visual control benchmarks and achieves 46.8% higher final driving performance than the next best baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: latent space structure aligns with task-relevant state changes (t-SNE visualizations show neighboring latent points correspond to similar robot configurations / driving situations), but latent coordinates are neural-network features (not directly mapped to human-interpretable variables).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>t-SNE visualization of latent embeddings color-coded by predicted value; averaging images from nearby latent points to show preserved agent configuration and varying backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Encoder: convnet with ~50-dim output; dynamics and reward MLPs with two hidden layers of 200 units each; training hyperparameters provided; driving experiments reported to take ~12 hours on a single GTX 1080 GPU. Replay buffer size 1e6, batch size 128. No total-parameter-count or multi-GPU training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared empirically to baselines: more robust to background distractors and requires no reconstruction loss; authors report faster learning than Castro (2020) bisimulation-distance implementation and higher final performance than reconstruction/contrastive baselines; specific FLOPs or wall-clock comparisons beyond the single GPU 12h run are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On CARLA driving after 100k training steps DBC outperformed baselines: 'distance (m)' metric 24% for DBC vs 17% (DeepMDP) and 12% (SAC); crash intensity: DBC 2673 ± 38.5 vs DeepMDP 1958 ± 15.6 and SAC 4604 ± 30.7; DBC final driving performance reported as 46.8% better than the next best baseline. On DeepMind Control with natural video distractors DBC substantially outperforms reconstruction- and contrastive-based baselines and is robust where others fail (figures show SOTA curves but per-task numeric scores not tabulated in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Design emphasizes task-relevant features over full visual fidelity: by optimizing bisimulation distances rather than reconstruction/prediction MSE, the model discards task-irrelevant visual detail (clouds, backgrounds) and preserves features that affect current and future reward, yielding better downstream control despite not optimizing traditional prediction/reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Avoiding reconstruction reduces representational fidelity to irrelevant visual detail (benefit for control), but the dynamics model is trained only to support the representation (not used for planning), which may limit multi-step predictive uses; authors note potential tradeoffs in metric choices (L1 vs L2) and distributional assumptions (Gaussian) and suggest these as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent L1 distance is used to match bisimulation; latent dynamics output Gaussian distributions so W2 closed-form can be used; encoder architecture based on Yarats et al. (conv trunk + two extra conv layers) -> 50-dim tanh; dynamics/reward models are 2-layer MLPs (200 units each); stop-gradient applied when predicting transitions into latent; combined with SAC (value allowed to backprop into encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Versus reconstruction-based latent models (e.g., SLAC, DeepMDP with reconstruction) and contrastive methods, DBC is more robust to task-irrelevant visual distractors and generalizes better to new backgrounds and related reward functions; compared to Castro (2020) bisimulation-distance method, DBC learns a simpler representation (direct L1 distance between encodings) and learns faster in policy optimization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors argue an optimal model for visual control is one whose latent distances equal the bisimulation metric (task-grounded compression). They recommend: (1) train latent space to match bisimulation (L1), (2) use probabilistic latent dynamics for the W2 term (closed-form), (3) allow value gradients into encoder, and (4) consider ensembles for uncertainty and explicit memory for partial observability as extensions. No single numeric optimal architecture is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1397.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1397.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepMDP (Gelada et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-space method that jointly learns latent dynamics and rewards with reconstruction or auxiliary losses to make the latent MDP predictive; shown to relate to bisimulation by proving that L2 distances can upper-bound bisimulation distance under Lipschitz assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepMDP: Learning continuous latent space models for representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepMDP latent model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a latent representation together with latent transition and reward predictors; typically combined with reconstruction or auxiliary objectives so that L2 distances in latent space serve as proxies for bisimulation-like distances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (latent dynamics + reward predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari and control domains (cited/interfaced in paper as baseline on DeepMind Control and driving tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel MSE or likelihood) and L2 prediction errors in latent space; Gelada et al. prove L2 latent distance upper-bounds bisimulation under Lipschitz assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No per-task fidelity numbers reported in this paper; DeepMDP used as a competitive baseline that performs well on some tasks but is less robust than DBC to complex visual distractors in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent L2 distances have some theoretical grounding (upper bound on bisimulation), but latent dimensions are neural features; interpretability is limited and requires visualization to inspect.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper beyond empirical comparisons and mention of theoretical connection to bisimulation; t-SNE visualizations used by authors for DBC highlight differences vs. DeepMDP.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified here; DeepMDP requires training latent models and typically reconstruction losses which add decoder computation and training cost relative to non-reconstructive methods.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DeepMDP performs well in some settings but is less robust than DBC in presence of complex visual distractors; reconstruction requirement can increase training instability and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On driving and distractor-rich control tasks in this paper DeepMDP performed worse than DBC (e.g., driving distance metric 17% vs DBC 24%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>DeepMDP’s objective promotes faithful latent prediction/reconstruction which can capture task-irrelevant predictable features, hurting downstream control in environments with abundant irrelevant visual variation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Faithful predictive/reconstructive latent models can preserve irrelevant visual details (higher visual fidelity) at the expense of compressed task-relevant representation and robustness; DeepMDP needs reconstruction to scale in some settings, making it more compute-heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent-space MDP with learned transition and reward predictors; often paired with reconstruction/auxiliary losses; relies on Lipschitz assumptions in theoretical bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DBC, DeepMDP is less robust to background distractors; compared to reconstruction-only approaches it is conceptually closer to bisimulation but still relies on reconstruction in practice (per prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe an optimal configuration; discussion suggests that directly learning bisimulation-like latent distances (as in DBC) can be preferable when backgrounds contain task-irrelevant predictable signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1397.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1397.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SLAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Latent Actor-Critic (Lee et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art model-based/latent dynamics method that learns stochastic latent dynamics with reconstruction and uses actor-critic training in latent space for control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SLAC latent dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns stochastic latent variables with a reconstruction loss (sequential autoencoder-style) and uses a latent actor-critic controller operating on the learned latent states; aims for accurate multi-step predictive latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic sequential latent variable model with reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DeepMind Control suite and other pixel-control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel-level) and latent predictive likelihoods; multi-step prediction accuracy typically used.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In clean (no-distractor) settings SLAC performs best among compared methods in this paper; no absolute numeric prediction errors reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent variables are stochastic and learned for prediction; interpretability limited and not emphasized in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specifically used in this paper; SLAC typically relies on latent visualizations if interpretability is required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reconstruction and stochastic latent variables increase model complexity and training cost; SLAC was reported as state-of-the-art in no-distractor settings but is 'easily distracted' by complex backgrounds in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Per authors' experiments: SLAC performs best in default (clean) settings but degrades significantly with distractors, making it less robust than DBC in those regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Best performer on clean DeepMind Control tasks among evaluated baselines in this paper; however, performance drops with simple or natural-video distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High predictive fidelity (reconstruction) helps when observations are fully task-relevant, but capturing all predictable visual detail causes distraction when many predictable but task-irrelevant signals exist.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High reconstruction fidelity improves performance in clean domains but is detrimental in domains with rich task-irrelevant visual dynamics; computational overhead of reconstruction and sequential latent inference is larger than non-reconstructive alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Sequential stochastic latent variables with reconstruction objective; planning/control in latent space with actor-critic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DBC, SLAC is more accurate for fully-observed, low-distraction settings but less robust to distractors; compared to reconstruction baselines, SLAC is a strong representative but still suffers from distractibility.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies that for environments where most predictable features are task-relevant, SLAC-style reconstructive latent models are effective; for distractor-rich real-world vision, task-grounded representations (e.g., bisimulation-based) are preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1397.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1397.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reconstruction Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reconstruction-based latent dynamics / VAE-style baselines (authors' implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline implemented by the authors that trains an encoder/decoder and latent dynamics with a pixel reconstruction loss (decoder present), aiming to learn lossless or low-error image representations for control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reconstruction (VAE/autoencoder) latent model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder architecture where decoder reconstructs pixels from latent; latent dynamics often trained to predict next latent; used as baseline to contrast with non-reconstructive DBC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (reconstructive autoencoder + latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based continuous control (DMC) and driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel MSE / negative log-likelihood) and reconstruction visual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported quantitatively in this paper; empirically these methods perform worse on distractor-rich settings because they preserve task-irrelevant information required for reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent space may preserve many visual details; t-SNE visualizations in paper show VAE-like embeddings are not organized by task-relevant value structure (contrast with DBC).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>t-SNE of latent embeddings (authors compare VAE t-SNE to DBC t-SNE).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Decoder adds significant compute and can require more hyperparameter tuning; exact costs not enumerated here but reconstruction models typically increase training time and instability.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less sample- and distractor-efficient than DBC in experiments; performs well in no-distractor settings but degrades with complex backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Per experiments, reconstruction baseline performs poorly in distractor-rich settings, failing to achieve the robustness of DBC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High pixel-fidelity does not imply good control utility when many pixels are task-irrelevant; reconstruction forces the model to allocate capacity to irrelevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Better visual fidelity (reconstruction) vs worse task-specific compression and robustness; reconstruction increases compute and may hurt downstream control.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Full decoder + pixel MSE objective; latent dynamics encouraged via reconstruction and prediction losses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DBC, contrastive, and DeepMDP, reconstruction baselines are more likely to capture irrelevant details and perform worse on distractor tests.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not proposed; authors argue avoiding reconstruction is beneficial for robustness to task-irrelevant visual variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1397.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1397.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive predictive / CURL-style baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive self-supervised methods that learn representations by pulling together augmented or temporally related views and pushing apart unrelated samples; used here as a baseline for representation learning without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation learning with contrastive predictive coding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contrastive predictive latent model (CPC / CURL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder trained with a contrastive loss that enforces similarity between augmentations or temporally proximate samples; dynamics may be grounded by predicting future embeddings, but no decoder is used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent representation model (contrastive self-supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based control tasks (DMC) used as baseline in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive loss / InfoNCE objectives; downstream task performance used to evaluate utility (no explicit W2/MSE reported).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Per experiments, contrastive methods perform poorly in distractor-rich settings compared to DBC; quantitative task scores shown in figures but specific fidelity numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Learned features are not guaranteed to focus on task-relevant causal ancestors without explicit augmentations or supervision; interpretability limited unless visualized.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specifically employed in this paper beyond empirical performance comparison; contrastive baseline used with same encoder architecture for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generally lower than reconstruction approaches (no decoder), but contrastive losses require careful negative sampling / augmentations; exact costs not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Contrastive methods were less robust to background distractors than DBC in experiments; no strict compute/time comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performed poorly on complex images and driving tasks relative to DBC (figures indicate substantially lower reward/score curves).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Contrastive objectives capture predictable features but, absent task-specific grounding, can emphasize predictable but task-irrelevant signals and thus may not translate into good control.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Avoids decoder compute but may require engineering of augmentations to align representation with control relevance; when augmentations absent or insufficient, performance degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive loss (InfoNCE) with same encoder architecture as DBC for ablations / baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DBC, contrastive baselines are less robust to complex distractors; compared to reconstructive models they avoid pixel-decoding cost but still fail to prioritize causal, task-relevant features without further supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that incorporating downstream task information into similarity measures (instead of purely contrastive augmentations) would improve contrastive methods; DBC implements such task-grounding via bisimulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1397.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1397.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Castro-2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable methods for computing state similarity in deterministic Markov decision processes (P. S. Castro, 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm to compute on-policy bisimulation metrics (distances d between states) directly (focused on distances rather than learned representation embeddings); authors implement and compare a function-approximation variant of this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable methods for computing state similarity in deterministic Markov decision processes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Castro (2020) bisimulation-distance function approximation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implements d(s_i,s_j) = psi(phi(s_i), phi(s_j)) where phi encodes observations and psi computes a scalar distance; trained to match reward differences plus discounted distance between successor encodings with target networks (hat phi, hat psi). It targets direct computation of bisimulation distances rather than constraining latent L1 distances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>bisimulation distance estimator (function-approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Deterministic MDPs; used here as a comparative bisimulation-based method in control experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained to match bisimulation objective (reward difference + discounted coupling of successor distributions); fidelity assessed by how well learned d matches on-policy bisimulation and by downstream control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Authors report that Castro's method can learn control surprisingly well but DBC learns faster; no numeric fidelity metrics reported beyond comparative RL curves.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>The explicitly-learned distance function psi over encoded states is somewhat interpretable as a learned metric, but the representation and psi network remain neural black boxes; less direct visualization of an embedding space compared to DBC's L1 latent embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Authors compare performance and note that Castro-style psi+phi can be used with their policy; no dedicated visualization method beyond performance curves.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Involves training an extra network psi and target networks (hat phi, hat psi); computational overhead higher than DBC's direct L1-latent-distance approach.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DBC is simpler (uses direct L1 distance) and empirically learns faster than Castro's function-approximation bisimulation in policy optimization experiments presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Can learn control in experiments but slower than DBC; no per-task numeric comparisons aside from plots showing DBC faster learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Directly learning a distance function can capture bisimulation structure, but the added complexity of a separate psi network and target networks makes the approach less streamlined for joint policy optimization compared to DBC where L1 latent distances are enforced directly.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Castro's method targets distance estimation directly (potentially higher fidelity to the bisimulation metric) at the cost of additional networks and training complexity; DBC trades this off for simpler encoder-only distance enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Separate psi network for distances, target networks for stability; function-approximation of bisimulation distances rather than forcing latent norm equality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DBC, Castro (2020) is more direct about estimating d but more complex to integrate; DBC's simpler loss (latent L1 ~ bisimulation) produces faster learning in the tested policy optimization setting.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that constraining latent L1 distances (DBC) is a simpler and effective practical configuration for learning bisimulation-like representations in policy optimization settings, though Castro's distance estimator remains a viable alternative particularly in settings focused solely on metric estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepMDP: Learning continuous latent space models for representation learning <em>(Rating: 2)</em></li>
                <li>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Scalable methods for computing state similarity in deterministic Markov decision processes. <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1397",
    "paper_id": "paper-518b827e340c26582b5093401283a4f5cff605b9",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DBC",
            "name_full": "Deep Bisimulation for Control",
            "brief_description": "A representation-learning + RL method that trains an encoder so L1 distances in latent space equal a learned on-policy bisimulation metric; uses a probabilistic latent dynamics model (Gaussian) only to ground the bisimulation objective and combines the representation with SAC for control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DBC latent world model (probabilistic Gaussian dynamics used for training)",
            "model_description": "Encoder (convolutional trunk -&gt; 50-dim tanh output) maps pixels to a latent Z where L1 distance is trained to match a bisimulation metric; a probabilistic dynamics model (MLP) predicts next-latent Gaussians (mean and covariance) given stop-gradient latent inputs and actions; training loss matches latent L1 distance to reward difference plus discounted W2 distance between predicted Gaussian transition distributions. The dynamics model is used only to provide the W2 term in the bisimulation loss (not used for planning in experiments).",
            "model_type": "latent world model (task-grounded, learned latent dynamics)",
            "task_domain": "continuous control from pixels (DeepMind Control suite) and autonomous driving (CARLA)",
            "fidelity_metric": "No direct pixel reconstruction error; fidelity of the dynamics model is measured inside the bisimulation loss via 2-Wasserstein (W2) between predicted Gaussian next-state distributions; downstream fidelity assessed by RL task performance (reward / success metrics) and value-bound theorems linking latent distances to V*.",
            "fidelity_performance": "No explicit numeric W2 or prediction-MSE values reported; empirical fidelity is demonstrated via downstream task performance (DBC is SOTA on distractor-robust visual control benchmarks and achieves 46.8% higher final driving performance than the next best baseline).",
            "interpretability_assessment": "Moderately interpretable: latent space structure aligns with task-relevant state changes (t-SNE visualizations show neighboring latent points correspond to similar robot configurations / driving situations), but latent coordinates are neural-network features (not directly mapped to human-interpretable variables).",
            "interpretability_method": "t-SNE visualization of latent embeddings color-coded by predicted value; averaging images from nearby latent points to show preserved agent configuration and varying backgrounds.",
            "computational_cost": "Encoder: convnet with ~50-dim output; dynamics and reward MLPs with two hidden layers of 200 units each; training hyperparameters provided; driving experiments reported to take ~12 hours on a single GTX 1080 GPU. Replay buffer size 1e6, batch size 128. No total-parameter-count or multi-GPU training reported.",
            "efficiency_comparison": "Compared empirically to baselines: more robust to background distractors and requires no reconstruction loss; authors report faster learning than Castro (2020) bisimulation-distance implementation and higher final performance than reconstruction/contrastive baselines; specific FLOPs or wall-clock comparisons beyond the single GPU 12h run are not given.",
            "task_performance": "On CARLA driving after 100k training steps DBC outperformed baselines: 'distance (m)' metric 24% for DBC vs 17% (DeepMDP) and 12% (SAC); crash intensity: DBC 2673 ± 38.5 vs DeepMDP 1958 ± 15.6 and SAC 4604 ± 30.7; DBC final driving performance reported as 46.8% better than the next best baseline. On DeepMind Control with natural video distractors DBC substantially outperforms reconstruction- and contrastive-based baselines and is robust where others fail (figures show SOTA curves but per-task numeric scores not tabulated in-text).",
            "task_utility_analysis": "Design emphasizes task-relevant features over full visual fidelity: by optimizing bisimulation distances rather than reconstruction/prediction MSE, the model discards task-irrelevant visual detail (clouds, backgrounds) and preserves features that affect current and future reward, yielding better downstream control despite not optimizing traditional prediction/reconstruction metrics.",
            "tradeoffs_observed": "Avoiding reconstruction reduces representational fidelity to irrelevant visual detail (benefit for control), but the dynamics model is trained only to support the representation (not used for planning), which may limit multi-step predictive uses; authors note potential tradeoffs in metric choices (L1 vs L2) and distributional assumptions (Gaussian) and suggest these as future work.",
            "design_choices": "Latent L1 distance is used to match bisimulation; latent dynamics output Gaussian distributions so W2 closed-form can be used; encoder architecture based on Yarats et al. (conv trunk + two extra conv layers) -&gt; 50-dim tanh; dynamics/reward models are 2-layer MLPs (200 units each); stop-gradient applied when predicting transitions into latent; combined with SAC (value allowed to backprop into encoder).",
            "comparison_to_alternatives": "Versus reconstruction-based latent models (e.g., SLAC, DeepMDP with reconstruction) and contrastive methods, DBC is more robust to task-irrelevant visual distractors and generalizes better to new backgrounds and related reward functions; compared to Castro (2020) bisimulation-distance method, DBC learns a simpler representation (direct L1 distance between encodings) and learns faster in policy optimization experiments.",
            "optimal_configuration": "Authors argue an optimal model for visual control is one whose latent distances equal the bisimulation metric (task-grounded compression). They recommend: (1) train latent space to match bisimulation (L1), (2) use probabilistic latent dynamics for the W2 term (closed-form), (3) allow value gradients into encoder, and (4) consider ensembles for uncertainty and explicit memory for partial observability as extensions. No single numeric optimal architecture is prescribed.",
            "uuid": "e1397.0",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "DeepMDP",
            "name_full": "DeepMDP (Gelada et al., 2019)",
            "brief_description": "A latent-space method that jointly learns latent dynamics and rewards with reconstruction or auxiliary losses to make the latent MDP predictive; shown to relate to bisimulation by proving that L2 distances can upper-bound bisimulation distance under Lipschitz assumptions.",
            "citation_title": "DeepMDP: Learning continuous latent space models for representation learning",
            "mention_or_use": "use",
            "model_name": "DeepMDP latent model",
            "model_description": "Learns a latent representation together with latent transition and reward predictors; typically combined with reconstruction or auxiliary objectives so that L2 distances in latent space serve as proxies for bisimulation-like distances.",
            "model_type": "latent world model (latent dynamics + reward predictor)",
            "task_domain": "Atari and control domains (cited/interfaced in paper as baseline on DeepMind Control and driving tasks)",
            "fidelity_metric": "Reconstruction loss (pixel MSE or likelihood) and L2 prediction errors in latent space; Gelada et al. prove L2 latent distance upper-bounds bisimulation under Lipschitz assumptions.",
            "fidelity_performance": "No per-task fidelity numbers reported in this paper; DeepMDP used as a competitive baseline that performs well on some tasks but is less robust than DBC to complex visual distractors in the reported experiments.",
            "interpretability_assessment": "Latent L2 distances have some theoretical grounding (upper bound on bisimulation), but latent dimensions are neural features; interpretability is limited and requires visualization to inspect.",
            "interpretability_method": "Not detailed in this paper beyond empirical comparisons and mention of theoretical connection to bisimulation; t-SNE visualizations used by authors for DBC highlight differences vs. DeepMDP.",
            "computational_cost": "Not quantified here; DeepMDP requires training latent models and typically reconstruction losses which add decoder computation and training cost relative to non-reconstructive methods.",
            "efficiency_comparison": "DeepMDP performs well in some settings but is less robust than DBC in presence of complex visual distractors; reconstruction requirement can increase training instability and compute.",
            "task_performance": "On driving and distractor-rich control tasks in this paper DeepMDP performed worse than DBC (e.g., driving distance metric 17% vs DBC 24%).",
            "task_utility_analysis": "DeepMDP’s objective promotes faithful latent prediction/reconstruction which can capture task-irrelevant predictable features, hurting downstream control in environments with abundant irrelevant visual variation.",
            "tradeoffs_observed": "Faithful predictive/reconstructive latent models can preserve irrelevant visual details (higher visual fidelity) at the expense of compressed task-relevant representation and robustness; DeepMDP needs reconstruction to scale in some settings, making it more compute-heavy.",
            "design_choices": "Latent-space MDP with learned transition and reward predictors; often paired with reconstruction/auxiliary losses; relies on Lipschitz assumptions in theoretical bounds.",
            "comparison_to_alternatives": "Compared to DBC, DeepMDP is less robust to background distractors; compared to reconstruction-only approaches it is conceptually closer to bisimulation but still relies on reconstruction in practice (per prior work).",
            "optimal_configuration": "Paper does not prescribe an optimal configuration; discussion suggests that directly learning bisimulation-like latent distances (as in DBC) can be preferable when backgrounds contain task-irrelevant predictable signals.",
            "uuid": "e1397.1",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "SLAC",
            "name_full": "Stochastic Latent Actor-Critic (Lee et al., 2020)",
            "brief_description": "A state-of-the-art model-based/latent dynamics method that learns stochastic latent dynamics with reconstruction and uses actor-critic training in latent space for control from pixels.",
            "citation_title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "mention_or_use": "use",
            "model_name": "SLAC latent dynamics model",
            "model_description": "Learns stochastic latent variables with a reconstruction loss (sequential autoencoder-style) and uses a latent actor-critic controller operating on the learned latent states; aims for accurate multi-step predictive latent dynamics.",
            "model_type": "latent world model (stochastic sequential latent variable model with reconstruction)",
            "task_domain": "DeepMind Control suite and other pixel-control tasks",
            "fidelity_metric": "Reconstruction loss (pixel-level) and latent predictive likelihoods; multi-step prediction accuracy typically used.",
            "fidelity_performance": "In clean (no-distractor) settings SLAC performs best among compared methods in this paper; no absolute numeric prediction errors reported here.",
            "interpretability_assessment": "Latent variables are stochastic and learned for prediction; interpretability limited and not emphasized in paper.",
            "interpretability_method": "Not specifically used in this paper; SLAC typically relies on latent visualizations if interpretability is required.",
            "computational_cost": "Reconstruction and stochastic latent variables increase model complexity and training cost; SLAC was reported as state-of-the-art in no-distractor settings but is 'easily distracted' by complex backgrounds in experiments here.",
            "efficiency_comparison": "Per authors' experiments: SLAC performs best in default (clean) settings but degrades significantly with distractors, making it less robust than DBC in those regimes.",
            "task_performance": "Best performer on clean DeepMind Control tasks among evaluated baselines in this paper; however, performance drops with simple or natural-video distractors.",
            "task_utility_analysis": "High predictive fidelity (reconstruction) helps when observations are fully task-relevant, but capturing all predictable visual detail causes distraction when many predictable but task-irrelevant signals exist.",
            "tradeoffs_observed": "High reconstruction fidelity improves performance in clean domains but is detrimental in domains with rich task-irrelevant visual dynamics; computational overhead of reconstruction and sequential latent inference is larger than non-reconstructive alternatives.",
            "design_choices": "Sequential stochastic latent variables with reconstruction objective; planning/control in latent space with actor-critic.",
            "comparison_to_alternatives": "Compared to DBC, SLAC is more accurate for fully-observed, low-distraction settings but less robust to distractors; compared to reconstruction baselines, SLAC is a strong representative but still suffers from distractibility.",
            "optimal_configuration": "Paper implies that for environments where most predictable features are task-relevant, SLAC-style reconstructive latent models are effective; for distractor-rich real-world vision, task-grounded representations (e.g., bisimulation-based) are preferable.",
            "uuid": "e1397.2",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Reconstruction Baseline",
            "name_full": "Reconstruction-based latent dynamics / VAE-style baselines (authors' implementation)",
            "brief_description": "A baseline implemented by the authors that trains an encoder/decoder and latent dynamics with a pixel reconstruction loss (decoder present), aiming to learn lossless or low-error image representations for control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reconstruction (VAE/autoencoder) latent model",
            "model_description": "Encoder-decoder architecture where decoder reconstructs pixels from latent; latent dynamics often trained to predict next latent; used as baseline to contrast with non-reconstructive DBC.",
            "model_type": "latent world model (reconstructive autoencoder + latent dynamics)",
            "task_domain": "Pixel-based continuous control (DMC) and driving",
            "fidelity_metric": "Reconstruction loss (pixel MSE / negative log-likelihood) and reconstruction visual quality.",
            "fidelity_performance": "Not reported quantitatively in this paper; empirically these methods perform worse on distractor-rich settings because they preserve task-irrelevant information required for reconstruction.",
            "interpretability_assessment": "Latent space may preserve many visual details; t-SNE visualizations in paper show VAE-like embeddings are not organized by task-relevant value structure (contrast with DBC).",
            "interpretability_method": "t-SNE of latent embeddings (authors compare VAE t-SNE to DBC t-SNE).",
            "computational_cost": "Decoder adds significant compute and can require more hyperparameter tuning; exact costs not enumerated here but reconstruction models typically increase training time and instability.",
            "efficiency_comparison": "Less sample- and distractor-efficient than DBC in experiments; performs well in no-distractor settings but degrades with complex backgrounds.",
            "task_performance": "Per experiments, reconstruction baseline performs poorly in distractor-rich settings, failing to achieve the robustness of DBC.",
            "task_utility_analysis": "High pixel-fidelity does not imply good control utility when many pixels are task-irrelevant; reconstruction forces the model to allocate capacity to irrelevant signals.",
            "tradeoffs_observed": "Better visual fidelity (reconstruction) vs worse task-specific compression and robustness; reconstruction increases compute and may hurt downstream control.",
            "design_choices": "Full decoder + pixel MSE objective; latent dynamics encouraged via reconstruction and prediction losses.",
            "comparison_to_alternatives": "Compared to DBC, contrastive, and DeepMDP, reconstruction baselines are more likely to capture irrelevant details and perform worse on distractor tests.",
            "optimal_configuration": "Not proposed; authors argue avoiding reconstruction is beneficial for robustness to task-irrelevant visual variation.",
            "uuid": "e1397.3",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Contrastive",
            "name_full": "Contrastive predictive / CURL-style baselines",
            "brief_description": "Contrastive self-supervised methods that learn representations by pulling together augmented or temporally related views and pushing apart unrelated samples; used here as a baseline for representation learning without reconstruction.",
            "citation_title": "Representation learning with contrastive predictive coding",
            "mention_or_use": "use",
            "model_name": "Contrastive predictive latent model (CPC / CURL)",
            "model_description": "Encoder trained with a contrastive loss that enforces similarity between augmentations or temporally proximate samples; dynamics may be grounded by predicting future embeddings, but no decoder is used.",
            "model_type": "latent representation model (contrastive self-supervision)",
            "task_domain": "Pixel-based control tasks (DMC) used as baseline in experiments",
            "fidelity_metric": "Contrastive loss / InfoNCE objectives; downstream task performance used to evaluate utility (no explicit W2/MSE reported).",
            "fidelity_performance": "Per experiments, contrastive methods perform poorly in distractor-rich settings compared to DBC; quantitative task scores shown in figures but specific fidelity numbers not reported.",
            "interpretability_assessment": "Learned features are not guaranteed to focus on task-relevant causal ancestors without explicit augmentations or supervision; interpretability limited unless visualized.",
            "interpretability_method": "Not specifically employed in this paper beyond empirical performance comparison; contrastive baseline used with same encoder architecture for fairness.",
            "computational_cost": "Generally lower than reconstruction approaches (no decoder), but contrastive losses require careful negative sampling / augmentations; exact costs not enumerated here.",
            "efficiency_comparison": "Contrastive methods were less robust to background distractors than DBC in experiments; no strict compute/time comparison provided.",
            "task_performance": "Performed poorly on complex images and driving tasks relative to DBC (figures indicate substantially lower reward/score curves).",
            "task_utility_analysis": "Contrastive objectives capture predictable features but, absent task-specific grounding, can emphasize predictable but task-irrelevant signals and thus may not translate into good control.",
            "tradeoffs_observed": "Avoids decoder compute but may require engineering of augmentations to align representation with control relevance; when augmentations absent or insufficient, performance degrades.",
            "design_choices": "Contrastive loss (InfoNCE) with same encoder architecture as DBC for ablations / baselines.",
            "comparison_to_alternatives": "Compared to DBC, contrastive baselines are less robust to complex distractors; compared to reconstructive models they avoid pixel-decoding cost but still fail to prioritize causal, task-relevant features without further supervision.",
            "optimal_configuration": "Paper suggests that incorporating downstream task information into similarity measures (instead of purely contrastive augmentations) would improve contrastive methods; DBC implements such task-grounding via bisimulation.",
            "uuid": "e1397.4",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Castro-2020",
            "name_full": "Scalable methods for computing state similarity in deterministic Markov decision processes (P. S. Castro, 2020)",
            "brief_description": "An algorithm to compute on-policy bisimulation metrics (distances d between states) directly (focused on distances rather than learned representation embeddings); authors implement and compare a function-approximation variant of this approach.",
            "citation_title": "Scalable methods for computing state similarity in deterministic Markov decision processes.",
            "mention_or_use": "use",
            "model_name": "Castro (2020) bisimulation-distance function approximation",
            "model_description": "Implements d(s_i,s_j) = psi(phi(s_i), phi(s_j)) where phi encodes observations and psi computes a scalar distance; trained to match reward differences plus discounted distance between successor encodings with target networks (hat phi, hat psi). It targets direct computation of bisimulation distances rather than constraining latent L1 distances.",
            "model_type": "bisimulation distance estimator (function-approximation)",
            "task_domain": "Deterministic MDPs; used here as a comparative bisimulation-based method in control experiments",
            "fidelity_metric": "Trained to match bisimulation objective (reward difference + discounted coupling of successor distributions); fidelity assessed by how well learned d matches on-policy bisimulation and by downstream control performance.",
            "fidelity_performance": "Authors report that Castro's method can learn control surprisingly well but DBC learns faster; no numeric fidelity metrics reported beyond comparative RL curves.",
            "interpretability_assessment": "The explicitly-learned distance function psi over encoded states is somewhat interpretable as a learned metric, but the representation and psi network remain neural black boxes; less direct visualization of an embedding space compared to DBC's L1 latent embedding.",
            "interpretability_method": "Authors compare performance and note that Castro-style psi+phi can be used with their policy; no dedicated visualization method beyond performance curves.",
            "computational_cost": "Involves training an extra network psi and target networks (hat phi, hat psi); computational overhead higher than DBC's direct L1-latent-distance approach.",
            "efficiency_comparison": "DBC is simpler (uses direct L1 distance) and empirically learns faster than Castro's function-approximation bisimulation in policy optimization experiments presented here.",
            "task_performance": "Can learn control in experiments but slower than DBC; no per-task numeric comparisons aside from plots showing DBC faster learning.",
            "task_utility_analysis": "Directly learning a distance function can capture bisimulation structure, but the added complexity of a separate psi network and target networks makes the approach less streamlined for joint policy optimization compared to DBC where L1 latent distances are enforced directly.",
            "tradeoffs_observed": "Castro's method targets distance estimation directly (potentially higher fidelity to the bisimulation metric) at the cost of additional networks and training complexity; DBC trades this off for simpler encoder-only distance enforcement.",
            "design_choices": "Separate psi network for distances, target networks for stability; function-approximation of bisimulation distances rather than forcing latent norm equality.",
            "comparison_to_alternatives": "Compared to DBC, Castro (2020) is more direct about estimating d but more complex to integrate; DBC's simpler loss (latent L1 ~ bisimulation) produces faster learning in the tested policy optimization setting.",
            "optimal_configuration": "Paper suggests that constraining latent L1 distances (DBC) is a simpler and effective practical configuration for learning bisimulation-like representations in policy optimization settings, though Castro's distance estimator remains a viable alternative particularly in settings focused solely on metric estimation.",
            "uuid": "e1397.5",
            "source_info": {
                "paper_title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepMDP: Learning continuous latent space models for representation learning",
            "rating": 2
        },
        {
            "paper_title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Scalable methods for computing state similarity in deterministic Markov decision processes.",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 1
        }
    ],
    "cost": 0.017115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LEARNING INVARIANT REPRESENTATIONS FOR REINFORCEMENT LEARNING WITHOUT RECONSTRUCTION</h1>
<p>Amy Zhang ${ }^{<em> 12}$ Rowan McAllister ${ }^{</em> 3}$ Roberto Calandra ${ }^{2}$ Yarin Gal ${ }^{4}$ Sergey Levine ${ }^{3}$<br>${ }^{1}$ McGill University<br>${ }^{2}$ Facebook AI Research<br>${ }^{3}$ University of California, Berkeley<br>${ }^{4}$ OATML group, University of Oxford</p>
<h4>Abstract</h4>
<p>We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.</p>
<h2>1 Introduction</h2>
<p>Learning control from images is important for many real world applications. While deep reinforcement learning (RL) has enjoyed many successes in simulated tasks, learning control from real vision is more complex, especially outdoors, where images reveal detailed scenes of a complex and unstructured world. Furthermore, while many RL algorithms can eventually learn control from real images given unlimited data, data-efficiency is often a necessity in real trials which are expensive and constrained to real-time. Prior methods for data-efficient learning of simulated visual tasks typically use representation learning. Representation learning summarizes images by encoding them into smaller vectored representations better suited for RL. For example, sequential autoencoders aim to learn lossless representations of streaming observations-sufficient to reconstruct current observations and predict future observations-from which various RL algorithms can be trained (Hafner et al., 2019; Lee et al., 2020; Yarats et al., 2021). However, such methods are taskagnostic: the models represent all dynamic elements they observe in the world, whether they are relevant to the task or not. We argue such representations can easily "distract" RL algorithms with irrelevant information in the case of real images. The issues of distraction is less evident in popular simulation MuJoCo and Atari tasks, since any change in observation space is likely task-relevant, and thus, worth representing. By contrast, visual images that autonomous cars observe contain predominately task-irrelevant information, like cloud shapes and architectural details, illustrated in Figure 1.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Rather than learning control-agnostic representations that focus on accurate reconstruction of clouds and buildings, we would rather achieve a more compressed representation from a lossy encoder, which only retains state information relevant to our task. If we would like to learn representations that capture only task-relevant elements of the state and are invariant to task-irrelevant information, intuitively we can utilize the reward signal to help determine task-relevance, as shown by [Jonschkowski \&amp; Brock (2015)]. As cumulative rewards are our objective, state elements are relevant not only if they influence the current reward, but also if they influence state elements in the future that in turn influence future rewards. This recursive relationship can be distilled into a recursive task-aware notion of state abstraction: an ideal representation is one that is predictive of reward, and also predictive of itself in the future.</p>
<p>We propose learning such an invariant representation using the bisimulation metric, where the distance between two observation encodings correspond to how "behaviourally different" ( [Ferns \&amp; Precup, 2014]) both observations are. Our main contribution is a practical representation learning method based on the bisimulation metric suitable for downstream control, which we call deep bisimulation for control (DBC). We additionally provide theoretical analysis that proves value bounds between the optimal value function of the true MDP and the optimal value function of the MDP constructed by the learned representation. Empirical evaluations demonstrate our nonreconstructive approach using bisimulation is substantially more robust to task-irrelevant distractors when compared to prior approaches that use reconstruction losses or contrastive losses. Our initial experiments insert natural videos into the background of MoJoCo control task as complex distraction. Our second setup is a high-fidelity highway driving task using CARLA ( [Dosovitskiy et al., 2017]), showing that our representations can be trained effectively even on highly realistic images with many distractions, such as trees, clouds, buildings, and shadows. For example videos see https://sites.google.com/view/deepbisim4control. Code is available at https://github.com/facebookresearch/deep_bisim4control.</p>
<h1>2 Related Work</h1>
<p>Our work builds on the extensive prior research on bisimulation in MDP state aggregation.
Reconstruction-based Representations. Early works on deep reinforcement learning from images ( [Lange \&amp; Riedmiller, 2010; Lange et al., 2012]) used a two-step learning process where first an auto-encoder was trained using reconstruction loss to learn a low-dimensional representation, and subsequently a controller was learned using this representation. This allows effective leveraging of large, unlabeled datasets for learning representations for control. In practice, there is no guarantee that the learned representation will capture useful information for the control task, and significant expert knowledge and tricks are often necessary for these approaches to work. In model-based RL, one solution to this problem has been to jointly train the encoder and the dynamics model end-to-end ( [Watter et al., 2015; Wahlström et al., 2015]) - this proved effective in learning useful task-oriented representations. <em>Hafner et al. (2019)</em> and <em>Lee et al. (2020)</em> learn latent state models using a reconstruction loss, but these approaches suffer from the difficulty of learning accurate long-term predictions and often still require significant manual tuning. <em>Gelada et al. (2019)</em> also propose a latent dynamics model-based method and connect their approach to bisimulation metrics, using a reconstruction loss in Atari. They show that $\ell_{2}$ distance in the DeepMDP representation upper bounds the bisimulation distance, whereas our objective directly learns a representation where distance in latent space is the bisimulation metric. Further, their results rely on the assumption that the learned representation is Lipschitz, whereas we show that, by directly learning a bisimilarity-based representation, we guarantee a representation that generates a Lipschitz MDP. We show experimentally that our non-reconstructive DBC method is substantially more robust to complex distractors.</p>
<p>Contrastive-based Representations. Contrastive losses are a self-supervised approach to learn useful representations by enforcing similarity constraints between data ( [van den Oord et al., 2018; Chen et al., 2020]). Similarity functions can be provided as domain knowledge in the form of heuristic data augmentation, where we maximize similarity between augmentations of the same data point ( [Laskin et al., 2020]) or nearby image patches ( [Hénaff et al., 2020]), and minimize similarity between different data points. In the absence of this domain knowledge, contrastive representations can be trained by predicting the future ( [van den Oord et al., 2018]). We compare to such an approach in our experiments, and show that DBC is substantially more robust. While contrastive losses do not require reconstruction, they do not inherently have a mechanism to determine downstream task</p>
<p>relevance without manual engineering, and when trained only for prediction, they aim to capture all predictable features in the observation, which performs poorly on real images for the same reasons world models do. A better method would be to incorporate knowledge of the downstream task into the similarity function in a data-driven way, so that images that are very different pixel-wise (e.g. lighting or texture changes), can also be grouped as similar w.r.t. downstream objectives.</p>
<p>Bisimulation. Various forms of state abstractions have been defined in Markov decision processes (MDPs) to group states into clusters whilst preserving some property (e.g. the optimal value, or all values, or all action values from each state) (Li et al., 2006). The strictest form, which generally preserves the most properties, is bisimulation (Larsen \&amp; Skou, 1989). Bisimulation only groups states that are indistinguishable w.r.t. reward sequences output given any action sequence tested. A related concept is bisimulation metrics (Ferns \&amp; Precup, 2014), which measure how "behaviorally similar" states are. Ferns et al. (2011) defines the bisimulation metric with respect to continuous MDPs, and propose a Monte Carlo algorithm for learning it using an exact computation of the Wasserstein distance between empirically measured transition distributions. However, this method does not scale well to large state spaces. Taylor et al. (2009) relate MDP homomorphisms to lax probabilistic bisimulation, and define a lax bisimulation metric. They then compute a value bound based on this metric for MDP homomorphisms, where approximately equivalent state-action pairs are aggregated. Most recently, Castro (2020) propose an algorithm for computing on-policy bisimulation metrics, but does so directly, without learning a representation. They focus on deterministic settings and the policy evaluation problem. We believe our work is the first to propose a gradient-based method for directly learning a representation space with the properties of bisimulation metrics and show that it works in the policy optimization setting.</p>
<h1>3 Preliminaries</h1>
<p>We start by introducing notation and outlining realistic assumptions about underlying structure in the environment. Then, we review state abstractions and metrics for state similarity.
We assume the underlying environment is a Markov decision process (MDP), described by the tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ the action space, $\mathcal{P}\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)$ the probability of transitioning from state $\mathbf{s} \in \mathcal{S}$ to state $\mathbf{s}^{\prime} \in \mathcal{S}$, and $\gamma \in[0,1)$ a discount factor. An "agent" chooses actions $\mathbf{a} \in \mathcal{A}$ according to a policy function $\mathbf{a} \sim \pi(\mathbf{s})$, which updates the system state $\mathbf{s}^{\prime} \sim \mathcal{P}(\mathbf{s}, \mathbf{a})$, yielding a reward $r=\mathcal{R}(\mathbf{s}) \in \mathbb{R}$. The agent's goal is to maximize the expected cumulative discounted rewards by learning a good policy: $\max <em _mathcal_P="\mathcal{P">{\pi} \mathbb{E}</em>}}\left[\sum_{t=0}^{\infty}\left|\gamma^{t} \mathcal{R}\left(\mathbf{s<em i="i">{t}\right)\right]\right.$. While our primary concern is learning from images, we do not address the partial-observability problem explicitly: we instead approximate stacked pixel observations as the fully-observed system state $\mathbf{s}$ (explained further in Appendix B).
Bisimulation is a form of state abstraction that groups states $\mathbf{s}</em>}$ and $\mathbf{s<em 0:="0:" _infty="\infty">{j}$ that are "behaviorally equivalent" (Li et al., 2006). For any action sequence $\mathbf{a}</em>}$, the probabilistic sequence of rewards from $\mathbf{s<em j="j">{i}$ and $\mathbf{s}</em>$ are identical. A more compact definition has a recursive form: two states are bisimilar if they share both the same immediate reward and equivalent distributions over the next bisimilar states (Larsen \&amp; Skou, 1989; Givan et al., 2003).
Definition 1 (Bisimulation Relations (Givan et al., 2003)). Given an MDP $\mathcal{M}$, an equivalence relation $B$ between states is a bisimulation relation if, for all states $\mathbf{s}<em j="j">{i}, \mathbf{s}</em>} \in \mathcal{S}$ that are equivalent under $B$ (denoted $\mathbf{s<em B="B">{i} \equiv</em>$ ) the following conditions hold:} \mathbf{s}_{j</p>
<p>$$
\begin{aligned}
\mathcal{R}\left(\mathbf{s}<em j="j">{i}, \mathbf{a}\right) &amp; =\mathcal{R}\left(\mathbf{s}</em> \
\mathcal{P}\left(G \mid \mathbf{s}}, \mathbf{a}\right) \quad \forall \mathbf{a} \in \mathcal{A<em j="j">{i}, \mathbf{a}\right) &amp; =\mathcal{P}\left(G \mid \mathbf{s}</em>
\end{aligned}
$$}, \mathbf{a}\right) \quad \forall \mathbf{a} \in \mathcal{A}, \quad \forall G \in \mathcal{S}_{B</p>
<p>where $\mathcal{S}<em _mathbf_s="\mathbf{s">{B}$ is the partition of $\mathcal{S}$ under the relation $B$ (the set of all groups $G$ of equivalent states), and $\mathcal{P}(G \mid \mathbf{s}, \mathbf{a})=\sum</em>\right)$.
Exact partitioning with bisimulation relations is generally impractical in continuous state spaces, as the relation is highly sensitive to infinitesimal changes in the reward function or dynamics. For this reason, Bisimulation Metrics (Ferns et al., 2011; Ferns \&amp; Precup, 2014; Castro, 2020) softens the concept of state partitions, and instead defines a pseudometric space $(\mathcal{S}, d)$, where a distance function $d: \mathcal{S} \times \mathcal{S} \mapsto \mathbb{R}_{\geq 0}$ measures the "behavioral similarity" between two states ${ }^{1}$.}^{\prime} \in G} \mathcal{P}\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a</p>
<p>Defining a distance $d$ between states requires defining both a distance between rewards (to soften Equation (1)), and distance between state distributions (to soften Equation (2)). Prior works use the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Wasserstein metric for the latter, originally used in the context of bisimulation metrics by <em>van Breugel &amp; Worrell (2001)</em>. The $p^{\text{th}}$ Wasserstein metric is defined between two probability distributions $\mathcal{P}<em j="j">{i}$ and $\mathcal{P}</em>}$ as $W_{p}(\mathcal{P<em j="j">{i},\mathcal{P}</em>} ; d)=\left(\inf_{\gamma^{\prime}\in\Gamma(\mathcal{P<em j="j">{i},\mathcal{P}</em>})}\int_{\mathcal{S}\times\mathcal{S}}d(\mathbf{s<em j="j">{i},\mathbf{s}</em>})^{c}\operatorname{dr}\left({}^{\prime}(\mathbf{s<em j="j">{i},\mathbf{s}</em>})\right)^{1/c}\right.$, where $\Gamma(\mathcal{P<em j="j">{i},\mathcal{P}</em>})$ is the set of all couplings of $\mathcal{P<em j="j">{i}$ and $\mathcal{P}</em>$. This is known as the “earth mover” distance, denoting the cost of transporting mass from one distribution to another <em>(Villani, 2003)</em>. Finally, the bisimulation metric is the reward difference added to the Wasserstein distance between transition distributions:</p>
<p>Definition 2 (Bisimulation Metric). From Theorem 2.6 in <em>Ferns et al. (2011)</em> with $c\in[0,1)$:</p>
<p>$d(\mathbf{s}<em j="j">{i},\mathbf{s}</em>})=\max_{\mathbf{a}\in\mathcal{A}}(1-c)\cdot|\mathcal{R<em i="i">{\mathbf{s}</em>}}^{\mathbf{a}}-\mathcal{R<em j="j">{\mathbf{s}</em>}}^{\mathbf{a}}|+c\cdot W_{1}(\mathcal{P<em i="i">{\mathbf{s}</em>}}^{\mathbf{a}},{\mathcal{P<em j="j">{\mathbf{s}</em>;d).$ (3)}}^{\mathbf{a}}</p>
<h2>4 Learning Representations for Control with Bisimulation Metrics</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Learning a bisimulation metric representation: shaded in blue is the main model architecture, it is reused for both states, like a Siamese network. The loss is the reward and discounted transition distribution distances (using Wasserstein metric $W$).</p>
<p>We propose Deep Bisimulation for Control (DBC), a data-efficient approach to learn control policies from unstructured, high-dimensional states. In contrast to prior work on bisimulation, which typically aims to learn a distance function of the form $d: \mathcal{S} \times \mathcal{S} \mapsto \mathbb{R}<em 1="1">{\geq 0}$ between states, our aim is instead to learn <em>representations</em> $\mathcal{Z}$ under which $\ell</em>$ that capture representations of states that are suitable to control, while discarding any information that is }$ distances correspond to bisimulation metrics, and then use these representations to improve reinforcement learning. Our goal is to learn encoders $\phi: \mathcal{S} \mapsto \mathcal{Z<em>irrelevant</em> for control. Any representation that relies on reconstruction of the state cannot do this, as these irrelevant details are still important for reconstruction. We hypothesize that bisimulation metrics can acquire this type of representation, without any reconstruction.</p>
<p>Bisimulation metrics are a useful form of state abstraction, but prior methods to train distance functions either do not scale to pixel observations <em>(Ferns et al., 2011)</em> (due to the max operator in Equation (3)), or were only designed for the (fixed) policy evaluation setting <em>(Castro, 2020)</em>. By contrast, we learn improved representations for policy inputs, as the policy improves online. Our $\pi^{*}$-bisimulation metric is learned with gradient decent, and we prove it converges to a fixed point in Theorem 1 under some assumptions. To train our encoder $\phi$ towards our desired relation $d(\mathbf{s}<em j="j">{i},\mathbf{s}</em>}):=\left|\phi(\mathbf{s<em j="j">{i})-\phi(\mathbf{s}</em>)\right|<em 1="1">{1}$, we draw batches of state pairs, and minimise the mean square error between the on-policy bisimulation metric and $\ell</em>$ distance in the latent space:</p>
<p>$J(\phi)=\left(\left|\mathbf{z}<em j="j">{i}-\mathbf{z}</em>\right|<em i="i">{1}-\left|r</em>}-r_{j}\right|-\gamma W_{2}\left(\hat{\mathcal{P}}(\cdot|\check{\mathbf{z}<em i="i">{i},\mathbf{a}</em>}),\check{\mathcal{P}}(\cdot|\check{\mathbf{z}<em j="j">{j},\mathbf{a}</em>,$ (4)})\right)\right)^{2</p>
<p>where $\mathbf{z}<em i="i">{i}=\phi(\mathbf{s}</em>}), \mathbf{z<em j="j">{j}=\phi(\mathbf{s}</em>\right|})$, $r$ are rewards, and $\check{\mathbf{z}}$ denotes $\phi(\mathbf{s})$ with stop gradients. Equation (4) also uses a probabilistic dynamics model $\check{\mathcal{P}}$ which outputs a Gaussian distribution. For this reason, we use the 2-Wasserstein metric $W_{2}$ in Equation (4), as opposed to the 1-Wasserstein in Equation (3), since the $W_{2}$ metric has a convenient closed form: $W_{2}(\mathcal{N}(\mu_{i},\Sigma_{i}),\mathcal{N}(\mu_{j},\Sigma_{j}))^{2}=\left|\mu_{i}-\mu_{j<em i="i">{2}^{2}+$ $\left|\Sigma</em>\right|}^{1/2}-\Sigma_{j}^{1/2<em _mathcal_F="\mathcal{F">{\mathcal{F}}^{2}$, where $|\cdot|</em>$ norm. Our model architecture and training is illustrated by Figure 2 and Algorithm 1.}}$ is the Frobenius norm. For all other distances we continue using the $\ell_{1</p>
<p>Incorporating control. We combine our representation learning approach (Algorithm 1) with the soft actor-critic (SAC) algorithm <em>(Haarnoja et al., 2018)</em> to devise a practical reinforcement learning method. We modified SAC slightly in Algorithm 2 to allow the value function to backprop to our encoder, which can improve performance further <em>(Yarats et al., 2021; Rakelly et al., 2019)</em>. Although, in principle, our method could be combined with any RL</p>
<p>algorithm, including the model-free DQN (Mnih et al., 2015), or model-based PETS (Chua et al., 2018). Implementation details and hyperparameter values of DBC are summarized in the appendix, Table 2. We train DBC by iteratively updating three components in turn: a policy $\pi$ (in this case SAC), an encoder $\phi$, and a dynamics model $\dot{\mathcal{P}}$ (lines 7-9, Algorithm 1). We found a single loss function was less stable to train. The inputs of each loss function $J(\cdot)$ in Algorithm 1 represents which components are updated. After each training step, the policy $\pi$ is used to step in the environment, the data is collected in a replay buffer $\mathcal{D}$, and a batch is randomly selected to repeat training.</p>
<h1>5 Generalization Bounds and Links to Causal Inference</h1>
<p>While DBC enables representation learning without pixel reconstruction, it leaves open the question of how good the resulting representations really are. In this section, we present theoretical analysis that bounds the suboptimality of a value function trained on the representation learned via DBC.</p>
<p>First, we show that our $\pi^{<em>}$-bisimulation metric converges to a fixed point, starting from the initialized policy $\pi_{0}$ and converging to an optimal policy $\pi^{</em>}$.
Theorem 1. Let met be the space of bounded pseudometrics on $\mathcal{S}$ and $\pi$ a policy that is continuously improving. Define $\mathcal{F}: \mathfrak{m e t} \mapsto \mathfrak{m e t}$ by</p>
<p>$$
\mathcal{F}(d, \pi)\left(\mathbf{s}<em j="j">{i}, \mathbf{s}</em>}\right)=(1-c)\left|r_{\mathbf{s<em _mathbf_s="\mathbf{s">{i}}^{\pi}-r</em><em _mathbf_s="\mathbf{s">{j}}^{\pi}\right|+c W(d)\left(\mathcal{P}</em><em _mathbf_s="\mathbf{s">{i}}^{\pi}, \mathcal{P}</em>\right)
$$}_{j}}^{\pi</p>
<p>Then $\mathcal{F}$ has a least fixed point $\tilde{d}$ which is a $\pi^{*}$-bisimulation metric.
Proof in appendix. As evidenced by Definition 2, the bisimulation metric has no direct dependence on the state space. Pixels can change, but bisimilarity will stay the same. Instead, bisimilarity is grounded in a recursion of future transition probabilities and rewards, which is closely related to the optimal value function. In fact, the bisimulation metric gives tight bounds on the optimal value function with discount factor $\gamma$. We show this using the property that the optimal value function is Lipschitz with respect to the bisimulation metric, see Theorem 5 in Appendix (Ferns et al., 2004). This result also implies that the closer two states are in terms of $\tilde{d}$, the more likely they are to share the same optimal actions. This leads us to a generalization bound on the optimal value function of an MDP constructed from a representation space using bisimulation metrics, $\left|\phi\left(\mathbf{s}<em j="j">{i}\right)-\phi\left(\mathbf{s}</em>\right)\right|<em i="i">{1}:=\tilde{d}\left(\mathbf{s}</em>$ to each $\epsilon$-cluster. This $\epsilon$ denotes the amount of approximation allowed, where large $\epsilon$ leads to a more compact bisimulation partition at the expense of a looser bound on the optimal value function.
Theorem 2 (Value bound based on bisimulation metrics). Given an MDP $\bar{\mathcal{M}}$ constructed by aggregating states in an $\epsilon$-neighborhood, and an encoder $\phi$ that maps from states in the original MDP $\mathcal{M}$ to these clusters, the optimal value functions for the two MDPs are bounded as}, \mathbf{s}_{j}\right)$. We can construct a partition of this space for some $\epsilon&gt;0$, giving us $n$ partitions where $\frac{1}{\epsilon}&lt;(1-c) \epsilon$. We denote $\phi$ as the encoder that maps from the original state space $\mathcal{S</p>
<p>$$
\left|V^{<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| \leq \frac{2 \epsilon}{(1-\gamma)(1-c)}
$$</p>
<p>Proof in appendix. As $\epsilon \rightarrow 0$ the optimal value function of the aggregated MDP converges to the original. Further, by defining a learning error for $\phi, \mathcal{L}:=\sup <em i="i">{\mathbf{s}</em>}, \mathbf{s<em i="i">{j} \in \mathcal{S}}\left[\left|\phi\left(\mathbf{s}</em>}\right)-\phi\left(\mathbf{s<em 1="1">{j}\right)\right|</em>}-\tilde{d}\left(\mathbf{s<em j="j">{i}, \mathbf{s}</em>:\left|V^{}\right)\right]$, we can update the bound in Theorem 2 to incorporate $\mathcal{L<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| \leq \frac{2 \epsilon+2 \mathcal{L}}{(1-\gamma)(1-c)}$.
MDP dynamics have a strong connection to causal inference and causal graphs, which are directed acyclic graphs (Jonsson \&amp; Barto, 2006; Schölkopf, 2019; Zhang et al., 2020). Specifically, the state and action at time $t$ causally affect the next state at time $t+1$. In this work, we care about the components of the state space that causally affect current and future reward. Deep bisimulation for control representations connect to causal feature sets, or the minimal feature set needed to predict a target variable (Zhang et al., 2020).
Theorem 3 (Connections to causal feature sets (Thm 1 in Zhang et al. (2020))). If we partition observations using the bisimulation metric, those clusters (a bisimulation partition) correspond to the causal feature set of the observation space with respect to current and future reward.
This connection tells us that these features are the minimal sufficient statistic of the current and future reward, and therefore consist of (and only consist of) the causal ancestors of the reward variable $r$.
Definition 3 (Causal Ancestors). In a causal graph where nodes correspond to variables and directed edges between a parent node $P$ and child node $C$ are causal relationships, the causal ancestors $A N(C)$ of a node are all nodes in the path from $C$ to a root node.</p>
<p>If there are interventions on distractor variables, or variables that control the rendering function $q$ and therefore the rendered observation but do not affect the reward, the causal feature set will be robust to these interventions, and correctly predict current and future reward in the linear function approximation setting (Zhang et al., 2020). As an example, in autonomous driving, an intervention can be a change from day to night which affects the observation space but not the dynamics or reward. Finally, we show that a representation based on the bisimulation metric generalizes to other reward functions with the same causal ancestors.</p>
<p>Theorem 4 (Task Generalization). Given an encoder $\phi: \mathcal{S} \mapsto \mathcal{Z}$ that maps observations to a latent bisimulation metric representation where $\left|\phi\left(\mathbf{s}<em j="j">{i}\right)-\phi\left(\mathbf{s}</em>\right)\right|<em i="i">{1}:=\tilde{d}\left(\mathbf{s}</em>$ encodes information about all the causal ancestors of the reward $A N(R)$.
Proof in appendix. This result shows that the learned representation will generalize to unseen reward functions, as long as the new reward function has a subset of the same causal ancestors. As an example, a representation learned for a robot to walk will likely generalize to learning to run, because the reward function depends on forward velocity and all the factors that contribute to forward velocity. However, that representation will not generalize to picking up objects, as those objects will be ignored by the learned representation, since they are not likely to be causal ancestors of a reward function designed for walking. Theorem 4 shows that the learned representation will be robust to spurious correlations, or changes in factors that are not in $A N(R)$. This complements Theorem 5, that the representation is a minimal sufficient statistic of the optimal value function, improving generalization over non-minimal representations.}, \mathbf{s}_{j}\right), \mathcal{Z</p>
<p>Theorem 5 ( $V^{<em>}$ is Lipschitz with respect to $\tilde{d}$ ). Let $V^{</em>}$ be the optimal value function for a given discount factor $\gamma$. If $c \geq \gamma$, then $V^{<em>}$ is Lipschitz continuous with respect to $\tilde{d}$ with Lipschitz constant $\frac{1}{1-c}$, where $\tilde{d}$ is a $\pi^{</em>}$-bisimulation metric.</p>
<p>$$
\left|V^{<em>}\left(\mathbf{s}_{i}\right)-V^{</em>}\left(\mathbf{s}<em i="i">{j}\right)\right| \leq \frac{1}{1-c} \tilde{d}\left(\mathbf{s}</em>\right)
$$}, \mathbf{s}_{j</p>
<p>See Theorem 5.1 in Ferns et al. (2004) for proof. We show empirical validation of these findings in Section 6.2.</p>
<h1>6 Experiments</h1>
<p>Our central hypothesis is that our non-reconstructive bisimulation based representation learning approach should be substantially more robust to task-irrelevant distractors. To that end, we evaluate our method in a clean setting without distractors, as well as a much more difficult setting with distractors. We compare against several baselines. The first is Stochastic Latent Actor-Critic (SLAC, Lee et al. (2020)), a state-of-the-art method for pixel observations on DeepMind Control that learns a dynamics model with a reconstruction loss. The second is DeepMDP (Gelada et al., 2019), a recent method that also learns a latent representation space using a latent dynamics model, reward model, and distributional Q learning, but for which they needed a reconstruction loss to scale up to Atari. Finally, we compare against two methods using the same architecture as ours but exchange our bisimulation loss with (1) a reconstruction loss ("Reconstruction") and (2) contrastive predictive coding (Oord et al., 2018) ("Contrastive") to ground the dynamics model and learn a latent representation.</p>
<h3>6.1 Control with Background Distraction</h3>
<p>In this section, we benchmark DBC and the previously described baselines on the DeepMind Control (DMC) suite (Tassa et al., 2018) in two settings and nine environments (Figure 3), finger_spin, cheetah_run, and walker_walk and additional environments in the appendix.</p>
<p>Default Setting. Here, the pixel observations have simple backgrounds as shown in Figure 3 (top row) with training curves for our DBC and baselines. We see SLAC, a recent state-of-the-art model-based representation learning method that uses reconstruction, generally performs best.</p>
<p>Simple Distractors Setting. Next, we include simple background distractors, shown in Figure 3 (middle row), with easy-to-predict motions. We use a fixed number of colored circles that obey the dynamics of an ideal gas (no attraction or repulsion between objects) with no collisions. Note the performance of DBC remains consistent, as other methods start decreasing.</p>
<p>Natural Video Setting. Then, we incorporate natural video from the Kinetics dataset (Kay et al., 2017) as background (Zhang et al., 2018), shown in Figure 3 (bottom row). The results confirm our hypothesis: although a number of prior methods can learn effectively in the absence of distractors,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Left observations: Pixel observations in DMC in the default setting (top row) of the finger spin (left column), cheetah (middle column), and walker (right column), with simple distractors (middle row), and natural video distractors (bottom row). Right training curves: Results comparing out DBC method to baselines on 10 seeds with 1 standard error shaded in the default setting. The grid-location of each graph corresponds to the grid-location of each observation.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: t-SNE of latent spaces learned with a bisimulation metric (left t-SNE) and VAE (right t-SNE) after training has completed, color-coded with predicted state values (higher value yellow, lower value purple). Neighboring points in the embedding space learned with a bisimulation metric have similar states and correspond to observations with the same task-related information (depicted as pairs of images with their corresponding embeddings), whereas no such structure is seen in the embedding space learned by VAE, where the same image pairs are mapped far away from each other.</p>
<p>when complex distractions are introduced, our non-reconstructive bisimulation based method attains substantially better results.</p>
<p>To visualize the representation learned with our bisimulation metric loss function in Equation (4), we use a t-SNE plot (Figure 4). We see that even when the background looks drastically different, our encoder learns to ignore irrelevant information and maps observations with similar robot configurations near each other. See Appendix D for another visualization.</p>
<h3>6.2 Generalization Experiments</h3>
<p>We test generalization of our learned representation in two ways. First, we show that the learned representation space can generalize to different types of distractors, by training with simple distractors and testing on the natural video setting. Second, we show that our learned representation can be useful reward functions other than those it was trained for.</p>
<p>Generalizing over backgrounds. We first train on the simple distractors setting and evaluate on natural video. Figure 5 shows an example of the simple distractors setting and performance during training time of two experiments, blue being the zero-shot transfer to the</p>
<p>natural video setting, and orange the baseline which trains on natural video. This result empirically validates that the representations learned by DBC are able to effectively learn to ignore the background, regardless of what the background contains or how dynamic it is.</p>
<p>Generalizing over reward functions. We evaluate (Figure 5) the generalization capabilities of the learned representation by training SAC with new reward functions walker_stand and walker_run using the fixed representation learned from walker_walk. This is empirical evidence that confirms Theorem 4.2 if the new reward functions are causally dependent on a subset of the same factors that determine the original reward function, then our representation is sufficient.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Generalization of a model trained on simple distractors environment and evaluated on kinetics (left). Generalization of an encoder trained on walker_walk environment and evaluated on walker_stand (center) and walker_run (right), all in the simple distractors setting. 10 seeds, 1 standard error shaded.</p>
<h1>6.3 Comparison with other Bisimulation Encoders</h1>
<p>Even though the purpose of bisimulation metrics by Castro (2020) is learning distances $d$, not representation spaces $\mathcal{Z}$, it nevertheless implements $d$ with function approximation: $d\left(\mathbf{s}<em j="j">{i}, \mathbf{s}</em>}\right)=$ $\psi\left(\phi\left(\mathbf{s<em j="j">{i}\right), \phi\left(\mathbf{s}</em>\right)\right)$ by encoding observations with $\phi$ before computing distances with $\psi$, trained as:</p>
<p>$$
J(\phi, \psi)=\left(\psi\left(\phi\left(\mathbf{s}<em j="j">{i}\right), \phi\left(\mathbf{s}</em>}\right)\right)-\left|r_{i}-r_{j}\right|-\gamma \hat{\psi}\left(\hat{\phi}\left(\mathcal{P}\left(\mathbf{s<em i="i">{i}, \pi\left(\mathbf{s}</em>}\right)\right)\right), \hat{\phi}\left(\mathcal{P}\left(\mathbf{s<em j="j">{j}, \pi\left(\mathbf{s}</em>
$$}\right)\right)\right)\right)\right)^{\top</p>
<p>where $\hat{\phi}$ and $\hat{\psi}$ are target networks. A natural question is: how does the encoder $\phi$ above perform in control tasks? We combine $\phi$ above with our policy in Algorithm 2 and use the same network $\psi$ (single hidden layer 729 wide). Figure 6 shows representations from Castro (2020) can learn control (surprisingly well given it was not designed to), but our method learns faster. Further, our method is simpler: by comparing Equation (8) to Equation (4), our method uses the $\ell_{1}$ distance between the encoding instead of introducing an addition network $\psi$.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h3>6.4 Autonomous Driving with Visual Redundancy</h3>
<p>Real-world control systems such as robotics and autonomous vehicles must contend with a huge variety of task-irrelevant information, such as irrelevant objects (e.g. clouds) and irrelevant details (e.g. obstacle color). To evaluate DBC on tasks with more realistic observations, we construct a highway driving scenario with photo-realistic visual observations using the CARLA simulator (Dosovitskiy et al., 2017) shown in Figure 7. The agent's goal is to drive as far as possible along CARLA's Town04's figure-8 the highway in 1000 time-steps without colliding into the 20 other moving vehicles or barriers. Our objective function rewards highway progression and penalises collisions: $r_{t}=\mathbf{v}<em _highway="{highway" _text="\text">{\text {ego }}^{\top} \hat{\mathbf{u}}</em>}} \cdot \Delta t-\lambda_{i} \cdot$ impulse $-\lambda_{s} \cdot|\text { steer }|$, where $\mathbf{v<em _highway="{highway" _text="\text">{\text {ego }}$ is the velocity vector of the ego vehicle, projected onto the highway's unit vector $\hat{\mathbf{u}}</em>=1$. While more specialized objectives exist like lane-keeping, this experiment's purpose is only to compare representations with observations more characteristic of real robotic tasks. We use five cameras on the vehicle's roof, each with 60 degree views. By concatenating the images together, our vehicle has a 300 degree view, observed as $84 \times 420$ pixels. Code and install instructions in appendix.}}$, and multiplied by time discretization $\Delta t=0.05$ to measure highway progression in meters. Collisions result in impulses $\in \mathbb{R}^{+}$, measured in Newtonseconds. We found a steering penalty steer $\in[-1,1]$ helped, and used weights $\lambda_{i}=10^{-4}$ and $\lambda_{s</p>
<p>Results in Figure 9 compare the same baselines as before, except for SLAC which is easily distracted (Figure 3). Instead we used SAC, which does not explicitly learn a representation, but performs surprisingly well from raw images. DeepMDP performs well too, perhaps given its similarly to bisimulation. But, Reconstruction and Contrastive methods again perform poorly with complex images. More intuitive metrics are in Table 1 and Figure 8 depicts the representation space as a t-SNE with corresponding observations. Each run took 12 hours on a GTX 1080 GPU.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: A t-SNE diagram of encoded first-person driving observations after 10k training steps of Algorithm 1, color coded by value (V in Algorithm 2). <strong>Top</strong>: the learned representation identifies an obstacle on the right side. Whether that obstacle is a dark wall, bright car, or truck is task-irrelevant: these states are behaviourally equivalent. <strong>Left</strong>: the ego vehicle has flipped onto its left side. The different wall colors, due to a setting sun, is irrelevant: all states are equally stuck and low-value (purple t-SNE color). <strong>Right</strong>: clear highway driving. Clouds and sun position are irrelevant.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Performance comparison with 3 seeds on the driving task. Our DBC method (red) performs better than DeepMDP (purple) or learning direct from pixels without a representation (SAC, green), and much better than contrastive methods (blue). Our method's final performance is 46.8% better than the next best baseline.</p>
<p>Table 1: Driving metrics, averaged over 100 episodes, after 100k training steps, with standard error. Arrow direction indicates if metric desired larger or smaller.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>successes (100m)</td>
<td>SAC</td>
<td>DeepMDP</td>
<td>DBC (ours)</td>
</tr>
<tr>
<td>distance (m)</td>
<td>12%</td>
<td>17%</td>
<td>24%</td>
</tr>
<tr>
<td>crash intensity</td>
<td>4604 ± 30.7</td>
<td>1958 ± 15.6</td>
<td>2673 ± 38.5</td>
</tr>
<tr>
<td>average: time</td>
<td>16.6% ± 0.019%</td>
<td>15.4% ± 0.015%</td>
<td>7.3% ± 0.012%</td>
</tr>
<tr>
<td>average brake</td>
<td>1.3% ± 0.006%</td>
<td>4.3% ± 0.033%</td>
<td>1.6% ± 0.022%</td>
</tr>
</tbody>
</table>
<h1>7 Discussion</h1>
<p>This paper presents Deep Bisimulation for Control: a new representation learning method that considers downstream control. Observations are encoded into representations that are <em>invariant</em> to different task-irrelevant details in the observation. We show this is important when learning control from outdoor images, or otherwise images with background "distractions". In contrast to other bisimulation methods, we show performance gains when distances in representation space match the bisimulation distance between observations.</p>
<p><strong>Future work:</strong> Several options exist for future work. First, our latent dynamics model $\hat{\mathcal{P}}$ was only used for training our encoder in Equation (4), but could also be used for multi-step planning in latent space. Second, estimating uncertainty could also be important to produce agents that can work in the real world, perhaps via an ensemble of models ${\hat{\mathcal{P}}<em k="1">{k}}</em>$, to detect—and adapt to—distributional shifts between training and test observations. Third, an undressed issue is that of partially observed settings (that assumed approximately full observability by using stacked images), possibly using explicit memory or implicit memory such as an LSTM. Finally, investigating which metrics (L1 or L2) and dynamics distributions (Gaussians or not) would be beneficial.}^{K</p>
<h1>References</h1>
<p>Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov decision processes. In Association for the Advancement of Artificial Intelligence (AAAI), 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML), pp. 1597-1607. PMLR, 2020.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Neural Information Processing Systems (NeurIPS), pp. 4754-4765, 2018.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Conference on Robot Learning (CoRL), pp. 1-16, 2017.
Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, and John Langford. Provably efficient RL with rich observations via latent state decoding. Computing Research Repository (CoRR), abs/1901.09018, 2019. URL http://arxiv.org/abs/1901.09018.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes. In Uncertainty in Artificial Intelligence (UAI), pp. 162-169, 2004. ISBN 0-9749039-0-6. URL http://dl.acm.org/citation.cfm?id=1036843.1036863.
Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous Markov decision processes. Society for Industrial and Applied Mathematics, 40(6):1662-1714, December 2011. ISSN 0097-5397. doi: 10.1137/10080484X. URL https://doi.org/10.1137/ 10080484X.
Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In Uncertainty in Artificial Intelligence (UAI), pp. 210-219, 2014.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. DeepMDP: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), International Conference on Machine Learning (ICML), volume 97, pp. 2170-2179, Jun 2019.
Robert Givan, Thomas L. Dean, and Matthew Greig. Equivalence notions and model minimization in Markov decision processes. Artificial Intelligence, 147:163-223, 2003.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning (ICML), pp. 1861-1870. PMLR, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning (ICML), pp. 2555-2565. PMLR, 2019.
Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In International Conference on Machine Learning (ICML), pp. 4182-4192. PMLR, 2020.
Rico Jonschkowski and Oliver Brock. Learning state representations with robotic priors. Autonomous Robots, 39(3):407-428, 2015.
Anders Jonsson and Andrew Barto. Causal graph based decomposition of factored MDPs. J. Mach. Learn. Res., 7:2259-2301, December 2006. ISSN 1532-4435.
Will Kay, João Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. Computing Research Repository (CoRR), 2017. URL http://arxiv.org/abs/1705.06950.
Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2010.
Sascha Lange, Martin Riedmiller, and Arne Voigtländer. Autonomous reinforcement learning on raw visual input data in a real world application. In International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2012. doi: 10.1109/IJCNN.2012.6252823.
K. G. Larsen and A. Skou. Bisimulation through probabilistic testing (preliminary report). In Symposium on Principles of Programming Languages, pp. 344-352. Association for Computing Machinery, 1989. ISBN 0897912942. doi: 10.1145/75277.75307. URL https://doi.org/ $10.1145 / 75277.75307$.</p>
<p>Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning (ICML), pp. 5639-5650. PMLR, 2020.
Alex Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. In Neural Information Processing Systems (NeurIPS), volume 33, pp. 741-752, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/08058bf500242562c0d031ff830ad094-Paper.pdf.
Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for MDPs. In International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2006.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL http://dx.doi.org/10.1038/nature14236.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on Machine Learning (ICML), pp. 5331-5340. PMLR, 2019.
Bernhard Schölkopf. Causality for machine learning, 2019.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. DeepMind control suite. Technical report, DeepMind, January 2018. URL https : //arxiv.org/abs/1801.00690.
Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bounding performance loss in approximate MDP homomorphisms. In Neural Information Processing (NeurIPS), pp. 1649-1656, 2009.
Franck van Breugel and James Worrell. Towards quantitative verification of probabilistic transition systems. In Fernando Orejas, Paul G. Spirakis, and Jan van Leeuwen (eds.), Automata, Languages and Programming, pp. 421-432. Springer, 2001. ISBN 978-3-540-48224-6. doi: 10.1007/ 3-540-48224-5_35.
Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. ArXiv, abs/1807.03748, 2018.
Cédric Villani. Topics in optimal transportation. American Mathematical Society, 012003.
Niklas Wahlström, Thomas Schön, and Marc Deisenroth. From pixels to torques: Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Neural Information Processing Systems (NeurIPS), pp. 2728-2736, 2015.
Denis Yarats and Ilya Kostrikov. Soft actor-critic (SAC) implementation in PyTorch. https: //github.com/denisyarats/pytorch_sac, 2020.
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In Association for the Advancement of Artificial Intelligence (AAAI), 2021.
Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural environment benchmarks for reinforcement learning. Computing Research Repository (CoRR), abs/1811.06032, 2018. URL http://arxiv . org/abs/1811.06032.
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block MDPs. In International Conference on Machine Learning (ICML), 2020.</p>
<h1>A Additional Theorems and Proofs</h1>
<p>Theorem 1. Let met be the space of bounded pseudometrics on $S$ and $\pi \in \Pi$ a policy that is continuously improving in the space of policies $\Pi$. Define $\mathcal{F}: \operatorname{met} \times \Pi \mapsto$ met by</p>
<p>$$
\mathcal{F}(d, \pi)\left(\mathbf{s}<em j="j">{i}, \mathbf{s}</em>}\right)=(1-c)\left|r_{\mathbf{s<em _mathbf_s="\mathbf{s">{i}}^{\pi}-r</em><em _mathbf_s="\mathbf{s">{j}}^{\pi}\right|+c W(d)\left(\mathcal{P}</em><em _mathbf_s="\mathbf{s">{i}}^{\pi}, \mathcal{P}</em>\right)
$$}_{j}}^{\pi</p>
<p>Then $\mathcal{F}$ has a least fixed point $\tilde{d}$ which is a $\pi^{*}$-bisimulation metric.</p>
<p>Proof. Ideally, to prove this theorem we show that $\mathcal{F}$ is monotonically increasing and continuous, and apply Fixed Point Theorem to show the existence of a fixed point that $\mathcal{F}$ converges to. Unfortunately, we can show that $\mathcal{F}$ under $\pi$ as $\pi$ monotonically converges to $\pi^{*}$ is not also monotonic, unlike the original bisimulation metric setting (Ferns et al., 2004) and the policy evaluation setting (Castro, 2020). We start the iterates $\mathcal{F}^{n}$ from bottom $\perp$, denoted as $\mathcal{F}^{n}(\perp)$. In Ferns et al. (2004) the $\max <em n="n">{\mathbf{a} \in \mathcal{A}}$ can be thought of as learning a policy between every two pairs of states to maximize their distance, and therefore this distance can only stay the same or grow over iterations of $\mathcal{F}$. In Castro (2020), $\pi$ is fixed, and under a deterministic MDP it can also be shown that distance between states $d</em>}\left(\mathbf{s<em j="j">{i}, \mathbf{s}</em>$ and getting updated:}\right)$ will only expand, not contract as $n$ increases. In the policy iteration setting, however, with $\pi$ starting from initialization $\pi_{0</p>
<p>$$
\pi_{k}(\mathbf{s})=\arg \max <em _mathbf_s="\mathbf{s">{\mathbf{a} \in \mathcal{A}} \sum</em>\right)\right]
$$}^{\prime} \in S}\left[r_{\mathbf{s s}^{\prime}}^{\mathbf{a}}+\gamma V^{\pi_{k-1}}\left(\mathbf{s}^{\prime</p>
<p>there is no guarantee that the distance between two states $d_{n-1}^{\pi_{k-1}}\left(\mathbf{s}<em j="j">{i}, \mathbf{s}</em>}\right)&lt;d_{n}^{\pi_{k}}\left(\mathbf{s<em j="j">{i}, \mathbf{s}</em>$, which is required for monotonicity.}\right)$ under policy iterations $\pi_{k-1}, \pi_{k}$ and distance metric iterations $d_{n-1}, d_{n}$ for $k, n \in \mathbb{N</p>
<p>Instead, we show that using the policy improvement theorem which gives us</p>
<p>$$
V^{\pi_{k}}(\mathbf{s}) \geq V^{\pi_{k-1}}(\mathbf{s}), \forall \mathbf{s} \in \mathcal{S}
$$</p>
<p>$\pi$ will converge to a fixed point using the Fixed Point Theorem, and taking the result by Castro (2020) that $\mathcal{F}^{\pi}$ has a fixed point for every $\pi \in \Pi$, we can show that a fixed point bisimulation metric will be found with policy iteration.</p>
<p>Theorem 2. Given a new aggregated MDP $\overline{\mathcal{M}}$ constructed by aggregating states in an $\epsilon$ neighborhood, and an encoder $\phi$ that maps from states in the original MDP $\mathcal{M}$ to these clusters, the optimal value functions for the two MDPs are bounded as</p>
<p>$$
\left|V^{<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| \leq \frac{2 \epsilon}{(1-\gamma)(1-c)}
$$</p>
<p>Proof. From Theorem 5.1 in Ferns et al. (2004) we have:</p>
<p>$$
(1-c)\left|V^{<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| \leq g(\mathbf{s}, \tilde{d})+\frac{\gamma}{1-\gamma} \max _{u \in \mathcal{S}} g(u, \tilde{d})
$$</p>
<p>where $g$ is the average distance between a state and all other states in its equivalence class under the bisimulation metric $\tilde{d}$. By specifying a $\epsilon$-neighborhood for each cluster of states we can replace $g$ :</p>
<p>$$
\begin{aligned}
(1-c)\left|V^{<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| &amp; \leq 2 \epsilon+\frac{\gamma}{1-\gamma} 2 \epsilon \
\left|V^{<em>}(\mathbf{s})-V^{</em>}(\phi(\mathbf{s}))\right| &amp; \leq \frac{1}{1-c}\left(2 \epsilon+\frac{\gamma}{1-\gamma} 2 \epsilon\right) \
&amp; =\frac{2 \epsilon}{(1-\gamma)(1-c)}
\end{aligned}
$$</p>
<p>Theorem 4. Given an encoder $\phi: \mathcal{S} \mapsto \mathcal{Z}$ that maps observations to a latent bisimulation metric representation where $\left|\phi\left(\mathbf{s}<em j="j">{i}\right)-\phi\left(\mathbf{s}</em>\right)\right|<em i="i">{1}:=\tilde{d}\left(\mathbf{s}</em>$ encodes information about all the causal ancestors of the reward $A N(R)$.}, \mathbf{s}_{j}\right)$, $\mathcal{Z</p>
<p>Proof. We assume a MDP with a state space $\mathcal{S}:=\left{\mathcal{S}^{1}, \ldots, \mathcal{S}^{K}\right}$ that can be factorized into $K$ variables with 1-step causal transition dynamics described by a causal graph $\mathcal{G}$ (example in Figure 10). We break the proof up into two parts: 1) show that if a factor $\mathcal{S}^{1} \notin A N(R)$ changes, the bisimulation distance between the original state $\mathbf{s}$ and the new state $\mathbf{s}^{\prime}$ is 0 . and 2 ) show that if a factor $\mathcal{S}^{j} \in A N(R)$ changes, the bisimulation distance can be $&gt;0$.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Causal graph of transition dynamics. Reward depends only on $\mathbf{s}^{1}$ as a causal parent, but $\mathbf{s}^{1}$ causally depends on $\mathbf{s}^{2}$, so $\operatorname{AN}(\mathrm{R})$ is the set $\left{\mathbf{s}^{1}, \mathbf{s}^{2}\right}$.</p>
<p>1) If $\mathcal{S}^{i} \notin A N(R)$, an intervention on that factor does not affect current or future reward.</p>
<p>$$
\begin{aligned}
\tilde{d}\left(\mathbf{s}<em j="j">{i}, \mathbf{s}</em>\right) &amp; =\max <em _mathbf_s="\mathbf{s">{a \in A}(1-c)\left|r</em><em _mathbf_s="\mathbf{s">{i}}^{\mathbf{a}}-r</em><em _mathbf_s="\mathbf{s">{j}}^{\mathbf{a}}\right|+c W(\tilde{d})\left(\mathcal{P}</em><em _mathbf_s="\mathbf{s">{i}}^{\mathbf{a}}, \mathcal{P}</em><em A="A" _in="\in" a="a">{j}}^{\mathbf{a}}\right) \
&amp; =\max </em>} c W(\tilde{d})\left(\mathcal{P<em i="i">{\mathbf{s}</em>}}^{\mathbf{a}}, \mathcal{P<em j="j">{\mathbf{s}</em>}}^{\mathbf{a}}\right) \quad \mathbf{s<em j="j">{i} \text { and } \mathbf{s}</em>
\end{aligned}
$$} \text { have the same reward. </p>
<p>If $\mathcal{S}^{i}$ does not affect future reward, then states $\mathbf{s}<em j="j">{i}$ and $\mathbf{s}</em>$ will have the same future reward conditioned on all future actions. This gives us</p>
<p>$$
\tilde{d}\left(\mathbf{s}, \mathbf{s}^{\prime}\right)=0
$$</p>
<p>2) If there is an intervention on $S^{j} \in A N(R)$ then current and/or future reward can change. If current reward changes, then we already have $\max <em _mathbf_s="\mathbf{s">{\mathbf{a} \in \mathcal{A}}(1-c)\left|r</em><em _mathbf_s="\mathbf{s">{i}}^{\mathbf{a}}-r</em><em i="i">{j}}^{\mathbf{a}}\right|&gt;0$, giving us $\tilde{d}\left(\mathbf{s}</em>}, \mathbf{s<em _mathbf_a="\mathbf{a">{j}\right)&gt;$ 0 . If only future reward changes, then those future states will have nonzero bisimilarity, and $\max </em>} \in \mathcal{A}} W(\tilde{d})\left(P_{\mathbf{s<em _mathbf_s="\mathbf{s">{i}}^{\mathbf{a}}, P</em><em i="i">{j}}^{\mathbf{a}}\right)&gt;0$, giving us $\tilde{d}\left(\mathbf{s}</em>\right)&gt;0$.}, \mathbf{s}_{j</p>
<h1>B Definition of State</h1>
<p>Since we are concerned primarily with learning from image observations, we could explicitly distinguish the image observation space $\mathcal{O}$ from an unknown state space $\mathcal{S}$. However, since we are not tackling the general POMDP problem, we consider the Block MDP (Du et al., 2019), which assumes the state space is latent, and that we are instead given access to an observation space $\mathcal{O}$ and rendering function $q: \mathcal{S} \mapsto \mathcal{O}$. The crucial assumption that distinguishes the Block MDP from partially observable MDPs is the following:
Assumption 1 (Block structure (Du et al., 2019)). Each observation o uniquely determines its generating state $\mathbf{s}$. That is, the observation space $\mathcal{O}$ can be partitioned into disjoint blocks $\mathcal{O}_{\text {s }}$, each containing the support of the conditional distribution $q(\mathbf{o} \mid \mathbf{s})$.</p>
<p>This assumption gives us the Markov property in the observation space $\mathbf{o} \in \mathcal{O}$. As an example, one can think of the proprioceptive state consisting of positions and velocities of actuators as the underlying state, and stacked pixel observations from a specific camera angle as a particular rendering function and corresponding observation space.</p>
<h2>C Additional DMC Results</h2>
<p>In Figure 11 we show performance on the default setting on 9 different environments from DMC. Figures 12 and 13 give performance on the simple distractors and natural video settings for all 9 environments.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Results for DBC in the default setting, in comparison to baselines with reconstruction loss, contrastive loss, and SLAC on 10 seeds with 1 standard error shaded.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Results for DBC in the simple distractors setting, in comparison to baselines with reconstruction loss, contrastive loss, DeepMDP, and SLAC on 10 seeds with 1 standard error shaded.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Results for our bisimulation metric method in the natural video setting, in comparison to baselines with reconstruction loss, contrastive loss, DeepMDP, and SLAC on 10 seeds with 1 standard error shaded.</p>
<h1>D Additional Visualizations</h1>
<p>In addition to Figure 4, we also took 10 nearby points in the t-SNE plot and average the observations, shown on the far left of Figure 14. Note the robot agent is quite crisp, which means neighboring points encode the agent in similar positions, but the backgrounds are very different, and so are blurry when averaged.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14: t-SNE of latent spaces learned with a bisimulation metric after training has completed, color-coded with predicted state values (higher value yellow, lower value purple). Neighboring points (right) in the embedding space learned with a bisimulation metric have similar encodings (middle). When we sample from the same latent point, and average the images, we see the robot configuration is crisp, meaning neighboring points encode the agent in similar positions, but the backgrounds are very different, and so are blurry when averaged.</p>
<h1>E Implementation Details</h1>
<p>We use the same encoder architecture as in Yarats et al. (2021), which is an almost identical encoder architecture as in Tassa et al. (2018), with two more convolutional layers to the convnet trunk. The encoder has kernels of size $3 \times 3$ with 32 channels for all the convolutional layers and set stride to 1 everywhere, except of the first convolutional layer, which has stride 2 , and interpolate with ReLU activations. Finally, we add tanh nonlinearity to the 50 dimensional output of the fully-connected layer.</p>
<p>For the reconstruction method, the decoder consists of a fully-connected layer followed by four deconvolutional layers. We use ReLU activations after each layer, except the final deconvolutional layer that produces pixels representation. Each deconvolutional layer has kernels of size $3 \times 3$ with 32 channels and stride 1 , except of the last layer, where stride is 2 .</p>
<p>The dynamics and reward models are both MLPs with two hidden layers with 200 neurons each and ReLU activations.</p>
<p>Soft Actor Critic (SAC) (Haarnoja et al., 2018) is an off-policy actor-critic method that uses the maximum entropy framework for soft policy iteration. At each iteration, SAC performs soft policy evaluation and improvement steps. The policy evaluation step fits a parametric soft Q-function $Q\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>$ by minimizing the soft Bellman residual,}\right)$ using transitions sampled from the replay buffer $\mathcal{D</p>
<p>$$
J(Q)=\mathbb{E}<em t="t">{\left(\mathbf{s}</em>}, \mathbf{s<em t="t">{t}, r</em>}, \mathbf{s<em t="t">{t+1}\right) \sim \mathcal{D}}\left[\left(Q\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)-r</em>\right]
$$}-\gamma \bar{V}\left(\mathbf{s}_{t+1}\right)\right)^{2</p>
<p>The target value function $\bar{V}$ is approximated via a Monte-Carlo estimate of the following expectation,</p>
<p>$$
\bar{V}\left(\mathbf{s}<em _mathbf_a="\mathbf{a">{t+1}\right)=\mathbb{E}</em><em t_1="t+1">{t+1} \sim \pi}\left[\bar{Q}\left(\mathbf{s}</em>}, \mathbf{a<em t_1="t+1">{t+1}\right)-\alpha \log \pi\left(\mathbf{a}</em>\right)\right]
$$} \mid \mathbf{s}_{t+1</p>
<p>where $\bar{Q}$ is the target soft Q-function parameterized by a weight vector obtained from an exponentially moving average of the Q-function weights to stabilize training. The policy improvement step then attempts to project a parametric policy $\pi\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>\right)$ by minimizing KL divergence between the policy and a Boltzmann distribution induced by the Q-function, producing the following objective,</p>
<p>$$
J(\pi)=\mathbb{E}<em t="t">{\mathbf{s}</em>} \sim \mathcal{D}}\left[\mathbb{E<em t="t">{\mathbf{a}</em>} \sim \pi}\left[\alpha \log \left(\pi\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>}\right)\right)-Q\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)\right]\right]
$$</p>
<p>We modify the Soft Actor-Critic PyTorch implementation by Yarats \&amp; Kostrikov (2020) and augment with a shared encoder between the actor and critic, the general model $f_{s}$ and task-specific models $f_{u}^{c}$. The forward models are multi-layer perceptions with ReLU non-linearities and two hidden layers of 200 neurons each. The encoder is a linear layer that maps to a 50 -dim hidden representation. The hyperparameters used for the RL experiments are in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter name</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Replay buffer capacity</td>
<td style="text-align: center;">$10^{6}$</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Critic learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Critic target update frequency</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Critic Q-function soft-update rate $\tau_{Q}$</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: left;">Critic encoder soft-update rate $\tau_{\phi}$</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: left;">Actor learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Actor update frequency</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Actor log stddev bounds</td>
<td style="text-align: center;">$[-5,2]$</td>
</tr>
<tr>
<td style="text-align: left;">Encoder learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Decoder learning rate</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Decoder weight decay</td>
<td style="text-align: center;">$10^{-7}$</td>
</tr>
<tr>
<td style="text-align: left;">Temperature learning rate</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Temperature Adam's $\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Init temperature</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 2: A complete overview of used hyper parameters.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Note that $d$ is a pseudometric, meaning the distance between two different states can be zero, corresponding to behavioral equivalence.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
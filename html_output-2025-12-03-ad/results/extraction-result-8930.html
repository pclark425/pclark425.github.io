<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8930 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8930</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8930</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e5c72b92c48d68594b290c84a8904da7c8335554</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e5c72b92c48d68594b290c84a8904da7c8335554" target="_blank">Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules to significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8930.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8930.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-AUGMENTER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-AUGMENTER (Large Language Model Augmenter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular framework that augments a fixed, black-box LLM (e.g., ChatGPT) with plug-and-play modules (Working Memory, Policy, Action Executor, Utility) to ground generation in external knowledge and iteratively revise responses using automated feedback until verification passes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (black-box LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT is used as a frozen, black-box, closed-book conversational LLM backbone in the experiments; the paper does not specify exact parameter counts or pretraining details for ChatGPT and treats it as an API-accessible model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>feedback-augmented iterative prompting (generate → verify → revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a candidate response with the LLM given consolidated external evidence; the Utility module scores/ verifies the response (e.g., KF1 or recall) and produces textual feedback; the Prompt Engine revises the prompt by inserting feedback (and optionally updated evidence) and re-queries the LLM; this loop repeats until the candidate passes verification or some stopping condition.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Information-seeking dialog (News Chat DSTC7, Customer Service DSTC11) and Wiki QA (OTT-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dialog tasks requiring grounded responses based on external knowledge and an open-domain multi-hop Wiki question answering benchmark that requires reasoning over passages and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Customer Service (BM25 knowledge): KF1 = 37.41% with rule-based feedback; News Chat (BM25): KF1 = 36.41% with feedback; Wiki QA (CORE consolidated evidence): F1 = 11.80% with feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Customer Service (BM25): KF1 = 34.07% without feedback; News Chat (BM25): KF1 = 34.96% without feedback; Wiki QA (CORE): F1 = 8.08% without feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Software-module-mediated iterative prompting: Utility module (model-based or rule-based) computes a utility score (e.g., KF1, recall) and generates verbalized feedback f = Q_ψ(q,e,o,h_q); Prompt Engine incorporates f and consolidated evidence into the LLM prompt; Working Memory stores state across iterations. Reflection is implemented via prompt engineering + external modules (retriever, entity-linker, chainer, utility) — no weights of the LLM are changed.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains when adding automated feedback: on Customer Service (BM25) KF1 increased from 34.07% → 37.41% (rule-based feedback) and 37.10% (self-criticism); on News Chat (BM25) KF1 increased 34.96% → 36.41%; on Wiki QA (CORE) F1 increased 8.08% → 11.80% when adding feedback. The paper also reports that combining utility functions and feedback-augmented prompting yields the best performance (Figure 5) and that trainable policy learning improves KF1 over random policy (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative feedback increases latency and API cost (slower user experience since LLM often queried multiple times); experiments largely used a rule-based policy due to ChatGPT bandwidth constraints; RL policy training was done with T5-Base rather than the expensive ChatGPT in-loop; closed-book ChatGPT still hallucinates or abstains (e.g., ChatGPT abstained ≈17% on Wiki QA) and there remains a performance gap to fine-tuned models (Ma et al., 2022) with large top-k consolidated evidence; utility depends on correctness of external evidence retrieval—incorrect or missing evidence limits improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against no-feedback baseline and rule-based feedback: feedback-augmented iterative prompting outperforms no-feedback. Table 4 shows rule-based feedback slightly outperforms the self-criticism variant in Customer Service (KF1 37.41 vs 37.10). The paper also compares variants such as prompting twice and re-ranking (which gave slightly higher KF1 than some baselines but performed worse than the feedback+utility combination) and ablates knowledge usage policies (No-knowledge, Self-ask, Always-use). Compared to fine-tuned systems (Ma et al., 2022) the augmented black-box approach still lags, attributed to lower consolidated evidence recall and required alignment for concise faithful reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations isolating feedback effect (Customer Service BM25): w/o Feedback KF1 = 34.07; w/ Rule-based Feedback KF1 = 37.41; w/ Self-criticism Feedback KF1 = 37.10 (Table 4). Additional ablations: combining utility functions with feedback-augmented prompting yields best results (Figure 5); policy variants (No-knowledge, Self-ask, Always-use) show Always-use highest KF1 but higher overhead; Self-ask better than No-knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8930.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8930.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-criticism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-criticism (using the LLM as a utility/critic to generate feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A form of model-based feedback where the LLM (ChatGPT) is prompted to evaluate its own candidate response and produce concrete feedback (self-critique) that the system uses to revise prompts and elicit improved answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (used both as generator and as a utility/critic in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT is used twice in some workflows: once to generate candidate responses and optionally again to self-evaluate/produce feedback (self-criticism) acting as a model-based utility function; parameters/size are not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-criticism (model-based utility feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt ChatGPT to evaluate a candidate response (given question, evidence, history) and give feedback on factuality/groundedness; use that generated textual feedback to re-prompt ChatGPT to produce a revised response that better aligns with the utility (e.g., KF1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Customer Service (DSTC11) and News Chat (DSTC7)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grounded conversational response generation tasks where responses should be aligned to external evidence; utility measured via Knowledge F1 and other text metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Customer Service (BM25): KF1 = 37.10% using self-criticism feedback (Table 4); News Chat and others show comparable improvements though rule-based feedback was slightly stronger in some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Customer Service (BM25): KF1 = 34.07% without feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based self-evaluation: ChatGPT is given (q, e, o, h_q) and asked to critique/generate feedback (f = Q_ψ(...)) which is then appended to the next prompt to revise the response. Implemented as part of the Utility module; can be template-based prompts to ChatGPT acting as a critic.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation (Table 4) shows self-criticism feedback improves KF1 from 34.07% → 37.10% (Customer Service BM25). The paper notes self-criticism provides more detailed suggestions than rule-based feedback and may be more helpful for complex tasks, though in these experiments it performed comparably to rule-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-criticism did not substantially outperform rule-based feedback in the reported experiments and was slightly lower in KF1 (37.10% vs 37.41%); generating feedback via the LLM incurs additional API cost/latency; dependence on the LLM's own evaluation can echo its biases or hallucinations if the critic is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to rule-based feedback and to the no-feedback baseline; rule-based feedback achieved marginally higher KF1 in Customer Service. The paper also evaluated a variant of prompting twice and re-ranking responses (different from explicit self-critique), which performed differently in ablations (slightly higher KF1 in one variant but worse than the combined utility+feedback approach).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 4: w/o Feedback KF1 = 34.07; w/ Rule-based Feedback KF1 = 37.41; w/ Self-criticism Feedback KF1 = 37.10. The authors note self-criticism gives more detailed suggestions and may be more beneficial on complex tasks, but in current tasks performs similarly to rule-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-domain question answering via chain of reasoning over heterogeneous knowledge <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Memory-assisted prompt editing to improve GPT-3 after deployment <em>(Rating: 2)</em></li>
                <li>Improving alignment of dialogue agents via targeted human judgements <em>(Rating: 1)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8930",
    "paper_id": "paper-e5c72b92c48d68594b290c84a8904da7c8335554",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "LLM-AUGMENTER",
            "name_full": "LLM-AUGMENTER (Large Language Model Augmenter)",
            "brief_description": "A modular framework that augments a fixed, black-box LLM (e.g., ChatGPT) with plug-and-play modules (Working Memory, Policy, Action Executor, Utility) to ground generation in external knowledge and iteratively revise responses using automated feedback until verification passes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (black-box LLM)",
            "model_description": "ChatGPT is used as a frozen, black-box, closed-book conversational LLM backbone in the experiments; the paper does not specify exact parameter counts or pretraining details for ChatGPT and treats it as an API-accessible model.",
            "reflection_method_name": "feedback-augmented iterative prompting (generate → verify → revise)",
            "reflection_method_description": "Generate a candidate response with the LLM given consolidated external evidence; the Utility module scores/ verifies the response (e.g., KF1 or recall) and produces textual feedback; the Prompt Engine revises the prompt by inserting feedback (and optionally updated evidence) and re-queries the LLM; this loop repeats until the candidate passes verification or some stopping condition.",
            "task_name": "Information-seeking dialog (News Chat DSTC7, Customer Service DSTC11) and Wiki QA (OTT-QA)",
            "task_description": "Dialog tasks requiring grounded responses based on external knowledge and an open-domain multi-hop Wiki question answering benchmark that requires reasoning over passages and tables.",
            "performance_with_reflection": "Customer Service (BM25 knowledge): KF1 = 37.41% with rule-based feedback; News Chat (BM25): KF1 = 36.41% with feedback; Wiki QA (CORE consolidated evidence): F1 = 11.80% with feedback.",
            "performance_without_reflection": "Customer Service (BM25): KF1 = 34.07% without feedback; News Chat (BM25): KF1 = 34.96% without feedback; Wiki QA (CORE): F1 = 8.08% without feedback.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Software-module-mediated iterative prompting: Utility module (model-based or rule-based) computes a utility score (e.g., KF1, recall) and generates verbalized feedback f = Q_ψ(q,e,o,h_q); Prompt Engine incorporates f and consolidated evidence into the LLM prompt; Working Memory stores state across iterations. Reflection is implemented via prompt engineering + external modules (retriever, entity-linker, chainer, utility) — no weights of the LLM are changed.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative gains when adding automated feedback: on Customer Service (BM25) KF1 increased from 34.07% → 37.41% (rule-based feedback) and 37.10% (self-criticism); on News Chat (BM25) KF1 increased 34.96% → 36.41%; on Wiki QA (CORE) F1 increased 8.08% → 11.80% when adding feedback. The paper also reports that combining utility functions and feedback-augmented prompting yields the best performance (Figure 5) and that trainable policy learning improves KF1 over random policy (Figure 3).",
            "limitations_or_failure_cases": "Iterative feedback increases latency and API cost (slower user experience since LLM often queried multiple times); experiments largely used a rule-based policy due to ChatGPT bandwidth constraints; RL policy training was done with T5-Base rather than the expensive ChatGPT in-loop; closed-book ChatGPT still hallucinates or abstains (e.g., ChatGPT abstained ≈17% on Wiki QA) and there remains a performance gap to fine-tuned models (Ma et al., 2022) with large top-k consolidated evidence; utility depends on correctness of external evidence retrieval—incorrect or missing evidence limits improvement.",
            "comparison_to_other_methods": "Compared against no-feedback baseline and rule-based feedback: feedback-augmented iterative prompting outperforms no-feedback. Table 4 shows rule-based feedback slightly outperforms the self-criticism variant in Customer Service (KF1 37.41 vs 37.10). The paper also compares variants such as prompting twice and re-ranking (which gave slightly higher KF1 than some baselines but performed worse than the feedback+utility combination) and ablates knowledge usage policies (No-knowledge, Self-ask, Always-use). Compared to fine-tuned systems (Ma et al., 2022) the augmented black-box approach still lags, attributed to lower consolidated evidence recall and required alignment for concise faithful reasoning.",
            "ablation_study_results": "Ablations isolating feedback effect (Customer Service BM25): w/o Feedback KF1 = 34.07; w/ Rule-based Feedback KF1 = 37.41; w/ Self-criticism Feedback KF1 = 37.10 (Table 4). Additional ablations: combining utility functions with feedback-augmented prompting yields best results (Figure 5); policy variants (No-knowledge, Self-ask, Always-use) show Always-use highest KF1 but higher overhead; Self-ask better than No-knowledge.",
            "uuid": "e8930.0",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Self-criticism",
            "name_full": "Self-criticism (using the LLM as a utility/critic to generate feedback)",
            "brief_description": "A form of model-based feedback where the LLM (ChatGPT) is prompted to evaluate its own candidate response and produce concrete feedback (self-critique) that the system uses to revise prompts and elicit improved answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (used both as generator and as a utility/critic in some experiments)",
            "model_description": "ChatGPT is used twice in some workflows: once to generate candidate responses and optionally again to self-evaluate/produce feedback (self-criticism) acting as a model-based utility function; parameters/size are not specified in the paper.",
            "reflection_method_name": "self-criticism (model-based utility feedback)",
            "reflection_method_description": "Prompt ChatGPT to evaluate a candidate response (given question, evidence, history) and give feedback on factuality/groundedness; use that generated textual feedback to re-prompt ChatGPT to produce a revised response that better aligns with the utility (e.g., KF1).",
            "task_name": "Customer Service (DSTC11) and News Chat (DSTC7)",
            "task_description": "Grounded conversational response generation tasks where responses should be aligned to external evidence; utility measured via Knowledge F1 and other text metrics.",
            "performance_with_reflection": "Customer Service (BM25): KF1 = 37.10% using self-criticism feedback (Table 4); News Chat and others show comparable improvements though rule-based feedback was slightly stronger in some runs.",
            "performance_without_reflection": "Customer Service (BM25): KF1 = 34.07% without feedback.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based self-evaluation: ChatGPT is given (q, e, o, h_q) and asked to critique/generate feedback (f = Q_ψ(...)) which is then appended to the next prompt to revise the response. Implemented as part of the Utility module; can be template-based prompts to ChatGPT acting as a critic.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Ablation (Table 4) shows self-criticism feedback improves KF1 from 34.07% → 37.10% (Customer Service BM25). The paper notes self-criticism provides more detailed suggestions than rule-based feedback and may be more helpful for complex tasks, though in these experiments it performed comparably to rule-based feedback.",
            "limitations_or_failure_cases": "Self-criticism did not substantially outperform rule-based feedback in the reported experiments and was slightly lower in KF1 (37.10% vs 37.41%); generating feedback via the LLM incurs additional API cost/latency; dependence on the LLM's own evaluation can echo its biases or hallucinations if the critic is imperfect.",
            "comparison_to_other_methods": "Compared directly to rule-based feedback and to the no-feedback baseline; rule-based feedback achieved marginally higher KF1 in Customer Service. The paper also evaluated a variant of prompting twice and re-ranking responses (different from explicit self-critique), which performed differently in ablations (slightly higher KF1 in one variant but worse than the combined utility+feedback approach).",
            "ablation_study_results": "Table 4: w/o Feedback KF1 = 34.07; w/ Rule-based Feedback KF1 = 37.41; w/ Self-criticism Feedback KF1 = 37.10. The authors note self-criticism gives more detailed suggestions and may be more beneficial on complex tasks, but in current tasks performs similarly to rule-based feedback.",
            "uuid": "e8930.1",
            "source_info": {
                "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-domain question answering via chain of reasoning over heterogeneous knowledge",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
            "rating": 2
        },
        {
            "paper_title": "Improving alignment of dialogue agents via targeted human judgements",
            "rating": 1
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        }
    ],
    "cost": 0.013094749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback ${ }^{\circ}$</h1>
<p>Baolin Peng ${ }^{\dagger}$ Michel Galley ${ }^{\dagger}$ Pengcheng $\mathrm{He}^{\dagger}$ Hao Cheng ${ }^{\dagger}$ Yujia Xie ${ }^{\dagger}$<br>Yu Hu ${ }^{\dagger}$ Qiuyuan Huang ${ }^{\dagger}$ Lars Liden ${ }^{\dagger}$ Zhou Yu ${ }^{\ddagger}$ Weizhu Chen ${ }^{\dagger}$ Jianfeng Gao ${ }^{\dagger}$<br>${ }^{\dagger}$ Microsoft Research ${ }^{\ddagger}$ Columbia University</p>
<h4>Abstract</h4>
<p>Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to realworld, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-AUGMENTER system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-AUGMENTER is empirically validated on two types of scenarios, taskoriented dialog and open-domain question answering. LLM-AUGMENTER significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Large Language models (LLMs), such as GPT-3 (Brown et al., 2020) and ChatGPT, have demonstrated an outstanding ability in generating fluent, coherent, and informative natural language texts. It is commonly understood that the impressive capabilities of these models stem from the abundance of world knowledge encoded therein and models' ability to generalize from that knowledge. However, the knowledge encoding of LLMs is lossy and the knowledge generalization could lead to "memory distortion." As a result, these models tend to hallucinate, which can cause damage when deployed for mission-critical tasks. In</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LLM-AUGMENTER improves a fixed LLM by (1) consolidating evidence from external knowledge for the LLM to generate responses grounded in evidence, and (2) revising LLM's (candidate) responses using automated feedback.
addition, even with exponentially growing model sizes, LLMs can never encode all information needed for many applications. For example, constant changes in real-world settings cause LLMs to quickly become stale for time-sensitive tasks such as news question answering, and many proprietary datasets are not available for LLM training due to privacy. While there is a growing interest in improving LLMs using external knowledge (e.g., Ghazvininejad et al., 2017; Guu et al., 2020; Zhong et al., 2022; Gao et al., 2019, 2022), almost all the previously proposed methods require finetuning the parameters of a LLM, which can be prohibitively expensive as the size of LLMs grows exponentially. Thus, it is highly desirable to augment a fixed LLM with plug-and-play (PnP) modules for mission-critical tasks.</p>
<p>In this paper, we present LLM-AUGMENTER to improve LLMs with external knowledge and automated feedback using PnP modules. As illustrated by the example in Figure 1, given a user query (e.g., regarding a 2013 Los Angeles</p>
<p>Galaxy player transfer), LLM-AUGMENTER first retrieves evidence from external knowledge (e.g., Web or task-specific datasets) and, if necessary, further consolidates evidence by linking retrieved raw evidence with related context (e.g., information of the entity "2013 Los Angeles Galaxy") and performing reasoning to form evidence chains (e.g., table-passage in the figure). Then, LLMAUGMENTER queries a fixed LLM (i.e., ChatGPT in our study) using a prompt that contains the consolidated evidence for ChatGPT to generate a candidate response grounded in external knowledge (evidence). LLM-AUGMENTER then verifies the candidate response e.g., by checking whether it hallucinates evidence. If so, LLM-AUGMENTER generates a feedback message (e.g., about the team "C.S.D. Municipal"). The message is used to revise the prompt to query ChatGPT again. The process iterates until a candidate response passes the verification and is sent to the user.</p>
<p>In addition to proposing LLM-AUGMENTER, to be detailed in Section 2, we make the following contributions. We perform an empirical study to validate the effectiveness of LLM-AUGMENTER using two tasks, information seeking dialog (Section 3) and open-domain Wiki question answering (Wiki QA) (Section 4). The study shows that LLM-AUGMENTER significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its generated responses. For example, on the dialog task of customer service, human evaluation shows LLMAUGMENTER improve ChatGPT by 32.3% in Usefulness (measuring the groundedness or hallucination of model responses) and 12.9% in Humanness (measuring the fluency and informativeness of model responses). The Wiki QA task is extremely challenging to ChatGPT in that answering these questions often requires multi-hop reasoning to piece together information of various modalities scattered across different documents. Our results show that although the closed-book ChatGPT performs poorly and often hallucinates, LLMAUGMENTER substantially improves the factuality score of the answers (absolute +10% in F1) by grounding ChatGPT's responses in consolidated external knowledge and automated feedback.</p>
<h2>2 LLM-AUGMENTER</h2>
<p>The architecture of LLM-AUGMENTER is illustrated in Figure 2. It consists of a set of PnP</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LLM-AUGMENTER architecture showing how its plug-and-play modules interact with the LLM and the user's environment.</p>
<p>modules (i.e., Working Memory, Policy, Action Executor, and Utility) to improve a fixed LLM (e.g., ChatGPT) with external knowledge and automated feedback to mitigate generation problems such as hallucination.</p>
<p>We formulate human-system conversation as a Markov Decision Process (MDP) described by a five-tuple (S, A, P, R, γ):</p>
<ul>
<li>S is an infinite set of dialog states, which encode information stored in Working Memory, including dialog history, user query, evidence, candidate response;</li>
<li>A is a set of actions that Policy picks to execute, including (1) calling Knowledge Consolidator to consolidate evidence from external knowledge and (2) calling Prompt Engine to query the LLM to generate candidate responses;</li>
<li>P(s′|s, a) gives the transition probability of entering a new state s′ after action a is taken in state s;</li>
<li>R(s, a) is the external reward received after taking action a in state s, which is provided by the environment (e.g., users or simulators); and</li>
<li>γ ∈ (0, 1] is a discount factor.</li>
</ul>
<p>In what follows, we describe in detail the modules of LLM-AUGMENTER.</p>
<h3>2.1 Working Memory</h3>
<p>This module tracks the dialog state that captures all essential information in the conversation so</p>
<p>far. The state is represented using a six-tuple $\left(q, e, o, u, f, h_{q}\right):$</p>
<ul>
<li>$q$ is the current user query;</li>
<li>$e$ is evidence for $q$, consolidated from external knowledge by Knowledge Consolidator;</li>
<li>$o$ is a set of the LLM-generated candidate responses for $q$;</li>
<li>$u$ is a score assessing the utility of each element of $o$, and $f$ is a verbalized feedback to guide the LLM to improve its utility - both $u$ and $f$ are generated by the Utility module (see Section 2.4); and</li>
<li>$h_{q}$ is the dialog history before $q$.</li>
</ul>
<p>Note that given user query $q$, LLMAUGMENTER can take multiple iterations to revise its response, with each iteration generating a candidate response based on evidence, feedback and utility, before sending the final response to the user, as illustrated in Figure 1.</p>
<h3>2.2 Policy</h3>
<p>This module selects the next system action that leads to the best expected reward $R$. These actions include (1) acquiring evidence $e$ for $q$ from external knowledge, (2) calling the LLM to generate a candidate response, and (3) sending a response to users if it passes the verification by the Utility module.</p>
<p>The policy can be implemented using manually crafted rules, or trained on human-system interactions. In this study, we implement a trainable policy $\pi$ as a neural network model parameterized by $\theta . \pi_{\theta}$ is optimized using REINFORCE (Williams, 1992) to maximize the expected reward as:</p>
<p>$$
\underset{\theta}{\operatorname{argmax}} \mathbb{E}<em _theta="\theta">{s \sim \mathcal{S}, a \sim \pi</em>[R(s, a)]
$$}</p>
<p>We find it effective to implement $\pi$ using a pre-trained model (e.g., T5), which allows us to not only leverage the capacity of the pre-trained model, but also to incorporate additional information through finetuning.</p>
<p>Policy learning typically requires large amounts of human-machine interactions, which can be costly to collect. To address the challenge, policy learning can be done in three stages:</p>
<ul>
<li>Bootstrapping from a rule-based policy: Domain experts encode task-specific knowledge and business logic into IF-THEN rules. For example, if a product name is mentioned in a user query for customer service, it is wise to always call Knowledge Consolidator to collect information of the product from a product database.</li>
<li>Learning with user simulators: We use a language model to simulate how human users interact with LLM-AUGMENTER. Any valid response from LLM-AUGMENTER that passes the evaluation of the Utility module can be used as a training example, allowing LLM-AUGMENTER to self-improve.</li>
<li>Finally, LLM-AUGMENTER interacts with human users to further refine its policy.</li>
</ul>
<p>In addition to Policy, the other trainable modules of LLM-AUGMENTER (i.e., Knowledge Consolidator and Utility) can also be optimized using the same learning method.</p>
<h3>2.3 Action Executor</h3>
<p>This module performs an action selected by the policy. It is composed of two components, Knowledge Consolidator and Prompt Engine.</p>
<h3>2.3.1 Knowledge Consolidator</h3>
<p>The Knowledge Consolidator augments LLMs with the capability of grounding their responses on external knowledge to mitigate hallucination when completing tasks, such as answering questions regarding latest news, and booking a table in a restaurant. Following (Ma et al., 2022), the Knowledge Consolidator is designed in a modular fashion, consisting of a knowledge retriever, an entity linker and, an evidence chainer.</p>
<p>Specifically, the retriever first generates a set of search queries based on $q$ and $h_{q}$, and then calls a set of APIs to retrieve raw evidence from various external knowledge sources, such as calling Bing Search APIs to query Web documents including Wiki articles and Reddit messages, and REST APIs to query task-specific databases for restaurant reviews and product specifications.</p>
<p>The retrieved raw evidence is sometimes incomplete and noisy. Thus, the entity linker enriches raw evidence with related context to form evidence graphs, i.e., linking each entity mentioned in raw evidence to its corresponding description</p>
<p>based on Wikipedia. Then, the chainer prunes irrelevant evidence from the graphs and forms a shortlist of evidence chains that are most relevant to queries. The consolidated evidence $e$ is then sent to Working Memory. Figure 1 shows an example of consolidated evidence for the anchored club "Los Angeles Galaxy", i.e., two evidence chains corresponding to the transfer players in 2013 season and the former clubs, respectively.</p>
<h3>2.3.2 Prompt Engine</h3>
<p>The Prompt Engine generates a prompt to query the LLM to generate a (candidate) response $o$ for $q$. The prompt is a text string that consists of task instruction, user query $q$, dialog history $h_{q}$, evidence $e$ if it is made available by Knowledge Consolidator, and feedback $f$ if it is made available by the Utility module. Prompts are task-specific, and details thereof are provided in Appendix A.</p>
<h3>2.4 Utility</h3>
<p>Given a candidate response $o$, the Utility module generates utility score $u$ and a corresponding feedback $f$ using a set of task-specific utility functions.</p>
<p>These utility functions ${ }^{2}$ access the alignment of the LLM's responses with user expectations or specific business requirements. For example, in an information seeking dialog, it is important that all LLM's responses are preciously grounded in external evidence to avoid generating misleading or inaccurate information. In a restaurant reservation dialog, the LLM responses should be conversational and focused on guiding the user through the reservation process, rather than engaging in offtopic chitchats.</p>
<p>Inspired by Glaese et al. (2022), there can be two distinct types of utility functions:</p>
<ul>
<li>Model-based utility functions assign preference scores to different dimensions of a response, such as fluency, informativeness and factuality. These functions are trained on precollected human preference data or annotated log data.</li>
<li>Rule-based utility functions, implemented using heuristics or programmed functions, measure whether a response complies with a specific rule.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In addition, we have developed a utility function to generate informative and actionable feedback to help revise prompts to allow the LLM to generate better responses. As shown in Figure 1, the utility function generates feedback "but there is no information about the number of international titles." Such a utility function is a text generation model $Q$ parameterized by $\psi$, and can be implemented as a seq2seq or auto-regression language model. It tasks as input user query $q$, evidence $e$, candidate response $o$ and dialog history $h_{q}$, and generates feedback in text $f$ as</p>
<p>$$
f=Q_{\psi}\left(q, e, o, h_{q}\right)
$$</p>
<p>Alternatively, LLMs and rule-based natural language generator can be used for feedback generation.</p>
<p>In the next two sections, we present our experiments to validate the effectiveness of LLMAUGMENTER in two types of distinct scenarios: (1) information seeking dialog, where the AI agent needs to generate informative and trustworthy responses based on a variety of external sources of knowledge, and (2) Wiki question answering, where the AI agent needs to answer questions by piecing together information of various modalities scattered among multiple Wiki documents.</p>
<h2>3 Information Seeking Dialog</h2>
<h3>3.1 Datasets</h3>
<p>News Chat: We repurpose the DSTC7 Track 2 task as an evaluation corpus for news conversation. The goal of this task is to generate informative responses that are grounded in external knowledge (i.e., news) and go beyond chitchat. We followed the data crawling process used in DSTC7 Task 2 (Galley et al., 2019). We started by selecting Reddit discussion threads that contained URLs in the description, which were crawled from various news-related subreddits during the time period of 2021-2022. We then restricted the URL domain to a curated list of news websites, and extracted the relevant oracle passage by selecting the most appropriate passage for the context based on ROUGE-F1 scores (Lin, 2004). In order to reduce noisy or irrelevant information, we only kept examples with an F1 score higher than a certain threshold, resulting in a total of 1370 examples for evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">K.C.</th>
<th style="text-align: center;">Feedback</th>
<th style="text-align: center;">KF1 $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">ROUGE $\uparrow$</th>
<th style="text-align: center;">chrF $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">BERTScore $\uparrow$</th>
<th style="text-align: center;">BARTScore $\uparrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">Avg. length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.71</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">16.78</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">7.34</td>
<td style="text-align: center;">82.14</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">26.98</td>
<td style="text-align: center;">58.94</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">34.96</td>
<td style="text-align: center;">6.71</td>
<td style="text-align: center;">22.25</td>
<td style="text-align: center;">27.02</td>
<td style="text-align: center;">9.35</td>
<td style="text-align: center;">$\mathbf{8 3 . 4 6}$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">26.89</td>
<td style="text-align: center;">46.74</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{3 6 . 4 1}$</td>
<td style="text-align: center;">$\mathbf{7 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 1 7}$</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">$\mathbf{0 . 3 5}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 7 1}$</td>
<td style="text-align: center;">54.24</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">57.44</td>
<td style="text-align: center;">19.24</td>
<td style="text-align: center;">38.89</td>
<td style="text-align: center;">40.02</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">86.65</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">44.35</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">60.76</td>
<td style="text-align: center;">21.49</td>
<td style="text-align: center;">40.56</td>
<td style="text-align: center;">42.14</td>
<td style="text-align: center;">18.50</td>
<td style="text-align: center;">86.89</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">42.15</td>
<td style="text-align: center;">47.19</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation scores (in \%) and average response lengths for the News Chat (DSTC7) dataset. BM25: Each model retrieves 5 knowledge snippets from the corresponding knowledge source. K.C. denotes Knowledge Consolidator.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">K.C.</th>
<th style="text-align: center;">Feedback</th>
<th style="text-align: center;">KF1 $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">ROUGE $\uparrow$</th>
<th style="text-align: center;">chrF $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">BERTScore $\uparrow$</th>
<th style="text-align: center;">BARTScore $\uparrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">Avg. length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.33</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">24.02</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">12.83</td>
<td style="text-align: center;">87.88</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">$\mathbf{4 7 . 9 9}$</td>
<td style="text-align: center;">28.81</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">34.07</td>
<td style="text-align: center;">$\mathbf{4 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 5 2}$</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">13.61</td>
<td style="text-align: center;">$\mathbf{8 7 . 9 6}$</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">47.21</td>
<td style="text-align: center;">32.65</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{3 7 . 4 1}$</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">$\mathbf{3 0 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 7 4}$</td>
<td style="text-align: center;">87.58</td>
<td style="text-align: center;">$\mathbf{2 . 0 9}$</td>
<td style="text-align: center;">44.71</td>
<td style="text-align: center;">45.07</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">45.63</td>
<td style="text-align: center;">6.54</td>
<td style="text-align: center;">29.77</td>
<td style="text-align: center;">33.32</td>
<td style="text-align: center;">16.93</td>
<td style="text-align: center;">89.35</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">54.38</td>
<td style="text-align: center;">33.04</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Augmenter</td>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">5.63</td>
<td style="text-align: center;">29.65</td>
<td style="text-align: center;">35.68</td>
<td style="text-align: center;">18.66</td>
<td style="text-align: center;">89.01</td>
<td style="text-align: center;">3.14</td>
<td style="text-align: center;">52.49</td>
<td style="text-align: center;">45.09</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation scores (in \%) and average response lengths for the Customer Service (DSTC11) dataset. BM25: Each model retrieves 5 knowledge snippets from the corresponding knowledge source. K.C. denotes Knowledge Consolidator.</p>
<p>Customer Service: We use DSTC11 Track 5 (Kim et al., 2023) as a showcase in a conversational customer service scenario. It expands upon the DSTC9 Track 1 dataset by incorporating subjective knowledge from customer reviews in addition to factual knowledge from FAQs. This allows users to have an engaging and informative conversational experience with the AI system. The dataset evaluates the ability of the AI agent to understand relevant user review posts and FAQs, and generate responses based on both reviews and FAQ snippets. It is collected based on the MultiWOZ 2.1 (Eric et al., 2019) dataset and includes users' knowledge-seeking queries that require the AI agent to use FAQs and user reviews to respond. There are 14768 dialog sessions for training and validation, and the test set is currently unavailable. Therefore, we used the validation set for our evaluations.</p>
<h3>3.2 Experiment Setup</h3>
<p>Language Model: Throughout this work, we focus on using ChatGPT as the backbone blackbox LLM. It is straightforward to apply LLMAUGMENTER to other LLMs, such as GPT3 (Brown et al., 2020) or PaLM (Chowdhery et al., 2022).</p>
<p>Knowledge Consolidator: For News Chat, Knowledge Consolidator includes a BM25 retriever over web documents linked from Reddit posts. For the Customer Service task, Knowl-
edge Consolidator includes a BM25-based retriever over the knowledge bases of FAQs and Yelp reviews.</p>
<p>Additionally, we also experiment with groundtruth knowledge, referred to as golden knowledge henceforth, which is used by human annotators during data collection, in our oracle experiments.</p>
<p>Prompt Engine: The prompt templates utilized for News Chat and Customer Service are shown in the appendix in Table 7 and Table 8, respectively.</p>
<p>Utility: The goal of this task is to generate responses that are coherent to the context and grounded in external knowledge. To evaluate the degree to which the generated responses are grounded in consolidated evidence, we use the utility score, Knowledge F1 (Shuster et al., 2021), to measure the overlap between a prediction and evidence which is either consolidated by Knowledge Consolidator or provided as golden knowledge. Feedback generation is accomplished using a template-based natural language generator. ${ }^{3}$ In addition, we use ChatGPT as a utility function, i.e., self-criticism to gather feedback by prompting ChatGPT to evaluate candidate responses and give feedback on how to improve them.</p>
<p>Policy: Due to ChatGPT's current limited bandwidth, we use a rule-based policy for our ex-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>periments involving ChatGPT. The prior knowledge about this task inspired us to design a policy that always uses Knowledge Consolidator, evaluates the quality of a candidate response using ChatGPT, and provides feedback to revise the prompt. Additionally, to test the viability of LLM-AUGMENTER with a trainable policy, we employ offline RL to train the parameters of Policy as Equation 1, where the policy model is based on T5-Base.</p>
<p>Evaluation: We evaluate the performance of LLM-AUGMENTER on information-seeking dialog tasks using both automatic metrics and human evaluations. Following the literature, we consider commonly used metrics, Knowledge F1 (KF1) and BLEU-4, in grounded conversational response generation and task-oriented dialog. BLEU (Papineni et al., 2002) measures the overlap between the model's output and the ground-truth human response, while KF1 (Lian et al., 2019) assesses the overlap with the knowledge that the human used as a reference during dataset collection. Additionally, we include ROUGE-1 (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) as these metrics have been found to best correlate with human judgment on the DSTC9 and DSTC11 customer support tasks (Kim et al., 2020). We further include BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019), chrF (Popović, 2015), which have been shown to be among the best-performing text generation metrics on dialog(Yeh et al., 2021; Peng et al., 2022). Lastly, we also consider BARTScore as it has been reported to be one of the best model-based metrics (Yuan et al., 2021). Given that BARTScore can be interpreted as a log-probability, we report results with its natural exponent (positive scores). Additionally, we perform a turn-level human evaluation to investigate whether responses are (1) useful and (2) humanlike. Following the evaluation protocol by (Peng et al., 2022), using Amazon Mechanical Turk, we hired master-level workers with lifetime HIT acceptance rate above $95 \%$, and asked them to answer two questions on usefulness (i.e., which response sounds more useful) and humanness (i.e., which speaker sounds more human).</p>
<h3>3.3 Automatic Evaluation Results</h3>
<p>The impact of using external knowledge: Experiment results are shown in Tables 1 and 2. We observe that ChatGPT achieves reasonable perfor-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning curve of ChatGPT-Companion with T5-Base as the policy module. The solid curves are the mean and the shaded regions are the maximum and minimum utility scores over 5 runs.
mance even in the zero-shot setting. However, with access to golden knowledge, the performance is dramatically improved. This suggests that while LLMs are able to encode a large amount of general knowledge in their parameters, they can still benefit from more specific, targeted knowledge. This is likely because LLMs are designed to handle a wide range of tasks and therefore may not always have access to the most relevant or up-to-date information for a given task. Our experiments show that providing LLMs with task-specific knowledge can significantly mitigate hallucination without sacrificing the fluency and informativeness of model-generated responses. As demonstrated in Tables 1 and 2, LLM-AUGMENTER mitigates ChatGPT's hallucination issue on both the news chat and customer service tasks. Specifically, we observe a significant improvement in KF1 scores of approximately 10 and 6 points, respectively, due to the use of evidence retrieved by Knowledge Consolidator.</p>
<p>The impact of using automated feedback: As listed in Tables 1 and 2, the results of using golden knowledge setting demonstrate that incorporating feedback from the Utility module leads to substantial improvement 3.3 points in KF1 on News Chat and 7.2 on Customer Service, respectively. Similarly, significant improvement can also be observed when using evidence provided by Knowledge Consolidator.</p>
<p>The impact of using trainable Policy: Figure 3 shows the learning curve of LLM-AUGMENTER on the customer service task. As we do not have an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation studies on different policies of LLM-AUGMENTER in Customer Service scenario.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Usefulness ↑</th>
<th>Humanness ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>34.07</td>
<td>30.92</td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td><strong>45.07</strong></td>
<td><strong>35.22</strong></td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation of ChatGPT and LLMAUGMENTER with BM25 in Customer Service scenario. All differences are significant (p &lt; 0.05). Inter-annotator agreements according Krippendorff's alpha (interval metric) are 0.15 and 0.07 respectively.</p>
<p>external reward that would require collecting data from real users, we instead define here our reward R as the KF1 utility function. This helps demonstrate the effectiveness of LLM-AUGMENTER in its reinforcement learning (RL) setup. As our experiments are akin to single turn interactions, we did not need to set discount factor γ, but future work may need to rely on it. We see that LLMAUGMENTER's reward on test data increases as the number of training episodes (dialog sessions) increases, surpassing a random policy after 600 interactions and ultimately reaching a KF1 score of approximately 37.5. Through these interactions, LLM-AUGMENTER is able to learn to effectively select the next system action to maximize the reward, which helps our system reduce hallucinations while generating fluent and informative responses.</p>
<h3>3.4 Human Evaluation Results</h3>
<p>We compare ChatGPT with and without LLMAUGMENTER. A total of 948 randomly selected examples from the customer service dataset are used for human evaluation. The evaluation results are converted from a 5-point Likert-like scale to a win/tie/loss scale for reporting, as shown</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: LLM-AUGMENTER benefits from the combination of using utility function and iterative improvement with feedback. The x-axis indicates the average number of ChatGPT prompting and the y-axis is the KF1. The studies are conducted in the Customer Service scenario with knowledge being provided by BM25.</p>
<p>in Table 3. We observe a strong preference for LLM-AUGMENTER over ChatGPT alone in terms of both Usefulness and Humanness. The result is consistent with the automatic evaluation result, discussed earlier.</p>
<h3>3.5 Ablation Study</h3>
<p>We conduct ablation experiments to evaluate the effect of various policies on the utilization of the knowledge consolidator. Figure 4 shows the performance of three different variants of the policy: 1) no-knowledge consolidator, in which the knowledge consolidator is not used, 2) Self-ask, in which the knowledge consolidator is only utilized when the LM suggests the use of external knowledge by prompting it whether to use, and (3) Always-use, in which the knowledge consolidator is always provided to the LM. Our results indicate that Self-ask policy achieves a significantly better KF1 score than the No-knowledge consolidator policy, with the ChatGPT model unable to answer user queries and suggesting knowledge consolidator access for 24% of examples. However, the Always-use policy, while achieving the best KF1 score, also incurred additional overhead in terms of knowledge consolidator access. These observations suggest that a trainable policy model should be employed to learn when to use external knowledge.</p>
<p>In addition, the evaluation results on the impact of different types of feedback for LLM-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feedback</th>
<th style="text-align: left;">KF1 $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">w/o Feedback</td>
<td style="text-align: left;">34.07</td>
</tr>
<tr>
<td style="text-align: left;">w/ Rule-based Feedback</td>
<td style="text-align: left;">37.41</td>
</tr>
<tr>
<td style="text-align: left;">w/ Self-criticism Feedback</td>
<td style="text-align: left;">37.10</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation studies on different feedback of LLM-AUGMENTER in Customer Service scenario.</p>
<p>Augmenter are listed in Table 4. We observe that self-criticism feedback enhances response quality make it more knowledge-grounded. Although its performance is comparable to that of rule-based feedback, it provides more detailed suggestions. We speculate that self-criticism will be more helpful for complex tasks. Some examples can be found in 6 .</p>
<p>To understand the impact of utility functions and feedback-augmented prompting on the performance of LLM-AUGMENTER, we conduct an analysis by turning each component on and off. Figure 5 illustrates the results of each variant. We observe that the combination of using utility functions and feedback-augmented prompting, i.e., 2 , achieves the best performance. In addition, always providing feedback (as shown in (4) also enhances the performance, although it requires additional model prompting. (5) represents prompting ChatGPT twice and re-ranking the response based on the utility functions, which results in a slightly higher KF1, but performs significantly worse than 2. These findings suggest that incorporating both utility functions and feedback is a more effective method for improving the alignment of LLMs.</p>
<h2>4 Wiki QA</h2>
<p>Instead of conversational evaluations, we focus on stress tests on ChatGPT here using open-domain question answering. As ChatGPT and other LLMs are mostly trained using abundant text from single web pages, we hypothesize that answering multi-hop questions involving scattered information across different pages/modalities can better serve the purpose. Due to this, closed-book LLMs are more likely to hallucinate. Moreover, the complex step-by-step reasoning can even be challenging for existing search systems to gather all necessary support evidence in one-shot. Thus, more advanced knowledge consolidation techniques are essential to elicit LLMs for proper grounding.</p>
<p>Lastly, different from conversational tasks where long-form responses are desirable, we mainly consider questions with concise short-form answers, i.e., there exists a significant style shift in responses. To align ChatGPT to this new scenario with distinct characteristics, extra instructions are needed.</p>
<h3>4.1 Dataset</h3>
<p>OTT-QA: The OTT-QA dataset is an opendomain question answering benchmark that considers multi-step joint reasoning over both tabular and textual information. It consists of around 40K instances built upon Wikipedia, including 400K tables and 6 M passages as the knowledge source. Solving the questions in OTT-QA requires diverse reasoning skills and can be divided into three categories: single-hop questions (13\%), two-hop questions ( $57 \%$ ), and multi-hop questions ( $30 \%$ ). In this paper, we denote the dataset as Wiki QA.</p>
<h3>4.2 Experiment Setups</h3>
<p>In the following, we describe the experimental setup for Wiki QA. Unless specified otherwise, the setups are identical to those used in Section 3.</p>
<p>Knowledge Consolidator: Here, the Knowledge Consolidator uses Wikipedia passages and tables as the knowledge source. Instead of using BM25 as done for dialog tasks, we resort to a dense model, DPR (Karpukhin et al., 2020), as the backbone retriever. For DPR, both question and passage/table inputs are represented by the corresponding special token [CLS] embeddings from their respective encoders, and retrieval is simply done via maximum inner product search in the vector space. Given a question, we use DPR to obtain the initial set of evidence, which includes tables and passages. As most WikiQA questions require reasoning hops across different pieces of information (e.g., hopping from the album table to its entry artist page in Figure 1), we contend that directly feeding this raw evidence set to Working Memory is insufficient for prompting LLMs. Thus, we further use additional intermediary modules, i.e., linker and chainer, from CORE (Ma et al., 2022) to consolidate the raw evidence, including connecting relevant documents, reranking evidence, and splicing them into evidence chains. We refer to Ma et al. (2022) for more details.</p>
<p>Prompt Engine: The prompt templates utilized for Wiki QA is shown in the appendix in Table 9.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Knowledge Consolidator</th>
<th>Feedback</th>
<th>Wiki QA</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>P $\uparrow$</td>
<td>R $\uparrow$</td>
<td>F1 $\uparrow$</td>
<td></td>
</tr>
<tr>
<td>ChatGPT</td>
<td>-</td>
<td>-</td>
<td>0.48</td>
<td>1.52</td>
<td>0.59</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>DPR</td>
<td>✗</td>
<td>2.08</td>
<td>4.31</td>
<td>2.38</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>CORE</td>
<td>✗</td>
<td>7.06</td>
<td>14.77</td>
<td>8.08</td>
<td></td>
</tr>
<tr>
<td>LLM-AUGMENTER</td>
<td>CORE</td>
<td>✓</td>
<td>8.93</td>
<td>33.87</td>
<td>11.80</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation results on Wiki QA. Each model retrieves top-5 knowledge snippets from the corresponding knowledge source. The top-5 answer recall of consolidated evidence (CORE) is 50.83.</p>
<p>Utility: Here, as a response to a given question is deemed to leverage information from the consolidated knowledge, we use recall as the utility score, i.e., preferring responses with higher token overlap with the corresponding evidence set. Similar to Section 3, we again consider a template-based natural language generator for giving feedback to ChatGPT.</p>
<p>Evaluation Metrics: As WikiQA mainly concerns short-form answers, we evaluate the generated responses using the token-level precision, recall and F1 scores against the annotated answers.</p>
<h3>4.3 Results</h3>
<p>Table 5 presents the evaluation results on Wiki QA. As expected, the closed-book model alone performs very poorly. Based on our manual inspections, we find that most error cases are hallucinated answers and ChatGPT abstains from answering for $17\%$ cases. We observe that incorporating knowledge obtained from either DPR or CORE significantly improves the F1 score. The substantial improvements observed over the closed-book ChatGPT model indicate the importance of enhancing LLMs with external knowledge. Compared with raw evidence from DPR (row 2), we observe that consolidated evidence from our proposed Knowledge Consolidator with CORE (row 3) is more useful to the frozen ChatGPT model, achieving more pronounced improvements across the board. This suggests that it is crucial to consolidate knowledge for eliciting blackbox LLMs to perform grounded reasoning. Lastly, consistent with the observations for news chat and customer service scenarios in Section 3, augmenting ChatGPT with automated feedback further improves alignments (adapting ChatGPT to perform multi-step grounded reasoning), leading to a substantial increase in recall and F1 scores.</p>
<p>Compared with the state-of-the-art fine-tuned model <em>Ma et al. (2022)</em> using top-50 consolidated evidence, there still remains a noticeable gap in performance. Besides a lower answer recall of the consolidated evidence, we attribute it to extra alignments required for ChatGPT to respond in a more concise way and conduct faithful step-by-step reasoning. Therefore, there is ample room for future explorations on elicitive prompting to achieve further improvements.</p>
<h2>5 Related Work</h2>
<p>Numerous LLMs for text generation <em>Radford et al. (2018)</em> have been proposed over the years, including very competitive ones such as GPT-3 <em>Brown et al. (2020); Ouyang et al. (2022)</em>, OPT <em>Zhang et al. (2022)</em>, GPT-j <em>Wang and Komatsuzaki (2021)</em>, and ChatGPT. However, most of them do not naturally incorporate external knowledge. To address this limitation, various works augment LLMs with knowledge consisting of e.g., personalized recommendations <em>Ghazvininejad et al. (2017)</em>, Wikipedia article and web search <em>Dinan et al. (2018); Shuster et al. (2022)</em>, structured and unstructured knowledge of task-oriented dialog <em>Peng et al. (2022)</em>. Recent advances have focused on jointly finetuning the retriever and generation components of retrieval-augmented text generation systems <em>Lewis et al. (2020); Zhang et al. (2021)</em>, but these methods are not applicable to black-box LLMs.</p>
<p>More recent work attempts to combine blackbox LLMs with external knowledge, such as incorporating external knowledge into prompts <em>Madaan et al. (2022); Lazaridou et al. (2022)</em>, making GPT-3 more faithful <em>He et al. (2022)</em>, and combining web knowledge with GPT-3 <em>Nakano et al. (2021)</em>. In very recent works related to ours, <em>Shi et al. (2023)</em> tune the ranker of a black-box LLM. <em>Schick et al. (2023)</em> tune black-box LLMs’</p>
<p>access to different APIs and show improvement on a variety of understanding and reasoning tasks. We consider these works to complementary to ours, as we assume our set of APIs to be given and fixed, and we instead focus more on when and what APIs to request, interactive feedback with the LLM, and developing a self-learning ability through utility functions.</p>
<h2>6 Limitations and Future Directions</h2>
<p>A main limitation of this work is that interactive feedback with a computationally expensive model such as ChatGPT can significantly slow down the user experience, as ChatGPT is often queried twice for a single response. However, we think this can translate into more choice for the user. For example, the initial ChatGPT response can be shown to the user as it is being decoded, and the user could then be informed that a more accurate response is available (depending on the utility function). Then, an impatient user can decide to ignore this option, while a user more mindful of response accuracy may decide to see the improved ChatGPT response. In task-oriented and high-stakes scenarios, we believe many users would prefer the slower but more accurate option.</p>
<p>The main results of the paper are with a policy designed manually, as due to the current highdemand for ChatGPT and its limited bandwidth. As reinforcement learning can be quite sample inefficient, we trained our policy using an LLM (T5Base) we could easily query, and these RL experiments demonstrate the effectiveness of LLMAUGMENTER. As ChatGPT becomes more available, we plan to update the paper with RL experiments involving ChatGPT. The current version of the paper does not include human evaluation, as the goal with our current utility function (KF1) shown we can make ChatGPT more grounded and our experiments suggest the responses of our best system are better at capturing the words of the (gold) knowledge. As we move to towards much utility functions such as safety, it will be important to add more fine-grained analyzes of the responses, and we will add human evaluation. In future work, we also plan to leverage interactions with real users and user feedbacks to train LLMAUGMENTER.</p>
<h2>7 Conclusions</h2>
<p>We introduced LLM-AUGMENTER, a framework for augmenting black-box LLMs (e.g., ChatGPT) with external knowledge and automated feedback. The external knowledge provided as part of the LLM prompts helps generate more responses that are more grounded into external knowledge relevant to the current conversation. The automated feedback elicits the "follow-up correction" abilities of models such as ChatGPT and InstructGPT in order to produce revised responses that rank higher according to some given utility functions (e.g., groundedness as measured by KF1). These various components are integrated together as part of an RL framework, which we optimize end-to-end using policy gradient. End-toend experiments with T5 show the effectiveness of LLM-AUGMENTER, while experiments on ChatGPT show significant increases both in terms of KF1 and a host of text generation metrics.</p>
<h2>Ethics Statement</h2>
<p>It is widely understood that large language models have the potential to generate harmful, offensive, and inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Weidinger et al., 2021). This paper is an attempt to address a major harm of LLMs, namely factual integrity. This paper does not address the problem of offensive content generation, but future work on LLMAUGMENTER could help mitigate such harm via, e.g., offensiveness-related utility functions.</p>
<p>As with other knowledge-augmented text generation applications, we cannot rule out that external sources could compromise the factuality of generated text. It is, therefore, important to encourage users to check the relevance of external sources that supplement the generated text.</p>
<h2>Acknowledgements</h2>
<p>We thank Saleema Amershi, Ahmed Awadallah, Nguyen Bach, Paul Bennett, Chris Brockett, Weixin Cai, Dhivya Eswaran, Adam Fourney, Hsiao-Wuen Hon, Chunyuan Li, Ricky Loynd, Hoifung Poon, Corby Rosset, Bin Yu, Sheng Zhang, and members of the Microsoft Research Deep Learning group for valuable discussions and comments.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario</p>
<p>Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations.</p>
<p>Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur. 2019. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669.</p>
<p>Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. 2019. Grounded response generation task at DSTC7. In AAAI Dialog System Technology Challenges Workshop.</p>
<p>Jianfeng Gao, Michel Galley, and Lihong Li. 2019. Neural approaches to conversational AI. Foundations and Trends in Information Retrieval, 13(23):127-298.</p>
<p>Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick Craswell. 2022. Neural approaches to conversational information retrieval. arXiv preprint arXiv:2201.05176.</p>
<p>Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model. CoRR, abs/1702.01932.</p>
<p>Amelia Glaese, Nathan McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy CampbellGillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, A. See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sovna Mokr'a, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William S. Isaac, John F. J. Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. ArXiv, abs/2209.14375.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909.</p>
<p>Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Seokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, and Dilek HakkaniTur. 2020. Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 278-289.</p>
<p>Seokhwan Kim, Spandana Gella, Di Jin, Alexandros Papangelis, Behnam Hedayatnia, Yang Liu, and Dilek Hakkani-Tür. 2023. DSTC11 track proposal: Task-oriented conversational modeling with subjective knowledge. https://github.com/ alexa/dstc11-track5.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401.</p>
<p>Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, and Hua Wu. 2019. Learning to select knowledge for response generation in dialog systems. In International Joint Conference on Artificial Intelligence.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In ACL workshop, pages $74-81$.</p>
<p>Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022. Open-domain question answering via chain of reasoning over heterogeneous knowledge. arXiv preprint arXiv:2210.12338.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deployment. arXiv preprint arXiv:2201.06009.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In $A C L$, pages $311-318$.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Lidén, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. 2022. GODEL: Largescale pre-training for goal-directed dialog. ArXiv, abs/2206.11309.</p>
<p>Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrievalaugmented black-box language models. ArXiv, abs/2301.12652.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.</p>
<p>Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, W.K.F. Ngan, Spencer Poff, Naman Goyal, Arthur D. Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. ArXiv, abs/2208.03188.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. ArXiv, abs/2112.04359.</p>
<p>Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256.</p>
<p>Yi-Ting Yeh, Maxine Eskenazi, and Shikib Mehri. 2021. A comprehensive assessment of dialog evaluation metrics. In arXiv.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer language models. ArXiv, abs/2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating text generation with BERT. CoRR, abs/1904.09675.</p>
<p>Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, and Bill Dolan. 2021. RetGen: A joint framework for retrieval and grounded text generation modeling. In AAAI Conference on Artificial Intelligence.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. ArXiv, abs/2205.12674.</p>
<h2>A Appendix</h2>
<p>Table 6 provides sample responses contrasting ChatGPT and LLM-Augmenter. First, we can see that ChatGPT fails to provide a response related to specific knowledge related to the user, e.g., a local Indian restaurant. In the second part of the table, we show LLM-Augmenter's Working Memory, which highlights the richer information retrieved from external knowledge to help the underling LLM (i.e., ChatGPT as well) generate more contentful responses. The first LLM response received by LLM-AUGMENTER is unfortunately not satisfactory, as the quality and specificity of LLM generation can be unpredictable. In this case, the Utility module has determined that the first response did not meet its criteria (i.e., KF1 above a given threshold), and issues a feedback to the LLM module (i.e., "response is inconsistent with the knowledge"). The second response received by LLM-AUGMENTER is much more satisfactory according to the utility function, and therefore sent to the user.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Instruction</th>
<th style="text-align: left;">I want you to act as a chatbot AI for travel planning. You need to <br> answer customer's questions nicely.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Context</td>
<td style="text-align: left;">User: Hello, I am looking for a moderately priced restaurant on the <br> north side of town. <br> Assistant: I recommend golden wok. It is in the north part of town <br> with a moderate price range. <br> User: Yes, I am looking for a moderately priced restaurant in the <br> north part of town. <br> Assistant: I recommend golden wok. It is in the north part of town <br> with a moderate price range. <br> User: I don't want Golden Wok, is there anything else?, <br> Assistant: Yes, your other option is The Nirala. It serves Indian food <br> and is also located in the north part of town. <br> User: Is their beer a good value for the money?</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT Response</td>
<td style="text-align: left;">I am sorry, I do not have information on the beer prices at The Nirala. <br> It is best to contact the restaurant directly for more information.</td>
</tr>
<tr>
<td style="text-align: left;">Working Memory</td>
<td style="text-align: left;">Review: They also have a modest drinks menu with some afford- <br> able prices, but I was more interested in the beautiful view from my <br> window seat, which allowed me to watch the sun setting as I dined. <br> Q: Is alcohol served at this restaurant? A: Yes, alcohol is served at <br> this restaurant. <br> Review: I found them to be overpriced and mediocre. <br> Review: The drinks ae priced well. <br> Review: Something that I disliked, however was the prices for their <br> drinks.</td>
</tr>
<tr>
<td style="text-align: left;">LLM-AUGMENTER Response</td>
<td style="text-align: left;">I am sorry, I do not have information on the value of their beer. I only <br> have information on the location and price range of the restaurant.</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based Feedback</td>
<td style="text-align: left;">The response is inconsistent with the knowledge. Please generate <br> again?</td>
</tr>
<tr>
<td style="text-align: left;">LLM-AUGMENTER Response</td>
<td style="text-align: left;">Some reviews mention that the drinks at the restaurant are priced <br> well, while others mention that they found the prices to be overpriced <br> and mediocre.</td>
</tr>
<tr>
<td style="text-align: left;">Human Response</td>
<td style="text-align: left;">Unfortunately the reviews are split, half say they are overpriced and <br> have say they are affordable.</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompt and example responses of different models for the customer service scenarios.</p>
<p>I want you to act as a chatbot. You need to answer user' questions nicely.
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・.
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
I want you to act as a chatbot. You will be presented with knowledge snippets. You need to answer user' questions nicely and accurately based on the knowledge snippets.
Working Memory: $\left[M_{t}\right]$
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
Table 7: Prompt Templates for News Chat. LLMs generated responses is highlighted with [].</p>
<p>I want you to act as a chatbot AI for travel planning. You need to answer customer's questions nicely.
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
I want you to act as a chatbot AI for travel planning. You will be presented with knowledge snippets. You need to answer customer's questions nicely and accurately based on the knowledge snippets.
Working Memory: $\left[M_{t}\right]$
Context:
User: $\left[U_{1}\right]$
Assistant: $\left[R_{1}\right]$
・・
User: $\left[U_{t}\right]$
Assistant: $\left[R_{t}\right]$
Table 8: Prompt Templates for Customer Service. LLMs generated response are highlighted with [].</p>
<p>I am a highly intelligent question answering bot that can answer questions. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with "Unknown".
Question: $[Q]$
Answer: $[A]$
I am a highly intelligent question answering bot, and can answer questions given some documents and tables. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with "Unknown".
Working Memory: $[M]$
Question: $[Q]$
Answer: $[A]$
Table 9: Prompt Templates for Wiki QA. LLMs generated response are highlighted with [].</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ If the KF1 score falls below a certain threshold, the feedback is "The response is inconsistent with the knowledge. Please generate again."&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
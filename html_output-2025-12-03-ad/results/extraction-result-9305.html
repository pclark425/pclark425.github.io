<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9305 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9305</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9305</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-278911793</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.21354v2.pdf" target="_blank">Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9305.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9305.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (standard / CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo (evaluated with standard prompting and Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 series model evaluated on Bengali Math Word Problems (SOMADHAN) and PatiGonit using standard and Chain-of-Thought (CoT) prompts in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (complex Bengali MWPs); PatiGonit (equation-based Bengali MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>SOMADHAN: complex Bengali grade-school math problems with step-by-step solutions (evaluate final-answer accuracy). PatiGonit: simpler equation-based Bengali MWPs (evaluate equation match).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and Chain-of-Thought (CoT) prompting; evaluated in zero-shot and few-shot (5-shot) settings; few-shot examples included CoT steps for the few-shot experiments; temperature set to 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting (no intermediate steps) compared to CoT prompting (explicit intermediate reasoning); zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SOMADHAN CoT few-shot: accuracy 30.0%; SOMADHAN CoT zero-shot: 24.0%; SOMADHAN standard few-shot: 24.0%; SOMADHAN standard zero-shot: 23.0%; PatiGonit few-shot (standard): 86.0% (equation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (30.0%) vs standard few-shot (24.0%) on SOMADHAN (difference reported as +6.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+6.0% accuracy (CoT few-shot vs standard few-shot on SOMADHAN)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper attributes improvement to CoT enabling stepwise decomposition of multi-step problems; CoT provides intermediate reasoning that guides final-answer correctness for complex MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SOMADHAN split used (800 train, 3200 test in reported experiments on annotated subset); evaluation focuses on final-answer equality; few-shot used 5 examples; fine-tuning experiment used 50 examples for GPT-3.5 (see separate finetune entry); due to token costs only 1000 test samples were used in evaluation runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9305.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (standard / CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI, evaluated with standard and Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal OpenAI GPT-4o model evaluated on SOMADHAN and PatiGonit with standard and CoT prompts in zero-shot and few-shot settings, showing strong reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported in paper as ≈1.3 trillion (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN and PatiGonit</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>SOMADHAN: complex Bengali MWPs final-answer accuracy; PatiGonit: equation-based accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and Chain-of-Thought (CoT) prompting; zero-shot and few-shot (5-shot with CoT examples); temperature set to 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting; zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SOMADHAN standard zero-shot: 79.0%; standard few-shot: 80.0%; CoT zero-shot: 80.4%; CoT few-shot: 83.0%. PatiGonit few-shot (standard): 99.0% (equation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (83.0%) vs standard few-shot (80.0%) on SOMADHAN (+3.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.0% accuracy (CoT few-shot vs standard few-shot on SOMADHAN)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT offers modest but consistent gains by eliciting intermediate reasoning steps; GPT-4o's large capacity already yields strong zero-shot performance, but CoT further refines multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 exemplars including CoT; fine-tuning for GPT-4o was not performed in reported experiments; evaluation compares final-answer equality; reported zero-shot standard performance already high (79%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9305.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.3 (70B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter LLaMA-3.3 model (Meta) that achieved the best reported accuracy on SOMADHAN when prompted with few-shot Chain-of-Thought examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (complex Bengali MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate final-answer accuracy on complex Bengali MWPs requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and Chain-of-Thought prompting; tested in zero-shot and few-shot (5-shot) settings; also tested with two prompt styles (Prompt Style 1 and Prompt Style 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting; Prompt Style 1 vs Prompt Style 2; zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SOMADHAN CoT few-shot (Prompt Style 2): 88% (top reported); CoT few-shot (Prompt Style 1): 87%; standard few-shot: 78%; standard zero-shot reported 78% (without CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (87–88%) vs standard few-shot (78%) -> reported +9% to +10% improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+9% (Prompt Style 1) to +10% (Prompt Style 2) accuracy gain for CoT few-shot vs standard few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger-capacity LLaMA-3.3 benefits substantially from CoT and from prompt design (Prompt Style 2 improved language alignment and accuracy); CoT helps break multi-step reasoning into solvable steps, producing large gains on complex MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 examples with CoT steps; Prompt Style 2 was introduced to improve LLaMA models' Bengali outputs and yielded the highest accuracy for Llama-3.3; evaluation uses final-answer equality; due to token cost only a subset (1000 test samples) was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9305.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 70B (negative CoT effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3 (70B parameters) evaluated under standard and CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter LLaMA-3 model that showed decreased accuracy on SOMADHAN when Chain-of-Thought prompting was applied compared to standard prompting in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (complex Bengali MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on multi-step Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting vs Chain-of-Thought prompting in zero-shot and few-shot; language output noted (English vs Bengali shift).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard zero-shot: 76.0% (highest reported for that model with standard prompting); CoT (zero-shot & few-shot): 65.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT (65.0%) vs standard zero-shot (76.0%) -> -11.0% absolute drop.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-11.0% (CoT caused a decrease in accuracy vs standard prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests language adaptation (responses shifted to Bengali under CoT) and the interplay with model behavior can reduce numeric accuracy for some LLaMA variants; indicates CoT isn't uniformly beneficial — model size and prompt style matter.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt Style 1 used in initial experiments; responses under standard prompting were in English while CoT produced Bengali outputs; few-shot used 5 exemplars; evaluation by final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9305.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 8B (negative CoT effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3 (8B parameters) evaluated with standard and CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller 8B-parameter LLaMA-3 model that suffered large accuracy drops when Chain-of-Thought prompting was applied on Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (complex Bengali MWPs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on complex Bengali math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and CoT prompting; evaluated in zero-shot and few-shot (5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting; zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard zero-shot: 47.0%; CoT zero-shot: 24.0%; CoT few-shot: 19.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (19.0%) vs standard zero-shot (47.0%) -> -28.0% absolute drop.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-23% to -28% (substantial negative effect of CoT vs standard prompting depending on setting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper hypothesizes that smaller models have insufficient capacity to benefit from CoT in Bengali; CoT can hurt performance when the model cannot reliably produce accurate intermediate reasoning in the target language.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 exemplars; note that standard prompting responses were sometimes in English whereas CoT tended to produce Bengali responses; evaluation uses final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9305.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1 (70B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>70B LLaMA-3.1 variant that modestly improved with CoT in zero-shot but showed no CoT gains in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on Bengali complex MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard and CoT prompting; zero-shot and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CoT zero-shot: 80.0% (reported best); this is +2% over standard zero-shot (78.0%); few-shot CoT showed no improvement over standard few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT zero-shot (80.0%) vs standard zero-shot (78.0%) -> +2.0%; CoT few-shot ≈ standard few-shot (no gain).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.0% in zero-shot; no effect in few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests CoT may help some large variants in zero-shot by eliciting reasoning, but few-shot exemplar choice and prompt style can saturate gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 examples; prompt styles explored; evaluation used final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9305.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2 90B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.2 (90B text-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger LLaMA-3.2 (90B) preview model that showed improvements with CoT in zero-shot but degraded in few-shot when CoT was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>90B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on complex Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and CoT prompting; zero-shot and few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting vs CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CoT zero-shot: 79.0% (reported best); described as +4% over standard zero-shot. CoT few-shot decreased by 3% compared to standard few-shot (exact few-shot percentages not explicitly enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT zero-shot vs standard zero-shot: +4.0%; CoT few-shot: -3.0% relative to standard few-shot (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+4.0% (zero-shot CoT gain) and -3.0% (few-shot CoT loss)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that CoT benefits can be setting-dependent: it helped zero-shot reasoning but interfered with few-shot exemplar-driven behavior for this model, suggesting interactions between exemplars and CoT instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 examples; prompt style variation applied; evaluation via final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9305.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-r1-distill-qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-r1-distill-qwen (32B, distilled from Qwen-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled Qwen-32B variant from Deepseek evaluated on SOMADHAN showing moderate gains from CoT few-shot prompting but producing English outputs for the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-r1-distill-qwen</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on complex Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and Chain-of-Thought prompting; few-shot (5-shot) and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot vs CoT few-shot (also zero-shot variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard few-shot: 44.0%; CoT few-shot: 51.0% (reported best).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (51.0%) vs standard few-shot (44.0%) -> +7.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+7.0% accuracy (CoT few-shot vs standard few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT improved reasoning-equation generation but model still produced answers in English; language output alignment remained a separate issue from CoT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses were English despite CoT; prompt style 2 was used in later experiments; few-shot used 5 exemplars; evaluation on SOMADHAN final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9305.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-r1-distill-llama-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-r1-distill-llama (70B, distilled from LLaMA-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled LLaMA-70B variant from Deepseek that gained accuracy with CoT few-shot prompting and produced Bengali outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-r1-distill-llama-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard and CoT prompting, few-shot (5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot vs CoT few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard few-shot: 60.0%; CoT few-shot: 66.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (66.0%) vs standard few-shot (60.0%) -> +6.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+6.0% accuracy (CoT few-shot vs standard few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT elicited better intermediate reasoning and improved final-answer generation; language alignment to Bengali was successful for this distilled variant.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 exemplars including CoT reasoning; prompt style 2 improved some models; evaluation uses final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9305.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5 32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5 (32B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qwen-2.5 32B model evaluated with standard and CoT prompting, showing modest gains from CoT few-shot prompting on SOMADHAN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on complex Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting and Chain-of-Thought prompting; few-shot (5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot vs CoT few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Standard few-shot: 68.0%; CoT few-shot: 71.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT few-shot (71.0%) vs standard few-shot (68.0%) -> +3.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.0% accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT marginally improved multi-step reasoning; prompt engineering and exemplar choice affect gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 5 exemplars with CoT steps; language output alignment noted for some models; evaluation via final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9305.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-qwq 32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-qwq (32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Qwen variant evaluated with CoT and few-shot prompting that failed to produce Bengali outputs despite CoT prompting; accuracy numbers not explicitly provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-qwq</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on Bengali MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard and CoT prompting; few-shot (5-shot) used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot vs CoT few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper notes that this model could not generate Bengali responses even with CoT, indicating CoT does not ensure language adaptation; no reliable accuracy numbers were provided in the text for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses remained in English despite CoT; specific accuracy metrics not reported in the paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9305.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo fine-tuned with Chain-of-Thought supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 fine-tuned on 50 CoT-annotated examples (JSONL) and evaluated on SOMADHAN, showing modest gains that plateaued and sometimes decreased with more epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (as stated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (fine-tuning experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy after supervised fine-tuning using examples containing chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Fine-tuning with 50 CoT-annotated examples (JSONL format) using OpenAI fine-tune API; then evaluated with standard zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Fine-tuned zero-shot vs non-fine-tuned zero-shot (standard prompting); also compared epoch counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Best fine-tuned result: 23.4% accuracy (learning rate 0.1, 10 epochs). Performance decreased to 12.6% with 15 epochs (and equivalent to 5 epochs), indicating instability/overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Fine-tuned 23.4% vs non-fine-tuned GPT-3.5 zero-shot baseline (reported earlier as 23.0% standard zero-shot) -> marginal uptick in best configuration; longer training led to degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Best reported +0.4% vs baseline, but more epochs degraded performance (to 12.6%), indicating non-monotonic effect of fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning with limited CoT examples can help but is sensitive to hyperparameters and number of epochs; risk of overfitting and diminishing returns noted.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tune used 50 examples, varied epochs (5,10,15) and learning rates; JSONL format; evaluation used standard zero-shot prompting after fine-tuning; training details (learning rate, epochs) reported in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9305.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9305.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA finetune variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA) finetuning configurations (three ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three LoRA-based fine-tuning configurations (Baseline, Higher Rank with Dropout, Memory-Efficient) applied to a model on SOMADHAN; these parameter-efficient finetunes yielded small absolute accuracies and showed configuration-dependent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA-finetuned LLM (configurations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SOMADHAN (LoRA fine-tuning ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Final-answer accuracy on SOMADHAN after parameter-efficient LoRA fine-tuning with different ranks, dropout, and training schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>LoRA fine-tuning with three configurations: Baseline (rank=16, dropout=0), Higher Rank w/ Dropout (rank=32, dropout=0.1), Memory-Efficient (lower batch size, higher grad-accum and lr). All used CoT examples during LoRA training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across three LoRA configurations (Baseline vs Higher-Rank w/Dropout vs Memory-Efficient).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (Finetune 1): 13.0% accuracy; Higher Rank w/ Dropout (Finetune 2): 12.0%; Memory-Efficient (Finetune 3): 17.0% (best among three).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Memory-Efficient (17.0%) vs Baseline (13.0%) -> +4.0% absolute; Higher-Rank w/Dropout performed slightly worse (12.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+4.0% (best LoRA config vs baseline LoRA config)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests training dynamics (batch size, accumulation, learning rate) and LoRA hyperparameters can materially affect outcomes; more parameters (higher rank) with dropout did not help, possibly due to over-parameterization or data scarcity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LoRA target modules and hyperparameters enumerated in Table 5 (r, lora_alpha, dropout, target modules, learning rate, batch size, grad accumulation, max steps); evaluation used SOMADHAN final-answer equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>DIVERSE (Diverse Verifier on Reasoning Step) <em>(Rating: 1)</em></li>
                <li>GSM8K (Grade School Math) dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9305",
    "paper_id": "paper-278911793",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (standard / CoT)",
            "name_full": "GPT-3.5 Turbo (evaluated with standard prompting and Chain-of-Thought prompting)",
            "brief_description": "OpenAI GPT-3.5 series model evaluated on Bengali Math Word Problems (SOMADHAN) and PatiGonit using standard and Chain-of-Thought (CoT) prompts in zero-shot and few-shot settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": "≈175B (as stated in paper)",
            "task_name": "SOMADHAN (complex Bengali MWPs); PatiGonit (equation-based Bengali MWPs)",
            "task_description": "SOMADHAN: complex Bengali grade-school math problems with step-by-step solutions (evaluate final-answer accuracy). PatiGonit: simpler equation-based Bengali MWPs (evaluate equation match).",
            "presentation_format": "Standard prompting and Chain-of-Thought (CoT) prompting; evaluated in zero-shot and few-shot (5-shot) settings; few-shot examples included CoT steps for the few-shot experiments; temperature set to 1.",
            "comparison_format": "Standard prompting (no intermediate steps) compared to CoT prompting (explicit intermediate reasoning); zero-shot vs few-shot.",
            "performance": "SOMADHAN CoT few-shot: accuracy 30.0%; SOMADHAN CoT zero-shot: 24.0%; SOMADHAN standard few-shot: 24.0%; SOMADHAN standard zero-shot: 23.0%; PatiGonit few-shot (standard): 86.0% (equation accuracy).",
            "performance_comparison": "CoT few-shot (30.0%) vs standard few-shot (24.0%) on SOMADHAN (difference reported as +6.0%).",
            "format_effect_size": "+6.0% accuracy (CoT few-shot vs standard few-shot on SOMADHAN)",
            "explanation_or_hypothesis": "Paper attributes improvement to CoT enabling stepwise decomposition of multi-step problems; CoT provides intermediate reasoning that guides final-answer correctness for complex MWPs.",
            "null_or_negative_result": false,
            "experimental_details": "SOMADHAN split used (800 train, 3200 test in reported experiments on annotated subset); evaluation focuses on final-answer equality; few-shot used 5 examples; fine-tuning experiment used 50 examples for GPT-3.5 (see separate finetune entry); due to token costs only 1000 test samples were used in evaluation runs.",
            "uuid": "e9305.0",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o (standard / CoT)",
            "name_full": "GPT-4o (OpenAI, evaluated with standard and Chain-of-Thought prompting)",
            "brief_description": "Large multimodal OpenAI GPT-4o model evaluated on SOMADHAN and PatiGonit with standard and CoT prompts in zero-shot and few-shot settings, showing strong reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": "reported in paper as ≈1.3 trillion (as stated in paper)",
            "task_name": "SOMADHAN and PatiGonit",
            "task_description": "SOMADHAN: complex Bengali MWPs final-answer accuracy; PatiGonit: equation-based accuracy.",
            "presentation_format": "Standard prompting and Chain-of-Thought (CoT) prompting; zero-shot and few-shot (5-shot with CoT examples); temperature set to 1.",
            "comparison_format": "Standard prompting vs CoT prompting; zero-shot vs few-shot.",
            "performance": "SOMADHAN standard zero-shot: 79.0%; standard few-shot: 80.0%; CoT zero-shot: 80.4%; CoT few-shot: 83.0%. PatiGonit few-shot (standard): 99.0% (equation accuracy).",
            "performance_comparison": "CoT few-shot (83.0%) vs standard few-shot (80.0%) on SOMADHAN (+3.0%).",
            "format_effect_size": "+3.0% accuracy (CoT few-shot vs standard few-shot on SOMADHAN)",
            "explanation_or_hypothesis": "CoT offers modest but consistent gains by eliciting intermediate reasoning steps; GPT-4o's large capacity already yields strong zero-shot performance, but CoT further refines multi-step reasoning.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 5 exemplars including CoT; fine-tuning for GPT-4o was not performed in reported experiments; evaluation compares final-answer equality; reported zero-shot standard performance already high (79%).",
            "uuid": "e9305.1",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.3 70B",
            "name_full": "LLaMA-3.3 (70B parameters)",
            "brief_description": "A 70B-parameter LLaMA-3.3 model (Meta) that achieved the best reported accuracy on SOMADHAN when prompted with few-shot Chain-of-Thought examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.3",
            "model_size": "70B",
            "task_name": "SOMADHAN (complex Bengali MWPs)",
            "task_description": "Evaluate final-answer accuracy on complex Bengali MWPs requiring multi-step reasoning.",
            "presentation_format": "Standard prompting and Chain-of-Thought prompting; tested in zero-shot and few-shot (5-shot) settings; also tested with two prompt styles (Prompt Style 1 and Prompt Style 2).",
            "comparison_format": "Standard prompting vs CoT prompting; Prompt Style 1 vs Prompt Style 2; zero-shot vs few-shot.",
            "performance": "SOMADHAN CoT few-shot (Prompt Style 2): 88% (top reported); CoT few-shot (Prompt Style 1): 87%; standard few-shot: 78%; standard zero-shot reported 78% (without CoT).",
            "performance_comparison": "CoT few-shot (87–88%) vs standard few-shot (78%) -&gt; reported +9% to +10% improvement.",
            "format_effect_size": "+9% (Prompt Style 1) to +10% (Prompt Style 2) accuracy gain for CoT few-shot vs standard few-shot",
            "explanation_or_hypothesis": "Larger-capacity LLaMA-3.3 benefits substantially from CoT and from prompt design (Prompt Style 2 improved language alignment and accuracy); CoT helps break multi-step reasoning into solvable steps, producing large gains on complex MWPs.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 5 examples with CoT steps; Prompt Style 2 was introduced to improve LLaMA models' Bengali outputs and yielded the highest accuracy for Llama-3.3; evaluation uses final-answer equality; due to token cost only a subset (1000 test samples) was evaluated.",
            "uuid": "e9305.2",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3 70B (negative CoT effect)",
            "name_full": "LLaMA-3 (70B parameters) evaluated under standard and CoT prompting",
            "brief_description": "A 70B-parameter LLaMA-3 model that showed decreased accuracy on SOMADHAN when Chain-of-Thought prompting was applied compared to standard prompting in some settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3",
            "model_size": "70B",
            "task_name": "SOMADHAN (complex Bengali MWPs)",
            "task_description": "Final-answer accuracy on multi-step Bengali MWPs.",
            "presentation_format": "Standard prompting vs Chain-of-Thought prompting in zero-shot and few-shot; language output noted (English vs Bengali shift).",
            "comparison_format": "Standard prompting vs CoT prompting.",
            "performance": "Standard zero-shot: 76.0% (highest reported for that model with standard prompting); CoT (zero-shot & few-shot): 65.0%.",
            "performance_comparison": "CoT (65.0%) vs standard zero-shot (76.0%) -&gt; -11.0% absolute drop.",
            "format_effect_size": "-11.0% (CoT caused a decrease in accuracy vs standard prompting)",
            "explanation_or_hypothesis": "Paper suggests language adaptation (responses shifted to Bengali under CoT) and the interplay with model behavior can reduce numeric accuracy for some LLaMA variants; indicates CoT isn't uniformly beneficial — model size and prompt style matter.",
            "null_or_negative_result": true,
            "experimental_details": "Prompt Style 1 used in initial experiments; responses under standard prompting were in English while CoT produced Bengali outputs; few-shot used 5 exemplars; evaluation by final-answer equality.",
            "uuid": "e9305.3",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3 8B (negative CoT effect)",
            "name_full": "LLaMA-3 (8B parameters) evaluated with standard and CoT prompting",
            "brief_description": "A smaller 8B-parameter LLaMA-3 model that suffered large accuracy drops when Chain-of-Thought prompting was applied on Bengali MWPs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3",
            "model_size": "8B",
            "task_name": "SOMADHAN (complex Bengali MWPs)",
            "task_description": "Final-answer accuracy on complex Bengali math problems.",
            "presentation_format": "Standard prompting and CoT prompting; evaluated in zero-shot and few-shot (5-shot).",
            "comparison_format": "Standard prompting vs CoT prompting; zero-shot vs few-shot.",
            "performance": "Standard zero-shot: 47.0%; CoT zero-shot: 24.0%; CoT few-shot: 19.0%.",
            "performance_comparison": "CoT few-shot (19.0%) vs standard zero-shot (47.0%) -&gt; -28.0% absolute drop.",
            "format_effect_size": "-23% to -28% (substantial negative effect of CoT vs standard prompting depending on setting)",
            "explanation_or_hypothesis": "Paper hypothesizes that smaller models have insufficient capacity to benefit from CoT in Bengali; CoT can hurt performance when the model cannot reliably produce accurate intermediate reasoning in the target language.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot used 5 exemplars; note that standard prompting responses were sometimes in English whereas CoT tended to produce Bengali responses; evaluation uses final-answer equality.",
            "uuid": "e9305.4",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1 70B",
            "name_full": "LLaMA-3.1 (70B parameters)",
            "brief_description": "70B LLaMA-3.1 variant that modestly improved with CoT in zero-shot but showed no CoT gains in few-shot settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1",
            "model_size": "70B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on Bengali complex MWPs.",
            "presentation_format": "Standard and CoT prompting; zero-shot and few-shot.",
            "comparison_format": "Standard prompting vs CoT prompting.",
            "performance": "CoT zero-shot: 80.0% (reported best); this is +2% over standard zero-shot (78.0%); few-shot CoT showed no improvement over standard few-shot.",
            "performance_comparison": "CoT zero-shot (80.0%) vs standard zero-shot (78.0%) -&gt; +2.0%; CoT few-shot ≈ standard few-shot (no gain).",
            "format_effect_size": "+2.0% in zero-shot; no effect in few-shot",
            "explanation_or_hypothesis": "Paper suggests CoT may help some large variants in zero-shot by eliciting reasoning, but few-shot exemplar choice and prompt style can saturate gains.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 5 examples; prompt styles explored; evaluation used final-answer equality.",
            "uuid": "e9305.5",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.2 90B",
            "name_full": "LLaMA-3.2 (90B text-preview)",
            "brief_description": "A larger LLaMA-3.2 (90B) preview model that showed improvements with CoT in zero-shot but degraded in few-shot when CoT was applied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.2",
            "model_size": "90B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on complex Bengali MWPs.",
            "presentation_format": "Standard prompting and CoT prompting; zero-shot and few-shot experiments.",
            "comparison_format": "Standard prompting vs CoT prompting.",
            "performance": "CoT zero-shot: 79.0% (reported best); described as +4% over standard zero-shot. CoT few-shot decreased by 3% compared to standard few-shot (exact few-shot percentages not explicitly enumerated).",
            "performance_comparison": "CoT zero-shot vs standard zero-shot: +4.0%; CoT few-shot: -3.0% relative to standard few-shot (reported).",
            "format_effect_size": "+4.0% (zero-shot CoT gain) and -3.0% (few-shot CoT loss)",
            "explanation_or_hypothesis": "Authors note that CoT benefits can be setting-dependent: it helped zero-shot reasoning but interfered with few-shot exemplar-driven behavior for this model, suggesting interactions between exemplars and CoT instructions.",
            "null_or_negative_result": null,
            "experimental_details": "Few-shot used 5 examples; prompt style variation applied; evaluation via final-answer equality.",
            "uuid": "e9305.6",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Deepseek-r1-distill-qwen-32B",
            "name_full": "Deepseek-r1-distill-qwen (32B, distilled from Qwen-32B)",
            "brief_description": "A distilled Qwen-32B variant from Deepseek evaluated on SOMADHAN showing moderate gains from CoT few-shot prompting but producing English outputs for the dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deepseek-r1-distill-qwen",
            "model_size": "32B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on complex Bengali MWPs.",
            "presentation_format": "Standard prompting and Chain-of-Thought prompting; few-shot (5-shot) and zero-shot.",
            "comparison_format": "Standard few-shot vs CoT few-shot (also zero-shot variants).",
            "performance": "Standard few-shot: 44.0%; CoT few-shot: 51.0% (reported best).",
            "performance_comparison": "CoT few-shot (51.0%) vs standard few-shot (44.0%) -&gt; +7.0%.",
            "format_effect_size": "+7.0% accuracy (CoT few-shot vs standard few-shot)",
            "explanation_or_hypothesis": "CoT improved reasoning-equation generation but model still produced answers in English; language output alignment remained a separate issue from CoT gains.",
            "null_or_negative_result": false,
            "experimental_details": "Responses were English despite CoT; prompt style 2 was used in later experiments; few-shot used 5 exemplars; evaluation on SOMADHAN final-answer equality.",
            "uuid": "e9305.7",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Deepseek-r1-distill-llama-70B",
            "name_full": "Deepseek-r1-distill-llama (70B, distilled from LLaMA-70B)",
            "brief_description": "A distilled LLaMA-70B variant from Deepseek that gained accuracy with CoT few-shot prompting and produced Bengali outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deepseek-r1-distill-llama-70B",
            "model_size": "70B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on Bengali MWPs.",
            "presentation_format": "Standard and CoT prompting, few-shot (5-shot).",
            "comparison_format": "Standard few-shot vs CoT few-shot.",
            "performance": "Standard few-shot: 60.0%; CoT few-shot: 66.0%.",
            "performance_comparison": "CoT few-shot (66.0%) vs standard few-shot (60.0%) -&gt; +6.0%.",
            "format_effect_size": "+6.0% accuracy (CoT few-shot vs standard few-shot)",
            "explanation_or_hypothesis": "CoT elicited better intermediate reasoning and improved final-answer generation; language alignment to Bengali was successful for this distilled variant.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 5 exemplars including CoT reasoning; prompt style 2 improved some models; evaluation uses final-answer equality.",
            "uuid": "e9305.8",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5 32B",
            "name_full": "Qwen-2.5 (32B parameters)",
            "brief_description": "Qwen-2.5 32B model evaluated with standard and CoT prompting, showing modest gains from CoT few-shot prompting on SOMADHAN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5",
            "model_size": "32B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on complex Bengali MWPs.",
            "presentation_format": "Standard prompting and Chain-of-Thought prompting; few-shot (5-shot).",
            "comparison_format": "Standard few-shot vs CoT few-shot.",
            "performance": "Standard few-shot: 68.0%; CoT few-shot: 71.0%.",
            "performance_comparison": "CoT few-shot (71.0%) vs standard few-shot (68.0%) -&gt; +3.0%.",
            "format_effect_size": "+3.0% accuracy",
            "explanation_or_hypothesis": "CoT marginally improved multi-step reasoning; prompt engineering and exemplar choice affect gains.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 5 exemplars with CoT steps; language output alignment noted for some models; evaluation via final-answer equality.",
            "uuid": "e9305.9",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-qwq 32B",
            "name_full": "Qwen-qwq (32B)",
            "brief_description": "A Qwen variant evaluated with CoT and few-shot prompting that failed to produce Bengali outputs despite CoT prompting; accuracy numbers not explicitly provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-qwq",
            "model_size": "32B",
            "task_name": "SOMADHAN",
            "task_description": "Final-answer accuracy on Bengali MWPs.",
            "presentation_format": "Standard and CoT prompting; few-shot (5-shot) used.",
            "comparison_format": "Standard few-shot vs CoT few-shot.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Paper notes that this model could not generate Bengali responses even with CoT, indicating CoT does not ensure language adaptation; no reliable accuracy numbers were provided in the text for this model.",
            "null_or_negative_result": null,
            "experimental_details": "Responses remained in English despite CoT; specific accuracy metrics not reported in the paper's text.",
            "uuid": "e9305.10",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-3.5 (fine-tuned)",
            "name_full": "GPT-3.5 Turbo fine-tuned with Chain-of-Thought supervision",
            "brief_description": "GPT-3.5 fine-tuned on 50 CoT-annotated examples (JSONL) and evaluated on SOMADHAN, showing modest gains that plateaued and sometimes decreased with more epochs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (fine-tuned)",
            "model_size": "≈175B (as stated)",
            "task_name": "SOMADHAN (fine-tuning experiment)",
            "task_description": "Final-answer accuracy after supervised fine-tuning using examples containing chain-of-thought reasoning.",
            "presentation_format": "Fine-tuning with 50 CoT-annotated examples (JSONL format) using OpenAI fine-tune API; then evaluated with standard zero-shot prompts.",
            "comparison_format": "Fine-tuned zero-shot vs non-fine-tuned zero-shot (standard prompting); also compared epoch counts.",
            "performance": "Best fine-tuned result: 23.4% accuracy (learning rate 0.1, 10 epochs). Performance decreased to 12.6% with 15 epochs (and equivalent to 5 epochs), indicating instability/overfitting.",
            "performance_comparison": "Fine-tuned 23.4% vs non-fine-tuned GPT-3.5 zero-shot baseline (reported earlier as 23.0% standard zero-shot) -&gt; marginal uptick in best configuration; longer training led to degradation.",
            "format_effect_size": "Best reported +0.4% vs baseline, but more epochs degraded performance (to 12.6%), indicating non-monotonic effect of fine-tuning.",
            "explanation_or_hypothesis": "Fine-tuning with limited CoT examples can help but is sensitive to hyperparameters and number of epochs; risk of overfitting and diminishing returns noted.",
            "null_or_negative_result": null,
            "experimental_details": "Fine-tune used 50 examples, varied epochs (5,10,15) and learning rates; JSONL format; evaluation used standard zero-shot prompting after fine-tuning; training details (learning rate, epochs) reported in Table 6.",
            "uuid": "e9305.11",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LoRA finetune variants",
            "name_full": "Low-Rank Adaptation (LoRA) finetuning configurations (three ablations)",
            "brief_description": "Three LoRA-based fine-tuning configurations (Baseline, Higher Rank with Dropout, Memory-Efficient) applied to a model on SOMADHAN; these parameter-efficient finetunes yielded small absolute accuracies and showed configuration-dependent effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LoRA-finetuned LLM (configurations reported)",
            "model_size": null,
            "task_name": "SOMADHAN (LoRA fine-tuning ablation)",
            "task_description": "Final-answer accuracy on SOMADHAN after parameter-efficient LoRA fine-tuning with different ranks, dropout, and training schedules.",
            "presentation_format": "LoRA fine-tuning with three configurations: Baseline (rank=16, dropout=0), Higher Rank w/ Dropout (rank=32, dropout=0.1), Memory-Efficient (lower batch size, higher grad-accum and lr). All used CoT examples during LoRA training.",
            "comparison_format": "Comparison across three LoRA configurations (Baseline vs Higher-Rank w/Dropout vs Memory-Efficient).",
            "performance": "Baseline (Finetune 1): 13.0% accuracy; Higher Rank w/ Dropout (Finetune 2): 12.0%; Memory-Efficient (Finetune 3): 17.0% (best among three).",
            "performance_comparison": "Memory-Efficient (17.0%) vs Baseline (13.0%) -&gt; +4.0% absolute; Higher-Rank w/Dropout performed slightly worse (12.0%).",
            "format_effect_size": "+4.0% (best LoRA config vs baseline LoRA config)",
            "explanation_or_hypothesis": "Paper suggests training dynamics (batch size, accumulation, learning rate) and LoRA hyperparameters can materially affect outcomes; more parameters (higher rank) with dropout did not help, possibly due to over-parameterization or data scarcity.",
            "null_or_negative_result": null,
            "experimental_details": "LoRA target modules and hyperparameters enumerated in Table 5 (r, lora_alpha, dropout, target modules, learning rate, batch size, grad accumulation, max steps); evaluation used SOMADHAN final-answer equality.",
            "uuid": "e9305.12",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "DIVERSE (Diverse Verifier on Reasoning Step)",
            "rating": 1,
            "sanitized_title": "diverse_diverse_verifier_on_reasoning_step"
        },
        {
            "paper_title": "GSM8K (Grade School Math) dataset",
            "rating": 1,
            "sanitized_title": "gsm8k_grade_school_math_dataset"
        }
    ],
    "cost": 0.0215085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEVERAGING LARGE LANGUAGE MODELS FOR BENGALI MATH WORD PROBLEM SOLVING WITH CHAIN OF THOUGHT REASONING
July 31, 2025</p>
<p>Bidyarthi Paul bidyarthipaul01@gmail.com 
Department of Computer Science and Engineering
Ahsanullah University of Science and Technology Tejgaon
Dhaka</p>
<p>Jalisha Jashim 
Department of Computer Science and Engineering
Ahsanullah University of Science and Technology Tejgaon
Dhaka</p>
<p>Mirazur Rahman Zim 
Department of Computer Science and Engineering
Ahsanullah University of Science and Technology Tejgaon
Dhaka</p>
<p>Sattar Aothoi 
Department of Computer Science and Engineering
Ahsanullah University of Science and Technology Tejgaon
Dhaka</p>
<p>Faisal Muhammad Shah faisal.cse@aust.edu 
Department of Computer Science and Engineering
Ahsanullah University of Science and Technology Tejgaon
Dhaka</p>
<p>LEVERAGING LARGE LANGUAGE MODELS FOR BENGALI MATH WORD PROBLEM SOLVING WITH CHAIN OF THOUGHT REASONING
July 31, 2025AE992444E3D1C4C746F883C538052C10arXiv:2505.21354v2[cs.CL]
Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required.Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task.This gap has limited progress in Bengali mathematical reasoning.To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions.We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context.Using SOMADHAN, we evaluated a range of large language models (LLMs)-including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen-through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning.CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic.LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting.We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost.Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs.We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.</p>
<p>Introduction</p>
<p>In this era, Question Answering (QA) systems are vital in Natural Language Processing (NLP).They are designed to understand and respond to user queries in a human-like manner, crucial for applications like search engines and virtual assistants ( [1], [2]).Improving these systems are essential due to the growing demand for quick and accurate information retrieval.The structure of QA systems varies with the domain and question types ( [3], [4]), with Math Word Problems (MWPs) being a notable subtype.</p>
<p>MWPs pose significant challenges in QA systems ( [5], [6], [7], [8], [9], [10]), requiring more than basic pattern recognition.They involve understanding mathematical operators, quantities, and their relationships to produce a solution equation.This process entails identifying numerical values, selecting appropriate operations, and forming mathematical expressions with unknown variables.Efforts to teach computers to solve MWPs date back to the 1960s, with researchers employing rule-based methods to mimic human problem-solving strategies [11].Recent advancements in NLP and machine learning have significantly improved the ability to tackle MWPs, particularly for simpler problems.</p>
<p>Chain-of-Thought Prompting</p>
<p>While machine learning and transformer-based models excel at solving simpler MWPs ( [12]), they often struggle with problems that require reasoning and multi-step solutions ( [13]).These MWPs demand understanding complex relationships between quantities and applying mathematical principles sequentially, which many models fail to handle effectively.Language models perform well in NLP tasks, but their reasoning abilities are limited, which cannot be solved by increasing model size ( [14]).Prompting-based tools fix this with QA tasks.A study( [15]) introduced CoT (Chain-of-Thought) prompting, which generates short sentences that mimic human problem-solving.</p>
<p>When solving a complex reasoning task, such as a multi-step math word problem, it is natural to break the problem into smaller intermediate steps and solve each one sequentially before arriving at the final answer.For instance: "After Era gives 2 lichies to her mom, she has 10 left.Then, after giving 3 to her dad, she has 7 left.So, the answer is 7." A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting ( [15]).Recent advancements on chain-of-thought prompting, have shown promise in enhancing the reasoning capabilities of large language models for solving complex MWPs.Chain-of-thought prompting involves breaking down a problem into intermediate reasoning steps, which are then solved sequentially to reach the final answer.A comparison between Standard Prompting and Chain-of-Thought (CoT) Prompting for solving math word problems (MWPs) using a language model is shown in Figure 1.In Standard Prompting, the model is presented with the question directly, without any reasoning steps, leading to incorrect answers due to a lack of logical breakdown.In contrast, CoT Prompting provides a step-by-step explanation, allowing the model to reason through the problem logically and arrive at the correct solution.By breaking the problem into smaller parts and solving it systematically, CoT significantly improves the model's ability to handle complex reasoning tasks, as demonstrated by the accurate outputs in the CoT example.</p>
<p>Studies ( [15]) have demonstrated that incorporating chain-of-thought reasoning into few-shot prompting significantly improves model performance on tasks such as math word problems, commonsense reasoning, and symbolic manipulation.By leveraging this method, large language models can better handle the intricate reasoning required for complex MWPs.</p>
<p>The challenge is even greater for low-resource languages like Bengali, where limited datasets and linguistic complexities further hinder the development of reasoning-based solutions.Bengali is a low-resource language for Math Word Problem (MWP) solving using Large Language Models, with 272.7 million speakers and seventh worldwide( [16]).Bengali language processing tools and models are difficult to develop due to a lack of datasets, linguistic resources, standardization, and dialectal variations, which hinders LLMs for MWPs solving.</p>
<p>In this study, we introduce a new dataset, SOMADHAN (Solution), which focuses on complex Bengali math word problems and includes intermediate reasoning steps (chain of thought) for each solution.We create this dataset to address the lack of high-quality, human-annotated resources for Bengali MWPs, a gap that severely limits the development and evaluation of reasoning-capable language models in low-resource languages.Unlike existing datasets, which are either automatically translated or limited to final-answer supervision, SOMADHAN provides detailed, step-by-step logical reasoning, enabling models to learn and demonstrate multi-step problem-solving-an essential skill for real-world mathematical understanding.Additionally, we utilized the publicly available "PatiGonit" dataset from a Github repository, which contains equation-based simple MWPs, to analyze models' ability to handle simple MWPs.These two datasets highlight the limitations of large language models, which can only solve straightforward math problems but struggle with complex ones requiring reasoning.Our proposed pipeline for Chain-of-Thought prompting, built on the SOMADHAN dataset, addresses this challenge by enabling large language models to tackle more reasoningintensive tasks.Several large language models, including GPT-4o [17], GPT-3.5 Turbo-16k [18], Llama-3 8B and 70B [19], llama-3.1-70b-versatileand 8b [19],llama-3.2-1b-previewand llama-3.2-3b-preview[19], llama-3.2-90b-textpreview[19], llama-3.3-70b[19], deepseek-r1-distill-qwen-32b, [20], deepseek-r1-distill-llama-70b [20], qwen-2.5-32b,and qwen-qwq-32b [21] were evaluated for their ability to generate intermediate steps and equations.Chain-of-thought (CoT) prompting consistently outperformed standard prompting.Among all the models evaluated, Llama-3.3 with CoT and few-shot prompting achieved the highest accuracy of 88%, outperforming all other models in both zero-shot and few-shot settings.GPT-4o showed strong performance in COT with few-shot learning across both datasets.For further experiments, we introduced LoRA (Low-Rank Adaptation) fine-tuning.LoRA was employed to adapt the models efficiently by modifying only a small subset of parameters, making it computationally and memory efficient.The goal of using LoRA was to see how well it could enhance the performance of large models like Llama-3 (8B) for the specific structure of Bengali math word problems while retaining their versatility for other general tasks.</p>
<p>The following are the main contributions of our study:</p>
<p>• Developed SOMADHAN dataset containing 8,792 complex Bengali Math Word Problems with their corresponding step by step solutions.</p>
<p>• Introduced a pipeline for applying chain-of-thought prompting using the SOMADHAN dataset, enabling large language models to handle complex reasoning tasks effectively.</p>
<p>• Introduced two Prompts to solve Complex Bengali Math Word Problems that requires reasoning steps.</p>
<p>• Performed a comparative analysis of GPT-4o, GPT-3.5 Turbo-16k, Llama-3 8B and 70B, llama-3.1-70bversatile,llama-3.1-8b,llama-3.2-1b-preview,llama-3.2-3b-preview,llama-3.2-90b-text-preview,llama-3.3-70b,deepseek-r1-distill-qwen-32b, deepseek-r1-distill-llama-70b, qwen-2.5-32b,and qwen-qwq-32b demonstrating the strengths of zero-shot prompting, few-shot prompting and fine-tuning to solve MWPs.</p>
<p>• Employed Low-Rank Adaptation (LoRA) to efficiently fine-tune large language models for Bengali math word problems, optimizing performance with minimal computational overhead.</p>
<p>Related Works</p>
<p>In this section, we discuss works related to our study.During our exploration, we found that no prior work has focused on solving Bengali Math Word Problems (MWPs) with Chain of Thoughts reasoning.However, there have been several studies on solving MWPs in English, which we categorize into six main approaches: Traditional Deep Learning Approaches, Transformer-Based Approaches, Hybrid and Ensemble Models, Methods utilizing intermediate steps, Prompting-based approaches, and Chain-of-Thought (CoT) Reasoning Approaches.Each of these categories is discusses in the following sections.</p>
<p>Traditional Deep Learning Approaches</p>
<p>These methods are increasingly being used to reduce manual effort and to enhance the performance of MWP solvers.One recent study [22] proposed a Bengali Word Problem dataset and set a Benchmark Evaluation for it using transformer and different Neural Machine Translation Models.97.30% accuracy was achieved via mT5.Another notable method involves using Sequence-to-Sequence (Seq2Seq) models to improve MWP solvers.A study by [23] proposed a Seq2Seq RNN model to solve math word problems (MWPs), integrating a GRU-based encoder, LSTM-based decoder, and a similarity-based retrieval mechanism for improved accuracy.While the model achieved 58.1% accuracy on Math23k and 16.1% on Alg514, the hybrid approach boosted performance to 64.7% and 70.1%, respectively.However, it struggled with diverse datasets and failed to generate novel equation templates.To address this, another study by [24] introduced equation normalization and explored Seq2Seq In another approach, [27] introduced the Graph-to-Tree (Graph2Tree) model, using graph-based encoders and tree-based decoders to effectively capture numerical relationships.It achieved accuracies of 83.7% on MAWPS and 77.4% on Math23k, addressing prior gaps in quantity reasoning.Similarly, [28] utilized BiGraphSAGE encoders and tree-based decoders in their Graph2Tree model, achieving 78.8% on MAWPS and 69.65% on MathQA but faced challenges in fully incorporating structural reasoning.</p>
<p>To further enhance MWP solvers, [29] proposed a multi-encoder, multi-decoder framework combining sequence-based and graph-based encoders with sequence and tree decoders.This approach improved equation generation and achieved 78.4% on the Math23k test set but still had gaps in leveraging textual structure.In another advancement, [30] introduced MWP-BERT, a numeracy-augmented pre-trained language model tailored for MWPs, achieving state-of-the-art results of 84.7% on Math23k, 76.2% on MathQA, and 91.2% on Ape-clean.Separately, [31] presented BERT, a bidirectional pre-trained model for language understanding, achieving 93.2% on SQuAD v1.1 and 83.1% on SQuAD v2.0.Building upon BERT, [32] enhanced it with RoBERTa by removing Next Sentence Prediction (NSP), applying dynamic masking, and using larger datasets, achieving state-of-the-art results like 90.2% on MNLI and 96.4</p>
<p>In a generative learning context, [33] introduced GPT-2, a generative pre-trained transformer excelling in zero-shot learning tasks, with notable performance on datasets such as LAMBADA (63.2%).For rule-based reasoning, [34] presented RuleTakers for reasoning over natural language rules, achieving 99% on synthetic datasets and over 90% on hand-authored rules, though paraphrased reasoning remained a challenge.Extending this work, [35] proposed RuleBERT, fine-tuning RoBERTa for probabilistic logic reasoning, achieving up to 99% accuracy on overlapping rules.Finally, [36] introduced EVR, a T5-based model for explainable reasoning, excelling in multi-hop tasks with interpretable steps, achieving 97.0% on DU5 and up to 98.1% on Birds-Electricity tasks.These advancements demonstrate significant progress in NLP for MWPs, reasoning, and language understanding across diverse datasets.</p>
<p>Transformer-Based Approaches</p>
<p>In a recent study, [37]  In another contribution, [38] introduced a novel approach for improving MWP solvers by generating linguistic variants of problem statements using GPT-3.These paraphrased problems were solved using a DeBERTa-based solver, with majority voting employed for final predictions.This approach addressed the lack of robustness in existing models to paraphrased problems.The method achieved accuracies of 91.0% on MAWPS, 79.1% on PARAMAWPS, and 63.5% on SVAMP.The datasets included MAWPS, PARAMAWPS, and SVAMP.</p>
<p>Hybrid and Ensemble Models</p>
<p>Hybrid models that combine different approaches have shown significant improvements.An ensemble model by [39] utilizing BiLSTM and LSTM with equation normalization achieved 69.2% accuracy on the Math23K dataset .In the paper [40] the GTS model, integrating GRU, TreeDecoder, and gated feedforward networks, achieved 74.3% on Math23K and 83.5% on the MAWPS single operation dataset.WARM has been introduced in [41] and it uses a weakly supervised approach with a bidirectional GRU encoder and three fully connected networks as the decoder, achieving 66.9% on All Arith and 56.0% on Math23K .[44] introduced the "Rationalize-Then-Predict" framework, a two-stage method aimed at improving model robustness in adversarial contexts by using rationalizers to extract relevant inputs before prediction.Although models like VIB and SPECTRA showed promise (e.g., 82.6% accuracy on FEVER), they remained vulnerable to strong attacks.In a related effort, [45] proposed BabbleLabble, a weak supervision approach leveraging natural language explanations and semantic parsers, achieving an F1 score of 50.1 on the Spouse dataset but facing challenges with noisy labels and generalization.Similarly, [46] highlighted the utility of intermediate annotations in reading comprehension, reporting 85% accuracy with Random Forest but acknowledging limited dataset diversity.Another study by [47] explored annotator rationales, improving SVM performance to 92.2% on the Polarity Dataset but identified issues with annotation quality and domain applicability.To address explanation integration, [48] developed REMOTE, which incorporated human-provided explanations to enhance language models, achieving 62.0% on HatEval and 92.7% on AmazonMusic, but faced scalability issues with labeled data.Further, [49] evaluated the utility of explanations for improving task performance, achieving 91.41% on e-SNLI while noting inconsistent benefits across tasks.Additionally, [50] introduced DREAM, which refined internal representations in QA models like Macaw, improving accuracy by 4% on CODAH but exposing limitations in coherent scene modeling.In a related context, [51] proposed Learning with Latent Language (L3) to parameterize multitask scenarios, showing promise in structured tasks like ShapeWorld (70% accuracy) but struggling with generalizing to abstract concepts.These studies collectively emphasize advancements in integrating explanations and rationales into AI systems while highlighting persistent challenges in scalability, robustness, and generalization.</p>
<p>In support of explanation-based modeling, [52] introduced the e-SNLI dataset, a natural language inference (NLI) benchmark that incorporates explanations, enabling models to predict decisions and justify them.Models like e-INFERSENT achieved 83.96% accuracy, while EXPLAIN THE PREDICT ATTENTION scored 81.71%, though reliance on spurious correlations in existing models revealed gaps in robustness.Moreover, [53] developed the CoS-E dataset and CAGE framework for commonsense reasoning, which improved model accuracy to 72.6% on the CoS-E dev split but highlighted limitations in explanation generation and dataset availability, underscoring the importance of leveraging annotated explanations for better interpretability.Finally, [54] proposed the Self-Taught Reasoner (STaR), which combines reasoning and rationalization with models like GPT-J and BERT, achieving notable accuracies, such as 72.6% on CommonsenseQA and GSM8K.However, STaR faced challenges in scalability, requiring significant resources for rationale generation and balancing the trade-off between accuracy and explainability.Collectively, these works highlight the potential of explanation-based approaches in enhancing transparency, reasoning, and trust in AI systems while identifying gaps in dataset availability and model generalization.</p>
<p>Prompting-Based Approaches</p>
<p>Recent advancements in prompting techniques have led to significant improvements in the performance of language models.Building on the concept of few-shot prompting introduced by [55], various methods have been explored to refine and optimize how models interact with input data.For example, [56] proposed automatic prompt learning techniques, enabling models to adapt their prompts based on the task at hand, rather than relying solely on predefined prompts.Additionally, studies such as those by [57], [58], and [59] have focused on providing models with task-specific instructions, allowing for better alignment with the desired output.</p>
<p>Another significant development in this area is the use of detailed task instructions, which has been shown to further enhance the performance of language models.In particular, [60], along with works by [57], [59], [58], and [61], emphasize that enriching input-output pairs with clear and specific guidance helps improve model understanding and reasoning, especially in complex tasks like math word problems.This trend of augmenting prompts with additional context and instructions has been pivotal in achieving state-of-the-art results in various natural language processing tasks, demonstrating the importance of prompt design in optimizing model behavior.</p>
<p>Chain-of-Thought (CoT) Reasoning Approaches</p>
<p>Chain of Thought (CoT) prompting has become a widely adopted method for improving the performance of language models, especially in tasks requiring multi-step reasoning.By breaking down a complex problem into smaller, more manageable steps, CoT helps models generate clearer and more accurate answers.For example, [15] demonstrated that CoT prompting significantly outperforms traditional methods for solving math word problems (MWPs), enhancing both the accuracy of the results and the transparency of the model's reasoning process.</p>
<p>Building on this foundation, [42] introduced DIVERSE (Diverse Verifier on Reasoning Step), a framework designed to improve language model reasoning by generating diverse prompts.This approach explores multiple reasoning paths for the same question and uses a verifier to filter out incorrect answers through a weighted voting system.Importantly, DIVERSE focuses on evaluating each reasoning step individually rather than the entire chain, which further refines the model's decision-making process.In a related effort, [43] introduced the concept of self-consistency, a decoding strategy that selects the most consistent answer from multiple reasoning paths, providing another layer of validation.When applied to arithmetic reasoning benchmarks, self-consistency has been shown to enhance CoT prompting, yielding more accurate and reliable results.</p>
<p>Our study explores methods for solving Math Word Problems (MWPs), categorized into Deep Learning, Transformerbased, Hybrid, Prompting, and Chain-of-Thought (CoT) Reasoning approaches.The most significant gap identified in this study is the complete absence of prior work addressing Math Word Problems (MWPs) in the Bengali language.This highlights a critical underrepresentation of Bengali in existing research on mathematical reasoning and natural language processing.In contrast, for English datasets, traditional models like Seq2Seq and Graph2Tree advanced MWP solving but struggled with diverse datasets and reasoning structures.Transformer-based models, including MWP-BERT and GPT-3, achieved state-of-the-art results but faced robustness issues.Prompting and CoT methods improved multi-step reasoning by leveraging task-specific instructions and diverse reasoning paths.A summary of the existing works is given in Table 1.</p>
<p>Problem Description</p>
<p>The problem is best framed as a model's ability to solve complex math word problems by generating intermediate reasoning steps and a final result.Evaluation focuses solely on the correctness of the final result, disregarding intermediate steps.</p>
<p>Let, x denotes the input math word problem, f(x) denotes the output generated by the model, consisting of a sequence of intermediate reasoning steps followed by a final result r pred , and r true denotes the ground truth final result for the problem.</p>
<p>The model's output can be represented as:
f(x) = (s 1 , s 2 , ...... ,s k , r pred )
where s 1 , s 2 , ...... ,s k are the intermediate steps and r pred is the predicted final result.</p>
<p>Corpus Creation</p>
<p>As per our exploration, we found no publicly available dataset addresses complex Bengali Math Word Problems (MWPs) with detailed reasoning steps and human-verified solutions.While a few Bengali MWP datasets exist, they primarily rely on automated translation, which leads to poor linguistic quality and a lack of meaningful reasoning structure.These limitations make them unsuitable for training or evaluating models that aim to perform multi-step mathematical reasoning in Bengali.To address this gap, we developed a new Bengali math word problem dataset, SOMADHAN.This dataset has been meticulously manually annotated by expert annotators, ensuring higher accuracy and contextual relevance in solving complex math problems with reasoning steps and solutions.This dataset focuses on problems that require advanced reasoning and step-by-step solutions, designed to facilitate the evaluation of large language models' reasoning capabilities.Figure 2 illustrates the pipeline we employed to develop the SOMADHAN dataset.required a substantial collection of problems reflecting complex reasoning processes.As a foundation, we utilized the publicly available GSM8K (Grade School Math)1 dataset [13], which contains a diverse set of English grade school math problems.GSM8k has four parts: train, train socratic, test, and test socratic.But specifically, we leveraged the train and test portions, comprising a total of 8,792 questions and solutions.GSM8K was selected for its comprehensive coverage of grade school math concepts, making it an ideal base for developing our dataset.Inspired by GSM8K, we created SOMADHAN (Bengali Grade School Math).Our dataset comprises of 8,792 Bengali Math Word Problems along with their step by step solutions.</p>
<p>GSM8K</p>
<p>PatiGonit Dataset: The second dataset used in this study, PatiGonit, was publicly available and collected from a Github repository.It consists of 10,000 simple Bengali math word problems, each paired with a corresponding equation as the solution.The dataset primarily focuses on elementary school-level problems, encompassing basic arithmetic and some introductory algebraic challenges.The problems are organized into two columns: one for the word problem and the other for the corresponding mathematical equation.PatiGonit serves as a valuable resource for analyzing simple equation-based math word problems.</p>
<p>Data Translation</p>
<p>In this section, we describe the dataset translation process for the SOMADHAN dataset.Manual data translation is inherently challenging as it requires not only linguistic accuracy but also the preservation of contextual meaning, particularly for reasoning-based tasks in math word problems.Any loss of reasoning structure during translation could compromise the dataset's usability for downstream applications like reasoning evaluation and problem-solving.</p>
<p>To ensure the reasoning quality and contextual fidelity of the translated texts, we employed expert human translators for the manual translation process.Comprehensive guidelines were established to provide translators with clear instructions on preserving mathematical and contextual details.These predefined standards, combined with the translators' expertise and judgment, ensured the integrity and consistency of the translated dataset.</p>
<p>Translator Indentity</p>
<p>For the translation process, we engaged professionals with expertise in English-to-Bengali translation to ensure that the translations were accurate, contextually consistent, and aligned with the reasoning requirements of the original texts.A team of five skilled individuals collaboratively worked on the translation task.The entire corpus of the SOMADHAN dataset was evenly distributed among them to ensure efficiency and maintain uniform quality across the translated content.The information regarding their expertise, experiments and other details are presented in Table 2.</p>
<p>Translation guidelines</p>
<p>To ensure accuracy, cultural relevance, and linguistic diversity, the following guidelines were established and provided to the translators:</p>
<p>• Translations should retain the original meaning and reasoning of the text, ensuring that mathematical logic and contextual nuances are preserved.• The Dollar symbol was consistently replaced with the Bengali Taka symbol to align with regional monetary conventions.• English numerals were replaced with their Bengali counterparts to maintain consistency with the Bengali script.• Individual names were replaced with culturally appropriate Bengali names.</p>
<p>• Similarly, Location names, objects, and food names were substituted with Bengali equivalents to ensure cultural relevance while maintaining the problem's original structure and meaning.</p>
<p>Challenges During Translation</p>
<p>Even with the predefined guidelines, translators encountered several challenges during the translation process, which are summarized in Figure 3.</p>
<p>Dataset Statistics</p>
<p>The SOMADHAN dataset is composed of 8,792 Bengali math word problems with intermediate reasoning steps and their corresponding answers.Currently, the dataset includes 4,000 manually annotated samples, each containing step-by-step reasoning and a final answer.Annotation of the remaining problems is ongoing, and we plan to release the complete version in a future update.Once finalized, we will publish Version 2 of the dataset on Mendeley Data to ensure continued accessibility and reproducibility.A sample is shown in Figure 4.</p>
<p>The PatiGonit dataset on the other hand comprises 10,000 equation-based Bengali math word problems.The dataset contains a variation of simple and complex equations; simple equations are defined as those containing only one mathematical operation, while complex equations involve multiple operations.Sample of the dataset is shown in Figure 5.</p>
<p>Methodology</p>
<p>In this study, our methodology centers on the use of large language models (LLMs) for solving Bengali math word problems.Specifically, we examined the performance of LLMs by constructing tailored prompts, which were then input into the models via API configurations.The outputs were thoroughly analyzed to assess the effectiveness of the models in addressing the challenges posed by Bengali math word problems.This approach allows us to highlight the comparative advantages of using LLMs for these types of tasks.The study utilizes two distinct datasets to evaluate model performance.The primary dataset, SOMADHAN, was specifically designed for Bengali math word problems and incorporated Chain of Thought (CoT) prompting to guide the models through a logical, step-by-step reasoning process.This setup was intended to assess the ability of LLMs to solve more complex problems requiring multi-step reasoning.In contrast, the PatiGonit dataset was employed to investigate the performance of LLMs in solving simpler equation-based problems in compared to Reasoning-based problems.</p>
<p>Evaluation measures</p>
<p>For the SOMADHAN dataset, the evaluation process involved comparing the model's final answer against the correct answer.Only the accuracy of the final solution was considered relevant for assessment.On the other hand, for the PatiGonit dataset, the evaluation was based on matching the predicted equations with the actual equations, which was done manually.In both cases, the evaluation was performed through manual checks to ensure the correctness of the results, guaranteeing the reliability of the accuracy measurements for both datasets.</p>
<p>Proposed Approaches</p>
<p>A methodology was explored for solving Bengali math word problems, leveraging large language models (LLMs) to assess their performance and suitability for this complex task.</p>
<p>Large-Language Models</p>
<p>When using large language models (LLMs), we evaluated their performance through in-context zero-shot and few-shot prompting, emphasizing the Chain of Thought (CoT) approach for reasoning-intensive tasks.This method involved designing prompts that guided the models to provide step-by-step explanations, enhancing clarity and interpretability.Figure 6 illustrates the schematic diagram of our proposed prompting approach.</p>
<p>Additionally, we applied fine-tuning to structure the CoT prompting more effectively.To balance creativity and coherence, we set the temperature to one for all models, enabling them to generate mathematical equations with detailed, logical explanations.This approach fully leveraged the models' potential while ensuring the clarity and precision necessary to solve complex mathematical problems.Gpt-4o and Gpt-3.5 Turbo: GPT models, developed by OpenAI, are state-of-the-art large language models designed for a wide range of natural language processing tasks.Notable examples include GPT-4o [17] with trillions of parameters and GPT-3.5 Turbo [18] with approximately 175 billion parameters.These models are known for their unparalleled ability to understand and generate human-like text, handle complex instructions, and solve intricate tasks with high precision.</p>
<p>Their success stems from extensive training on diverse datasets and leveraging advanced attention mechanisms to capture contextual nuances.In mathematical problem-solving, GPT models excel by employing the Chain of Thought (CoT) approach, generating detailed intermediate reasoning steps that enhance logical accuracy and interpretability.By breaking problems into smaller, manageable parts, CoT ensures systematic reasoning and correct final results.This capability makes GPT models exceptionally effective for tasks requiring deep comprehension, step-by-step execution, and transparent problem-solving processes.</p>
<p>Llama (Large Language Model Meta AI): LLaMA (Large Language Model Meta AI) is a series of advanced language models developed by MetaAI.These models [19], including LLaMA-3 8B, LLaMA-3 70B, LLaMA-3.1 70B, LLaMA-3.1 8B, LLaMA-3.2 1B-preview, LLaMA-3.2 3B-preview, LLaMA-3.2 90B-text-preview, and LLaMA-3.3 70B, represent significant strides in NLP with their efficient scaling and fine-tuning capabilities.Unlike other large models, LLaMA focuses on optimizing performance through parameter-efficient designs, making them particularly effective for specific tasks even with smaller sizes.The LLaMA-3 series introduces improvements in token processing, context understanding, and task adaptability, demonstrating remarkable performance across a variety of benchmarks.</p>
<p>Despite having fewer parameters than GPT-4, LLaMA models excel in tasks like mathematical reasoning by leveraging optimized architectures and structured pre-training to produce accurate, coherent outputs.They effectively apply a structured reasoning approach inspired by the Chain of Thought (CoT) process, enabling step-by-step problem-solving by breaking complex tasks into intermediate steps.This systematic methodology allows LLaMA models to process and solve tasks efficiently while maintaining clarity and precision.Their ability to utilize CoT-style reasoning ensures reliable solutions and highlights their effectiveness in tackling challenges that demand logical progression and detailed analysis.</p>
<p>Deepseek: In addition to the LLaMA series, we also incorporated models from Deepseek and Qwen to further assess their performance in solving complex Bengali Math Word Problems (MWPs).The Deepseek-r1-distill-qwen-32B model [20] is a distilled version of the Qwen-32B model, specifically optimized for efficient performance while reducing computational overhead.This model leverages the distillation process, which involves training a smaller model to mimic the behavior of a larger model.Through this process, the Deepseek-r1-distill-qwen-32B retains the essential capabilities of the Qwen-32B model, allowing it to perform a wide range of natural language processing (NLP) tasks, including complex problem-solving.The distillation technique enhances the model's efficiency by significantly reducing the computational resources required for inference, making it a suitable choice for tasks involving complex mathematical reasoning.</p>
<p>Similarly, the Deepseek-r1-distill-llama-70B model [20] is a distilled variant of the LLaMA-70B model.This model is designed to achieve parameter efficiency while maintaining high levels of performance, particularly in tasks that require multi-step reasoning, such as solving math word problems.By distilling the larger LLaMA-70B model, the Deepseek-r1-distill-llama-70B maintains the essential performance characteristics of its predecessor while optimizing its computational resource usage.This balance between model performance and resource efficiency makes the Deepseek-r1-distill-llama-70B well-suited for handling the complex and computationally intensive nature of solving Bengali Math Word Problems.</p>
<p>Qwen: Additionally, we integrated Qwen models, specifically Qwen-2.5-32B[21] and Qwen-qwq-32B [21], both of which are robust models designed to handle multi-tasking and complex reasoning.The Qwen-2.5-32B model, with its 32 billion parameters, excels in addressing a wide range of natural language processing (NLP) challenges, including multi-step reasoning and complex mathematical computations.It has demonstrated impressive generalization capabilities across various benchmarks, particularly in tasks that require structured problem-solving.This model's architecture is specifically optimized to manage intricate problem-solving tasks, making it well-suited for Bengali Math Word Problems (MWPs).</p>
<p>Meanwhile, the Qwen-qwq-32B model further enhances the performance of the Qwen-2.5 by incorporating advanced optimizations that enhance its efficiency in solving mathematical problems.These optimizations improve the model's ability to handle tasks requiring deep reasoning and multi-step calculations, making it more efficient and accurate in complex problem-solving scenarios.Together, these models represent the latest advancements in large language models, offering significant improvements in the Bengali MWP-solving framework and contributing meaningfully to its effectiveness in solving computationally challenging problems.</p>
<p>Prompting Techniques</p>
<p>While working with large language models (LLMs), we aimed to make our instructions clear and easy to follow so the models could better understand our tasks and give accurate responses.This was important because LLMs often generate different answers depending on how the prompts are designed.Creating these prompts was a step-by-step process with challenges due to the way information is represented differently across models.Our study focused on improving instructions and prompts to guide the models in acting like math instructors and solving math problems with accurate solutions.We also created three sets of instructions and developed unique prompt styles for our proposed dataset.Effective prompting is particularly important for encouraging the models to adopt a Chain of Thought (CoT) approach.Chain-of-thought (CoT) prompting has several key advantages for improving how language models reason:</p>
<ol>
<li>
<p>Helps models break complex problems into smaller, easier steps, allowing them to focus on tasks that need more reasoning.</p>
</li>
<li>
<p>Gives us a clearer view of how the model arrives at an answer, making it easier to spot where the reasoning went wrong (though fully explaining a model's thinking is still challenging).</p>
</li>
<li>
<p>Can be used for various tasks, like solving math problems, understanding everyday situations, and manipulating symbols, and can potentially apply to any task that humans can solve using language.</p>
</li>
<li>
<p>CoT reasoning can be triggered in large language models by simply including examples of reasoning steps in a few-shot prompt.</p>
</li>
</ol>
<p>Zero Shot Prompting</p>
<p>Zero-shot prompting is particularly useful for testing a model's inherent capabilities and adaptability without the need for extra fine-tuning or example-based context.In our approach, we applied zero-shot prompting for both Chain-of-Thought (CoT) and standard prompting.For CoT, we provided clear chain-of-thought instructions that described the task and outlined the expected output, enabling the models to generate intermediate reasoning steps autonomously.This method encourages the LLMs to create their own context and refine their reasoning process, ultimately leading to more accurate and logical results.In contrast, for standard prompting, no prior instructions on solving the problem were given.The model was simply presented with the question, and it was expected to generate the answer without any guidance on breaking down the problem.To enhance the reasoning process, two separate system instructions were incorporated in our prompting techniques.Figure 7 and Figure 8 shows the two types of system instruction (Prompt-1 &amp; Prompt-2) for CoT with zero shot Prompting.Moreover, Figure 12 illustrates the system instruction that we have used for the standard prompting.</p>
<p>Few Shot Prompting</p>
<p>In our approach, we focused on leveraging few-shot learning, which has been shown to outperform zero-shot learning, as demonstrated in studies by [55] and [62].Unlike zero-shot, where no prior examples are provided, few-shot learning uses a small number of example prompts to guide the model's reasoning process.We conducted experiments with GPT-4, GPT-3.5, the LLaMA-3 series, Deepseek, and Qwen models, utilizing training data to enhance their performance.To optimize API costs, we manually included five few-shot prompts.For our two datasets, we designed distinct prompts.Several key steps were taken: we taught the model how to approach and reason through the task, followed by question-and-answer prompts for generating answers.In contrast to zero-shot prompting, which solely relies on the model's pre-existing knowledge without examples, our few-shot approach provides explicit examples of the task, allowing the model to adapt to the specific problem structure.Finally, math problem tests were conducted to assess the model's performance.Our method further augments each few-shot example by incorporating a chain of thought, improving accuracy and clarity in the results.In Figure 9, 10, and 11 we showcase examples of few-shot prompts for both of the datasets.Additionally, Figure 12 illustrates the system instruction that we have used for the standard prompting.</p>
<p>Fine Tuning</p>
<p>To further improve model performance on complex Bengali math word problems, we explore fine-tuning techniques tailored to reasoning-based tasks.In this section, we describe two approaches-standard fine-tuning using OpenAI's GPT-3.5 and parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)-both enhanced with chain-of-thought supervision.</p>
<p>Gpt-3.5 Standard Tuning</p>
<p>Fine-tuning [63] enhances few-shot learning by training on a larger set of examples, resulting in improved performance across various tasks.In this approach, we integrated chain of thought to encourage models to generate step-by-step reasoning in their solutions.Once a model is fine-tuned, fewer examples are required in the prompt, which reduces costs and enables faster response times.The fine-tuning process consists of the following steps, which differ from few-shot prompting: We chose to fine-tune the recommended model, GPT-3.5-turbo-0125.During this process, we varied only the number of epochs to optimize the model's performance.To build the fine-tuned model, we used 50 examples with appropriate instructions, including explicit chain-of-thought reasoning for complex problems.This ensured the model not only provided answers but also demonstrated the thought process behind solving math problems.The examples followed the JSONL file format as outlined in the OpenAI API documentation. Figure 13 shows an example of how the JSON file was structured to create the fine-tuned model.</p>
<p>Our SOMADHAN dataset required proper chain-of-thought reasoning to produce correct solutions, so fine-tuning was essential.The models were trained to break down complex problems into intermediate steps, ensuring that the reasoning behind the answers was explicit and interpretable.In contrast, the PatiGonit dataset only contains math word problem equations, which require less complex reasoning.For this dataset, GPT-3.5 Turbo and GPT-4 models performed well with few-shot prompting, so extensive fine-tuning was not necessary.</p>
<p>LoRA Fine Tuning</p>
<p>For SOMADHAN dataset, we employed Low-Rank Adaptation (LoRA) [64] for fine-tuning.LoRA improves finetuning by learning two smaller weight-updating matrices, allowing the large model to remain frozen while only the smaller matrices are updated with new data.After training, the updated matrices are recombined with the original weights to create the final model.This method significantly reduces the number of parameters and accelerates the
{"role":"assistant", "content": "দেল মাট ৫ জন মিহলা রেয়েছন। তারা সকেলই ৬ িদন িতিদন ৩ মাইল হাঁ েটন তাই ৩<em>৬ = ১৮ মাইল| িম ৬ িদেনর িতিদন অিতির ২ মাইল হাঁ েট তাই ২</em>৬ = &lt;&lt;2*৬=১২&gt;&gt;১২ মাইল| সু িম িমর অেধ ক দূ র অিত ম কের তাই ১২/২ = &lt;&lt;১২/২=৬&gt;&gt;৬ মাইল| একসােথ, মিহলারা ১৮+১২+৬ = &lt;&lt;১৮+১২+৬=৩৬&gt;&gt;৩৬ মাইল হাঁ েটন #### 36"}] 2. . . . .</p>
<p>50.</p>
<p>Figure 13: Structure of JSON file format fine-tuning process while saving storage.As part of this process, we also integrated chain-of-thought examples into the fine-tuning dataset to encourage the model to generate logical, step-by-step reasoning.Figure 14 shows an overview of the process.6 Experimental Setup</p>
<p>To evaluate the performance of large language models on Bengali Math Word Problems, we designed a comprehensive experimental setup.This includes configuring the computational environment, organizing train-test splits for both datasets, and defining evaluation metrics tailored to reasoning-based and equation-based tasks.</p>
<p>Our work was conducted on Google Colab Notebook with Python 3.10.12,PyTorch 2.0.1, a Tesla T4 GPU (15 GB), 12.5 GB of RAM, and 64 GB of disk space.</p>
<p>Train-Test-Validation splits</p>
<p>For our SOMADHAN dataset's prompting and evaluation, we divided the dataset into two parts: 80% for testing and 20% for training.Specifically, 3200 examples were assigned for testing, while 800 examples were reserved for training.</p>
<p>Prompting and fine-tuning samples were taken from the training set, and evaluation was done using the testing set.The same splitting technique was applied for the PatiGonit dataset.However, due to token costs, only 1000 samples from the testing set were considered for evaluation in our experiments for both datasets.This decision allowed us to manage resource constraints effectively while still obtaining valuable insights into model performance.</p>
<p>Evaluation metrics SOMADHAN Dataset Evaluation</p>
<p>The model's output can be represented as:
f(x) = (s 1 , s 2 , ...... ,s k , r pred )
where s 1 , s 2 , ...... ,s k are the intermediate steps and r pred is the predicted final result.</p>
<p>For evaluation, the intermediate reasoning steps s 1 , s 2 , ...... ,s k are not considered.Only the final result r pred is compared with the ground truth r true :
Evaluation(f (x), r true ) = 1 if r pred = r true , 0 if r pred ̸ = r true .(1)
The model is considered to have successfully solved the problem if: r pred = r true If the dataset contains N problems x 1 , x 2 , ...... , x N , the accuracy A is computed as:
A = 1 N N i=1 1(r pred,i = r true,i )
where 1(•) is the indicator function, which equals 1 if the condition is true and 0 otherwise.</p>
<p>PatiGonit Dataset Evaluation</p>
<p>We used equation accuracy, which compares the predicted equation with the correct equation rather than focusing on the final solution.This metric more effectively measures the model's ability to generate the correct equation, regardless of minor variations in the numerical result.By evaluating the structure of the equation itself, this method ensures that the model is solving the problem with the right logical steps, even if the final solution is not exactly the same.</p>
<p>The model's output can be represented as:
f(x) = (e pred )
where e pred is the equation generated by the model.</p>
<p>Evaluation focuses on comparing the predicted equation e pred with the ground truth equation e true .This approach ensures that the model generates the correct mathematical equation, which is the primary focus of our evaluation.</p>
<p>Result Analysis</p>
<p>The result analysis section provides a detailed evaluation of the model's performance on the SOMADHAN and PatiGonit datasets.We examined the accuracy to evaluate solution correctness to assess the effectiveness of our proposed approaches.</p>
<p>Chain of Thoughts based results</p>
<p>The results in Table 3 and !4 show key distinctions between Standard prompting and Chain of Thought (CoT) prompting, highlighting their respective strengths and weaknesses.In Table 3, the performance of six different large language models (LLMs) is compared across the SOMADHAN and PatiGonit datasets using both Standard prompting and Chain of Thought (CoT) prompting, with evaluations based on Zero-Shot and Few-Shot settings.Prompt Style-1 was used for the experiments in this table.The models include GPT-3.5, GPT-4o, and various versions of the Llama series.</p>
<p>Table 3: Performance comparison of Chain of Thoughts (CoT) prompting (Prompt-1) versus Standard prompting for various large language models on the "SOMADHAN" dataset.A benchmark has been set for the "PatiGonit" dataset using Few Shot prompting (Prompt-3) technique.All metrics are Accuracy (%).(*) represents that the accuracy is correct but the responses of the model was in English.The GPT-3.5 model achieves notable results in both zero-shot and few-shot settings.On the SOMADHAN dataset, it reaches 23.0% accuracy in zero-shot and 24.0% accuracy in few-shot using standard prompting.However, with Chain of Thought (CoT) prompting, GPT-3.5'sperformance improves, reaching 24.0% accuracy in zero-shot and 30.0% in few-shot, which is a significant increase of 6.0%.This indicates the effectiveness of the CoT approach for handling more complex tasks.It also performs decently in the PatiGonit dataset, achieving 86.0% with few-shot prompting.GPT-4o, with approximately 1.3 trillion parameters, demonstrates substantial improvements over GPT-3.5.For SOMADHAN, the model reaches 79.0% in zero-shot and 80.0% in few-shot using standard prompting.With CoT prompting, its performance dramatically improves, achieving 80.4% in zero-shot and 83.0% in few-shot, marking an impressive 3.0% increase in the few-shot setting.This shows that GPT-4o can handle complex reasoning tasks effectively with CoT.In the PatiGonit dataset, GPT-4o achieves 99.0% accuracy in few-shot, nearly perfect, emphasizing its advanced capabilities.</p>
<p>In the case of Llama-3 models with 70B and 8B parameters, the performance differs significantly between standard prompting and Chain of Thought (CoT) prompting.For Llama-3 (70B), the highest accuracy was achieved with standard prompting in the zero-shot setting, reaching 76% accuracy.However, it is important to note that all responses were in English, even though the dataset required responses in Bengali.When CoT prompting was applied, the accuracy dropped slightly to 65% in zero-shot and few-shot settings.Despite the decrease in accuracy, the responses shifted to Bengali, indicating that while CoT improved the language adaptation, it did not significantly improve the accuracy when compared to standard prompting.For Llama-3 (8B), the performance was considerably lower due to the reduced parameter size.The highest accuracy for Llama-3 (8B) was 47% using standard prompting in zero-shot, but when CoT was applied, the accuracy further decreased to 24% in zero-shot and 19% in few-shot.This suggests that smaller models like Llama-3 (8B) do not benefit as much from CoT prompting in case of Bengali as larger models do.In this case, CoT did not help improve accuracy significantly, and the language shift to Bengali did not overcome the challenges posed by the small parameter size.</p>
<p>For Llama-3.1 (70B), the highest accuracy was achieved using Chain of Thought (CoT) prompting with zero-shot, where it reached 80%.This represents a notable improvement of 2% over the standard zero-shot prompting, showcasing the positive impact of CoT on the model's performance.However, when few-shot prompting was applied along with CoT, the performance remained the same as with standard few-shot prompting, indicating that CoT did not contribute additional improvements in the few-shot setting for this model.For Llama-3.2 (90B), the best accuracy was also achieved with Chain of Thought prompting in the zero-shot setting, where it attained 79%.This marked a 4% increase in accuracy compared to standard zero-shot prompting, highlighting the benefit of CoT for this model.However, when few-shot prompting was used in combination with CoT, the accuracy declined by 3% compared to standard few-shot prompting, suggesting that while CoT improved performance in the zero-shot setting, it did not have a similar positive effect in the few-shot setting for this model.For Llama-3.3 (70B), the highest accuracy of 87% was achieved on the Somadhan dataset, outperforming the other models.Without the application of Chain of Thought (CoT), the model achieved its highest accuracy of 78% using standard prompting.However, when CoT was applied with few-shot prompting, the model's performance significantly improved by 9%, reaching the peak accuracy of 87%.This demonstrates the considerable enhancement that CoT along with few shot can provide, especially in tasks involving more complex reasoning, and highlights Llama-3.3 as the top-performing model in this comparison.</p>
<p>In particular, the Llama-3 (70B) and Llama-3 (8B) models exhibited a noticeable drop in performance when using CoT for Bengali language tasks.Despite their strong performance in English, the models struggled to achieve comparable results in Bengali, likely due to the complexity of language-specific reasoning in CoT and their relatively smaller parameter sizes.While CoT improved the performance for larger models like GPT-4o, the benefits for Llama-3 models were more modest, especially in few-shot settings.This disparity highlights the importance of model size and prompt style in achieving optimal performance.To address these challenges, we decided to apply a different prompt style (Prompt Style 2) on the same dataset to investigate whether it could provide a more effective approach, particularly for the Llama models when handling Bengali language tasks.This experiment aims to explore potential improvements and provide better insights into the effectiveness of CoT in multilingual settings.In Table 4, we incorporated Prompt Style 2 to further analyze the performance of various LLM models on the SOMADHAN dataset, comparing standard prompting with Chain of Thoughts (CoT) prompting in both zero-shot and few-shot settings.The aim of this experiment was to determine whether Prompt Style 2 could improve accuracy, particularly for Llama series models and newer models from the Deepseek and Qwen series.</p>
<p>For the Deepseek-r1-distill-qwen model, the highest accuracy of 51% was achieved with CoT and few-shot prompting, showing a significant improvement over the 44% accuracy observed with standard few-shot prompting.However, it is important to note that all responses were in English, which deviates from the desired output in Bengali.Even with Chain of Thought and few-shot prompting, the response language format could not be improved.Similarly, the Deepseek-r1-distill-llama model achieved an impressive 66% accuracy with CoT and few-shot prompting, a 7% increase over the standard few-shot performance of 60%.Unlike the previous Deepseek model, this model's responses were in Bengali, aligning with the intended language output.</p>
<p>For the Qwen-2.5 model with 32 billion parameters, the highest accuracy of 71% was achieved using Chain of Thought with few-shot prompting.This represents a 3% improvement compared to the 68% accuracy observed with standard few-shot prompting.On the other hand, the Qwen-qwq model, despite incorporating Chain of Thought and few-shot prompting, could not generate responses in Bengali.All responses from this model were in English, indicating that the language adaptation was not achieved even with the use of Chain of Thought.</p>
<p>The Llama-3 model with 8 billion parameters did not achieve better results when Chain of Thought (CoT) prompting was introduced.The highest accuracy was achieved with standard prompting in the zero-shot setting, which reached 47%.Instead of increasing accuracy, the use of Chain of Thought actually decreased the accuracy percentage.Furthermore, despite this accuracy, all responses were in English rather than Bengali, which deviates from the intended output.On the other hand, the Llama-3 model with 70 billion parameters performed better, achieving 73% accuracy compared to 67% with standard prompting, reflecting a 6% improvement with the use of CoT.The smaller model (8B) again struggled with Chain of Thought prompting even with a change in Prompt style, possibly due to its limited capacity for processing more intricate tasks or adapting to Bengali.The larger Llama-3 (70B), however, demonstrated a noticeable improvement with the new prompt style.This suggests that parameter size and prompt play a crucial role in the effectiveness of Chain of Thought prompting, especially when dealing with multi-step reasoning and language adaptation.The Llama-3.1 (8B), Llama-3.2 (1B), and Llama-3.2 (3B) models also did not show any improvements with the introduction of Chain of Thought (CoT) prompting.In fact, their accuracy remained below expectations, and in some cases, even decreased.The Llama-3.1 (8B) model, for instance, still achieved its highest accuracy with standard zero-shot prompting (48%), and CoT prompted a decrease in accuracy.Similarly, Llama-3.2 (1B) and Llama-3B also failed to demonstrate any notable improvements with and without CoT prompting, and their performance remained subpar in both zero-shot and few-shot settings.Even with the introduction of Prompt Style 2, Llama-3.3 (70B) continues to achieve the highest accuracy, reaching an impressive 88% on our proposed dataset.This performance was attained with the inclusion of Chain of Thought (CoT) and few-shot prompting, showing a remarkable 10% improvement over the standard few-shot prompting accuracy of 78%.This substantial increase highlights the significant benefits of applying CoT prompting to advanced models like Llama-3.3, demonstrating its effectiveness in handling complex reasoning tasks.The model's ability to achieve the highest accuracy, even with the new prompt style, further underscores the potential of CoT for enhancing model performance, particularly for larger, more capable models like Llama-3.3.</p>
<p>The Chain of Thought (CoT) prompting has proven to be a powerful approach in solving complex reasoning tasks, particularly in the domain of mathematical problems in Bengali.By breaking down intricate problems into smaller, manageable steps, CoT enables the models to better understand the logical structure of the task and produce more accurate solutions.In our experiments, CoT significantly enhanced the performance of larger models like GPT-4o and Llama-3.3, demonstrating its ability to handle multi-step reasoning.Moreover, the introduction of different prompt styles, such as Prompt Style 2, has further improved the efficacy of CoT by better aligning the model's responses with the intended language output.This highlights the importance of fine-tuning both the prompting techniques and the model parameters to optimize performance, particularly in multilingual settings like Bengali, where language-specific nuances can significantly impact the model's ability to reason and generate accurate answers.By strategically incorporating CoT with an appropriate prompt style, we can enhance the model's capacity to tackle more complex tasks with higher accuracy and language adaptation, thus improving overall performance in real-world applications.quicker optimization but also increased the risk of overfitting due to the lack of regularization.The moderate parameters provided a reference point for evaluating more complex setups, showing the limitations of basic fine-tuning without dropout.</p>
<p>LoRA Fintune based results</p>
<p>Higher Rank with Dropout (Finetune 2): Incorporating a higher rank (32) and dropout (0.1), this configuration aimed to increase model capacity and regularization.However, it achieved a slightly lower accuracy of 12%, despite its cautious learning rate of 1e-4.This result suggests that the added complexity and regularization may not align well with the dataset's requirements, possibly due to over-parameterization or insufficient data to fully utilize the additional capacity.</p>
<p>Memory-Efficient with Lower Batch Size (Finetune 3): The third configuration utilized memory-efficient adjustments, including a lower batch size and increased gradient accumulation steps, alongside a higher learning rate of 2e-4.This setup achieved the highest accuracy of 17%.The reduced batch size likely allowed for better gradient updates, while the increased accumulation steps improved learning stability.The higher learning rate might have further facilitated effective convergence by optimizing balancing and learning dynamics.6 presents a performance comparison between the fine-tuned GPT-3.5 (turbo-0125) and GPT-4o, evaluated using standard zero-shot prompting.The results reveal notable differences in accuracy.To build the fine-tuned model, we utilized 50 examples with appropriate instructions, including explicit chain-of-thought reasoning for complex problems.This approach ensured that the model not only provided answers but also demonstrated the reasoning process behind solving math problems.The examples followed the JSONL file format as outlined in the OpenAI API documentation. Figure 13 illustrates an example of how the JSON file was structured to create the fine-tuned model.Fine-tuning GPT-3.5 with a learning rate of 0.1 over 10 epochs achieved the best performance, with an accuracy of 23.4%.However, increasing the number of epochs from 10 to 15 did not yield any further improvements, as the model's performance plateaued at 12.6%, which was equivalent to the performance achieved at 5 epochs.This suggests that fine-tuning for a higher number of epochs may lead to diminishing returns or potential overfitting.</p>
<p>In contrast, GPT-4o, which was not fine-tuned but instead evaluated using its standard zero-shot prompting capability, demonstrated a significantly superior performance of 79.2%.This highlights GPT-4o's inherent ability to generalize across tasks without the need for task-specific fine-tuning, significantly outperforming GPT-3.5, even after fine-tuning.</p>
<p>Appendix A illustrates the OpenAI Playground examples and their outputs and presents the responses of Llama 3.3 on Bengali Math Word Problems, and Appendix B showcases the correct and incorrect responses of GPT models on Bengali Math Word Problems.</p>
<p>Conclusion and Future Work</p>
<p>In this study, we presented a novel approach for solving Bengali Math Word Problems (MWPs) using Large Language Models (LLMs) with Chain of Thought (CoT) prompting.We introduced the SOMADHAN dataset, specifically designed to address Bengali MWPs requiring complex reasoning and multi-step solutions.Our experiments demonstrated that CoT prompting significantly improved the performance of LLMs, particularly for reasoning-intensive tasks in Bengali, a low-resource language.Among the models evaluated, Llama-3.3 outperformed all other models, achieving the highest accuracy of 88% with CoT and few-shot prompting.The GPT-4o model, with its substantial number of parameters, also performed exceptionally well, showing high accuracy in both zero-shot and few-shot settings.Additionally, we leveraged Low-Rank Adaptation (LoRA) fine-tuning techniques to optimize large models for Bengali MWPs.Our findings contribute to advancing the field of Bengali language processing and educational technologies, providing a foundation for future work in multilingual problem-solving tasks.</p>
<p>Q:Figure 1 :
1
Figure 1: Chain-of-Thought (CoT) Prompting enables LLMs (Large Language Model)s to improve complex reasoning of MWPs (Math Word Problems)</p>
<p>Figure 2 :
2
Figure 2: Pipeline for the development of the SOMADHAN dataset</p>
<p>Figure 3 :
3
Figure 3: Challenges faced during the translation of BGSM8K dataset</p>
<p>Question:Figure 4 :Figure 5 :
45
Figure 4: Sample example of SOMADHAN Dataset</p>
<p>Figure 6 :
6
Figure 6: Schematic diagram of our proposed prompting approach</p>
<p>Figure 7 :
7
Figure 7: Instruction (Prompt-1) for SOMADHAN Dataset (Zero Shot Prompting)</p>
<p>Figure 8 :
8
Figure 8: Instruction (Prompt-2) for SOMADHAN Dataset (Zero Shot Prompting)</p>
<p>1 . 3 .
13
Compile and upload the required data for training, ensuring the inclusion of chain-of-thought examples where necessary 2. Develop a new model that is optimized for better performance, focusing on improving reasoning abilities with intermediate steps Analyze the outcomes and make necessary adjustments to ensure that the model produces clear, logical reasoning along with its final answers 4. Utilize the refined and optimized model, leveraging the chain of thought to maintain accurate, interpretable problem-solving</p>
<p>Figure 9 :
9
Figure 9: Instruction (Prompt-1) for SOMADHAN Dataset (Few (5) Shot Prompting)</p>
<p>Figure 10 :
10
Figure 10: Instruction (Prompt-2) for SOMADHAN Dataset (Few (5) Shot Prompting)</p>
<p>content: সু মন এক ট দাকান থেক ৫ ট প ল িকেনেছ। িত ট প েলর দাম ৮ টাকা। সু মন সব প েলর জন কত টাকা খরচ কেরেছ?, role: assistant content: ক = ৫ × ৮ content: এক ট িব ংেয় ১২ ট ক আঁ কার জন একজন িচ িশ ীর েয়াজন হয়। িত ট ম রং করেত ৭ ঘ া সময় লােগ। যিদ স ইিতমেধ ই ৫ ট ঘর রং কের ফেল, তাহেল বািকটা রং করেত তার আর কত সময় লাগেব?(c) User Question</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: Instruction (Prompt-3) for PatiGonit Dataset (Few (5) Shot) Prompting</p>
<p>Figure 14 :
14
Figure 14: LoRA Fine-Tuning Process: During and After Training (LoRA, 2024)[65]</p>
<ol>
<li>3
3
Gpt-3.5 Finetune result</li>
</ol>
<p>Figure 17 :
17
Figure 17: Response generated by LLaMA 3.3 model on a Bengali Math Word Problem using Chain-of-Thought reasoning.</p>
<p>Figure 18 :
18
Figure 18: Example of correct chains of thought produced by the GPT-3.5 for the SOMADHAN dataset for Few Shot</p>
<p>Figure 19 :
19
Figure 19: Example of correct chains of thought produced by the GPT-4o for the SOMADHAN dataset for Few Shot</p>
<p>Figure 20 :
20
Figure 20: Example of correct chains of thought produced by the GPT-3.5 for the SOMADHAN dataset for Fine Tuning</p>
<p>architectures, including BiLSTM, ConvS2S, and Transformer models.Their ensemble approach achieved 68.4% accuracy on Math23k, outperforming individual models (66.7% for BiLSTM, 64.2% for ConvS2S, and 62.3% for Transformer), emphasizing normalization and model diversity.In</p>
<p>[26]ted work,[25]developed the Variational Neural Machine Translation (VNMT) model with latent variables to enhance semantic understanding, achieving BLEU scores of 32.07 (Chinese-English) and 19.58 (English-German), outperforming traditional models but facing challenges with long sentences.An improvement to Seq2Seq models was proposed by[26], who incorporated Copy and Alignment mechanisms, further optimized via Reinforcement Learning (RL), achieving accuracies of 44.5% on Alg514, 64.0% on NumWord, and 23.3% on Dolphin18KT6.Their hybrid model significantly boosted performance to 82.5%, 65.8%, and 33.2%, respectively.</p>
<p>Table 1 :
1
A Summary of Existing Works in Solving Math Word Problems
Paper TitleApproachModelDatasetAccuracyLanguageEmpoweringTransformerTransformer,PatiGonitPatiGonit:BengaliBengali EducationBasedmT5, BanglaT5,97.30%with AI: SolvingApproach,mBART50Bengali Math WordNeural MachineProblems throughTranslationTransformer(NMT)Models [22]Math word problemEnhanced MaskDeBERTaMAWPS,MAWPS:Englishsolving byDecoder and aPARA-91.0%,generatingVotingMAWPSPARA-linguistic variantsMechanismMAWPS:of problem79.1%statements [38]Chain-of-thoughtChain-of-GPT-3, LaMDA,GSM8KGSM8K:Englishprompting elicitsThoughtPaLM, UL2,63.1%reasoning in largePromptingCodexlanguage models[15]Making LargeDiverse VerifierOpenAI Models:GSM8KGSM8K:EnglishLanguage Modelson Reasoningdavinci,83.2%Better ReasonersSteptext-davinci-002,with Step-Awarecode-davinci-002Verifier [42]Self-consistencySelf-GPT-3:GSM8KGSM8K:Englishimproves chain ofConsistency,code-davinci-001,83.2%thought reasoningChain-of-code-davinci-002,in language modelsThoughtLaMDA-137B,[43]PromptingPaLM-540B(CoT)2.4 Methods Utilizing Intermediate StepsNumerous studies have demonstrated the many benefits of teaching neural networks to produce intermediate stepsthrough fine-tuning or training. Natural language intermediate steps, in particular, have shown promise in enhancingmodel robustness and interpretability. For instance,</p>
<p>Table 2 :
2
Detailed Informations of Translators
DetailsTranslator 1Translator 2Translator 3Translator 4Translator 5RoleGraduateUnder-graduate Under-graduate Under-graduate Under-graduateAge3023242422Research fieldNLPNLPNLPNLPNLPExperience4 years2 years1 years2 years1 years</p>
<p>Table 4 :
4
Performance comparison of Chain of Thoughts (CoT) prompting (Prompt-2) versus Standard prompting for various large language models on the "SOMADHAN" dataset.All metrics are Accuracy (%).(<em>) represents that the accuracy is correct but the responses of the model was in English.
ModelsParametersStandard Zero Shot Few (5) Shot Zero Shot Few (5) Shot Chain of Thoughts (CoT)deepseek-r1-distill-qwen32B39.0</em>44.0*48.0  *  (+9.0)</p>
<p>Table 5 :
5
Ablation Study with three variations of LoRA finetuning on SOMADHAN dataset.All metrics are Accuracy (%).
ParameterBaselineHigher Rank with DropoutMemory Efficient with Lower Batch SizeLoRA Parametersr163216lora alpha163216lora dropout00.10.1target modules{q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj}{q_proj, k_proj, v_proj, o_proj}{q_proj, v_proj, o_proj}use gradient checkpointingunslothTrueTrueTraining Argumentslearning rate2e-41e-42e-4per device train batch size241gradient accumulation steps428warmup steps5105max steps6010060Performance13.012.017.0From</p>
<p>Table 5 ,
5
Baseline Configuration (Finetune 1): This setup, characterized by a rank of 16, no dropout, and a moderate learning rate of 2e-4, achieved an accuracy of 13%.The simplicity of this configuration likely facilitated</p>
<p>Table 6 :
6
Performance comparison of finetuned Gpt-3.5 with Gpt-4o.All metrics are Accuracy (%).
ModelsLearning Rate Epochs Performance512.0Gpt-3.5 turbo-01250.11023.01512.0Gpt-4o(Standard Prompting--79.0Zero Shot)Table
https://github.com/openai/grade-school-math/tree/master/grade_school_math/data
While our approach showed promising results, several limitations need to be acknowledged.First, due to resource constraints and token costs, we limited our evaluation to 1000 testing samples from both the SOMADHAN and PatiGonit datasets.This reduced sample size might have impacted the generalizability of the results, and future studies could benefit from evaluating a larger subset of the dataset.Additionally, while CoT prompting improved model performance in many cases, the language adaptation for smaller models, particularly the LLaMA series, was not optimal.Models such as Llama-3 (8B) struggled to effectively solve Bengali MWPs, indicating that smaller models may not perform as well as larger ones when it comes to complex reasoning tasks in low-resource languages.The reliance on English responses for some models, even after incorporating CoT prompting, also highlights the challenge of multilingual task adaptation.For future research, expanding the SOMADHAN dataset to include a larger variety of Bengali Math Word Problems with more diverse reasoning steps could improve model training and evaluation.Future work should also focus on the development of even more efficient fine-tuning strategies, such as further optimizations of Low-Rank Adaptation (LoRA), to handle the increasing complexity of language models without incurring high computational costs.Investigating methods to ensure better language output alignment in multilingual models, such as fine-tuning for Bengali language generation, would be an essential step toward improving the overall effectiveness of LLMs in real-world applications.AcknowledgmentsThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.
Enhancing multiple-choice question answering through sequential fine-tuning and curriculum learning strategies. Gulsum Yigit, Mehmet Fatih, Amasyali , Knowledge and Information Systems. 65112023</p>
<p>A seq2seq-based approach to question answering over knowledge bases. Linjuan Wu, Peiyun Wu, Xiaowang Zhang, Semantic Technology: 9th Joint International Conference. Hangzhou, ChinaSpringer2019. November 25-27, 2019. 20209</p>
<p>Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension. Anna Rogers, Matt Gardner, Isabelle Augenstein, ACM Computing Surveys. 552021</p>
<p>Ask me: A question answering system via dynamic memory networks. Gulsum Yigit, Mehmet Fatih, Amasyali , Innovations in Intelligent Systems and Applications Conference (ASYU). 2019. 2019</p>
<p>A goal-driven tree-structured neural model for math word problems. Zhipeng Xie, Shichao Sun, International Joint Conference on Artificial Intelligence. 2019</p>
<p>Teacher-student networks with multiple decoders for solving math word problem. Jipeng Zhang, Roy , Ka-Wei Lee, Ee-Peng Lim, Wei Qin, Lei Wang, Jie Shao, Qianru Sun, International Joint Conference on Artificial Intelligence. 2020</p>
<p>Mwp-bert: Numeracy-augmented pre-training for math word problem solving. Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, NAACL-HLT. 2021</p>
<p>Templatebased math word problem solvers with recursive neural networks. Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli Gao, Bing Tian Dai, Heng Tao Shen, AAAI Conference on Artificial Intelligence. 2019</p>
<p>Graph-to-tree learning for solving math word problems. Jipeng Zhang, Lei Wang, Roy , Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim, Annual Meeting of the Association for Computational Linguistics. 2020</p>
<p>Solving math word problems with multi-encoders and multi-decoders. Yibin Shen, Cheqing Jin, International Conference on Computational Linguistics. 2020</p>
<p>Some challenges and grand challenges for computational intelligence. Edward A Feigenbaum, Journal of the ACM (JACM). 5012003</p>
<p>Mathbot-a deep learning based elementary school math word problem solver. Anish Kumar Nayak, Rajeev Patwari, Viswanathan Subramanian, 807</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>List of languages by total number of speakers. Wikipedia, 2024</p>
<p>Gpt-4o: A new multimodal ai model. 2024OpenAI</p>
<p>Gpt-3.5 turbo model documentation. 2023OpenAI</p>
<p>Llama 3: Open and efficient foundation language models. A I Meta, 2024</p>
<p>Deepseek platform documentation. Deepseek, 2025</p>
<p>Qwen chat interface. Qwen, 2025</p>
<p>Empowering bengali education with ai: Solving bengali math word problems through transformer models. Jalisha Jashim, Era , Bidyarthi Paul, Tahmid Sattar Aothoi, Mirazur Rahman Zim, Faisal Muhammad Shah, 2024 27th International Conference on Computer and Information Technology (ICCIT). IEEE2024</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of the 2017 conference on empirical methods in natural language processing. the 2017 conference on empirical methods in natural language processing2017</p>
<p>Translating a math word problem to an expression tree. Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu, arXiv:1811.056322018arXiv preprint</p>
<p>Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, Min Zhang, arXiv:1605.07869Variational neural machine translation. 2016arXiv preprint</p>
<p>Neural math word problem solver with reinforcement learning. Danqing Huang, Jing Liu, Chin-Yew Lin, Jian Yin, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational Linguistics2018</p>
<p>Graph-to-tree learning for solving math word problems. Jipeng Zhang, Lei Wang, Roy , Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Graph-to-tree neural networks for learning structured input-output translation with applications to semantic parsing and math word problem. Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu, Sheng Zhong, arXiv:2004.137812020arXiv preprint</p>
<p>Solving math word problems with multi-encoders and multi-decoders. Yibin Shen, Cheqing Jin, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, arXiv:2107.13435Mwp-bert: Numeracy-augmented pre-training for math word problem solving. 2021arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, arXiv:2002.05867Transformers as soft reasoners over language. 2020arXiv preprint</p>
<p>Rulebert: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, arXiv:2109.130062021arXiv preprint</p>
<p>Explainable multi-hop verbal reasoning through internal monologue. Zhengzhong Liang, Steven Bethard, Mihai Surdeanu, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, arXiv:2107.13435Mwp-bert: Numeracy-augmented pre-training for math word problem solving. 2021arXiv preprint</p>
<p>Math word problem solving by generating linguistic variants of problem statements. Md Syed Rifat Raiyan, Shah Nafis Faiyaz, Jawad Md, Mohsinul Kabir, Kabir, Mahmud Hasan, Md Kamrul Hasan, arXiv:2306.138992023arXiv preprint</p>
<p>Translating a math word problem to an expression tree. Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu, arXiv:1811.056322018arXiv preprint</p>
<p>A goal-driven tree-structured neural model for math word problems. Zhipeng Xie, Shichao Sun, Ijcai. 2019</p>
<p>Warm: A weakly (+ semi) supervised model for solving math word problems. Oishik Chatterjee, Isha Pandey, Aashish Waikar, Vishwajeet Kumar, Ganesh Ramakrishnan, arXiv:2104.067222021arXiv preprint</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 2022b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Howard Chen, Jacqueline He, Karthik Narasimhan, Danqi Chen, arXiv:2204.11790Can rationalization improve robustness?. 2022arXiv preprint</p>
<p>Training classifiers with natural language explanations. Braden Hancock, Martin Bringmann, Paroma Varma, Percy Liang, Stephanie Wang, Christopher Ré, Proceedings of the conference. the conferenceNIH Public Access201820181884</p>
<p>Benefits of intermediate annotations in reading comprehension. Dheeru Dua, Sameer Singh, Matt Gardner, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Using "annotator rationales" to improve machine learning for text categorization. Omar Zaidan, Jason Eisner, Christine Piatko, Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference. 2007</p>
<p>Refining language models with compositional explanations. Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, Xiang Ren, Advances in Neural Information Processing Systems. 202134</p>
<p>When can models learn from explanations? a formal framework for understanding the roles of explanation data. Peter Hase, Mohit Bansal, arXiv:2102.022012021arXiv preprint</p>
<p>Dream: Uncovering mental models behind language models. Yuling Gu, Bhavana Dalvi Mishra, Peter Clark, 2022NAACL</p>
<p>Jacob Andreas, Dan Klein, Sergey Levine, arXiv:1711.00482Learning with latent language. 2017arXiv preprint</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 201831</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, arXiv:1906.023612019arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Super-naturalinstructions. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, arXiv:2204.07705Generalization via declarative instructions on 1600+ nlp tasks. 2022arXiv preprint</p>
<p>Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, arXiv:2303.12528Multilingual evaluation of generative ai. 2023arXiv preprint</p>
<p>Fine-tuning gpt-3.5 models. 2023OpenAI</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. Hugging Face, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8914 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8914</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8914</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452" target="_blank">ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work makes one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via the ChemBERTa model, and suggests that transformers offer a promising avenue of future work for molecular representation learning and property prediction.</p>
                <p><strong>Paper Abstract:</strong> GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8914.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8914.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-style transformer (6 layers, 12 attention heads) pretrained with masked language modeling on SMILES/SELFIES strings from PubChem subsets to produce molecular representations for downstream property prediction tasks (MoleculeNet).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (RoBERTa-style, masked language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6 layers, 12 attention heads (parameter count not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on canonicalized SMILES from PubChem subsets (100K, 250K, 1M, 10M); a curated set of 77M SMILES was prepared (full 77M pretraining left to future work).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction for medicinal chemistry tasks (brain penetrability, toxicity, target inhibition; evaluated on BBBP, ClinTox, HIV, Tox21).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to generate novel chemicals in this study; trained via masked language modeling (MLM). The model supports masked-token prediction and attention-based inspection but was not employed to directly propose novel molecules here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — ChemBERTa was not used to generate or design novel molecules in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Model is fine-tuned to specific property-prediction tasks by appending a linear classification head and backpropagating on labeled datasets; no conditional molecule generation or property-conditioned design performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream classification metrics: ROC-AUC and PRC-AUC on MoleculeNet tasks (BBBP, ClinTox CT_TOX, HIV, Tox21 SR-p53). Also analyzed scaling of downstream performance with pretraining data size (ΔROC-AUC and ΔPRC-AUC reported).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemBERTa provides competitive but not state-of-the-art downstream performance vs. Chemprop baselines. Example (ChemBERTa pretrained on 10M): BBBP ROC 0.643 (vs D-MPNN 0.708), ClinTox ROC 0.733 (vs D-MPNN 0.906), HIV ROC 0.622 (vs D-MPNN 0.752), Tox21 ROC 0.728 (exceeding some baselines) while PRC often lower. Pretraining scale improves performance (average ΔROC-AUC +0.110 and ΔPRC-AUC +0.059 when scaling 100K→10M across tasks evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to directed Message Passing Neural Network (D-MPNN), Random Forest and SVM on Morgan fingerprints; generally underperforms D-MPNN but shows gains with increased pretraining data. Authors note GNNs provide useful inductive biases and hybrid graph-transformer approaches may offer better sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not applied to molecule generation in this work; lower sample efficiency relative to graph methods on evaluated tasks; not state-of-the-art on MoleculeNet benchmarks (except Tox21 ROC); engineering and compute required to scale to full PubChem/ZINC; environmental cost of large-scale pretraining noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8914.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8914.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES RNN seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent sequence-to-sequence models trained on SMILES that generate focused libraries for lead optimization and lead-generation (typical approach in prior cheminformatics work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES RNN (seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (sequence-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; referenced as prior work training RNNs on SMILES corpora to generate libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — focused library generation and lead optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of SMILES sequences from a trained RNN (sequence generation); used to create focused molecular libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper (only cited as an example of generative SMILES models).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Typically produces focused libraries by training or fine-tuning on target-relevant molecules; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported here (this paper only cites the approach in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as a standard approach in cheminformatics for molecule generation; no experimental results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited historically as a non-transformer generative approach; the current paper positions transformers as an alternative with different tradeoffs (scalability, pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8914.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8914.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE continuous molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variational autoencoder (VAE)-based approach that learns continuous latent representations of molecules enabling optimization and generation of candidate compounds for chemical design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular VAE (continuous latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; cited as prior data-driven chemical design work using molecular datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular design via latent-space optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Optimization and sampling in continuous latent space followed by decoding to SMILES to propose new molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported here; original work used latent-space search to propose novel candidates (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables property-directed optimization by searching latent space toward desired objectives; specific evaluation not included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of data-driven automatic chemical design; this paper does not re-run or evaluate the method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as part of prior generative modeling literature; no direct comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8914.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8914.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grammar VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar variational autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational autoencoder that incorporates grammar constraints to improve syntactic validity of generated molecular strings (e.g., SMILES), used in chemical generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grammar variational autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grammar VAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder with grammar constraints</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation / chemical design with improved syntactic validity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Latent-space sampling/decoding constrained by grammar to produce valid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Grammar constraints aim to improve validity of generated molecules; further specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as relevant generative-modeling prior art; no experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other generative approaches (RNNs, VAEs) but no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8914.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8914.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer architecture applied to chemical reaction prediction that outputs product predictions and calibrated uncertainties; an example of transformer use in chemistry (primarily reaction prediction/retrosynthesis-related).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (sequence-to-sequence for reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Reaction prediction / retrosynthesis planning (chemical synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence transformer used to predict reaction outcomes (product SMILES) and uncertainties; applicable for synthesis planning rather than de novo molecular library generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable in the context described here (focus is reaction prediction rather than generating novel candidate molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Applied to reaction prediction tasks with uncertainty calibration; not described here as a generative molecule design tool.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example where transformers have been applied in chemistry (reaction prediction); not experimentally evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Referenced as a domain-specific transformer success in chemistry compared to earlier RNN approaches; no comparative experiments shown here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8914.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8914.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-referencing Embedded Strings (SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A molecular string representation designed so that every valid SELFIES corresponds to a valid molecule, intended to improve robustness of machine-learning-driven molecule generation and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-referencing Embedded Strings (selfies): A 100% robust molecular string representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFIES (string representation used with transformer pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Molecular string representation (robust encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>In this paper, ChemBERTa was pretrained on SELFIES-encoded molecules (same PubChem subsets used in SMILES experiments; specific subset for SELFIES pretraining not enumerated beyond Tox21 SR-p53 downstream test).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Intended to support molecule generation and modeling by ensuring validity of decoded strings; used here for representation learning for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Representation designed to improve robustness of generated molecules (every valid SELFIES maps to a valid molecule). In this paper, SELFIES was used to pretrain ChemBERTa but not to generate novel compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — SELFIES was not used here to generate new molecules; downstream performance on property tasks showed no significant difference vs SMILES in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Hypothesized to yield more robust models for generation tasks, but in this study SELFIES pretraining did not significantly change downstream Tox21 SR-p53 performance versus SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream performance measured by PRC-AUC on Tox21 SR-p53 for the SELFIES-pretrained ChemBERTa; no significant difference found.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemBERTa pretrained on SELFIES showed no significant improvement over SMILES-pretrained ChemBERTa on the Tox21 SR-p53 downstream task in this paper; authors recommend further benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SELFIES proposed as a potentially more robust alternative to SMILES for generative tasks; here it was compared to SMILES only in property-prediction pretraining and found similar results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Authors note that further benchmarking is needed; SELFIES did not confer measurable downstream gains in their limited evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder <em>(Rating: 2)</em></li>
                <li>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery <em>(Rating: 1)</em></li>
                <li>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery <em>(Rating: 1)</em></li>
                <li>Smiles-bert: large scale unsupervised pre-training for molecular property prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8914",
    "paper_id": "paper-95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "ChemBERTa",
            "name_full": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
            "brief_description": "A RoBERTa-style transformer (6 layers, 12 attention heads) pretrained with masked language modeling on SMILES/SELFIES strings from PubChem subsets to produce molecular representations for downstream property prediction tasks (MoleculeNet).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChemBERTa",
            "model_type": "Transformer (RoBERTa-style, masked language model)",
            "model_size": "6 layers, 12 attention heads (parameter count not reported)",
            "training_data": "Pretrained on canonicalized SMILES from PubChem subsets (100K, 250K, 1M, 10M); a curated set of 77M SMILES was prepared (full 77M pretraining left to future work).",
            "application_domain": "Molecular property prediction for medicinal chemistry tasks (brain penetrability, toxicity, target inhibition; evaluated on BBBP, ClinTox, HIV, Tox21).",
            "generation_method": "Not used to generate novel chemicals in this study; trained via masked language modeling (MLM). The model supports masked-token prediction and attention-based inspection but was not employed to directly propose novel molecules here.",
            "novelty_of_chemicals": "Not applicable — ChemBERTa was not used to generate or design novel molecules in this paper.",
            "application_specificity": "Model is fine-tuned to specific property-prediction tasks by appending a linear classification head and backpropagating on labeled datasets; no conditional molecule generation or property-conditioned design performed in this work.",
            "evaluation_metrics": "Downstream classification metrics: ROC-AUC and PRC-AUC on MoleculeNet tasks (BBBP, ClinTox CT_TOX, HIV, Tox21 SR-p53). Also analyzed scaling of downstream performance with pretraining data size (ΔROC-AUC and ΔPRC-AUC reported).",
            "results_summary": "ChemBERTa provides competitive but not state-of-the-art downstream performance vs. Chemprop baselines. Example (ChemBERTa pretrained on 10M): BBBP ROC 0.643 (vs D-MPNN 0.708), ClinTox ROC 0.733 (vs D-MPNN 0.906), HIV ROC 0.622 (vs D-MPNN 0.752), Tox21 ROC 0.728 (exceeding some baselines) while PRC often lower. Pretraining scale improves performance (average ΔROC-AUC +0.110 and ΔPRC-AUC +0.059 when scaling 100K→10M across tasks evaluated).",
            "comparison_to_other_methods": "Compared directly to directed Message Passing Neural Network (D-MPNN), Random Forest and SVM on Morgan fingerprints; generally underperforms D-MPNN but shows gains with increased pretraining data. Authors note GNNs provide useful inductive biases and hybrid graph-transformer approaches may offer better sample efficiency.",
            "limitations_and_challenges": "Not applied to molecule generation in this work; lower sample efficiency relative to graph methods on evaluated tasks; not state-of-the-art on MoleculeNet benchmarks (except Tox21 ROC); engineering and compute required to scale to full PubChem/ZINC; environmental cost of large-scale pretraining noted.",
            "uuid": "e8914.0",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SMILES RNN seq2seq",
            "name_full": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "brief_description": "Recurrent sequence-to-sequence models trained on SMILES that generate focused libraries for lead optimization and lead-generation (typical approach in prior cheminformatics work).",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "mention_or_use": "mention",
            "model_name": "SMILES RNN (seq2seq)",
            "model_type": "Recurrent Neural Network (sequence-to-sequence)",
            "model_size": null,
            "training_data": "Not specified in this paper; referenced as prior work training RNNs on SMILES corpora to generate libraries.",
            "application_domain": "Drug discovery — focused library generation and lead optimization.",
            "generation_method": "Direct generation of SMILES sequences from a trained RNN (sequence generation); used to create focused molecular libraries.",
            "novelty_of_chemicals": "Not reported in this paper (only cited as an example of generative SMILES models).",
            "application_specificity": "Typically produces focused libraries by training or fine-tuning on target-relevant molecules; details not provided in this paper.",
            "evaluation_metrics": "Not reported here (this paper only cites the approach in related work).",
            "results_summary": "Mentioned as a standard approach in cheminformatics for molecule generation; no experimental results provided in this paper.",
            "comparison_to_other_methods": "Cited historically as a non-transformer generative approach; the current paper positions transformers as an alternative with different tradeoffs (scalability, pretraining).",
            "limitations_and_challenges": "Not discussed in detail in this paper.",
            "uuid": "e8914.1",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "VAE continuous molecular design",
            "name_full": "Automatic chemical design using a data-driven continuous representation of molecules",
            "brief_description": "Variational autoencoder (VAE)-based approach that learns continuous latent representations of molecules enabling optimization and generation of candidate compounds for chemical design.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "mention_or_use": "mention",
            "model_name": "Molecular VAE (continuous latent space)",
            "model_type": "Variational Autoencoder (VAE)",
            "model_size": null,
            "training_data": "Not specified in this paper; cited as prior data-driven chemical design work using molecular datasets.",
            "application_domain": "Drug discovery / molecular design via latent-space optimization.",
            "generation_method": "Optimization and sampling in continuous latent space followed by decoding to SMILES to propose new molecules.",
            "novelty_of_chemicals": "Not reported here; original work used latent-space search to propose novel candidates (details not provided in this paper).",
            "application_specificity": "Enables property-directed optimization by searching latent space toward desired objectives; specific evaluation not included in this paper.",
            "evaluation_metrics": "Not reported in this paper.",
            "results_summary": "Cited as an example of data-driven automatic chemical design; this paper does not re-run or evaluate the method.",
            "comparison_to_other_methods": "Mentioned as part of prior generative modeling literature; no direct comparisons provided here.",
            "limitations_and_challenges": "Not discussed in this paper.",
            "uuid": "e8914.2",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Grammar VAE",
            "name_full": "Grammar variational autoencoder",
            "brief_description": "A variational autoencoder that incorporates grammar constraints to improve syntactic validity of generated molecular strings (e.g., SMILES), used in chemical generation tasks.",
            "citation_title": "Grammar variational autoencoder",
            "mention_or_use": "mention",
            "model_name": "Grammar VAE",
            "model_type": "Variational Autoencoder with grammar constraints",
            "model_size": null,
            "training_data": "Not specified in this paper.",
            "application_domain": "Molecule generation / chemical design with improved syntactic validity.",
            "generation_method": "Latent-space sampling/decoding constrained by grammar to produce valid SMILES.",
            "novelty_of_chemicals": "Not reported here.",
            "application_specificity": "Grammar constraints aim to improve validity of generated molecules; further specifics not provided in this paper.",
            "evaluation_metrics": "Not provided in this paper.",
            "results_summary": "Cited as relevant generative-modeling prior art; no experiments reported here.",
            "comparison_to_other_methods": "Mentioned alongside other generative approaches (RNNs, VAEs) but no direct comparisons in this paper.",
            "limitations_and_challenges": "Not discussed in this paper.",
            "uuid": "e8914.3",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Molecular Transformer",
            "name_full": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "brief_description": "A transformer architecture applied to chemical reaction prediction that outputs product predictions and calibrated uncertainties; an example of transformer use in chemistry (primarily reaction prediction/retrosynthesis-related).",
            "citation_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "Transformer (sequence-to-sequence for reaction prediction)",
            "model_size": null,
            "training_data": "Not specified in this paper.",
            "application_domain": "Reaction prediction / retrosynthesis planning (chemical synthesis).",
            "generation_method": "Sequence-to-sequence transformer used to predict reaction outcomes (product SMILES) and uncertainties; applicable for synthesis planning rather than de novo molecular library generation.",
            "novelty_of_chemicals": "Not applicable in the context described here (focus is reaction prediction rather than generating novel candidate molecules).",
            "application_specificity": "Applied to reaction prediction tasks with uncertainty calibration; not described here as a generative molecule design tool.",
            "evaluation_metrics": "Not detailed in this paper.",
            "results_summary": "Cited as an example where transformers have been applied in chemistry (reaction prediction); not experimentally evaluated in this work.",
            "comparison_to_other_methods": "Referenced as a domain-specific transformer success in chemistry compared to earlier RNN approaches; no comparative experiments shown here.",
            "limitations_and_challenges": "Not discussed in this paper.",
            "uuid": "e8914.4",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SELFIES",
            "name_full": "Self-referencing Embedded Strings (SELFIES)",
            "brief_description": "A molecular string representation designed so that every valid SELFIES corresponds to a valid molecule, intended to improve robustness of machine-learning-driven molecule generation and manipulation.",
            "citation_title": "Self-referencing Embedded Strings (selfies): A 100% robust molecular string representation",
            "mention_or_use": "use",
            "model_name": "SELFIES (string representation used with transformer pretraining)",
            "model_type": "Molecular string representation (robust encoding)",
            "model_size": null,
            "training_data": "In this paper, ChemBERTa was pretrained on SELFIES-encoded molecules (same PubChem subsets used in SMILES experiments; specific subset for SELFIES pretraining not enumerated beyond Tox21 SR-p53 downstream test).",
            "application_domain": "Intended to support molecule generation and modeling by ensuring validity of decoded strings; used here for representation learning for property prediction.",
            "generation_method": "Representation designed to improve robustness of generated molecules (every valid SELFIES maps to a valid molecule). In this paper, SELFIES was used to pretrain ChemBERTa but not to generate novel compounds.",
            "novelty_of_chemicals": "Not applicable — SELFIES was not used here to generate new molecules; downstream performance on property tasks showed no significant difference vs SMILES in the experiments reported.",
            "application_specificity": "Hypothesized to yield more robust models for generation tasks, but in this study SELFIES pretraining did not significantly change downstream Tox21 SR-p53 performance versus SMILES.",
            "evaluation_metrics": "Downstream performance measured by PRC-AUC on Tox21 SR-p53 for the SELFIES-pretrained ChemBERTa; no significant difference found.",
            "results_summary": "ChemBERTa pretrained on SELFIES showed no significant improvement over SMILES-pretrained ChemBERTa on the Tox21 SR-p53 downstream task in this paper; authors recommend further benchmarking.",
            "comparison_to_other_methods": "SELFIES proposed as a potentially more robust alternative to SMILES for generative tasks; here it was compared to SMILES only in property-prediction pretraining and found similar results.",
            "limitations_and_challenges": "Authors note that further benchmarking is needed; SELFIES did not confer measurable downstream gains in their limited evaluation.",
            "uuid": "e8914.5",
            "source_info": {
                "paper_title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Grammar variational autoencoder",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery",
            "rating": 1
        },
        {
            "paper_title": "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery",
            "rating": 1
        },
        {
            "paper_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
            "rating": 1
        }
    ],
    "cost": 0.015187499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</h1>
<p>Seyone Chithrananda<br>University of Toronto<br>seyone.chithrananda@utoronto.ca<br>Gabriel Grand<br>Reverie Labs<br>gabe@reverielabs.com<br>Bharath Ramsundar<br>DeepChem<br>bharath.ramsundar@gmail.com</p>
<h4>Abstract</h4>
<p>GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. While not at state-of-the-art, ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.</p>
<h2>1 Motivation</h2>
<p>Molecular property prediction has seen a recent resurgence thanks to the success of graph neural networks (GNNs) on various benchmark tasks [1, 2, 3, 4, 5, 6]. However, data scarcity remains a fundamental challenge for supervised learning in a domain in which each new labelled data point requires costly and time-consuming laboratory testing. Determining effective methods to make use of large amounts of unlabeled structure data remains an important unsolved challenge.
Over the past two years, the transformer [7, 8] has emerged as a robust architecture for learning self-supervised representations of text. Transformer pretraining plus task-specific finetuning provides substantial gains over previous approaches to many tasks in natural language processing (NLP) [9, 10, 11]. Meanwhile, software infrastructure for transformers is maturing rapidly: HuggingFace [12] provides streamlined pretraining and finetuning pipelines, while packages like BertViz [13] offer sophisticated interfaces for attention visualization. Given the availability of millions of SMILES strings, transformers offer an interesting alternative to both expert-crafted and GNN-learned fingerprints. In particular, the masked language-modeling (MLM) pretraining task [8] commonly used for BERT-style architectures is analogous to atom masking tasks used in graph settings [14]. Moreover, since modern transformers are engineered to scale to massive NLP corpora, they offer practical advantages over GNNs in terms of efficiency and throughput.
Though simple in concept, the application of transformers to molecular data presents several questions that are severely underexplored. For instance: How does pretraining dataset size affect downstream task performance? What tokenization strategies work best for SMILES? Does replacing SMILES</p>
<p>with a more robust string representation like SELFIES [15] improve performance? We aim to address these questions via one of the first systematic evaluations of transformers on molecular property prediction tasks.</p>
<h1>2 Related Work</h1>
<p>In cheminformatics, there is a long tradition of training language models directly on SMILES to learn continuous latent representations [16, 17, 18]. Typically, these are RNN sequence-to-sequence models and their goal is to facilitate auxiliary lead optimization tasks; e.g., focused library generation [19]. Thus far, discussion of the transformer architecture in chemistry has been largely focused on a particular application to reaction prediction [20].
Some recent work has pretrained transformers for molecular property prediction and reported promising results [21, 22]. However, the datasets used for pretraining have been relatively small ( 861 K compounds from ChEMBL and 2M compounds from ZINC, respectively). Other work has used larger pretraining datasets ( 18.7 M compounds from ZINC) [23] but the effects of pretraining dataset size, tokenizer, and string representation were not explored. In still other work, transformers were used for supervised learning directly without pretraining [24].
Recently, a systematic study of self-supervised pretraining strategies for GNNs helped to clarify the landscape of those methods [14]. Our goal is to undertake a similar investigation for transformers to assess the viability of this architecture for property prediction.</p>
<h2>3 Methods</h2>
<p>ChemBERTa is based on the RoBERTa [25] transformer implementation in HuggingFace [12]. Our implementation of RoBERTa uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. So far, we have released 15 pre-trained ChemBERTa models on the Huggingface's model hub; these models have collectively received over 30,000 Inference API calls to date. ${ }^{1}$
We used the popular Chemprop library for all baselines [6]. We trained the directed Message Passing Neural Network (D-MPNN) with default hyperparameters as well as the sklearn-based [26] Random Forest (RF) and Support Vector Machine (SVM) models from Chemprop, which use 2048-bit Morgan fingerprints from RDKit [27, 28].</p>
<h3>3.1 PreTraining on PubChem 77M</h3>
<p>We adopted our pretraining procedure from RoBERTa, which masks $15 \%$ of the tokens in each input string. We used a max. vocab size of 52 K tokens and max. sequence length of 512 tokens. We trained for 10 epochs on all PubChem subsets except for the 10M subset, on which we trained for 3 epochs to avoid observed overfitting. Our hypothesis is that, in learning to recover masked tokens, the model forms a representational topology of chemical space that should generalize to property prediction tasks.</p>
<p>For pretraining, we curated a dataset of 77M unique SMILES from PubChem [29], the world's largest open-source collection of chemical structures. The SMILES were canonicalized and globally shuffled to facilitate large-scale pretraining. We divided this dataset into subsets of $100 \mathrm{~K}, 250 \mathrm{~K}, 1 \mathrm{M}$, and 10M. Pretraining on the largest subset took approx. 48 hours on a single NVIDIA V100 GPU. We make this dataset publicly available and leave pretraining on the full 77 M set to future work.</p>
<h3>3.2 Finetuning on MoleculeNet</h3>
<p>We evaluated our models on several classification tasks from MoleculeNet [30] selected to cover a range of dataset sizes ( $1.5 \mathrm{~K}-41.1 \mathrm{~K}$ examples) and medicinal chemistry applications (brain penetrability, toxicity, and on-target inhibition). These included the BBBP, ClinTox, HIV, and Tox21 datasets. For datasets with multiple tasks, we selected a single representative task: the clinical toxicity (CT_TOX) task from ClinTox and the p53 stress-response pathway activation (SR-p53) task from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| | BBBP | | ClinTox (CT_TOX) | | HIV | | Tox21 (SR-p53) | |
| | 2,039 | | 1,478 | | 41,127 | | 7,831 | |
| | ROC | PRC | ROC | PRC | ROC | PRC | ROC | PRC |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ChemBERTa 10M | 0.643 | 0.620 | 0.733 | 0.975 | 0.622 | 0.119 | 0.728 | 0.207 |
| D-MPNN | 0.708 | 0.697 | 0.906 | 0.993 | 0.752 | 0.152 | 0.688 | 0.429 |
| RF | 0.681 | 0.692 | 0.693 | 0.968 | 0.780 | 0.383 | 0.724 | 0.335 |
| SVM | 0.702 | 0.724 | 0.833 | 0.986 | 0.763 | 0.364 | 0.708 | 0.345 |</p>
<p>Table 1: Comparison of ChemBERTa pretrained on 10M PubChem compounds and Chemprop baselines on selected MoleculeNet tasks. We report both ROC-AUC and PRC-AUC to give a full picture of performance on class-imbalanced tasks.</p>
<p>Tox21. For each dataset, we generated an 80/10/10 train/valid/test split using the scaffold splitter from DeepChem [31]. During finetuning, we appended a linear classification layer and backpropagated through the base model. We finetuned models for up to 25 epochs with early stopping on ROCAUC. We release a tutorial in DeepChem which allows users to go through loading a pre-trained ChemBERTa model, running masked prediction tasks, visualizing the attention of the model on several molecules, and fine-tuning the model on the Tox21 SR-p53 dataset.</p>
<h2>4 Results</h2>
<p>On the MoleculeNet tasks that we evaluated, ChemBERTa approaches, but does not beat, the strong baselines from Chemprop (Table 1). Nevertheless, downstream performance of ChemBERTa scales well with more pretraining data (Fig. 1). On average, scaling from 100K to 10M resulted in $\Delta$ ROC-AUC $=+0.110$ and $\Delta$ PRC-AUC $=+0.059$. (HIV was omitted from this analysis due to resource constraints.) These results suggest that ChemBERTa learns more robust representations with additional data and is able to leverage this information when learning downstream tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Scaling the pretraining size (100K, 250K, 1M, 10M) produces consistent improvements in downstream task performance on BBBP, ClinTox, and Tox21. Mean $\Delta$ AUC across all three tasks with a 68% confidence interval is shown in light blue.</p>
<p>While Tox21 ROC-AUC is better than the baselines, PR-AUC is considerably lower.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Attention in GNNs highlights a problematic ketone in a Tox21 compound. (b) Attention over SMILES tokens in ChemBERTa provides a close analogue to graph attention. (c) Neural stack trace enables fine-grained introspection of neuron behavior. (b - c) produced via BertViz [13].</p>
<h3>4.1 Tokenizers</h3>
<p>Our default tokenization strategy uses a Byte-Pair Encoder (BPE) from the HuggingFace tokenizers library [12]. BPE is a hybrid between character and word-level representations, which allows for the handling of large vocabularies in natural language corpora. Motivated by the intuition that rare and unknown words can often be decomposed into multiple known subwords, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters [32]. We compare this tokenization algorithm with a custom SmilesTokenizer based on a regex from [20], which we have released as part of DeepChem [31].3</p>
<p>To compare tokenizers, we pretrained two identical models on the PubChem-1M set. The pretrained models were evaluated on the Tox21 SR-p53 task. We found that the SmilesTokenizer narrowly outperformed BPE by ΔPRC-AUC = +0.015. Though this result suggests that a more semantically-relevant tokenization may provide performance benefits, further benchmarking on additional datasets is needed to validate this finding.</p>
<h3>4.2 SMILES vs. SELFIES</h3>
<p>In addition to SMILES, we pretrained ChemBERTA on SELFIES (SELF-referencing Embedded Strings) [15]. SELFIES is an alternate molecular string representation designed for machine learning. Because every valid SELFIES corresponds to a valid molecule, we hypothesized that SELFIES would lead to a more robust model. However, we found no significant difference in downstream performance on the Tox21 SR-p53 task. Further benchmarking is needed to validate this finding.</p>
<h3>4.3 Attention Visualization</h3>
<p>We used BertViz [13] to inspect the attention heads of ChemBERTa (SmilesTokenizer version) on Tox21, and contrast them to the molecular graph visualization of an attention-based GNN. We found certain neurons that were selective for chemically-relevant functional groups, and aromatic rings. We also observed other neurons that tracked bracket closures – a finding in keeping with results on attention-based RNNs showing the ability to track nested parentheses [33, 34].</p>
<h2>5 Discussion</h2>
<p>In this work, we introduce ChemBERTa, a transformer architecture for molecular property prediction. Initial results show that MLM pretraining provides a boost in predictive power for models on selected downstream tasks from MoleculeNet. However, with the possible exception of Tox21, ChemBERTa still performs below state-of-the-art on these tasks.</p>
<p>Our current analysis covers only a small portion of the hypothesis space we hope to explore. We plan to expand our evaluations to all of MoleculeNet, undertake more systematic hyperparameter</p>
<p><sup>3</sup>https://deepchem.readthedocs.io/en/latest/tokenizers.html#smilestokenizer</p>
<p>tuning, experiment with larger masking rates, and explore multitask finetuning. In parallel, we aim to scale up pretraining, first to the full PubChem 77M dataset, then to even larger sets like ZINC-15 (with 270 million compounds). This work will require us to improve our engineering infrastructure considerably.</p>
<p>As we scale up, we are also actively investigating methods to improve sample efficiency. Alternative text-based pretraining methods like ELECTRA may be useful [10]. Separately, there is little question that graph representations provide useful inductive biases for learning molecular structures. Recent hybrid graph transformer models [22, 35] may provide better sample efficiency while retaining the scalability of attention-based architectures.</p>
<h1>Broader Impact</h1>
<p>A core goal of AI for drug discovery is to accelerate the development of new and potentially life-saving medicines. Research to improve the accuracy and generalizability of molecular property prediction methods contributes directly to these aims. Nevertheless, machine learning-and particularly largescale pretraining of the form we undertake here-is a resource-intensive process that has a growing carbon footprint [36]. According to the Machine Learning Emissions Calculator (https://mlco2. github.io/impact), we estimate that our pretraining generated roughly $17.1 \mathrm{~kg} \mathrm{CO}_{2}$ eq (carbondioxide equivalent) of emissions. Fortunately, Google Cloud Platform, which we used for this work, is certified carbon-neutral and offsets $100 \%$ of emissions (https://cloud.google.com/ sustainability). Even as we advocate for further exploration of large-scale pretraining for property prediction, we also encourage other researchers to be mindful of the environmental impact of these efforts and opt for sustainable cloud compute solutions where possible.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We would like to thank the Tyler Cowen and the Emergent Ventures fellowship for providing the research grant to S.C. for cloud computing and various research expenses, alongside the Thiel Foundation for funding the grant. Thanks to Mario Krenn, Alston Lo, Akshat Nigam, Professor Alan Aspuru-Guzik and the entire Aspuru-Guzik group for early discussions and mentorship regarding the potiential for applying large-scale transformers on molecular strings, as well as in motivating the utilization of SELFIES in this work.</p>
<p>We would also like to thank the entire DeepChem team for their support and early discussions on fostering the ChemBERTa concept, and helping with designing and hosting the Tokenizers API and ChemBERTa tutorial. Thanks to the Reverie team for authorizing our usage of the PubChem 77M dataset, which was processed, filtered and split by them.</p>
<h2>References</h2>
<p>[1] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 22242232, 2015.
[2] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595-608, 2016.
[3] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[4] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
[5] Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen. Convolutional embedding of attributed molecular graphs for physical property prediction. Journal of chemical information and modeling, 57(8):1757-1772, 2017.
[6] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molec-</p>
<p>ular representations for property prediction. Journal of chemical information and modeling, 59(8):3370-3388, 2019.
[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
[9] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.
[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[12] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, pages arXiv-1910, 2019.
[13] Jesse Vig. A multiscale visualization of attention in the transformer model. CoRR, abs/1906.05714, 2019.
[14] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
[15] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 2020.
[16] Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. In Proceedings of the 8th ACM international conference on bioinformatics, computational biology, and health informatics, pages 285-294, 2017.
[17] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.
[18] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.
[19] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120-131, 2018.
[20] Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
[21] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738, 2019.
[22] Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.
[23] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pages 429-436, 2019.</p>
<p>[24] Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer network. arXiv preprint arXiv:1905.12712, 2019.
[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.
[26] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
[27] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742-754, 2010.
[28] Greg Landrum et al. Rdkit: Open-source cheminformatics. 2006.
[29] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102-D1109, 2019.
[30] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.
[31] B Ramsundar, P Eastman, E Feinberg, J Gomes, K Leswing, A Pappu, M Wu, and V Pande. Deepchem: Democratizing deep-learning for drug discovery, quantum chemistry, materials science and biology, 2016.
[32] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.
[33] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks can perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.
[34] Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. Learning the dyck language with attention-based seq2seq models. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 138-146, 2019.
[35] Anonymous. Modelling drug-target binding affinity using a bert based graph neural network. Submitted to International Conference on Learning Representations, 2021.
[36] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The main model directory can be viewed here. Each model includes the specific tokenizer (BPE, SMILEStokenized), representation (SMILES, SELFIES) and number of training steps ( $\left\lceil 150 \mathrm{k}^{\prime}\right.$ ) appended in its name.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
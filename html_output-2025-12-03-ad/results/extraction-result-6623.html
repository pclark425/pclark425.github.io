<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6623 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6623</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6623</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-d7726499e1d9a1ae883d792b82b58068d9a4de91</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7726499e1d9a1ae883d792b82b58068d9a4de91" target="_blank">Learning to Rehearse in Long Sequence Memorization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The Rehearsal Memory (RM) is proposed to enhance long-sequence memorization by self-supervised rehearsal with a history sampler to alleviate the gradual forgetting of early information.</p>
                <p><strong>Paper Abstract:</strong> Existing reasoning tasks often have an important assumption that the input contents can be always accessed while reasoning, requiring unlimited storage resources and suffering from severe time delay on long sequences. To achieve efficient reasoning on long sequences with limited storage resources, memory augmented neural networks introduce a human-like write-read memory to compress and memorize the long input sequence in one pass, trying to answer subsequent queries only based on the memory. But they have two serious drawbacks: 1) they continually update the memory from current information and inevitably forget the early contents; 2) they do not distinguish what information is important and treat all contents equally. In this paper, we propose the Rehearsal Memory (RM) to enhance long-sequence memorization by self-supervised rehearsal with a history sampler. To alleviate the gradual forgetting of early information, we design self-supervised rehearsal training with recollection and familiarity tasks. Further, we design a history sampler to select informative fragments for rehearsal training, making the memory focus on the crucial information. We evaluate the performance of our rehearsal memory by the synthetic bAbI task and several downstream tasks, including text/video question answering and recommendation on long sequences.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6623.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6623.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rehearsal Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A slot-based, differentiable write–read external memory trained with self-supervised rehearsal (recollection and familiarity) and an independent history sampler to improve long-range memorization and task reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rehearsal Memory (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compresses an input stream into a fixed-size memory M of K slot vectors (segment-level encoding). Writes by slot-to-item attention to produce aligned slot inputs and a gated GRU update per slot; reads via multi-hop soft-attention over slots for reasoning and via encoder-decoder multi-head attention during rehearsal. Training includes task-specific supervision plus two self-supervised rehearsal tasks (recollection: reconstruct masked historical fragments via contrastive loss; familiarity: binary discrimination of seen vs. corrupted fragments) and a separately trained history sampler that selects informative fragments based on attention to guide rehearsal.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external write–read slot memory (fixed-size slot buffer / rehearsal memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Dense slot feature vectors (one vector per memory slot, dimension d_x), representing compressed segment information</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: slot-to-item attention to align segment features to slots followed by a GRU gated update (m_k^{t+1} = GRU(m_k^t, l_k^t)); Read: multi-hop attention (softmax over slot attention weights) during reasoning; during rehearsal the memory is used as keys/values in encoder–decoder multi-head attention so decoder tokens can attend to all slots. History sampler selects fragments by attention weights computed from a teacher model that can access raw X.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI (synthetic QA); NarrativeQA (long-text QA); ActivityNet-QA (long-video QA); XLong (lifelong sequence recommendation); Synthetic long-sequence reasoning benchmark (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>question answering (short and long sequence), video QA, long-sequence recommendation, synthetic multi-hop memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 0.33 ± 0.15, best error 0.12 (error rate); NarrativeQA: Val MRR 29.4, Test MRR 28.7 (MRR); ActivityNet-QA: Accuracy 36.3% (accuracy); XLong (recommendation): AUC 0.8817 (AUC); Synthetic dataset: Early 28.42, Later 31.71 (accuracy %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation (memory present but without self-supervised rehearsal): NarrativeQA Val MRR 27.9 / Test MRR 27.5; ActivityNet-QA Accuracy 24.6%; XLong AUC 0.8745; Synthetic: Early 25.79 / Later 31.38. (These are reported ablations where rehearsal losses are removed.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), Mean Reciprocal Rank (MRR) (NarrativeQA), Accuracy (ActivityNet-QA), AUC (XLong), Accuracy (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors emphasize storage and reasoning efficiency (O(1) storage independent of |X|) and reusability of memory; require choosing memory slot number K and segment length N (increasing K improves performance until a plateau around 20 slots); additional training overhead due to self-supervised rehearsal and need to train an independent history sampler (teacher) that has access to raw X during training. Larger mask ratio (50%) required in rehearsal increases reliance on memory during training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RM still sensitive to memory configuration: too few slots or too-short segments degrade performance (evidence may be scattered); slightly underperforms the SOTA directly-reasoning HCRN on ActivityNet-QA; requires a history sampler trained with access to raw X (privileged information) during training—though sampler is not needed at test time; benefits reduced if a random sampler is used rather than the learned history sampler.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhu Zhang, Chang Zhou, Jianxin Ma, Zhijie Lin, Jingren Zhou, Hongxia Yang, Zhou Zhao (2021). Learning to Rehearse in Long Sequence Memorization. Proceedings of the 38th International Conference on Machine Learning, PMLR 139.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6623.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented neural network that couples a neural controller with a differentiable external memory (read/write) allowing learned addressing and content-based/temporal links for read/write operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External differentiable memory with learned read/write controllers; used as a memory-based baseline that stores and retrieves content via differentiable addressing mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external write–read memory (memory-augmented neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External memory matrix of vector slots (content vectors) with learned addressing and temporal/link structures (as described in DNC literature)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned differentiable read/write controllers with content-based addressing and learned link mechanisms (as per DNC paradigm); in this paper used as a baseline with multi-hop attention read adaptation for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI (synthetic QA); NarrativeQA (long-text QA); ActivityNet-QA (video QA); Synthetic long-sequence benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>synthetic QA, long-text QA, video QA, synthetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 16.7 ± 7.6, best error 3.8 (error rate); NarrativeQA Val MRR 25.8, Test MRR 25.2; ActivityNet-QA Accuracy 30.3%; Synthetic: Early 20.56, Later 26.59 (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned in related work as a memory-augmented approach that enables reasoning from compressed memory; no paper-specific runtime/latency trade-offs reported here beyond general comments about storage/efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In experiments DNC shows relatively poor performance on long-sequence tasks and in early evidence retrieval in the synthetic benchmark (suffers from gradual forgetting on long inputs, as discussed in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., ... & Lillicrap, T. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6623.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early memory-augmented neural network that provides a differentiable external memory with learnable read/write heads to enable algorithmic tasks and episodic storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Controller-network coupled with a differentiable external memory and content/location-based read/write heads; cited as a classic MANN motivating memory-based long-sequence reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external write–read memory (MANN)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory matrix of vector slots storing content vectors</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Differentiable read/write with learned addressing (content and positional/location addressing), as described in the original NTM work</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned in related work; paper notes MANNs (including NTM) learn maintenance only from final task losses and can forget early contents when sequences are long.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as suffering from gradual forgetting when trained only with final-answer supervision on long input streams (no specialized long-term rehearsal losses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6623.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NUTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Stored-Program Memory (referred to as NUTM in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented model from prior work that stores both data and programs in memory to perform computations; used here as a memory-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural stored-program memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural stored-program memory (NUTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-based model that stores and manipulates both data and program-like representations in an external memory; used here as a competitive memory baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory-augmented external memory (stored-program memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory slots encoding stored data and program-like structures (vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned memory operations for storing and retrieving stored-program style contents (content-based addressing and manipulation as in the cited work); used here with the same multi-hop attention read as other baselines for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>synthetic QA, long-text QA, video QA, synthetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 5.6 ± 1.9, best error 3.3; NarrativeQA Val MRR 27.7, Test MRR 27.2; ActivityNet-QA Accuracy 33.1%; Synthetic: Early 24.31, Later 29.71 (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not specifically quantified in this paper beyond general limitations of prior MANNs (forgetting, equal treatment of information).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shown to perform worse than RM on the evaluated long-sequence tasks, indicating limitations in long-term memorization compared to rehearsal-trained memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Le, H., Tran, T., & Venkatesh, S. (2019). Neural stored-program memory. arXiv preprint arXiv:1906.08862.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6623.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DMSDNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed Memory based Self-Supervised DNC (DMSDNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent variant that augments a differentiable neural computer with distributed memory and a self-supervised memory loss to encourage writing current inputs into memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distributed memory based self-supervised differentiable neural computer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DMSDNC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A DNC variant that incorporates a self-supervised memory loss focusing on remembering current information and uses distributed memory blocks; used here as a memory-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external memory with distributed/multi-block structure</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory blocks / slots holding vector representations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Differentiable read/write (DNC-like) with added self-supervised memory loss to encourage storing current inputs</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>synthetic QA, long-text QA, video QA, synthetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 1.53 ± 1.33, best error 0.16; NarrativeQA Val MRR 28.1, Test MRR 27.5; ActivityNet-QA Accuracy 32.4%; Synthetic: Early 24.92, Later 30.74 (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes DMSDNC focuses on remembering current information; authors of current paper argue DMSDNC ignores long-term rehearsal and thus does not fully solve gradual forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>According to the paper, DMSDNC focuses on remembering current inputs and may not address long-term memorization; in experiments DMSDNC performs worse than RM on long-sequence tasks that require long-term recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Park, T., Choi, I., & Lee, M. (2020). Distributed memory based self-supervised differentiable neural computer. arXiv preprint arXiv:2007.10637.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6623.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Attentive Associative Memory (STM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory module that uses self-attention mechanisms for associative memory operations, used as a memory-based baseline for long-sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attentive associative memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STM (self-attentive associative memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Associative memory implemented with self-attention; stores information at segment/sentence level and retrieves via attention for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>self-attentive associative memory (attention-based external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fragment/slot vectors (aggregated using self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention-based associative read/write operations (self-attention for associative memory retrieval); in experiments its reasoning module was normalized to the paper's multi-hop attention setup.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>synthetic QA, long-text QA, video QA, synthetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 0.39 ± 0.18, best error 0.15; NarrativeQA Val MRR 27.2, Test MRR 26.7; ActivityNet-QA Accuracy 33.7%; Synthetic: Early 23.55, Later 29.64 (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>STM benefits from sentence/segment-level modeling; no specific resource/latency trade-offs are reported beyond general memory capacity considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs worse than RM in experiments, indicating limitations in long-term memorization when not combined with rehearsal objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Le, H., Tran, T., & Venkatesh, S. (2020). Self-attentive associative memory. arXiv preprint arXiv:2002.03519.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6623.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compressive Transformer (CT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer variant that compresses past activations into a smaller compressed memory to extend context length beyond standard Transformer limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Compressive transformers for long-range sequence modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Compressive Transformer (CT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a memory by mapping past (long) transformer activations into a compressed FIFO compressed memory (a queue) so older contexts are preserved in compressed form; used as a memory-based baseline (Transformer-based compression).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>compressed replay memory (compressed FIFO queue of past activations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compressed past activations / compressed segments (vectors) stored in a FIFO structure</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Transformer-style attention augmented by a compressed memory; compression performed periodically to push older activations into a smaller compressed buffer (FIFO), which is then used as read keys/values</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI; NarrativeQA; ActivityNet-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>synthetic QA, long-text QA, video QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI: mean error 0.81 ± 0.26, best error 0.34; NarrativeQA Val MRR 28.7, Test MRR 28.3; ActivityNet-QA Accuracy 35.4%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Compression is efficient but implemented as a FIFO queue: when the slot number is small, CT's memorization range is severely limited and it completely forgets contents beyond the queue range; sensitivity to slot count observed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Per paper, CT degrades badly when the number of slots is too few because the FIFO compressed memory forgets contents beyond its range; RM outperforms CT when slots are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rae, J. W., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. P. (2019). Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6623.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2E-MN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-End Memory Network (E2E-MN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory network that stores facts in a memory and performs multiple hops of attention for reasoning; used as a directly-reasoning baseline in NarrativeQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>End-to-End Memory Network (E2E-MN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory network that stores input facts and performs multi-hop attention over them for query answering; in this paper used as a directly-reasoning baseline (accesses raw X when answering).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory network (stored fact memory accessed via attention/hops)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored fact embeddings in an addressable memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Multi-hop attention over stored facts (memory slots) to obtain query-relevant evidence</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NarrativeQA (long-text QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-text question answering / multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>NarrativeQA Val MRR 29.1, Test MRR 28.6 (reported as a directly-reasoning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MRR</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Treated as a directly-reasoning baseline (requires access to raw X at query time), so it does not have the storage-efficiency benefits of compressed rehearsal memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Direct methods (including E2E-MN when accessing raw X) require holding full input X and re-encoding at query time, which is costly for very long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). End-to-end memory networks. In Advances in Neural Information Processing Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6623.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HPMN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Personalized Memory Network (HPMN) / Hierarchical memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical RNN-based memory model for lifelong sequential user behavior modeling used as a baseline for long-sequence recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lifelong sequential modeling with personalized memorization for user response prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HPMN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds memory via hierarchical RNNs to capture long-term user behavior; used as a memory-based baseline on the XLong recommendation dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical RNN-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hierarchically aggregated behavior vectors (RNN hidden states aggregated into memory structures)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Hierarchical aggregation and attention over hierarchical memory representations for prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XLong (lifelong sequence recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-sequence recommendation / user behavior modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>XLong AUC 0.8645</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not specifically enumerated in this paper; HPMN is presented as a memory-based competitor that can capture long-term personalization but is outperformed by RM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In experiments HPMN underperforms RM and some directly reasoning models on XLong, suggesting limitations in representing long-term interests compared to RM's rehearsal approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ren, K., Qin, J., Fang, Y., Zhang, W., Zheng, L., Bian, W., Zhou, G., Xu, J., Yu, Y., Zhu, X., et al. (2019). Lifelong sequential modeling with personalized memorization for user response prediction. In Proceedings of SIGIR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6623.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6623.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIMN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-Integrated Multi-Interest Network (MIMN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-based model that introduces a write–read memory to capture multiple user interests for long sequential recommendation; used as a baseline on XLong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Practice on long sequential user behavior modeling for click-through rate prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MIMN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Introduces a write–read memory to model and retrieve multiple user interests from long behavior sequences; used as a memory baseline for recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>write–read external memory for multi-interest representation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory slots representing different user interest vectors</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write-read operations to update memory with behavior and retrieve relevant interests for prediction (paper-specific details from Pi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XLong (lifelong sequence recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-sequence recommendation / user behavior modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>XLong AUC 0.8731</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>AUC</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Presented as a memory-based approach that improves recommendation by modeling multiple interests; specific trade-offs not detailed in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Outperformed by RM in experiments on XLong (RM AUC 0.8817), indicating RM's rehearsal training yields better long-term interest representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Pi, Q., Bian, W., Zhou, G., Zhu, X., & Gai, K. (2019). Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of KDD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Rehearse in Long Sequence Memorization', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Compressive transformers for long-range sequence modelling <em>(Rating: 2)</em></li>
                <li>Distributed memory based self-supervised differentiable neural computer <em>(Rating: 2)</em></li>
                <li>Self-attentive associative memory <em>(Rating: 2)</em></li>
                <li>Lifelong sequential modeling with personalized memorization for user response prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6623",
    "paper_id": "paper-d7726499e1d9a1ae883d792b82b58068d9a4de91",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "RM",
            "name_full": "Rehearsal Memory",
            "brief_description": "A slot-based, differentiable write–read external memory trained with self-supervised rehearsal (recollection and familiarity) and an independent history sampler to improve long-range memorization and task reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Rehearsal Memory (RM)",
            "agent_description": "Compresses an input stream into a fixed-size memory M of K slot vectors (segment-level encoding). Writes by slot-to-item attention to produce aligned slot inputs and a gated GRU update per slot; reads via multi-hop soft-attention over slots for reasoning and via encoder-decoder multi-head attention during rehearsal. Training includes task-specific supervision plus two self-supervised rehearsal tasks (recollection: reconstruct masked historical fragments via contrastive loss; familiarity: binary discrimination of seen vs. corrupted fragments) and a separately trained history sampler that selects informative fragments based on attention to guide rehearsal.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external write–read slot memory (fixed-size slot buffer / rehearsal memory)",
            "memory_representation": "Dense slot feature vectors (one vector per memory slot, dimension d_x), representing compressed segment information",
            "memory_access_mechanism": "Write: slot-to-item attention to align segment features to slots followed by a GRU gated update (m_k^{t+1} = GRU(m_k^t, l_k^t)); Read: multi-hop attention (softmax over slot attention weights) during reasoning; during rehearsal the memory is used as keys/values in encoder–decoder multi-head attention so decoder tokens can attend to all slots. History sampler selects fragments by attention weights computed from a teacher model that can access raw X.",
            "task_name": "bAbI (synthetic QA); NarrativeQA (long-text QA); ActivityNet-QA (long-video QA); XLong (lifelong sequence recommendation); Synthetic long-sequence reasoning benchmark (custom)",
            "task_category": "question answering (short and long sequence), video QA, long-sequence recommendation, synthetic multi-hop memorization",
            "performance_with_memory": "bAbI: mean error 0.33 ± 0.15, best error 0.12 (error rate); NarrativeQA: Val MRR 29.4, Test MRR 28.7 (MRR); ActivityNet-QA: Accuracy 36.3% (accuracy); XLong (recommendation): AUC 0.8817 (AUC); Synthetic dataset: Early 28.42, Later 31.71 (accuracy %)",
            "performance_without_memory": "Ablation (memory present but without self-supervised rehearsal): NarrativeQA Val MRR 27.9 / Test MRR 27.5; ActivityNet-QA Accuracy 24.6%; XLong AUC 0.8745; Synthetic: Early 25.79 / Later 31.38. (These are reported ablations where rehearsal losses are removed.)",
            "has_comparative_results": true,
            "performance_metric": "Error (bAbI), Mean Reciprocal Rank (MRR) (NarrativeQA), Accuracy (ActivityNet-QA), AUC (XLong), Accuracy (synthetic)",
            "tradeoffs_reported": "Authors emphasize storage and reasoning efficiency (O(1) storage independent of |X|) and reusability of memory; require choosing memory slot number K and segment length N (increasing K improves performance until a plateau around 20 slots); additional training overhead due to self-supervised rehearsal and need to train an independent history sampler (teacher) that has access to raw X during training. Larger mask ratio (50%) required in rehearsal increases reliance on memory during training.",
            "limitations_or_failure_cases": "RM still sensitive to memory configuration: too few slots or too-short segments degrade performance (evidence may be scattered); slightly underperforms the SOTA directly-reasoning HCRN on ActivityNet-QA; requires a history sampler trained with access to raw X (privileged information) during training—though sampler is not needed at test time; benefits reduced if a random sampler is used rather than the learned history sampler.",
            "citation": "Zhu Zhang, Chang Zhou, Jianxin Ma, Zhijie Lin, Jingren Zhou, Hongxia Yang, Zhou Zhao (2021). Learning to Rehearse in Long Sequence Memorization. Proceedings of the 38th International Conference on Machine Learning, PMLR 139.",
            "uuid": "e6623.0",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer (DNC)",
            "brief_description": "A memory-augmented neural network that couples a neural controller with a differentiable external memory (read/write) allowing learned addressing and content-based/temporal links for read/write operations.",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "use",
            "agent_name": "Differentiable Neural Computer (DNC)",
            "agent_description": "External differentiable memory with learned read/write controllers; used as a memory-based baseline that stores and retrieves content via differentiable addressing mechanisms.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external write–read memory (memory-augmented neural network)",
            "memory_representation": "External memory matrix of vector slots (content vectors) with learned addressing and temporal/link structures (as described in DNC literature)",
            "memory_access_mechanism": "Learned differentiable read/write controllers with content-based addressing and learned link mechanisms (as per DNC paradigm); in this paper used as a baseline with multi-hop attention read adaptation for fairness.",
            "task_name": "bAbI (synthetic QA); NarrativeQA (long-text QA); ActivityNet-QA (video QA); Synthetic long-sequence benchmark",
            "task_category": "synthetic QA, long-text QA, video QA, synthetic memorization",
            "performance_with_memory": "bAbI: mean error 16.7 ± 7.6, best error 3.8 (error rate); NarrativeQA Val MRR 25.8, Test MRR 25.2; ActivityNet-QA Accuracy 30.3%; Synthetic: Early 20.56, Later 26.59 (accuracy)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)",
            "tradeoffs_reported": "Mentioned in related work as a memory-augmented approach that enables reasoning from compressed memory; no paper-specific runtime/latency trade-offs reported here beyond general comments about storage/efficiency.",
            "limitations_or_failure_cases": "In experiments DNC shows relatively poor performance on long-sequence tasks and in early evidence retrieval in the synthetic benchmark (suffers from gradual forgetting on long inputs, as discussed in the paper).",
            "citation": "Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., ... & Lillicrap, T. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476.",
            "uuid": "e6623.1",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "NTM",
            "name_full": "Neural Turing Machine (NTM)",
            "brief_description": "An early memory-augmented neural network that provides a differentiable external memory with learnable read/write heads to enable algorithmic tasks and episodic storage.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "agent_name": "Neural Turing Machine (NTM)",
            "agent_description": "Controller-network coupled with a differentiable external memory and content/location-based read/write heads; cited as a classic MANN motivating memory-based long-sequence reasoning.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external write–read memory (MANN)",
            "memory_representation": "Memory matrix of vector slots storing content vectors",
            "memory_access_mechanism": "Differentiable read/write with learned addressing (content and positional/location addressing), as described in the original NTM work",
            "task_name": "",
            "task_category": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Mentioned in related work; paper notes MANNs (including NTM) learn maintenance only from final task losses and can forget early contents when sequences are long.",
            "limitations_or_failure_cases": "Cited as suffering from gradual forgetting when trained only with final-answer supervision on long input streams (no specialized long-term rehearsal losses).",
            "citation": "Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.",
            "uuid": "e6623.2",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "NUTM",
            "name_full": "Neural Stored-Program Memory (referred to as NUTM in paper)",
            "brief_description": "A memory-augmented model from prior work that stores both data and programs in memory to perform computations; used here as a memory-based baseline.",
            "citation_title": "Neural stored-program memory",
            "mention_or_use": "use",
            "agent_name": "Neural stored-program memory (NUTM)",
            "agent_description": "A memory-based model that stores and manipulates both data and program-like representations in an external memory; used here as a competitive memory baseline.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory-augmented external memory (stored-program memory)",
            "memory_representation": "Memory slots encoding stored data and program-like structures (vectors)",
            "memory_access_mechanism": "Learned memory operations for storing and retrieving stored-program style contents (content-based addressing and manipulation as in the cited work); used here with the same multi-hop attention read as other baselines for fair comparison.",
            "task_name": "bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark",
            "task_category": "synthetic QA, long-text QA, video QA, synthetic memorization",
            "performance_with_memory": "bAbI: mean error 5.6 ± 1.9, best error 3.3; NarrativeQA Val MRR 27.7, Test MRR 27.2; ActivityNet-QA Accuracy 33.1%; Synthetic: Early 24.31, Later 29.71 (accuracy)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)",
            "tradeoffs_reported": "Not specifically quantified in this paper beyond general limitations of prior MANNs (forgetting, equal treatment of information).",
            "limitations_or_failure_cases": "Shown to perform worse than RM on the evaluated long-sequence tasks, indicating limitations in long-term memorization compared to rehearsal-trained memory.",
            "citation": "Le, H., Tran, T., & Venkatesh, S. (2019). Neural stored-program memory. arXiv preprint arXiv:1906.08862.",
            "uuid": "e6623.3",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DMSDNC",
            "name_full": "Distributed Memory based Self-Supervised DNC (DMSDNC)",
            "brief_description": "A recent variant that augments a differentiable neural computer with distributed memory and a self-supervised memory loss to encourage writing current inputs into memory.",
            "citation_title": "Distributed memory based self-supervised differentiable neural computer",
            "mention_or_use": "use",
            "agent_name": "DMSDNC",
            "agent_description": "A DNC variant that incorporates a self-supervised memory loss focusing on remembering current information and uses distributed memory blocks; used here as a memory-based baseline.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external memory with distributed/multi-block structure",
            "memory_representation": "Memory blocks / slots holding vector representations",
            "memory_access_mechanism": "Differentiable read/write (DNC-like) with added self-supervised memory loss to encourage storing current inputs",
            "task_name": "bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark",
            "task_category": "synthetic QA, long-text QA, video QA, synthetic memorization",
            "performance_with_memory": "bAbI: mean error 1.53 ± 1.33, best error 0.16; NarrativeQA Val MRR 28.1, Test MRR 27.5; ActivityNet-QA Accuracy 32.4%; Synthetic: Early 24.92, Later 30.74 (accuracy)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)",
            "tradeoffs_reported": "Paper notes DMSDNC focuses on remembering current information; authors of current paper argue DMSDNC ignores long-term rehearsal and thus does not fully solve gradual forgetting.",
            "limitations_or_failure_cases": "According to the paper, DMSDNC focuses on remembering current inputs and may not address long-term memorization; in experiments DMSDNC performs worse than RM on long-sequence tasks that require long-term recall.",
            "citation": "Park, T., Choi, I., & Lee, M. (2020). Distributed memory based self-supervised differentiable neural computer. arXiv preprint arXiv:2007.10637.",
            "uuid": "e6623.4",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "STM",
            "name_full": "Self-Attentive Associative Memory (STM)",
            "brief_description": "A memory module that uses self-attention mechanisms for associative memory operations, used as a memory-based baseline for long-sequence tasks.",
            "citation_title": "Self-attentive associative memory",
            "mention_or_use": "use",
            "agent_name": "STM (self-attentive associative memory)",
            "agent_description": "Associative memory implemented with self-attention; stores information at segment/sentence level and retrieves via attention for reasoning.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "self-attentive associative memory (attention-based external memory)",
            "memory_representation": "Fragment/slot vectors (aggregated using self-attention)",
            "memory_access_mechanism": "Attention-based associative read/write operations (self-attention for associative memory retrieval); in experiments its reasoning module was normalized to the paper's multi-hop attention setup.",
            "task_name": "bAbI; NarrativeQA; ActivityNet-QA; Synthetic benchmark",
            "task_category": "synthetic QA, long-text QA, video QA, synthetic memorization",
            "performance_with_memory": "bAbI: mean error 0.39 ± 0.18, best error 0.15; NarrativeQA Val MRR 27.2, Test MRR 26.7; ActivityNet-QA Accuracy 33.7%; Synthetic: Early 23.55, Later 29.64 (accuracy)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA), Accuracy (synthetic)",
            "tradeoffs_reported": "STM benefits from sentence/segment-level modeling; no specific resource/latency trade-offs are reported beyond general memory capacity considerations.",
            "limitations_or_failure_cases": "Performs worse than RM in experiments, indicating limitations in long-term memorization when not combined with rehearsal objectives.",
            "citation": "Le, H., Tran, T., & Venkatesh, S. (2020). Self-attentive associative memory. arXiv preprint arXiv:2002.03519.",
            "uuid": "e6623.5",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "CT",
            "name_full": "Compressive Transformer (CT)",
            "brief_description": "A Transformer variant that compresses past activations into a smaller compressed memory to extend context length beyond standard Transformer limits.",
            "citation_title": "Compressive transformers for long-range sequence modelling",
            "mention_or_use": "use",
            "agent_name": "Compressive Transformer (CT)",
            "agent_description": "Maintains a memory by mapping past (long) transformer activations into a compressed FIFO compressed memory (a queue) so older contexts are preserved in compressed form; used as a memory-based baseline (Transformer-based compression).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "compressed replay memory (compressed FIFO queue of past activations)",
            "memory_representation": "Compressed past activations / compressed segments (vectors) stored in a FIFO structure",
            "memory_access_mechanism": "Transformer-style attention augmented by a compressed memory; compression performed periodically to push older activations into a smaller compressed buffer (FIFO), which is then used as read keys/values",
            "task_name": "bAbI; NarrativeQA; ActivityNet-QA",
            "task_category": "synthetic QA, long-text QA, video QA",
            "performance_with_memory": "bAbI: mean error 0.81 ± 0.26, best error 0.34; NarrativeQA Val MRR 28.7, Test MRR 28.3; ActivityNet-QA Accuracy 35.4%",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Error (bAbI), MRR (NarrativeQA), Accuracy (ActivityNet-QA)",
            "tradeoffs_reported": "Compression is efficient but implemented as a FIFO queue: when the slot number is small, CT's memorization range is severely limited and it completely forgets contents beyond the queue range; sensitivity to slot count observed.",
            "limitations_or_failure_cases": "Per paper, CT degrades badly when the number of slots is too few because the FIFO compressed memory forgets contents beyond its range; RM outperforms CT when slots are limited.",
            "citation": "Rae, J. W., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. P. (2019). Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507.",
            "uuid": "e6623.6",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "E2E-MN",
            "name_full": "End-to-End Memory Network (E2E-MN)",
            "brief_description": "A memory network that stores facts in a memory and performs multiple hops of attention for reasoning; used as a directly-reasoning baseline in NarrativeQA experiments.",
            "citation_title": "End-to-end memory networks",
            "mention_or_use": "use",
            "agent_name": "End-to-End Memory Network (E2E-MN)",
            "agent_description": "Memory network that stores input facts and performs multi-hop attention over them for query answering; in this paper used as a directly-reasoning baseline (accesses raw X when answering).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory network (stored fact memory accessed via attention/hops)",
            "memory_representation": "Stored fact embeddings in an addressable memory",
            "memory_access_mechanism": "Multi-hop attention over stored facts (memory slots) to obtain query-relevant evidence",
            "task_name": "NarrativeQA (long-text QA)",
            "task_category": "long-text question answering / multi-hop reasoning",
            "performance_with_memory": "NarrativeQA Val MRR 29.1, Test MRR 28.6 (reported as a directly-reasoning baseline)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "MRR",
            "tradeoffs_reported": "Treated as a directly-reasoning baseline (requires access to raw X at query time), so it does not have the storage-efficiency benefits of compressed rehearsal memory.",
            "limitations_or_failure_cases": "Direct methods (including E2E-MN when accessing raw X) require holding full input X and re-encoding at query time, which is costly for very long sequences.",
            "citation": "Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). End-to-end memory networks. In Advances in Neural Information Processing Systems.",
            "uuid": "e6623.7",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "HPMN",
            "name_full": "Hierarchical Personalized Memory Network (HPMN) / Hierarchical memory",
            "brief_description": "A hierarchical RNN-based memory model for lifelong sequential user behavior modeling used as a baseline for long-sequence recommendation.",
            "citation_title": "Lifelong sequential modeling with personalized memorization for user response prediction",
            "mention_or_use": "use",
            "agent_name": "HPMN",
            "agent_description": "Builds memory via hierarchical RNNs to capture long-term user behavior; used as a memory-based baseline on the XLong recommendation dataset.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "hierarchical RNN-based memory",
            "memory_representation": "Hierarchically aggregated behavior vectors (RNN hidden states aggregated into memory structures)",
            "memory_access_mechanism": "Hierarchical aggregation and attention over hierarchical memory representations for prediction",
            "task_name": "XLong (lifelong sequence recommendation)",
            "task_category": "long-sequence recommendation / user behavior modeling",
            "performance_with_memory": "XLong AUC 0.8645",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "AUC",
            "tradeoffs_reported": "Not specifically enumerated in this paper; HPMN is presented as a memory-based competitor that can capture long-term personalization but is outperformed by RM.",
            "limitations_or_failure_cases": "In experiments HPMN underperforms RM and some directly reasoning models on XLong, suggesting limitations in representing long-term interests compared to RM's rehearsal approach.",
            "citation": "Ren, K., Qin, J., Fang, Y., Zhang, W., Zheng, L., Bian, W., Zhou, G., Xu, J., Yu, Y., Zhu, X., et al. (2019). Lifelong sequential modeling with personalized memorization for user response prediction. In Proceedings of SIGIR.",
            "uuid": "e6623.8",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "MIMN",
            "name_full": "Memory-Integrated Multi-Interest Network (MIMN)",
            "brief_description": "A memory-based model that introduces a write–read memory to capture multiple user interests for long sequential recommendation; used as a baseline on XLong.",
            "citation_title": "Practice on long sequential user behavior modeling for click-through rate prediction",
            "mention_or_use": "use",
            "agent_name": "MIMN",
            "agent_description": "Introduces a write–read memory to model and retrieve multiple user interests from long behavior sequences; used as a memory baseline for recommendation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "write–read external memory for multi-interest representation",
            "memory_representation": "Memory slots representing different user interest vectors",
            "memory_access_mechanism": "Write-read operations to update memory with behavior and retrieve relevant interests for prediction (paper-specific details from Pi et al.)",
            "task_name": "XLong (lifelong sequence recommendation)",
            "task_category": "long-sequence recommendation / user behavior modeling",
            "performance_with_memory": "XLong AUC 0.8731",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "AUC",
            "tradeoffs_reported": "Presented as a memory-based approach that improves recommendation by modeling multiple interests; specific trade-offs not detailed in the current paper.",
            "limitations_or_failure_cases": "Outperformed by RM in experiments on XLong (RM AUC 0.8817), indicating RM's rehearsal training yields better long-term interest representations.",
            "citation": "Pi, Q., Bian, W., Zhou, G., Zhu, X., & Gai, K. (2019). Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of KDD.",
            "uuid": "e6623.9",
            "source_info": {
                "paper_title": "Learning to Rehearse in Long Sequence Memorization",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural turing machines",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "Compressive transformers for long-range sequence modelling",
            "rating": 2
        },
        {
            "paper_title": "Distributed memory based self-supervised differentiable neural computer",
            "rating": 2
        },
        {
            "paper_title": "Self-attentive associative memory",
            "rating": 2
        },
        {
            "paper_title": "Lifelong sequential modeling with personalized memorization for user response prediction",
            "rating": 1
        }
    ],
    "cost": 0.022793249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning to Rehearse in Long Sequence Memorization</h1>
<p>Zhu Zhang ${ }^{<em> 12}$ Chang Zhou ${ }^{</em> 2}$ Jianxin Ma ${ }^{2}$ Zhijie Lin ${ }^{1}$ Jingren Zhou ${ }^{2}$ Hongxia Yang ${ }^{2}$ Zhou Zhao ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Existing reasoning tasks often have an important assumption that the input contents can be always accessed while reasoning, requiring unlimited storage resources and suffering from severe time delay on long sequences. To achieve efficient reasoning on long sequences with limited storage resources, memory augmented neural networks introduce a human-like write-read memory to compress and memorize the long input sequence in one pass, trying to answer subsequent queries only based on the memory. But they have two serious drawbacks: 1) they continually update the memory from current information and inevitably forget the early contents; 2) they do not distinguish what information is important and treat all contents equally. In this paper, we propose the Rehearsal Memory (RM) to enhance long-sequence memorization by self-supervised rehearsal with a history sampler. To alleviate the gradual forgetting of early information, we design self-supervised rehearsal training with recollection and familiarity tasks. Further, we design a history sampler to select informative fragments for rehearsal training, making the memory focus on the crucial information. We evaluate the performance of our rehearsal memory by the synthetic bAbI task and several downstream tasks, including text/video question answering and recommendation on long sequences.</p>
<h2>1. Introduction</h2>
<p>In recent years, the tremendous progress of neural networks has enabled machines to perform reasoning given the input contents $X$ and a query $Q$, e.g., infer the answer of given questions from the text/video stream in text/video question answering (Seo et al., 2016; Jin et al., 2019a; Le et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2020b), or predict whether a user will click the given item based on the user behavior sequence in recommender systems (Ren et al., 2019; Pi et al., 2019; Zhang et al., 2021). Studies that achieve top performances at such reasoning tasks usually have an important assumption that the raw input contents $X$ can be always accessed while answering the query $Q$. In this setting, the complex interaction between $X$ and $Q$ can be designed to extract query-relevant information from $X$ with little loss, such as co-attention interaction (Xiong et al., 2016; Jin et al., 2019b). Though these methods (Seo et al., 2016; Le et al., 2020b) can effectively handle these reasoning tasks, they require unlimited storage resources to hold the original input $X$. Further, they have to encode the whole input contents and develop the elaborate interaction from scratch, which are time-consuming. This is not acceptable for online services that require instant response such as recommender systems, as the input sequence becomes extremely long (Ren et al., 2019).</p>
<p>To achieve efficient reasoning on long sequences with limited storage resources, memory augmented neural networks (MANNs) (Graves et al., 2014; 2016) introduce a write-read memory $M$ with fixed-size capacity (size much smaller than $|X|$ ) to compress and remember the input contents $X$. In the inference phase, they can capture queryrelevant clues directly from the memory $M$, i.e., the raw input $X$ is not needed at the time of answering $Q$. This procedure is very similar to the daily situation of our human beings, i.e., we may not know the tasks $Q$ that we will answer in the future when we are experiencing current events, but we have the instincts to continually memorize our experiences within the limited memory capacity, from which we can rapidly recall and draw upon past events to guide our behaviors given the present tasks (Moscovitch et al., 2016; Baddeley, 1992). Such human-like memory-based methods bring three benefits for long-sequence reasoning: 1) storage efficiency: we only need to maintain the limited memory $M$ rather than $X ; 2$ ) reasoning efficiency: inference over $M$ and $Q$ is more lightweight than inference over $X$ and $Q$ from scratch ; 3) high reusability: the maintained memory $M$ can be reused for any query $Q$.</p>
<p>However, existing MANNs have two serious drawbacks for memory-based long-sequence reasoning. First, these approaches ignore the long-term memorization ability of the memory. They learn how to maintain the memory $M$ only</p>
<p>by back-propagated losses to the final answer and do not design any specific training target for long-term memorization, which inevitably lead to the gradual forgetting of early contents (Le et al., 2019a). That is, when dealing with the long input sequences, these approaches may fail to answer the query relevant to early contents due to the lack of long-term memorization. Second, determining what to remember in the memory with limited capacity is crucial to retain sufficient clues for subsequent $Q$. This is especially challenging since the information compression procedure in $M$ is totally not aware of $Q$. But existing MANNs do not distinguish what information is important and treat all contents equally. Thus, due to lack of information discrimination, these approaches may store too much meaningless information but lose vital evidence for subsequent reasoning.</p>
<p>In this paper, we propose the Rehearsal Memory (RM) to enhance long-sequence memorization by self-supervised rehearsal with a history sampler. To overcome gradual forgetting of early information and increase the generalization ability of the memorization technique, we develop two extra self-supervised rehearsal tasks to recall the recorded history contents from the memory. The two tasks are inspired by the observation that human beings can recall details nearby some specific events and distinguish whether a series of events happened in the history, which respectively correspond to two different memory processes revealed in cognitive, neuropsychological, and neuroimaging studies, i.e., recollection and familiarity (Yonelinas, 2002; Moscovitch et al., 2016). Concretely, the recollection task aims to predict the masked items in history fragments $H$, which are sampled from the original input stream and parts of items are masked as the prediction target. This task tries to endow the memory with the recollection ability that enables one to relive past episodes. And the familiarity task tries to distinguish whether a historical fragment $H$ ever appears in the input stream, where we directly sample positive fragments from the input stream and replace parts of the items in positive ones as negative fragments. This task resembles the familiarity process that recognizes experienced events or stimulus as familiar.</p>
<p>To make the rehearsal memory have the ability of remembering the crucial information, we further train an independent history sampler to select informative fragments $H$ for selfsupervised rehearsal training. Similar to the teacher-student architecture in knowledge distillation (Hinton et al., 2015), we expect the history sampler (i.e. the teacher) to capture the characteristic of important fragments in the current environment and guide the rehearsal memory (i.e. the student) to remember task-relevant clues. Concretely, we independently train a conventional reasoning model that can access raw contents $X$ while answering the query $Q$ as the history sampler. The model contains the attention interaction between history fragments $H$ and the query $Q$, where the attention
weight can be regarded as the importance of each fragment. After training, the history sampler can select the vital fragments based on the attention weights for self-supervised rehearsal training. This is similar to the procedure where human beings learn to memorize meaningful experiences, i.e., we have gone through a lot of tasks to slowly understand which information is likely to be used in future tasks and pay more attention to them during memorization (Moscovitch et al., 2016).</p>
<p>In conclusion, we propose the self-supervised memory rehearsal to enhance the long-sequence memorization for subsequent reasoning. We design the self-supervised recollection and familiarity tasks to solve how to rehearse, which can alleviate the gradual forgetting of early information. Further, we adopt a history sampler to decide what to rehearse, which guides the memory to remember critical information. We illustrate the ability of our rehearsal memory via the synthetic bAbI task and several downstream tasks, including text/video question answering and recommendation on long sequences.</p>
<h2>2. Related Works</h2>
<p>Memory augmented neural networks (MANNs) introduce the external memory to store and access the past contents by differentiable write-read operators. Neural Turing Machine (NTM) (Graves et al., 2014) and Differentiable Neural Computer (DNC) (Graves et al., 2016) are the typical MANNs for human-like memorization and reasoning, whose inferences rely only on the memory with limited capacity rather than starting from the original input. In this line of research, Rae et al. (2016) adopt the sparse memory accessing to reduce computational cost. Csordás \&amp; Schmidhuber (2019) introduce the key/value separation problem of content-based addressing and adopt a mask for memory operations as a solution. Le et al. (2019b) manipulate both data and programs stored in memory to perform universal computations. And Santoro et al. (2018); Le et al. (2020a) consider the complex relational reasoning with the information they remember.</p>
<p>However, these works exploit MANNs mainly to help capture complex dependencies in dealing with input sequences, but do not explore the potential of MANNs in the field of memory-based long-sequence reasoning. They learn how to maintain the memory only by back-propagated losses to the final answer but do not design specific training target for long-term memorization, inevitably incurring gradual forgetting of early contents during memorizing long sequences (Le et al., 2019a). Recently, there are a few works trying to alleviate this problem. Le et al. (2019a) propose to measure "remember" ability by the final gradient on the early input, and adopt a uniform writing operation on the memory to balance between maximizing memorization and forgetting. Munkhdalai et al. (2019) design the meta-learned</p>
<p>neural memory instead of the conventional array-structured memory and memorize the current and past information by reconstructing the written values via the memory function. Besides, Compressive Transformer (Rae et al., 2019) maps the past memory to a smaller compressed memory for long-range sequence learning, where the compressed memory preserves much original information by a high compression rate. But considering the compressed memory is implemented by a FIFO queue, it will completely forget the contents beyond a fixed range.</p>
<p>Our approach is different and parallel to these techniques, we try to enhance long-sequence memorization by selfsupervised memory rehearsal, i.e., recall the recorded history contents from the memory to overcome gradual forgetting of early information. We design the recollection task to enable the memory to relive past episodes and adopt the familiarity task to make the memory recognize experienced events. A recent work (Park et al., 2020) also introduces a self-supervised memory loss to ensure how well the current input is written to the memory, but it only focuses on remembering the current information and ignoring the long-term memorization. Further, compared to previous techniques that have no assumptions on what behavior will be remembered the most, we propose a history sampler to distinguish the characteristic of important fragments in the current environment and guide the memory rehearsal to remember task-relevant clues.</p>
<h2>3. Rehearsal Memory</h2>
<h3>3.1. Problem Formulation</h3>
<p>Given the input stream $\mathbf{X}=\left{\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \cdots\right}$ and a query $\mathbf{Q}$, the directly reasoning methods (Seo et al., 2016; Le et al., 2020b) learn the model $\mathcal{T}(\mathbf{X}, \mathbf{Q})$ to predict the answer $\mathbf{A}$. These is an important assumption that the input stream $\mathbf{X}$ can be always accessed while reasoning. And complex interaction between $\mathbf{X}$ and $\mathbf{Q}$ can be designed to extract query-relevant information in $\mathcal{T}(\mathbf{X}, \mathbf{Q})$. Obviously, these methods have to store the original input $\mathbf{X}$ and infer the answer $\mathbf{A}$ from scratch when the query $\mathbf{Q}$ is known. In this paper, we explore the human-like memory-based reasoning on long sequences, where we compress the input stream $\mathbf{X}$ into a fixed-size memory $\mathbf{M}=\left{\mathbf{m<em k="1">{k}\right}</em>)$.}^{K}$ with $K$ memory slots and then infer the answer $\mathbf{A}$ for any relevant query $\mathbf{Q}$ by $\mathbf{A}=\mathcal{R}(\mathbf{M}, \mathbf{Q})$. Here we only need to store the compressed memory $\mathbf{M}$, which can be updated in real-time and reused for a series of queries. Since the slot number $K$ in the memory is irrelevant to the input length $|X|$, this setting only requires $O(1)$ storage space rather than $O(|X|)$ in directly reasoning model $\mathcal{T}(\mathbf{X}, \mathbf{Q</p>
<p>As shown in Figure 1, we apply a rehearsal memory machine $\mathcal{G}<em k="k">{\Theta}(\mathbf{X})$ to compress the input stream $\mathbf{X}$ into rehearsal memory $\mathbf{M}=\left{\mathbf{m}</em>\right}<em _xi="\xi">{k=1}^{K}$ with $K$ memory slots. During the training stage, we simultaneously develop self-supervised rehearsal training and task-specific reasoning training based on the memory $M$. For self-supervised rehearsal training, we develop a rehearsal model $\mathcal{H}</em>}(\mathbf{M}, \mathbf{H})$ to reconstruct the masked history fragments (recollection task) and distinguish positive history fragments from negative ones (familiarity task), where $\mathbf{H}$ means the critical history fragments that are selected by the history sampler $\mathcal{S<em _Omega="\Omega">{\Psi}(\mathbf{Q}, \mathbf{X})$. For taskspecific reasoning training, we develop the task-specific reason model $\mathcal{R}</em>}(\mathbf{M}, \mathbf{Q})$ to answer the given query $\mathbf{Q}$. During the testing stage, we maintain the rehearsal memory $\mathbf{M}=\mathcal{G<em _Omega="\Omega">{\Theta}(\mathbf{X})$ from the stream $\mathbf{X}$ and then infer the answer $\mathbf{A}$ for any relevant query $\mathbf{Q}$ by $\mathbf{A}=\mathcal{R}</em>}(\mathbf{M}, \mathbf{Q})$, where the rehearsal model $\mathcal{H<em _Psi="\Psi">{\xi}(\mathbf{M}, \mathbf{H})$ and the history sampler $\mathcal{S}</em>)$ are no longer needed.}(\mathbf{Q}, \mathbf{X</p>
<h3>3.2. Rehearsal Memory Machine</h3>
<p>We deal with the input stream $\mathbf{X}$ from the segment level rather than item level, i.e., we cut the input sequence into fixed-length segments and memorize them into the rehearsal memory segment-by-segment. Compared to existing MANNs (Graves et al., 2014; 2016), which store the input stream item-by-item orderly with a RNN-based controller, our segment-level memorization can further capture the bidirectional context of each item and improve the modeling efficiency. We denote the $t$-th segment as $\mathbf{X}^{t}=\left{\mathbf{x}<em n="1">{n}^{t}\right}</em>}^{N}$ with $N$ items and the current memory as $\mathbf{M}^{t}=\left{\mathbf{m<em k="1">{k}^{t}\right}</em>}^{K}$, where we have recorded $t-1$ segments in $\mathbf{M}^{t}$. The $\mathbf{x<em k="k">{n}^{t}$ and $\mathbf{m}</em>$.
We first model the $t$-th segment by a Transformer encoder (Vaswani et al., 2017) and obtain the sequence features $\mathbf{F}^{t}=\left{\mathbf{f}}^{t}$ have the same dimension $d_{x<em n="1">{n}^{t}\right}</em>}^{N}$ with dimension $d_{x}$. After it, we apply a memory update module to write $\mathbf{F}^{t}$ into $\mathbf{M}^{t}$. We apply a slot-to-item attention to align the sequence features to slot features in the current memory $\mathbf{M}^{t}$, and then develop the gate-based update. Concretely, we first calculate the slot-to-item attention matrix where each element means the relevance of a slot-item pair, and then learn aligned features $\mathbf{L}^{t}=\left{\mathbf{l<em k="1">{k}^{t}\right}</em>$ for each slot, given by}^{K</p>
<p>$$
\begin{aligned}
&amp; \alpha_{k n}^{t}=\mathbf{w}<em 1="1">{a}^{\top} \tanh \left(\mathbf{W}</em>}^{a} \mathbf{m<em 2="2">{k}^{t}+\mathbf{W}</em>}^{a} \mathbf{f<em k="k" n="n">{n}^{t}+\mathbf{b}^{a}\right) \
&amp; \hat{\alpha}</em>}^{t}=\frac{\exp \left(\alpha_{k n}^{t}\right)}{\sum_{j=1}^{K} \exp \left(\alpha_{j n}^{t}\right)}, \mathbf{l<em n="1">{k}^{t}=\sum</em>}^{N} \hat{\alpha<em n="n">{k n}^{t} \mathbf{f}</em>
\end{aligned}
$$}^{t</p>
<p>where $\mathbf{W}<em _model="{model" _text="\text">{1}^{a} \in \mathbb{R}^{d</em>}} \times d_{x}}, \mathbf{W<em _model="{model" _text="\text">{2}^{a} \in \mathbb{R}^{d</em>}} \times d_{x}}$ and $\mathbf{b}^{a} \in$ $\mathbb{R}^{d_{\text {model }}}$ are the projection matrices and bias. $\mathbf{w<em k="k">{a}^{\top}$ is the row vector. Next, the $k$-th slot feature $\mathbf{m}</em>}^{t}$ is updated with its aligned feature $\mathbf{l<em x="x">{k}^{t}$ based on a GRU unit with $d</em>$-d hidden states, given by</p>
<p>$$
\mathbf{m}<em k="k">{k}^{t+1}=\operatorname{GRU}\left(\mathbf{m}</em>\right)
$$}^{t}, \mathbf{l}_{k}^{t</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The Framework of Rehearsal Memory and Self-Supervised Rehearsal Training.
where $\mathbf{I}<em k="k">{k}^{t}$ is the current input of the GRU unit and $\mathbf{m}</em>$ for convenience.}^{t}$ is the hidden state at the $t$-th step. And $\mathbf{m}_{k}^{t+1}$ is the new slot feature after the gate-based update. After memorizing $T$ segments, we can obtain rehearsal memory $\mathbf{M}^{T+1}$ and we denote it by $\mathbf{M</p>
<h3>3.3. Self-Supervised Rehearsal Training</h3>
<p>Based on the maintained memory $M$, we apply the memory rehearsal technique to enhance long-sequence memorization. We first design the self-supervised recollection and familiarity tasks to solve how to rehearse, which can alleviate the gradual forgetting of early information. We next adopt a history sampler to decide what to rehearse, which guides the memory to remember critical task-relevant clues.</p>
<h3>3.3.1. How to Rehearse: Self-Supervised ReCOLLECTION and FAMILIARITY TASKS</h3>
<p>We design the rehearsal model $\mathcal{H}_{\xi}(\mathbf{M}, \mathbf{H})$ with recollection and familiarity tasks. The recollection task reconstructs the masked positive history fragments to enable the memory to relive past episodes. And the familiarity task tries to distinguish positive history fragments from negative ones for making the memory recognize experienced events.</p>
<p>First, we apply an independent history sampler to select the $B$ segments from the input stream as the history fragment set $\mathbf{H}=\left{H^{b}\right}<em 1="1">{b=1}^{B}$, which is illustrated in the next
section. Each fragment $H^{b}=\left{h</em>\right}$ contains $N$ items and each item $h_{}^{b}, h_{2}^{b}, \cdots, h_{N}^{b<em>}$ corresponds to a feature $\mathbf{x}_{</em>}$. For the $b$-th fragment, we randomly mask 50\% of items in the fragment and add an especial item [cls] at the beginning to obtain the masked positive history fragment $H^{b+}=\left{h_{\left[\mathrm{cls}\right]}^{b+}, h_{1}^{b+}, h_{\left[\mathrm{m}<em N="N">{1}\right]}^{b+}, \cdots, h</em>}^{b+}\right}$, where $h_{\left[\mathrm{m<em _xi="\xi">{1}\right]}^{b+}$ means the first masked item. In order to guarantee that the model $\mathcal{H}</em>}(\mathbf{M}, \mathbf{H})$ reconstructs the masked fragment by utilizing the maintained memory $\mathbf{M}$ rather than only relying on fragment context, we set the mask ratio to 50\% instead of 15\% in BERT (Devlin et al., 2019). Moreover, we construct the masked negative history fragment $H^{b-}=\left{h_{\left[\mathrm{cls}\right]}^{b-}, h_{1}^{b-}, h_{\left[\mathrm{m<em N="N">{1}\right]}^{b-}, \cdots, h</em>\right}}^{b-}\right}$ by replacing $50 \%$ of unmasked items in the positive fragment, where the replacement items are sampled from other input stream to make the negative fragment distinguishable. Here we construct the positive fragment set $\mathbf{H}^{+}=\left{H^{b+<em b="1">{b=1}^{B}$ and corresponding negative fragment set $\mathbf{H}^{-}=\left{H^{b-}\right}</em>$. Finally, we obtain}^{B}$ from the original fragment set $\mathbf{H}$. Next, we adopt a bidirectional Transformer decoder (Vaswani et al., 2017) without the future masking to model each history fragment $H^{b+} / H^{b-}$. In the decoder, each history item can interact with all other items in the fragment. The rehearsal memory $\mathbf{M}$ is input to the "encoder-decoder multi-head attention sub-layer" in each decoder layer, where the queries come from the previous decoder layer and the memory slots are regarded as the keys and values. This allows each item in the decoder to attend over all slot features in the memory $\mathbf{M</p>
<p>the features $\left{\mathbf{r}<em 1="1">{\mid d s}^{b+/ b-}, \mathbf{r}</em>}^{b+/ b-}, \mathbf{r<em 1="1">{\left[m</em>}\right]}^{b+/ b-}, \cdots, \mathbf{r<em _="*">{N}^{b+/ b-}\right}$ where each $\mathbf{r}</em>$.}^{b+/ b-}$ has the dimension $d_{x</p>
<p>Recollection Task. We first predict the masked items of positive history fragments to build the item-level reconstruction for the recollection task. Considering there are too many item types, we apply the contrastive training (He et al., 2020; Chen et al., 2020; Zhang et al., 2020) based on the ground truth and other sampled items. For the $N / 2$ masked items, we compute the recollection loss for the $b$-th fragment by</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _left_m__i="\left[m_{i">{i}^{b}=\log \frac{\exp \left(\mathbf{r}</em>}\right]}^{b+}\right. \cdot \mathbf{y<em _left_m__i="\left[m_{i">{i})}{\exp \left(\mathbf{r}</em>}\right]}^{b+}\right.} \cdot \mathbf{y<em j="1">{i})+\sum</em>}^{J} \exp \left(\mathbf{r<em i="i">{\left[m</em>}\right]}^{b+}\right. \cdot \mathbf{y<em c="c" e="e" r="r">{j})} \
&amp; \mathcal{L}</em>
\end{aligned}
$$}^{b}=-\frac{2}{N} \sum_{i=1}^{N / 2} \mathcal{L}_{i}^{b</p>
<p>where $\mathbf{y}<em x="x">{i} \in \mathbb{R}^{d</em>}}$ is the feature of ground truth of the $i$-th masked item, $\mathbf{y<em x="x">{j} \in \mathbb{R}^{d</em>}}$ is the feature of sampled items and $\mathbf{r<em i="i">{\left[m</em>$ is the inner product of two features.}\right]}^{b+} \cdot \mathbf{y}_{*</p>
<p>Familiarity Task. Next, we predict whether the masked history fragment ever appears in the current input stream, i.e. distinguish positive history fragments from negative ones. This training objective makes the memory learn the ability of recognizing experienced events. Concretely, we project each feature $\mathbf{r}_{\mid d s]}^{b+/ b-}$ into a confident score $s^{b+/ b-} \in(0,1)$ by a linear layer with the sigmoid activation, and calculate the familiarity loss by</p>
<p>$$
\mathcal{L}_{f a m}^{b}=-\log \left(s^{b+}\right)+\log \left(1-s^{b-}\right)
$$</p>
<p>where $\mathcal{L}_{f a m}^{b}$ is the familiarity loss for the $b$-th pair of positive and negative fragments in $\mathbf{H}^{+} / \mathbf{H}^{-}$.</p>
<h3>3.3.2. What to Rehearse: History Sampler</h3>
<p>Existing MANNs (Graves et al., 2014; 2016) often have no assumptions on what information needs to be remembered the most. But due to the limited capacity of the memory, it is crucial to distinguish what contents are important for subsequent inference and pay more attention to them during memorization. Thus, we further train a history sampler $\mathcal{S}<em _Theta="\Theta">{\Psi}(\mathbf{Q}, \mathbf{X})$ to select informative history fragments $\mathbf{H}$ for self-supervised rehearsal training, which is independent to the memory machine $\mathbf{M}=\mathcal{G}</em>$, learns the ability of distinguishing task-relevant important fragments and guides the rehearsal memory (i.e., the student) to remember critical clues.}(\mathbf{X})$. In knowledge distillation (Hinton et al., 2015), the teacher model can access the privileged information and transfer the knowledge to the student model. Similar to it, our history sampler, which is the teacher and can access raw contents $\mathbf{X}$ while answering the query $\mathbf{Q</p>
<p>Concretely, we independently train a directly reasoning model as the history sampler $\mathcal{S}<em c="c">{\Psi}(\mathbf{Q}, \mathbf{X})$, which can access
raw contents $\mathbf{X}$ while answering the query $\mathbf{Q}$. We first cut the entire input $\mathbf{X}$ into $C$ history fragments $\left{H</em>\right}<em c="c">{c=1}^{C}$ just like the rehearsal memory machine, where each fragment contains $N$ items. Next, we obtain the fragment features $\left{\mathbf{h}</em>\right}<em _model="{model" _text="\text">{c=1}^{C}$ by averaging the item features in each fragment. After it, we develop the attention-based reasoning for the query $\mathbf{Q}$ on these fragment features. The query feature $\mathbf{q} \in \mathbb{R}^{d</em>}}}$ is modeled by the task-specific encoder in different downstream tasks, which is introduced in Section A of the supplementary material. Given the query feature $\mathbf{q}$ and fragment features $\left{\mathbf{h<em c="1">{c}\right}</em>$, we conduct the attention method to aggregate query-relevant clues from fragments, given by}^{C</p>
<p>$$
\begin{aligned}
&amp; \beta_{c}=\mathbf{w}<em 1="1">{h}^{\top} \tanh \left(\mathbf{W}</em>}^{b} \mathbf{q}+\mathbf{W<em c="c">{2}^{b} \mathbf{h}</em>\right) \
&amp; \hat{\beta}}+\mathbf{b}^{b<em c="c">{c}=\frac{\exp \left(\beta</em>}\right)}{\sum_{j=1}^{C} \exp \left(\beta_{j}\right)}, \mathbf{e}=\sum_{c=1}^{C} \hat{\beta<em c="c">{c} \mathbf{h}</em>
\end{aligned}
$$</p>
<p>where $\mathbf{W}<em _model="{model" _text="\text">{1}^{b} \in \mathbb{R}^{d</em>}} \times d_{\text {model }}}, \mathbf{W<em _model="{model" _text="\text">{2}^{b} \in \mathbb{R}^{d</em>}} \times d_{x}}$ and $\mathbf{b}^{b} \in \mathbb{R}^{d_{\text {model }}}$ are the projection matrices and bias. And $\mathbf{w<em c="c">{h}^{\top}$ is the row vector. We then obtain the reasoning feature $\mathbf{a}=[\mathbf{e} ; \mathbf{q}]$ by concatenating the query and query-relevant fragment features, and design the final reasoning layer for different tasks, shown in Section A of the supplementary material. After independent training, the attention weights $\left{\beta</em>\right}<em c="c">{c=1}^{C}$ can be regarded as the importance score of each fragment for the query $\mathbf{Q}$. Thus, we can sample the vital history fragments with high attention weights for each $(\mathbf{X}, \mathbf{Q})$ pair. Specifically, to guarantee the sampled fragments appear in the entire input stream, we select $B / 2$ fragments from $\left{H</em>\right}<em c="c">{c=1}^{C / 2}$ with large weights and choose another $B / 2$ fragments from $\left{H</em>\right}<em b="1">{c=C / 2}^{C}$ to constitute the fragment set $\mathbf{H}=\left{H^{b}\right}</em>}^{B}$. The final recollection loss $\mathcal{L<em _fam="{fam" _text="\text">{\text {rec }}$ and familiarity loss $\mathcal{L}</em>$ are computed by}</p>
<p>$$
\mathcal{L}<em _mathcal_S="\mathcal{S" _sim="\sim" b="b">{r e c}=\mathbb{E}</em><em c="c" e="e" r="r">{\Psi}}\left[\mathcal{L}</em>}^{b}\right], \quad \mathcal{L<em _mathcal_S="\mathcal{S" _sim="\sim" b="b">{f a m}=\mathbb{E}</em><em a="a" f="f" m="m">{\Psi}}\left[\mathcal{L}</em>\right]
$$}^{b</p>
<h3>3.4. Task-Specific Reasoning Training</h3>
<p>Besides self-supervised rehearsal training, we simultaneously develop task-specific reasoning training. For several downstream tasks, we propose different task-specific reason model $\mathcal{R}<em r="r">{\Omega}(\mathbf{M}, \mathbf{Q})$ based on the memory $\mathbf{M}$. Here we adopt the simple and mature components in the reason model for a fair comparison. The details are introduced in Section A of the supplementary material. Briefly, we first learn the query representation $\mathbf{q}$ by a task-specific encoder and then perform the multi-hop attention-based reasoning. Finally, we obtain the reason loss $\mathcal{L}</em>)$.
Eventually, we combine the rehearsal and reason losses to train our model, given by}$ from $\mathcal{R}_{\Omega}(\mathbf{M}, \mathbf{Q</p>
<p>$$
\mathcal{L}<em 1="1">{r m}=\lambda</em>} \mathcal{L<em 2="2">{r e c}+\lambda</em>} \mathcal{L<em 3="3">{f a m}+\lambda</em>
$$} \mathcal{L}_{r</p>
<p>where $\lambda_{1}, \lambda_{2}$ and $\lambda_{3}$ are applied to adjust the balance of three losses.</p>
<h2>4. Experiments</h2>
<p>In this section, we first verify our rehearsal memory on the widely-used short-sequence reasoning task bAbI. Next, we mainly compare our approach with diverse baselines on several long-sequence reasoning tasks. We then perform ablation studies on the memory rehearsal techniques and analyze the impact of crucial hyper-parameters.</p>
<h3>4.1. Experiment Setting</h3>
<p>Model Setting. We first introduce the common model settings for all downstream tasks. We set the layer number of the Transformer encoder and bi-directional Transformer decoder to 3. The head number in Multi-Head Attention is set to 4 . We set $\lambda_{1}, \lambda_{2}$ and $\lambda_{3}$ to $1.0,0.5$ and 1.0 , respectively. The number $B$ of history fragments is set to 6 . During training, we apply an Adam optimizer (Duchi et al., 2011) to minimize the multi-task loss $\mathcal{L}_{r m}$, where the initial learning rate is set to 0.001 .</p>
<p>Baseline. We compare our rehearsal memory with the directly reasoning methods and the memory-based reasoning approaches. The directly reasoning baselines are different in downstream tasks and the memory-based baselines mainly are DNC (Graves et al., 2016), NUTM (Le et al., 2019b), DMSDNC (Park et al., 2020), STM (Le et al., 2020a) and Compressive Transformer (CT) (Rae et al., 2019). For a fair comparison, we modify the reasoning module of memorybased baselines to be consistent with our rehearsal memory, i.e. we conduct multi-hop attention-based reasoning based on the built memory. And the number of memory slots in these baselines is also set to K. Besides, we set the core number of NUTM to 4, the query number of STM to 8 and the memory block number of DMSDNC to 2. As for CT, the layer number of the Transformer is set to 3 as our rehearsal memory and the compression rate is set 5 .</p>
<h3>4.2. Rehearsal Memory on Short-Sequence Reasoning</h3>
<p>The bAbI dataset (Weston et al., 2015) is a synthetic text question answering benchmark and widely applied to evaluate the memorization and reasoning performance of MANNs. This dataset contains 20 reasoning tasks and requires to be solved with one common model. Although most of these tasks only give short-sequence text input (less than 100 words) and existing methods (Park et al., 2020; Le et al., 2020a) have solved these tasks well, we still compare our rehearsal memory with other memory-based baselines to verify the short-sequence reasoning performance. We set the $d_{x}$ and $d_{\text {model }}$ to 128 . The number K of memory slots is set to 20 . And we naturally take each sentence in input</p>
<p>Table 1. Performance Comparisons for Synthetical bAbI Task: mean $\pm$ std. and best error over 10 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">mean $\pm$ std error</th>
<th style="text-align: center;">best error</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DNC</td>
<td style="text-align: center;">$16.7 \pm 7.6$</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;">NUTM</td>
<td style="text-align: center;">$5.6 \pm 1.9$</td>
<td style="text-align: center;">3.3</td>
</tr>
<tr>
<td style="text-align: center;">DMSDNC</td>
<td style="text-align: center;">$1.53 \pm 1.33$</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: center;">STM</td>
<td style="text-align: center;">$0.39 \pm 0.18$</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">$0.81 \pm 0.26$</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: center;">RM</td>
<td style="text-align: center;">$\mathbf{0 . 3 3} \pm \mathbf{0 . 1 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 2}$</td>
</tr>
</tbody>
</table>
<p>texts as a segment and the maximum length N of segments is set to 15 . Due to limited word types in this dataset, we sample all other words as negative items in $\mathcal{L}_{\text {rec }}$.</p>
<p>The results are summarized in Table 1. The RM model solves these bAbI tasks with near-zero error and outperforms existing baselines at the mean and best error rate of 10 runs, verifying that our RM method can conduct effective memorization and reasoning on short-sequence tasks. In these methods, DNC, NUTM and DMSDNC model the input contents word-by-word. STM processes input texts as a sentence-level sequence. And CT and RM methods cut the input texts into sentences for modeling. From the results, we can find STM, CT and RM achieve lower error rates than other baselines, suggesting the importance of sentencelevel modeling. Considering the bAbI tasks are close to being solved, we design another difficult synthetic task in Section B of the supplementary material to evaluate our RM model.</p>
<h3>4.3. Rehearsal Memory on Long-Sequence Reasoning</h3>
<p>We then compare our approach with diverse baselines on several long-sequence reasoning tasks.</p>
<h3>4.3.1. LONG-SEQUENCE TEXT QUESTION ANSWERING</h3>
<p>We apply the NarrativeQA dataset (Kočiskỳ et al., 2018) with long input contents for long-sequence text question answering. This dataset contains 1,572 stories and corresponding summaries generated by humans, where each summary contains more than 600 tokens on average. And there are 46,765 questions in total. We adopt the multi-choice form to answer the given question based on a summary, where other answers for questions associated with the same summary are regarded as answer candidates. We compute the mean reciprocal rank (MRR) as the metric, i.e., the rank of the correct answer among candidates. Besides the memory-based methods, we adopt directly reasoning model AS Reader (Kadlec et al., 2016) and E2E-MN (Sukhbaatar et al., 2015) as baselines. The AS Reader applies a pointer network to generate the answer and E2E-MN employs the end-to-end memory</p>
<p>Table 2. Performance Comparisons for Long-Sequence Text Question Answering on NarrativeQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Val MRR</th>
<th style="text-align: center;">Test MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AS Reader</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">25.9</td>
</tr>
<tr>
<td style="text-align: center;">E2E-MN</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: center;">DNC</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">NUTM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">27.2</td>
</tr>
<tr>
<td style="text-align: center;">DMSDNC</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;">STM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.3</td>
</tr>
<tr>
<td style="text-align: center;">RM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">$\mathbf{2 9 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 7}$</td>
</tr>
</tbody>
</table>
<p>network to conduct multi-hop reasoning. For our rehearsal memory, we set the $d_{x}$ and $d_{\text {model }}$ to 256 . The number K of memory slots is set to 20 . We naturally take each sentence in summaries as a segment and the maximum length N of segments is set to 20 . And we sample all other words as negative items in $\mathcal{L}_{\text {rec }}$.</p>
<p>We report the results in Table 2. Our RM method obtains the best performance among memory-based approaches, which demonstrates our self-supervised rehearsal training with the history sampler can effectively enhance long-sequence memorization and reasoning. Further, the RM model slightly outperforms early directly reasoning methods AS Reader and E2E-MN, showing the ability of efficient reasoning on long sequences with limited storage resources.</p>
<h3>4.3.2. Long-Sequence Video Question Answering</h3>
<p>The ActivityNet-QA dataset (Yu et al., 2019) contains 5,800 videos from the ActivityNet (Caba Heilbron et al., 2015). The average video duration of this dataset is about 180s and is the longest in VQA datasets. We compare our method with four directly reasoning baselines, including three basic models E-VQA, E-MN, E-SA from (Yu et al., 2019) and the SOTA model HCRN (Le et al., 2020b). For our rehearsal memory, we set the $d_{x}$ and $d_{\text {model }}$ to 256 . The number K of memory slots and length N of segments are both set to 20 . And in $\mathcal{L}_{\text {rec }}$, we select 30 other frame features from the video as the sampled items.</p>
<p>As shown in Table 3, the RM method obtains a better performance than other memory-based baselines. Compared to the best baseline CT, our RM model further achieves the $0.9 \%$ absolute improvement, showing the effectiveness of our model designs and self-supervised rehearsal training. Moreover, the RM method outperforms the basic directly reasoning baselines E-VQA, E-MN and E-SA, but slightly worse than the SOTA method HCRN. This suggests our rehearsal memory can reduce the gap between memory-based and directly reasoning paradigms.</p>
<p>Table 3. Performance Comparisons for Long-Term Video Question Answering on ActivityNet-QA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">E-VQA</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">E-MN</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">27.9</td>
</tr>
<tr>
<td style="text-align: center;">E-SA</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;">HCRN</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">37.6</td>
</tr>
<tr>
<td style="text-align: center;">DNC</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">30.3</td>
</tr>
<tr>
<td style="text-align: center;">NUTM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;">DMSDNC</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;">STM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">33.7</td>
</tr>
<tr>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: center;">RM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Performance Comparisons for Lifelong Sequence Recommendation on XLong.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GRU4REC</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">0.8702</td>
</tr>
<tr>
<td style="text-align: center;">Caser</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">0.8390</td>
</tr>
<tr>
<td style="text-align: center;">RUM</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">0.8649</td>
</tr>
<tr>
<td style="text-align: center;">DIEN</td>
<td style="text-align: center;">Directly</td>
<td style="text-align: center;">0.8793</td>
</tr>
<tr>
<td style="text-align: center;">HPMN</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">0.8645</td>
</tr>
<tr>
<td style="text-align: center;">MIMN</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">0.8731</td>
</tr>
<tr>
<td style="text-align: center;">RM</td>
<td style="text-align: center;">Memory-Based</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 1 7}$</td>
</tr>
</tbody>
</table>
<h3>4.3.3. Lifelong Sequence Recommendation</h3>
<p>The lifelong sequence recommendation (Ren et al., 2019) aims to predict whether the user will click a given item based on long sequences, thus it can be regarded as a longsequence reasoning task. The XLong dataset (Ren et al., 2019) is sampled from the click logs on Alibaba. The length of historical behavior sequences in this dataset is 1000 . We compare our method with four directly reasoning methods GRU4REC (Hidasi et al., 2015), Caser (Tang \&amp; Wang, 2018), DIEN (Zhou et al., 2019), RUM (Chen et al., 2018) and two memory-based methods HPMN (Ren et al., 2019) and MIMN (Pi et al., 2019), where the HPMN method builds the memory by hierarchical RNNs and the MIMN method introduces a write-read memory as in (Graves et al., 2014). For our rehearsal memory, we set the $d_{x}$ and $d_{\text {model }}$ to 64 . The number K of memory slots and length N of segments are both set to 20 . And in $\mathcal{L}_{\text {rec }}$, we select 200 items from the large item set as the sampled items.</p>
<p>The results are shown in Table 4. our RM method not only outperforms other memory-based approaches, but also achieves better performance than directly reasoning base-</p>
<p>Table 5: Ablation Results about the Rehearsal Losses and History Sampler.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>NarrativeQA</th>
<th></th>
<th>ActNet-QA</th>
<th>XLong</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Val</td>
<td>Test</td>
<td>Acc.</td>
<td>AUC</td>
</tr>
<tr>
<td>w/o. rehearsal</td>
<td>27.9</td>
<td>27.5</td>
<td>24.6</td>
<td>0.8745</td>
</tr>
<tr>
<td>only $\mathcal{L}_{\text {rec }}$</td>
<td>29.2</td>
<td>28.6</td>
<td>36.0</td>
<td>0.8802</td>
</tr>
<tr>
<td>only $\mathcal{L}_{\text {fam }}$</td>
<td>28.6</td>
<td>28.1</td>
<td>35.4</td>
<td>0.8776</td>
</tr>
<tr>
<td>random sampler</td>
<td>28.7</td>
<td>28.3</td>
<td>35.7</td>
<td>0.8813</td>
</tr>
<tr>
<td>Full</td>
<td>29.4</td>
<td>28.7</td>
<td>36.3</td>
<td>0.8817</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation Results about the Transformer Encoder and Mask Ratio.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>NarrativeQA</th>
<th></th>
<th>ActNet-QA</th>
<th>XLong</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Val</td>
<td>Test</td>
<td>Acc.</td>
<td>AUC</td>
</tr>
<tr>
<td>GRU Encoder</td>
<td>29.1</td>
<td>28.3</td>
<td>35.9</td>
<td>0.8789</td>
</tr>
<tr>
<td>15\% Mask Ratio</td>
<td>29.0</td>
<td>28.5</td>
<td>36.0</td>
<td>0.8809</td>
</tr>
<tr>
<td>Full</td>
<td>29.4</td>
<td>28.7</td>
<td>36.3</td>
<td>0.8817</td>
</tr>
</tbody>
</table>
<p>lines. This is because our rehearsal memory can aggregate and organize the long-term interests from user behavior sequences and these interests can be activated during nextitem prediction. But the directly reasoning approaches may fail to learn such informative interest representations.</p>
<h3>4.4 Ablation Study for Self-Supervised Rehearsal</h3>
<p>We next perform ablation studies on the self-supervised rehearsal losses and history sampler. Concretely, we first completely discard the self-supervised rehearsal training to produce the ablation model RM (w/o. rehearsal). We then remove the recollection or familiarity loss to produce two ablation models RM (only $\mathcal{L}<em _rec="{rec" _text="\text">{\text {fam }}$ ) and RM (only $\mathcal{L}</em>$ ), where the history sampler is still retained. Next, we replace the independent history sampler with a random sampler to generate the ablation model RM (random sampler), which randomly selects $B$ history fragments from the input stream for rehearsal training.}</p>
<p>We conduct the ablation experiments on NarrativeQA, ActivityNet-QA and XLong datasets. The results are reported in Table 5. We can find the full model outperforms the model RM (w/o. rehearsal), demonstrating the selfsupervised rehearsal training with the history sampler can further boost the long-sequence memorization and reasoning ability of rehearsal memory. Further, the full model has better performance than RM (only $\mathcal{L}<em _rec="{rec" _text="\text">{\text {fam }}$ ) and RM (only $\mathcal{L}</em>$ ) achieves better results than RM
}}$ ) on all metrics, which illustrates two rehearsal tasks are both helpful for alleviating the issue of gradual forgetting. And RM (only $\mathcal{L}_{\text {rec }<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Effect of the Memory Slot Number K.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Effect of the Segment Length N on the ActivityNet-QA dataset.
(only $\mathcal{L}_{\text {fam }}$ ), showing the recollection task that enables the memory to relive past episode is more important for rehearsal training. Moreover, the ablation model RM (random sampler) has the performance degradation than the full model. This fact indicates it is critical to select informative history fragments for rehearsal training. By the guidance of the history sampler, the rehearsal memory can remember task-relevant clues for subsequent reasoning.</p>
<h3>4.5 Ablation Study for Model Setting</h3>
<p>In this section, we conduct ablation study about the model settings. Existing MANNs often use a RNN as their controller, but we apply a Transformer encoder to improve the sequential modeling ability of RM. Thus, we replace the Transformer encoder in the rehearsal memory machine with a bi-directional GRU encoder. As shown in Table 6, the full model achieves better performance than the model with the GRU Encoder, verifying the effectiveness of the Transformer encoder.</p>
<p>For the masked history fragments, we set the mask ratio to $50 \%$ instead of $15 \%$ in BERT. We compare the results of two mask ratios in Table 6. We can find the full model with the $50 \%$ mask ratio outperforms the one with the $15 \%$ mask ratio. This fact suggests that the large mask ratio makes the rehearsal model $\mathcal{H}_{\xi}(\mathbf{M}, \mathbf{H})$ utilize the maintained memory $\mathbf{M}$ than only relying on fragment context, and is beneficial for the self-supervised rehearsal training of RM.</p>
<h3>4.6. Hyper-Parameters Analysis</h3>
<p>We then explore the effect of two crucial hyper-parameters: the memory slot number $K$ and the segment length $N$. We first set the slot number $K$ to $[10,15,20,25]$ and compare our RM method with two baselines STM and CT on the NarrativeQA and ActivityNet-QA datasets. We display the results in Figure 2. We note that the performance of all three methods gradually improves with the increase of slot number and slowly reaches the bottleneck. When the number of memory slots exceeds 20 , more slots can not bring much improvement. By comparison, we can find our RM method achieves the best performance on different slot numbers, verifying the effectiveness and stability of our rehearsal memory. Moreover, the performance of CT is terrible when the slot number is too few. This is because the CT method implements the compressed memory by a FIFO queue and completely forgets the contents beyond the queue, i.e., its memorization range severely depends on the slot number.</p>
<p>We then set the segment length $N$ to $[10,15,20,25,30]$ and report the results on the ActivityNet-QA dataset in Figure 3. We can find that when the segment length is set to 10 , the RM method achieves poor results and the performance is relatively stable when the segment length changes between 20 and 30. This is because when the segment is too short, important evidence may be scattered in different segments, and the model cannot effectively capture the evidence and infer the answer.</p>
<h2>5. Conclusions</h2>
<p>In this paper, we propose the self-supervised rehearsal to enhance long-sequence memorization. We design the selfsupervised recollection and familiarity tasks to alleviate the gradual forgetting of early information. Further, we adopt a history sampler to guide the memory to remember critical information. Extensive experiments on a series of downstream tasks verify the performance of our method. For future work, we will further explore the property of rehearsal memory.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by the National Key R\&amp;D Program of China under Grant No. 2018AAA0100603. This research is supported by the National Natural Science Foundation of China under Grant No. 61836002 and No.62072397, and the Zhejiang Natural Science Foundation LR19F020006.</p>
<h2>References</h2>
<p>Baddeley, A. Working memory. Science, 255(5044):556559, 1992.</p>
<p>Caba Heilbron, F., Escorcia, V., Ghanem, B., and Carlos Niebles, J. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 961-970, 2015.</p>
<p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.</p>
<p>Chen, X., Xu, H., Zhang, Y., Tang, J., Cao, Y., Qin, Z., and Zha, H. Sequential recommendation with user memory networks. In Proceedings of the eleventh ACM international conference on web search and data mining, pp. $108-116,2018$.</p>
<p>Csordás, R. and Schmidhuber, J. Improving differentiable neural computers through memory masking, deallocation, and link distribution sharpness control. arXiv preprint arXiv:1904.10278, 2019.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference on The North American Chapter of the Association for Computational Linguistics, 2019.</p>
<p>Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):21212159, 2011.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.</p>
<p>Hidasi, B., Karatzoglou, A., Baltrunas, L., and Tikk, D. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.</p>
<p>Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Jin, W., Zhao, Z., Gu, M., Yu, J., Xiao, J., and Zhuang, Y. Multi-interaction network with object relation for video question answering. In Proceedings of the ACM International Conference on Multimedia, pp. 1193-1201, 2019a.</p>
<p>Jin, W., Zhao, Z., Gu, M., Yu, J., Xiao, J., and Zhuang, Y. Video dialog via multi-grained convolutional selfattention context networks. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 465-474, 2019b.</p>
<p>Kadlec, R., Schmid, M., Bajgar, O., and Kleindienst, J. Text understanding with the attention sum reader network. arXiv preprint arXiv:1603.01547, 2016.</p>
<p>Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328, 2018.</p>
<p>Le, H., Tran, T., and Venkatesh, S. Learning to remember more with less memorization. arXiv preprint arXiv:1901.01347, 2019a.</p>
<p>Le, H., Tran, T., and Venkatesh, S. Neural stored-program memory. arXiv preprint arXiv:1906.08862, 2019b.</p>
<p>Le, H., Tran, T., and Venkatesh, S. Self-attentive associative memory. arXiv preprint arXiv:2002.03519, 2020a.</p>
<p>Le, T. M., Le, V., Venkatesh, S., and Tran, T. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9972-9981, 2020b.</p>
<p>Moscovitch, M., Cabeza, R., Winocur, G., and Nadel, L. Episodic memory and beyond: the hippocampus and neocortex in transformation. Annual review of psychology, 67:105-134, 2016.</p>
<p>Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A. Metalearned neural memory. In Advances in Neural Information Processing Systems, pp. 13331-13342, 2019.</p>
<p>Park, T., Choi, I., and Lee, M. Distributed memory based self-supervised differentiable neural computer. arXiv preprint arXiv:2007.10637, 2020.</p>
<p>Pi, Q., Bian, W., Zhou, G., Zhu, X., and Gai, K. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pp. 2671-2679, 2019.</p>
<p>Rae, J., Hunt, J. J., Danihelka, I., Harley, T., Senior, A. W., Wayne, G., Graves, A., and Lillicrap, T. Scaling memoryaugmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pp. 3621-3629, 2016.</p>
<p>Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.</p>
<p>Ren, K., Qin, J., Fang, Y., Zhang, W., Zheng, L., Bian, W., Zhou, G., Xu, J., Yu, Y., Zhu, X., et al. Lifelong sequential modeling with personalized memorization for user response prediction. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 565-574, 2019.</p>
<p>Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., and Lillicrap, T. Relational recurrent neural networks. In Advances in neural information processing systems, pp. 7299-7310, 2018.</p>
<p>Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.</p>
<p>Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, pp. 2440-2448, 2015.</p>
<p>Tang, J. and Wang, K. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 565-573, 2018.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.</p>
<p>Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merriënboer, B., Joulin, A., and Mikolov, T. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Xiong, C., Zhong, V., and Socher, R. Dynamic coattention networks for question answering. arXiv preprint arXiv:1611.01604, 2016.</p>
<p>Yonelinas, A. P. The nature of recollection and familiarity: A review of 30 years of research. Journal of memory and language, 46(3):441-517, 2002.</p>
<p>Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and Tao, D. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the American Association for Artificial Intelligence, volume 33, pp. 9127-9134, 2019.</p>
<p>Zhang, S., Yao, D., Zhao, Z., Chua, T.-S., and Wu, F. Causerec: Counterfactual user sequence synthesis for sequential recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021.</p>
<p>Zhang, Z., Zhao, Z., Lin, Z., He, X., et al. Counterfactual contrastive learning for weakly-supervised visionlanguage grounding. Advances in Neural Information Processing Systems, 33:18123-18134, 2020.</p>
<p>Zhou, G., Mou, N., Fan, Y., Pi, Q., Bian, W., Zhou, C., Zhu, X., and Gai, K. Deep interest evolution network for click-through rate prediction. In Proceedings of the American Association for Artificial Intelligence, volume 33, pp. 5941-5948, 2019.</p>
<h2>A. Task-Specific Reason Models</h2>
<p>In this section, we introduce the task-specific reason model $\mathcal{R}<em _model="{model" _text="\text">{\Omega}(\mathbf{M}, \mathbf{Q})$, where $\mathbf{M}$ is the built memory and $\mathbf{Q}$ is the given query. Specifically, we first model the query feature $\mathbf{q} \in \mathbb{R}^{d</em>$.}}}$ by a task-specific encoder. For the synthetic task, the given query $\mathbf{Q}$ is a one-hot vector and we directly obtain $\mathbf{q}$ by an embedding layer. For long-sequence text and video QA tasks, the query $\mathbf{Q}$ is a sentence and we apply a bi-directional GRU to learn the sentence feature $\mathbf{q}$. As for the recommendation task with long sequences, the given query is a target item with the unique id and we likewise learn an embedding layer to obtain the feature $\mathbf{q</p>
<p>Next, we develop the multi-hop attention-based reasoning on rehearsal memory $\mathbf{M}$. Concretely, at each step $c$, we capture the importance memory feature $\mathbf{e}^{c} \in \mathbb{R}^{d_{x}}$ from $\mathbf{M}$ based on the current query $\mathbf{q}^{c-1}$ using an attention method, given by</p>
<p>$$
\begin{aligned}
&amp; \gamma_{k}^{c}=\mathbf{w}<em 1="1">{c}^{\top} \tanh \left(\mathbf{W}</em>}^{c} \mathbf{q}^{c-1}+\mathbf{W<em k="k">{2}^{c} \mathbf{m}</em>\right) \
&amp; \hat{\gamma}}+\mathbf{b}^{c<em k="k">{k}^{c}=\frac{\exp \left(\gamma</em>}^{c}\right)}{\sum_{j=1}^{K} \exp \left(\gamma_{j}^{c}\right)}, \mathbf{e}^{c}=\sum_{k=1}^{K} \hat{\gamma<em k="k">{k}^{c} \mathbf{m}</em>
\end{aligned}
$$</p>
<p>where $\mathbf{W}<em _model="{model" _text="\text">{1}^{c} \in \mathbb{R}^{d</em>}} \times d_{\text {model }}}, \mathbf{W<em _model="{model" _text="\text">{2}^{c} \in \mathbb{R}^{d</em>}} \times d_{x}}$ and $\mathbf{b}^{c} \in \mathbb{R}^{d_{\text {model }}}$ are the projection matrices and bias. And $\mathbf{w<em _model="{model" _text="\text">{c}^{\top}$ is the row vector. We then produce the next query $\mathbf{q}^{c}=\mathbf{W}^{q}\left[\mathbf{e}^{c} ; \mathbf{q}^{c-1}\right] \in \mathbb{R}^{d</em>$. The hyper-parameter $C$ is set to $2,2,2$ and 1 for synthetic experiments, text QA, video QA and sequence recommendation, respectively.}}}$, where $\mathbf{W}^{q} \in$ $\mathbb{R}^{d_{\text {model }} \times\left(d_{x}+d_{\text {model }}\right)}$ is the projection matrix and $\mathbf{q}^{0}$ is the original $\mathbf{q}$. After C steps, we obtain the reason feature $\mathbf{q}^{C</p>
<p>After it, we design the final reasoning layer for different tasks. For synthetic experiments and long-sequence video QA with fixed answer sets, we directly apply a classification layer to select the answer and develop the cross-entropy loss $\mathcal{L}<em i="i">{r}$. But the text QA dataset NarrativeQA provides different candidate answers for each query, we first model each candidate feature $\mathbf{a}</em>}$ by another bi-directional GRU and then concatenate $\mathbf{a<em r="r">{i}$ with $\mathbf{q}^{C}$ to predict the conference score for each candidate. Finally, we also learn the cross-entropy loss $\mathcal{L}</em>$.}$ based on answer probabilities. As for the sequence recommendation task, we can directly compute a confidence score based on $\mathbf{q}^{C}$ by a linear layer and build the binary loss function $\mathcal{L}_{r</p>
<h2>B. Synthetic Experiment</h2>
<p>Synthetic Dataset. We first introduce the setting of the synthetic task. Here we abstract the general concepts of reasoning tasks (QA/VQA/Recommendation) to construct the synthetic task. We define the input sequence as a Stream and each item in the sequence as a Fact, where the stream</p>
<p>Table 7. Performance Comparisons on Synthetic Data. $R_{f}=400$, $R_{l}=200, R_{q}=40, R_{a}=30, R_{c}=5$.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Setting</th>
<th>Early</th>
<th>Later</th>
</tr>
</thead>
<tbody>
<tr>
<td>Directly Reason</td>
<td>Directly</td>
<td>13.57</td>
<td>13.41</td>
</tr>
<tr>
<td>Multi-Hop Reason</td>
<td>Directly</td>
<td>34.38</td>
<td>34.50</td>
</tr>
<tr>
<td>DNC</td>
<td>Memory-Based</td>
<td>20.56</td>
<td>26.59</td>
</tr>
<tr>
<td>NUTM</td>
<td>Memory-Based</td>
<td>24.31</td>
<td>29.71</td>
</tr>
<tr>
<td>STM</td>
<td>Memory-Based</td>
<td>23.55</td>
<td>29.64</td>
</tr>
<tr>
<td>DMSDNC</td>
<td>Memory-Based</td>
<td>24.92</td>
<td>30.74</td>
</tr>
<tr>
<td>RM (w/o. rehearsal)</td>
<td>Memory-Based</td>
<td>25.79</td>
<td>31.38</td>
</tr>
<tr>
<td>RM</td>
<td>Memory-Based</td>
<td>$\mathbf{2 8 . 4 2}$</td>
<td>$\mathbf{3 1 . 7 1}$</td>
</tr>
</tbody>
</table>
<p>and fact can correspond to the text sequence and word token in text QA. We set the number of fact types to $R_{f}$, that is, each fact can be denoted by a $R_{f}$-d one-hot vector and obtain the fact feature by a trainable embedding layer. Considering reasoning tasks often need to retrieve vital clues related to the query from the given input and then infer the answer, we define the query-relevant facts in the stream as the Evidence and regard the Evidence-Query-Answer triple as the Logic Chain. Given a stream and a query, we need to infer the answer if the stream contains the evidence. Specifically, we set the number of query types to $R_{q}$ and each query can be denoted by a $R_{q}$-d one-hot vector. For each query, we set the number of answer types to $R_{a}$. That is, there are totally $R_{q} * R_{a}$ query-answer pairs and we need to synthesize $R_{q} * R_{a}$ corresponding evidences of each pair. Each evidence is denoted by a sequence of facts $\left{\operatorname{fact}<em R__c="R_{c">{1}, \cdots, \operatorname{fact}</em>$ facts from the group as the evidence, and then assign the evidence to a query-answer pair to generate a fixed logic chain.}}\right}$, which continuously appear in the input stream. And $R_{c}$ is the length of the evidence. During the evidence synthesis, we first define 20 different groups and uniformly split these facts and queries to 20 groups. Next, if a query belongs to group $k$, we randomly sample $R_{c</p>
<p>Eventually, we synthetic 400 data samples for each logic chain to train the models. Each sample contains the input stream with $R_{l}$ items, a query and an answer. Concretely, we first sample $R_{l}$ facts as a sequence and then place the evidence in the sequence, where we guarantee each streamquery pair corresponds to a unique answer.</p>
<p>Baselines and Model Details. The Directly Reason method first models the input stream by RNN to obtain the stream feature, then concatenates the stream feature with the query feature and predicts the answer by a linear layer. The Multi-Hop Reason method further applies multiple attention layers after RNN-based stream modeling to capture the query-relevant clues. In the main experiment,</p>
<p>we set the dataset hyper-parameters $R_{f}, R_{l}, R_{q}, R_{a}$ and $R_{c}$ to 400, 200, 40, 30, and 5, respectively. The facts of the evidence may appear in different stages of the input stream. Early means the facts appear in the preceding $50 \%$ of the stream and Later means the facts appear in the subsequent $50 \%$. For our rehearsal memory, we set the $d_{x}$ and $d_{\text {model }}$ to 128. The number K of memory slots and length N of segments are set to 20 and 10, respectively. And we sample all other facts as negative items in $\mathcal{L}_{\text {rec }}$.</p>
<p>Evaluation Results. Table 7 reports the performance comparison between our method and baselines, where RM is the full model and RM (w/o. rehearsal) only employs the task-specific reasoning training. Overall, directly reasoning methods have close early and later performance, but memory-based approaches DNC, NUTM, STM, DMSDNC and RM (w/o. rehearsal) achieve the terrible early performance due to the gradual forgetting. By the self-supervised rehearsal training, our RM significantly improves the early accuracy and achieves the best memory-based reasoning performance. This fact suggests our proposed memory rehearsal can alleviate the gradual forgetting of early information and make the memory remember critical information from the input stream. Besides, RM (w/o. rehearsal) outperforms other memory-based methods, which indicates our rehearsal memory machine can better memorize the long-term information even without the rehearsal training. Moreover, we can find the Directly Reason approach achieves the worst performance but the Multi-Hop Reason method has a high accuracy, which demonstrates the performance of directly reasoning methods mainly depends on the complicated interaction between the input contents and queries.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Equal contribution ${ }^{1}$ Zhejiang University, China ${ }^{2}$ DAMO Academy, Alibaba Group, China. Correspondence to: Zhou Zhao $&lt;$ zhaozhou@zju.edu.cn $&gt;$.</p>
<p>Proceedings of the $38^{\text {th }}$ International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7726 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7726</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7726</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-262044956</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.09727v1.pdf" target="_blank">When Large Language Models Meet Citation: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Citations in scholarly work serve the essential purpose of acknowledging and crediting the original sources of knowledge that have been incorporated or referenced. Depending on their surrounding textual context, these citations are used for different motivations and purposes. Large Language Models (LLMs) could be helpful in capturing these fine-grained citation information via the corresponding textual context, thereby enabling a better understanding towards the literature. Furthermore, these citations also establish connections among scientific papers, providing high-quality inter-document relationships and human-constructed knowledge. Such information could be incorporated into LLMs pre-training and improve the text representation in LLMs. Therefore, in this paper, we offer a preliminary review of the mutually beneficial relationship between LLMs and citation analysis. Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation. We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship. We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7726.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7726.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPECTER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPECTER: document-level representation learning using citation-informed transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A citation-aware Transformer approach that incorporates inter-document citation links into pretraining via a triplet-loss objective to produce document-level embeddings useful for literature understanding and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SPECTER: document-level representation learning using citation-informed transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SPECTER: document-level representation learning using citation-informed transformers</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Arman Cohan et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2020</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SPECTER</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses citation relationships as incidental supervision: constructs triplets (query, positive cited, negative non-cited) and pretrains a Transformer to produce document embeddings that pull cited documents closer and push unrelated documents apart.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Document text (e.g., title/abstract) plus citation graph links (inter-document relations)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Dense document embeddings (document-level representations)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Contrastive / triplet-loss pretraining (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Citation-aware Transformer (BERT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports SPECTER achieves document-level representations that capture inter-document relatedness and are effective for downstream literature tasks (e.g., citation prediction, retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Standard token/sentence objectives ignore inter-document structure; SPECTER addresses this but full multi-document, graph-structured knowledge extraction remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7726.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sci-Mult</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sci-Mult: Pre-training multi-task contrastive learning models for scientific literature understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task LLM pretraining framework that includes citation prediction as a sub-task and employs hard negative mining and contrastive learning to share knowledge across tasks while reducing task interference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pre-training multi-task contrastive learning models for scientific literature understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Pre-training multi-task contrastive learning models for scientific literature understanding</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yu Zhang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Sci-Mult</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Multi-task pretraining where citation prediction is one task; uses contrastive objectives with hard negatives to align citing/cited pairs and to transfer knowledge across scientific-document tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific document text and citation pairs (citing/cited)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Shared contextual representations for multiple literature-understanding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Contrastive learning with hard negative mining (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-task Transformer-based LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Citation pairs and scientific corpora (not specified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to facilitate knowledge sharing across tasks and improve citation-prediction related performance in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential task-specific interference and reliance on quality of negative sampling; details not fully specified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7726.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConGraT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConGraT: self-supervised contrastive pretraining for joint graph and text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised framework that learns joint representations for textual content and graph nodes (e.g., citation graphs) via batch-wise contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>William Brannon et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ConGraT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns separate but aligned text and node (graph) embeddings with a contrastive loss that pulls related text-node pairs together and pushes unrelated pairs apart, enabling multi-modal scientific-document representations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textual content and parent graph (e.g., citation graph) node context</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Aligned text and graph node embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Batch-wise contrastive learning (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contrastive Transformer + graph embedding components</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Graph-structured scientific datasets such as citation graphs (e.g., PubMed graph cited in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey notes ConGraT learns individual representations for texts and nodes simultaneously and is effective for tasks requiring joint graph/text understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires graph data and careful design of positive/negative sampling; scaling to very large graphs is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7726.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Aware Distillation (GRAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distillation framework that transfers graph-encoded knowledge from a GNN teacher into a graph-free student model via a shared language model to improve scalability while preserving citation-graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Train your own GNN teacher: Graph-aware distillation on textual graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Train your own GNN teacher: Graph-aware distillation on textual graphs</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Costas Mavromatis et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GRAD</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Jointly optimizes a GNN teacher and a graph-free student via a shared LM, enabling the student to capture graph-derived signals without requiring GNN inference at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textual documents and their citation graph (text-rich graph inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-informed, graph-free textual model (distilled LM representations)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Knowledge distillation (teacher-student optimization), not prompt-based</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shared Language Model with GNN teacher / LM student</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Textual graphs built from citation data (survey references PubMed examples)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey highlights GRAD addresses scalability when combining GNNs and LMs by distilling graph knowledge into a student model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Distillation quality depends on teacher performance; trade-offs in fidelity vs. scalability; detailed limits not enumerated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7726.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphFormers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphFormers: GNN-nested transformers for representation learning on textual graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that integrates graph neural network components into Transformer blocks to jointly model textual content and graph structure for tasks like citation prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphformers: Gnn-nested transformers for representation learning on textual graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Graphformers: Gnn-nested transformers for representation learning on textual graph</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Junhan Yang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GraphFormers</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Nests GNN aggregation within Transformer layers so the model iteratively combines text encoding and graph aggregation, enabling global, graph-aware node semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textual nodes (paper text) plus citation graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-aware textual node representations for downstream tasks (e.g., citation prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Joint graph-text iterative fusion (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer with integrated GNN modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Citation datasets (DBLP citation prediction task noted in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey notes GraphFormers was effective on the DBLP citation prediction task, indicating better integration of graph and text features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Complex architecture may be harder to scale; joint modeling requires careful training strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7726.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LinkBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LinkBERT: Pretraining language models with document links</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining strategy that treats the corpus as a graph of interlinked documents, placing linked documents in the same context for masked language modeling and a document relation prediction objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linkbert: Pretraining language models with document links</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Linkbert: Pretraining language models with document links</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Michihiro Yasunaga et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LinkBERT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Samples linked documents together during pretraining and optimizes MLM plus a document-relation prediction objective so the model learns cross-document dependencies from citations/links.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Linked document contexts (documents connected by citation or hyperlinks)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Document-contextualized language model representations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Document relation prediction and MLM (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-style Transformer with link-aware pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey states LinkBERT augments pretraining by co-locating linked documents and adding relation prediction, improving cross-document representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on link/graph coverage in corpus; sampling/batching strategy design is important to avoid spurious dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7726.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PATTON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PATTON: Language model pretraining on text-rich networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining approach designed for text-rich networks that integrates network-contextualized masked language modeling and masked node prediction to capture correlation between text and network structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Patton: Language model pretraining on text-rich networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Patton: Language model pretraining on text-rich networks</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Wentao Bowen Jin et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PATTON</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines MLM with masked node prediction over networks like citation and co-authorship graphs to produce models that jointly model text attributes and network structure.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textual documents and their network links (e.g., citation/co-authorship networks)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Network-contextualized textual representations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Network-contextualized masked language modeling + masked node prediction (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Network-aware Transformer pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey indicates PATTON captures correlations between text and network structure, improving document-level downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires text-rich networks and careful integration of network prediction objectives; scalability and generalization across domains need validation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7726.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citationsum (Luo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citationsum: Citation-aware graph contrastive learning for scientific paper summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A summarization method that incorporates citation graph structure via graph contrastive learning to produce citation-aware summaries of scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Citationsum: Citation-aware graph contrastive learning for scientific paper summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Citationsum: Citation-aware graph contrastive learning for scientific paper summarization</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zheheng Luo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Citationsum</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Leverages citation graphs and graph contrastive objectives alongside textual encoders to create summaries that are informed by the citation network and inter-document relations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Citation sentences / citation graph plus source paper text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-aware abstractive summaries of papers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Graph contrastive learning combined with LLM encoders (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM/text encoder + graph-contrastive components</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports that incorporating citation-graph information via contrastive learning improves summarization quality compared to text-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on quality and coverage of citation graph; balancing graph and text signals is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7726.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CiteText-Gen (Mao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citesum / Citesum (Mao): Citation text-guided scientific extreme summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses generative LLMs (BART-large, PEGASUS-large) trained on citation text, abstracts, and conclusions to produce extreme (single-sentence) summaries of scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yuning Mao et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Citesum</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Feeds document-level inputs (citation text, abstract, conclusion) into generative seq2seq LLMs (BART-large, PEGASUS-large) to train models that generate concise single-sentence summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Citation text, abstracts, conclusions (document-level text)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extreme single-sentence abstractive summaries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Fine-tuning generative seq2seq models (no prompting techniques noted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-large, PEGASUS-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey states these generative models can effectively produce comprehensive single-sentence summaries when trained on citation-augmented inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain adaptation and limited supervision are challenges; risk of missing multi-document context beyond provided inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7726.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BACO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BACO: Background knowledge-and content-based framework for citing sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-source encoder-decoder approach that encodes citation networks and textual context to generate citation sentences for multiple references and supports intent control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Baco: A background knowledge-and content-based framework for citing sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Baco: A background knowledge-and content-based framework for citing sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yubin Ge et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BACO</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encodes both citation graph information and texts from citing/cited papers; generator attends to salient abstract sentences and citation function signals to generate citation sentences (multi-reference, intent-controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Citation network structure plus text from citing and cited papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated citation sentences (natural language) with intent control</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Multi-source pointer-generator with cross-attention (model-based generation, not prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder generation model with cross-attention (as described)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey indicates BACO generates citation sentences leveraging both graph and textual signals and can incorporate citation functions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generative citation sentence quality depends on accurate salience scoring and citation-function modeling; multi-reference generation is complex.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7726.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model trained for scientific knowledge with a novel citation token in pretraining intended to improve citation recommendation and scientific content generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ross Taylor et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Galactica (citation-token pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Introduces a citation token in pretraining to encode citation structure within a generative LLM, aiming to improve science-specific generation and citation recommendation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Large scientific corpus with citation metadata</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated scientific text and improved citation recommendation outputs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Pretraining with a citation token; not prompt-focused in survey</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey notes Galactica explored a citation token in pretraining and its influence on citation recommendation; details are mentioned but not quantified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Ethical, IP, and citation-quality concerns raised for LLMs in scientific contexts; Galactica highlighted the need for citation-aware and responsible model designs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7726.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7726.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Roethel et al. early-fusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-fusion of graph-represented information with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments language models with graph-represented information by providing the GNN component with graph and static textual data and feeding BERT with combined textual and graph features (early fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enriching language models with graph-based context information to better understand textual data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Enriching language models with graph-based context information to better understand textual data</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Albert Roethel et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Early-fusion graph+LM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Enhances language models by an early-fusion strategy where graph encoders and text encoders interact so that LMs receive both textual and graph-derived inputs for richer contextual understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Static textual data plus graph-structured information (e.g., citation graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-augmented text representations / improved downstream predictions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Early-fusion of graph encodings into LM inputs (no prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT with graph encoder (GNN) early-fusion</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports that early-fusion improves LM performance on tasks by giving the model access to both text and graph data simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Design complexity and appropriate fusion timing are challenges; may require more compute and careful training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPECTER: document-level representation learning using citation-informed transformers <em>(Rating: 2)</em></li>
                <li>Train your own GNN teacher: Graph-aware distillation on textual graphs <em>(Rating: 2)</em></li>
                <li>Graphformers: Gnn-nested transformers for representation learning on textual graph <em>(Rating: 2)</em></li>
                <li>Linkbert: Pretraining language models with document links <em>(Rating: 2)</em></li>
                <li>Patton: Language model pretraining on text-rich networks <em>(Rating: 2)</em></li>
                <li>Citationsum: Citation-aware graph contrastive learning for scientific paper summarization <em>(Rating: 2)</em></li>
                <li>Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 1)</em></li>
                <li>Baco: A background knowledge-and content-based framework for citing sentence generation <em>(Rating: 1)</em></li>
                <li>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7726",
    "paper_id": "paper-262044956",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "SPECTER",
            "name_full": "SPECTER: document-level representation learning using citation-informed transformers",
            "brief_description": "A citation-aware Transformer approach that incorporates inter-document citation links into pretraining via a triplet-loss objective to produce document-level embeddings useful for literature understanding and retrieval.",
            "citation_title": "SPECTER: document-level representation learning using citation-informed transformers",
            "mention_or_use": "mention",
            "paper_title": "SPECTER: document-level representation learning using citation-informed transformers",
            "authors": "Arman Cohan et al.",
            "year": 2020,
            "method_name": "SPECTER",
            "method_description": "Uses citation relationships as incidental supervision: constructs triplets (query, positive cited, negative non-cited) and pretrains a Transformer to produce document embeddings that pull cited documents closer and push unrelated documents apart.",
            "input_type": "Document text (e.g., title/abstract) plus citation graph links (inter-document relations)",
            "output_type": "Dense document embeddings (document-level representations)",
            "prompting_technique": "Contrastive / triplet-loss pretraining (no prompting)",
            "model_name": "Citation-aware Transformer (BERT-style)",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey reports SPECTER achieves document-level representations that capture inter-document relatedness and are effective for downstream literature tasks (e.g., citation prediction, retrieval).",
            "limitations": "Standard token/sentence objectives ignore inter-document structure; SPECTER addresses this but full multi-document, graph-structured knowledge extraction remains challenging.",
            "counterpoint": null,
            "uuid": "e7726.0"
        },
        {
            "name_short": "Sci-Mult",
            "name_full": "Sci-Mult: Pre-training multi-task contrastive learning models for scientific literature understanding",
            "brief_description": "A multi-task LLM pretraining framework that includes citation prediction as a sub-task and employs hard negative mining and contrastive learning to share knowledge across tasks while reducing task interference.",
            "citation_title": "Pre-training multi-task contrastive learning models for scientific literature understanding",
            "mention_or_use": "mention",
            "paper_title": "Pre-training multi-task contrastive learning models for scientific literature understanding",
            "authors": "Yu Zhang et al.",
            "year": 2023,
            "method_name": "Sci-Mult",
            "method_description": "Multi-task pretraining where citation prediction is one task; uses contrastive objectives with hard negatives to align citing/cited pairs and to transfer knowledge across scientific-document tasks.",
            "input_type": "Scientific document text and citation pairs (citing/cited)",
            "output_type": "Shared contextual representations for multiple literature-understanding tasks",
            "prompting_technique": "Contrastive learning with hard negative mining (no prompting)",
            "model_name": "Multi-task Transformer-based LLM",
            "model_size": "",
            "datasets_used": "Citation pairs and scientific corpora (not specified in survey)",
            "evaluation_metric": "",
            "reported_results": "Reported to facilitate knowledge sharing across tasks and improve citation-prediction related performance in the survey summary.",
            "limitations": "Potential task-specific interference and reliance on quality of negative sampling; details not fully specified in survey.",
            "counterpoint": null,
            "uuid": "e7726.1"
        },
        {
            "name_short": "ConGraT",
            "name_full": "ConGraT: self-supervised contrastive pretraining for joint graph and text embeddings",
            "brief_description": "A self-supervised framework that learns joint representations for textual content and graph nodes (e.g., citation graphs) via batch-wise contrastive objectives.",
            "citation_title": "Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings",
            "mention_or_use": "mention",
            "paper_title": "Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings",
            "authors": "William Brannon et al.",
            "year": 2023,
            "method_name": "ConGraT",
            "method_description": "Learns separate but aligned text and node (graph) embeddings with a contrastive loss that pulls related text-node pairs together and pushes unrelated pairs apart, enabling multi-modal scientific-document representations.",
            "input_type": "Textual content and parent graph (e.g., citation graph) node context",
            "output_type": "Aligned text and graph node embeddings",
            "prompting_technique": "Batch-wise contrastive learning (no prompting)",
            "model_name": "Contrastive Transformer + graph embedding components",
            "model_size": "",
            "datasets_used": "Graph-structured scientific datasets such as citation graphs (e.g., PubMed graph cited in survey)",
            "evaluation_metric": "",
            "reported_results": "Survey notes ConGraT learns individual representations for texts and nodes simultaneously and is effective for tasks requiring joint graph/text understanding.",
            "limitations": "Requires graph data and careful design of positive/negative sampling; scaling to very large graphs is nontrivial.",
            "counterpoint": null,
            "uuid": "e7726.2"
        },
        {
            "name_short": "GRAD",
            "name_full": "Graph-Aware Distillation (GRAD)",
            "brief_description": "A distillation framework that transfers graph-encoded knowledge from a GNN teacher into a graph-free student model via a shared language model to improve scalability while preserving citation-graph information.",
            "citation_title": "Train your own GNN teacher: Graph-aware distillation on textual graphs",
            "mention_or_use": "mention",
            "paper_title": "Train your own GNN teacher: Graph-aware distillation on textual graphs",
            "authors": "Costas Mavromatis et al.",
            "year": 2023,
            "method_name": "GRAD",
            "method_description": "Jointly optimizes a GNN teacher and a graph-free student via a shared LM, enabling the student to capture graph-derived signals without requiring GNN inference at deployment.",
            "input_type": "Textual documents and their citation graph (text-rich graph inputs)",
            "output_type": "Graph-informed, graph-free textual model (distilled LM representations)",
            "prompting_technique": "Knowledge distillation (teacher-student optimization), not prompt-based",
            "model_name": "Shared Language Model with GNN teacher / LM student",
            "model_size": "",
            "datasets_used": "Textual graphs built from citation data (survey references PubMed examples)",
            "evaluation_metric": "",
            "reported_results": "Survey highlights GRAD addresses scalability when combining GNNs and LMs by distilling graph knowledge into a student model.",
            "limitations": "Distillation quality depends on teacher performance; trade-offs in fidelity vs. scalability; detailed limits not enumerated in survey.",
            "counterpoint": null,
            "uuid": "e7726.3"
        },
        {
            "name_short": "GraphFormers",
            "name_full": "GraphFormers: GNN-nested transformers for representation learning on textual graphs",
            "brief_description": "An architecture that integrates graph neural network components into Transformer blocks to jointly model textual content and graph structure for tasks like citation prediction.",
            "citation_title": "Graphformers: Gnn-nested transformers for representation learning on textual graph",
            "mention_or_use": "mention",
            "paper_title": "Graphformers: Gnn-nested transformers for representation learning on textual graph",
            "authors": "Junhan Yang et al.",
            "year": 2021,
            "method_name": "GraphFormers",
            "method_description": "Nests GNN aggregation within Transformer layers so the model iteratively combines text encoding and graph aggregation, enabling global, graph-aware node semantics.",
            "input_type": "Textual nodes (paper text) plus citation graph structure",
            "output_type": "Graph-aware textual node representations for downstream tasks (e.g., citation prediction)",
            "prompting_technique": "Joint graph-text iterative fusion (no prompting)",
            "model_name": "Transformer with integrated GNN modules",
            "model_size": "",
            "datasets_used": "Citation datasets (DBLP citation prediction task noted in survey)",
            "evaluation_metric": "",
            "reported_results": "Survey notes GraphFormers was effective on the DBLP citation prediction task, indicating better integration of graph and text features.",
            "limitations": "Complex architecture may be harder to scale; joint modeling requires careful training strategies.",
            "counterpoint": null,
            "uuid": "e7726.4"
        },
        {
            "name_short": "LinkBERT",
            "name_full": "LinkBERT: Pretraining language models with document links",
            "brief_description": "Pretraining strategy that treats the corpus as a graph of interlinked documents, placing linked documents in the same context for masked language modeling and a document relation prediction objective.",
            "citation_title": "Linkbert: Pretraining language models with document links",
            "mention_or_use": "mention",
            "paper_title": "Linkbert: Pretraining language models with document links",
            "authors": "Michihiro Yasunaga et al.",
            "year": 2022,
            "method_name": "LinkBERT",
            "method_description": "Samples linked documents together during pretraining and optimizes MLM plus a document-relation prediction objective so the model learns cross-document dependencies from citations/links.",
            "input_type": "Linked document contexts (documents connected by citation or hyperlinks)",
            "output_type": "Document-contextualized language model representations",
            "prompting_technique": "Document relation prediction and MLM (no prompting)",
            "model_name": "BERT-style Transformer with link-aware pretraining",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey states LinkBERT augments pretraining by co-locating linked documents and adding relation prediction, improving cross-document representation.",
            "limitations": "Relies on link/graph coverage in corpus; sampling/batching strategy design is important to avoid spurious dependencies.",
            "counterpoint": null,
            "uuid": "e7726.5"
        },
        {
            "name_short": "PATTON",
            "name_full": "PATTON: Language model pretraining on text-rich networks",
            "brief_description": "A pretraining approach designed for text-rich networks that integrates network-contextualized masked language modeling and masked node prediction to capture correlation between text and network structures.",
            "citation_title": "Patton: Language model pretraining on text-rich networks",
            "mention_or_use": "mention",
            "paper_title": "Patton: Language model pretraining on text-rich networks",
            "authors": "Wentao Bowen Jin et al.",
            "year": 2023,
            "method_name": "PATTON",
            "method_description": "Combines MLM with masked node prediction over networks like citation and co-authorship graphs to produce models that jointly model text attributes and network structure.",
            "input_type": "Textual documents and their network links (e.g., citation/co-authorship networks)",
            "output_type": "Network-contextualized textual representations",
            "prompting_technique": "Network-contextualized masked language modeling + masked node prediction (no prompting)",
            "model_name": "Network-aware Transformer pretraining",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey indicates PATTON captures correlations between text and network structure, improving document-level downstream tasks.",
            "limitations": "Requires text-rich networks and careful integration of network prediction objectives; scalability and generalization across domains need validation.",
            "counterpoint": null,
            "uuid": "e7726.6"
        },
        {
            "name_short": "Citationsum (Luo)",
            "name_full": "Citationsum: Citation-aware graph contrastive learning for scientific paper summarization",
            "brief_description": "A summarization method that incorporates citation graph structure via graph contrastive learning to produce citation-aware summaries of scientific papers.",
            "citation_title": "Citationsum: Citation-aware graph contrastive learning for scientific paper summarization",
            "mention_or_use": "mention",
            "paper_title": "Citationsum: Citation-aware graph contrastive learning for scientific paper summarization",
            "authors": "Zheheng Luo et al.",
            "year": 2023,
            "method_name": "Citationsum",
            "method_description": "Leverages citation graphs and graph contrastive objectives alongside textual encoders to create summaries that are informed by the citation network and inter-document relations.",
            "input_type": "Citation sentences / citation graph plus source paper text",
            "output_type": "Citation-aware abstractive summaries of papers",
            "prompting_technique": "Graph contrastive learning combined with LLM encoders (no prompting)",
            "model_name": "LLM/text encoder + graph-contrastive components",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey reports that incorporating citation-graph information via contrastive learning improves summarization quality compared to text-only baselines.",
            "limitations": "Depends on quality and coverage of citation graph; balancing graph and text signals is nontrivial.",
            "counterpoint": null,
            "uuid": "e7726.7"
        },
        {
            "name_short": "CiteText-Gen (Mao et al.)",
            "name_full": "Citesum / Citesum (Mao): Citation text-guided scientific extreme summarization",
            "brief_description": "Uses generative LLMs (BART-large, PEGASUS-large) trained on citation text, abstracts, and conclusions to produce extreme (single-sentence) summaries of scientific papers.",
            "citation_title": "Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision",
            "mention_or_use": "mention",
            "paper_title": "Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision",
            "authors": "Yuning Mao et al.",
            "year": 2022,
            "method_name": "Citesum",
            "method_description": "Feeds document-level inputs (citation text, abstract, conclusion) into generative seq2seq LLMs (BART-large, PEGASUS-large) to train models that generate concise single-sentence summaries.",
            "input_type": "Citation text, abstracts, conclusions (document-level text)",
            "output_type": "Extreme single-sentence abstractive summaries",
            "prompting_technique": "Fine-tuning generative seq2seq models (no prompting techniques noted)",
            "model_name": "BART-large, PEGASUS-large",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey states these generative models can effectively produce comprehensive single-sentence summaries when trained on citation-augmented inputs.",
            "limitations": "Domain adaptation and limited supervision are challenges; risk of missing multi-document context beyond provided inputs.",
            "counterpoint": null,
            "uuid": "e7726.8"
        },
        {
            "name_short": "BACO",
            "name_full": "BACO: Background knowledge-and content-based framework for citing sentence generation",
            "brief_description": "A multi-source encoder-decoder approach that encodes citation networks and textual context to generate citation sentences for multiple references and supports intent control.",
            "citation_title": "Baco: A background knowledge-and content-based framework for citing sentence generation",
            "mention_or_use": "mention",
            "paper_title": "Baco: A background knowledge-and content-based framework for citing sentence generation",
            "authors": "Yubin Ge et al.",
            "year": 2021,
            "method_name": "BACO",
            "method_description": "Encodes both citation graph information and texts from citing/cited papers; generator attends to salient abstract sentences and citation function signals to generate citation sentences (multi-reference, intent-controlled).",
            "input_type": "Citation network structure plus text from citing and cited papers",
            "output_type": "Generated citation sentences (natural language) with intent control",
            "prompting_technique": "Multi-source pointer-generator with cross-attention (model-based generation, not prompt engineering)",
            "model_name": "Encoder-decoder generation model with cross-attention (as described)",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey indicates BACO generates citation sentences leveraging both graph and textual signals and can incorporate citation functions.",
            "limitations": "Generative citation sentence quality depends on accurate salience scoring and citation-function modeling; multi-reference generation is complex.",
            "counterpoint": null,
            "uuid": "e7726.9"
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A large language model for science",
            "brief_description": "A large language model trained for scientific knowledge with a novel citation token in pretraining intended to improve citation recommendation and scientific content generation.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "paper_title": "Galactica: A large language model for science",
            "authors": "Ross Taylor et al.",
            "year": 2022,
            "method_name": "Galactica (citation-token pretraining)",
            "method_description": "Introduces a citation token in pretraining to encode citation structure within a generative LLM, aiming to improve science-specific generation and citation recommendation capabilities.",
            "input_type": "Large scientific corpus with citation metadata",
            "output_type": "Generated scientific text and improved citation recommendation outputs",
            "prompting_technique": "Pretraining with a citation token; not prompt-focused in survey",
            "model_name": "Galactica LLM",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey notes Galactica explored a citation token in pretraining and its influence on citation recommendation; details are mentioned but not quantified in the survey.",
            "limitations": "Ethical, IP, and citation-quality concerns raised for LLMs in scientific contexts; Galactica highlighted the need for citation-aware and responsible model designs.",
            "counterpoint": null,
            "uuid": "e7726.10"
        },
        {
            "name_short": "Roethel et al. early-fusion",
            "name_full": "Early-fusion of graph-represented information with language models",
            "brief_description": "An approach that augments language models with graph-represented information by providing the GNN component with graph and static textual data and feeding BERT with combined textual and graph features (early fusion).",
            "citation_title": "Enriching language models with graph-based context information to better understand textual data",
            "mention_or_use": "mention",
            "paper_title": "Enriching language models with graph-based context information to better understand textual data",
            "authors": "Albert Roethel et al.",
            "year": 2023,
            "method_name": "Early-fusion graph+LM",
            "method_description": "Enhances language models by an early-fusion strategy where graph encoders and text encoders interact so that LMs receive both textual and graph-derived inputs for richer contextual understanding.",
            "input_type": "Static textual data plus graph-structured information (e.g., citation graphs)",
            "output_type": "Graph-augmented text representations / improved downstream predictions",
            "prompting_technique": "Early-fusion of graph encodings into LM inputs (no prompting)",
            "model_name": "BERT with graph encoder (GNN) early-fusion",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "Survey reports that early-fusion improves LM performance on tasks by giving the model access to both text and graph data simultaneously.",
            "limitations": "Design complexity and appropriate fusion timing are challenges; may require more compute and careful training.",
            "counterpoint": null,
            "uuid": "e7726.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPECTER: document-level representation learning using citation-informed transformers",
            "rating": 2,
            "sanitized_title": "specter_documentlevel_representation_learning_using_citationinformed_transformers"
        },
        {
            "paper_title": "Train your own GNN teacher: Graph-aware distillation on textual graphs",
            "rating": 2,
            "sanitized_title": "train_your_own_gnn_teacher_graphaware_distillation_on_textual_graphs"
        },
        {
            "paper_title": "Graphformers: Gnn-nested transformers for representation learning on textual graph",
            "rating": 2,
            "sanitized_title": "graphformers_gnnnested_transformers_for_representation_learning_on_textual_graph"
        },
        {
            "paper_title": "Linkbert: Pretraining language models with document links",
            "rating": 2,
            "sanitized_title": "linkbert_pretraining_language_models_with_document_links"
        },
        {
            "paper_title": "Patton: Language model pretraining on text-rich networks",
            "rating": 2,
            "sanitized_title": "patton_language_model_pretraining_on_textrich_networks"
        },
        {
            "paper_title": "Citationsum: Citation-aware graph contrastive learning for scientific paper summarization",
            "rating": 2,
            "sanitized_title": "citationsum_citationaware_graph_contrastive_learning_for_scientific_paper_summarization"
        },
        {
            "paper_title": "Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision",
            "rating": 2,
            "sanitized_title": "citesum_citation_textguided_scientific_extreme_summarization_and_domain_adaptation_with_limited_supervision"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Baco: A background knowledge-and content-based framework for citing sentence generation",
            "rating": 1,
            "sanitized_title": "baco_a_background_knowledgeand_contentbased_framework_for_citing_sentence_generation"
        },
        {
            "paper_title": "Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings",
            "rating": 1,
            "sanitized_title": "congrat_selfsupervised_contrastive_pretraining_for_joint_graph_and_text_embeddings"
        }
    ],
    "cost": 0.019320249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>When Large Language Models Meet Citation: A Survey
18 Sep 2023</p>
<p>Yang Zhang 
School of Computing
Faculty of Science and Engineering
Macquarie University
SydneyAustralia</p>
<p>School of Information Management
Wuhan University
WuhanChina</p>
<p>Yufei Wang 
School of Computing
Faculty of Science and Engineering
Macquarie University
SydneyAustralia</p>
<p>Kai Wang 
School of Computer Science and Engineering
Nanyang Technological University
Singapore</p>
<p>Quan Z Sheng 
School of Computing
Faculty of Science and Engineering
Macquarie University
SydneyAustralia</p>
<p>Lina Yao 
CSIRO's Data61
SydneyAustralia</p>
<p>Computer Science and Engineering
University of New South Wales
SydneyAustralia</p>
<p>Adnan Mahmood 
School of Computing
Faculty of Science and Engineering
Macquarie University
SydneyAustralia</p>
<p>Wei Emma Zhang 
School of Computer and Mathematical Sciences
The University of Adelaide
AdelaideAustralia</p>
<p>Rongying Zhao 
School of Information Management
Wuhan University
WuhanChina</p>
<p>When Large Language Models Meet Citation: A Survey
18 Sep 2023F4D87C67D9A16A8E7C95D4BD89B5964CarXiv:2309.09727v1[cs.DL]
Citations in scholarly work serve the essential purpose of acknowledging and crediting the original sources of knowledge that have been incorporated or referenced.Depending on their surrounding textual context, these citations are used for different motivations and purposes.Large Language Models (LLMs) could be helpful in capturing these fine-grained citation information via the corresponding textual context, thereby enabling a better understanding towards the literature.Furthermore, these citations also establish connections among scientific papers, providing high-quality interdocument relationships and human-constructed knowledge.Such information could be incorporated into LLMs pre-training and improve the text representation in LLMs.Therefore, in this paper, we offer a preliminary review of the mutually beneficial relationship between LLMs and citation analysis.Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation.We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship.We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation.</p>
<p>Introduction</p>
<p>Citations facilitate the exchange of specialized knowledge by enabling authors and readers to make targeted references across multiple contexts at the same time.A better understanding of the textual context associated with citations could lead to a precise and rich scientific literature network.With the emergence of Large Language Models (LLMs), * *Corresponding Author:yang.zhang@mq.edu.ausuch as ChatGPT/GPT41 and PaLM22 , there is a growing interest in collaboratively leveraging the capabilities of LLMs and citations to mutually enhance one another.</p>
<p>As depicted in Figure 1, there exists a mutually beneficial relationship between LLMs and citations.On the one hand, LLMs can provide highquality textual features for understanding citation textual context.In this paper, we view LLMs as large Transformer-based language models trained on massive document corpora using various selfsupervised objectives.Depending on the architectural structure, LLMs can be categorized into a) Encoder-only LLMs (e.g., BERT); b) Decoder-only LLMs (e.g., GPT series) and c) Encoder-decoder LLMs (e.g., T5 series).They can provide highquality, dense vectors for text and generate coherent and fluent natural language.Empirically, LLM-based models could effectively comprehend citation textual context and produce high-quality citation analysis results.On the other hand, citations could establish high-quality literature linkage in LLM pre-training corpus.Such linkage revealing multi-hop knowledge and information spanning across multiple documents is largely overlooked by many existing LLMs.By integrating the continuously growing and up-to-date citation network, LLMs would learn how humans acquire knowledge and make discoveries across different but relevant sources (Mavromatis et al., 2023).</p>
<p>Motivated by this remarkable co-development connection between LLMs and citations, how to effectively leverage the unique strengths from both sides has emerged as a promising research direction.There are a few studies that attempt to review relevant papers in this domain.However, they either only discuss the citation analysis tasks setup and designs (Ding et al., 2014;Lyu et al., 2021) or simply focus on the general machine learning and NLP technologies applications on citation analysis tasks (Jha et al., 2017;Iqbal et al., 2021).There are limited research efforts summarizing how stateof-the-art LLMs help with citation analysis tasks.Furthermore, none of these studies particularly look into the mutually beneficial relationship between LLMs and citations (i.e., how LLMs could further benefit from citation-relation data).We refer readers to Table 1 for more details on previous citation-related surveys.</p>
<p>To address this disparity, we undertake an initial survey to explore how LLMs and citation analysis tasks can derive mutual benefits from each other.Figure 2 shows the taxonomy of this sur-vey.We first review how LLMs contribute to the in-text citation analysis tasks, including citation classification, citation summarization, and citation recommendation.In general, the strong text representations from LLMs are helpful in modelling the complex textual context around the citations.We then examine how LLMs could benefit from various citation corpus.This includes directly using citations as LLMs training data, incorporating citations as LLMs training signals, and leveraging citations as LLMs structural inputs.Incorporating information from multiple documents helps LLMs to learn multi-hop knowledge that does not exist in single-document training data (Roethel et al., 2023).Finally, we provide an overview of future research opportunities in tightening the connections between citations and LLMs.</p>
<p>LLMs for Citation</p>
<p>In this section, we discuss how LLMs improve the performance of various citation-related tasks, including citation classification, citation-based summarization, and citation recommendation.As shown in Figure 3, LLMs achieve this goal by providing high-quality text embedding, strong text generation capability, and flexibility to incorporate citation structure information.The authors summarize studies that have addressed the issue of identifying and classifying citation context.They examine the most recent techniques and data repositories used for citation context analysis.</p>
<p>Systematic Bibliometrics</p>
<p>This study employed a meta-synthesis approach to develop a new comprehensive classification of citation motivations, based on previous research.</p>
<p>Systematic Computer Science</p>
<p>The primary focus of this survey is on publications that employ natural language processing and machine learning techniques to analyze citations.</p>
<p>Figure 3: Large Language Models (LLMs) improve citation-related tasks via their high-quality text embedding, strong text generation capability and feasibility to incorporate citation structure information.</p>
<p>LLMs for Citation Classification</p>
<p>Consistent with earlier studies, we perceive citation classification task as comprising two unique components: classification based on citation intent and citation sentiment.(Teufel et al., 2006;Abu-Jbara et al., 2013;Zhang et al., 2021).Following (Teufel et al., 2006;Zhang et al., 2021), we also consider Citation sentiment as part of the citation function in the citation classification task.The task of citation classification involves assigning a functional label (e.g., "Background Information", "Method" or "Positive") to a specific citation instance.This is done by providing inputs (e.g., textual citation context and citing sentences) to the classification model.The fundamental idea is to determine the citing reasons and sentiment via their surrounding textual context.</p>
<p>Given the time and cost constraints associated with manual citation function labeling, and the growth in scientific literature, the need for au-tomated citation function classification, typically seen as a single-label classification task, has surged (Iqbal et al., 2021).Various sophisticated deep learning models have been applied to this task (Cohan et al., 2019).In recent times, the employment of LLMs has gained significant traction in the realm of citation function classification tasks (Roman et al., 2021).This is largely attributed to the superior performance these models deliver, courtesy of their impressive prowess in handling text classification tasks.</p>
<p>High-quality Text Representations.Incorporating LLMs, e.g., BERT (Devlin et al., 2019), SciB-ERT (Beltagy et al., 2019a), and T5 (Raffel et al., 2020), into citation function classification tasks has shown considerable advantages.These models excel at achieving superior text representation, thereby significantly aiding the enhancement of citation classification task performance (Maheshwari et al., 2021;Visser and Dunaiski, 2022;Lauscher et al., 2021;Roman et al., 2021;Te et al., 2022).For example, Visser and Dunaiski (2022) assesses the use of BERT for citation classification of scientific literature by leveraging the "[CLS]" as the output vector and further fine-tuning the citation classification model to obtain better performance.</p>
<p>Integration with other ML Modules.LLMs can not only provide high-quality text representation along but also integrate other machine learning modules.For instance, Budi and Yaniasih (2023) propose a multi-output convolutional neural network model based classifier, and they use BERT as the initial word embedding encoder to encode the input text and obtain better performance of citation classification task.Wang et al. (2020b) propose a hybrid approach that combines the Gradient Boosting Decision Tree (GBDT) model with the BERT model.They utilize linear blending to harmonize the stacking models and the BERT model, thereby producing their ultimate outcomes.</p>
<p>Generation Based Classification.Above LLMs are commonly used as discriminative LLMs (i.e., encoder-only) that can produce hidden representations to the input text.There are also Generative LLMs (i.e., Seq2Seq or Decoder-only) that are capable of directly generating high-quality text.Recently, Zhang et al. (2022) propose a novel generative citation function classification model based on T5 (Raffel et al., 2020).Instead of mapping neural hidden states to label probability, they directly train the model to generate the citation function labels (e.g., Motivation) at the decoder side for the given inputs.Such generative LLMs can also be high-quality synthetic data generators.Gupta et al. (2023) combine the GPT-2 into their framework to obtain the high-quality synthetic data for citation function classification task.In the same trend, Zhang et al. (2023a) propose HyBridDA, which combines the retrieved results from the large unlabelled corpus and fine-tunes the GPT-2 model to generate desired and novel citation sentences to tackle the data sparsity and data imbalance issue.The generated syntactic data are then combined with existing real training data to fine-tune SciB-ERT as their final classification model.</p>
<p>LLMs for Citation-based summarization</p>
<p>Aggregating all citation sentences referring to a paper can yield a significant amount of information about the paper itself (Abu-Jbara and Radev, 2011).Citation-based summarization harnesses the citations from a scientific article or its reference citation network to assemble a succinct summary.The summary aims to highlight the main contributions of papers, based on citation sentences from papers that cite the source document.The initial attempt uses sentence clustering and ranking methods (Luo et al., 2023).</p>
<p>With the progression of research, automatic citation-based summarization can be broadly divided into two distinct categories as follows: (a) document-level citation context span based summarization (Qazvinian and Radev, 2008).Techniques involving span identification and varied feature extraction have been utilized (Cohan and Goharian, 2015); (b) citation network structured aware summarization.More recent research works put forward the idea that citation graphs can be instrumental in creating top-notch summaries of scientific papers (Chen et al., 2022;Luo et al., 2023).LLMs, with their proficiency in deriving complex text representations from dense vectors and generating high-quality text, are gaining prominence in complex tasks like citation-based summarization.Hence, there's an enhanced capability to encode citation context and integrate citation network graphs, thereby further boosting citation-based summarization efforts (Luo et al., 2023).Document-level Features.Given their superior capability to derive text representation from textual input, LLMs can be employed for capturing document-level features within a citation recommendation system.Saini et al. (2023) introduce a multi-view clustering approach that utilizes dualembedding spaces and citation contexts for document summarization, leveraging document encoders like BERT, GPT-2, and XLNET to capture document-level features.Roy and Mercer (2022) propose a method for establishing connections between citing sentences and corresponding citation sentences in the referred literature.Their framework pairs every sentence in a publication with all sentences from the referenced document, then seeks to identify semantically similar pairs.The selected sentences from the referenced paper, which have semantic similarities with the publication's sentences, are considered cited statements.This process entails the development of a citation linkage framework using both sequential and treestructured models based on Bio-RoBERTa.In addition, Mao et al. (2022) feed document-level text, i.e., citation text, abstract, and conclusion into the generative models BART-large (Lewis et al., 2020) and PEGASUS-large (Zhang et al., 2020) as training data, using the subsequent trained models to effectively produce comprehensive scientific extreme summarization, i.e. single sentence summaries.</p>
<p>Incorporation with Graph Structure.The above citation-based summarization methods only consider surface document information.However, relevant citations can provide rich literature network information.Thus, some research propose to combine LLMs with Graph-based neural networks to effectively incorporate these additional information.Chen et al. (2022) first shift through comparative citations to identify objects of comparison.They then construct a comparative scientific summarization corpus and put forward the Comparative Graph-based Summarization BERT (CGSUM-BERT).This model is designed to craft comparative summaries with the assistance of citation guidance.Cai et al. (2022) propose a model called COVIDSum for summarizing COVID-19 scientific papers, which leverages a linguistically enriched SciBERT framework.The process begins with the extraction of key sentences from the source papers and the construction of word co-occurrence graphs.A SciBERT-based sequence encoder is then employed to encode the sentences, while a Graph Attention Networks-based graph encoder processes the word co-occurrence graphs.In the final stage, these two encodings are fused to generate an abstract summary of each scientific paper.</p>
<p>LLMs for Citation Sentence Generation</p>
<p>The emerging and valuable task of generating citation sentences independently is made possible through the capabilities of LLMs.Xing et al. (2020) initially train an implicit citation text extraction model using BERT.They utilize this model to create an extensive training dataset for the citation text generation task.Subsequently, they introduce and train a multi-source pointer-generator network equipped with a cross-attention mechanism for citation text generation.BACO (Ge et al., 2021) employs its encoder to encode both the citation network and the text from both citing and cited papers.BACO's generator assesses the significance of sentences within the cited paper's abstract and leverages this knowledge for text generation.Moreover, the framework undergoes additional training with citation functions.Both of them are dedicated to the generation of citation sentences for multiple references while also incorporating intent control.</p>
<p>LLMs for Citation Recommendation</p>
<p>Citation recommendation serves as an invaluable tool for academic publishers and scholars (Pillai and Deepthi, 2022) by producing suitable references that align with the specified query text (Iqbal et al., 2021).Taylor et al. (2022) constitutes the most extensive and comprehensive endeavor to pretrain literature-specific LLMs.Additionally, the Galactica paper introduces a novel citation token for pre-training and explores its influence on the citation recommendation task.</p>
<p>In a formal context, a citation recommendation model is like a helpful tool that suggests a selection of academic papers that researchers might find useful.Suppose we have a collection of papers and a group of researchers.The citation recommendation model evaluates how beneficial a specific paper from this collection could be to a particular researcher.This evaluation is done by a so-called utility function that measures the benefit.In other words, it determines how well a paper matches a researcher's needs or interests.The model then recommends a paper that maximizes this benefit for each researcher.This is typically reflected in the form of a rating given by the researchers to the suggested papers.</p>
<p>Conventionally, various components of citation recommendation systems heavily relied on traditional machine learning models (Yasunaga et al., 2019).However, the recent surge in popularity of Large Language Models (LLMs) in these systems is primarily due to their exceptional text comprehension capabilities.We observe that the use of LLMs in these aforementioned types of citation recommendations bears similarities.Ranking Based Citation Recommendation.Ranking-based citation recommendation is to rank the corresponding citations based on the given textual inputs.LLMs' capacity to understand and generate high-quality text can add a level of abstraction that traditional machine learning models often lack, making these models more adaptable to varied and complex tasks.For example, Nogueira et al. ( 2020) interprets citation recommendation as a ranking problem and propose to use a twotier method involving initial candidate generation, followed by a re-ranking process.Within this structure, an effective combination tailored to the scientific domain is utilized, which includes a "bag of words" retrieval process, succeeded by a re-scoring phase using a BERT model.A more comprehensive method, Deepcite that uses a hybrid neural network and focuses on content is introduced by Wang et al. (2020a).Initially, the BERT model is used to pull out complex semantic meanings from the text.Then, to gain both the local and sequence context in sentences, they use a multi-scale CNN model and a Bi-LSTM model.This results in matching text vectors that create sets of potential citations.Finally, a deep neural network rearranges these sets in order of relevance, considering both their individual scores and other various features.Similarly, Gu et al. (2022a) investigate prefetching by utilizing nearest neighbor search within text embeddings, developed through a hierarchical attention network.They introduce a method named the Hierarchical Attention encoder (HAtten).When paired with a SciBERT reranker, fine-tuned on local citation recommendation tasks, it delivers high prefetch recall for a defined pool of candidates awaiting reranking.Also, Abbas et al. ( 2022) proposed a simply but more effective method by employing Word2Vec and BERT models to represent their dataset through word embeddings, and introduce using Bi-LSTM based ranker to classify the rhetorical zones of research articles.Their system computes similarity using rhetorical zone embeddings to effectively tackle the cold-start problem.It is a prevalent occurrence for a single context to be substantiated by multiple co-citation pairs.To tackle this issue, Zhang and Ma (2022) propose Multi-Positive BERT Model (MP-BERT4REC) which is designed</p>
<p>Direct Citation</p>
<p>Citation for LLMs</p>
<p>In previous sections, we discuss how LLMs help with various citation analysis tasks and provide rich and precise understandings about the deep semantics of citations.Interestingly, citations also establish high-quality literature linkage, potentially improving LLMs in understanding highly organized texts from multiple documents, and enabling their acquisition of multi-hop knowledge by covering diverse academic fields' research.As shown in</p>
<p>Citation as LLMs Training Data</p>
<p>The most straightforward way to leverage citations for LLMs is to construct citations as training data for LLMs pre-training or fine-tuning.Some researchers focus on applying citation corpus to train LLMs, while others explore training LLMs via citation-related tasks.</p>
<p>Pre-training Literature-specific LLMs.The success of general LLMs, such as BERT, motivates further exploration of literature-specific LLMs.As a core role in scientific literature data, the citationrelated text is bound to participate in the pretraining process.Following the BERT language model, SciBert (Beltagy et al., 2019b) is pre-trained on a vast multi-domain corpus consisting of 1.14M scientific papers.SsciBert (Shen et al., 2023) is pre-trained on papers from Social Science Citation Index (SSCI) journals, compensating for the lack of LLMs on the asocial science domain.Gu et al. (2022b) focus on the bio-medicine literature and propose PubMedBERT with the strategy of domainspecific pre-training from scratch on a PubMed corpus.</p>
<p>Citation-related Multi-task Learning.In the multi-task pre-training, the LLM model is trained to perform all tasks with data annotated for different tasks.As citation data creates rich structure information, citation-related tasks also play a role in pertaining LLMs.One respective citation task, citation prediction, is to find a pair of citing and cited scientific papers.To achieve multi-task pretraining involving citation prediction, some recent studies utilize the contrastive Learning technique to design pre-training objectives.Given a query paper q and the positive and negative candidates, c + and c  , a common contrastive learning objective is to pull q and relevant c + together while pushing q and irrelevant c  apart.Specifically, Sci-Mult (Zhang et al., 2023b) is a novel multi-task LLM pre-training framework for scientific literature understanding tasks.Treating citation prediction as a sub-task, SciMult focuses on facilitating common knowledge sharing across different tasks while preventing task-specific skill interference.To achieve it, the hard negative mining technique used in the citation prediction task is employed for scientific paper contrastive learning in different tasks (Cohan et al., 2020).Utilizing a batch-wise contrastive learning objective, ConGraT (Brannon et al., 2023) is a self-supervised framework for simultaneously learning individual representations for texts and nodes within a parent graph, such as citation graphs in Pubmed with each paper related to a specific node.Wang et al. (2022a) propose E5, a new family of text embeddings trained using weak supervision signals in a contrastive manner on a large text pair dataset, CCPairs.As an important data part of CCPairs, each instance from scientific literature includes a title, abstract, and a pair of citing and cited papers.</p>
<p>Citation as LLMs Training Signals</p>
<p>Citations create rich and high-quality semantic links between scientific documents.However, the mainstream pre-training objectives of LLMs, such as Masked Language Modelling (MLM), purely encode texts and do not take inter-document structure information into consideration.To fill this gap, recent studies draw attention to document-level LLMs and propose new sampling strategies and pre-training objectives considering these document connections, especially citations.</p>
<p>Citation-aware LLMs Training Objective.Previous LLMs like BERT are aimed at token-and sentence-level training objectives.Some recent language models are proposed specifically for document-level representation, making it more suited for document-level downstream applications such as literature classification and citation recommendation.SPECTER (Cohan et al., 2020) achieves document-level representation using citation-aware Transformers.It captures interdocument relatedness in the citation graphs, utilizes citations as an inter-document incidental supervision signal, and transforms this signal into a triplet-loss pre-training objective.Raman et al. (2022) propose a novel document retrieval method that combines intra-document content with interdocument relations in learning document representations.Benefiting from the inter-document citation relationships, a contrastive learning-based quintuplet loss function is designed, which pushes semantically similar documents closer and structurally unrelated documents further apart in the representation space.Additionally, the model varies the separation margins between the documents based on the strength of their relationships.PATTON (Jin et al., 2023) is specifically designed to handle textrich networks which are made up of text documents and their interconnections, such as citations and co-authorships in an academic context.Unlike current pre-training approaches that primarily focus on texts without considering inter-document structural data, Patton integrates two pre-training strategies: network-contextualized masked language modeling and masked node prediction, to effectively capture the correlation between text attributes and network structures.</p>
<p>Citation-aware Data Sampling Strategies.A bias in pre-training language models has been revealed that the model creates stronger dependencies between text segments appearing in the same training example than those in separate examples (Levine et al., 2022).Thereby, some studies propose new text sampling and batching strategies considering document connections.LinkBERT (Yasunaga et al., 2022) treats a text corpus as a graph of interlinked documents and creates language model inputs that put linked documents within the same context.Then, the Bert-based language model is pre-trained with two self-supervised objectives, namely masked language modeling and the newly proposed document relation prediction.SciNCL (Ostendorff et al., 2022) is an LM-based approach for scientific document representation learning, which utilizes controlled nearest neighbor sampling over citation graph embeddings for contrastive learning.The method introduces the concept of continuous similarity, enabling the model to recognize similarities between scientific papers even without direct citation links, and avoids collisions between negative and positive samples by controlling the sampling margin between them.</p>
<p>Citation as LLMs Structural Inputs</p>
<p>The above approaches simply use pure textual data as the inputs to LLMs and treat citation graphs as data sources or training objectives.However, they could potentially overlook the complex and finegrained citation graph structure information.With the rapid development of Graph Neural Networks (GNNs), recent researchers propose to integrate GNNs into LLMs by fusing the node representation of citation graphs, facilitating better understandings of scientific literature as well as natural language.We categorize previous research into Cascaded Citation Inputs Fusion and Joint Citation Inputs Fusion.</p>
<p>Cascaded Citation Inputs Fusion.Some researchers propose to infuse citation graph inputs using a cascaded model architecture where LLMs independently encode the textual input as embeddings and then GNNs are applied to amalgamate these embeddings.Guan and Jiang (2022) presents Citation Graph Collaborative Filtering (CGCF), which is a combination of document representation and Graph Neural Network, to improve automated recommendation systems for scientific articles.The method begins with building a user-paper bipartite graph based on citation relations, then initializes the paper's embedding using its title and abstract through a pre-trained language model.The final step refines node embeddings by GNNs to achieve a more holistic and in-depth representation of user-paper interactions and semantic content.Wang et al. (2022b) propose DisenCite, a novel disentangled representation-based model for automatically generating citation text by integrating paper text representation from LMs and citation graph representation from GNNs.Unlike prior approaches, this method produces context-specific citation text, allowing for the generation of different citation types for the same paper.</p>
<p>Joint Citation Inputs Fusion.The above architecture is marked by a key limitation: it models textual features in isolation, which detracts from its effectiveness.Therefore, some scholars further explore various novel architectures to combine citation graphs and LLMs.GraphFormers (Yang et al., 2021) integrates Graph Neural Networks (GNN) components into the transformer blocks of language models for more efficient textual graph representation learning.i.e. they demonstrated the model was effective on the DBLP citation prediction task.Unlike existing works, GraphFormers blends text encoding and graph aggregation into an iterative workflow, thereby comprehending each node's semantics from a global perspective.Furthermore, the authors propose a progressive learning strategy where the model is incrementally trained on manipulated and original data to enhance its ability to integrate information on a graph.Mavromatis et al. (2023) present a novel Graph-Aware Distillation framework (GRAD) to address the scalability issue in combining Graph Neural Networks (GNNs) with Language Models (LMs) by using citations graphs.Unlike traditional knowledge distillation, GRAD concurrently optimizes a GNN teacher model and a graph-free student model via a shared LM, which allows the student model to use the graph information encoded by the teacher model.Roethel et al. (2023) enhance deep learning language models by incorporating graphrepresented information from citation graphs with an early fusion strategy, where the GNN component has access only to the graph and to the static textual data, while BERT is fed directly with both textual and graph information.</p>
<p>Future Directions</p>
<p>In the previous sections, we have reviewed how LLMs and citations mutually benefit from each other.However, there are still many challenges and open problems that need to be addressed.In this section, we discuss the future directions in further tightening LLMs and citations.</p>
<p>Citation for Responsible and Accountable LLMs</p>
<p>Large Language Models (LLMs) offer revolutionary advantages but also present distinctive challenges, particularly in the domains of intellectual property (IP) and ethical considerations.Brown et al. (2020) takes a novel approach to address these challenges by drawing parallels between LLMs and well-established web systems by using citation as the essitail component in LLMs.Furthermore, they suggest that a comprehensive citation system for Large Language Models (LLMs) should encompass both non-parametric and parametric content.</p>
<p>To steer future endeavors aimed at constructing more responsible and transparent Large Language Models (LLMs), a set of research challenges in this domain should be considered.</p>
<p>Zero-shot/Few-shot Learning for Citation-related Tasks</p>
<p>As discussed in Sec. 2, many LLM-based approaches heavily rely on a large amount of labeled training data, ranging from a few hundred to a few thousand.However, recent state-of-the-art LLMs, such as GPT-3 (Brown et al., 2020) and ChatGPT, have demonstrated superior zero-shot and few-shot ability in handling various NLP tasks.It is interesting to explore the strengths and weaknesses of the existing LLMs when handling citation-related tasks e.g, citation function classification and citation sentiment classification.As general LLMs are normally trained with very limited academic training corpora, one could verify whether the LLMs trained with a sufficiently large portion of academic corpora are equipped with stronger zero/few-shot ability than these general LLMs.</p>
<p>Citation Instruction Tuning</p>
<p>Recently, instruction tuning that distils various skills and knowledge from existing strong LLMs (Wei et al., 2022;Sanh et al., 2022;Ouyang et al., 2022;Wang et al., 2022c;Singh et al., 2022), has received significant research interests.Recently, (Zhang et al., 2023b) propose to unify three citation-related tasks as a set of unified instructions for better learning multi-task knowledge.It will be interesting to further unify a diverse set of citationrelated tasks as instructions and explore whether fine-tuning the LLMs with these instructions can further improve the performance of these LLMs and generalize well on those unseen tasks.</p>
<p>Expanding Citation Networks For LLMs</p>
<p>In Sec. 3, we only consider incorporating citation text into LLMs.However, the citation is not the only type of data that builds linkage between different text (e.g., papers, documents, web pages) (Getoor and Diehl, 2005).It will be interesting to expand the existing citation network to other types of data sources.For example, linking scientific papers to relevant knowledge graph entities or social media posts that talk about similar topics or papers.Such an expanded graph includes more diverse textual information and should be helpful in training or fine-tuning high-quality LLMs.</p>
<p>LLMs-based In-text Citation Analysis</p>
<p>While the primary focus of this paper revolves around the realms of Natural Language Processing (NLP) and Machine Learning (ML), it also encompasses discussions that draw from bibliometrics journals, such as Scientometrics, thereby fostering an interdisciplinary dialogue.For instance, consider a bibliometric study within the context of Large Language Models (LLMs).An illustrative example could involve the characterization of citation distribution emanating from GPT-4 or the examination of potential biases in citations within generative LLMs.Notably, tasks like citation sentiment classification serve as essential precursor steps in the analysis undertaken by bibliometric researchers.</p>
<p>Conclusion</p>
<p>This paper pioneers a summary of existing interdisciplinary research on how Large Language Models (LLMs) contribute to citation-related tasks.We engage in a comprehensive study exploring how citation network and citation semantic information enhance LLMs for superior text representation.We then encapsulate the shortcomings of current re-search and put forth several potential research pathways.Our efforts could potentially be beneficial for researchers engaged with LLMs and citationrelated tasks.</p>
<p>Limitation</p>
<p>A constraint of our study is its exclusive focus on primary studies published in English, neglecting significant research on citation functions published in other languages such as Chinese.</p>
<p>Figure 1 :
1
Figure 1: The mutually beneficial relationship between Large Language Models (LLMs) and citations.</p>
<p>Figure 2 :
2
Figure 2: A taxonomy of the research for Large Language Models (LLMs) and citations.</p>
<p>Recommendation.Direct citation recommendation involves picking the bestsuited item from a collection of potential recommendations.By converting unstructured text data into high-dimensional, meaningful vectors, LLMs bolster citation recommendation systems' ability to thoroughly analyze and utilize the inherent substance of scientific papers, thereby enhancing recommendation efficacy.For instance,Jeong et al. (2020) introduces a deep learning model and a wellstructured dataset designed for context-aware paper citation recommendation.Their model, which consists of a document encoder and a context encoder, utilizes both Graph Convolutional Networks layer and BERT to directly predict the label of the recommended citation.The model is trained using cross-entropy as the loss function.In a similar way,Bhowmick et al. (2021) infuse their citation recommendation system with citation context encoded by SciBERT, along with citation history and co-authorship data processed through GCN.This system aims to directly predict the probability of the label tied to the proposed output.Dai et al. (2023) also propose a novel neural network-based approach called Citation Relational BERT with Heterogeneous Deep Graph Convolutional Network (CRB-HDGCN) for the task of inline citation recommendation related to COVID-19.The model leverages a retrained BERT on an augmented citation sentence corpus and HDGCN to extract reliable vectors and subsequently predict potential citations directly.</p>
<p>Figure 4 :
4
Figure 4: Citations are helpful and unique resources in improving and shaping LLMs.They can act as LLMs training objects, as LLMs training data and as LLMs structural inputs.</p>
<p>Figure 4, in this section, we summarize the research progress of leveraging citation linkage data to improve LLMs through different dimensions.According to the usage of citations, we divide recent relevant research into three categories: Citation as LLMs Training Data, Citation as LLMs Training Signals, and Citation as LLMs Structural Inputs.</p>
<p>Table 1 :
1
Previous Surveys of Citation Related Tasks
Article Year Research Method Research DomainReview FocusA survey on the foundations, methodologies, and applications of2014Non-systematicBibliometricscitation content analysis, providing an overview of the current stateof the art in the citation function analysis.This paper presents a summary of current research in Natural2016Non-systematicComputer ScienceLanguage Processing-driven citation analysis, highlighting the experiments and practical examples that demonstrate the significanceof this field.2017Non-systematicComputer Science
https://chat.openai.com/
https://ai.google/discover/palm2</p>
<p>A deep learning approach for contextaware citation recommendation using rhetorical zone classification and similarity to overcome cold-start problem. Muhammad Azeem Abbas, Saheed Ajayi, Muhammad Bilal, Ade Oyegoke, Maruf Pasha, Hafiz Tauqeer, Ali , Journal of Ambient Intelligence and Humanized Computing. 2022</p>
<p>Purpose and polarity of citation: Towards nlpbased bibliometrics. Amjad Abu-Jbara, Jefferson Ezra, Dragomir Radev, Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the North American chapter of the association for computational linguistics: Human language technologies2013</p>
<p>Coherent citation-based summarization of scientific papers. Amjad Abu, -Jbara , Dragomir Radev, Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. the 49th annual meeting of the association for computational linguistics: Human language technologies2011</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019aarXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019b. November 3-7, 2019</p>
<p>Augmenting context-aware citation recommendations with citation and co-authorship history. Anubrata Bhowmick, Ashish Singhal, Shenghui Wang, 18th International Conference on Scientometrics and Informetrics, ISSI 2021. International Society for Scientometrics and Informetrics2021</p>
<p>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, Deb Roy, CoRR, abs/2305.143212023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Understanding the meanings of citations using sentiment, role, and citation function classifications. Indra Budi, Yaniasih Yaniasih, Scientometrics. 12812023</p>
<p>Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers. Xiaoyan Cai, Sen Liu, Libin Yang, Yan Lu, Jintao Zhao, Dinggang Shen, Tianming Liu, Journal of Biomedical Informatics. 1271039992022</p>
<p>Comparative graph-based summarization of scientific papers guided by comparative citations. Jingqiang Chen, Chaoxiang Cai, Xiaorui Jiang, Kejia Chen, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>Structural scaffolds for citation intent classification in scientific publications. Arman Cohan, Waleed Ammar, Madeleine Van Zuylen, Field Cady, arXiv:1904.016082019arXiv preprint</p>
<p>SPECTER: document-level representation learning using citationinformed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Scientific article summarization using citation-context and article's discourse structure. Arman Cohan, Nazli Goharian, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Heterogeneous deep graph convolutional network with citation relational bert for covid-19 inline citation recommendation. Expert Systems with Applications. Tao Dai, Jie Zhao, Dehong Li, Shun Tian, Xiangmo Zhao, Shirui Pan, 2023213118841</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Contentbased citation analysis: The next generation of citation analysis. Ying Ding, Guo Zhang, Tamy Chambers, Min Song, Xiaolong Wang, Chengxiang Zhai, Journal of the association for information science and technology. 6592014</p>
<p>Baco: A background knowledge-and content-based framework for citing sentence generation. Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021</p>
<p>Link mining: a survey. Lise Getoor, Christopher P Diehl, 10.1145/1117454.1117456SIGKDD Explor. 722005</p>
<p>Local citation recommendation with hierarchical-attention text encoder and scibert-based reranking. Nianlong Gu, Yingqiang Gao, Richard Hr Hahnloser, Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022. Stavanger, NorwaySpringer2022a. April 10-14, 2022Proceedings, Part I</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, ACM Trans. Comput. Heal. 31232022b</p>
<p>A hybrid model based on pre-trained language model and graph neural networks for scientific paper recommendation. Suyang Guan, Ting Jiang, 4th International Conference on Machine Learning, Big Data and Business Intelligence (MLB-DBI). 2022. 2022</p>
<p>Inline citation classification using peripheral context and time-evolving augmentation. Priyanshi Gupta, Yash Kumar Atri, Apurva Nagvenkar, Sourish Dasgupta, Tanmoy Chakraborty, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer2023</p>
<p>Survey about citation context analysis: Tasks, techniques, and resources. Myriam Hernndez, - Alvarez, Jos M Gomez, Natural Language Engineering. 2232016</p>
<p>A decade of in-text citation analysis based on natural language processing and machine learning techniques: An overview of empirical studies. Sehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani, Salem Alelyani, Raheel Nawaz, Lutz Bornmann, Scientometrics. 12682021</p>
<p>A context-aware citation recommendation model with bert and graph convolutional networks. Chanwoo Jeong, Sion Jang, Eunjeong Park, Sungchul Choi, Scientometrics. 1242020</p>
<p>Nlp-driven citation analysis for scientometrics. Rahul Jha, Amjad-Abu Jbara, Vahed Qazvinian, Dragomir R Radev, Natural Language Engineering. 2312017</p>
<p>Patton: Language model pretraining on text-rich networks. Wentao Bowen Jin, Yu Zhang, Yu Zhang, Xinyang Meng, Qi Zhang, Jiawei Zhu, Han, CoRR, abs/2305.122682023</p>
<p>Learning neural textual representations for citation recommendation. Thanh Binh, Inigo Kieu, Son Bao Jauregi Unanue, Hieu Pham, Xuan Phan, Massimo Piccardi, 2020 25th International Conference on Pattern Recognition (ICPR). IEEE2021</p>
<p>Multicite: Modeling realistic citations requires moving beyond the single-sentence singlelabel setting. Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie Johnson, David Jurgens, Arman Cohan, Kyle Lo, arXiv:2107.004142021arXiv preprint</p>
<p>The inductive bias of in-context learning: Rethinking pretraining example design. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBart2020</p>
<p>Citationsum: Citation-aware graph contrastive learning for scientific paper summarization. Zheheng Luo, Qianqian Xie, Sophia Ananiadou, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>The classification of citing motivations: a meta-synthesis. Dongqing Lyu, Xuanmin Ruan, Juan Xie, Ying Cheng, Scientometrics. 12642021</p>
<p>Scibert sentence representation for citation context classification. Himanshu Maheshwari, Bhavyajeet Singh, Vasudeva Varma, Proceedings of the Second Workshop on Scholarly Document Processing. the Second Workshop on Scholarly Document Processing2021</p>
<p>Citesum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision. Yuning Mao, Ming Zhong, Jiawei Han, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Train your own GNN teacher: Graph-aware distillation on textual graphs. Costas Mavromatis, N Vassilis, Shen Ioannidis, Da Wang, Soji Zheng, Jun Adeshina, Han Ma, Christos Zhao, George Faloutsos, Karypis, CoRR, abs/2304.106682023</p>
<p>Navigation-based candidate expansion and pretrained language models for citation recommendation. Rodrigo Nogueira, Zhiying Jiang, Kyunghyun Cho, Jimmy Lin, Scientometrics. 1252020</p>
<p>Neighborhood contrastive learning for scientific document representations with citation embeddings. Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>A survey on citation recommendation system. S Reshma, Pillai, Deepthi, 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT). IEEE2022</p>
<p>Scientific paper summarization using citation summary networks. Vahed Qazvinian, Dragomir Radev, Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). the 22nd International Conference on Computational Linguistics (Coling 2008)2008</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Structure and semantics preserving document representations. Natraj Raman, Sameena Shah, Manuela Veloso, SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. Madrid, SpainACM2022. July 11 -15, 2022</p>
<p>Enriching language models with graph-based context information to better understand textual data. Albert Roethel, Maria Ganzha, Anna Wrblewska, 10.48550/arXiv.2305.11070CoRR, abs/2305.110702023</p>
<p>Citation intent classification using word embedding. Muhammad Roman, Abdul Shahid, Shafiullah Khan, Anis Koubaa, Lisu Yu, Ieee Access. 92021</p>
<p>Biocite: A deep learning-based citation linkage framework for biomedical research articles. Sudipta Singha, Roy , Robert E Mercer, Proceedings of the 21st Workshop on Biomedical Language Processing. the 21st Workshop on Biomedical Language Processing2022</p>
<p>Multi-view multi-objective clustering-based framework for scientific document summarization using citation context. Naveen Saini, Saichethan Miriyala Reddy, Sriparna Saha, Jose G Moreno, Antoine Doucet, Applied Intelligence. 2023</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, International Conference on Learning Representations. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason , Alan Fries, Ryan Teehan, Le Teven, Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush2022</p>
<p>Sscibert: a pre-trained language model for social science texts. Si Shen, Jiangfeng Liu, Litao Lin, Ying Huang, Lin Zhang, Chang Liu, Yutong Feng, Dongbo Wang, Scientometrics. 12822023</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, 10.48550/arXiv.2211.13308CoRR, abs/2211.133082022</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Citation context classification: Critical vs non-critical. Sonita Te, Amira Barhoumi, Martin Lentschat, Frdrique Bordignon, Cyril Labb, Franois Portet, Proceedings of the Third Workshop on Scholarly Document Processing. the Third Workshop on Scholarly Document Processing2022</p>
<p>Automatic classification of citation function. Simone Teufel, Advaith Siddharthan, Dan Tidhar, Proceedings of the 2006 conference on empirical methods in natural language processing. the 2006 conference on empirical methods in natural language processing2006</p>
<p>Sentiment and intent classification of in-text citations using bert. Ruan Visser, Marcel Dunaiski, Proceedings of 43rd Conference of the South African Insti. 43rd Conference of the South African Insti202285</p>
<p>Content-based hybrid deep neural network citation recommendation method. Leipeng Wang, Yuan Rao, Qinyu Bian, Shuo Wang, Data Science: 6th International Conference of Pioneering Computer Scientists, Engineers and Educators. Taiyuan, ChinaSpringer2020a. September 18-21, 20202020Proceedings, Part II 6</p>
<p>Text embeddings by weakly-supervised contrastive pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, CoRR, abs/2212.035332022a</p>
<p>Disencite: Graph-based disentangled representation learning for context-specific citation generation. Yifan Wang, Yiping Song, Shuai Li, Chaoran Cheng, Wei Ju, Ming Zhang, Sheng Wang, Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event. AAAI Press2022b. February 22 -March 1, 2022</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022c</p>
<p>Gbdt and bert: a hybrid solution for recognizing citation intent. Zhe Wang, Rundong Shi, Shijie Li, Peng Yan, Studies. 552020b</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Graphformers: Gnn-nested transformers for representation learning on textual graph. Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, Xing Xie, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems. NeurIPS2021. 2021. 2021. December 6-14, 2021</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Linkbert: Pretraining language models with document links. Michihiro Yasunaga, Jure Leskovec, Percy Liang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLR2020</p>
<p>Mp-bert4rec: Recommending multiple positive citations for academic manuscripts via content-dependent bert and multipositive triplet. Yang Zhang, Qiang Ma, IEICE TRANSACTIONS on Information and Systems. 105112022</p>
<p>Tdm-cfc: Towards document-level multi-label citation function classification. Yang Zhang, Yufei Wang, Z Quan, Adnan Sheng, Wei Emma Mahmood, Rongying Zhang, Zhao, International Conference on Web Information Systems Engineering. Springer2021</p>
<p>Hybrid data augmentation for citation function classification. Yang Zhang, Yufei Wang, Z Quan, Adnan Sheng, Wei Mahmood, Rongying Zhang, Zhao, IJCNN 2023: International Joint Conference on Neural Networks. IEEE2023a</p>
<p>Towards employing native information in citation function classification. Yang Zhang, Rongying Zhao, Yufei Wang, Haihua Chen, Adnan Mahmood, Munazza Zaib, Wei Emma Zhang, Quan Z Sheng, Scientometrics. 2022</p>
<p>Pre-training multi-task contrastive learning models for scientific literature understanding. Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, Jianfeng Gao, 10.48550/arXiv.2305.14232CoRR, abs/2305.142322023b</p>            </div>
        </div>

    </div>
</body>
</html>
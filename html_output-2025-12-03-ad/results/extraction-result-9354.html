<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9354 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9354</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9354</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-45b7c7448768b51b6dbd9b76495c9cd9d110bd91</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/45b7c7448768b51b6dbd9b76495c9cd9d110bd91" target="_blank">ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work calls for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation, and presents ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery.</p>
                <p><strong>Paper Abstract:</strong> The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9354.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9354.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model (Anthropic) evaluated in this work as the best-performing backbone LLM for program-generation language agents across four scientific subdomains; tested under direct prompting, OpenHands CodeAct, and self-debug frameworks to generate self-contained Python programs that perform data-driven scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Anthropic LLM (version named in paper). Specific parameter count and pretraining corpus not provided in this paper; described as trained for strong code-generation and instruction following by Anthropic (reference in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate self-contained Python programs that load heterogeneous scientific datasets, process data, build models or visualizations, and save outputs for domain-specific analysis (e.g., training toxicity prediction models, molecule analyses, map visualizations, EEG time-series analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Success Rate (SR) — task success per executable success criteria; Valid Execution Rate (VER); CodeBERTScore (CBS) measuring similarity to gold program; API Cost (USD); rubric-based human ratings; GPT-4o judge for figure quality where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Best reported (self-debug, with expert-provided knowledge): SR = 34.3%, CBS = 87.1, VER = 86.3%, Cost ≈ $0.061 per task. Without expert knowledge (self-debug): SR = 32.4%, CBS = 86.4, VER = 92.2%, Cost ≈ $0.057 per task. (Values from Table 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Agent framework (self-debug > direct prompting; CodeAct sometimes worse), availability of expert-provided knowledge (can improve SR and CBS but sometimes reduces VER), model's code-generation specialization, ability to execute and use execution feedback, dataset heterogeneity and task complexity (longer gold programs correlated with failure), difficulty using discipline-specific tools/APIs, hallucinated API usage when provided unfamiliar tool suggestions, data contamination and shortcuts (mitigated by dataset modification).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other LLMs in same experimental protocols (Llama-3.1-Instruct-70B/405B, Mistral-Large-2, GPT-4o, OpenAI o1-preview) and across three agent frameworks (Direct Prompting, OpenHands CodeAct, Self-Debug). Claude-3.5-Sonnet with self-debug achieves the best SR among non-o1 models in this study (34.3% with knowledge), outperforming Llama and Mistral under same framework while costing modestly more than some open-weight LLMs but far less than OpenAI o1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails frequently on complex tasks requiring extensive data processing and model development (Bioinformatics & Computational Chemistry); struggles with discipline-specific libraries (e.g., Geopandas, Biopsykit) leading to incorrect or hallucinated API usage; performance sensitive to provided knowledge (can lower VER if specified tools are unfamiliar); many successful tasks are skewed to simpler programs (< mean gold program length); cannot fully automate end-to-end discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Execution feedback (self-debug) substantially improves program quality and task success; agent designs must weigh cost vs. capability (Claude self-debug was more cost-effective than some tool-rich frameworks); expert-provided knowledge helps but agents need better abilities to leverage it without introducing tool-usage errors; improving LLMs' abilities to process heterogeneous scientific data and to correctly use domain-specific tools/APIs will be important next steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9354.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9354.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-inference-cost OpenAI model variant evaluated as a reference in this benchmark; it generates additional reasoning tokens (extra inference-time compute) and attains higher task success rates at much higher API cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI model (o1-preview) that, as deployed in experiments, generates additional internal reasoning tokens for extended inference compute; not fully comparable due to different hyperparameters and inability to be used with OpenHands in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same program-generation/data-analysis tasks as other LLMs: produce runnable Python programs for dataset loading, processing, modeling/visualization, and saving outputs across the four scientific subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same metrics: SR, VER, CBS, API Cost; figure judged by GPT-4o where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported best (self-debug without knowledge): SR = 42.2%, CBS = 88.4, VER = 92.2%, Cost ≈ $0.636 per task. With knowledge (self-debug): SR = 41.2%, CBS = 88.9, VER = 91.2%, Cost ≈ $0.713 per task. (Table 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Extra inference-time compute / internal reasoning token generation improved performance; framework choice (self-debug helpful); cost budget (much higher cost correlated with higher SR); task complexity and domain heterogeneity still limit success.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other LLMs in the paper, o1 achieves the highest SR but at >10x API cost relative to other evaluated LLMs; authors caution direct comparison due to different inference-time settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher performance comes with substantially higher monetary cost; model was not evaluated in all frameworks (incompatible with OpenHands in this study), so tool-use advantages/limitations less explored; still fails majority of tasks (SR ~42% means >50% failure).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Increasing inference-time compute can boost performance but substantially raises cost; practical agent design should trade off cost vs. marginal performance gains; do not assume high SR equates to end-to-end automation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9354.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9354.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI model used in the benchmark to generate code for scientific data-driven tasks and to judge figure quality (GPT-4o as judge); evaluated under direct prompting, CodeAct, and self-debug frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o variant (version dated in paper). Exact parameterization not provided here; used both as a generation backbone and as a figure-quality judge in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate runnable Python programs that implement data loading, processing, modeling, and visualization for domain-specific scientific tasks; also used to evaluate figure outputs (GPT-4o judge).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>SR, VER, CBS, Cost; for figures GPT-4o judge scores averaged over 3 responses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Best reported (OpenHands CodeAct, with knowledge): SR = 27.5% (direct prompting SR=11.8%, self-debug SR=22.6% without knowledge; with knowledge self-debug SR=23.5%); self-debug with knowledge SR=23.5%, CBS ≈ 85.6, VER ≈ 71.6%, Cost ≈ $0.046 (self-debug with knowledge). (Table 3 values vary by framework.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Agent framework influences performance (OpenHands CodeAct allowed GPT-4o to leverage tools better than other LLMs), ability to use web browser in CodeAct increases effectiveness, execution feedback (self-debug) improves results, dataset heterogeneity and domain-specific tool familiarity affect outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with Claude, Mistral, Llama variants and OpenAI o1; GPT-4o performs well in CodeAct due to better tool usage, but is outperformed by o1 in raw SR and by Claude in cost-effective self-debug performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sensitive to agent framework; performs worse when restricted from tool-rich environments (self-debug), and still fails many complex tasks requiring domain-specific processing; prone to implementation-level errors in generated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Tool-enabled frameworks (OpenHands CodeAct) can magnify model strengths if the model effectively uses tools (e.g., web browser); combining tool-usage capability with execution feedback is promising but requires models that reliably handle tool APIs and bash operations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9354.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9354.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Large-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-Large-2 (MistralAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight LLM (reported 123B in MistralAI materials) evaluated for code-generation across scientific program tasks; shows competitive performance in some frameworks, particularly self-debug.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Large-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLM (referred to as Mistral-Large-2 in paper, marketed as strong for code generation). Paper lists model name and cites MistralAI announcement; exact training details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate Python programs for heterogeneous scientific data tasks (data loading, processing, modeling, visualization) across four disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>SR, VER, CBS, Cost; rubric-based human evaluation for detailed stages.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Best reported (self-debug, with knowledge): SR = 27.5%, CBS = 86.8, VER = 78.4%, Cost ≈ $0.036 per task. Without knowledge (self-debug): SR = 23.5%, CBS = 85.1, VER = 83.3%, Cost ≈ $0.034. (Table 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Framework (self-debug beneficial), presence of expert-provided knowledge (improves SR/CBS), model's code-generation training strength, dataset heterogeneity and task complexity, ability to use domain-specific tools in CodeAct was limited compared to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other open-weight LLMs (Llama-3.1 variants) and proprietary models (GPT-4o, Claude), Mistral performs strongly under self-debug but is outperformed by the best proprietary models (o1, Claude) in absolute SR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with discipline-specific tools and complex data-processing tasks; still fails majority of tasks; when using CodeAct, had limited ability to properly use some bash/tool actions compared to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Self-debug is an effective and cost-efficient framework for models like Mistral; improving tool-API handling and domain-specific library knowledge would likely yield larger gains than simply scaling inference-time compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9354.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9354.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-Instruct (70B and 405B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-weight Llama-3.1 Instruct variants evaluated (70B and 405B) as backbones for program-generation agents; evaluated across frameworks showing modest success relative to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-Instruct (70B, 405B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight Llama-3.1 instruct-family models (two sizes studied: 70B and 405B) referenced in paper (Dubey et al., 2024). Paper does not provide training corpus details beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same program-generation/data-analysis tasks: produce runnable Python programs to carry out domain-specific data analysis, modeling, and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>SR, VER, CBS, Cost; rubric-based human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Best reported (self-debug, with knowledge): for 70B SR = 16.7%, CBS = 83.4, VER = 73.5%, Cost ≈ $0.008; for 405B self-debug with knowledge SR ≈ 13.7%, CBS = 83.6, VER = 79.4%, Cost ≈ $0.055. (Table 3 provides per-framework values; direct prompting and CodeAct values are lower.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale (405B sometimes better VER but not necessarily higher SR), framework (self-debug helps), availability of expert knowledge (mixed effects), ability to use complex tooling in CodeAct (some struggle), dataset heterogeneity and program complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to larger or more code-focused models (Mistral, GPT-4o, Claude, o1), Llama variants underperform in SR; self-debug improves results but not to the level of top proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower SR relative to other evaluated LLMs; struggle particularly with CodeAct tool usage (bash, file edits) and with complex domain-specific processing; when given expert knowledge, sometimes produced programs using unfamiliar specified tools and introduced execution errors.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Self-debug substantially helps even for these models; improving robustness in tool-API usage and better domain-specific library knowledge would likely improve task success more than naive scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Executable code actions elicit better LLM agents <em>(Rating: 1)</em></li>
                <li>Opendevin: An open platform for ai software developers as generalist agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9354",
    "paper_id": "paper-45b7c7448768b51b6dbd9b76495c9cd9d110bd91",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Claude-3.5-Sonnet (Anthropic)",
            "brief_description": "A proprietary large language model (Anthropic) evaluated in this work as the best-performing backbone LLM for program-generation language agents across four scientific subdomains; tested under direct prompting, OpenHands CodeAct, and self-debug frameworks to generate self-contained Python programs that perform data-driven scientific tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet",
            "model_description": "Proprietary Anthropic LLM (version named in paper). Specific parameter count and pretraining corpus not provided in this paper; described as trained for strong code-generation and instruction following by Anthropic (reference in paper).",
            "scientific_subdomain": "Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience",
            "simulation_task": "Generate self-contained Python programs that load heterogeneous scientific datasets, process data, build models or visualizations, and save outputs for domain-specific analysis (e.g., training toxicity prediction models, molecule analyses, map visualizations, EEG time-series analysis).",
            "evaluation_metric": "Success Rate (SR) — task success per executable success criteria; Valid Execution Rate (VER); CodeBERTScore (CBS) measuring similarity to gold program; API Cost (USD); rubric-based human ratings; GPT-4o judge for figure quality where applicable.",
            "simulation_accuracy": "Best reported (self-debug, with expert-provided knowledge): SR = 34.3%, CBS = 87.1, VER = 86.3%, Cost ≈ $0.061 per task. Without expert knowledge (self-debug): SR = 32.4%, CBS = 86.4, VER = 92.2%, Cost ≈ $0.057 per task. (Values from Table 3 of the paper.)",
            "factors_affecting_accuracy": "Agent framework (self-debug &gt; direct prompting; CodeAct sometimes worse), availability of expert-provided knowledge (can improve SR and CBS but sometimes reduces VER), model's code-generation specialization, ability to execute and use execution feedback, dataset heterogeneity and task complexity (longer gold programs correlated with failure), difficulty using discipline-specific tools/APIs, hallucinated API usage when provided unfamiliar tool suggestions, data contamination and shortcuts (mitigated by dataset modification).",
            "comparison_baseline": "Compared against other LLMs in same experimental protocols (Llama-3.1-Instruct-70B/405B, Mistral-Large-2, GPT-4o, OpenAI o1-preview) and across three agent frameworks (Direct Prompting, OpenHands CodeAct, Self-Debug). Claude-3.5-Sonnet with self-debug achieves the best SR among non-o1 models in this study (34.3% with knowledge), outperforming Llama and Mistral under same framework while costing modestly more than some open-weight LLMs but far less than OpenAI o1.",
            "limitations_or_failure_cases": "Fails frequently on complex tasks requiring extensive data processing and model development (Bioinformatics & Computational Chemistry); struggles with discipline-specific libraries (e.g., Geopandas, Biopsykit) leading to incorrect or hallucinated API usage; performance sensitive to provided knowledge (can lower VER if specified tools are unfamiliar); many successful tasks are skewed to simpler programs (&lt; mean gold program length); cannot fully automate end-to-end discovery workflows.",
            "author_recommendations_or_insights": "Execution feedback (self-debug) substantially improves program quality and task success; agent designs must weigh cost vs. capability (Claude self-debug was more cost-effective than some tool-rich frameworks); expert-provided knowledge helps but agents need better abilities to leverage it without introducing tool-usage errors; improving LLMs' abilities to process heterogeneous scientific data and to correctly use domain-specific tools/APIs will be important next steps.",
            "uuid": "e9354.0",
            "source_info": {
                "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "OpenAI o1-preview",
            "name_full": "OpenAI o1-preview",
            "brief_description": "A high-inference-cost OpenAI model variant evaluated as a reference in this benchmark; it generates additional reasoning tokens (extra inference-time compute) and attains higher task success rates at much higher API cost.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-preview",
            "model_description": "OpenAI model (o1-preview) that, as deployed in experiments, generates additional internal reasoning tokens for extended inference compute; not fully comparable due to different hyperparameters and inability to be used with OpenHands in this study.",
            "scientific_subdomain": "Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience",
            "simulation_task": "Same program-generation/data-analysis tasks as other LLMs: produce runnable Python programs for dataset loading, processing, modeling/visualization, and saving outputs across the four scientific subdomains.",
            "evaluation_metric": "Same metrics: SR, VER, CBS, API Cost; figure judged by GPT-4o where applicable.",
            "simulation_accuracy": "Reported best (self-debug without knowledge): SR = 42.2%, CBS = 88.4, VER = 92.2%, Cost ≈ $0.636 per task. With knowledge (self-debug): SR = 41.2%, CBS = 88.9, VER = 91.2%, Cost ≈ $0.713 per task. (Table 3.)",
            "factors_affecting_accuracy": "Extra inference-time compute / internal reasoning token generation improved performance; framework choice (self-debug helpful); cost budget (much higher cost correlated with higher SR); task complexity and domain heterogeneity still limit success.",
            "comparison_baseline": "Compared to other LLMs in the paper, o1 achieves the highest SR but at &gt;10x API cost relative to other evaluated LLMs; authors caution direct comparison due to different inference-time settings.",
            "limitations_or_failure_cases": "Higher performance comes with substantially higher monetary cost; model was not evaluated in all frameworks (incompatible with OpenHands in this study), so tool-use advantages/limitations less explored; still fails majority of tasks (SR ~42% means &gt;50% failure).",
            "author_recommendations_or_insights": "Increasing inference-time compute can boost performance but substantially raises cost; practical agent design should trade off cost vs. marginal performance gains; do not assume high SR equates to end-to-end automation.",
            "uuid": "e9354.1",
            "source_info": {
                "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "An OpenAI model used in the benchmark to generate code for scientific data-driven tasks and to judge figure quality (GPT-4o as judge); evaluated under direct prompting, CodeAct, and self-debug frameworks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI GPT-4o variant (version dated in paper). Exact parameterization not provided here; used both as a generation backbone and as a figure-quality judge in evaluations.",
            "scientific_subdomain": "Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience",
            "simulation_task": "Generate runnable Python programs that implement data loading, processing, modeling, and visualization for domain-specific scientific tasks; also used to evaluate figure outputs (GPT-4o judge).",
            "evaluation_metric": "SR, VER, CBS, Cost; for figures GPT-4o judge scores averaged over 3 responses.",
            "simulation_accuracy": "Best reported (OpenHands CodeAct, with knowledge): SR = 27.5% (direct prompting SR=11.8%, self-debug SR=22.6% without knowledge; with knowledge self-debug SR=23.5%); self-debug with knowledge SR=23.5%, CBS ≈ 85.6, VER ≈ 71.6%, Cost ≈ $0.046 (self-debug with knowledge). (Table 3 values vary by framework.)",
            "factors_affecting_accuracy": "Agent framework influences performance (OpenHands CodeAct allowed GPT-4o to leverage tools better than other LLMs), ability to use web browser in CodeAct increases effectiveness, execution feedback (self-debug) improves results, dataset heterogeneity and domain-specific tool familiarity affect outcome.",
            "comparison_baseline": "Compared with Claude, Mistral, Llama variants and OpenAI o1; GPT-4o performs well in CodeAct due to better tool usage, but is outperformed by o1 in raw SR and by Claude in cost-effective self-debug performance.",
            "limitations_or_failure_cases": "Sensitive to agent framework; performs worse when restricted from tool-rich environments (self-debug), and still fails many complex tasks requiring domain-specific processing; prone to implementation-level errors in generated programs.",
            "author_recommendations_or_insights": "Tool-enabled frameworks (OpenHands CodeAct) can magnify model strengths if the model effectively uses tools (e.g., web browser); combining tool-usage capability with execution feedback is promising but requires models that reliably handle tool APIs and bash operations.",
            "uuid": "e9354.2",
            "source_info": {
                "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mistral-Large-2",
            "name_full": "Mistral-Large-2 (MistralAI)",
            "brief_description": "An open-weight LLM (reported 123B in MistralAI materials) evaluated for code-generation across scientific program tasks; shows competitive performance in some frameworks, particularly self-debug.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-Large-2",
            "model_description": "Open-weight LLM (referred to as Mistral-Large-2 in paper, marketed as strong for code generation). Paper lists model name and cites MistralAI announcement; exact training details not provided in this paper.",
            "scientific_subdomain": "Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience",
            "simulation_task": "Generate Python programs for heterogeneous scientific data tasks (data loading, processing, modeling, visualization) across four disciplines.",
            "evaluation_metric": "SR, VER, CBS, Cost; rubric-based human evaluation for detailed stages.",
            "simulation_accuracy": "Best reported (self-debug, with knowledge): SR = 27.5%, CBS = 86.8, VER = 78.4%, Cost ≈ $0.036 per task. Without knowledge (self-debug): SR = 23.5%, CBS = 85.1, VER = 83.3%, Cost ≈ $0.034. (Table 3.)",
            "factors_affecting_accuracy": "Framework (self-debug beneficial), presence of expert-provided knowledge (improves SR/CBS), model's code-generation training strength, dataset heterogeneity and task complexity, ability to use domain-specific tools in CodeAct was limited compared to GPT-4o.",
            "comparison_baseline": "Compared to other open-weight LLMs (Llama-3.1 variants) and proprietary models (GPT-4o, Claude), Mistral performs strongly under self-debug but is outperformed by the best proprietary models (o1, Claude) in absolute SR.",
            "limitations_or_failure_cases": "Struggles with discipline-specific tools and complex data-processing tasks; still fails majority of tasks; when using CodeAct, had limited ability to properly use some bash/tool actions compared to GPT-4o.",
            "author_recommendations_or_insights": "Self-debug is an effective and cost-efficient framework for models like Mistral; improving tool-API handling and domain-specific library knowledge would likely yield larger gains than simply scaling inference-time compute.",
            "uuid": "e9354.3",
            "source_info": {
                "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3.1-Instruct",
            "name_full": "Llama-3.1-Instruct (70B and 405B)",
            "brief_description": "Open-weight Llama-3.1 Instruct variants evaluated (70B and 405B) as backbones for program-generation agents; evaluated across frameworks showing modest success relative to other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-Instruct (70B, 405B)",
            "model_description": "Open-weight Llama-3.1 instruct-family models (two sizes studied: 70B and 405B) referenced in paper (Dubey et al., 2024). Paper does not provide training corpus details beyond citation.",
            "scientific_subdomain": "Bioinformatics; Computational Chemistry; Geographical Information Science; Psychology & Cognitive Neuroscience",
            "simulation_task": "Same program-generation/data-analysis tasks: produce runnable Python programs to carry out domain-specific data analysis, modeling, and visualization.",
            "evaluation_metric": "SR, VER, CBS, Cost; rubric-based human evaluation.",
            "simulation_accuracy": "Best reported (self-debug, with knowledge): for 70B SR = 16.7%, CBS = 83.4, VER = 73.5%, Cost ≈ $0.008; for 405B self-debug with knowledge SR ≈ 13.7%, CBS = 83.6, VER = 79.4%, Cost ≈ $0.055. (Table 3 provides per-framework values; direct prompting and CodeAct values are lower.)",
            "factors_affecting_accuracy": "Model scale (405B sometimes better VER but not necessarily higher SR), framework (self-debug helps), availability of expert knowledge (mixed effects), ability to use complex tooling in CodeAct (some struggle), dataset heterogeneity and program complexity.",
            "comparison_baseline": "Compared to larger or more code-focused models (Mistral, GPT-4o, Claude, o1), Llama variants underperform in SR; self-debug improves results but not to the level of top proprietary models.",
            "limitations_or_failure_cases": "Lower SR relative to other evaluated LLMs; struggle particularly with CodeAct tool usage (bash, file edits) and with complex domain-specific processing; when given expert knowledge, sometimes produced programs using unfamiliar specified tools and introduced execution errors.",
            "author_recommendations_or_insights": "Self-debug substantially helps even for these models; improving robustness in tool-API usage and better domain-specific library knowledge would likely improve task success more than naive scaling.",
            "uuid": "e9354.4",
            "source_info": {
                "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Executable code actions elicit better LLM agents",
            "rating": 1
        },
        {
            "paper_title": "Opendevin: An open platform for ai software developers as generalist agents",
            "rating": 1
        }
    ],
    "cost": 0.016614749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SCIENCEAGENTBENCH: <br> Toward Rigorous AsSESSMENT of LANGUAGE AGENTS FOR DATA-DRIVEN SCIENTIFIC DISCOVERY</h1>
<p>Ziru Chen ${ }^{1 <em>}$, Shijie Chen ${ }^{1 </em>}$, Yuting Ning ${ }^{1}$, Qianheng Zhang ${ }^{3}$, Boshi Wang ${ }^{1}$, Botao Yu ${ }^{1}$, Yifei $\mathbf{L i}^{1}$, Zeyi Liao ${ }^{1}$, Chen Wei ${ }^{3}$, Zitong $\mathbf{L u}^{3}$, Vishai Dey ${ }^{1}$, Mingyi Xue ${ }^{5}$, Frazier N. Baker ${ }^{1,6}$, Benjamin Burns ${ }^{1}$, Daniel Adu-Ampratwum ${ }^{2}$, Xuhui Huang ${ }^{5}$, Xia Ning ${ }^{1,2,6}$, Song Gao ${ }^{3}$, Yu Su ${ }^{1}$, Huan Sun ${ }^{1 *}$<br>${ }^{1}$ Department of Computer Science and Engineering, OSU ${ }^{2}$ College of Pharmacy, OSU<br>${ }^{3}$ Department of Geography, UW-Madison ${ }^{4}$ Department of Psychology, OSU<br>${ }^{5}$ Department of Chemistry, UW-Madison ${ }^{6}$ Department of Biomedical Informatics, OSU<br>Website: https://osu-nlp-group.github.io/ScienceAgentBench/</p>
<h4>Abstract</h4>
<p>The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peerreviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve $32.4 \%$ of the tasks independently and $34.3 \%$ with expert-provided knowledge. In addition, we evaluate OpenAI o1preview with direct prompting and self-debug, which can boost the performance to $42.2 \%$, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have shown remarkable capabilities beyond text generation, including reasoning (Wei et al., 2022; Yao et al., 2023), tool learning (Schick et al., 2023; Wang et al., 2024a), and code generation (Chen et al., 2021; Yang et al., 2024a). These abilities have piqued significant research interests in developing LLM-based language agents to automate scientific discovery end-to-end. For instance, Majumder et al. (2024a) urge the community to build automated systems for end-to-end data-driven discovery, an increasingly important workflow in many disciplines (Hey et al., 2009) that leverages existing datasets to derive new findings. More recently, Lu et al. (2024) claim to have built The AI Scientist, an agent that is capable of automating the entire re-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top: Distribution of sub-tasks in ScienceAgentBench. Each task in our benchmark consists of one or more of these sub-tasks and requires successful completion of all sub-tasks to achieve the task goal. Bottom: Heterogeneous datasets involved: (a) a cell image in Bioinformatics, (b) a molecular activity visualization in Computational Chemistry, (c) a flooding risk map in Geographical Information Science, and (d) an EEG time series in Psychology and Cognitive Neuroscience.</p>
<p>search workflow, from generating ideas to running experiments and writing papers. This ambitious claim has sparked both excitement and skepticism about the true capabilities of such agents.</p>
<p>In this work, we contend that for a language agent to fully automate data-driven discovery, it must be able to complete all essential tasks in the workflow, such as model development, data analysis, and visualization. Thus, we advocate careful evaluations of the agents' performance on these tasks, before claiming they can automate data-driven discovery end-to-end. Such an assessment strategy helps grasp a more solid understanding of an agent's strengths and limitations than purely relying on end-to-end evaluations, e.g., using an LLM-based reviewer to assess generated papers (Lu et al., 2024). Yet, high-quality benchmarks focusing on individual tasks in real-world scientific workflows are lacking for objective assessment and continued development of agents for data-driven discovery.</p>
<p>To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven discovery. The construction of ScienceAgentBench follows three key design principles. (1) Scientific authenticity through co-design with subject matter experts: We ensure the authenticity of tasks in our benchmark by directly extracting them from peer-reviewed publications and engaging nine subject matter experts (incl. senior Ph.D. students and professors) from the respective disciplines to validate them. This approach also minimizes the generalization gap for agents developed on our benchmark to real-world scenarios. In total, we curate 102 diverse tasks from 44 peer-reviewed publications in four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology &amp; Cognitive Neuroscience (Figure 1). (2) Rigorous graded evaluation: Reliable evaluation for language agents is notably difficult due to the open-endedness and complexity of data-driven discovery tasks. We first unify the target output for every task as a self-contained Python program, and then employ an array of evaluation metrics that examine the generated programs, execution results (e.g., rendered figures or test set predictions), and costs. We also provide step-by-step rubrics specific to each task to enable graded evaluation. (3) Careful multi-stage quality control: Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns due to LLM pre-training.</p>
<p>We comprehensively evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct (Wang et al., 2024c), and self-debug. In addition, we evaluate OpenAI o1 with direct prompting and self-debug. Since OpenAI o1 generates additional reasoning tokens for extensive inference-time compute, we only report its performances as references and focus on analyzing other LLMs: Surprisingly, without expert-provided knowledge, Claude-3.5-Sonnet using self-debug can successfully solve 10.8% more tasks than using OpenHands CodeAct while costing 17 times less API fees. This result resonates with recent findings that agent designs should jointly consider costs and performance to maximize their practical utility (Kapoor et al., 2024). Still, given three attempts for each task, the best agent can only solve 32.4% of the tasks independently and 34.3% of them with expert-provided knowledge. These results also suggest lan-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example Computational Chemistry task in ScienceAgentBench with four components.
guage agents cannot yet automate essential tasks in data-driven discovery nor the research pipelines end-to-end, in contrast to claims in recent work such as Lu et al. (2024).</p>
<p>Despite their current mediocre performance, we believe language agents hold significant potential in augmenting human scientists' productivity: For each task in our benchmark, it takes a trained annotator at least 2.5-3 hours on average to adapt an existing program from public sources, and potentially much longer for a subject matter scientist to write the program from scratch. In contrast, a language agent can usually generate a meaningful program draft within 10 minutes. In the long run, ScienceAgentBench will serve as a benchmark for rigorously measuring progress toward developing language agents to assist scientists in data-driven scientific discovery.</p>
<h1>2 SCIENCEAGENTBENCH</h1>
<p>In this section, we introduce ScienceAgentBench, which aims to evaluate agents on essential tasks in a data-driven discovery workflow. Before automating the entire workflow end-to-end, we envision language agents to first serve as science co-pilots that can write code to process, analyze, and visualize data. Similar to co-pilots for software development, we target scientist users who might know how to write such code but want to save hours of programming effort with language agents. Hence, we formulate each task as a code generation problem, whose output is easily verifiable and directly usable by a scientist without additional modification efforts.</p>
<h3>2.1 Problem Formulation</h3>
<p>Given a natural language instruction, a dataset, and some optional expert-provided knowledge, an agent shall generate a program to complete the assigned task and save it to Python source code file. Each instance in our benchmark contains four components (Figure 2):
(a) Task Instruction, which describes the goal of an essential task in data-driven discovery and its output requirements. To resemble real-world settings, we keep the instructions concise and avoid unnecessary details when describing task goals. This setup also retains the open-endedness of datadriven discovery and encourages the development of practical agents that do not rely on prescriptive directions from scientists. We provide example task instructions in Appendix C for each discipline.
(b) Dataset Information, which contains the dataset's directory structure and a preview of its content. For agents without file navigation tools, they need such information to correctly use the dataset</p>
<p>in their generated programs. For agents that can navigate file systems, it also helps them save a few turns of interactions to read datasets from the programming environment.
(c) Expert-Provided Knowledge, which includes explanations for scientific terms, formulas to conduct analysis, and example usages of programming tools. These pieces of knowledge are provided by subject matter experts, including senior Ph.D. students and professors, and are optional inputs to an agent. In Section 4, we show that while with such information, language agents' knowledge gap in involved disciplines can be mitigated to some extent, they still fall short utilizing it effectively.
(d) Annotated Program, which is adapted from an open-source code repository released by a peerreviewed scientific publication. As shown in Figure 2, each program is self-contained with package imports, function and class implementations, and a main procedure to carry out the task. An agent is expected to produce similar programs that can be executed independently, e.g. by a Python interpreter, but not necessarily using the same tools as those in the annotated programs.</p>
<h1>2.2 Data Collection</h1>
<p>Task Annotation. We start by forming a group of nine graduate students to annotate the tasks in four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology \&amp; Cognitive Neuroscience. Within each discipline, we search for peer-reviewed publications that release their code and data under permissive licenses (Appendix J). Then, we follow five steps to annotate each task: (1) Identify a reasonably documented code example that is selfcontained and convert it into a task in our benchmark. (2) Collect and preprocess datasets used in the code. (3) Annotate the reference program by revising the referred code to analyze datasets in our benchmark. (4) Implement task-specific success criteria as an executable script and use GPT-4o to draft fine-grained rubrics for evaluation. (5) Write the instruction and dataset information for this task. We gathered 110 tasks initially but discarded four because their programs require long execution time or nontrival environment setup. This leaves us with 106 tasks for validation.</p>
<p>Data Contamination and Shortcut Mitigation. In our preliminary studies, we have noticed that some agents, such as OpenHands, may take shortcuts to solve a task. For example, when asked to develop a machine learning model, they may directly read and report the ground-truth labels in the test set without writing the training code. Such perfect results are actually cheating and will hurt evaluation validity. In addition, because datasets and programs in our benchmark are open-sourced, they are subject to data contamination in LLM training. To mitigate these issues, we devise two strategies to modify the datasets: (1) For each dataset, we randomly remove five data points from its test set. If an LLM-generated program uses automatic data loaders that appeared in the training corpora, it will produce results misaligned to our setup and fail the success criteria. In some cases, we have to skip this step if it would break the completeness of a dataset, e.g., if it results in an incomplete geographical map. (2) For tasks involving model development, we re-split the dataset, keep the test set labels only for evaluation, and replace them with dummy values, such as -1 for classification tasks. These two strategies effectively mitigate data contamination and agent shortcut concerns by failing agents that recite memorized code or attempt to directly report test set labels. See Appendix F.2: Example F. 4 for a case study.</p>
<p>Expert Validation. We engage nine subject matter experts, including senior Ph.D. students and professors from the four involved disciplines, to validate each task and provide additional knowledge. For each task, we present to experts with its instruction, dataset information, annotated program, and task rubrics. The experts are asked to validate the tasks by completing a questionnaire (Appendix G), which can be summarized as four steps: (1) Validate if an annotated task represents a realistic task in their data-driven discovery workflow. (2) Review whether a task instruction gives an accurate high-level description of the program and uses professional languages in their disciplines. (3) Provide up to three pieces of knowledge that might be needed for solving each task. (4) Make necessary revisions to the rubrics for grading the program. Then, following the experts' feedback, we revise 41 task instructions and remove three tasks that are not representative enough for scientific workflows in their disciplines. With 103 tasks remaining, our publication-oriented annotation strategy is shown to be effective in collecting real-world tasks.</p>
<p>Annotator Verification. To ensure data quality, we work with the nine annotators for another round of task verification. We ask the annotators to verify tasks that are not composed by themselves and execute programs to reproduce the results. During this process, we refine 29 task annotations and</p>
<p>Table 1: Representative examples of task-specific success criteria in ScienceAgentBench. To keep the table concise, we omit output requirements in the task instructions and show the task goals.</p>
<table>
<thead>
<tr>
<th>Task Instruction</th>
<th>Subtasks</th>
<th>Success Criteria</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train a multitask model on the Clintox dataset to predict a drug’s toxicity and FDA approval status.</td>
<td>Feature Engineering Deep Learning</td>
<td>The trained model gets $\geq 0.77$ ROC-AUC score on the test set.</td>
</tr>
<tr>
<td>Develop a drug-target interaction model with the DAVIS dataset to repurpose the antiviral drugs for COVID.</td>
<td>Feature Engineering Deep Learning</td>
<td>The top-5 repurposed drugs match the gold top-5 drugs.</td>
</tr>
<tr>
<td>Analyze the inertial measurement unit (IMU) data collected during sleep and compute sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping.</td>
<td>Computational Analysis</td>
<td>Each computed endpoint is close (math.isclose in Python) to the corresponding gold answer.</td>
</tr>
<tr>
<td>Analyze Toronto fire stations and their service coverage. Visualize the results to identify coverage gaps.</td>
<td>Map Visualization</td>
<td>The resulting figure gets $\geq 60$ score by the GPT-4o Judge.</td>
</tr>
</tbody>
</table>
<p>discard one more task whose result is hard to replicate with the same program due to randomness. We finalize ScienceAgentBench with 102 high-quality tasks for data-driven scientific discovery.</p>
<h1>2.3 Evaluation</h1>
<p>While it is a preferable feature, the open-endedness of tasks in our benchmark introduces a crucial evaluation challenge. Specifically, our evaluation strategy has to accommodate diverse setup requirements of programs generated by different agents. To address this challenge, we implement a pipeline to set up a conda environment flexibly for any program. Before evaluation, the conda environment is initialized with seven basic Python packages: numpy, pandas, matplotlib, pytorch, tensorflow, rdkit, and tf_keras. To evaluate each program, we first use pipreqs to analyze it and generate a file listing all packages used. Then, according to the file, we use pip-tools ${ }^{2}$ and handcrafted rules to update the conda environment and properly configure the packages. We execute each program in the customized environment and calculate the evaluation metrics.</p>
<p>Program Evaluation. We comprehensively evaluate each generated program with four metrics. (1) Valid Execution Rate (VER) checks if the program can execute without errors and save its output with the correct file name. (2) Success Rate (SR) examines whether a program output meets the success criteria for each task goal (Table 1), such as test set performance, prediction-answer matches, and visualization quality. To automatically check these criteria, we implement them as evaluation programs for each task during annotation. By nature, SR is conditioned on valid execution: If a program has execution errors or does not save its output correctly, its $\mathbf{S R}$ will be 0 . Both VER and SR are binary metrics. (3) CodeBERTScore (CBS) measures how closely the generated program resembles the annotated one with contextual embeddings and calculates the F1 metric for matched token embeddings (Zhou et al., 2023). If $\mathbf{S R}=1$ for a program, we change its $\mathbf{C B S}$ to 1.0 as well to reflect task success. (4) API Cost (Cost) calculates the average cost (in USD) to complete one task in our benchmark, since it is important for language agents to control their cost and optimize their design for better practical utility (Kapoor et al., 2024).</p>
<p>Figure Evaluation. If the task output is a figure, we follow existing work (Wu et al., 2024; Yang et al., 2024b) to evaluate its quality using GPT-4o as a judge, which is shown to correlate reasonably well with human raters. We use Yang et al. (2024b)'s prompt to request GPT-4o to compare the program-produced figure with the ground-truth and respond with a score on its quality. For evaluation stability, we sample 3 responses and use the average score to compute success rates.</p>
<p>Rubric-Based Evaluation. Outcome-based evaluation metrics, which require a program to correctly implement all steps for the task, can sometimes be too stringent. For example, an agent would be underrated by these metrics if it gets all steps right but output formatting wrong. As a comple-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Comparison of ScienceAgentBench to representative benchmarks. ${ }^{\dagger}$ DiscoveryBench-Real is evaluating the quality of generated programs indirectly through the natural language hypothesis, while ScienceAgentBench's focus is to rigorously assess the programs and their execution results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Code Gen Complexity</th>
<th style="text-align: center;">Task <br> Sources</th>
<th style="text-align: center;">Heterogeneous Data Processing</th>
<th style="text-align: center;">Shortcut Prevention</th>
<th style="text-align: center;">Scientific Subjects</th>
<th style="text-align: center;"># Test Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TaskBench (Shen et al., 2024)</td>
<td style="text-align: center;">No Code Gen</td>
<td style="text-align: center;">Synthetic</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28,271</td>
</tr>
<tr>
<td style="text-align: center;">SWE-Bench (Jimenez et al., 2024)</td>
<td style="text-align: center;">File-Level Edit</td>
<td style="text-align: center;">GitHub</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2,294</td>
</tr>
<tr>
<td style="text-align: center;">BioCoder-Py (Tang et al., 2024c)</td>
<td style="text-align: center;">Function-Level</td>
<td style="text-align: center;">GitHub</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1,126</td>
</tr>
<tr>
<td style="text-align: center;">ML-Bench (Tang et al., 2024b)</td>
<td style="text-align: center;">Line-Level</td>
<td style="text-align: center;">GitHub</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">260</td>
</tr>
<tr>
<td style="text-align: center;">MLAgentBench (Huang et al., 2024b)</td>
<td style="text-align: center;">File-Level Edit</td>
<td style="text-align: center;">Kaggle</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">DiscoveryBench-Real <br> (Majumder et al., 2024b)</td>
<td style="text-align: center;">Code Gen ${ }^{\dagger}$</td>
<td style="text-align: center;">27 Publications</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">239</td>
</tr>
<tr>
<td style="text-align: center;">SciCode (Tian et al., 2024)</td>
<td style="text-align: center;">Function-Level</td>
<td style="text-align: center;">Publications</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">BLADE (Gu et al., 2024)</td>
<td style="text-align: center;">Function-Level</td>
<td style="text-align: center;">31 Publications</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">ScienceAgentBench (Ours)</td>
<td style="text-align: center;">File-Level Gen</td>
<td style="text-align: center;">44 Publications</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">102</td>
</tr>
</tbody>
</table>
<p>ment to the outcome-based metrics, we introduce rubric-based evaluation to assess the generated programs at more fine-grained levels. Considering the characteristics of data-driven discovery tasks, we structure the rubrics into five stages: Data Loading, Data Processing, Modeling or Visualization, Output formatting, and Output Saving. To accelerate the annotation process, we first use GPT-4o to generate the rubrics by designating multiple milestones with scores for the five stages. Then, each rubric is refined by an expert (Appendix H). In this work, we leverage the rubrics to conduct human evaluation for generated programs (Section 4.2). We deem that automating this rubric-based evaluation approach, such as developing an LLM-based judge, is a meaningful future direction.</p>
<h1>2.4 COMPARISON WITH EXISTING BENCHMARKS</h1>
<p>ScienceAgentBench differs from other benchmarks with a unique ensemble of research challenges (Table 2). (1) Tasks in our benchmark require an agent to generate a standalone program file from scratch, in contrast to JSON API calls in TaskBench, abstract workflow descriptions in DiscoveryBench, or a few lines of code completion or edits in other benchmarks. To do so, an agent needs to have a deep understanding of the task, decompose it into classes and functions appropriately, and implement them. (2) Our benchmark adapts 44 peer-reviewed publications and covers a variety of real-world datasets in four different disciplines. Compared to ML-Bench and DiscoveryBench, our ScienceAgentBench includes more heterogeneous datasets that have complex structures (Figure 1), such as cell images, chemical structure-activity relationships, and geographical maps with multiple layers. (3) ScienceAgentBench is also one of the two benchmarks that tries to mitigate data contamination and agent shortcut issues, which helps establish valid evaluation. (4) Our benchmark has a medium scale of 102 tasks. Although smaller than benchmarks with synthetic or easier tasks, this scale is reasonable to evaluate agents, considering the annotation difficulty and evaluation cost. We will discuss the limitations of our benchmark and other relate work in Appendix A and B.</p>
<h2>3 EXPERIMENTAL SETUP</h2>
<p>We experiment with three open-weight LLMs, Llama-3.1-Instruct-70B, 405B (Dubey et al., 2024), and Mistral-Large-2 (123B) (MistralAI, 2024), and two proprietary LLMs, GPT-4o (OpenAI, 2024a) and Claude-3.5-Sonnet (Anthropic, 2024). For all experiments, we use the same hyperparameters, temperature $=0.2$ and top_p $=0.95$, and perform 0 -shot prompting ${ }^{3}$ via the APIs. In addition, we evaluate OpenAI o1 ${ }^{4}$ (OpenAI, 2024b) with its default hyperparameters. The prompts are included in Appendix I. We evaluate the LLMs under three different (agent) frameworks:</p>
<p>Direct Prompting. Direct prompting is a simple framework that does not interact with any programming environment. Given the task inputs, it prompts an LLM to generate a corresponding program in one pass. We use this framework to show the basic code generation capability of each LLM.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>OpenHands CodeAct. OpenHands ${ }^{5}$ (Wang et al., 2024c) is a generalist agent development framework for code generation and software engineering. It provides three kinds of tools in the environment and defines different actions for an agent to interact with the tools: Python code interpreter, bash shell, and web browser. Among the agents developed with OpenHands, we use the bestperforming CodeActAgent v1.9 (Wang et al., 2024b) in our experiments. This agent unifies all actions in OpenHands, including the agent-computer interface commands (Yang et al., 2024a) to read and edit local files, into a large action space of different Python API calls. We experiment with the CodeActAgent v1.9 using different LLMs to test the effectiveness of OpenHands' framework design for code generation tasks in data-driven discovery. For simplicity, we shorten the name of this agent framework as OpenHands CodeAct.</p>
<p>Self-Debug. Self-debug (Chen et al., 2024a) is a code generation framework for LLMs to execute their generated programs, access execution results, and then reflect on the results to improve each program iteratively. In this work, we re-implement self-debug with three modifications. First, we do not instruct the LLMs to generate reflections before debugging the code, since self-reflection may not always yield better results (Chen et al., 2024b; Huang et al., 2024a; Jiang et al., 2024). Second, we allow early exits if the backbone LLM generates the same program for two consecutive debugging turns. Finally, before running each program, we use pipreqs and pip-tools to set up the environment. We do not initialize the self-debug environment with any of the basic packages or provide the rules to configure some packages that are used for evaluation (Section 2.3). Even though self-debug might not be able to use some packages due to this design choice, we want to ensure fair comparisons with other baselines, which also have no access to these information.</p>
<p>To improve evaluation stability, we repeat each task with three independent runs in all experiments. Then we select the best run according to the metrics in the following order: maximum SR, maximum VER, maximum CBS, and minimum Cost. We refer to the next metric in this order to break ties. For example, if two programs generated for a task both have $\mathbf{S R}=0$, we pick the one with higher VER. Finally, we report each metric based on the average performance of selected runs. We also include the mean performances out of three runs and standard deviations in Appendix E.1.</p>
<h1>4 ReSults and ANALYSIS</h1>
<p>Through comprehensive experiments (Table 3), we show that the latest LLMs and agents can only achieve low-to-moderate task success rates. Although OpenAI o1 shows better performance, we do not compare it with other LLMs due to its additional inference-time reasoning and different hyperparameters. We provide a case study on OpenAI o1 in Appendix F. 3 and focus on analyzing agents based on other LLMs in this section: Given three attempts for each task, Claude-3.5-Sonnet with self-debug demonstrates the best performance ( $34.3 \%$ SR) when using expert-provided knowledge. This result underline that LLM-based agents are not yet capable of fully addressing realistic and challenging data-driven discovery tasks, such as those in ScienceAgentBench.</p>
<h3>4.1 MAIN ReSults</h3>
<p>Direct Prompting vs. Self-Debug: Execution feedback is necessary for LLMs to generate useful programs. As shown in Table 3, directly prompting LLMs cannot unleash their full potential in programming for data-driven discovery tasks. Without executing its code, even the best performing LLM, Claude-3.5-Sonnet, can only solve $16.7 \%$ of the tasks independently and $20.6 \%$ with additional knowledge. For most failed tasks, we share similar findings with Liang et al. (2024) that LLM-generated programs have correct high-level structures but implementation-level errors, such as missing steps or wrong API usage. Compared to direct prompting, self-debug can nearly double Claude-3.5-Sonnet's success rate ( $16.7 \rightarrow 32.4 ; 1.94 \times$ ) without extra knowledge. With expertprovided knowledge, Claude-3.5-Sonnet using self-debug also shows decent improvement over direct prompting. It achieves 13.7 absolute gains on $\mathbf{S R}(20.6 \rightarrow 34.3 ; 1.67 \times)$ and 45.1 absolute gains on VER $(41.2 \rightarrow 86.3 ; 2.09 \times)$. These results highlight the effectiveness of the simple self-debug framework and the importance of enabling LLMs to execute and revise their code for complex tasks.</p>
<p>OpenHands CodeAct vs. Self-Debug: Agent designs should consider costs and capabilities of LLMs. For four of the five LLMs evaluated, self-debug demonstrates better performance than</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Results on ScienceAgentBench. The best performances (with and without knowledge) for each framework are in bold. The overall best performances are underlined. ${}^{\ddagger}$ OpenAI o1 generates additional reasoning tokens for extensive inference-time compute (with different hyperparameters), so it might be unfair to compare the model with other LLMs, yet we include it for reference.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Without Knowledge</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">With Knowledge</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">CBS</td>
<td style="text-align: center;">VER</td>
<td style="text-align: center;">Cost $\downarrow$</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">CBS</td>
<td style="text-align: center;">VER</td>
<td style="text-align: center;">Cost $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Direct Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-70B</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-405B</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-Large-2 (2407)</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (2024-05-13)</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">0.012</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.5-Sonnet (2024-06-20)</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">0.017</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI o1-preview ${ }^{\ddagger}$</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">0.236</td>
</tr>
<tr>
<td style="text-align: center;">OpenHands CodeAct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-70B</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-405B</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">0.740</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-Large-2 (2407)</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">0.759</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (2024-05-13)</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">1.094</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.5-Sonnet (2024-06-20)</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">0.900</td>
</tr>
<tr>
<td style="text-align: center;">Self-Debug</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-70B</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">0.008</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-Instruct-405B</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">0.055</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-Large-2 (2407)</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">0.036</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (2024-05-13)</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">0.046</td>
</tr>
<tr>
<td style="text-align: center;">Claude-3.5-Sonnet (2024-06-20)</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">0.061</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI o1-preview ${ }^{\ddagger}$</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">0.713</td>
</tr>
</tbody>
</table>
<p>OpenHands CodeAct, with GPT-4o as the only exception (Table 3). By examining the trajectories, we find that GPT-4o is better at leveraging tools in OpenHands than other LLMs. For instance, it is the only LLM that search for more details about the provided knowledge with the web browser. In contrast, other LLMs are still struggling with specialized bash commands in OpenHands to edit programs correctly (Example in Appendix F.1). We hypothesize that GPT-4o may have been trained to better follow instructions for language agents and to better use complex tools like a web browser.</p>
<p>When it comes to self-debug, which has a more straightforward design, GPT-4o loses its advantage and underperforms Mistral-Large-2 and Claude-3.5-Sonnet, both of which are trained for better code generation according to their reports (MistralAI, 2024; Anthropic, 2024). Most surprisingly, without the help of expert-provided knowledge, Claude-3.5-Sonnet using self-debug can successfully solve $10.8 \%$ more tasks ( $21.6 \rightarrow 32.4$ SR) than using OpenHands while costing 17 times less API fees ( $\$ 0.958 \rightarrow \$ 0.057$ ), which is a critical factor to consider for practical applications. Overall, our results resonate with recent findings on agent design (Kapoor et al., 2024; Xia et al., 2024): (1) LLM-based agents do not always benefit from a large action space with complex tools; and (2) both cost and performance should be considered when designing or selecting agent frameworks.</p>
<p>With vs. Without Expert-Provided Knowledge: Expert-provided knowledge does not always lead to metric improvement. On one hand, we observe that expert-provided knowledge leads to consistent improvements on SR and CBS for most agents (Table 3). These agents can effectively leverage helpful information in the knowledge, such as API names and some concrete steps in the task, to generate a high-quality program draft that closely resembles the annotated gold program and then use execution feedback to address implementation errors.</p>
<p>On the other hand, we notice that there are performance decreases on VER for most agents. These decreases can be attributed to two reasons. (1) Expert-provided knowledge specifies some specific tools that are less familiar to the agents. Originally, they would only use basic tools like rdkit and sklearn in their generated programs, which are free of execution errors. With provided knowledge, the agents would use those specified tools to generate programs, which often contain incorrect API usage and hallucinated API calls. (2) The agents do not know how to solve some tasks without</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Task performance analysis of Claude-3.5-Sonnet with self-debug and expert-provided knowledge. Left: Distribution of lines in gold programs for succeeded and failed tasks. The red vertical line marks the average length (58.6 lines) of all gold programs in the benchmark. Right: Task error rates for each sub-task category in each discipline.
expert-provided knowledge and would generate some executable but less meaningful programs, e.g., to produce an empty figure. While additional knowledge helps them to produce more concrete modeling or analysis, such programs are error-prone and hard to fix with execution feedback (Appendix F.2). For these reasons, despite decreases in VER, we argue that expert-provided knowledge helps agents to generate more useful programs from a scientist user's perspective, as reflected by SR and CBS, and future AI agents should improve their abilities to better leverage such information.</p>
<p>Language agents cannot solve complex data-driven discovery tasks yet. Our further analysis on the best performing agent, Claude-3.5-Sonnet with self-debug and expert-provided knowledge, show that it is not yet capable of addressing complex tasks in data-driven discovery. To estimate the complexity of tasks, we visualize the number of lines in their corresponding gold programs using box plot (Figure 3; Left). More than $75 \%$ of succeeded tasks lean to the simpler side because their gold programs have less than 58.6 lines, which is the mean length of all gold programs in the benchmark. In other words, language agents still fail on many tasks with complex gold programs.</p>
<p>To understand the task failures, we break them down by different disciplines and sub-task categories (Figure 3; Right). For Bioinformatics and Computational Chemistry, the agent mostly fails on tasks involving data processing and model development. This is because data in these two disciplines are highly heterogeneous, including cell images, molecules, and genes, which can be hard to process. Without correctly processed data, the agent would also not be able to develop and train a functioning model, not to mention choosing appropriate configurations for various models such as Convolutional or Graph Neural Networks used in the tasks. For Geographical Information Science and Psychology \&amp; Cognitive Neuroscience, their tasks usually require discipline-specific tools, such as Geopandas and Biopsykit, to analyze the datasets. However, existing LLMs fall short of using these tools and can generate incorrect or hallucinated API usage in the programs. Given these shortcomings, we argue that current language agents cannot yet automate data-driven discovery tasks or the full research pipeline, in contrast to claims made in recent work such as Lu et al. (2024).</p>
<h1>4.2 Human Evaluation</h1>
<p>Evaluation Setup. To further investigate the performance of Claude-3.5-Sonnet with self-debug (the best-performing agent), we conduct a rubric-based human evaluation of all the 102 programs generated using expert-provided knowledge. With the task-specific rubrics validated by experts (examples in Appendix H) and gold programs as references, each generated program is rated by two different evaluators who participated in data collection. To reduce possible noises in ratings, the evaluators only mark whether a rubric item is met by the LLM-generated program. For each stage, we add up points for satisfied rubric items and normalize them by total available points to the range of $0-100$. Similarly, we calculate the overall score considering all items. The final score of each program is the average of two evaluators' ratings.</p>
<p>Additionally, one purpose of this human evaluation is to assign partial credits to the generated program even if it is not correct (Section 2.3). Therefore, we do not provide the evaluators with program execution results and hide task success outcomes. Although this setup encourages evaluators to ex-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Rubric-based human ratings for 102 programs generated by Claude-3.5-Sonnet with self-debug and expert-provided knowledge. We show the overall distributions and those for the five stages in our rubrics (Section 2.3). The blue boxes are distributions for failed tasks, and the orange ones are for succeeded tasks. The open dots represent outliers in the distributions.</p>
<p>amine LLM-generated programs carefully, it also introduces some noise. For example, there are tasks where both a feed-forward neural network and a random forest model can achieve satisfying performance on the test set. While the gold program implements the neural network, the agent chooses to use random forest. Since each rubric is derived from a gold program and reflects its implementation, there are chances that the evaluator overlooks such an equivalence. Also, for output formatting, we observe some subjective variance when judging the formats of figures, such as colors, scales, and text labels, according to the rubrics and gold programs. As a result, successful programs would not always receive a perfect human rating.</p>
<p><strong>Results and Analysis.</strong> As shown in Figure 4, data loading and processing, the first two stages in data-driven discovery tasks can distinguish successful programs from failed ones. Except for a few outliers, almost all successful programs receive a perfect human rating for data loading. In contrast, 25% of the failed programs have their rating below 50 in the first stage. For data processing, the rating distribution of successful programs skews toward the full score, while that of failed programs skews toward a score between 20 and 50. These human evaluation results correspond to an intuitive explanation: If the dataset were not loaded or processed correctly, it would be impossible to solve a task successfully, regardless of the code implementation for consequent stages.</p>
<p>In the third stage, modeling or visualization, human ratings for successful and failed programs are also different: The median score of successful programs is already at the 75th percentile of failed program ratings. This indicates that human evaluators agree with the <strong>SR</strong> metric and prefer programs passing all success criteria for the task, even though they may have some minor issues. For output formatting and saving, we find no difference between the two groups of programs, indicating that LLMs like Claude-3.5-Sonnet can follow such instructions reasonably well.</p>
<p>Overall, human ratings for succeeded and failed programs form two overlapped but distinguishable distributions, which meets our motivation to complement outcome-based metrics with fine-grained evaluation. These ratings agree with our main result and suggest that some LLM-generated programs are close to success but hindered by some bottlenecks, such as data loading and processing. Future research may, for example, improve language agents' capability to better process scientific data.</p>
<h2>5 CONCLUSION</h2>
<p>We introduce ScienceAgentBench, a new benchmark to evaluate language agents for data-driven scientific discovery. We compile 102 diverse, real-world tasks from 44 peer-reviewed publications across four scientific disciplines and engage nine subject matter experts to ensure data quality. Through comprehensive experiments on five LLMs and three frameworks, we show that the best-performing agent, Claude-3.5-Sonnet with self-debug, can only solve 34.3% of the tasks when using expert-provided knowledge. Our results and analysis suggest that current language agents cannot yet automate tasks for data-driven discovery or a whole research pipeline. By introducing ScienceAgentBench, we advocate the use of language agents to assist human scientists with tedious tasks in their workflows and call for more rigorous assessments of such agents.</p>
<h1>ETHICS STATEMENT</h1>
<p>Our benchmark is constructed by adapting open-source code and data, to which we respect their creators' ownership and intellectual property. In Appendix J, we have made our best effort to cite the original papers, list the repositories, and provide their licenses. Still, we acknowledge that two repositories are copyrighted and believe their terms for use are compatible with our research purpose (Table J.4, J.5). We welcome requests from the original authors to modify or remove relevant tasks if needed.</p>
<p>Meanwhile, agents developed with ScienceAgentBench should consider potential safety issues in deployment, especially when performing Bioinformatics and Computational Chemistry tasks. This work contributes an evaluation benchmark to assess existing language agents rigorously, which has limited or no risk in inadvertently synthesizing toxic or dangerous chemicals. Yet, we are aware that the safety of language agents for science is an important research topic (Tang et al., 2024a) and have discussed with our subject matter experts about the risk of synthesizing toxic or dangerous chemicals: (1) Our Bioinformatics and Computational Chemistry tasks focus on property prediction, feature analyses, and molecule visualization, which does not involve synthesis or generation of biological or chemical substances. (2) Unlike Coscientist (Boiko et al., 2023), agents evaluated in our submission are not connected to any laboratory hardwares. Thus, it is impossible for these agents to produce any dangerous chemicals or substances on their own. Even if they were to be instructed to write code for chemical synthesis in real-world applications, human intervention is still required to grant the access to laboratories, reagents, and equipment. (3) The target outputs for every task in ScienceAgentBench are unified as self-contained Python programs. Therefore, the evaluated agents only generate code for processing, analyzing and visualizing scientific data that is already publicly available. They are not instructed to generate chemical reactions or synthesis pathways. We also recommend the developers of these agents to consider such potential risks seriously and provide effective intervention and feedback mechanisms for users.</p>
<h2>AUTHOR CONTRIBUTIONS</h2>
<p>Z. Chen led the project, formulated the benchmark, organized data collection and human evaluation, implemented programs and evaluation scripts for experiments, conducted all experiments on GPT-4o/Claude/Mistral, and wrote the manuscript. S. Chen designed and implemented rubric-based evaluation, and helped to run direct prompting, self-debug, and OpenHands experiments on Llama 3.1 70B/405B, optimize the evaluation scripts, and revise the manuscript. Y. Ning helped to run some experiments on OpenHands with Llama 3.1 70B/405B. Z. Chen, S. Chen, Y. Ning, Q. Zhang, B. Wang, B. Yu, Y. Li, Z. Liao, and C. Wei worked as the student annotators to collect the benchmark, verify the tasks, and evaluate generated programs based on rubrics. Z. Lu, V. Dey, M. Xue, F. Baker, B. Burns, D. Adu-Ampratwum, X. Huang, X. Ning, and S. Gao are subject matter experts who validated the tasks and provided task-specific knowledge. In addition, S. Gao provided substantial guidance in Geographical Information Science task collection and constructive feedback on the project during weekly discussions. Y. Su and H. Sun are senior authors who oversaw this project, contributed to the core ideas, and revised the manuscript. H. Sun conceived the research problem.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors would thank colleagues from the OSU NLP group and the NSF-funded ICICLE AI Institute for constructive feedback. This research was sponsored in part by NSF OAC 2112606, Amazon, and Ohio Supercomputer Center (Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<h2>REFERENCES</h2>
<p>Mathijs Andeweg and Tom Kuijpers. Model how land subsidence affects flooding, April 2024. URL https://learn.arcgis.com/en/projects/ model-how-land-subsidence-affects-flooding/.</p>
<p>Anthropic. Claude 3.5 sonnet. Jun 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet.</p>
<p>Gordon Bell, Tony Hey, and Alex Szalay. Beyond the data deluge. Science, 323(5919):1297-1298, 2009. doi: 10.1126/science.1170411. URL https://www.science.org/doi/abs/10. 1126/science. 1170411 .</p>
<p>Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. SUPER: Evaluating agents on setting up and executing tasks from research repositories. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1262212645, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main. 702.</p>
<p>Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624:570-578, 2023. doi: https://doi.org/10.1038/ s41586-023-06792-0.</p>
<p>Cédric Bouysset and Sébastien Fiorucci. ProLIF: a library to encode molecular interactions as fingerprints. Journal of Cheminformatics, 13(1):72, 2021. doi: 10.1186/s13321-021-00548-6.</p>
<p>Daniel Brand, Nicolas Riesterer, Hannah Dames, and Marco Ragni. Analyzing the differences in human reasoning viajoint nonnegative matrix factorization. Proceedings of the Annual Meeting of the Cognitive Science Society, 42, 2020. URL https://escholarship.org/uc/item/ 0br9k22g.</p>
<p>Danila Bredikhin, Ilia Kats, and Oliver Stegle. Muon: multimodal omics analysis framework. Genome Biology, 23(1), 2022. doi: 10.1186/s13059-021-02577-8.
O. J. M. Béquignon, B. J. Bongers, W. Jespers, A. P. IJzerman, B. van der Water, and G. J. P. van Westen. Cellprofiler: image analysis software for identifying and quantifying cell phenotypes. Journal of Cheminformatics, 15(3), 2023. doi: 10.1186/s13321-022-00672-x.</p>
<p>Longbing Cao. Data science: A comprehensive overview. ACM Comput. Surv., 50(3), jun 2017. ISSN 0360-0300. doi: 10.1145/3076253. URL https://doi.org/10.1145/3076253.</p>
<p>Anne E. Carpenter, Thouis R. Jones, Michael R. Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A. Guertin, Joo Han Chang, Robert A. Lindquist, Jason Moffat, Polina Golland, and David M. Sabatini. Cellprofiler: image analysis software for identifying and quantifying cell phenotypes. Genome Biology, 7(R100), 2006. doi: 10.1186/gb-2006-7-10-r100.</p>
<p>Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark: /19495/f5s1ph73.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=KuPixIqPiq.</p>
<p>Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search useful for LLM planning? it depends on the discriminator. In Lun-Wei Ku, Andre Martins, and</p>
<p>Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13659-13678, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.acl-long. 738 .</p>
<p>Simone Ciuti, Tyler B. Muhlym, Dale G. Paton, Allan D. McDevitt, Marco Musiani, and Mark S. Boyce. Human selection of elk behavioural traits in a landscape of fear. Proceedings of the Royal Society B, 279:4407-4416, 2012. doi: 10.1098/rspb.2012.1483.</p>
<p>Andrew Dawson. eofs: A library for eof analysis of meteorological, oceanographic, and climate data. Journal of Open Research Software, 4(1), 2016. doi: https://doi.org/10.5334/jors. 122.</p>
<p>Pierre-Paul De Breuck, Matthew L Evans, and Gian-Marco Rignanese. Robust model benchmarking and bias-imbalance in data-driven materials science: a case study on modnet. Journal of Physics: Condensed Matter, 33(40):404002, jul 2021a. doi: 10.1088/1361-648X/ac1280. URL https: //dx.doi.org/10.1088/1361-648X/ac1280.</p>
<p>Pierre-Paul De Breuck, Geoffroy Hautier, and Gian-Marco Rignanese. Materials property prediction for limited datasets enabled by feature selection and joint learning with modnet. npj Computational Materials, 7(1), 2021b. doi: 10.1038/s41524-021-00552-2.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=kiYqbO3wqw.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang,</p>
<p>Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.</p>
<p>ESRL Predict deforestation in the amazon rain forest, January 2024a. URL https://learn.arcgis.com/en/projects/ predict-deforestation-in-the-amazon-rain-forest/.</p>
<p>ESRI. Assess access to public transit, February 2024b. URL https://learn.arcgis.com/ en/projects/assess-access-to-public-transit/.</p>
<p>ESRI. Build a model to connect mountain lion habitat, August 2024c. URL https://learn.arcgis.com/en/projects/ build-a-model-to-connect-mountain-lion-habitat/.</p>
<p>ESRI. Assess burn scars with satellite imagery, June 2024d. URL https://learn.arcgis. com/en/projects/assess-burn-scars-with-satellite-imagery/.</p>
<p>Sunny Fleming. Model animal home range, March 2024. URL https://learn.arcgis. com/en/projects/model-animal-home-range/.</p>
<p>Adam Gayoso, Romain Lopez, Galen Xing, Pierre Boyeau, Valeh Valiollah Pour Amiri, Justin Hong, Katherine Wu, Michael Jayasuriya, Edouard Mehlman, Maxime Langevin, Yining Liu, Jules Samaran, Gabriel Misrachi, Achille Nazaret, Oscar Clivio, Chenling Xu, Tal Ashuach, Mariano Gabitto, Mohammad Lotfollahi, Valentine Svensson, Eduardo da Veiga Beltrame, Vitalii Kleshchevnikov, Carlos Talavera-López, Lior Pachter, Fabian J. Theis, Aaron Streets, Michael I. Jordan, Jeffrey Regier, and Nir Yosef. A python library for probabilistic analysis of single-cell omics data. Nature Biotechnology, 40:163-166, 2022. doi: 10.1038/s41587-021-01206-w.</p>
<p>David E. Graff, Eugene I. Shakhnovich, and Connor W. Coley. Accelerating high-throughput virtual screening through molecular pool-based active learning. Chemical Science, 12(22):7866-7881, 2021. doi: $10.1039 /$ D0SC06805E.</p>
<p>Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey Heer, and Tim Althoff. Blade: Benchmarking language model agents for data-driven science, 2024. URL https://arxiv.org/abs/2408.09667.</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. WebVoyager: Building an end-to-end web agent with large multimodal models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6864-6890, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.371.</p>
<p>Tony Hey, Stewart Tansley, Kristin Tolle, and Jim Gray. The Fourth Paradigm: DataIntensive Scientific Discovery. Microsoft Research, October 2009. ISBN 978-0-9825442-0-4. URL https://www.microsoft.com/en-us/research/publication/ fourth-paradigm-data-intensive-scientific-discovery/.</p>
<p>Tom Hourigan. NOAA Deep Sea Corals Research and Technology Program, 1 2023. URL https: //www.gbif.org/dataset/df8e3fb8-3da7-4104-a866-748f6da20a3c.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview. net/forum?id=IkmD3fKBPQ.</p>
<p>Kexin Huang, Tianfan Fu, Lucas M Glass, Marinka Zitnik, Cao Xiao, and Jimeng Sun. Deeppurpose: A deep learning library for drug-target interaction prediction. Bioinformatics, 2020.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=1Fs1LvjYQW.</p>
<p>Ryan Jacobs, Tam Mayeshiba, Ben Afflerbach, Luke Miles, Max Williams, Matthew Turner, Raphael Finkel, and Dane Morgan. The materials simulation toolkit for machine learning (mastml ): An automated open source toolkit to accelerate data-driven materials research. Computational Materials Science, 176:109544, 2020. ISSN 0927-0256. doi: https://doi.org/10. 1016/j.commatsci.2020.109544. URL https://www.sciencedirect.com/science/ article/pii/S0927025620300355.</p>
<p>Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, and Daniel Khashabi. Self-[in]correct: Llms struggle with discriminating self-generated responses, 2024. URL https://arxiv.org/abs/2404.04298.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66.</p>
<p>John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino RomeraParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583-589, 2021. doi: https://doi.org/10.1038/s41586-021-03819-2.</p>
<p>Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. Ai agents that matter, 2024. URL https://arxiv.org/abs/2407.01502.</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic visual web tasks. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881-905, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long. 50.</p>
<p>Eric Krause. Analyze urban heat using kriging, July 2024. URL https://learn.arcgis. com/en/projects/analyze-urban-heat-using-kriging/.</p>
<p>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. BioMistral: A collection of open-source pretrained large language models for medical domains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 5848-5864, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl. 348.</p>
<p>Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, and Muhao Chen. GeoLM: Empowering language models for geospatially grounded language understanding. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 5227-5240, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.317. URL https://aclanthology.org/2023. emnlp-main. 317 .</p>
<p>Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, and Thomas Zimmermann. Can gpt-4 replicate empirical software engineering research? Proc. ACM Softw. Eng., 1(FSE), jul 2024. doi: 10.1145/3660767. URL https://doi.org/10.1145/ 3660767 .</p>
<p>Guanyu Lin, Tao Feng, Pengrui Han, Ge Liu, and Jiaxuan You. Paper copilot: A self-evolving and efficient llm system for personalized academic assistance, 2024. URL https://arxiv.org/ abs/2409.04593.</p>
<p>Anika Liu, Moritz Walter, Peter Wright, Aleksandra Bartosik, Daniela Dolciami, Abdurrahman Elbasir, Hongbin Yang, and Andreas Bender. Prediction and mechanistic analysis of druginduced liver injury (dili) based on chemical structure. Biology Direct, 16(6), 2021. doi: $10.1186 / \mathrm{s} 13062-020-00285-0$.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/ abs/2408.06292.</p>
<p>Zitong Lu and Julie Golomb. Generate your neural signals from mine: individual-to-individual eeg converters. Proceedings of the Annual Meeting of the Cognitive Science Society, 45, 2023. URL https://escholarship.org/uc/item/5xn0885t.</p>
<p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, and Peter Clark. Position: Data-driven discovery with large generative models. In Forty-first International Conference on Machine Learning, 2024a. URL https://openreview.net/ forum?id=5SpjhZNXtt.</p>
<p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. Discoverybench: Towards data-driven discovery with large language models, 2024b. URL https://arxiv.org/abs/2407.01725.</p>
<p>Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Brammer, François Lespinasse, Hung Pham, Christopher Schölzel, and S. H. Annabel Chen. NeuroKit2: A Python toolbox for neurophysiological signal processing. Behavior Research Methods, 53(4):1689-1696, February 2021. doi: $10.3758 / \mathrm{s} 13428-020-01516-\mathrm{y}$.</p>
<p>Mariia Matveieva and Pavel Polishchuk. Benchmarks for interpretation of qsar models. Journal of Cheminformatics, 41(13), 2021. doi: 10.1186/s13321-021-00519-x.
F. Maussion, A. Butenko, N. Champollion, M. Dusch, J. Eis, K. Fourteau, P. Gregor, A. H. Jarosch, J. Landmann, F. Oesterle, B. Recinos, T. Rothenpieler, A. Vlug, C. T. Wild, and B. Marzeion. The open global glacier model (oggm) v1.1. Geoscientific Model Development, 12(3):909931, 2019. doi: 10.5194/gmd-12-909-2019. URL https://gmd.copernicus.org/ articles/12/909/2019/.</p>
<p>MistralAI. Large enough. Jul 2024. URL https://mistral.ai/news/ mistral-large-2407.</p>
<p>Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L. Chevrier, Kristin A. Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. Computational Materials Science, 68:314-319, 2013. ISSN 0927-0256. doi: https: //doi.org/10.1016/j.commatsci.2012.10.028. URL https://www.sciencedirect.com/ science/article/pii/S0927025612006295.</p>
<p>OpenAI. Hello gpt-4o. May 2024a. URL https://openai.com/index/hello-gpt-4o.
OpenAI. Introducing openai o1-preview. Sep 2024b. URL https://openai.com/index/ introducing-openai-o1-preview/.</p>
<p>Bharath Ramsundar, Bowen Liu, Zhenqin Wu, Andreas Verras, Matthew Tudor, Robert P. Sheridan, and Vijay Pande. Is multitask deep learning practical for pharma? Journal of Chemical Information and Modeling, 57(8):2068-2076, 2017. doi: 10.1021/acs.jcim.7b00146. URL https://doi.org/10.1021/acs.jcim.7b00146. PMID: 28692267.</p>
<p>Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. Deep Learning for the Life Sciences. O'Reilly Media, 2019. https://www.amazon.com/ Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.</p>
<p>Sebastian Raschka, Anne M. Scott, Mar Huertas, Weiming Li, and Leslie A. Kuhn. Automated Inference of Chemical Discriminants of Biological Activity, pp. 307-338. Springer New York, New York, NY, 2018. ISBN 978-1-4939-7756-7. doi: 10.1007/978-1-4939-7756-7_16. URL https://doi.org/10.1007/978-1-4939-7756-7_16.</p>
<p>Robert Richer, Arne Küderle, Martin Ullrich, Nicolas Rohleder, and Bjoern M. Eskofier. Biopsykit: A python package for the analysis of biopsychological data. Journal of Open Source Software, 6 (66):3702, 2021. doi: 10.21105/joss.03702. URL https://doi.org/10.21105/joss. 03702 .</p>
<p>Nicolas Riesterer, Daniel Brand, Hannah Dames, and Marco Ragni. Modeling human syllogistic reasoning:the role of "no valid conclusion". Proceedings of the Annual Meeting of the Cognitive Science Society, 41, 2019. URL https://escholarship.org/uc/item/5xm1m8h8.</p>
<p>AShlee Robinson. Chart coral and sponge distribution factors with python, October 2023. URL https://learn.arcgis.com/en/projects/ chart-coral-and-sponge-distribution-factors-with-python/.</p>
<p>Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, 3 edition, 2010.</p>
<p>Jason Schatz and Christopher J Kucharik. Urban climate effects on extreme temperatures in madison, wisconsin, usa. Environmental Research Letters, 10(9):094024, 2015.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task automation. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. URL https://openreview.net/forum?id=ZUbraGNpAq.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers, 2024. URL https://arxiv.org/abs/ 2409.04109 .</p>
<p>Gregor Sturm, Tamas Szabo, Georgios Fotakis, Marlene Haider, Dietmar Rieder, Zlatko Trajanoski, and Francesca Finotello. Scirpy: a Scanpy extension for analyzing single-cell T-cell receptorsequencing data. Bioinformatics, 36(18):4817-4818, 07 2020. ISSN 1367-4803. doi: 10. 1093/bioinformatics/btaa611. URL https://doi.org/10.1093/bioinformatics/ btaa611.</p>
<p>Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph C Wu, Rabindra V Shivnaraine, and James Zou. ADMET-AI: a machine learning ADMET platform for evaluation of large-scale chemical libraries. Bioinformatics, 40(7):btae416, 06 2024. ISSN 1367-4811. doi: 10. 1093/bioinformatics/btae416. URL https://doi.org/10.1093/bioinformatics/ btae416.</p>
<p>Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, and Mark Gerstein. Prioritizing safeguarding over autonomy: Risks of llm agents for science, 2024a. URL https://arxiv.org/abs/2402.04247.</p>
<p>Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, and Mark Gerstein. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code, 2024b. URL https://arxiv.org/abs/ 2311.09835 .</p>
<p>Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark B Gerstein. BioCoder: a benchmark for bioinformatics code generation with large language models. Bioinformatics, 40 (Supplement_1):i266-i276, 2024c. ISSN 1367-4811. doi: 10.1093/bioinformatics/btae230. URL https://doi.org/10.1093/bioinformatics/btae230.</p>
<p>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024. URL https: //arxiv.org/abs/2407.13168.</p>
<p>Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Philipp Angerer, Volker Bergen, Pierre Boyeau, Maren Büttner, Gokcen Eraslan, David Fischer, Max Frank, Justin Hong, Michal Klein, Marius Lange, Romain Lopez, Mohammad Lotfollahi, Malte D. Luecken, Fidel Ramirez, Jeffrey Regier, Sergei Rybakov, Anna C. Schaar, Valeh Valiollah Pour Amiri, Philipp Weiler, Galen Xing, Bonnie Berger, Dana Pe'er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle, Fabian J. Theis, and Scverse Community. The scverse project provides a computational ecosystem for single-cell omics data analysis. Nature Biotechnology, 41(5), 2023. doi: 10.1038/ s41587-023-01733-8.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 27172739, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.153. URL https://aclanthology.org/2023.acl-long.153.</p>
<p>Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. LLMs in the imaginarium: Tool learning through simulated trial and error. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10583-10604, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.acl-long. 570 .</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veličković, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620:47-60, 2023b. doi: https://doi.org/10.1038/s41586-023-06221-2.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=jJ9BoXAfFa.</p>
<p>Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Opendevin: An open platform for ai software developers as generalist agents, 2024c. URL https://arxiv.org/abs/2407.16741.</p>
<p>Logan Ward, Alexander Dunn, Alireza Faghaninia, Nils E.R. Zimmermann, Saurabh Bajaj, Qi Wang, Joseph Montoya, Jiming Chen, Kyle Bystrom, Maxwell Dylla, Kyle Chard, Mark Asta, Kristin A. Persson, G. Jeffrey Snyder, Ian Foster, and Anubhav Jain. Matminer: An open source toolkit for materials data mining. Computational Materials Science, 152:60-69, 2018. ISSN 0927-0256. doi: https://doi.org/10.1016/j.commatsci.2018.05.018. URL https: //www.sciencedirect.com/science/article/pii/S0927025618303252.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24824-24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.
F. Alexander Wolf, Philipp Angerer, and Fabian J. Theis. Scanpy: large-scale single-cell gene expression data analysis. Genome Biology, 19(15), 2018. doi: 10.1186/s13059-017-1382-0.</p>
<p>Felix Wong, Erica J. Zheng, Jacqueline A. Valeri, Nina M. Donghia, Melis N. Anahtar, Satotaka Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, Abigail L. Manson, Jens</p>
<p>Friedrichs, Ralf Helbig, Behnoush Hajian, Dawid K. Fiejtek, Florence F. Wagner, Holly H. Soutter, Ashlee M. Earl, Jonathan M. Stokes, Lars D. Renner, and James J. Collins. Discovery of a structural class of antibiotics with explainable deep learning. Nature, 626:177-185, 2024. doi: $10.1038 / \mathrm{s} 41586-023-06887-8$.</p>
<p>Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. Plot2code: A comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots, 2024. URL https://arxiv.org/abs/ 2405.07990 .</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chem. Sci., 9:513-530, 2018. doi: 10.1039/C7SC02664A. URL http://dx.doi.org/ 10.1039/C7SC02664A.</p>
<p>Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llmbased software engineering agents, 2024. URL https://arxiv.org/abs/2407.01489.</p>
<p>John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent computer interfaces enable software engineering language models, 2024a.</p>
<p>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, and Maosong Sun. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization, 2024b. URL https: //arxiv.org/abs/2402.11453.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X.</p>
<p>Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun. LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=1Y6XTF9tPv.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=yLClGs7701.</p>
<p>Paul Zandbergen. Run geoprocessing tools with python, March 2024. URL https://learn. arcgis.com/en/projects/run-geoprocessing-tools-with-python/.</p>
<p>Amanda J Zellmer and Barbara S Goto. Urban wildlife corridors: Building bridges for wildlife and people. Frontiers in Sustainable Cities, 4:954089, 2022.</p>
<p>Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. A comprehensive survey of scientific large language models and their applications in scientific discovery, 2024. URL https://arxiv.org/abs/2406.10833.</p>
<p>Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V(ision) is a generalist web agent, if grounded. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 61349-61385. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/ v235/zheng24e.html.</p>
<p>Zhiling Zheng, Oufan Zhang, Ha L. Nguyen, Nakul Rampal, Ali H. Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and cofs. ACS Central Science, 9(11): 2161-2170, 2023. doi: 10.1021/acscentsci.3c01087. URL https://doi.org/10.1021/ acscentsci.3c01087.</p>
<p>Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. CodeBERTScore: Evaluating code generation with pretrained models of code. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 13921-13937, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.859. URL https://aclanthology.org/2023. emnlp-main. 859 .</p>
<p>Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= oKn9c6ytLx.</p>
<p>Abubakr Ziedan, Cassidy Crossland, Candace Brakewood, Philip Pugliese, and Harrison Ooi. Investigating the preferences of local residents toward a proposed bus network redesign in chattanooga, tennessee. Transportation Research Record, 2675(10):825-840, 2021.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/All-Hands-AI/OpenHands, originally named as OpenDevin.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
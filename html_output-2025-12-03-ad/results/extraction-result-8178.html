<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8178 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8178</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8178</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-274965733</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.15266v1.pdf" target="_blank">On the Structural Memory of LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Memory plays a pivotal role in enabling large language model~(LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8178.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8178.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MemoryStudy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agents with structured memory (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental study of memory-augmented LLM agents evaluating four structural memory types (chunks, knowledge triples, atomic facts, summaries) plus a mixed combination, and three retrieval methods (single-step, reranking, iterative) across six long-context QA and reading/dialogue tasks using GPT-4o-mini-128k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Unspecified LLM-based agent (this paper's evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A generic LLM-based agent architecture that (1) generates structural memories from raw documents via prompts, (2) stores memories as embeddings, (3) retrieves relevant memories with a Retriever and optional LLM-based reranker/refiner, and (4) generates answers using Memory-Only or Memory-Doc contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-128k</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The underlying LLM used for generation and retrieval-guided prompting in experiments; run with temperature 0.2 and 4k token input window; memory embeddings created with text-embedding-3-small.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, Lo-CoMo, QuAL-ITY</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of six datasets covering multi-hop long-context QA (HotPotQA, 2WikiMultihopQA, MuSiQue), single-hop long-context QA (NarrativeQA), dialogue-based long-context QA (Lo-CoMo) and reading-comprehension (QuAL-ITY). Agents must retrieve relevant long-context information and answer questions requiring multi-step or long-context reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / long-context reasoning / dialogue understanding / reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structural, retrieval-augmented memory (chunks, knowledge triples, atomic facts, summaries, and mixed union)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-generated structured memories are embedded (text-embedding-3-small) and stored; retrieval is performed by a Retriever (semantic vector search) with three modes: single-step top-K, LLM-based reranking of candidates, and iterative retrieval with LLM query refinement; retrieved memories are prepended to the LLM either directly (Memory-Only) or used to locate original documents (Memory-Doc).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents transformed into: continuous text chunks, knowledge triples (<head; relation; tail>), atomic facts (concise fact sentences), summaries (condensed document descriptions), and Mixed = union of all types.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>single-step semantic top-K retrieval; reranking via LLM prompt over initial candidates; iterative retrieval where retrieved memories refine the query via an LLM over N iterations then final retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Best reported: Mixed memory + Iterative retrieval — HotPotQA EM 67.00, F1 82.11; 2WikiMultihopQA EM 51.00, F1 68.15; MuSiQue EM 39.00, F1 61.38; NarrativeQA EM 12.50, F1 28.36; Lo-CoMo EM 7.85, F1 45.25; QuAL-ITY ACC 79.50. (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Full Content baseline (no structured memory retrieval): HotPotQA EM 55.50, F1 75.77; 2WikiMultihopQA EM 44.00, F1 54.33; MuSiQue EM 36.00, F1 51.60; NarrativeQA EM 7.00, F1 24.99; Lo-CoMo EM 13.61, F1 41.82; QuAL-ITY ACC 81.50. (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comprehensive comparative experiments across memory structures and retrieval methods (single-step, reranking, iterative); hyperparameter sweeps for K (retrieved top-K), R (reranked top-R), T (per-iteration retrieved), and N (iteration turns); robustness tests adding random noisy documents; comparison of two answer-generation approaches (Memory-Only vs Memory-Doc). Findings: iterative retrieval outperforms single-step and reranking generally; mixed memory outperforms individual structures overall and is most noise-resistant; chunks/summaries better for long-context tasks; triples/atomic facts better for relational/precision tasks; Memory-Doc helps extensive-context tasks while Memory-Only favors precision/multi-hop tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Mixed memory yields the most balanced and robust performance across tasks and noise. 2) Iterative retrieval consistently gives the best retrieval accuracy and downstream QA scores. 3) Memory structure should be matched to task: chunks & summaries for long-context/abstraction tasks, knowledge triples & atomic facts for relational and precision-demanding multi-hop QA. 4) Memory-Doc is preferable for extensive-context tasks; Memory-Only is preferable for precision-focused reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Experiments restricted to six datasets and four task types (no self-evolving or social simulation domains); noise experiments limited to random document noise (contradictory/irrelevant noise not explored); hyperparameter ranges constrained by compute; QuAL-ITY accuracy baseline higher for Full Content in some cases (e.g., QuAL-ITY ACC 81.50 vs mixed/memory ACC 79.50), showing memory does not universally improve every metric/task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8178.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8178.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiAgent: Hierarchical working memory management for LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work referenced as using sub-goals as memory chunks to manage working memory in LLM agents, supporting task continuity and coherence over long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HiAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical agent that uses sub-goals as chunked working memory to maintain long-horizon task coherence (described in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>long-horizon agent tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring hierarchical decomposition and long-horizon planning; HiAgent uses chunked sub-goal memories to manage continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon planning / agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory implemented as memory chunks (sub-goals)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Sub-goal chunks stored and used to manage working memory; details referenced from HiAgent (not experimentally evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Sub-goal chunks (segments representing sub-tasks/goals).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as motivating example for chunk memory; no ablation/results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an approach that uses chunked sub-goals to manage working memory for long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8178.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8178.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arigraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arigraph: Learning knowledge graph world models with episodic memory for LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced related work that adopts knowledge triples combining semantic and episodic memories to store factual and detailed information, suitable for complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Arigraph: Learning knowledge graph world models with episodic memory for llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Arigraph</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that constructs and uses knowledge-graph style triples (episodic + semantic) as memory to support reasoning (described in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>complex reasoning / multi-hop QA (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require relational reasoning and world-model style memory via knowledge triples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge graph-based reasoning / multi-hop QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge triples (graph/episodic-semantic hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Storage of triples representing entity relations and events; referenced as beneficial for complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triples <head; relation; tail> capturing facts and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as motivating the triples memory structure; no direct experimental ablation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as example where triple-structured memory supports precise relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8178.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8178.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphReader (Graphreader: Building graph-based agent to enhance long-context abilities of LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as a graph-based agent that improves precision in multi-hop QA by organizing information in graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphreader: Building graph-based agent to enhance long-context abilities of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A graph-based agent approach that constructs structured graph memories to enhance long-context reasoning and precision in multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-hop question answering / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Organize long-context information into graphs to improve multi-hop reasoning precision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop QA / graph-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-structured memory / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Graph construction from documents and use of graph-structured memories (cited; not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Nodes/edges representing entities and relations (graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior work motivating structured memory approaches; no internal ablation data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as improving precision in multi-hop QA via graph-structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8178.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8178.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent/GraphReader-variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReadAgent / ReadAgent-like gist-memory approaches (cited: ReadAgent / A human-inspired reading agent with gist memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach that compresses memory episodes into gist-like summaries and organizes them in a structured memory directory to support long-context reading tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A human-inspired reading agent with gist memory of very long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent / gist-memory agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that compress episodes into concise summaries (gist memory) for long-context reading and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reading comprehension / long-context QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compress large contexts into summaries to serve as memory for downstream QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reading comprehension / long-context summarization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summaries / gist memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Document-level summaries generated by LLM prompts and stored as condensed memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Summaries capturing global document content and key details.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as motivation and example of summary-based memory; no experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Summaries are effective for abstraction-heavy or long-context tasks (cited and empirically supported in this paper's comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8178.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8178.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT / MemGPT-like memory-operating systems for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited system (MemGPT) discussed in related work as an example where memory is central to agent operation (LLMs as operating systems with memory modules).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-agent framework that integrates memory modules to support ongoing agent operation and stateful behavior (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general agent operation / long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use memory modules to manage state and long-term information for continuous agent operation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>agent memory management / long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory modules (general)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory store integrated into an agent OS-like architecture (referenced; not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as part of landscape of memory-equipped agents; no results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to motivate importance of memory in agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Structural Memory of LLM Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model <em>(Rating: 2)</em></li>
                <li>Arigraph: Learning knowledge graph world models with episodic memory for llm agents <em>(Rating: 2)</em></li>
                <li>A human-inspired reading agent with gist memory of very long contexts <em>(Rating: 2)</em></li>
                <li>Graphreader: Building graph-based agent to enhance long-context abilities of large language models <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>Generate-then-ground in retrieval-augmented generation for multi-hop question answering <em>(Rating: 1)</em></li>
                <li>Precise zero-shot dense retrieval without relevance labels <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8178",
    "paper_id": "paper-274965733",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "LLM-MemoryStudy",
            "name_full": "LLM-based agents with structured memory (this study)",
            "brief_description": "Experimental study of memory-augmented LLM agents evaluating four structural memory types (chunks, knowledge triples, atomic facts, summaries) plus a mixed combination, and three retrieval methods (single-step, reranking, iterative) across six long-context QA and reading/dialogue tasks using GPT-4o-mini-128k.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Unspecified LLM-based agent (this paper's evaluation)",
            "agent_description": "A generic LLM-based agent architecture that (1) generates structural memories from raw documents via prompts, (2) stores memories as embeddings, (3) retrieves relevant memories with a Retriever and optional LLM-based reranker/refiner, and (4) generates answers using Memory-Only or Memory-Doc contexts.",
            "model_name": "GPT-4o-mini-128k",
            "model_description": "The underlying LLM used for generation and retrieval-guided prompting in experiments; run with temperature 0.2 and 4k token input window; memory embeddings created with text-embedding-3-small.",
            "task_name": "HotPotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, Lo-CoMo, QuAL-ITY",
            "task_description": "A suite of six datasets covering multi-hop long-context QA (HotPotQA, 2WikiMultihopQA, MuSiQue), single-hop long-context QA (NarrativeQA), dialogue-based long-context QA (Lo-CoMo) and reading-comprehension (QuAL-ITY). Agents must retrieve relevant long-context information and answer questions requiring multi-step or long-context reasoning.",
            "task_type": "question answering / long-context reasoning / dialogue understanding / reading comprehension",
            "memory_used": true,
            "memory_type": "structural, retrieval-augmented memory (chunks, knowledge triples, atomic facts, summaries, and mixed union)",
            "memory_mechanism": "LLM-generated structured memories are embedded (text-embedding-3-small) and stored; retrieval is performed by a Retriever (semantic vector search) with three modes: single-step top-K, LLM-based reranking of candidates, and iterative retrieval with LLM query refinement; retrieved memories are prepended to the LLM either directly (Memory-Only) or used to locate original documents (Memory-Doc).",
            "memory_representation": "External documents transformed into: continuous text chunks, knowledge triples (&lt;head; relation; tail&gt;), atomic facts (concise fact sentences), summaries (condensed document descriptions), and Mixed = union of all types.",
            "memory_retrieval_method": "single-step semantic top-K retrieval; reranking via LLM prompt over initial candidates; iterative retrieval where retrieved memories refine the query via an LLM over N iterations then final retrieval.",
            "performance_with_memory": "Best reported: Mixed memory + Iterative retrieval — HotPotQA EM 67.00, F1 82.11; 2WikiMultihopQA EM 51.00, F1 68.15; MuSiQue EM 39.00, F1 61.38; NarrativeQA EM 12.50, F1 28.36; Lo-CoMo EM 7.85, F1 45.25; QuAL-ITY ACC 79.50. (Table 1)",
            "performance_without_memory": "Full Content baseline (no structured memory retrieval): HotPotQA EM 55.50, F1 75.77; 2WikiMultihopQA EM 44.00, F1 54.33; MuSiQue EM 36.00, F1 51.60; NarrativeQA EM 7.00, F1 24.99; Lo-CoMo EM 13.61, F1 41.82; QuAL-ITY ACC 81.50. (Table 1)",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Comprehensive comparative experiments across memory structures and retrieval methods (single-step, reranking, iterative); hyperparameter sweeps for K (retrieved top-K), R (reranked top-R), T (per-iteration retrieved), and N (iteration turns); robustness tests adding random noisy documents; comparison of two answer-generation approaches (Memory-Only vs Memory-Doc). Findings: iterative retrieval outperforms single-step and reranking generally; mixed memory outperforms individual structures overall and is most noise-resistant; chunks/summaries better for long-context tasks; triples/atomic facts better for relational/precision tasks; Memory-Doc helps extensive-context tasks while Memory-Only favors precision/multi-hop tasks.",
            "key_findings": "1) Mixed memory yields the most balanced and robust performance across tasks and noise. 2) Iterative retrieval consistently gives the best retrieval accuracy and downstream QA scores. 3) Memory structure should be matched to task: chunks & summaries for long-context/abstraction tasks, knowledge triples & atomic facts for relational and precision-demanding multi-hop QA. 4) Memory-Doc is preferable for extensive-context tasks; Memory-Only is preferable for precision-focused reasoning.",
            "limitations_or_challenges": "Experiments restricted to six datasets and four task types (no self-evolving or social simulation domains); noise experiments limited to random document noise (contradictory/irrelevant noise not explored); hyperparameter ranges constrained by compute; QuAL-ITY accuracy baseline higher for Full Content in some cases (e.g., QuAL-ITY ACC 81.50 vs mixed/memory ACC 79.50), showing memory does not universally improve every metric/task.",
            "uuid": "e8178.0",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "HiAgent",
            "name_full": "HiAgent: Hierarchical working memory management for LLM agents",
            "brief_description": "Related work referenced as using sub-goals as memory chunks to manage working memory in LLM agents, supporting task continuity and coherence over long-horizon tasks.",
            "citation_title": "Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model",
            "mention_or_use": "mention",
            "agent_name": "HiAgent",
            "agent_description": "Hierarchical agent that uses sub-goals as chunked working memory to maintain long-horizon task coherence (described in related work).",
            "model_name": null,
            "model_description": null,
            "task_name": "long-horizon agent tasks (general)",
            "task_description": "Tasks requiring hierarchical decomposition and long-horizon planning; HiAgent uses chunked sub-goal memories to manage continuity.",
            "task_type": "long-horizon planning / agent tasks",
            "memory_used": true,
            "memory_type": "working memory implemented as memory chunks (sub-goals)",
            "memory_mechanism": "Sub-goal chunks stored and used to manage working memory; details referenced from HiAgent (not experimentally evaluated in this paper).",
            "memory_representation": "Sub-goal chunks (segments representing sub-tasks/goals).",
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as motivating example for chunk memory; no ablation/results in this paper.",
            "key_findings": "Cited as an approach that uses chunked sub-goals to manage working memory for long-horizon tasks.",
            "limitations_or_challenges": null,
            "uuid": "e8178.1",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Arigraph",
            "name_full": "Arigraph: Learning knowledge graph world models with episodic memory for LLM agents",
            "brief_description": "Referenced related work that adopts knowledge triples combining semantic and episodic memories to store factual and detailed information, suitable for complex reasoning.",
            "citation_title": "Arigraph: Learning knowledge graph world models with episodic memory for llm agents",
            "mention_or_use": "mention",
            "agent_name": "Arigraph",
            "agent_description": "Agent that constructs and uses knowledge-graph style triples (episodic + semantic) as memory to support reasoning (described in related work).",
            "model_name": null,
            "model_description": null,
            "task_name": "complex reasoning / multi-hop QA (general)",
            "task_description": "Tasks that require relational reasoning and world-model style memory via knowledge triples.",
            "task_type": "knowledge graph-based reasoning / multi-hop QA",
            "memory_used": true,
            "memory_type": "knowledge triples (graph/episodic-semantic hybrid)",
            "memory_mechanism": "Storage of triples representing entity relations and events; referenced as beneficial for complex reasoning.",
            "memory_representation": "Triples &lt;head; relation; tail&gt; capturing facts and relations.",
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as motivating the triples memory structure; no direct experimental ablation in this paper.",
            "key_findings": "Cited as example where triple-structured memory supports precise relational reasoning.",
            "limitations_or_challenges": null,
            "uuid": "e8178.2",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphReader",
            "name_full": "GraphReader (Graphreader: Building graph-based agent to enhance long-context abilities of LLMs)",
            "brief_description": "Referenced as a graph-based agent that improves precision in multi-hop QA by organizing information in graph structures.",
            "citation_title": "Graphreader: Building graph-based agent to enhance long-context abilities of large language models",
            "mention_or_use": "mention",
            "agent_name": "GraphReader",
            "agent_description": "A graph-based agent approach that constructs structured graph memories to enhance long-context reasoning and precision in multi-hop QA.",
            "model_name": null,
            "model_description": null,
            "task_name": "multi-hop question answering / long-context reasoning",
            "task_description": "Organize long-context information into graphs to improve multi-hop reasoning precision.",
            "task_type": "multi-hop QA / graph-based reasoning",
            "memory_used": true,
            "memory_type": "graph-structured memory / knowledge graph",
            "memory_mechanism": "Graph construction from documents and use of graph-structured memories (cited; not evaluated here).",
            "memory_representation": "Nodes/edges representing entities and relations (graph).",
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as prior work motivating structured memory approaches; no internal ablation data in this paper.",
            "key_findings": "Cited as improving precision in multi-hop QA via graph-structured memory.",
            "limitations_or_challenges": null,
            "uuid": "e8178.3",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ReadAgent/GraphReader-variants",
            "name_full": "ReadAgent / ReadAgent-like gist-memory approaches (cited: ReadAgent / A human-inspired reading agent with gist memory)",
            "brief_description": "Referenced approach that compresses memory episodes into gist-like summaries and organizes them in a structured memory directory to support long-context reading tasks.",
            "citation_title": "A human-inspired reading agent with gist memory of very long contexts",
            "mention_or_use": "mention",
            "agent_name": "ReadAgent / gist-memory agents",
            "agent_description": "Agents that compress episodes into concise summaries (gist memory) for long-context reading and comprehension.",
            "model_name": null,
            "model_description": null,
            "task_name": "reading comprehension / long-context QA",
            "task_description": "Compress large contexts into summaries to serve as memory for downstream QA.",
            "task_type": "reading comprehension / long-context summarization",
            "memory_used": true,
            "memory_type": "summaries / gist memory",
            "memory_mechanism": "Document-level summaries generated by LLM prompts and stored as condensed memory entries.",
            "memory_representation": "Summaries capturing global document content and key details.",
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as motivation and example of summary-based memory; no experiments in this paper.",
            "key_findings": "Summaries are effective for abstraction-heavy or long-context tasks (cited and empirically supported in this paper's comparisons).",
            "limitations_or_challenges": null,
            "uuid": "e8178.4",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT / MemGPT-like memory-operating systems for LLMs",
            "brief_description": "Cited system (MemGPT) discussed in related work as an example where memory is central to agent operation (LLMs as operating systems with memory modules).",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "An LLM-agent framework that integrates memory modules to support ongoing agent operation and stateful behavior (cited in related work).",
            "model_name": null,
            "model_description": null,
            "task_name": "general agent operation / long-term memory",
            "task_description": "Use memory modules to manage state and long-term information for continuous agent operation.",
            "task_type": "agent memory management / long-term memory",
            "memory_used": true,
            "memory_type": "long-term memory modules (general)",
            "memory_mechanism": "Memory store integrated into an agent OS-like architecture (referenced; not evaluated here).",
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as part of landscape of memory-equipped agents; no results in this paper.",
            "key_findings": "Cited to motivate importance of memory in agent systems.",
            "limitations_or_challenges": null,
            "uuid": "e8178.5",
            "source_info": {
                "paper_title": "On the Structural Memory of LLM Agents",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model",
            "rating": 2,
            "sanitized_title": "hiagent_hierarchical_working_memory_management_for_solving_longhorizon_agent_tasks_with_large_language_model"
        },
        {
            "paper_title": "Arigraph: Learning knowledge graph world models with episodic memory for llm agents",
            "rating": 2,
            "sanitized_title": "arigraph_learning_knowledge_graph_world_models_with_episodic_memory_for_llm_agents"
        },
        {
            "paper_title": "A human-inspired reading agent with gist memory of very long contexts",
            "rating": 2,
            "sanitized_title": "a_humaninspired_reading_agent_with_gist_memory_of_very_long_contexts"
        },
        {
            "paper_title": "Graphreader: Building graph-based agent to enhance long-context abilities of large language models",
            "rating": 2,
            "sanitized_title": "graphreader_building_graphbased_agent_to_enhance_longcontext_abilities_of_large_language_models"
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Generate-then-ground in retrieval-augmented generation for multi-hop question answering",
            "rating": 1,
            "sanitized_title": "generatethenground_in_retrievalaugmented_generation_for_multihop_question_answering"
        },
        {
            "paper_title": "Precise zero-shot dense retrieval without relevance labels",
            "rating": 1,
            "sanitized_title": "precise_zeroshot_dense_retrieval_without_relevance_labels"
        }
    ],
    "cost": 0.01430625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Structural Memory of LLM Agents
17 Dec 2024</p>
<p>Ruihong Zeng 
University of Glasgow</p>
<p>Jinyuan Fang j.fang.2@research.gla.ac.uk 
University of Glasgow</p>
<p>Siwei Liu siwei.liu@abdn.ac.uk 
University of Aberdeen</p>
<p>Zaiqiao Meng zaiqiao.meng@glasgow.ac.uk 
University of Glasgow</p>
<p>On the Structural Memory of LLM Agents
17 Dec 2024637475C957B6B0C97C369F96409ED5ADarXiv:2412.15266v1[cs.CL]
Memory plays a pivotal role in enabling large language model (LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems.While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored.This paper investigates how memory structures and memory retrieval methods affect the performance of LLMbased agents.Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components.In addition, we evaluate three widely used memory retrieval methods: singlestep retrieval, reranking, and iterative retrieval.Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios.Our investigation aims to inspire further research into the design of memory systems for LLM-based agents. 1</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (Minaee et al., 2024) have attracted widespread attention in natural language tasks due to their remarkable capability.Recent advancements have significantly accelerated the development of LLM-based agents, with research primarily focusing on profile (Park et al., 2023;Hong et al.), planning (Qian et al., 2024;Qiao et al., 2024), action (Qin et al., 2023  et al., 2024c), self-evolving (Zhang et al., 2024a) and memory (Packer et al., 2023;Lee et al., 2024).These innovations have unlocked a wide range of applications across diverse applications (Li et al., 2023;Wang et al., 2024b;Chen et al., 2024).</p>
<p>A fundamental element that underpins the effectiveness of LLM-based agents is the memory module.In cognitive science (Simon and Newell, 1971;Anderson, 2013), memory is the cornerstone of human cognition, enabling the storage, retrieval, and drawing from past experiences for strategic thinking and decision-making.Similarly, the memory module is vital for LLM-based agents by facilitating the retention and organization of past interactions, supporting complex reasoning capabilities, e.g., multi-hop question answering (QA) (Li et al., 2024a;Lee et al., 2024), and ensuring consistency and continuity in user interactions (Nuxoll and Laird, 2007).</p>
<p>Developing an effective memory module in LLM-based agents typically involves two critical components: structural memory generation and memory retrieval methods (Wang et al., 2024a;Zhang et al., 2024b).Among the various memory structures used by agents, chunks (Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Lee et al., 2024) are the most prevalent.For instance, HiAgent (Hu et al., 2024) utilizes sub-goals as memory chunks to manage the working memory of LLM-based agents, ensuring task continuity and coherence, while Arigraph (Anokhin et al., 2024) adopts knowledge triples, which combine both semantic and episodic memories to store factual and detailed information, making it suitable for complex reasoning tasks.Meanwhile, ReadAgent (Li et al., 2024a) compresses memory episodes into gits memory with summaries manner, organizing them within a structured memory directory.</p>
<p>Upon reviewing the aforementioned memory structures, an important but under-explored question arises: Which memory structures are best suited for specific tasks, and how do their distinct characteristics impact the performance of LLM-based agents?This question mirrors how humans organize memory into distinct forms, such as episodic memory for recalling events and semantic memory for understanding relationships (Simon and Newell, 1971;Anderson, 2013).Each form serves a unique purpose, enabling humans to tackle a variety of challenges with flexibility and precision.Moreover, humans rely on effective retrieval processes to access relevant memories, ensuring the accurate recall of past experiences for problemsolving.This highlights the need to jointly explore memory structures and retrieval methods to enhance the reasoning capabilities and overall effectiveness of LLM-based agents.</p>
<p>To bridge this gap, we systematically explore the impact of various memory structures and retrieval methods in LLM-based agents.Specifically, we evaluate existing four types of memory structures: chunks (Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Li et al., 2024a).Building on these, we explore the potential of mixed memory structures, which combine multiple types of memories to examine whether their complementary characteristics can enhance performance.Additionally, we assess the robustness of these memory structures to noise, as understanding their reliability under such conditions is essential for ensuring effectiveness across diverse tasks.Furthermore, we investigate three memory retrieval methods, including single-step retrieval (Packer et al., 2023), reranking (Gao et al., 2023a), and iterative retrieval (Li et al., 2024b), to uncover how different combinations of retrieval methods and memory structures influence overall performance.</p>
<p>The main contributions of this work can be summarized as follows: (1) We present the first comprehensive study on the impact of memory struc-tures and memory retrieval methods in LLM-based agents on six datasets across four tasks: multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension.(2) Our findings reveal that mixed memory consistently achieves balanced and competitive performance across diverse tasks.Chunks and summaries excel in tasks involving extensive and lengthy context (e.g., reading comprehension and dialogue understanding), while knowledge triples and atomic facts are particularly effective for relational reasoning and precision in multihop and single-hop QA.Additionally, mixed memory demonstrates remarkable resilience to noise.</p>
<p>(3) Iterative retrieval stands out as the most effective memory retrieval method across most tasks, such as multi-hop QA, dialogue understanding and reading comprehension.</p>
<p>Related Works</p>
<p>LLM-based Agents</p>
<p>The advent of Large Language Model (LLM) has positioned them as a transformative step towards achieving Artificial General Intelligence (AGI) (Wang et al., 2024a), offering robust capabilities for the development of LLM-based agents (Xi et al., 2023;Xu et al., 2024).Current research in this field primarily focuses on agent planning (Wang et al., 2023;Yao et al., 2024;Qian et al., 2024;Qiao et al., 2024), reflection mechanisms (Shinn et al., 2024;Zhang et al., 2024a), external tools utilization (Qin et al., 2023;Wang et al., 2024c), self-evolving capabilities (Zhang et al., 2024a) and memory modules (Hu et al., 2024;Lee et al., 2024).</p>
<p>Memory Structures</p>
<p>Memory module serves as the foundation of LLMbased agents, enabling them to structure knowledge, retrieve relevant information, and leverage prior experiences for reasoning tasks (Zhang et al., 2024b).Among the widely adopted memory structures of memory module are chunks (Packer et al., 2023;Liu et al., 2023;Hu et al., 2024), knowledge triples (Anokhin et al., 2024), atomic facts (Li et al., 2024a), and summaries (Lee et al., 2024).For instance, HiAgent (Hu et al., 2024) incorporates sub-goals as memory chunks to maintain task continuity and coherence across interactions.On the other hand, GraphReader (Li et al., 2024a)   that improves precision in multi-hop question answering tasks.In this paper, we investigate how various memory structures impact the performance of LLM-based agents.</p>
<p>Memory Retrieval</p>
<p>The memory retrieval method is another critical component of the memory module, enabling LLMbased agents to retrieve relevant memories to advanced reasoning.To facilitate this, LLM-based agents often employ retrieval-augmented generation (RAG) (Lewis et al., 2020;Fang et al., 2024), where relevant memories are first retrieved and then used to generate answers with LLMs.In this setting, the retrieved memories are prepended to the queries and serve as input to the LLM to generate response (Ram et al., 2023).The most straightforward retrieval method is the single-step retrieval (Packer et al., 2023;Zhong et al., 2024), which aims to identify the Top-K most relevant memories for the query.Additionally, reranking (Gao et al., 2023a;Ji et al., 2024) leverages the language understanding capabilities of LLMs to prioritize retrieved memories, while iterative retrieval (Li et al., 2024b;Shi et al., 2024) focuses on reformulating queries to improve retrieval accuracy.These innovations make memory retrieval more adaptive and consistent with the query, maintaining effective performance across diverse and complex tasks.In this paper, we explore how different combinations of retrieval methods and memory structures influence overall performance.</p>
<p>Methodology</p>
<p>Figure 2 illustrates the overview of the memory module within LLM-based agents, highlighting three key components: Structural Memory Generation, Memory Retrieval Methods and Answer Generation.This section begins with an introduction to structural memory generation in § 3.1.Next, we introduce memory retrieval methods in § 3.2.Finally, § 3.3 discusses answer generation methods.</p>
<p>Structural Memory Generation</p>
<p>Structural memory generation enables agents to organize raw documents into structured representations.By transforming unstructured documents D q into structural memory M q , the agent gains the ability to store, retrieve, and reason over information more effectively.In this work, we explore four distinct forms of structural memory: chunks C q , knowledge triples T q , atomic facts A q , or summaries S q .The generation process for each structural memory is detailed as follows: Chunks (C q ).Chunks (Gao et al., 2023b) are a widely used form of structural memory in LLMbased agents.Each chunk represents a continuous segment of text from a document, typically constrained to a fixed number of tokens L. Formally, raw documents D q can be divided into a series of chunks, as defined: C q (D q ) = {c 1 , c 2 , . . ., c j }, where each chunk c j contains at most L tokens.</p>
<p>Chunks</p>
<p>Definition: Chunks are continuous, fixedlength segments of text from the document.Example: Generated chunks C q :</p>
<p>(1) Moneybomb (alternatively money bomb, money-bomb, or fundraising bomb) is a neologism coined in 2007;</p>
<p>(2) to describe a grassroots fundraising effort over a brief fixed time period.</p>
<p>Knowledge Triples (T q ).Knowledge triples represent a structured form of memory that captures semantic relationships between entities.Each triple is composed of three components: a head entity, a relation, and a tail entity, represented in the format ⟨head; relation; tail entity⟩.Following previous works (Anokhin et al., 2024;Fang et al., 2024), raw documents D q are processed by an LLM guided by a tailored prompt P T to generate a set of semantic triples T q .The generation process can be formally defined as: T q = LLM(D q , P T ).</p>
<p>Knowledge Triples</p>
<p>Definition: Knowledge triples capture relationships between entities.Example: Generated triples T q : (1) ⟨Moneybomb; type; neologism⟩;</p>
<p>(2) ⟨Moneybomb; coined in; 2007⟩.</p>
<p>Atomic Facts (A q ).Atomic facts are the smallest, indivisible units of information, presented as concise sentences that capture essential details.They represent a granular form of structural memory, simplifying raw documents by preserving critical entities, actions, and attributes.Following Li et al. (2024a), atomic facts are generated from raw documents D q using an LLM guided by a tailored prompt P A , formally denoted as: A q = LLM(D q , P A ).</p>
<p>Atomic Facts</p>
<p>Definition: Atomic facts are the smallest units of indivisible information.Example: Generated atomic facts A q :</p>
<p>(1) Moneybomb is also known as money bomb, money-bomb, or fundraising bomb;</p>
<p>(2) Moneybomb is a neologism.</p>
<p>Summaries (S q ).Summaries provide a condensed and comprehensive description of documents, cap-turing both global content and key details.Following Lee et al. (2024), summaries are generated from raw documents D q using an LLM guided by a tailored prompt P S , defined as: S q = LLM(D q , P S ).</p>
<p>Summaries</p>
<p>Definition: Summaries compress the document into a comprehensive description.Example: Generated summaries S q : Moneybomb, alternatively referred to as money bomb, money-bomb, or fundraising bomb, is a neologism coined in 2007.It describes a grassroots fundraising effort that occurs over a brief fixed time period.</p>
<p>Mixed (M Mixed q</p>
<p>). Mixed memories represent a composite form of structural memory, combining all the aforementioned types: chunks, knowledge triples, atomic facts, and summaries.This integration provides a comprehensive representation, formally defined as follows:
M Mixed q = C q ∪ T q ∪ A q ∪ S q .
Details of the prompts used by the LLM for generating each type of structural memory, e.g., P T , P A and P S , are provided in Appendix B.</p>
<p>Memory Retrieval Methods</p>
<p>Given the generated structural memories M q , we employ a memory retrieval method to identify and integrate the most relevant supporting memories M r ⊂ M q for the query q.Without this step, the agent would need to process all available memories, leading to inefficiency and potential inaccuracies due to irrelevant information.Our study mainly focuses on three retrieval approaches: single-step retrieval (Robertson et al., 2009;Rubin et al., 2022), reranking (Gao et al., 2023a;Ji et al., 2024), and iterative retrieval (Li et al., 2024b;Shi et al., 2024).The details of each memory retrieval method are outlined as follows:</p>
<p>Single-step Retrieval.In the single-step retrieval process, the goal is to identify the Top-K memories M r that are most relevant to the query q.This process is formally defined as: M r = Retriever(q, M q , K), where the Retriever (Robertson et al., 2009;Rubin et al., 2022) serves as the core component.</p>
<p>Reranking.In the reranking process (Gao et al., 2023a;Dong et al., 2024), an initial retriever selects a candidate set of Top-K memories M i , which are then reranked by an LLM prompted with P Rerank based on their relevance scores.From this reranked list, the Top-R memories M r , selected in descending order of relevance scores, are identified as the most relevant.This step enhances retrieval precision by leveraging the LLM to strengthen query-memory connections, filtering out irrelevant memories, and prioritizing the most pertinent memories for the query.This process is formally defined as: M r = LLM(q, M i , R, P R ) , where M i = Retriever(q, M q , K).</p>
<p>Iterative Retrieval.The iterative retrieval approach (Gao et al., 2023b) begins with an initial query q 0 = q and retrieves the Top-T most relevant structural memories M j .These retrieved memories are used to refine the query through an LLM prompted by P Refine .This process is repeated over N iterations, refining the query to produce the final version q N that is informative for retrieving relevant memories.Formally, the iterative retrieval process can be defined as follows: q j = LLM(M j , P Refine ), where M j = Retriever(q j−1 , M q , T ).After N iterations, the final refined query q N is used to retrieve the Top-K most relevant memories for answer generation.This step can be expressed as: M r = Retriever(q N , M q , K).The detailed prompts P Rerank and P Refine can be found in Appendix B.</p>
<p>Answer Generation</p>
<p>Finally, the agent leverages the LLM to generate the answer based on the retrieved memory.To achieve this, we propose two methods of answer generation.In the first method, termed Memory-Only, the retrieved memories M r are directly utilized as the context for generating the answer.The second method, termed Memory-Doc, uses the retrieved memories to locate their corresponding original documents from D q .These documents then serve as the context for answer generation, providing the agent with more detailed and contextually enriched information.</p>
<p>Experiments</p>
<p>4.1 Datasets.We conduct experiments on six datasets across four tasks.For multi-hop long-context QA datasets, we experiment with HotPotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022).The single-hop long-context QA task is evaluated with Narra-tiveQA (Kočiskỳ et al., 2018) from Longbench (Bai et al., 2023).Additionally, we leverage the Lo-CoMo dataset (Maharana et al., 2024) for dialoguebased long-context QA task, while the QuAL-ITY (Pang et al., 2022) dataset is used for the reading comprehension QA task 2 .</p>
<p>Evaluation.</p>
<p>To evaluate QA performance, we follow previous work (Li et al., 2024a) and use standard metrics such as Exact Match (EM) score and F1 score for the datasets HotPotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA and LoCoMo.For QuAL-ITY, we follow the approach in (Lee et al., 2024) and use accuracy as the evaluation metric, with 25% indicating chance performance.</p>
<p>Implementation Details.</p>
<p>In our experiments, we use GPT-4o-mini-128k with a temperature setting of 0.2.The input window is set to 4k tokens, while the maximum chunk size is up to 1k tokens.For text embedding, we employ the text-embedding-3-small model 3 from OpenAI and store the vectorized memories using LangChain (Chase, 2022).</p>
<p>Results and Analysis</p>
<p>Impact of Memory Structures</p>
<p>Finding 1: Mixed memories delivers more balanced performance.The results as presented in Table 1 reveal key insights into the impact of various memory structures on task performance:</p>
<p>(1) Mixed memories consistently outperform other memory structures.This is particularly evident under iterative retrieval, where mixed memories achieve the highest F1 scores of 82.11% on Hot-PotQA and 68.15% on 2WikiMultihopQA.(2) Chunks excel in tasks requiring a balance between concise and comprehensive contexts, as shown in datasets with long contexts.This is evidenced by its F1 score of 31.63% on NarrativeQA and an accuracy of 78.5% on QuALITY under reranking.Summaries, which condense large contexts, is effective for tasks demanding abstraction, as shown by its competitive F1 score of 32.26% on NarrativeQA and solid performance on LoCoMo.(3) Knowledge triples and atomic facts are particularly effective for relational reasoning and precision.Knowledge  triples achieve an F1 score of 62.06% on 2Wiki-MultihopQA under iterative retrieval, while atomic facts achieve an F1 score of 81.29% on HotPotQA.These findings emphasize the importance of tailoring memory structures to specific task requirements and demonstrate that integrating complementary memory types in mixed memories significantly enhances performance across tasks.</p>
<p>Impact of Memory Retrieval Methods</p>
<p>Finding 2: Iterative retrieval as the optimal retrieval method.The results in Table 1 demonstrate the significant influence of the retrieval method on performance: (1) Iterative retrieval consistently outperforms the others, achieving the highest scores across most datasets.Notably, with mixed memories, iterative retrieval achieved an F1 score of 82.11% on HotPotQA and 68.15% on 2WikiMul-tihopQA, showcasing its ability to refine queries iteratively for enhanced accuracy.(2) Reranking demonstrates strong performance on datasets with moderate complexity.For instance, it achieved F1 scores of 44.27% on LoCoMo and 28.19% on NarrativeQA with atomic fact memory.</p>
<p>(3) In contrast, single-step retrieval performs competitively in tasks requiring minimal contextual integration.Using summary memory, it achieved an F1 score of 32.93% on NarrativeQA, leveraging abstraction to extract coherent information.These findings emphasize the importance of aligning retrieval mechanisms with task requirements, and iterative retrieval excels in reasoning tasks.</p>
<p>Impact of Answer Generation Approaches</p>
<p>Finding 3: Extensive Context tasks favor Memory-Doc, while precision tasks benefit from Memory-Only.As shown in Figure 3, which compares their performance across various datasets.retrieving documents through retrieved memories provides a more comprehensive understanding, much like how humans integrate immediate recall with broader context to interpret complex narratives.In contrast, for datasets involving multi-hop reasoning and dialogue understanding, such as Hot-PotQA and LoCoMo, the Memory-Only approach proves to be the more effective strategy.These findings highlight that tasks requiring extensive context benefit from the Memory-Doc approach, which incorporates broader document-level information for enriched responses.On the other hand, tasks prioritizing precision are better suited to the Memory-Only approach, ensuring focused and accurate retrieval.</p>
<p>Hyperparameter Sensitivity</p>
<p>Effect of Number of Retrieved Memories K. We first evaluate the impact of K in single-step retrieval, with a limit of K = 200 due to computational resource limitations.As depicted in summaries improve up to K = 100 but then declined at K = 200, likely due to noise introduced by retrieving excessive memories.These findings indicate that the optimal K depends on both the dataset and memory structure.While moderate K values generally enhance performance, excessively large values can introduce irrelevant information, leading to a degraded performance.</p>
<p>Effect of Number of Reranked Memories R. To evaluate the impact of R in reranking, we investigate performance across a range of values, with a maximum R of 75 due to computational cost constraints, while fixing K at 100.As depicted in Figure 5, the results highlight that increasing the number of reranked memories does not always lead to better performance.For instance, chunks achieve the highest F1 score at R = 10 in Hot-PotQA, with a subsequent decline in performance beyond R = 50.This pattern is consistent with triples and atomic facts, indicating that selecting a smaller number of highly relevant memories can outperform retrieving and reranking larger sets, which often introduces noise.A similar trend can be observed in LoCoMo.These findings suggest that reranking is more effective when it focuses on a smaller subset of highly relevant memories.</p>
<p>Effect of Number of Retrieved Memories T on Each Iteration.We first investigate performance across a range of values of T using iterative retrieval, with a maximum T of 75 and N of 4 due to computational cost constraints while keeping K fixed at 100.As illustrated in Figure 6, increasing the number of retrieved memories per iteration generally improves performance across datasets, though the gains diminish beyond a certain threshold.For instance, in HotPotQA, atomic facts achieve an F1 score of approximately 81% at T = 50, with minimal additional gains from increasing T further.Similarly, in LoCoMo, chunks improve up to T = 50 before declining at T = 75.</p>
<p>These results indicate that while increasing T can enhance query refinement and performance, excessively large T values may introduce noise, ultimately reducing effectiveness.</p>
<p>Effect of Number of Iteration Turns N .Next, we examine the impact of iteration turns N , with the number of retrieved memories T fixed at 50.As depicted in Figure 6, the results reveal that increasing N initially enhances performance significantly, but the rate of improvement diminishes as N continues to rise.For HotPotQA, both triples and summars show notable gains from N = 1 to N = 3, after which the improvements become marginal.In the case of LoCoMo, triples, atomic facts, and summaries reach a peak at N = 3 and stop increasing afterwards.These results suggest that an intermediate number of iteration turns, typically between 2 and 3, achieves optimal performance improvements, striking a balance between maximizing effectiveness and minimizing resource expenditure.</p>
<p>Impact of Noise Documents</p>
<p>Finding 4: Mix memory excels in noise resilience.</p>
<p>Finally, we evaluate the robustness of various memory structures under increasing levels of noise using single-step retrieval with a fixed K = 100.As depicted in Figure 8, the performance of all memory structures declines as the number of noise documents increases.For HotPotQA, the mix memory consistently achieves the highest F1 scores, demon- strating superior resilience to noise.While triples and summaries exhibit similar rates of decline, the chunks experience a slower decline, maintaining a competitive F1 score when increasing the number of noise documents.A similar pattern is shown in LoCoMo.These findings reveal the robustness of the mixed memory structure, which consistently outperforms others across datasets, making it the most effective choice in noisy environments.</p>
<p>Conclusion &amp; Future Work</p>
<p>In this paper, we present the first comprehensive study on the impact of structural memories and memory retrieval methods in LLM-based agents, aiming to identify the most suitable memory structures for specific tasks and explore how retrieval methods influence performance.This study yielded several key findings: (1) Mixed memories consistently deliver balanced performance.Chunks and summaries excel in tasks involving lengthy contexts, such as reading comprehension and dialogue understanding, while knowledge triples and atomic facts are effective for relational reasoning and precision in multi-hop and single-hop QA.</p>
<p>(2) Mixed memories also demonstrate remarkable resilience to noise.(3) Iterative retrieval stands out as the most effective memory retrieval method, consistently outperforming in tasks such as multi-hop QA, dialogue understanding and reading comprehension.While these findings provide valuable insights, further research is needed to explore how memory impacts areas such as self-evolution and social simulation, highlighting the importance of investigating how structural memories and retrieval techniques support these applications.</p>
<p>Limitations</p>
<p>We identify the following limitations in our work:</p>
<p>(1) Our experiments are limited to tasks such as multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension, which restricts the applicability of our findings to other complex domains like self-evolving agents or social simulation.Investigating the role of memory structures and retrieval methods in these topics could provide broader insights;</p>
<p>(2) The evaluation of memory robustness primarily considers random document noise, leaving other challenging noise types, such as irrelevant or contradictory information, unexplored.Investigating these addition noise in future studies could offer a more comprehensive understanding of memory resilience; (3) Due to computational constraints, we limit the hyperparameter ranges (e.g., K, R, T , N ) in memory retrieval methods.Expanding these ranges in future research could yield deeper insights into their impact on performance.</p>
<p>Figure 1 :
1
Figure 1: The framework of LLM-based agents, where we focus on the study of memory modules, including memory structures and retrieval methods.</p>
<p>Figure 2 :
2
Figure 2: Overview of the memory module workflow in LLM-based agents.Raw information is organized into structural memories, which are processed through retrieval methods to identify the most relevant memories for the query, enabling the generation of precise and contextually enriched responses.</p>
<p>Figure 3 :
3
Figure 3: Performance across six datasets using two answer generation approaches: Memory-Only and Memory-Doc.</p>
<p>Figure 4 :
4
Figure 4: Performance of different numbers of retrieved memories K on HotPotQA and LoCoMo using singlestep retrieval.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Performance of different numbers of reranked memories R on HotPotQA and LoCoMo in reranking.</p>
<p>Figure 7 :
7
Figure 7: Performance of different numbers of retrieved memories N in each interaction on HotPotQA and Lo-CoMo using iterative retrieval.</p>
<p>Figure 8 :
8
Figure 8: Performance across varying numbers of noise documents using single-step retrieval.</p>
<p>Atomic Facts Fact: The</p>
<p>massive coordinate online donation drive was conducted on behalf of presidential candidate Ron Paul.
The term Moneybomb wascoined by Trevor Lyman todescribe a massive onlinedonation drive on behalf ofa presidential candidatethat was the first chairmanof what PAC?Structural MemoryChunksTriplesMoneybomb (alternatively money bomb, money-bomb, or fundraising bomb) is aMoneybombneologism coined in 2007 to describe aCoined TermSupportedgrassroots fundraising effort over a brieffixed time period, ...Trevor LymanRon PaulSummariesSummary: Trevor Lyman coined the term"moneybomb" to refer to a massive,coordinated online donation drive organizedin support of presidential candidate RonPaul, ...employsatomic facts to compress chunks into finer details,providing agents with highly granular information</p>
<p>User Structural Memory Top-K Retriever Single-step Retrieval Structural Memory Top-K Retriever Top-R Rerank Rerank Retrieval Structural Memory Top-T Retriever Iterative Retrieval Enhanced Query LLM N Turns Memory Retrieval Raw Information Moneybomb</p>
<p>(alternatively money bomb, money-bomb, or fundraising bomb) is a neologism coined in 2007 to describe a grassroots fundraising effort over a brief fixed time period, usually to support a candidate for election by dramatically increasing, ...</p>
<p>LLMCitizens for a Sound EconomyResponse</p>
<p>Table 1 :
1
Overall Performance (%) of various memory structures utilizing different retrieval methods across six datasets.The best performance is marked in boldface, while the second-best performance is underlined.
Memory StructureHotPotQA2WikiMultihopQAMuSiQueNarrativeQALoCoMoQuALITYEMF1EMF1EMF1EMF1EMF1ACCFull Content55.50 75.77 44.0054.3336.00 51.60 7.00 24.99 13.61 41.8281.50Single-step RetrievalChunks61.50 76.93 43.5059.1735.50 54.45 13.50 29.78 9.95 40.6376.00Triples59.50 74.09 44.5060.8231.00 50.13 11.50 22.04 8.42 41.0861.50Atomic Facts62.50 77.22 39.5058.6330.50 51.31 13.50 27.49 9.42 42.9271.50Summaries57.00 74.81 42.0057.2134.00 52.83 16.50 32.93 10.99 44.9476.00Mixed60.00 77.10 48.5065.2533.00 51.65 14.50 29.86 10.47 44.7378.00RerankingChunks63.00 77.35 45.0061.3137.00 55.32 16.00 31.63 9.95 43.4778.50Triples61.00 76.75 43.5055.4326.50 42.05 10.00 20.65 8.83 41.8260.00Atomic Facts63.00 78.31 40.5059.3128.50 49.95 14.00 28.19 8.90 44.2767.50Summaries61.00 77.80 45.0061.1835.50 54.59 16.00 32.26 12.04 44.8375.00Mixed65.00 78.58 45.5061.7734.00 52.45 11.98 28.02 9.42 44.5177.50Iterative RetrievalChunks63.00 79.10 46.5062.1337.00 56.78 14.50 30.88 10.47 45.1477.00Triples64.00 78.78 47.5062.0638.00 55.93 10.50 21.67 9.47 41.4160.50Atomic Facts65.50 81.29 44.0063.8934.50 57.55 14.50 28.28 9.95 43.6267.50Summaries60.50 78.11 46.5062.3533.50 53.12 17.00 31.79 12.04 43.9375.00Mixed67.00 82.11 51.0068.1539.00 61.38 12.50 28.36 7.85 45.2579.5072ChunksTriplesAtomic Facts Summaries
2 More details and statistics about the datasets are provided in Appendix A.3 https://platform.openai.com/docs/guides/embeddings/</p>
<p>A DatasetsWe conduct experiments on the following six datasets across four tasks, including multi-hop QA, single-hop QA, dialogue understanding and reading comprehension.The statistical information of datasets is provided in Table2.B PromptsIn this section, we present the prompts employed in our experiments, with detailed descriptions provided in the respective subsections.B.1 Prompt for Generating Knowledge TriplesThe prompt used for extracting knowledge triples from a document is illustrated in Figure9.B.2 Prompt for Generation SummariesThe prompt designed for generating document summaries is depicted in Figure10.B.3 Prompt for Generating Atomic FactsThe prompt for generating atomic facts from a document is shown in Figure11.B.4 Prompt for Reranking RetrievedMemories The prompt used for reranking retrieved memories is presented in Figure12.B.5 Prompt for Iterative Refining QueryThe prompt for iterative query refinement is provided in Figure13.You are a knowledge graph constructor tasked with extracting knowledge triples in the form of <head entity; relation; tail entity> from a document.Each triple denotes a specific relationship between entities or an event.The head entity and tail entity can be the provided title or phrases in the text.If multiple tail entities share the same relation with a head entity, aggregate these tail entities using commas.Format your output in the form of <head entity; relation; tail entity>.are a helpful assistant responsible for generating a comprehensive summary of the data provided below.Make sure to include information collected from all the documents.If the provided documents are contradictory, please resolve the contradictions and provide a single, coherent summary.Make sure it is written in third person, and include the names so we have the full context.Demonstrations{DOCUMENT}Summaries:Figure10: Prompt for generating summaries from a document.You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a context.Key Elements:The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text's narrative.Atomic Fact:The smallest, indivisible facts, presented as concise sentences.These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.Requirements:1. Ensure that all the atomic facts contain full and complete information, reflecting the entire context of the sentence without omitting any key details. 2. Ensure that all identified key elements are reflected within the corresponding atomic facts.3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).A list of documents is shown below.Each document has a number next to it along with a summary of the document.A question is also provided.Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well as the relevance score.The relevance score is a number from 1-10 based on how relevant you think the document is to the question.Respond with the numbers of <strong>all</strong> the documents along with a relevance score.Follow the examples to answer the input question by reasoning step-by-step.Output both reasoning steps and the answer.DemonstrationsDemonstrations:##### Question: Nobody Loves You was written by John Lennon and released on what album that was issued by Apple Records, and was written, recorded, and released during his 18 month separation from Yoko Ono? Thought: The album issued by Apple Records, and written, recorded, and released during John Lennon's 18 month separation from Yoko Ono is Walls and Bridges.Nobody Loves You was written by John Lennon on Walls and Bridges album.So the answer is: Walls and Bridges.Question: What is known as the Kingdom and has National Route 13 stretching towards its border?Thought: Cambodia is officially known as the Kingdom of Cambodia.National Route 13 streches towards border to Cambodia.So the answer is: Cambodia.Question: Jeremy Theobald and Christopher Nolan share what profession?Thought: Jeremy Theobald is an actor and producer.Christopher Nolan is a director, producer, and screenwriter.Therefore, they both share the profession of being a producer.So the answer is: producer.
The architecture of cognition. John R Anderson, 2013Psychology Press</p>
<p>Arigraph: Learning knowledge graph world models with episodic memory for llm agents. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev, arXiv:2407.043632024arXiv preprint</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, arXiv:2308.145082023arXiv preprint</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, arXiv:2404.18231From persona to personalization: A survey on role-playing language agents. 2024arXiv preprint</p>
<p>Don't forget to connect! improving rag with graph-based reranking. Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, Anton Tsitsulin, arXiv:2405.184142024arXiv preprint</p>
<p>TRACE the evidence: Constructing knowledge-grounded reasoning chains for retrievalaugmented generation. Jinyuan Fang, Zaiqiao Meng, Craig Macdonald, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Precise zero-shot dense retrieval without relevance labels. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023a1</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972023barXiv preprint</p>
<p>Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Metagpt: Meta programming for a multi-agent collaborative framework. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, The Twelfth International Conference on Learning Representations. </p>
<p>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo, arXiv:2408.095592024arXiv preprint</p>
<p>Dynamic and textual graph generation via largescale llm-based agent simulation. Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding, arXiv:2410.098242024arXiv preprint</p>
<p>The narrativeqa reading comprehension challenge. Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette, Transactions of the Association for Computational Linguistics. 62018</p>
<p>A human-inspired reading agent with gist memory of very long contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F Canny, Ian Fischer, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024OpenReview.net</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, arXiv:2406.14550Graphreader: Building graph-based agent to enhance long-context abilities of large language models. 2024aarXiv preprint</p>
<p>Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks. Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024b</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, arXiv:2310.06500Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Thinkin-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>Evaluating very long-term conversational memory of LLM agents. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196Large language models: A survey. 2024arXiv preprint</p>
<p>Extending cognitive architecture with episodic memory. M Andrew, John E Nuxoll, Laird, AAAI. 2007</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Quality: Question answering with long input texts, yes!. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Chatdev: Communicative agents for software development. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, Transactions of the Association for Computational Linguistics. 112023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Generate-then-ground in retrieval-augmented generation for multi-hop question answering. Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsLong Papers20241</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Human problem solving: The state of the theory in 1970. A Herbert, Allen Simon, Newell, American psychologist. 2621451971</p>
<p>Musique: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 6182024a</p>
<p>Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. Xintao Wang, Yunze Xiao, Jen-Tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024b1</p>
<p>Rcagent: Cloud root cause analysis by autonomous agents with tool-augmented large language models. Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, Qingsong Wen, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024c</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao, arXiv:2404.14741Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering. 2024arXiv preprint</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu, arXiv:2402.17574Agentpro: Learning to evolve via policy-level reflection and optimization. 2024aarXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024barXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>            </div>
        </div>

    </div>
</body>
</html>
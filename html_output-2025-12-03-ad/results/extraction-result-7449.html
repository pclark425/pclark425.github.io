<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7449 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7449</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7449</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-17a6116e5bbd8b87082cbb2e795885567300c483</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483" target="_blank">Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work focuses on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting, and presents a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7449.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7449.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FORMATSPREAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FORMATSPREAD (Bayesian exploration of prompt formats)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm and grammar for efficiently sampling and estimating the performance interval (min/max) over a set of semantically-equivalent prompt formats under a fixed evaluation budget using Bayesian (Thompson) sampling; does not require access to model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to multiple LLMs (evaluated on LLaMA-2, Falcon, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FORMATSPREAD is model-agnostic (black-box) and is used to query autoregressive LMs via inference; experiments in the paper evaluate open-source LLaMA-2 variants, Falcon variants, and GPT-3.5-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 13B, 70B, GPT-3.5 API)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 classification and multiple-choice tasks (subset of Super-NaturalInstructions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse classification and multiple-choice tasks drawn from Super-NaturalInstructions (binary/3/4-class classifications and multiple-choice), evaluated with ranking accuracy and exact prefix matching.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompting: instruction + n few-shot formatted examples + query, where the few-shot and query are formatted according to a sampled prompt format from a defined grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Grammar-defined variations (separators, casing, enumerations, spacing, delimiters, item wrappers); few-shot (1 or 5 shots) vs zero-shot experiments; ranking accuracy and exact-prefix matching used as metrics; FORMATSPREAD samples up to hundreds of plausible formats and allocates an evaluation budget E (mini-batch size B) using Thompson sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>performance spread (max - min) measured in accuracy (ranking accuracy or exact-prefix matching); also reports median/mean spreads across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported capability: with a budget of 51,200 evaluations, Thompson sampling estimates spread within 1 accuracy point of true spread (320 formats, B=20 example shown).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Black-box inference; Thompson sampling with Beta priors initialized using original-format accuracy; evaluation budgets and number of sampled formats vary (e.g., 320 formats, E up to 40,000); B (mini-batch) typically 20 or 250 in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7449.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-format sensitivity (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate sensitivity of LLMs to semantically-equivalent prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across 53 Super-NaturalInstructions classification tasks and multiple models, semantically-equivalent prompt formatting choices cause large variance in measured task accuracy; this sensitivity persists across model sizes, instruction tuning, and few-shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-{7B,13B,70B}, Falcon-7B, Falcon-7B-Instruct, GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LMs (open-source LLaMA-2 and Falcon variants; GPT-3.5 as API model). Includes instruction-tuned variants (e.g., Falcon-7B-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B, GPT-3.5 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 tasks from Super-NaturalInstructions (classification and multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Human-written instruction tasks with 2/3/4 basic fields; each task evaluated on assumed dataset size 1,000; few-shot examples fixed per task when varying format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot templates built from a grammar of plausible formats; each format is semantically equivalent by grammar but differs in separators, casing, enumerations, joining characters, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Experiments sample 10–320 formats per task; use 1-shot and 5-shot settings; instruction string concatenated with formatted few-shot examples using '\n\n' spacing; ranking accuracy (primary) and exact prefix matching (secondary) reported; also open-ended tasks with ROUGE/BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (probability ranking / exact-prefix matching); also reporting spread (max-min) across formats</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Median spread across tasks and models: ~7.5 accuracy points (absolute) when sampling 10 formats; across many settings median ~7.5 points, with ~10 points on average across 50+ tasks and models (paper reports ~10 points average); spread observed up to 76 percentage points for some model/task/format combinations (see LLaMA-2-13B), and many tasks show double-digit absolute differences.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Observed absolute spread per task across formats often ~7.5 points median; extremes up to 76 points absolute for specific cases (paper-reported maximum).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1- and 5-shot few-shot prompting; ranking accuracy typically used; sampling of formats from grammar; budgets vary (10 sampled formats for lower-bound analyses; up to 320 formats for exhaustive FORMATSPREAD trials).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7449.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B max spread</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extremely large format-induced performance change observed for LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that formatting choices between semantically-equivalent prompts can cause up to a 76 percentage-point difference in accuracy for LLaMA-2-13B on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive LLaMA-2 model (13B parameters); used in few-shot classification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of Super-NaturalInstructions classification tasks (specific task(s) not always enumerated for the 76-point max in the text)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification / multiple-choice tasks where only prompt formatting differs (semantically-equivalent formats sampled from grammar).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot templated prompts; semantic equivalence ensured by grammar but with different separators, casing, delimiters, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Reported spread observed when sampling plausible equivalent formats (number of formats/sample size not always specified for the single maximum but aggregated experiments use 10–320 formats; ranking accuracy metric typically used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (ranking accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Up to 76 percentage points absolute difference in accuracy between equivalent formats (e.g., from near 0% to ~76% on the same task under different formats).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Up to -/+76 percentage points absolute between worst and best formats for a given task.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot (1- and 5-shot analyzed); comparisons across sampled formats from grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7449.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B task280 atomic change</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atomic formatting change causing huge accuracy swing on task280 (LLaMA-2-7B, 1-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On Super-NaturalInstructions task280, a very small formatting change (removing ':' separators) caused LLaMA-2-7B 1-shot ranking accuracy to jump from 4.3% to 82.6% (absolute increase of 78.3 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive LLaMA-2 base model (7B parameters), evaluated in 1-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task280 (StereoSet-inspired stereotyping classification in Super-NaturalInstructions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a short passage, classify it into one of four stereotype/anti-stereotype types (gender, profession, race, religion).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot few-shot prompt where changing a separator in the field template (e.g., from 'passage: <text> || answer: <text>' to 'passage <text> || answer <text>') is the only change.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>atomic prompt formatting (separator punctuation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Single-constant atomic change: removing ':' (colon) separators between descriptor and text placeholder; few-shot count = 1; evaluated with probability ranking accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probability-ranking accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Format p1: 4.3% accuracy; Format p2: 82.6% accuracy (absolute difference 78.3 percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Format p1 (with ':' separators) = 4.3% accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+78.3 percentage points absolute (from p1 to p2)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1-shot LLaMA-2-7B; evaluated on 250 samples for these format comparisons (table derived from analysis over sampled formats).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7449.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B other atomic examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selected atomic-format changes producing large accuracy differences (LLaMA-2-7B, 1-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple tasks show single-constant changes (e.g., adding/removing colons, changing item numbering/casing) producing large absolute accuracy jumps (examples include task317, task190, task904, task320, task322, task279).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLaMA-2 7B evaluated on 1-shot classification tasks from Super-NaturalInstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task317, task190, task904, task320, task322, task279 (examples from Table 2/4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various classification/multiple-choice tasks from Super-NaturalInstructions where single-constant formatting changes impacted model outputs greatly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot few-shot prompts with atomic formatting changes (separators, enumerations, casing changes, newline placement).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>atomic prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Examples: task317 (Passage/Answer colon spacing) improved from 7.6% to 63.8% (diff 56.2 points); task190 (enumeration format change) +25.4 points; task904 (input/output separator change) +19.8 points; task320 (changing semicolons/newlines) +11.5 points; task322 (casing COMMENT vs comment) +10.0 points; task279 (ALL CAPS vs Title) +6.9 points. Metrics shown are probability ranking unless noted otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probability-ranking accuracy (table derived values)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: task317: 7.6% -> 63.8% (+56.2 pp); task190: 36.0% -> 61.4% (+25.4 pp); task904: 41.8% -> 61.6% (+19.8 pp); task320: 36.1% -> 47.6% (+11.5 pp); task322: 61.4% -> 71.4% (+10.0 pp); task279: 37.2% -> 44.1% (+6.9 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>lower-performing format listed for each example (see performance_value)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Absolute increases per example listed above (range ~6.9 to 56.2 percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1-shot LLaMA-2-7B; probability-ranking metric; comparisons derived from sampled format evaluations (table of atomic changes).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7449.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-comparison reversal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Format-dependent reversal of comparative model rankings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The relative ordering of model performance can be reversed solely by changing prompt format: e.g., LLaMA-2-13B vs LLaMA-2-70B and LLaMA-2-7B vs Falcon-7B show nontrivial probabilities of reversal when using different formats, and many reversals are statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pairs: LLaMA-2-13B vs LLaMA-2-70B; LLaMA-2-7B vs Falcon-7B (also other pairs analyzed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparative evaluation across open-source autoregressive LMs under sampled plausible prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B vs 70B; 7B vs 7B (Falcon)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (1- and 5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks evaluated across different models and different prompt formats; compare probability that M's superiority under one format is reversed under another format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts sampled from grammar; compare across formats p and p'.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt formatting / model comparison methodology</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Define threshold d (e.g., d=0.02). Compute probability that M' beats M by >= d using format p' given M beat M' by >= d using format p. Performed over 53 tasks and 1- and 5-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probability that ordering is reversed; also used paired significance tests (one-sided McNemar/pairwise chi-squared) across same samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: LLaMA-2-13B vs -70B reverse trend by at least d=0.02 with probability 0.141; LLaMA-2-7B vs Falcon-7B reverse trend with probability 0.140. In many cases both comparisons were statistically significant (p < 0.05): 76% and 47% respectively for two reported model-comparison sets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Probability of reversal ~0.14 for reported pairs; percentage of reversals that were statistically significant in paired tests reported as 76% and 47% for two model-comparison instances.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1- and 5-shot; ranking accuracy; 1000-sample statistical tests (paired McNemar tests) used to assess significance.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Many reversals were statistically significant under paired one-sided McNemar tests (p < 0.05); specific rates: 76% and 47% for two reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7449.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 spread (320 formats)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed prompt-format sensitivity in GPT-3.5-Turbo (API) across many sampled formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using FORMATSPREAD on GPT-3.5 across 320 plausible formats and 53 tasks, the paper reports a median spread of 6.4 accuracy points and observed spreads up to 56 accuracy points (using exact prefix matching because logits were not available).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-gated autoregressive LLM (ChatGPT family); evaluated via black-box queries (no access to logits).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed (API model; commonly referred to as GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 53 classification tasks; prompts sampled from grammar; evaluated with exact-prefix matching for GPT-3.5 (no logits available for ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt templates sampled from grammar (320 formats), 1-shot setting often reported in FORMATSPREAD trials.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>FORMATSPREAD run with 320 formats and a budget (examples: under $10 average cost per task reported); used exact prefix matching metric for GPT-3.5 because logits were inaccessible for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact prefix matching accuracy; spread (max-min)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Median spread = 6.4 percentage points absolute across 53 tasks; maximum observed spread up to 56 percentage points for some task/format combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Median +6.4 pp spread across formats; occasional up to +56 pp between worst and best formats.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>320 sampled formats; black-box querying of API; exact prefix matching metric used for GPT-3.5; reported average cost under $10 per task for evaluating 320 formats across 53 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7449.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B quantized spreads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Format sensitivity observed in LLaMA-2-70B (4-bit quantized) across 320 formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Even with a large model (LLaMA-2-70B) and quantized 4-bit inference, FORMATSPREAD finds substantial spreads: median spread 0.171 (17.1 percentage points), mean 0.221, std 0.200; 25% of tasks had spread >=0.292 and maximum spread 0.876 (87.6 percentage points) using probability ranking across 53 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B (4-bit quantized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open-source autoregressive model, evaluated under 4-bit quantized inference (to reduce inference costs) and tested with FORMATSPREAD.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of classification tasks sampled from Super-NaturalInstructions; formats sampled from grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot templated prompts sampled from the grammar (320 plausible formats)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>FORMATSPREAD with budget of 40,000 evaluations across 320 formats; probability-ranking accuracy used for LLaMA-2-70B experiments since logits accessible; reported median/mean/std/percentiles and maximum spread values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probability-ranking accuracy; spread measured in fraction [0,1] converted to percentage points in description</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Median spread = 0.171 (17.1 percentage points); mean = 0.221 (22.1 pp); std = 0.200; 25th percentile of tasks had spread >= 0.292 (29.2 pp); maximum observed spread = 0.876 (87.6 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Per-task best-worst format differences as above (example: up to 87.6 pp between worst and best formats for one task).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1-shot and 5-shot analyses reported; probability ranking metric; evaluation budget E = 40,000 across 320 formats; 4-bit quantized inference used.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7449.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atomic-feature impacts (S1, item2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent contribution analysis of atomic prompt-format features (separators, casing, item numbering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis conditioning on individual constant choices finds that separators between descriptor and placeholder (S1) and number-format changes in enumerations (F_item2) most strongly predict accuracy differences, whereas other constants (S2, item1 wrappers, casing) do not reliably predict performance alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (primary for this analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLaMA-2 7B used to measure distributional effects of atomic feature choices across sampled formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>31 tasks (subset used for this detailed feature analysis), sampled formats</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks from Super-NaturalInstructions used to estimate how single-constant choices condition the accuracy distribution over sampled formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts generated by grammar with controlled conditioning on a single constant choice (e.g., fix S1 to ':', fix F_item2 to letters vs numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>atomic prompt formatting (feature-level)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Computed accuracy for 500 random formats with 250 samples each on 31 tasks for 1-shot LLaMA-2-7B; evaluated whether conditioned accuracy distributions are 'weakly' or 'strongly' different (boxplot quartile and whisker criteria). Table reports percentage of tasks where constants yield weak/strong differences: e.g., S1 weak diff 43% of tasks, strong diff 22%; F_item2 weak diff 45%, strong diff 10%; F_casing weak diff 3%, strong diff 0%; S2 and F_item1 show 0% in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>probability-ranking accuracy distributions (boxplot comparisons), counts of tasks showing weak/strong differences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: S1 median spread 0.132; S1 yields weak distributional differences in 43% of tasks and strong differences in 22% of tasks. F_item2 median spread 0.173 with 45% weak / 10% strong differences. Other constants often show 0% strong differences.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Atomic features S1 and F_item2 can yield notable absolute accuracy shifts (median spreads ~0.132–0.173).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1-shot LLaMA-2-7B; 500 format samples; 250 eval samples per format for distributional estimates; classification via ranking accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7449.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atomic-change probability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability that an atomic formatting change yields large accuracy change</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured over many sampled atomic changes across tasks, a non-trivial fraction produce meaningful accuracy swings: 24% of atomic changes cause >=5 percentage-point change under exact prefix matching; 11% under probability ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (primary reported numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLaMA-2-7B measured on sampled atomic format perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 tasks (analysis aggregated across tasks), multiple atomic-change samples</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Atomic change = single-constant perturbation of a prompt format (e.g., change casing, a separator, an item wrapper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Atomically perturbed few-shot prompt formats sampled from the grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>atomic prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Sampled atomic changes and measured the resulting change in accuracy using both exact-prefix matching and probability-ranking metrics; plotted distribution of impact magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fraction of atomic changes exceeding given absolute accuracy thresholds (e.g., >=5 percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24% of atomic changes have >=5 percentage-point impact under exact prefix matching; 11% under probability ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Non-negligible probability (11–24%) that a single-constant change causes >=5 pp accuracy difference depending on metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multiple tasks, sampled atomic changes per task (e.g., 30 sampled atomic changes per task in some plots); both exact-prefix and ranking metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7449.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7449.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding identifiability correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-format identifiability in prompt embeddings and correlation with performance spread</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt formats are highly identifiable from the model's last hidden (prompt) embedding — XGBoost on top principal components achieves >=0.98 format classification accuracy using top 100 PCs — and separability (classifier accuracy on top 2 PCs) moderately correlates with observed performance spread (r=0.424 for 1-shot, r=0.555 for 5-shot, exact-prefix matching).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (embedding analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Analyze last hidden layer activations (pre-generation) as prompt embeddings; dimensionality 4096, PCA applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (hidden dim 4096)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>31 tasks analyzed for embedding identifiability; 10 formats sampled per task; 1000 evaluation examples per format collected</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure whether a classifier can predict which prompt format generated a given prompt embedding and whether that separability correlates with performance spread across formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Various few-shot formatted prompts sampled from grammar; embeddings are computed over entire prompt immediately before generating the first token.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>internal representation / prompt formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Collected 8000 prompt embeddings (800 per format * 10 formats) for training and 200 per format for evaluation; used top-n PCA components (as low as 2) and trained XGBoost classifier to predict format identity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>format-classification accuracy from embeddings; Pearson correlation between classifier accuracy (top 2 PCs) and performance spread</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using top 100 PCs, classifier accuracy >= 0.98 for all 31 tasks; correlation between 2-PC classifier accuracy and spread: r=0.424 (p=8.04e-6) for 1-shot (exact-prefix matching), r=0.555 for 5-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Format embeddings separability moderately predicts larger performance spread; high identifiability implies format is a deterministic transformation of prompt embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LLaMA-2-7B; 10 formats per task; 1-shot and 5-shot settings analyzed; PCA applied to 4096-dim embeddings; XGBoost classifier used.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Correlation p-values reported (e.g., p=8.04e-6 for r=0.424).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>GrIPS: Gradient-free, edit-based instruction search for prompting large language models <em>(Rating: 1)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 1)</em></li>
                <li>Prompt waywardness: The curious case of discretized interpretation of continuous prompts <em>(Rating: 1)</em></li>
                <li>Jailbroken: How does llm safety training fail? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7449",
    "paper_id": "paper-17a6116e5bbd8b87082cbb2e795885567300c483",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "FORMATSPREAD",
            "name_full": "FORMATSPREAD (Bayesian exploration of prompt formats)",
            "brief_description": "An algorithm and grammar for efficiently sampling and estimating the performance interval (min/max) over a set of semantically-equivalent prompt formats under a fixed evaluation budget using Bayesian (Thompson) sampling; does not require access to model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to multiple LLMs (evaluated on LLaMA-2, Falcon, GPT-3.5)",
            "model_description": "FORMATSPREAD is model-agnostic (black-box) and is used to query autoregressive LMs via inference; experiments in the paper evaluate open-source LLaMA-2 variants, Falcon variants, and GPT-3.5-Turbo.",
            "model_size": "various (7B, 13B, 70B, GPT-3.5 API)",
            "task_name": "53 classification and multiple-choice tasks (subset of Super-NaturalInstructions)",
            "task_description": "Diverse classification and multiple-choice tasks drawn from Super-NaturalInstructions (binary/3/4-class classifications and multiple-choice), evaluated with ranking accuracy and exact prefix matching.",
            "problem_format": "Few-shot prompting: instruction + n few-shot formatted examples + query, where the few-shot and query are formatted according to a sampled prompt format from a defined grammar.",
            "format_category": "prompt style / prompt formatting",
            "format_details": "Grammar-defined variations (separators, casing, enumerations, spacing, delimiters, item wrappers); few-shot (1 or 5 shots) vs zero-shot experiments; ranking accuracy and exact-prefix matching used as metrics; FORMATSPREAD samples up to hundreds of plausible formats and allocates an evaluation budget E (mini-batch size B) using Thompson sampling.",
            "performance_metric": "performance spread (max - min) measured in accuracy (ranking accuracy or exact-prefix matching); also reports median/mean spreads across tasks",
            "performance_value": "Reported capability: with a budget of 51,200 evaluations, Thompson sampling estimates spread within 1 accuracy point of true spread (320 formats, B=20 example shown).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Black-box inference; Thompson sampling with Beta priors initialized using original-format accuracy; evaluation budgets and number of sampled formats vary (e.g., 320 formats, E up to 40,000); B (mini-batch) typically 20 or 250 in some analyses.",
            "statistical_significance": null,
            "uuid": "e7449.0",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prompt-format sensitivity (aggregate)",
            "name_full": "Aggregate sensitivity of LLMs to semantically-equivalent prompt formatting",
            "brief_description": "Across 53 Super-NaturalInstructions classification tasks and multiple models, semantically-equivalent prompt formatting choices cause large variance in measured task accuracy; this sensitivity persists across model sizes, instruction tuning, and few-shot counts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-{7B,13B,70B}, Falcon-7B, Falcon-7B-Instruct, GPT-3.5-Turbo",
            "model_description": "Autoregressive transformer LMs (open-source LLaMA-2 and Falcon variants; GPT-3.5 as API model). Includes instruction-tuned variants (e.g., Falcon-7B-Instruct).",
            "model_size": "7B, 13B, 70B, GPT-3.5 (API)",
            "task_name": "53 tasks from Super-NaturalInstructions (classification and multiple-choice)",
            "task_description": "Human-written instruction tasks with 2/3/4 basic fields; each task evaluated on assumed dataset size 1,000; few-shot examples fixed per task when varying format.",
            "problem_format": "Few-shot templates built from a grammar of plausible formats; each format is semantically equivalent by grammar but differs in separators, casing, enumerations, joining characters, etc.",
            "format_category": "prompt style / prompt formatting",
            "format_details": "Experiments sample 10–320 formats per task; use 1-shot and 5-shot settings; instruction string concatenated with formatted few-shot examples using '\\n\\n' spacing; ranking accuracy (primary) and exact prefix matching (secondary) reported; also open-ended tasks with ROUGE/BERTScore.",
            "performance_metric": "accuracy (probability ranking / exact-prefix matching); also reporting spread (max-min) across formats",
            "performance_value": "Median spread across tasks and models: ~7.5 accuracy points (absolute) when sampling 10 formats; across many settings median ~7.5 points, with ~10 points on average across 50+ tasks and models (paper reports ~10 points average); spread observed up to 76 percentage points for some model/task/format combinations (see LLaMA-2-13B), and many tasks show double-digit absolute differences.",
            "baseline_performance": null,
            "performance_change": "Observed absolute spread per task across formats often ~7.5 points median; extremes up to 76 points absolute for specific cases (paper-reported maximum).",
            "experimental_setting": "1- and 5-shot few-shot prompting; ranking accuracy typically used; sampling of formats from grammar; budgets vary (10 sampled formats for lower-bound analyses; up to 320 formats for exhaustive FORMATSPREAD trials).",
            "statistical_significance": null,
            "uuid": "e7449.1",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-13B max spread",
            "name_full": "Extremely large format-induced performance change observed for LLaMA-2-13B",
            "brief_description": "The paper reports that formatting choices between semantically-equivalent prompts can cause up to a 76 percentage-point difference in accuracy for LLaMA-2-13B on some tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_description": "Open-source autoregressive LLaMA-2 model (13B parameters); used in few-shot classification experiments.",
            "model_size": "13B",
            "task_name": "Subset of Super-NaturalInstructions classification tasks (specific task(s) not always enumerated for the 76-point max in the text)",
            "task_description": "Classification / multiple-choice tasks where only prompt formatting differs (semantically-equivalent formats sampled from grammar).",
            "problem_format": "Few-shot templated prompts; semantic equivalence ensured by grammar but with different separators, casing, delimiters, etc.",
            "format_category": "prompt formatting",
            "format_details": "Reported spread observed when sampling plausible equivalent formats (number of formats/sample size not always specified for the single maximum but aggregated experiments use 10–320 formats; ranking accuracy metric typically used).",
            "performance_metric": "accuracy (ranking accuracy)",
            "performance_value": "Up to 76 percentage points absolute difference in accuracy between equivalent formats (e.g., from near 0% to ~76% on the same task under different formats).",
            "baseline_performance": null,
            "performance_change": "Up to -/+76 percentage points absolute between worst and best formats for a given task.",
            "experimental_setting": "Few-shot (1- and 5-shot analyzed); comparisons across sampled formats from grammar.",
            "statistical_significance": null,
            "uuid": "e7449.2",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-7B task280 atomic change",
            "name_full": "Atomic formatting change causing huge accuracy swing on task280 (LLaMA-2-7B, 1-shot)",
            "brief_description": "On Super-NaturalInstructions task280, a very small formatting change (removing ':' separators) caused LLaMA-2-7B 1-shot ranking accuracy to jump from 4.3% to 82.6% (absolute increase of 78.3 percentage points).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_description": "Open-source autoregressive LLaMA-2 base model (7B parameters), evaluated in 1-shot setting.",
            "model_size": "7B",
            "task_name": "task280 (StereoSet-inspired stereotyping classification in Super-NaturalInstructions)",
            "task_description": "Given a short passage, classify it into one of four stereotype/anti-stereotype types (gender, profession, race, religion).",
            "problem_format": "1-shot few-shot prompt where changing a separator in the field template (e.g., from 'passage: &lt;text&gt; || answer: &lt;text&gt;' to 'passage &lt;text&gt; || answer &lt;text&gt;') is the only change.",
            "format_category": "atomic prompt formatting (separator punctuation)",
            "format_details": "Single-constant atomic change: removing ':' (colon) separators between descriptor and text placeholder; few-shot count = 1; evaluated with probability ranking accuracy.",
            "performance_metric": "probability-ranking accuracy",
            "performance_value": "Format p1: 4.3% accuracy; Format p2: 82.6% accuracy (absolute difference 78.3 percentage points)",
            "baseline_performance": "Format p1 (with ':' separators) = 4.3% accuracy",
            "performance_change": "+78.3 percentage points absolute (from p1 to p2)",
            "experimental_setting": "1-shot LLaMA-2-7B; evaluated on 250 samples for these format comparisons (table derived from analysis over sampled formats).",
            "statistical_significance": null,
            "uuid": "e7449.3",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-7B other atomic examples",
            "name_full": "Selected atomic-format changes producing large accuracy differences (LLaMA-2-7B, 1-shot)",
            "brief_description": "Multiple tasks show single-constant changes (e.g., adding/removing colons, changing item numbering/casing) producing large absolute accuracy jumps (examples include task317, task190, task904, task320, task322, task279).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_description": "Autoregressive LLaMA-2 7B evaluated on 1-shot classification tasks from Super-NaturalInstructions.",
            "model_size": "7B",
            "task_name": "task317, task190, task904, task320, task322, task279 (examples from Table 2/4)",
            "task_description": "Various classification/multiple-choice tasks from Super-NaturalInstructions where single-constant formatting changes impacted model outputs greatly.",
            "problem_format": "1-shot few-shot prompts with atomic formatting changes (separators, enumerations, casing changes, newline placement).",
            "format_category": "atomic prompt formatting",
            "format_details": "Examples: task317 (Passage/Answer colon spacing) improved from 7.6% to 63.8% (diff 56.2 points); task190 (enumeration format change) +25.4 points; task904 (input/output separator change) +19.8 points; task320 (changing semicolons/newlines) +11.5 points; task322 (casing COMMENT vs comment) +10.0 points; task279 (ALL CAPS vs Title) +6.9 points. Metrics shown are probability ranking unless noted otherwise.",
            "performance_metric": "probability-ranking accuracy (table derived values)",
            "performance_value": "Examples: task317: 7.6% -&gt; 63.8% (+56.2 pp); task190: 36.0% -&gt; 61.4% (+25.4 pp); task904: 41.8% -&gt; 61.6% (+19.8 pp); task320: 36.1% -&gt; 47.6% (+11.5 pp); task322: 61.4% -&gt; 71.4% (+10.0 pp); task279: 37.2% -&gt; 44.1% (+6.9 pp).",
            "baseline_performance": "lower-performing format listed for each example (see performance_value)",
            "performance_change": "Absolute increases per example listed above (range ~6.9 to 56.2 percentage points)",
            "experimental_setting": "1-shot LLaMA-2-7B; probability-ranking metric; comparisons derived from sampled format evaluations (table of atomic changes).",
            "statistical_significance": null,
            "uuid": "e7449.4",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Model-comparison reversal",
            "name_full": "Format-dependent reversal of comparative model rankings",
            "brief_description": "The relative ordering of model performance can be reversed solely by changing prompt format: e.g., LLaMA-2-13B vs LLaMA-2-70B and LLaMA-2-7B vs Falcon-7B show nontrivial probabilities of reversal when using different formats, and many reversals are statistically significant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "pairs: LLaMA-2-13B vs LLaMA-2-70B; LLaMA-2-7B vs Falcon-7B (also other pairs analyzed)",
            "model_description": "Comparative evaluation across open-source autoregressive LMs under sampled plausible prompt formats.",
            "model_size": "13B vs 70B; 7B vs 7B (Falcon)",
            "task_name": "53 Super-NaturalInstructions tasks (1- and 5-shot)",
            "task_description": "Same tasks evaluated across different models and different prompt formats; compare probability that M's superiority under one format is reversed under another format.",
            "problem_format": "Few-shot prompts sampled from grammar; compare across formats p and p'.",
            "format_category": "prompt formatting / model comparison methodology",
            "format_details": "Define threshold d (e.g., d=0.02). Compute probability that M' beats M by &gt;= d using format p' given M beat M' by &gt;= d using format p. Performed over 53 tasks and 1- and 5-shot settings.",
            "performance_metric": "probability that ordering is reversed; also used paired significance tests (one-sided McNemar/pairwise chi-squared) across same samples.",
            "performance_value": "Example: LLaMA-2-13B vs -70B reverse trend by at least d=0.02 with probability 0.141; LLaMA-2-7B vs Falcon-7B reverse trend with probability 0.140. In many cases both comparisons were statistically significant (p &lt; 0.05): 76% and 47% respectively for two reported model-comparison sets.",
            "baseline_performance": null,
            "performance_change": "Probability of reversal ~0.14 for reported pairs; percentage of reversals that were statistically significant in paired tests reported as 76% and 47% for two model-comparison instances.",
            "experimental_setting": "1- and 5-shot; ranking accuracy; 1000-sample statistical tests (paired McNemar tests) used to assess significance.",
            "statistical_significance": "Many reversals were statistically significant under paired one-sided McNemar tests (p &lt; 0.05); specific rates: 76% and 47% for two reported comparisons.",
            "uuid": "e7449.5",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 spread (320 formats)",
            "name_full": "Observed prompt-format sensitivity in GPT-3.5-Turbo (API) across many sampled formats",
            "brief_description": "Using FORMATSPREAD on GPT-3.5 across 320 plausible formats and 53 tasks, the paper reports a median spread of 6.4 accuracy points and observed spreads up to 56 accuracy points (using exact prefix matching because logits were not available).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "API-gated autoregressive LLM (ChatGPT family); evaluated via black-box queries (no access to logits).",
            "model_size": "undisclosed (API model; commonly referred to as GPT-3.5)",
            "task_name": "53 Super-NaturalInstructions tasks (classification)",
            "task_description": "Same 53 classification tasks; prompts sampled from grammar; evaluated with exact-prefix matching for GPT-3.5 (no logits available for ranking).",
            "problem_format": "Few-shot prompt templates sampled from grammar (320 formats), 1-shot setting often reported in FORMATSPREAD trials.",
            "format_category": "prompt formatting",
            "format_details": "FORMATSPREAD run with 320 formats and a budget (examples: under $10 average cost per task reported); used exact prefix matching metric for GPT-3.5 because logits were inaccessible for ranking.",
            "performance_metric": "exact prefix matching accuracy; spread (max-min)",
            "performance_value": "Median spread = 6.4 percentage points absolute across 53 tasks; maximum observed spread up to 56 percentage points for some task/format combinations.",
            "baseline_performance": null,
            "performance_change": "Median +6.4 pp spread across formats; occasional up to +56 pp between worst and best formats.",
            "experimental_setting": "320 sampled formats; black-box querying of API; exact prefix matching metric used for GPT-3.5; reported average cost under $10 per task for evaluating 320 formats across 53 tasks.",
            "statistical_significance": null,
            "uuid": "e7449.6",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-70B quantized spreads",
            "name_full": "Format sensitivity observed in LLaMA-2-70B (4-bit quantized) across 320 formats",
            "brief_description": "Even with a large model (LLaMA-2-70B) and quantized 4-bit inference, FORMATSPREAD finds substantial spreads: median spread 0.171 (17.1 percentage points), mean 0.221, std 0.200; 25% of tasks had spread &gt;=0.292 and maximum spread 0.876 (87.6 percentage points) using probability ranking across 53 tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B (4-bit quantized)",
            "model_description": "Large open-source autoregressive model, evaluated under 4-bit quantized inference (to reduce inference costs) and tested with FORMATSPREAD.",
            "model_size": "70B",
            "task_name": "53 Super-NaturalInstructions classification tasks",
            "task_description": "Same set of classification tasks sampled from Super-NaturalInstructions; formats sampled from grammar.",
            "problem_format": "Few-shot templated prompts sampled from the grammar (320 plausible formats)",
            "format_category": "prompt formatting",
            "format_details": "FORMATSPREAD with budget of 40,000 evaluations across 320 formats; probability-ranking accuracy used for LLaMA-2-70B experiments since logits accessible; reported median/mean/std/percentiles and maximum spread values.",
            "performance_metric": "probability-ranking accuracy; spread measured in fraction [0,1] converted to percentage points in description",
            "performance_value": "Median spread = 0.171 (17.1 percentage points); mean = 0.221 (22.1 pp); std = 0.200; 25th percentile of tasks had spread &gt;= 0.292 (29.2 pp); maximum observed spread = 0.876 (87.6 percentage points).",
            "baseline_performance": null,
            "performance_change": "Per-task best-worst format differences as above (example: up to 87.6 pp between worst and best formats for one task).",
            "experimental_setting": "1-shot and 5-shot analyses reported; probability ranking metric; evaluation budget E = 40,000 across 320 formats; 4-bit quantized inference used.",
            "statistical_significance": null,
            "uuid": "e7449.7",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Atomic-feature impacts (S1, item2)",
            "name_full": "Independent contribution analysis of atomic prompt-format features (separators, casing, item numbering)",
            "brief_description": "Analysis conditioning on individual constant choices finds that separators between descriptor and placeholder (S1) and number-format changes in enumerations (F_item2) most strongly predict accuracy differences, whereas other constants (S2, item1 wrappers, casing) do not reliably predict performance alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B (primary for this analysis)",
            "model_description": "Autoregressive LLaMA-2 7B used to measure distributional effects of atomic feature choices across sampled formats.",
            "model_size": "7B",
            "task_name": "31 tasks (subset used for this detailed feature analysis), sampled formats",
            "task_description": "Classification tasks from Super-NaturalInstructions used to estimate how single-constant choices condition the accuracy distribution over sampled formats.",
            "problem_format": "Few-shot prompts generated by grammar with controlled conditioning on a single constant choice (e.g., fix S1 to ':', fix F_item2 to letters vs numbers).",
            "format_category": "atomic prompt formatting (feature-level)",
            "format_details": "Computed accuracy for 500 random formats with 250 samples each on 31 tasks for 1-shot LLaMA-2-7B; evaluated whether conditioned accuracy distributions are 'weakly' or 'strongly' different (boxplot quartile and whisker criteria). Table reports percentage of tasks where constants yield weak/strong differences: e.g., S1 weak diff 43% of tasks, strong diff 22%; F_item2 weak diff 45%, strong diff 10%; F_casing weak diff 3%, strong diff 0%; S2 and F_item1 show 0% in many cases.",
            "performance_metric": "probability-ranking accuracy distributions (boxplot comparisons), counts of tasks showing weak/strong differences",
            "performance_value": "Examples: S1 median spread 0.132; S1 yields weak distributional differences in 43% of tasks and strong differences in 22% of tasks. F_item2 median spread 0.173 with 45% weak / 10% strong differences. Other constants often show 0% strong differences.",
            "baseline_performance": null,
            "performance_change": "Atomic features S1 and F_item2 can yield notable absolute accuracy shifts (median spreads ~0.132–0.173).",
            "experimental_setting": "1-shot LLaMA-2-7B; 500 format samples; 250 eval samples per format for distributional estimates; classification via ranking accuracy.",
            "statistical_significance": null,
            "uuid": "e7449.8",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Atomic-change probability",
            "name_full": "Probability that an atomic formatting change yields large accuracy change",
            "brief_description": "Measured over many sampled atomic changes across tasks, a non-trivial fraction produce meaningful accuracy swings: 24% of atomic changes cause &gt;=5 percentage-point change under exact prefix matching; 11% under probability ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B (primary reported numbers)",
            "model_description": "Autoregressive LLaMA-2-7B measured on sampled atomic format perturbations.",
            "model_size": "7B",
            "task_name": "53 tasks (analysis aggregated across tasks), multiple atomic-change samples",
            "task_description": "Atomic change = single-constant perturbation of a prompt format (e.g., change casing, a separator, an item wrapper).",
            "problem_format": "Atomically perturbed few-shot prompt formats sampled from the grammar.",
            "format_category": "atomic prompt formatting",
            "format_details": "Sampled atomic changes and measured the resulting change in accuracy using both exact-prefix matching and probability-ranking metrics; plotted distribution of impact magnitudes.",
            "performance_metric": "fraction of atomic changes exceeding given absolute accuracy thresholds (e.g., &gt;=5 percentage points)",
            "performance_value": "24% of atomic changes have &gt;=5 percentage-point impact under exact prefix matching; 11% under probability ranking.",
            "baseline_performance": null,
            "performance_change": "Non-negligible probability (11–24%) that a single-constant change causes &gt;=5 pp accuracy difference depending on metric.",
            "experimental_setting": "Multiple tasks, sampled atomic changes per task (e.g., 30 sampled atomic changes per task in some plots); both exact-prefix and ranking metrics reported.",
            "statistical_significance": null,
            "uuid": "e7449.9",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Embedding identifiability correlation",
            "name_full": "Prompt-format identifiability in prompt embeddings and correlation with performance spread",
            "brief_description": "Prompt formats are highly identifiable from the model's last hidden (prompt) embedding — XGBoost on top principal components achieves &gt;=0.98 format classification accuracy using top 100 PCs — and separability (classifier accuracy on top 2 PCs) moderately correlates with observed performance spread (r=0.424 for 1-shot, r=0.555 for 5-shot, exact-prefix matching).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B (embedding analysis)",
            "model_description": "Analyze last hidden layer activations (pre-generation) as prompt embeddings; dimensionality 4096, PCA applied.",
            "model_size": "7B (hidden dim 4096)",
            "task_name": "31 tasks analyzed for embedding identifiability; 10 formats sampled per task; 1000 evaluation examples per format collected",
            "task_description": "Measure whether a classifier can predict which prompt format generated a given prompt embedding and whether that separability correlates with performance spread across formats.",
            "problem_format": "Various few-shot formatted prompts sampled from grammar; embeddings are computed over entire prompt immediately before generating the first token.",
            "format_category": "internal representation / prompt formatting",
            "format_details": "Collected 8000 prompt embeddings (800 per format * 10 formats) for training and 200 per format for evaluation; used top-n PCA components (as low as 2) and trained XGBoost classifier to predict format identity.",
            "performance_metric": "format-classification accuracy from embeddings; Pearson correlation between classifier accuracy (top 2 PCs) and performance spread",
            "performance_value": "Using top 100 PCs, classifier accuracy &gt;= 0.98 for all 31 tasks; correlation between 2-PC classifier accuracy and spread: r=0.424 (p=8.04e-6) for 1-shot (exact-prefix matching), r=0.555 for 5-shot.",
            "baseline_performance": null,
            "performance_change": "Format embeddings separability moderately predicts larger performance spread; high identifiability implies format is a deterministic transformation of prompt embeddings.",
            "experimental_setting": "LLaMA-2-7B; 10 formats per task; 1-shot and 5-shot settings analyzed; PCA applied to 4096-dim embeddings; XGBoost classifier used.",
            "statistical_significance": "Correlation p-values reported (e.g., p=8.04e-6 for r=0.424).",
            "uuid": "e7449.10",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "rating": 2,
            "sanitized_title": "supernaturalinstructions_generalization_via_declarative_instructions_on_1600_nlp_tasks"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "GrIPS: Gradient-free, edit-based instruction search for prompting large language models",
            "rating": 1,
            "sanitized_title": "grips_gradientfree_editbased_instruction_search_for_prompting_large_language_models"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 1,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Prompt waywardness: The curious case of discretized interpretation of continuous prompts",
            "rating": 1,
            "sanitized_title": "prompt_waywardness_the_curious_case_of_discretized_interpretation_of_continuous_prompts"
        },
        {
            "paper_title": "Jailbroken: How does llm safety training fail?",
            "rating": 1,
            "sanitized_title": "jailbroken_how_does_llm_safety_training_fail"
        }
    ],
    "cost": 0.02139975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QUANTIFYING LANGUAGE MODELS' SENSITIVITY TO Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</h1>
<p>Melanie Sclar ${ }^{1} \quad$ Yejin Choi ${ }^{1,2} \quad$ Yulia Tsvetkov ${ }^{1} \quad$ Alane Suhr ${ }^{3}$<br>${ }^{1}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{2}$ Allen Institute for Artificial Intelligence ${ }^{3}$ University of California, Berkeley<br>msclar@cs.washington.edu</p>
<h4>Abstract</h4>
<p>As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights ${ }^{1}$. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
<h2>1 INTRODUCTION</h2>
<p>As the capabilities of LLMs have rapidly improved, their sensitivity to input prompt features has been used to optimize performance via prompt engineering (White et al., 2023). However, there has been little work in characterizing this sensitivity, especially to seemingly innocuous feature choices that preserve prompt meaning and intent. In this work, we analyze the sensitivity of widely used, open-source LLMs to a class of features that should not influence a prompt's interpretation: formatting choices. We find that pre-trained LLMs are sensitive to these choices in unpredictable ways, with accuracy varying in up to 76 points for LLaMA-2-13B between equivalent formats, and $\sim 10$ accuracy points on average across $50+$ tasks and several models. We also show that this variance is not eliminated by adding few-shot examples, increasing model size, or instruction tuning.</p>
<p>Designing prompt templates is a critical part of effectively using a pre-trained language model. This design process includes making choices about wording, choosing few-shot examples for in-context learning, and making decisions about seemingly trivial features like formatting. This process, and often even the resulting templates, is rarely reported or discussed in research papers, under the assumption that performance variance across these choices is insignificant compared to variance across data points or models. However, some anecdotal evidence points to formatting choices actually having a significant influence on model behavior (Aghajanyan, 2023). In some cases, researchers report a limited number of manually generated formats to show that scaling trends hold despite perfor-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Slight modifications in prompt format templating may lead to significantly different model performance for a given task. Each <text> represents a different variable-length placeholder to be replaced with actual data samples. Example shown corresponds to 1-shot LLaMA-2-7B performances for task280 from SuperNaturalInstructions (Wang et al., 2022). This StereoSet-inspired task (Nadeem et al., 2021) requires the model to, given a short passage, classify it into one of four types of stereotype or anti-stereotype (gender, profession, race, and religion).
mance being significantly different (Schick et al., 2021). The assumption that formatting does not influence overall model performance may become problematic when improvements over existing approaches are attributed to the amount and source of training data, number of parameters, or model architecture, without also accounting for changes in prompt format. Ignoring variance across formats may also negatively affect user experience, e.g. if users inadvertently choose formats the LLM does not perform well on.</p>
<p>Our proposed tool, FORMATSPREAD, enables a systematic analysis of these variances across a wide set of semantically equivalent prompt formats within a user-specified computational budget. We find that choices in formatting few-shot examples during in-context learning introduce spurious biases that may lead to significantly different conclusions in model performance. The sensitivity to formatting choices that we discover across widely-used, open-source models suggests that future research would benefit from reporting a performance spread over a sufficient sample of plausible formats, instead of simply reporting the formatting used and its performance, as is currently standard. Moreover, we argue that this reporting is crucial when comparing the performance of different models, as we show the influence of formatting choices only weakly correlates between models, thus making and fixing a formatting choice could introduce a significant confounding factor.</p>
<p>Fully exploring the space of prompt formats is intractable, as computation costs scale linearly with the number of formats considered. FORMATSPREAD efficiently explores the space of prompt formats under a user-specified computational budget using Bayesian optimization. FORMATSPREAD does not require access to the model weights, allowing its use on API-gated models: we find a spread up to 56 accuracy points with a median spread of 6.4 accuracy points with GPT3.5 across 320 formats and 53 tasks at a cost of under 10USD on average per task. Beyond facilitating evaluation, we also propose a suite of analyses to further characterize model sensitivity to formatting. Among other results, we show that the separability of continuous prompt embeddings correlates with the spread observed in task performance.</p>
<h1>2 OVERVIEW</h1>
<p>We evaluate LLM performance over the space of prompt formats that may plausibly be chosen by a non-adversarial user when designing a prompt for a target task, where the space of formats is defined by a grammar (§3.1). Our grammar's definition naturally induces a definition of semantic equivalence among formats. We quantify model sensitivity in terms of performance range in a target task across the space of equivalent prompt formats to the original choice (§4.2). We cast the problem of searching across this space as a bandit problem, and propose FORMATSPREAD (§3), which consists of a grammar (§3.1) and a procedure to estimate the minimum and maximum performance across a set of semantically equivalent formats given a pre-defined metric (§3.2). FORMATSPREAD uses Bayesian optimization to identify the expected performance range with low additional computational cost (§4.5) all without requiring access to model weights, which enables use on API-gated</p>
<p>LLMs. Furthermore, we perform in-depth analysis of this observed sensitivity, including by quantifying the contribution of individual feature choices to the final performance (§4.3) and measuring the identifiability of a format based solely on a model's internal, continuous representation of any prompt via correlation with model performance (§4.4).</p>
<h1>3 Measuring Sensitivity with FormatSpread</h1>
<h3>3.1 Grammar of Plausible Prompt Formats</h3>
<p>We construct a grammar that defines both the space of plausible prompt formats and semantic equivalence between formats. The grammar is manually constructed, as opposed to automatically induced from data, to guarantee a higher level of precision when defining the set of equivalent formats. Our grammar is directly tested by verifying that it can generate the formatting associated with 100+ Super-NaturalInstructions tasks (Wang et al., 2022).</p>
<p>Our grammar consists of fields that are composed to create a prompt format. For example, the format 'Passage: <text> || Answer: <text>', has basic fields 'Passage: <text>', and 'Answer: <text>', denoted $a_{1}$, and $a_{2}$. Each basic field consists of a descriptor (e.g. 'Passage'), a separator (e.g. ' : ' ), and a text placeholder to replace with each data point. We define basic fields as $B_{1}(d, s, f):=f(d) s&lt;$ text&gt; using Backus-Naur notation, where $d$ is a descriptor string, $s \in \mathcal{S}<em _casing="{casing" _text="\text">{1}$ a separator, and $f \in \mathcal{F}</em>,\right.$ ' || ' $)$.}}$ a function that alters $d$ while preserving meaning. Thus, in our example, $a_{1}=B_{1}($ Passage, ': ', id $)$ and $a_{2}=B_{1}($ Answer, ': ', id $)$, with id the identity function. We define joining several fields as $B_{2}^{(n)}\left(X_{1}, \ldots, X_{n}, c\right):=X_{1} c X_{2} c \ldots c X_{n}$, with $c \in \mathcal{C}$ being a space. Our example's prompt format may be written as $B_{2}^{(2)}\left(a_{1}, a_{2</p>
<p>The grammar also supports enumeration, which is defined as joining several basic fields, each representing a different list item. For example, the enumeration 'Option (A) : <text>, Option (B) : <text>, Option (C) : <text>' may be written as $B_{2}^{(3)}\left(a_{1}, a_{2}, a_{3},\right.$ ' $\left.\left.\left.\right|^{1}\right)^{\prime}\right)$, where $a_{i}=$ $B_{1}\left(e_{i}, \prime: \prime\right.$ ', id $)$. In our example, $e_{1}$ represents 'Option (A)', and may in turn be written as the concatenation $e_{i}:=d s_{2} f_{\text {item }}(i)$ with $d=$ 'option', $s_{2}=$ ' (single space), and $f_{\text {item }}(1)=$ '(A)'. Each $f_{\text {item }}$ transforms an item $i$ using a number format (e.g. letters or Roman numerals, denoted as $\mathcal{F}<em _item1="{item1" _text="\text">{\text {item } 2}$ ) and an item wrapper (e.g. (A) or [A], denoted as $\mathcal{F}</em>$ ).}</p>
<p>In summary, we define valid prompt formats as those accepted by the following grammar:</p>
<p>$$
\begin{aligned}
B_{0}(): &amp; =&lt;\text { text }&gt; \
B_{0}^{\prime}(d, s) &amp; :=f(d) s \quad \text { with } s \in \mathcal{S}<em _casing="{casing" _text="\text">{1}, f \in \mathcal{F}</em> \
B_{1}(d, s, f) &amp; :=f(d) s&lt;\text { text }&gt;\quad \text { with } s \in \mathcal{S}}<em _casing="{casing" _text="\text">{1}, f \in \mathcal{F}</em> \
B_{2}^{(n)}\left(X_{1}, \ldots, X_{n}, c\right) &amp; :=X_{1} c \ldots c X_{n} \quad \text { with } c \in \mathcal{C}, X_{i} \in\left{B_{0}, B_{0}^{\prime}, B_{1}, B_{2}, B_{3}\right} \forall i \
B_{3}^{(n)}\left(d, j_{1}, \ldots, j_{n}, s_{1}, s_{2}, c, f_{1}, f_{2}\right):= &amp; B_{2}^{(n)}\left(B_{1}\left(e_{1}, s_{1}, f_{2}\right)\right), \ldots, B_{1}\left(e_{n}, s_{1}, f_{2}\right), c\right) \
&amp; \text { where } e_{i}:=f_{2}(d) s_{2} f_{1}\left(j_{i}\right), j_{i} \in \mathbb{N}}<em 1="1">{0} \forall i \
&amp; s</em>} \in \mathcal{S<em 2="2">{1}, s</em>} \in \mathcal{S<em 1="1">{2}, f</em>} \in \mathcal{F<em 2="2">{\text {item }}, f</em>
\end{aligned}
$$} \in \mathcal{F}_{\text {casing }</p>
<p>Our grammar defines valid formats as finite compositions of $B_{0}, B_{0}^{\prime}, B_{1}, B_{2}, B_{3}$. The sets $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}$, $\mathcal{F<em _item="{item" _text="\text">{\text {casing }}, \mathcal{F}</em>$ ) to guarantee semantic equivalence; one may also define a set of functions that paraphrases the descriptor, e.g., via synonym replacement. Appendix A. 2 contains the full list of values we use for the constant sets, as well as a visualization of a prompt template generated from the grammar.}}$ (two sets of separators, spaces, casing functions, and itemizing functions respectively) are pre-defined by the user. Throughout this work, we instantiate all sets with values typically observed in human-written prompt formats. We intentionally only modify the casing of descriptors (via $\mathcal{F}_{\text {casing }</p>
<p>Prompt Format Equivalence. Two prompt formats $p_{1}, p_{2}$ are equivalent if they represent the same rule application $B_{i}$, the descriptors (if any) are the same, and the sub-elements (if any) are equivalent. Appendix A. 1 contains the formal definition of equivalence. The grammar's strict definition allows us to assume that sets of equivalent formats share equivalent meanings. When measuring sensitivity (§3.2), we explore only the space of formats equivalent to a task's original format.</p>
<p>Contextual Restrictions. We define restrictions to the combinations of spaces and separators to further ensure naturalness. For example, if $B_{2}\left(X_{1}, \ldots, X_{n}, c\right)$ where $c$ does not contain a newline, then each $X_{i}$ 's separators and any subcomponents' separators should not contain a newline. This</p>
<p>avoids unnatural formats like Input: $\backslash$ n <text> Output: $\backslash$ n <text>. We also allow for adding conditions that force constants (separators, spaces, etc.) in different applications of $B_{i}$ to be equal. When measuring sensitivity to format perturbations, if two separators or spaces are equal in an original format, they are forced to jointly change to be considered equivalent. Appendix A. 3 contains all contextual restrictions.</p>
<p>Final Prompt Construction. Given a valid format $p$ accepted by the grammar, the final prompt is constructed by concatenating with space $c$ an instruction string inst, $n$ fewshot data points $D_{1}, \ldots, D_{n}$ exemplifying the task, and a data point $D_{n+1}$ to be solved. All few-shot examples $D_{i}$ are formatted using $p$. Thus, the final prompt template is: inst $c p\left(D_{1}\right) c p\left(D_{2}\right) c \ldots c p\left(D_{n}\right) c p\left(D_{n+1}\right)$. Since $D_{n+1}$ 's output will be generated by the model, an empty string is added in place of the answer in the last field in the template. Prompt construction will modify inst to match specific choices encoded in $p$ : concretely, if $p$ enumerates valid multiple-choice options as characters $x_{1} \ldots x_{n}$, we ensure inst refers to these choices as $x_{1} \ldots x_{n}$.</p>
<h1>3.2 Measuring Sensitivity</h1>
<p>We measure how plausible choices in prompt formatting influence quantifiable metrics of generated outputs. Given a set of plausible formats $\left{p_{1}, \ldots, p_{n}\right}$, a dataset $\mathcal{D}$, and a scalar metric $m$, let the performance interval be $\left[\min <em i="i">{i} m\left(p</em>\right), \max }, \mathcal{D<em i="i">{i} m\left(p</em>\right)\right]$. We define the performance spread or simply spread as $\max }, \mathcal{D<em i="i">{i} m\left(p</em>\right)-\min }, \mathcal{D<em i="i">{i} m\left(p</em>\right)$. Higher spread indicates more sensitivity to variance within the space of plausible, semantically-equivalent formats. While our method is agnostic to the scalar metric $m$ used, and one could consider a number of metrics including text length, formality, or toxicity, throughout this work we focus our analysis on estimated task accuracy $a c c$. Due to ease in automatic evaluation, here we evaluate on classification tasks.}, \mathcal{D</p>
<p>Our goal is to compute spread for a given model and task. A comprehensive approach would be to fully evaluate each plausible format $p_{i}$ on the entire evaluation dataset $\mathcal{D}$. This increases the cost of reporting a model's performance linearly with $n$, which becomes computationally infeasible for large values of $n$. Following prior gradient-free prompt engineering work (Zhou et al., 2023; Pryzant et al., 2023), we model our problem as a multi-arm bandit. Given a random sample of $n$ formats (arms) $p_{1}, \ldots, p_{n}$ for a task, an arm $p_{i}$ 's hidden value is the actual performance $m\left(p_{i}, \mathcal{D}\right)$ when evaluated on the full dataset $\mathcal{D}$, and the reward for pulling the arm is an estimate $m\left(p_{i}, \hat{\mathcal{D}}\right)$ where $\hat{\mathcal{D}} \subset \mathcal{D},|\hat{\mathcal{D}}|=B$ (mini-batch size) and no element of $\hat{\mathcal{D}}$ has yet been evaluated with $p_{i}$.</p>
<p>We assume a budget of $E$ total data point evaluations. We first search for the highest performing format with budget $E / 2$, and then for the lowest performing format with budget $E / 2$. Evaluations done for the first exploration are readily available for the second exploration, which yields a more informative prior for many formats. We consider two well-known regret minimization bandit algorithms: Thompson sampling (used in FORMATSPREAD) and Upper Confidence Bound (UCB).</p>
<p>Thompson Sampling. This simple, high-performing Bayesian inference heuristic randomly draws each arm according to its probability of being optimal (Chapelle \&amp; Li, 2011). Each $m\left(p_{i}, \mathcal{D}\right)$ is modeled as a random variable, and since with our target metric each data point evaluation is a Bernoulli trial, it is natural to model $m\left(p_{i}, \hat{\mathcal{D}}\right)$ as a Beta distribution. In each round, Thompson sampling draws from each $m\left(p_{i}, \hat{\mathcal{D}}\right)$ and chooses the best arm $\hat{i}$ (Algorithm 1). It then updates $\hat{i}$ according to the number of observed successes $r$, and the corresponding $B-r$ failures, within $\hat{\mathcal{D}}$.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Thompson</span><span class="w"> </span><span class="nx">Sampling</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">Bernoulli</span><span class="w"> </span><span class="nx">Bandits</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">success</span><span class="w"> </span><span class="nx">counters</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="nx">times</span><span class="w"> </span><span class="nx">armed</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">drawn</span><span class="w"> </span><span class="nx">counter</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Take</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">Beta</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">alpha_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">+</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">beta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">-</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Draw</span><span class="w"> </span><span class="nx">arm</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">i</span><span class="p">}=</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">minimization</span><span class="w"> </span><span class="nx">problems</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">+</span><span class="nx">r</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">+</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Thompson sampling allows for setting informative priors $\left(\alpha_{i}, \beta_{i}\right)$ based on domain knowledge to accelerate runtime. Appendix A. 4 details the exact priors we use. To our knowledge, we are the first to consider a Bayesian sampling method for prompt optimization.</p>
<p>Upper Confidence Bound (UCB) Sampling. UCB (Lai et al., 1985) computes an upper confidence bound to each arm's performance, derived from Chernoff's bound. The key difference with Thompson sampling is in how $\theta_{i}^{(t)}$ is defined. In UCB's frequentist approach, $\theta_{i}^{(t)}$ is assigned the estimated accuracy plus the upper confidence bound: $\theta_{i}^{(t)} \leftarrow S_{i} / N_{i}+c \sqrt{\log (t) / N_{i}}$. We use $c=2$ following Pryzant et al. (2023), who find UCB with $c=2$ to be most effective for prompt optimization.</p>
<p>Naive Sampling. Each prompt format is evaluated on $E / n$ points (with appropriate rounding).</p>
<h1>4 Characterizing Prompt Format Variance with FormatSpread</h1>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>Data. We use a subset of 53 tasks from Super-NaturalInstructions (Wang et al., 2022) with diverse human-written formats and instructions, comprising 19 multiple-choice tasks and 34 classification tasks with ${2,3,4}$ basic fields. Appendix B. 1 details the exact task selection procedure. To construct the final prompt template, we concatenate each task's instruction and $n$ formatted few-shot examples using $\backslash \mathrm{n} \backslash \mathrm{n}$ as spacing. While selection and ordering of few-shot examples is a component of prompt design influencing features of model output (Lu et al., 2022), our work focuses on prompt formatting. To remove this confounder, we fix the exact choice and ordering of examples for each task and for a given number of shots $n$. Few-shot examples for each task are chosen randomly within each dataset and are not used for evaluation. We evaluate task data samples on an arbitrary order fixed across settings. Datasets are assumed to be of size 1,000 for fair evaluation across tasks.</p>
<p>Models. We evaluate LLaMA-2-{7B,13B,70B} (Touvron et al., 2023), Falcon-7B and Falcon-7BInstruct (Almazrouei et al., 2023), GPT-3.5-Turbo (Schulman et al., 2022), all autoregressive LMs.</p>
<p>Task Evaluation Metrics. We use two popular measures for computing accuracy: exact prefix matching and probability ranking. In exact prefix matching, we check if the output's prefix matches the expected answer after normalization (casing, spacing, newlines). Ranking accuracy computes the rate that the expected answer is the highest-ranked valid option (in multiple choice and classification tasks) according to the model's output distribution. Results are reported using ranking accuracy unless specified otherwise. Appendix B. 2 shows additional analysis of exact prefix matching, with spreads even higher than those shown in Section 4.2, and including how formatting choice affects task degeneration (i.e., not answering any valid option).</p>
<h3>4.2 PROMPT FORMATS HAVE A LARGE PERFORMANCE SPREAD, NOT ELIMINATED BY INCREASING FEW-SHOT EXAMPLES OR MODEL SIZE, NOR WITH INSTRUCTION TUNING</h3>
<p>For each evaluation task we randomly sample 10 plausible prompt formats and use FORMATSPREAD to compute performance spread for each modeling and $n$-shot choice (Figure 3). We find significant performance spread across tasks, with a median spread of 7.5 accuracy points across choices in the model and the number of few-shot examples. $20 \%$ of tasks consistently result in a spread of at least 15 accuracy points for all LLaMA-2 settings, and at least 9 points for all Falcon settings. We observe several tasks with performance spread over 70 accuracy points. Because this analysis uses only 10 randomly sampled formats, it represents a lower bound of the true spreads for each task. Furthermore, there exists significant performance spread regardless of increased model size (Figure 2a and Figure 11 for Llama-2-70B), instruction tuning (Figure 2b), or number of few-shot examples (Figure 2c; Figure 2a and 2b plot 1- and 5-shot jointly). Appendix B. 2 demonstrates similar results on a selection of non-classification tasks, and expands the spread discussion to plotting the entire accuracy distribution, along with a dispersion metric.</p>
<p>Comparison trends between models are often reversed just by choosing different formats. Assuming model $M$ is better than $M^{\prime}$ by at least $d$ accuracy using prompt $p$, we compute how often $M^{\prime}$ achieves at least $d$ higher accuracy than $M$ under a different format $p^{\prime}$. Figure 4 shows these trends are often reversed: LLaMA-2-13B and -70B reverse trend by at least $d=0.02$ with probability 0.141; LLaMA-2-7B and Falcon-7B reverse trend by at least $d=0.02$ with probability 0.140. Strikingly, often both experiments (first using $p$, and then $p^{\prime}$ ) were statistically significant (p-value $&lt;0.05$ ) on 1000 samples $^{2}: 76 \%$ and $47 \%$ respectively for the two model comparisons</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Spread comparison between evaluating the same task under different models or $n$-shots.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Spread across models and $n$-shots.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Probability that model $M$ performs worse than $M^{\prime}$ by at least $d$ when using format $p^{\prime}$, given that $M$ performed better than $M^{\prime}$ by at least $d$ using format $p$. 53 tasks, 1- and 5-shot.</p>
<p>mentioned above. We find that formats yielding high performance for model $M$ may not yield high performance for $M^{\prime}$, implying that formats may not be inherently good or bad (Appendix B.2).</p>
<h3>4.3 How do individual features contribute to performance?</h3>
<p>We analyze how choices in particular constants (i.e. $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}, \mathcal{F<em _item="{item" _text="\text">{\text {casing }}, \mathcal{F}</em>}}$ ) independently influence task performance across different formats. Figure 5 shows the distribution of accuracy for 500 sampled prompts conditioned on the choice of $\mathcal{S<em 1="1">{1}$ (the separator between a descriptor and the text placeholder) for one task in Super-NaturalInstructions. When comparing the individual influence of two feature choices, we measure both weak and strong notions of dissimilarity between distributions of accuracy across prompts conditioned on a chosen feature. We say two constant choices yield weakly different accuracy distributions if the values between the first quartile ( $Q</em>$ ' and ' : ' (fourth and sixth) are only weakly different.}$ ) and third quartile ( $Q_{3}$ ) do not intersect. This is equivalent to the boxes in a boxplot not overlapping. We say two constant choices yield strongly different accuracy distributions if the ranges $\left[2.5 Q_{1}-1.5 Q_{3}, 2.5 Q_{3}+1.5 Q_{1}\right]$ do not overlap (adjusted to end in a data point). This is equivalent to two boxplots with their whiskers not overlapping. In Figure 5, ' $\backslash \mathrm{n} \backslash \mathrm{c</p>
<p>We compute accuracy for 500 random formats with 250 samples each on 31 tasks for 1-shot Llama-2-7B. Table 1 shows that choices in $\mathcal{S}<em _item1="{item1" _text="\text">{2}, \mathcal{F}</em>}}, \mathcal{F<em 1="1">{\text {casing }}$ do not independently predict performance differences (weakly or strongly): although these features can have a large performance variance and thus should be explored with FORMATSPREAD, they cannot be used to independently predict accuracy changes. Other constant sets have varying degrees of differences, with $\mathcal{S}</em>$ (number format changes in enumerations) having the most individual impact. All tasks with strong dissimilarities are shown in Appendix B.4.}$ (separators) and $\mathcal{F}_{\text {item2 }</p>
<p>Small prompt variations often yield large performance differences. Table 2 shows a selection of tasks where changing a single constant on a format (e.g., casing in task322) results in large accuracy differences. Figure 6 shows that regardless of the scoring criterion used, a significant ratio of these atomic changes are associated with large accuracy changes. For example, 24\% of atomic changes have an associated accuracy change of at least 5 points when using exact prefix matching as scoring criteria ( $11 \%$ when using probability ranking).</p>
<p>The space of prompt format accuracy is highly non-monotonic, which makes local search algorithms over the space less effective. Let $\left(p_{1}, p_{2}, p_{3}\right)$ be a prompt format triple such that $p_{i+1}$ is obtained by making an atomic change to $p_{i}$. We argue that if the prompt format space is smooth, we should often see a triples' accuracy to be strictly monotonic over $i$. We choose 24 tasks ( 13 multiple choice,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Example of accuracy variance for different choices of constants in $\mathcal{S}_{1}$ for task1283.</p>
<p>Table 1: Tasks where at least two constants yield different performance (weakly different if their boxes in a boxplot do not overlap, strongly if boxes including whiskers do not overlap).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Median Spread <br> (range $[0,1]$ )</th>
<th style="text-align: center;">Weak <br> Diff.</th>
<th style="text-align: center;">Strong <br> Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{C}$</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{S}_{1}$</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">$43 \%$</td>
<td style="text-align: center;">$22 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{S}_{2}$</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {item1 }}$</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {item2 }}$</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {casing }}$</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of atomic changes' impact on accuracy using probability ranking (prefix matching shown in Table 4). $}$ represents a text field; $p_{2}$ yields higher accuracy than $p_{1}$ for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Id</th>
<th style="text-align: left;">Prompt Format 1 $\left(p_{1}\right)$</th>
<th style="text-align: left;">Prompt Format 2 $\left(p_{2}\right)$</th>
<th style="text-align: left;">Acc $p_{1}$</th>
<th style="text-align: left;">Acc $p_{2}$</th>
<th style="text-align: left;">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">task280</td>
<td style="text-align: left;">passage: ${}\backslash$ answer: $}$</td>
<td style="text-align: left;">passage ${}\backslash$ answer $}$</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.826</td>
<td style="text-align: left;">0.783</td>
</tr>
<tr>
<td style="text-align: left;">task317</td>
<td style="text-align: left;">Passage: : $}$ Answer: : $}$</td>
<td style="text-align: left;">Passage: : $}$ Answer: : $}$</td>
<td style="text-align: left;">0.076</td>
<td style="text-align: left;">0.638</td>
<td style="text-align: left;">0.562</td>
</tr>
<tr>
<td style="text-align: left;">task190</td>
<td style="text-align: left;">Sentence[II] - {}</td>
<td style="text-align: left;">Sentence[A] - {}Sentence[B] - {}</td>
<td style="text-align: left;">0.360</td>
<td style="text-align: left;">0.614</td>
<td style="text-align: left;">0.254</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\cdots$ Answer $\backslash t {}$</td>
<td style="text-align: left;">$\cdots$ Answer $\backslash t {}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">task904</td>
<td style="text-align: left;">input: : $}$ \n output: : $}$</td>
<td style="text-align: left;">input: : $}$ \n output: : $}$</td>
<td style="text-align: left;">0.418</td>
<td style="text-align: left;">0.616</td>
<td style="text-align: left;">0.198</td>
</tr>
<tr>
<td style="text-align: left;">task320</td>
<td style="text-align: left;">target - {} \n{} \nanswer - {}</td>
<td style="text-align: left;">target - {}; \n{}; \nanswer - {}</td>
<td style="text-align: left;">0.361</td>
<td style="text-align: left;">0.476</td>
<td style="text-align: left;">0.115</td>
</tr>
<tr>
<td style="text-align: left;">task322</td>
<td style="text-align: left;">COMMENT: {} ANSWER: {}</td>
<td style="text-align: left;">comment: {} answer: {}</td>
<td style="text-align: left;">0.614</td>
<td style="text-align: left;">0.714</td>
<td style="text-align: left;">0.100</td>
</tr>
<tr>
<td style="text-align: left;">task279</td>
<td style="text-align: left;">Passage : {}. Answer : {}</td>
<td style="text-align: left;">PASSAGE : {}. ANSWER : {}</td>
<td style="text-align: left;">0.372</td>
<td style="text-align: left;">0.441</td>
<td style="text-align: left;">0.069</td>
</tr>
</tbody>
</table>
<p>11 non-multiple choice), sample 300 $\left(p_{1}, p_{2}, p_{3}\right)$ triples for each, and the compute accuracy (using exact prefix matching) of each $p_{i}$ on 250 samples. 32.4 and $33.6 \%$ of triples were monotonic for multiple-choice and non-multiple-choice tasks respectively. Given that random shuffling within a triple will result in monotonicity $33.3 \%$ of the time, this suggests that local search mechanisms like simulated annealing may not be effective as they require a locally smooth search space.</p>
<h1>4.4 PROMPT FORMATS ARE IDENTIFIABLE TRANSFORMATIONS OF PROMPT EMBEDDINGS</h1>
<p>Prompt format choices represent a deterministic transformation of the input, even if its impact on the resulting performance is hard to predict. We represent prompt embeddings as the last hidden layer obtained when processing the whole input prompt (immediately before generating the first token). We demonstrate that format choice yields a highly identifiable transformation over this embedding, which suggests that formats can be seen as transformations of the output probability distribution.</p>
<p>For each task, and for both ${1,5}$-shot settings, we collect prompt embeddings from LLaMA-2-7B corresponding to 10 randomly sampled valid formats for 1000 evaluation examples. We train an XGBoost (Chen \&amp; Guestrin, 2016) classifier that maps from the top $n$ principal components of a prompt embedding to the prompt format. ${ }^{3}$ We find that although the original prompt embeddings are of size $4,096^{4}$, using just the top 100 principal components can result in a classifier with $\geq 0.98$ accuracy in format identification for all 31 tasks analyzed. Figure 7 shows the accuracy of format classification given a fixed number of principal components. ${ }^{5}$ We find that classifier accuracy given just the top two components correlates moderately with the spread of performance in the prompts they represent $\left(0.424, p=8.04 \cdot 10^{-6} ; 0.555\right.$ for the 5 -shot setting; using exact prefix matching).</p>
<h3>4.5 FAST EXPLORATION OF THE PROMPT FORMATTING SPACE: FORMATSPREAD</h3>
<p>In Section 4.2, we demonstrate that even when sampling just 10 formats from the space of plausible formats, we still observe significant performance spread on many tasks. However, this is only a lower</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Probability that an atomic change (e.g. changing a space, separator) has a given impact in accuracy for two scoring criteria. 53 tasks, 30 sampled atomic changes each.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Probability of observing a spread increase of at least $d$ when increasing sample size from $k_{1}$ to $k_{2}$ formats. 31 tasks, 100 trials each.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Cumulative ratio of tasks that can be classified with at most $a$ accuracy using the top principal components of the last decoding layer of the prompt.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Difference between the true sample spread and each algorithm-found spread with respect to $E$ (evaluation budget). 320 formats, $B=20$, average of 5 trials over 31 tasks shown.
bound of the spread a task may exhibit when increasing the number of formats: for example, about $17 \%$ of tasks are expected to increase their spread by at least 5 accuracy points when increasing from 10 to 20 sampled formats. Figure 8 quantifies the expected increase in spread when increasing the number of formats by evaluating 500 formats on 250 samples each and computing expected gains.</p>
<p>Figure 9 compares the efficiency of Thompson sampling, UCB, and naive sampling for estimating spread with respect to a budget $E$ (Section 3.2). To ensure accurate reports, we compute and show the true spread of the highest- and lowest-performing formats chosen by each method using all data. With a budget of 51,200 evaluations, Thompson sampling results in a spread within 1 accuracy point of the true spread, while naive sampling finds a spread within 4 points, and UCB within 11.</p>
<p>Finally, we use FormatSpread to measure sensitivity of several models where inference is expensive. With a budget of 40,000 evaluations and 320 prompt formats, we find that 1-shot LLaMA-2-70B-ran using 4-bit quantization (Dettmers et al., 2022)-yields a median spread of 0.171 (mean $=0.221$, std $=0.200$, using probability ranking across 53 tasks; $25 \%$ of tasks had a spread of 0.292 or higher, with a maximum spread of 0.876 ), and GPT-3.5 yields a median spread of 0.064 (mean $=0.110$, std $=0.115$, across 53 tasks using exact prefix matching given that we do not have access to the full logits; $25 \%$ of tasks had a spread of 0.148 or higher, with a maximum spread of 0.562 ), showing sensitivity to formatting is still present even on larger models. 5-shot LLaMA-270B still shows high spreads, with $25 \%$ of tasks having a spread of 0.310 and a maximum of 0.841 . See spread visualization in Figure 25, and a list of best and worst formats found in Table 6.</p>
<h1>5 Related Work</h1>
<p>The task of automatically finding the best-performing prompt for a given task without changing model parameters has recently gained attention, given the constantly improving yet somewhat unpredictable performance of LLMs. Prior work has often focused on discovering optimal prompts with gradient-based methods, which are effective, but often lead to disfluent or unnatural prompts (Shin</p>
<p>et al., 2020), which can be mitigated with a Langevin dynamics-based method (Shi et al., 2022). Another approach is to learn, optimize, and insert continuous representations of prompts and tasks as input to models (Qin \&amp; Eisner, 2021; Lester et al., 2021; Ding et al., 2022; Itharco et al., 2023). These methods also require access to the LLM's parameters, thus cannot be applied to models behind an API. In contrast, FORMATSPREAD does not assume access to any model internals. Prior gradientfree work has focused on edit-based enumeration over human-written prompts (Prasad et al., 2023), reinforcement learning (Deng et al., 2022), and by using LLMs themselves (Zhou et al., 2023; Gao et al., 2021). These works aim to achieve competitive task performance, even if the meaning of the prompt or instruction is modified. To our knowledge, we are the first to focus specifically on prompt formatting variance, a quintessential example of semantic equivalence.</p>
<p>Jailbreaking refers to the behavior of intentionally manipulating prompts to elicit inappropriate or sensitive responses, or otherwise reveal parts of the prompt that were intentionally not revealed. While the objective differs from our work, jailbreaking works (Wei et al., 2023; Zou et al., 2023) share the underlying technical question of finding the lowest-performing prompt. Our methods differ, since Wei et al. (2023) evaluate human-generated attacks to guide adversarial prompt design, and Zou et al. (2023) uses gradient-based search methods simultaneously across multiple models.</p>
<p>Some existing work has explored the influence of certain prompt design choices on model performance, for example the prompt's language (Gonen et al., 2022), the ordering of few-shot examples (Lu et al., 2022), and their patterns (Madaan et al., 2023). Other work has focused on providing textual interpretations of continuous prompt representations (Khashabi et al., 2022). Beyond autoregressive LLMs, existing work has focused on performance variance in masked language models (Elazar et al., 2021; Jiang et al., 2020). Our work follows efforts in other domains that explore the influence of spurious features on research evaluations, e.g., in deep reinforcement learning (Islam et al., 2017; Henderson et al., 2018) and statistical machine translation (Clark et al., 2011).</p>
<h1>6 DISCUSSION</h1>
<p>We introduce FORMATSPREAD, an algorithm that estimates the performance spread across prompt formatting choices. ${ }^{6}$ We use FORMATSPREAD to evaluate the spread of several widely-used opensource LLMs for classification tasks in few-shot learning settings. We find that spread is large regardless of model choice, even when increasing model size, number of few-shots, or when using instruction tuning. FORMATSPREAD is designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. For example, with a computational budget of exploring only $5 \%$ of the entire search space for task with 2,500 test examples and 320 plausible formats, we are able to estimate spread within 2 accuracy points of the true spread.</p>
<p>We also characterize the space of prompt formats, finding that it is largely non-monotonic and that few atomic features can be predictors of performance alone, although the separability of format embeddings is highly correlated with observed performance spread. These findings informed the design of our search procedure, where local search methods are not advantageous.</p>
<p>Our findings suggest that performance spread caused by arbitrary prompt formatting choices may influence conclusions made about model performance, especially when comparing models on benchmark tasks. Thus, we recommend that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible formats. However, we want to emphasize that single-format evaluation may still be sufficient for many use cases. For example, for researchers or practitioners who build systems on top of LLMs, choosing a single prompt format that works sufficiently well for use in this larger system is a valid methodological choice. However, we encourage future research to compute FORMATSPREAD when comparing their systems to out-of-the-box models, to ensure fair baseline representation. Furthermore, FORMATSPREAD can be used to identify lower-bound performance of a model or system. For example, when using a model for socially impactful tasks, such as stereotype classification in Figure 1, it is important to report the range of accuracy a non-adversarial user might encounter. Likewise, it is crucial to consider robustness to spurious features when claiming that models possess general abilities, such as theory of mind; and beneficial to report when e.g. exploring model biases. We leave it to future research to develop regularization procedures either during training or with an already-trained model to make models robust to diverse formatting choices.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>7 ACKNOWLEDGEMENTS</h1>
<p>We thank Jillian Fisher, Sachin Kumar, Angela Zhou, and the Berkeley NLP group for valuable discussions. This work was conducted while A.S. was a Young Investigator at AI2. This material is based upon work partly funded by the DARPA CMO under Contract No. HR001120C0124, by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), by NSF DMS-2134012, IIS2125201, IIS-2203097, by NSF CAREER Grant No. IIS2142739, and an Alfred P. Sloan Foundation Fellowship. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily state or reflect those of the United States Government or any agency thereof.</p>
<h2>REFERENCES</h2>
<p>Armen Aghajanyan. Tweet: Susan \&amp; I found MMLU performance jump 6-10 points in the 40 s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one. June 2023. URL https://twitter.com/ArmenAgha/status/1669084129261162497.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.</p>
<p>Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural information processing systems, 24, 2011.</p>
<p>Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pp. 785-794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/ 2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785.</p>
<p>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. $176-181,2011$.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369-3391, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.222. URL https://aclanthology.org/2022.emnlp-main. 222.</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.</p>
<p>Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. $105-113,2022$.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. pp. 3816-3830, August 2021. doi: 10.18653/v1/2021.acl-long.295. URL https: //aclanthology.org/2021.acl-long. 295.</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.</p>
<p>Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. pp. 1935-1952, July 2023. doi: 10.18653/v1/ 2023.acl-long.108. URL https://aclanthology.org/2023.acl-long. 108.</p>
<p>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations, 2023.</p>
<p>Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, and Yejin Choi. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3631-3643, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.266. URL https://aclanthology.org/2022.naacl-main. 266.</p>
<p>Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4-22, 1985.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. pp. 80868098, May 2022. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology. org/2022.acl-long.556.</p>
<p>Aman Madaan, Katherine Hermann, and Amir Yazdanbakhsh. What makes chain-of-thought prompting effective? a counterfactual study. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1448-1535, 2023.</p>
<p>Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356-5371, 2021.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based instruction search for prompting large language models. pp. 3845-3864, May 2023. doi: 10.18653/ v1/2023.eacl-main.277. URL https://aclanthology.org/2023.eacl-main. 277.</p>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.</p>
<p>Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021.</p>
<p>Timo Schick, Sahana Udupa, and Hinrich Schütze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9:1408-1424, 2021.</p>
<p>John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022.</p>
<p>Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539, 2022.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. pp. 4222-4235, November 2020. doi: 10.18653/v1/2020.emnlp-main.346. URL https: //aclanthology.org/2020.emnlp-main. 346.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. pp. 5085-5109, December 2022. doi: 10.18653/v1/2022.emnlp-main. 340. URL https://aclanthology.org/2022.emnlp-main. 340.</p>
<p>Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=92gvk82DE-.</p>
<p>Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.</p>
<h1>A Grammar Definition and InStantIation Details</h1>
<h2>A. 1 Equivalence Relation Definition</h2>
<p>Precisely, $p_{1} \sim p_{2}$ if and only if at least one of the following hold: $p_{1}=p_{2}=B_{0}$; or $p_{i}=$ $B_{0}^{\prime}\left(d_{i}, s_{i}\right)$ with $d_{1}=d_{2}$; or $p_{i}=B_{1}\left(d_{i}, s_{i}, f_{i}\right)$ with $d_{1}=d_{2}$; or $p_{i}=B_{2}^{(n)}\left(X_{1, i}, \ldots, X_{n, i}, c_{i}\right)$ with $X_{j, 1} \sim X_{j, 2} \forall 1 \leq j \leq n$; or $p_{i}=B_{3}^{(n)}\left(d_{i}, j_{1, i}, \ldots, j_{n, i}, s_{1}, s_{2}, c, f\right)$ where $d_{1}=d_{2}$ and $j_{k, 1}=j_{k, 2} \forall 1 \leq k \leq n$. It is possible that generated formats equivalent in their string representation are not equivalent according to this equivalence relation.</p>
<h2>A.1.1 Visualization of Prompt Format's Parsing and Full Format Generation</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Visualization of a complex prompt format showing its parsing and which constants or functions affect each part of the format.</p>
<p>Figure 10 shows a visualization of how a complex format is parsed using our defined grammar. A full prompt consists of an instruction, n few-shots and a data point to solve. For example, if the instruction was Given a sentence and two words that appear in it, answer which one of the two (A or B) appeared first in the sentence., a full prompt may look as follow. Note that we always use $\backslash \mathrm{n} \backslash \mathrm{n}$ as space character between instruction and few-shots. The example below shows a 1-shot prompt. It is simply illustrative and does not correspond to any of the tasks considered.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Given</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="nt">words</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">appear</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">it</span><span class="o">,</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">which</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">B</span><span class="o">)</span><span class="w"> </span><span class="nt">appeared</span><span class="w"> </span><span class="nt">first</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">sentence</span><span class="o">.</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">quick</span><span class="w"> </span><span class="nt">brown</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="nt">jumps</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="o">):</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">B</span><span class="o">):</span><span class="w"> </span><span class="nt">brown</span>
<span class="nt">ANSWER</span><span class="o">:</span><span class="w"> </span><span class="nt">B</span>
<span class="nt">Over</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="o">):</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">B</span><span class="o">):</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">ANSWER</span><span class="o">:</span>
</code></pre></div>

<p>FormatSpreAd forces all instantiations of a multiple choice variable to change jointly to maintain coherence, and this includes text in the instruction. Therefore, when changing the option items from A and B to I and II, the prompt will be generated as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Given</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="nt">words</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">appear</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">it</span><span class="o">,</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">which</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">II</span><span class="o">)</span><span class="w"> </span><span class="nt">appeared</span><span class="w"> </span><span class="nt">first</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">sentence</span><span class="o">.</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">quick</span><span class="w"> </span><span class="nt">brown</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="nt">jumps</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="o">):</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">II</span><span class="o">):</span><span class="w"> </span><span class="nt">brown</span>
<span class="nt">ANSWER</span><span class="o">:</span><span class="w"> </span><span class="nt">II</span>
<span class="nt">Over</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="o">):</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">II</span><span class="o">):</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">ANSWER</span><span class="o">:</span>
</code></pre></div>

<h1>A. 2 ALLOWED VALUES FOR EACH SET $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}, \mathcal{F<em _ITEM="{ITEM" _text="\text">{\text {CASING }}, \mathcal{F}</em>$}</h1>
<p>$$
\begin{aligned}
&amp; \mathcal{S}<em 2="2">{1}=\left{{ }^{\prime \prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \backslash \mathrm{n}^{\prime}, \mathrm{n}^{\prime},--^{\prime}, \prime^{\prime}, \prime^{\prime} ; \backslash \mathrm{n}^{\prime}, \prime\left|^{\prime}, \prime&lt;\operatorname{sep}&gt;\right.^{\prime}, \prime--^{\prime}, \prime, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \prime, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \prime, \prime^{\prime}, \prime,^{\prime}\right} \
&amp; \mathcal{S}</em> \
&amp; \mathcal{C}=\left{{ }^{\prime \prime}, \prime::^{\prime}, \prime^{\prime}::^{\prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{n} \backslash \mathrm{t}^{\prime}, \backslash \mathrm{n}^{\prime}, \prime^{\prime}:^{\prime}, \prime,-^{\prime}, \prime^{\prime}, \prime^{\prime} \backslash \mathrm{n}^{\prime}, \backslash \mathrm{n} \backslash \mathrm{t}^{\prime}, \prime^{\prime}, \prime^{\prime}::^{\prime}, \prime-^{\prime}, \backslash \mathrm{t}^{\prime}\right} \
&amp; \mathcal{F}}=\left{{ }^{\prime \prime}, \prime^{\prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{t}^{\prime}\right} \text { (no space, single space, double space, tab) <em _item="{item" _text="\text">{\text {casing }}=\left{\mathbf{f}(\mathrm{x})=\mathrm{x}, \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { title }(), \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { upper }(), \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { lower }()\right} \
&amp; \mathcal{F}</em>}}=\left{x \mapsto f(g(x)) \mid \text { such that } f \in \mathcal{F<em _item2="{item2" _text="\text">{\text {item1 }} \wedge g \in \mathcal{F}</em>\right} \
&amp; \mathcal{F}}<em _item2="{item2" _text="\text">{\text {item1 }}=\left{\mathrm{x} \mapsto(\mathrm{x}), \mathrm{x} \mapsto \mathrm{x} ., \mathrm{x} \mapsto \mathrm{x}), \mathrm{x} \mapsto \mathrm{x} ., \mathrm{x} \mapsto[\mathrm{x}], \mathrm{x} \mapsto&lt;\mathrm{x}&gt;\right} \
&amp; \mathcal{F}</em>\right. \
&amp; \left.\mathrm{x} \rightarrow 0 \mathrm{x} 215 \mathrm{~F}+\mathrm{x}+1, \mathrm{x} \rightarrow \operatorname{ROMAN}[\mathrm{x}] . \text { lower }(), \mathrm{x} \rightarrow \operatorname{ROMAN}[\mathrm{x}] . \text { upper }()\right}
\end{aligned}
$$}}=\left{\mathrm{x} \rightarrow \mathrm{x}+1, \mathrm{x} \rightarrow{ }^{\prime} \mathrm{A}^{\prime}+\mathrm{x}, \mathrm{x} \rightarrow{ }^{\prime} \mathrm{a}^{\prime}+\mathrm{x</p>
<p>Enumerations are indexed from (i.e., "1, 2, 3" rather than "0, 1, 2"). ROMAN[x] represents the Roman numerals written in regular ASCII characters. ' $0 \times 215 \mathrm{~F}^{\prime}+\mathrm{x}$ represent the series of Unicode characters for Roman numerals. ... denotes a spacing character for clarity.</p>
<h2>A. 3 RESTRICTIONS TO PROMPT FORMATS SPACES AND SEPARATORS' COMBINATIONS</h2>
<p>We define several restrictions to ensure format naturalness. Users can additionally customize FORMATSPREAD by defining their own rules and restrictions between values. Our rules are as follows:</p>
<ul>
<li>If $B_{2}\left(X_{1}, \ldots, X_{n}, c\right)$ where $c$ does not contain a newline, then each $X_{i}$ 's separators and any subcomponents' separators should not contain a newline.</li>
<li>Similar to the rule above, if $B_{3}^{(n)}\left(d, j_{1}, \ldots, j_{n}, s_{1}, s_{2}, c, f_{1}, f_{2}\right)$ such that some separator contains a newline (i.e. $s_{1}$ contains a newline and/or $s_{2}$ contains a newline) then the space $c$ must also contain a newline.</li>
<li>For $B_{1}(d, s, f):=f(d) s&lt;$ text $&gt;, s$ must not be the empty string (i.e., there has to be some separation between descriptor and text).</li>
<li>Having $c$ be an empty string space in $B_{2}^{(n)}$ is only allowed if the first $n-1$ components are $B_{1}$ fields with an empty <text>. Similarly, the newline restrictions mentioned above only apply if the <text> is not empty. This rarely happens in prompt formats, but there are formats such as Question: <text> Options: A. <text> B. <text> where the Options: do not have a corresponding field.</li>
</ul>
<h2>A. 4 THOMPSON SAMPLING PriORS</h2>
<p>For the first exploration (i.e., finding the best-performing prompt format), we set an informative prior $\operatorname{Beta}(\alpha, \beta):=\operatorname{Beta}\left(\max \left(\frac{\beta \cdot x}{1-x}, 1.1\right), 5\right)$ for all arms $p_{i}$, where $x$ is the original format's accuracy. Our goal is to set an informative prior where the expected value of the prior distribution is the original format accuracy $x$, since a priori it is the only information we have about performance.</p>
<p>This restricts the parameters as follows:</p>
<p>$$
\begin{aligned}
\mathbb{E}[\operatorname{Beta}(\alpha, \beta)]=\frac{\alpha}{\alpha+\beta} &amp; =x \
\alpha &amp; =\alpha \cdot x+\beta \cdot x \
\alpha &amp; =\frac{\beta \cdot x}{1-x}
\end{aligned}
$$</p>
<p>Since $\beta$ will modulate how confident is the prior, and we want to avoid the model being overconfident, we fix $\beta=5$. Because we want to have an informative prior $\operatorname{Beta}(\alpha, \beta)$ with a Gaussian-like PDF, we force $\alpha&gt;1$ and $\beta&gt;1$. In extreme cases, forcing $\alpha&gt;1$ might alter the expected value. The first exploration's priors are thus exactly $\operatorname{Beta}(\alpha, \beta)$ with $\alpha=\max \left(\frac{\beta \cdot x}{1-x}, 1.1\right)$ and $\beta=5$ for all arms $p_{i}$.</p>
<p>For the second exploration (i.e., finding the worst-performing prompt format), the model has access to the first explorations' counters $S_{i}^{(E / B)}$ and $N_{i}^{(E / B)}$. Therefore, we set the second exploration's priors to be $\operatorname{Beta}\left(\alpha+S_{i}^{(E / B)}, \beta+\left(N_{i}^{(E / B)}-S_{i}^{(E / B)}\right)\right)$.</p>
<h1>B Additional Experiments' Information and Plots</h1>
<h2>B. 1 TASK SELECTION</h2>
<p>We use a number of heuristics to filter Super-NaturalInstructions tasks to our set of 53 evaluation tasks. Datasets should have at least 1000 samples to be considered. We also remove tasks whose instructions are too long (over 3,000 characters) and datasets with inputs longer than 2,000 characters, given that this makes performing inference at scale intractable. We also filter datasets whose valid outputs include more than 20 different strings, given that we focus on classification tasks.</p>
<p>We also removed tasks where we found a priori performance on the task was $0 \%$ accuracy using LLaMA-2-7B 1-shot. Some Super-NaturalInstructions tasks are derived from the same original dataset, but ask different questions. We did not include more than 4 tasks from the same original dataset.</p>
<p>Finally, we also searched for having socially impactful tasks. Those tasks were the only SuperNaturalInstructions tasks where we included a format if one was not provided by the dataset.</p>
<p>The selected tasks were the following 53: task050, task065, task069, task070, task114, task133, task155, task158, task161, task162, task163, task190, task213, task214, task220, task279, task280, task286, task296, task297, task316, task317, task319, task320, task322, task323, task325, task326, task327, task328, task335, task337, task385, task580, task607, task608, task609, task904, task905, task1186, task1283, task1284, task1297, task1347, task1387, task1419, task1420, task1421, task1423, task1502, task1612, task1678, task1724.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Comparison between Llama-2-7B and Llama-2-70B spreads. Llama-2-70B was computed using 4bit quantization (Dettmers et al., 2022).</p>
<p>B. 2 Additional Results for Section 4.2</p>
<p>Table 3: Ratio of prompt format pairs $\left(p_{1}, p_{2}\right)$ such that if $p_{1}$ is worse than $p_{2}$ using model $M_{1}$, then the same trend holds for $M_{2}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model 1 <br> $\left(M_{1}\right)$</th>
<th style="text-align: center;">Model 2 <br> $\left(M_{2}\right)$</th>
<th style="text-align: center;">Performance Relative <br> Ordering Preservation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">Llama-2-13b</td>
<td style="text-align: center;">$57.46 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">Falcon-2-7b</td>
<td style="text-align: center;">$55.91 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-7b</td>
<td style="text-align: center;">Falcon-7b-Inst</td>
<td style="text-align: center;">$61.11 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Probability of a prompt $p$ being worse than $p^{\prime}$ by at least $d$ points using model $M^{\prime}$, given that prompt $p$ was better than prompt $p^{\prime}$ when using model $M$.</p>
<p>Formats are not inherently good or bad. Table 3 shows that if format $p_{1}$ has lower performance than format $p_{2}$ under model $M$, there is $&lt;0.62$ probability that this trend would hold under another model $M^{\prime}$ (random chance is 0.5 ). This weak relative order preservation suggests that prompt format performance in a model may not be extrapolated to a different model, or in other words, that there are no inherently good or bad formats. This finding is further supported by Figure 12, which shows that findings of a format being better or worse than another are often inconsistent across models.</p>
<p>Experiments with exact prefix matching accuracy. Here we show results with using exact prefix matching to compute accuracy. Often, failures in prefix matching are associated with degeneration, i.e., cases where the model does not answer any of the valid options, motivating the use of ranking accuracy. Degeneration makes models (specially smaller models) more unlikely to have high accuracy out of the box. As seen in Figure 6, prefix matching is linked to having higher changes when performing atomic changes. Moreover, exact prefix matching can lead to lower performance as generation is less constrained (see Figure 16). Table 4 shows examples of atomic changes yielding large accuracy changes with exact prefix matching metric.</p>
<p>Figure 13c shows spread remains regardless of model size increase, architecture change, or number of few-shot examples also when using exact prefix matching as accuracy metric. In line with the results shown for probability ranking in Section 4.2, Figure 15 shows that the probability of reversing performance trends between two models just by changing prompt remains high when using exact prefix matching as metric. Strikingly, spread is significantly higher than in the probability ranking setting (see Figure 14), with median spread ranging from 12 to 28 accuracy points depending on the model used. This further motivates the need for running FORMATSPREAD when benchmarking models with this accuracy metric. This increased spread may be partly due to degeneration, as we will detail next.</p>
<p>Degeneration. Sometimes when a model does not generate the correct answer with exact prefix matching, it also does not generate a valid response, i.e. it degenerates. We will now quantify this phenomenon using 53 SuperNaturalInstructions classification and multiple choice tasks.</p>
<p>Given a model, a task, and a format, let the centered mass be the ratio of examples where the model's output matched with any valid option (regardless of correctness). Table 5 shows that the correlation between accuracy and centered mass is moderate or high depending on the model. This suggests that very often when a model does not return a valid answer, it does not return any valid answer</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Spread comparison between evaluating the same task under different models or n-shots using exact prefix matching as accuracy metric.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Spread across models and $n$-shots. Exact prefix matching metric.
<img alt="img-14.jpeg" src="img-14.jpeg" />
(a) Accuracy boxplot for the selected 53 Super Natural-Instructions tasks, option ranking metric.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 15: Probability that model $M$ performs worse than $M^{\prime}$ by at least $d$ when using format $p^{\prime}$, given that $M$ performed better than $M^{\prime}$ by at least $d$ using format $p .53$ tasks, 1- and 5-shot. Exact prefix matching metric.
<img alt="img-16.jpeg" src="img-16.jpeg" />
(b) Accuracy boxplot selected 53 Super NaturalInstructions tasks, exact prefix matching metric.</p>
<p>Figure 16: Accuracy metric used can strongly impact final performance. 53 Super NaturalInstructions tasks shown. Ranking accuracy yields higher accuracies overall.
at all. This is especially true for Falcon models, where we observe an almost perfect correlation between accuracy and centered mass. In conclusion, prompt format chosen often do not solely affect accuracy, but they also affect the frequency in which a model is actually able to perform a task. This will especially affect tasks for which there are no alternative metrics. Further research may focus specifically on targeting features that cause degeneration.</p>
<p>Experiments with Instruction Induction tasks. All experiments thus far focused solely on classification tasks. We will now focus on tasks that require generating (short) text, and cannot be framed as classification tasks. We selected 10 tasks from Instruction Induction (Honovich et al., 2023) that require generating a unique, valid string to be considered a correct response. Examples include identifying the second letter of a word, adding numbers, or answering a synonym to a given word. Instruction Induction tasks also show a wide range of difficulty, resulting in varied settings to be</p>
<p>Table 4: Examples of atomic changes' impact on accuracy using prefix matching (probability ranking shown in Table 2). $}$ represents a text field; $p_{2}$ yields higher accuracy than $p_{1}$ for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Id</th>
<th style="text-align: center;">Prompt Format $1\left(p_{1}\right)$</th>
<th style="text-align: center;">Prompt Format $2\left(p_{2}\right)$</th>
<th style="text-align: center;">Acc $p_{1}$</th>
<th style="text-align: center;">Acc $p_{2}$</th>
<th style="text-align: center;">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">task213</td>
<td style="text-align: center;">Title: {} Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;">Title:({} Sentence $&lt;$ X $&gt;$ : : {}</td>
<td style="text-align: center;">0.113</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">| Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;">| Sentence $&lt;$ X $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : {} } }</td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&lt;$ i $&gt;$ : : : {} } }</td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Answer: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task296</td>
<td style="text-align: center;">Sentence I : : {} \nSentence II</td>
<td style="text-align: center;">Sentence I ) : {} \nSentence II</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.321</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">) : {} \nSentence III ) : {}</td>
<td style="text-align: center;">) : {} \nSentence III ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence IV ) : {} \nSentence</td>
<td style="text-align: center;">\nSentence IV ) : {} \nSentence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">V ) : {} \nSentence VI ) : {}</td>
<td style="text-align: center;">V ) : {} \nSentence VI ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence VII ) : {} \nSentence</td>
<td style="text-align: center;">\nSentence VII ) : {} \nSentence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VIII ) : {} \nSentence IX ) : {}</td>
<td style="text-align: center;">VIII ) : {} \nSentence IX ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence X ) : {}, I : {},</td>
<td style="text-align: center;">\nSentence X ) : {}, I, : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">II. : {}, Answer: {}</td>
<td style="text-align: center;">, 2. : {}, Answer: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task905</td>
<td style="text-align: center;">Tweet::: {}, \nLabel::: {},</td>
<td style="text-align: center;">Tweet::{}, \nLabel::{},</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.307</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nAnswer::: {}</td>
<td style="text-align: center;">\nAnswer:: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task317</td>
<td style="text-align: center;">Passage:: {} \nAnswer:: {}</td>
<td style="text-align: center;">Passage::{} \nAnswer::{}</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.301</td>
</tr>
<tr>
<td style="text-align: center;">task280</td>
<td style="text-align: center;">passage {}\n answer {}</td>
<td style="text-align: center;">passage: {}\n answer: {}</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;">task050</td>
<td style="text-align: center;">SENTENCE - {} \nQUESTION - {}</td>
<td style="text-align: center;">SENTENCE\n\t{} \nQUESTION\n\t{}</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nANSWER - {}</td>
<td style="text-align: center;">\nANSWER\n\t{}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task070</td>
<td style="text-align: center;">Beginning - {}\nMiddle [I]{}</td>
<td style="text-align: center;">Beginning - {}\nMiddle I]{}</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.157</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">, Middle [II]{}\nEnding -</td>
<td style="text-align: center;">, Middle II]{}\nEnding -</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">{} \nAnswer - {}</td>
<td style="text-align: center;">{} \nAnswer - {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Correlation between accuracy using exact prefix matching and the centered mass (the opposite of degeneration). 53 tasks, 10 formats each, evaluated on 1000 samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">n-shot</th>
<th style="text-align: center;">correlation between accuracy <br> $\&amp;$ and centered mass</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">$5.1 \mathrm{E}-77$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">$4.9 \mathrm{E}-98$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">$5.8 \mathrm{E}-61$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">$9.2 \mathrm{E}-67$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">$7.1 \mathrm{E}-233$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">$8.4 \mathrm{E}-228$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">$3.6 \mathrm{E}-289$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">$5.5 \mathrm{E}-277$</td>
</tr>
</tbody>
</table>
<p>analyzed (see Figure 18b). Given that the collection does not contain human-generated formats, we applied a simple 'Input: {}\n Output: {}' format. Results for 1-shot and 5-shot settings show spread is still high across models and n-shot choices (see Figure 17).</p>
<p>Tasks are: antonyms, diff, first_word_letter, larger_animal, letters_list, num_to_verbal, second_word_letter, singular_to_plural, sum, synonyms.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 17: Spread comparison between evaluating the same task under different models or n-shots for Instruction Induction tasks. Exact prefix matching used as accuracy metric.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 18: Instruction Induction tasks' spreads and accuracy across models. Exact prefix matching is used as accuracy metric.</p>
<p>Experiments with continuous metrics in open-ended text generation tasks. Throughout the paper we focus on tasks with a single valid output, whether in classification tasks or in short-text generation tasks. This decision is intentional, since it guarantees that a variation in the metric truly represents a variation in model performance. We have shown that spread remains high when considering option ranking or exact prefix matching as accuracy metric.</p>
<p>Since LLMs are often used in more open-ended generation contexts, we will now explore the performance variance across prompt formats when considering sentence-length generation tasks (e.g. generate the next sentence of a story, given the four initial sentences of a story, generate a question whose answer is the sentence given). To analyze the automatic generations, we use two widely used metrics: ROUGE-L (Lin, 2004), and BERTScore (Zhang et al., 2019). The first is an n-gram-based metric, and the latter is a model-based metric, and both are $[0,1]$ metrics where higher is better. Figure 19 shows that variance remains high for LLaMA-2-7B regardless of the metric and the number of n-shots considered, with LLaMA-2-7B 5-shot having $25 \%$ of tasks with a ROUGE-L spread of 0.098 or higher, and a BERTScore spread of 0.09 or higher.</p>
<p>We observe that the median spread is sometimes smaller than in the accuracy tasks. This may be because although ROUGE, BERTScore, and accuracy are all $[0,1]$ metrics, typical metric values may be different, which may in turn affect the final spread (an absolute difference). We leave it to future work to quantify the differences in style or content that each format may be inducing.</p>
<p>Finally, it is worth noting that text generation metrics are known to be noisier, and thus not all metric decreases necessarily correspond to a true performance loss, as is the case for accuracy in single-valid-output tasks. We used 17 SuperNatural Instructions tasks: task037, task038,
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 19: Spread across n-shots for LLaMA-2-7B, considering ROUGE-L and BERTScore metrics. 17 sentence-level open-generation tasks are considered, all extracted from SuperNatural Instructions. 10 prompt formats are considered for each task.
task040, task067, task071, task072, task105, task216, task223, task240, task348, task389, task443, task845, task1326, task1401, task1613. We selected the 17 open-ended text generation tasks among those with at least 1000 samples, with some formatting present in the original task (e.g. 'Passage: ' <text>). We only considered tasks whose instructions were under 1,000 characters and that contained inputs no longer than 5,000 characters.</p>
<p>We limit generations to 50 tokens. To parse model outputs more faithfully, and given that none of our expected generations include a newline, we only consider a model’s generation up to the first newline (excluding leading spaces and newlines in a given generation). This consideration is important given that often models start to generate a new data sample from scratch, immediately after generating the requested answer.
Characterizing a model's accuracy distribution beyond spread. Spread gives a quantitative jump in information with respect to informing a single point in the performance distribution since it measures the distribution range (maximum minus minimum). However, distributions that may share the same range, may yield a widely different probability of obtaining each value in the distribution. Figure 20 plots the accuracy distribution of 30 tasks, sorted in decreasing order by standard deviation. Tasks with high standard deviation reflect a higher likelihood of obtaining dissimilar values when making a formatting selection; Figure 20 shows that the median standard distribution is $\sigma \approx 0.04$, which can be considered high in our context.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 20: Accuracy distribution across 500 formats for 30 tasks evaluated on 250 samples each, sorted by standard deviation in decreasing order. LLaMA-2-7B 1-shot, option ranking metric.</p>
<p>On factors influencing spread besides prompt formatting. We believe many factors beyond formatting may be influencing performance variance, but were unable to find a feature that reliably predicts spread. We found that the average prompt length in a task has a negligible correlation with its performance spread: $r=0.228\left(p=1.4 \times 10^{-7}\right)$ for exact prefix matching metric, and $r=-0.022(p=0.615)$ for option ranking metric, when jointly considering all models and nshots. Similarly, the standard deviation of the prompt length had negligible correlation with spread: $r=0.125(p=0.004)$ for exact prefix matching, and $r=-0.099(p=0.024)$ for option ranking metric. When considering each model individually, only LLaMA-2-7B with exact prefix matching showed a correlation $|r|&gt;0.5$, with the average prompt length having a correlation $r=0.559$ $p=6.86 \times 10^{-10}$. All other settings had $|r|&lt;0.36$.</p>
<h1>B. 3 PCA EXAMPLES</h1>
<p>Section 4.4 systematically analyzes whether we can predict the prompt format that generated a given pre-softmax activation layer (i.e., prompt embeddings) by using solely its top- $n$ principal components. Figure 21 shows the top two principal components for two different tasks where all 10 formats considered are easily identifiable solely with a prompt embedding's top two principal compoenents.
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 21: Plot of the top two principal components of the last decoder layer of the prompt, as a representation of the output probability distribution. Two different tasks shown, with each prompt format shown in a different color.</p>
<h2>B. 4 Notable Features</h2>
<p>As discussed in Section 4.3, sometimes the choice of a constant may lead to significantly different accuracy ranges. Figures 22,23, and 24 show all strongly dissimilar choices of constants found on any given task, across 53 Super Natural-Instructions tasks, and on both accuracy metrics considered throughout the work. As can be appreciated, choices of constants do not consistently predict performance in isolation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We thoroughly describe the limitations of our method in Appendix C.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
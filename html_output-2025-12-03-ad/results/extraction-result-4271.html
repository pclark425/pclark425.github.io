<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4271 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4271</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4271</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-270351696</p>
                <p><strong>Paper Title:</strong> Fine-tuning large language models for chemical text mining</p>
                <p><strong>Paper Abstract:</strong> Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists. The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature. This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition, reaction role labelling, metal–organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the conversion of reaction paragraphs to action sequences. The fine-tuned LLMs demonstrated impressive performance, significantly reducing the need for repetitive and extensive prompt engineering experiments. For comparison, we guided ChatGPT (GPT-3.5-turbo) and GPT-4 with prompt engineering and fine-tuned GPT-3.5-turbo as well as other open-source LLMs such as Mistral, Llama3, Llama2, T5, and BART. The results showed that the fine-tuned ChatGPT models excelled in all tasks. They achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data. They even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data. Notably, fine-tuned Mistral and Llama3 show competitive abilities. Given their versatility, robustness, and low-code capability, leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4271.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4271.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses large language models as tools to read scientific literature and extract structured knowledge and, speculatively, to distill patterns, hypotheses, or theories from the corpus of domain literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning large language models for chemical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generic LLMs (e.g. GPT-3.5/GPT-4, Mistral, Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>speculative literature distillation / fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The paper speculates that LLMs, when fine-tuned or combined with curated laboratory databases, could incorporate extensive 'fuzzy knowledge' from scientific literature to identify experimental patterns, predict properties/outcomes, and generate new chemical hypotheses or theories. No concrete multi-step pipeline for extracting qualitative laws is implemented in this work; the paper frames this as a future research direction involving (i) aggregation of formatted experimental data, (ii) fine-tuning or multimodal expansion of LLMs on that data, and (iii) using the models to surface empirical generalizations or hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>chemistry / materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors assert that fine-tuned LLMs can reliably extract structured experimental data from literature and suggest that the same capability could be extended to distill higher-level patterns or hypotheses from aggregated literature, but this is presented as conjecture rather than demonstrated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>This is speculative: the paper notes open challenges including hallucination, ambiguity in human language, formatting compliance, token-length/context-window limits, multimodal content (figures/diagrams), and the need for curated/annotated data and permissions for copyrighted literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4271.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4271.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo (supervised fine-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors fine-tuned GPT-3.5-turbo on multiple chemical text mining tasks (entity extraction, reaction role labelling, MOF information, NMR extraction, and procedure→action conversion) and showed strong, often state-of-the-art, performance for structured extraction from chemical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning large language models for chemical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>supervised fine-tuning (jsonl upload to OpenAI; few epochs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The authors formatted extraction tasks as sequence-to-sequence pairs (input paragraph → formatted structured output), converted training data to jsonl and uploaded to OpenAI's fine-tuning endpoint. Fine-tuning used small, task-specific annotated datasets (from tens to thousands of examples), few epochs, low learning rates, and selection of best epoch by evaluation-set performance. They compared prompt-only (zero-/few-shot) in-context learning with supervised fine-tuning and found fine-tuning produced more stable, prompt-independent outputs that aligned with strict formatting requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varied by task (examples: Paragraph2Compound sampled up to 10,000 examples from millions of USPTO patent paragraphs; Paragraph2MOFInfo dataset: 658 paragraphs total; Paragraph2NMR: 600 annotations; Paragraph2Action: 1,060 hand-annotated training items, augmented to 14,168 in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>chemistry (chemical literature, patents, experimental procedures, MOF synthesis, NMR reports)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>task-specific automatic metrics and human evaluation for format compliance: entity-level precision/recall/F1, sequence-level Levenshtein similarity, exact match accuracy, modified BLEU, and manual inspection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Task-level highlights reported: Paragraph2Compound F1 approaching ~90% (with sufficient fine-tuning data); Paragraph2RXNRole product extraction F1 = 77.1% and role labelling F1 = 83.0% (both outperforming prior SOTA models); Paragraph2MOFInfo exact match accuracy single-reaction = 82.7%, multi-reaction = 68.8%; Paragraph2Action full-sentence exact accuracy up to 69.0% after augmentation (with 14,168 examples), modified BLEU and Levenshtein similarity also improved. The abstract summarizes exact accuracy ranges ~69%–95% across tasks for fine-tuned ChatGPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against prompt-only GPT-3.5/GPT-4 few-shot and prior task-specific models (e.g. ChemBERT, ChemRxnBERT, task-adaptively pre-trained transformers). Fine-tuned GPT-3.5-turbo outperformed prompt-only in-context methods and often outperformed or matched SOTA task-adapted models that used far more in-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervised fine-tuning of GPT-3.5 on modest, high-quality annotated datasets yields robust, prompt-independent extraction of structured data from chemical literature and outperforms prompt-only usage; fine-tuning converges quickly with small data and generalizes efficiently; fine-tuned models are practical, low-code toolkits for automated curation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Fine-tuning costs (monetary and compute) grow with data; inability to fine-tune GPT-4 at time of study; hallucination risk reduced but not eliminated; strict formatting requirements can still fail on ambiguous inputs; copyright/access to literature; multimodal content not handled; risk of overfitting with too many epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4271.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4271.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (prompt-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (used in prompt engineering / in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used with prompt engineering (zero-/few-shot) to attempt extraction tasks; it sometimes produced plausible text but underperformed compared with fine-tuned models on strict exact-match/formatted extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning large language models for chemical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>prompt engineering / in-context learning (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The authors used zero-shot and few-shot prompts (examples up to the context/window limit) to guide GPT-4 to extract structured data from chemical paragraphs. They progressively increased the number of in-context examples (up to token/window limits) to measure in-context learning performance and compared it to fine-tuning on equal-sized example sets.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>chemistry (chemical literature)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>automatic metrics (exact match accuracy, BLEU, Levenshtein similarity) on held-out test sets for extraction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prompt-only GPT-4 reached limited performance on complex transformation tasks (e.g. Paragraph2Action: best prompt-only GPT-4 with 60-shot in-context learning achieved ~32.7% full-sentence exact accuracy, BLEU 65.0, Levenshtein similarity 72.8), substantially below fine-tuned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to fine-tuned GPT-3.5 and fine-tuned open-source LLMs; in-context GPT-4 performed worse on strict exact-match extraction tasks despite its stronger general capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In-context learning with GPT-4 is attractive (no training) but fragile for strict, formatted extraction across diverse paragraphs, and often requires extensive prompt engineering that still underperforms supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Context/token window limits constrain how many examples can be provided; prompt engineering is time-consuming and unstable; outputs may violate exact formatting required for downstream databases or robotic execution; hallucinations and inconsistency persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4271.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4271.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source LLMs (Mistral / Llama3 / Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative open-source instruction-tuned LLMs (Mistral-7b-instruct-v0.2, Llama3-8b-instruct, Llama2-13b-chat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors fine-tuned several open-source LLMs and found that Mistral and Llama3, when fine-tuned, showed competitive performance with proprietary GPT-3.5 on chemical text mining tasks and offer local deployment advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning large language models for chemical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7b-instruct-v0.2; Llama3-8b-instruct; Llama2-13b-chat (qlora fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral), 8B (Llama3), 13B (Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>full-parameter fine-tuning (and Q-LoRA for Llama2) on task-specific annotated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Authors performed full-parameter fine-tuning (on 4×40GB A100 for Mistral & Llama3) and Q-LoRA fine-tuning for Llama2-13b-chat to adapt models to sequence-to-sequence extraction tasks. They adjusted hyperparameters (learning rates, LoRA settings) and used multitask learning to improve generation of long multi-attribute outputs for some models (e.g., T5/BART required multitask learning). Inference speed was improved using vllm.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varied by task (same datasets used as for GPT experiments: e.g., USPTO-sourced paragraphs, MOF dataset of ~658 paragraphs, NMR 600 annotations, action datasets 1,060 to 14,168 augmented examples)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>chemistry / materials science (chemical literature)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>automatic metrics (F1, Levenshtein similarity, exact match accuracy, modified BLEU) and inspection of post-processing-free output ratios</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned Mistral-7b and Llama3-8b achieved competitive exact-match and similarity metrics on extraction tasks; for Paragraph2Action Mistral fine-tuned on 1,060 examples achieved ~64.8% full-sentence exact accuracy (comparable to GPT-3.5 fine-tuned). Many outputs required little to no post-processing (>99% post-processing-free for some LLMs) compared to T5/BART.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against fine-tuned GPT-3.5 and prompt-only GPT-3.5/GPT-4 as well as task-specific BERT-like models and seq2seq baselines (T5/BART). Open-source fine-tuned models were competitive, narrowing the gap with proprietary GPTs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-source LLMs, when carefully fine-tuned (including Q-LoRA where needed), can perform close to proprietary models on structured extraction tasks, offering a practical route for local deployment and lower-cost extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Some seq2seq open-source models (T5/BART) struggled with exact formatting and tokenization/vocabulary limitations, needing additional post-processing. Fine-tuning large open-source models requires GPU resources; user must manage hyperparameters and inference optimization (vllm).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis <em>(Rating: 2)</em></li>
                <li>Automated extraction of chemical synthesis actions from experimental procedures <em>(Rating: 2)</em></li>
                <li>An Extensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT <em>(Rating: 1)</em></li>
                <li>Automated chemical reaction extraction from scientific literature <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4271",
    "paper_id": "paper-270351696",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LLMs (general)",
            "name_full": "Large Language Models (general)",
            "brief_description": "The paper discusses large language models as tools to read scientific literature and extract structured knowledge and, speculatively, to distill patterns, hypotheses, or theories from the corpus of domain literature.",
            "citation_title": "Fine-tuning large language models for chemical text mining",
            "mention_or_use": "mention",
            "model_name": "generic LLMs (e.g. GPT-3.5/GPT-4, Mistral, Llama)",
            "model_size": null,
            "method_name": "speculative literature distillation / fine-tuning",
            "method_description": "The paper speculates that LLMs, when fine-tuned or combined with curated laboratory databases, could incorporate extensive 'fuzzy knowledge' from scientific literature to identify experimental patterns, predict properties/outcomes, and generate new chemical hypotheses or theories. No concrete multi-step pipeline for extracting qualitative laws is implemented in this work; the paper frames this as a future research direction involving (i) aggregation of formatted experimental data, (ii) fine-tuning or multimodal expansion of LLMs on that data, and (iii) using the models to surface empirical generalizations or hypotheses.",
            "number_of_papers": null,
            "domain_or_field": "chemistry / materials science",
            "type_of_laws_extracted": null,
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "The authors assert that fine-tuned LLMs can reliably extract structured experimental data from literature and suggest that the same capability could be extended to distill higher-level patterns or hypotheses from aggregated literature, but this is presented as conjecture rather than demonstrated in the paper.",
            "challenges_limitations": "This is speculative: the paper notes open challenges including hallucination, ambiguity in human language, formatting compliance, token-length/context-window limits, multimodal content (figures/diagrams), and the need for curated/annotated data and permissions for copyrighted literature.",
            "uuid": "e4271.0",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5-turbo (fine-tuned)",
            "name_full": "OpenAI GPT-3.5-turbo (supervised fine-tuned variant)",
            "brief_description": "The authors fine-tuned GPT-3.5-turbo on multiple chemical text mining tasks (entity extraction, reaction role labelling, MOF information, NMR extraction, and procedure→action conversion) and showed strong, often state-of-the-art, performance for structured extraction from chemical literature.",
            "citation_title": "Fine-tuning large language models for chemical text mining",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (OpenAI)",
            "model_size": null,
            "method_name": "supervised fine-tuning (jsonl upload to OpenAI; few epochs)",
            "method_description": "The authors formatted extraction tasks as sequence-to-sequence pairs (input paragraph → formatted structured output), converted training data to jsonl and uploaded to OpenAI's fine-tuning endpoint. Fine-tuning used small, task-specific annotated datasets (from tens to thousands of examples), few epochs, low learning rates, and selection of best epoch by evaluation-set performance. They compared prompt-only (zero-/few-shot) in-context learning with supervised fine-tuning and found fine-tuning produced more stable, prompt-independent outputs that aligned with strict formatting requirements.",
            "number_of_papers": "varied by task (examples: Paragraph2Compound sampled up to 10,000 examples from millions of USPTO patent paragraphs; Paragraph2MOFInfo dataset: 658 paragraphs total; Paragraph2NMR: 600 annotations; Paragraph2Action: 1,060 hand-annotated training items, augmented to 14,168 in some experiments)",
            "domain_or_field": "chemistry (chemical literature, patents, experimental procedures, MOF synthesis, NMR reports)",
            "type_of_laws_extracted": null,
            "example_laws_extracted": null,
            "evaluation_method": "task-specific automatic metrics and human evaluation for format compliance: entity-level precision/recall/F1, sequence-level Levenshtein similarity, exact match accuracy, modified BLEU, and manual inspection",
            "performance_metrics": "Task-level highlights reported: Paragraph2Compound F1 approaching ~90% (with sufficient fine-tuning data); Paragraph2RXNRole product extraction F1 = 77.1% and role labelling F1 = 83.0% (both outperforming prior SOTA models); Paragraph2MOFInfo exact match accuracy single-reaction = 82.7%, multi-reaction = 68.8%; Paragraph2Action full-sentence exact accuracy up to 69.0% after augmentation (with 14,168 examples), modified BLEU and Levenshtein similarity also improved. The abstract summarizes exact accuracy ranges ~69%–95% across tasks for fine-tuned ChatGPT models.",
            "comparison_baseline": "Compared against prompt-only GPT-3.5/GPT-4 few-shot and prior task-specific models (e.g. ChemBERT, ChemRxnBERT, task-adaptively pre-trained transformers). Fine-tuned GPT-3.5-turbo outperformed prompt-only in-context methods and often outperformed or matched SOTA task-adapted models that used far more in-domain data.",
            "key_findings": "Supervised fine-tuning of GPT-3.5 on modest, high-quality annotated datasets yields robust, prompt-independent extraction of structured data from chemical literature and outperforms prompt-only usage; fine-tuning converges quickly with small data and generalizes efficiently; fine-tuned models are practical, low-code toolkits for automated curation.",
            "challenges_limitations": "Fine-tuning costs (monetary and compute) grow with data; inability to fine-tune GPT-4 at time of study; hallucination risk reduced but not eliminated; strict formatting requirements can still fail on ambiguous inputs; copyright/access to literature; multimodal content not handled; risk of overfitting with too many epochs.",
            "uuid": "e4271.1",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 (prompt-only)",
            "name_full": "OpenAI GPT-4 (used in prompt engineering / in-context learning)",
            "brief_description": "GPT-4 was used with prompt engineering (zero-/few-shot) to attempt extraction tasks; it sometimes produced plausible text but underperformed compared with fine-tuned models on strict exact-match/formatted extraction tasks.",
            "citation_title": "Fine-tuning large language models for chemical text mining",
            "mention_or_use": "use",
            "model_name": "GPT-4 (OpenAI)",
            "model_size": null,
            "method_name": "prompt engineering / in-context learning (few-shot)",
            "method_description": "The authors used zero-shot and few-shot prompts (examples up to the context/window limit) to guide GPT-4 to extract structured data from chemical paragraphs. They progressively increased the number of in-context examples (up to token/window limits) to measure in-context learning performance and compared it to fine-tuning on equal-sized example sets.",
            "number_of_papers": null,
            "domain_or_field": "chemistry (chemical literature)",
            "type_of_laws_extracted": null,
            "example_laws_extracted": null,
            "evaluation_method": "automatic metrics (exact match accuracy, BLEU, Levenshtein similarity) on held-out test sets for extraction tasks",
            "performance_metrics": "Prompt-only GPT-4 reached limited performance on complex transformation tasks (e.g. Paragraph2Action: best prompt-only GPT-4 with 60-shot in-context learning achieved ~32.7% full-sentence exact accuracy, BLEU 65.0, Levenshtein similarity 72.8), substantially below fine-tuned performance.",
            "comparison_baseline": "Compared to fine-tuned GPT-3.5 and fine-tuned open-source LLMs; in-context GPT-4 performed worse on strict exact-match extraction tasks despite its stronger general capabilities.",
            "key_findings": "In-context learning with GPT-4 is attractive (no training) but fragile for strict, formatted extraction across diverse paragraphs, and often requires extensive prompt engineering that still underperforms supervised fine-tuning.",
            "challenges_limitations": "Context/token window limits constrain how many examples can be provided; prompt engineering is time-consuming and unstable; outputs may violate exact formatting required for downstream databases or robotic execution; hallucinations and inconsistency persist.",
            "uuid": "e4271.2",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Open-source LLMs (Mistral / Llama3 / Llama2)",
            "name_full": "Representative open-source instruction-tuned LLMs (Mistral-7b-instruct-v0.2, Llama3-8b-instruct, Llama2-13b-chat)",
            "brief_description": "The authors fine-tuned several open-source LLMs and found that Mistral and Llama3, when fine-tuned, showed competitive performance with proprietary GPT-3.5 on chemical text mining tasks and offer local deployment advantages.",
            "citation_title": "Fine-tuning large language models for chemical text mining",
            "mention_or_use": "use",
            "model_name": "Mistral-7b-instruct-v0.2; Llama3-8b-instruct; Llama2-13b-chat (qlora fine-tuned)",
            "model_size": "7B (Mistral), 8B (Llama3), 13B (Llama2)",
            "method_name": "full-parameter fine-tuning (and Q-LoRA for Llama2) on task-specific annotated datasets",
            "method_description": "Authors performed full-parameter fine-tuning (on 4×40GB A100 for Mistral & Llama3) and Q-LoRA fine-tuning for Llama2-13b-chat to adapt models to sequence-to-sequence extraction tasks. They adjusted hyperparameters (learning rates, LoRA settings) and used multitask learning to improve generation of long multi-attribute outputs for some models (e.g., T5/BART required multitask learning). Inference speed was improved using vllm.",
            "number_of_papers": "varied by task (same datasets used as for GPT experiments: e.g., USPTO-sourced paragraphs, MOF dataset of ~658 paragraphs, NMR 600 annotations, action datasets 1,060 to 14,168 augmented examples)",
            "domain_or_field": "chemistry / materials science (chemical literature)",
            "type_of_laws_extracted": null,
            "example_laws_extracted": null,
            "evaluation_method": "automatic metrics (F1, Levenshtein similarity, exact match accuracy, modified BLEU) and inspection of post-processing-free output ratios",
            "performance_metrics": "Fine-tuned Mistral-7b and Llama3-8b achieved competitive exact-match and similarity metrics on extraction tasks; for Paragraph2Action Mistral fine-tuned on 1,060 examples achieved ~64.8% full-sentence exact accuracy (comparable to GPT-3.5 fine-tuned). Many outputs required little to no post-processing (&gt;99% post-processing-free for some LLMs) compared to T5/BART.",
            "comparison_baseline": "Compared against fine-tuned GPT-3.5 and prompt-only GPT-3.5/GPT-4 as well as task-specific BERT-like models and seq2seq baselines (T5/BART). Open-source fine-tuned models were competitive, narrowing the gap with proprietary GPTs.",
            "key_findings": "Open-source LLMs, when carefully fine-tuned (including Q-LoRA where needed), can perform close to proprietary models on structured extraction tasks, offering a practical route for local deployment and lower-cost extraction pipelines.",
            "challenges_limitations": "Some seq2seq open-source models (T5/BART) struggled with exact formatting and tokenization/vocabulary limitations, needing additional post-processing. Fine-tuning large open-source models requires GPU resources; user must manage hyperparameters and inference optimization (vllm).",
            "uuid": "e4271.3",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Automated extraction of chemical synthesis actions from experimental procedures",
            "rating": 2,
            "sanitized_title": "automated_extraction_of_chemical_synthesis_actions_from_experimental_procedures"
        },
        {
            "paper_title": "An Extensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT",
            "rating": 1,
            "sanitized_title": "an_extensive_benchmark_study_on_biomedical_text_generation_and_mining_with_chatgpt"
        },
        {
            "paper_title": "Automated chemical reaction extraction from scientific literature",
            "rating": 2,
            "sanitized_title": "automated_chemical_reaction_extraction_from_scientific_literature"
        }
    ],
    "cost": 0.015348249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-tuning large language models for chemical text mining †</p>
<p>Wei Zhang 
Qinggong Wang 
Nanjing University of Chinese Medicine
138 Xianlin Road210023NanjingChina</p>
<p>Xiangtai Kong 
Jiacheng Xiong 
Shengkun Ni 
Duanhua Cao 
Buying Niu 
Mingan Chen 
Yameng Li 
ProtonUnfold Technology Co
Ltd, SuzhouChina</p>
<p>Runze Zhang 
Yitian Wang 
Lehan Zhang 
Xutong Li 
Zhaopi Xiong 0000-0002-3323-3092
ProtonUnfold Technology Co
Ltd, SuzhouChina</p>
<p>Qian Shi 
Lingang Laboratory
200031ShanghaiChina</p>
<p>Ziming Huang 
Medizinische Klinik und Poliklinik I
Klinikum der Universität München
Ludwig-Maximilians-Universität
MunichGermany</p>
<p>Zunyun Fu fuzunyun@simm.ac.cn 
Drug Discovery and Design Center
State Key Laboratory of Drug Research
Shanghai Institute of Materia Medica
Chinese Academy of Sciences
555 Zuchongzhi Road201203ShanghaiChina</p>
<p>Mingyue Zheng myzheng@simm.ac.cn </p>
<p>University of Chinese Academy of Sciences
No. 19A Yuquan Road100049BeijingChina</p>
<p>College of Pharmaceutical Sciences
Innovation Institute for Articial Intelligence in Medicine of Zhejiang University
Zhejiang University
310058HangzhouZhejiangChina</p>
<p>School of Physical Science and Technology
ShanghaiTech University
201210ShanghaiChina</p>
<p>Fine-tuning large language models for chemical text mining †
EE258A80EFF6DD33EE851342B48D641310.1039/d4sc00924jReceived 6th February 2024 Accepted 2nd June 2024
Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists.The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature.This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition, reaction role labelling, metal-organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the conversion of reaction paragraphs to action sequences.The fine-tuned LLMs demonstrated impressive performance, significantly reducing the need for repetitive and extensive prompt engineering experiments.For comparison, we guided ChatGPT (GPT-3.5-turbo)and GPT-4 with prompt engineering and fine-tuned GPT-3.5-turbo as well as other opensource LLMs such as Mistral, Llama3, Llama2, T5, and BART.The results showed that the fine-tuned ChatGPT models excelled in all tasks.They achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data.They even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data.Notably, finetuned Mistral and Llama3 show competitive abilities.Given their versatility, robustness, and low-code capability, leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction.</p>
<p>Introduction</p>
<p>Chemical text mining is a crucial foundation in chemical research.It creates extensive databases that provide access to physicochemical properties and synthetic routes for experimental chemists.Additionally, it accumulates rich data and insights for computational chemists to use for modelling and predicting.More than just extracting information from chemical texts, the rule-based transformation of chemical text is particularly interesting.4][5] This allows them to be understood and executed by robotics for automated syntheses.</p>
<p>However, converting structured data from intricate scientic literature is a challenging task, especially due to the complexity and heterogeneity of chemical language.As a result, a number of text-mining tools have been developed.For instance, Chem-DataExtractor 6,7 was created to extract chemical entities and their associated properties, measurements and relationships from chemical documents, using unsupervised word clustering, conditional random elds, rule-based grammar and dictionary matching.ChemRxnExtractor, 8 a BERT-like model, was designed to extract the product and label associated reaction roles such as the reactant, catalyst, solvent, and temperature from paragraphs of synthesis experiments.Vaucher et al. 1,2 developed task-adaptive pre-trained transformers to convert the synthesis protocol paragraphs into action sequences.Syn-thReader 3 was built to convert literature syntheses to executable XDL formats, containing a series of domain-specic algorithms with predened rules.Historically, the focus has been on designing models and algorithms specic to certain tasks, requiring extensive domain knowledge and sophisticated data processing.These tools, challenging to adapt for diverse extraction tasks, oen require complementary collaboration to manage complex information extraction tasks, thus limiting their versatility and practicality.</p>
<p>Recently, large language models (LLMs), represented by ChatGPT released in November 2022, have shown the potential for Articial General Intelligence (AGI).LLMs, such as GPT-3.5 and GPT-4, can generate logical insights or content that meets requirements based on human instructions.We are entering a new era where AGI and medicinal chemists might work together.0][11] However, LLMs tend to "hallucinate", meaning they generate unintended text that misaligns with established facts and real-world knowledge. 12,13Moreover, objectively evaluating the results of open-ended questions remains a signicant challenge.</p>
<p>At this juncture, LLMs may still nd it difficult to accurately answer factual and knowledge-based questions.However, using LLMs for knowledge extraction tasks should greatly alleviate hallucination and fully leverage their powerful text comprehension and processing capabilities, making them promising universal tools for chemical text mining.For instance, Zheng et al. 14 used prompt engineering to guide ChatGPT in extracting information about metal-organic framework (MOF) synthesis.Patiny et al. 15 tried to use ChatGPT to extract FAIR (Findable, Accessible, Interoperable, Reusable) data from publications.However, their approach of using LLMs simply based on prompt engineering tends to achieve poor performance in exact accuracy.According to the biomedical benchmark study by Chen et al., 16 ChatGPT performed signicantly worse on biomedical text mining compared to existing models.These ndings seem to contradict the common belief in the LLMs' superior comprehension abilities.Either way, LLMs have limitations due to their model architecture and memory, including a maximum length of prompt tokens.Besides, human expressions can be ambiguous, incomplete, vague, and difficult to rene.Outputs may not strictly adhere to formatting requirements, leading to misunderstanding and poor performance in mining complex text, such as patents or scientic literature.Therefore, zero-shot or few-shot prompts are oen insufficient to address the diversity of scenarios and cannot guarantee the quality of extracted data.</p>
<p>In this study, we extensively explored the effectiveness of ne-tuning LLMs on ve challenging tasks in chemical text mining: compound entity recognition, reaction role annotation, metal-organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and conversion reaction paragraphs into action sequences.We found that ne-tuning GPT models signicantly enhances performance in text mining tasks, compared to prompt-only versions, while also reducing dependency on the repetitive and extensive prompt engineering experiments.Meanwhile, we also evaluated prevalent generative pre-trained language models, such as Mistral, 17 Llama3, 18 Llama2, 19 T5, 20 and BART. 21Among these, ne-tuned ChatGPT (GPT-3.5-turbo)models achieved state-of-the-art (SOTA) performance across all ve tasks.Remarkably, it even outperformed models that have been trained specically for each task and subsequently netuned, based on a signicantly larger amount of in-domain data.This study highlights the potential of ne-tuning LLMs to revolutionize complex knowledge extraction with their versatility, robustness, and low code capability.Fine-tuned LLMs can be easily generalizable and can optimize the laborintensive and time-consuming data collection workow, even with few data.This will accelerate the discovery and creation of novel substances, making them powerful tools for universal use.</p>
<p>Results and discussion</p>
<p>Overview of chemical text mining tasks</p>
<p>Given the complex and diverse information embedded in chemical literature, we designed ve extraction tasks to demonstrate the potential and practicality of LLMs in chemical text mining (Fig. 1).The Paragraph2Compound task is a relatively simple task, aiming to extract all chemical compound entities from the given paragraph.The Paragraph2RXNRole task is to label the reaction roles including the product, reactant, catalyst, temperature, solvent, time, and yield in the paragraph.The Paragraph2MOFInfo task is to extract all MOF synthesis information including the compound name, metal source, metal amount, linker, linker amount, modulator, modulator amount or volume, solvent, solvent volume, reaction temperature and reaction time.The Paragraph2NMR task is designed to extract the IUPAC name, experimental conditions including frequency and solvent as well as chemical shi data for both 1 H NMR and 13 C NMR spectra.The Paragraph2Action task is to convert experimental procedures to structured synthetic steps (action sequences).The details of datasets used for the ve chemical text mining tasks are listed in Table S1.† All tasks are unied to sequence-to-sequence formats to facilitate the use of LLMs.The details about using LLMs with promptengineering and ne-tuning can be found in the Methods section.</p>
<p>Paragraph2Compound-extract all chemical entities.Fig. 2a illustrates the process of random sampling from millions of paragraph-entity pairs, which refer to UPSTO annotations.It starts by randomly selecting 10 000 samples, followed by randomly picking 1000, then 100, and nally 10.This sampling process ensures that each smaller subset is included in the larger one, with each subset used for individual training.Fig. 2b demonstrates the performance of prompt-only models and netuned models, which are evaluated on a consistent evaluation set of 1000 samples across varying training data sizes.These results are obtained from three independent trials.In the case of prompt-only models, randomness is intentionally introduced by altering the prompt and examples (Fig. 2c and S2 †).Given the task's straightforward nature and clear instructions, even the prompt-only language models achieved decent F1 scores over 0.6.For ne-tuned models, the sampling and training process for the training set is repeated three times, as depicted in Fig. 2a.As shown in Fig. 2b, all ne-tuned models demonstrate a performance improvement, especially in terms of the F1 score and Jaccard index, proportional to the increase in dataset size.These models outperform the prompt-only models designed for this task.When the training data size is substantial enough, the F1 scores of the ne-tuned models can reach close to 90%, and the Jaccard index can approach 80%.Notably, ne-tuned LLMs such as GPT-3.5-turboshowed minimal uctuations and superior performance.However, it is essential to emphasize that the cost of ne-tuning GPT-3.5-turboincreased tenfold with each tenfold increase in data volume.Our experimentation was capped at 10 000 training samples for 3 epochs due to OpenAI's limitations, resulting in a nearly 90-dollar expense to ne-tune GPT-3.5-turbo-alow cost-effective investment in computational resources.In contrast, other ne-tuned language models have displayed notable cost advantages in this relatively simple compound name entity recognition task.</p>
<p>Paragraph2RXNRole-extract the product and label the reaction role.According to Guo et al., 8 the Paragraph2RXNRole task comprises two subtasks.The rst is to extract the central product, and the second is to label the associated reaction roles within specied paragraphs (Fig. 3a).For the two tasks, Guo et al. developed two-stage BERT-like token-multi-classication models.To enable a fair comparison with generative language models, we converted the data into sequence-to-sequence formats by adding <Role*Compound*Role> annotations to the input paragraphs.We then converted the language models' outputs back into lists of BIO-tags, followed by post-processing to align with the original BIO-tag labels for assessment.Notably, even when utilizing prompt engineering with 20-shot examples (Fig. S3 and S4 †), GPT-3.5 and GPT-4 perform poorly on two Paragraph2RXNRole tasks, which may result from the complicated syntax cases and limited context length (Fig. 3b and c).However, the ne-tuned GPT models perform well.</p>
<p>For product extraction, the ne-tuned GPT-3.5-turbo(best over one epoch) achieved an F1 score of 77.1%, slightly surpassing the previous SOTA approach, ChemBERT, which scored 76.2% (Fig. 3b).For reaction role labelling, the netuned GPT-3.5-turbo(best over ve epochs) achieved an F1 score of 83.0%, signicantly outperforming the previous SOTA approach, ChemRxnBERT, which scored 78.7% (Fig. 3c).It's notable that the ne-tuned GPT-3.5-turbomodels, which cost only $1 and $5 respectively, demonstrated extremely high costeffectiveness with small training datasets.In contrast, Chem-BERT was domain-adaptive pre-trained on 9 478 043 sentences from 200 000 journal articles, and ChemRxnBERT was further task-adaptive trained on 944 733 reaction-inclusive sentences.We should also mention that the outputs of ne-tuned GPTs, Mistrals and Llamas align almost perfectly with the input text, with over 99% post-processing-free ratios.On the other hand,  S7. † (c) Performance of reaction role labelling.Concrete values can be found in Table S8.† most outputs of ne-tuned T5 and BART require additional alignment due to their tokenization and vocabulary limitations, with a ratio of only 31% that does not require post-processing.Even aer post-processing, the F1 scores of T5 and BART were signicantly lower than those of token-classication BERT-like models or large language models.</p>
<p>Paragraph2MOFInfo-extraction of MOF synthesis information.Our re-annotated dataset for the Paragraph2MOFInfo task displayed in Fig. 4a mostly contains single reaction paragraphs with a few featuring multiple reactions.We used Levenshtein similarity and exact accuracy as metrics to objectively assess the models' ability to extract formatted data that fully comply with the customized requirements in the task.This approach is more objective and accurate with less manual intervention, compared to the manual analysis and evaluation used by Zheng et al. 14 The dataset is divided into a training set and a test set, each containing 329 samples.We evaluated the performance of ne-tuned GPT-3.5-turbo by varying the size of training data from 10 to 329, and observed convergence on the testing set, suggesting saturation in the amount of training data (Fig. 4b).The ne-tuned GPT-3.5-turbosignicantly outperforms the GPT-3.5-turbowith prompt engineering, improving exact match accuracy by over 20% for both single and multiple reactions (Fig. 4c, and S5 †).It also surpasses other ne-tuned models, especially when handling complex multi-reaction paragraphs.</p>
<p>Exact accuracy rates for single and multiple reactions are 82.7% and 68.8%, respectively (Fig. 4c).As depicted in Fig. 4d and e, while most models achieve high Levenshtein similarity across the 11 parameters, only a few maintain high exact accuracy, which is the golden metric that we mainly focus on.</p>
<p>Considering that some MOF synthesis paragraphs may include multiple reactions, we provide an example of multireaction extraction by various models in Fig. 4f.The paragraph includes two reactions, the rst with (R)-H3PIA and bipy as linkers, providing all reaction conditions explicitly, and the second with the substitution of (R)-H3PIA with (S)-H3PIA, keeping all other conditions unchanged.Most models successfully interpreted the semantics and extracted two reactions from the MOF synthesis paragraph.However, only the ne-tuned ChatGPT perfectly extracted information that matched our annotated ground truth.Other models showed varying degrees of incompleteness, particularly with items involving multiple components and their quantities.</p>
<p>Paragraph2NMR-extract NMR chemical shis and conditions.The impact of training set sizes and the use of prompt engineering on the performance of ne-tuning GPT-3.5-turbo in extracting NMR information is illustrated in Fig. 5a.Regardless of the training data size for ne-tuning (ranging from 25 to 300), or the presence of prompt engineering, there are hardly any signicant uctuations in performance.This holds true for metrics such as Levenshtein similarity and exact match accuracy of the ne-tuned GPT-3.5-turbo when the numbers of training samples exceed 50.This demonstrates the strong learning capability and robustness of LLMs.Fig. 5b illustrates the performance of different generative language models using the same 200 training data.In terms of Levenshtein similarity, a metric based on the edit distance, almost all ne-tuned language models achieved impressive scores, outperforming GPT models that solely rely on prompt engineering (Fig. 5b and S6 †).However, when considering the exact match accuracy metric, where each character must perfectly align with the ground truth count, LLMs such as GPTs, Mistral, and Llama3 take the lead.Though ne-tuned T5 and BART manage to extract the majority of the text, they oen miss or mistakenly copy several characters.This contributes to a signicant decrease in their exact match accuracy metric, as shown in Fig. 5c.In this context, the extraction of long complex text by LLMs is more standardized and high-quality, aligning more closely with human expectations.It is worth noting that using carefully designed prompts has almost no impact on the results, which proves that the ne-tuned LLMs are prompt independent.Most importantly, ne-tuning open-source LLMs such as Mistral-7b-instruct-v-0.2 and Llama3-8b-instruct provides an alternative approach for deploying text mining locally, given its exceptionally high exact match accuracy.</p>
<p>Paragraph2Action-action sequence extracted from an experimental procedure.The above-mentioned extraction tasks simply require the model to replicate specic information from the paragraph.However, the Paragraph2Action task requires the model to understand and transform the paragraph and convert experimental procedures to structured synthetic steps (action sequences).Clearly, GPT models with prompt engineering have difficulty with this task, especially when it involves multiple complex conversions and insufficient prompt descriptions (Table 1, Fig. S7 †).To gauge the maximum potential of GPT models using only prompts, we incrementally increased the number of transformation examples from 6 to 60.Despite encompassing all types of actions at least once and nearly reaching the token limit of 4096 for GPT-3.5-turbo and 8192 for GPT-4, their performance in the few-shot scenario remains disappointingly poor.The currently best-performing LLM GPT-4 with 60 examples for in-context learning achieved only 32.7% full sentence exact accuracy, a BLEU score of 65.0, and a Levenshtein similarity of 72.8.However, ne-tuning pretrained language models with a small amount of data could yield decent results (Table 1).Remarkably, ne-tuning LLMs such as Mistral-7b-instruct-v0.2 and GPT-3.5-turbo on 1060 hand annotated training data, we achieved 64.8% and 63.6% full sentence exact accuracy.The ne-tuning process took only 8 minutes (2 epochs) for Mistral on 4 × A100 and 1 hour (4 epochs) for GPT-3.5-turbo.These metrics surpass the SOTA results previously reported by Vaucher et al., 1 which used an ensemble of three models, each task-adaptively pre-trained on 2 million rule-based data and rened on 14 168 augmented data.Interestingly, further improvement was achieved by augmenting the training data size to 14 168 when ne-tuning GPT-3.5turbo.This resulted in 69.0%full sentence exact accuracy, an 86.4 modied BLEU score, and an 89.9% Levenshtein similarity (Table 1).For autonomous robots, it is challenging to generate instructions that follow strict syntax rules.Fine-tuning LLMs plays a crucial role in bridging the gap between fuzzy natural language and structured machine-executable programming languages, signicantly improving the accuracy of customization with a small amount of annotated data.In similar tasks involving "fuzzy rules" or hard-to-dene extraction, ne-tuning LLMs might offer considerable advantages in tailoring the transformation.</p>
<p>Comparison of different methods for chemical text mining</p>
<p>Chemical text mining expedites scientic discovery in chemistry.Previously, tasks involving complex chemical language and sophisticated processing depended on rule-based matching algorithms and custom-built domain-specic models.Now, leveraging universal LLMs' semantic understanding, long context window, and generation abilities offers promising and general approaches.These methods are illustrated in Fig. 6 Undoubtedly, leveraging LLMs with prompt engineering is the most attractive approach because it does not require writing any code or retraining model parameters, only interacting with the large model through natural language instructions.However, relying solely on instructions without any examples (zero-shot) also makes it difficult to standardize the output of LLMs, which is crucial for formatting data extraction tasks.In the case of extracting NMR based solely on instructions (Fig. S8 †), we repeatedly modify the instructions to ensure that the model can generate expected formatting results on a certain paragraph.However, when we used this carefully designed prompt for other paragraphs containing NMR, the extraction  S12 and S13.† (c) Examples of error extractions by T5 and BART, compared with the ground truth.Edge Article Chemical Science results did not meet the qualied formatting requirements again.This zero-shot approach resulted in poor performance across all ve tasks, even using GPT-4.</p>
<p>Apart from instructions, providing few example pairs of paragraph-extraction as context can help LLMs learn the extraction patterns.In these few-shot sceneries (Fig. 2c, S2-S7 †), as shown in Table 1, increasing the number of examples leads LLMs to extract more structured outputs.Ideally, the whole training set should serve as context.However, the upper limit of in-context learning is constrained by the maximum input length due to the memory limitation.The versions of GPT-3.5-Turbo-0613 and GPT-4-0613 we tested were limited to 4096 and 8192 tokens, respectively.Hence, comparing prompt engineering methods in zero-shot and few-shot sceneries to ne-tuned models trained with complete datasets can be somewhat unfair.</p>
<p>To compare the performance of in-context learning and netuning approaches objectively, we should use an equal number of examples for both context and the ne-tuning data set.Here, we tested the latest version of GPT-3.5-turbo-0125, which expands the context length to 16 K and supports ne-tuning.We used a variety of action sequences during sampling to cover as many action types as possible.As the number of examples increased from 30 to 60, 90 and 120, both the performances of in-context learning and ne-tuning are increasing (Table S14 †).Even when the same number of examples was provided for in-context learning as ne-tuning, the ne-tuned model typically outperforms by 10-20% on metrics like exact match accuracy and modied BLEU score.This could be attributed to information loss in in-context learning, while ne-tuning adjusts parameters to learn extraction patterns, thus maintaining higher accuracy.</p>
<p>In the test, we also nd two features of ne-tuning LLMs: rapid performance convergence with small amounts of data and efficient training generalization.For the four tasks utilizing manually annotated data, the LLM's performance rapidly improved and converged with increasing sample sizes (Fig. S11 †).This highlights that hundreds of high-quality data are enough to train an effective extractor, which is typically a manageable workload for manual annotation.Besides, LLMs can be easily adapted for specic text extraction tasks, requiring only a few epochs and a low learning rate for ne-tuning (Table S3 †).However, they are also prone to overtting if trained for an excessive number of epochs.</p>
<p>Promising performance and potential of ne-tuning LLMs on chemical data mining.In this study, we have demonstrated the impressive efficacy, exibility, and high exact accuracy of ne-tuning LLMs, regarding all kinds of text mining tasks as generative problems.An examination of incorrect predictions revealed that only a small proportion were entirely incorrect, while most were acceptable alternatives to the ground truth or even pointed out the incorrect labels (Fig. S12-S16 †).These errors can be attributed to inconsistent annotation standards and the inherent ambiguity of terms with multiple interpretations or functions.Therefore, improving the formatted data extraction requires continuous efforts, including the renement of specic rules and the enrichment of annotations prone to misinterpretation during training and inference.With detailed specications and high-quality formatted data, the ne-tuning method based on LLMs is highly reliable.</p>
<p>Starting with ve chemical extraction tasks, we have proved the effectiveness of ne-tuning LLMs in the relatively small testing sets.This approach, when utilized for large-scale extraction in the future, promises to greatly improve data collection efficiency and accelerate scientic research and experimentation.For the Paragraph2MOFInfo task, we can document the synthesis conditions along with other key information such as MOF structures, pore characteristics, and functional performance.Using these data, we can develop machine learning models to optimize the synthesis novel MOF materials with functions such as new catalysts, gas storage and separation.For the Para-graph2NMR task, we can collect extensive NMR data with the corresponding compound names from millions of synthesis literature documents.This can help create an NMR database for retrieving similar spectra and structures, as well as constructing predictive models to identify molecules structures and analysing complex mixtures, which support drug development and quality control.For the action sequence transformation task, the extracted information is benecial for automatic and robotic synthesis.It will improve reproducibility and minimize human errors, especially in high-throughput experiments.</p>
<p>Apart from the ve mentioned extraction tasks, it can be easily extended to tasks related to extracting information from scientic literature and transforming data into a simple user-friendly reaction format 22 that is both human-and machine-readable.This approach will signicantly contribute to the development of extensive databases like the Open Reaction Database, 23,24 Sci-Finder 25 and Reaxys, 26 which gather comprehensive synthesis data through automated curation and expert verication, to make data more ndable, accessible, interoperable, and reusable (FAIR).</p>
<p>Nevertheless, leveraging ne-tuned LLMs is still insufficient to extract all synthesis information from chemical literature, which contains extensive complex gure and form contents.Recently, some tools have been developed to recognize molecular images 27,28 and reaction diagrams 29,30 from the literature.Integrating LLMs with these image recognition tools or developing advanced large multimodal models (LMMs) may be a promising unied solution for further chemical data mining.Notably, when extracting large amounts of data from copyrighted literature, it's essential to access the necessary permissions from scientic publications.</p>
<p>Herein, we have scratched the surface of the vast potential of LLMs in chemistry and materials science by ne-tuning LLMs for chemical text mining.We may notice that the gap between opensource language models and proprietary GPTs (GPT-3.5-turboand GPT-4) has been narrowing from Llama2 to Llama3 and Mistral.This progress is due to the concerted efforts of researchers and communities in the direction of LLMs.Technically, advancements like more effective ne-tuning strategies, improved open-source model architectures, faster inference approaches, wider context windows, higher quality corpus, and lower computational costs in the era of LLMs are anticipated to further enhance text mining.Meanwhile, it's more essential to consider what else can be achieved with LLMs and how we can develop more effective LLMs for chemistry and materials science.For instance, LLMs have the potential to revolutionize predictive modelling by incorporating the extensive "fuzzy knowledge" encapsulated within scientic literature, especially in chemistry and drug discovery.By combining empirical results with documented knowledge, LLMs could assist chemists identify patterns in experiments that might otherwise be missed, predict properties of compounds and outcomes of reactions, and even generate new chemical hypotheses and theories.Furthermore, the integration of LLMs' comprehension with specialized tools could substantially lower the barrier of chemists to use these tools throughout the entire workow, thanks to interactive interfaces in natural language.Future research could investigate how to merge formatted laboratory data with the wealth of information in scientic literature and develop the multimodal capability to enrich specic domain knowledge for LLMs.This endeavour will require a sustained, long-term effort.</p>
<p>Conclusions</p>
<p>In this work, we have demonstrated the effectiveness of netuning LLMs in chemical text mining.We conducted ve complex tasks: compound entity recognition, reaction role labelling, MOF synthesis information extraction, NMR data extraction, and the transformation of reaction procedures to action sequences.Chemical text mining remains a challenging professional domain when leveraging language model mining, even with prompt engineering.However, LLMs that are netuned with appropriate annotations can produce structured outputs that perfectly full human requirements not easily expressed in natural language.This feature fully utilizes their natural language understanding and formatting capability.Using chemical text mining as an example, this study provides guidance on ne-tuning of LLMs to serve as universal knowledge extraction toolkits.These toolkits can be easily extended for automated extraction from documents and rule-based formatted transformations.Our work lays the groundwork for the applications of LLMs in information extraction within the chemical domain, which will catalyse data-driven innovations in chemical and materials science.</p>
<p>Methods</p>
<p>Dataset preparation</p>
<p>For the Paragraph2Compound task, we compiled an automatically annotated dataset.This dataset is based on the publicly accessed USPTO subset extracted by Lowe et al., 31,32 and includes millions of chemical reaction paragraphs from patents, each paired with compound tags.We used regular expressions to identify compound labels within each paragraph, separating them with the "j" symbol based on their sequential occurrence in the paragraph.For the Paragraph2RXNRole task, we used the manually annotated dataset by Guo et al., 8 following the same data partitioning strategy.We transformed the data from the BIO-token classication format into a sequence-tosequence format using the annotation scheme "<Role*compound*Role>".We processed paragraphs containing multiple central products and related reactions into several input and output pairs.For the Paragraph2MOFInfo task, we manually checked and re-annotated the raw data of Zheng et al., 14 transforming them into a sequence-to-sequence format.This dataset comprises MOF synthesis paragraphs, extraction by ChatGPT, and human-evaluated answers.For the Para-graph2NMR task, we manually curated a dataset of 600 highquality annotations.These were mainly sourced from various literature studies on PubMed to ensure a wide diversity.The task aims to extract information such as the IUPAC name, experimental conditions, including the frequency and solvent, and chemical shi data from both 1 H NMR and 13 C NMR spectra.For the Paragraph2Action task, we utilized the handannotated dataset by Vaucher et al., employing the same data partitioning strategy.This dataset is derived from the Pistachio dataset by NextMove soware. 33The details of datasets used for the ve chemical text mining tasks are listed in Table S1.†</p>
<p>Prompt-only ChatGPT</p>
<p>Prompt-only interaction enables users to efficiently communicate with large language models through simple prompts.This guides the model to produce relevant responses without further training.In a zero-shot scenario, the model generates responses using only a descriptive prompt and its pre-trained knowledge.However, in a few-shot approach, the model uses a small number of examples to improve its understanding and responses.To maximize the performance, we selected diverse examples and ensured a large number of tokens.We interacted with ChatGPT using API keys and employed model versions GPT-3.5-turbo-0613 and GPT-4-0613.The zero-shot and fewshot prompts for chemical text mining tasks can be found in Fig. S2-S8.†</p>
<p>Fine-tuning ChatGPT</p>
<p>Since late August 2023, supervised ne-tuning capabilities have been available for the GPT-3.5-turbomodel. 34The aim is to enhance performance in specic scenarios customized based on private data.In this study, we ne-tuned the GPT-3.5-turbo-0613model for chemical text mining scenarios on ve tasks.When discussing the performance in the Comparison of different methods for chemical text mining section, we netuned the latest GPT-3.5-turbo-0125model for fair comparison, which expanded the context length to 16 K and supported ne-tuning as well.We formatted the data into jsonl and uploaded them to OpenAI's cloud servers, and then initiated ne-tuning jobs.Once the training was complete, the ne-tuned GPT-3.5-turbomodel was ready for inference.API keys were requisite throughout the training and inference procedures.Fine-tuning for the GPT-4-turbo model is not available now and is highly expected in the future.</p>
<p>Fine-tuning open-source language models</p>
<p>We selected the most widely used and representative generative pre-trained language models such as Mistral, 17 Llama3, 18 Llama2, 19 T5, 20 and BART. 21These serve as baselines for a comprehensive comparison with the ne-tuned ChatGPT across ve chemical text mining tasks.Considering performance, efficiency, and hardware resource constraints, we used full parameter ne-tuning for Mistral-7b-instruct-v0.2 and Llama3-8b-instruct on 4 × 40 GB A100, and full parameter netuning for BART-base and T5-base on 1 × 40 GB A100.We applied multitask-learning to BART and T5 in the Para-graph2MOFInfo task and Paragraph2NMR task due to their limitations in generating multi-attribute long sentences (Fig. S9 and S10 †), aiming to enhance their performance.This approach signicantly improved their performance.For Llama2, we used Q-LoRA 35 to efficiently ne-tune llama2-13b-chat on 1 × 40 GB A100.This method maintains most of the performance of full parameter ne-tuning while signicantly reducing computational demands.We used vllm 36 to speed up the inference of LLMs such as Mistral-7b-instruct-v0.2, Llama3-8b-instruct, and Llama2-13b-chat, which is tens of times faster than Hugging Face's pipeline.The inference of all ne-tuned models can run on 1 × 40 GB A100.To ensure optimal performance, we adjusted hyperparameters such as learning rates, lora_r, and lora_alpha during the ne-tuning process of baseline models (Table S2 †).The hardware resources, memory cost, and runtimes of ne-tuning are provided for reference (Table S3 †).More details of training, pre-processing, and post-processing can be found in the ESI.†</p>
<p>Metrics for evaluation</p>
<p>Since ne-tuning ChatGPT does not allow for early stopping based on optimal validation loss, we report the performances of all models at the best epoch selected from the evaluation set for fair comparison.Given the task specics, we use metrics including precision, recall, and F1 score for evaluating entitylevel performance.For sentence-level performance assessment, we use Levenshtein similarity, exact match accuracy, partial accuracy, and a modied BLEU score.</p>
<p>Fig. 1
1
Fig. 1 Schematics of cheminformatics insights to be extracted from paragraphs.And illustration of the five practical tasks in chemical text mining with the respective example outputs, including Paragraph2Compound, Paragraph2RXNRole, Paragraph2MOFInfo, Paragraph2NMR, and Paragraph2Action.</p>
<p>Fig. 2
2
Fig. 2 (a) The workflow of sampling and training based on the USPTO dataset for the Paragraph2Compound task.(b) The performance of different models across varying sizes of the training set.The data point and the shaded areas represent, respectively, the mean values and standard deviations derived from three independent trials.(c) Example of the zero-shot and few-shot prompts utilized.</p>
<p>Fig. 3
3
Fig. 3 (a) Data formats of two subtasks in the Paragraph2RXNRole task.(b) Performance of product extraction.Concrete values can be found in TableS7.† (c) Performance of reaction role labelling.Concrete values can be found in TableS8.†</p>
<p>Fig. 4
4
Fig. 4 (a) A statistic of the Paragraph2MOFInfo dataset.(b) The performance of fine-tuned GPT-3.5-turboacross varying sizes of the training set.(c) Mean performance of Levenshtein similarity and exact match accuracy for extracting paragraphs containing single reactions and multiple reactions, respectively, by different models.Concrete values can be found in Table S9.† (d) Levenshtein similarity for 11 parameters in the Paragraph2MOFInfo task.Concrete values can be found in TableS10.† (e) Exact match accuracy for 11 parameters in the Paragraph2MOFInfo task.Concrete values can be found in TableS11.† (f) An example of extractions by different models from a multi-reaction MOF synthesis paragraph.The cells in yellow represented the ground truth.The cells in green represented the exact match predictions.The cells in blue represented the incorrect predictions.</p>
<p>. In prompt engineering scenarios, LLMs' parameters remain xed, solely relying on the provided examples to extract from new paragraphs.As for the training and ne-tuning process, a model learns the statistic extraction patterns from the training data by adjusting and optimizing the internal parameters.</p>
<p>Fig. 5
5
Fig. 5 (a) The performance of fine-tuned GPT-3.5-turbo with and without prompt engineering as it varies with training data size in the Par-graph2NMR task.(b) Heat map illustrating Levenshtein similarity and exact match accuracy of various models in extracting NMR information.Concrete values can be found in TablesS12 and S13.† (c) Examples of error extractions by T5 and BART, compared with the ground truth.</p>
<p>Fig. 6
6
Fig. 6 Diagram of different approaches for text extraction.</p>
<p>Table 1
1
Performance on the Paragraph2Action task a
Cost905 mean tokens1374 mean tokens1670 mean tokens2598 mean tokens3610 mean tokens861 mean tokens1357 mean tokens1631 mean tokens2546 mean tokens3611 mean tokens7010 mean tokens, $ 41-6 min on 1 × 40 GB A10010 min on 1 × 40 GB A10040 min on 1 × 40 GB A10030 min on 4 × 40 GB A1008 min on 4 × 40 GB A1004 epochs, total 1 h, $ 4-30 min on 1 × 40 GB A100100 min on 1 × 40 GB A1005 hours on 1 × 40 GB A100100 min on 4 × 40 GB A10030 min on 4 × 40 GB A1005 epochs, total 1.5 h, $ 92---Levenshteinsimilarity59.462.364.365.866.054.569.263.065.167.772.845.983.986.886.086.388.788.176.484.887.187.584.887.289.985.786.786.6Modied BLEUscore38.643.144.447.049.544.751.453.856.759.865.022.573.281.880.382.285.984.864.774.484.184.381.484.386.481.584.385.075% acc.34.742.342.645.547.244.951.156.558.261.663.321.977.683.280.783.285.582.762.880.182.484.180.486.486.980.481.882.490% acc.16.819.323.325.926.423.330.733.035.840.043.815.165.971.666.870.273.671.647.768.574.171.667.073.378.167.370.571.3100%acc.8.28.813.114.813.913.420.721.922.726.132.713.151.157.756.859.764.863.637.852.059.762.256.064.269.056.859.460.8StrategyPrompt engineeringwithout ne-tuningPrompt engineeringwithout ne-tuningNo task-adaptivepretraining and ne-tuningon hand-annotated data (1060)No task-adaptive pretraining and ne-tuning on augmenteddata (14 168)ne-Task-adaptive pretraining (2 M) andtuning on hand-annotatedata (1060)ne-Task-adaptive pretraining (2 M) andtuning on augmenteddata (14 168)ModelGPT-3.5-turbo (6-shot)GPT-3.5-turbo (12-shot)GPT-3.5-turbo (18-shot)GPT-3.5-turbo (24-shot)GPT-3.5-turbo (30-shot)GPT-4 (6-shot)GPT-4 (12-shot)GPT-4 (18-shot)GPT-4 (24-shot)GPT-4 (30-shot)GPT-4 (60-shot)Transformer (single model)<em>BART-base (ne-tuned)T5-base (ne-tuned)Lama2-13b-chat (qlora ne-tuned)Lama3-8b-instruct (ne-tuned)Mistral-7b-instruct-v0.2 (ne-tuned)GPT-3.5-turbo (ne-tuned)Transformer (single model)</em>BART-base (ne-tuned)T5-base (ne-tuned)Llama2-13b-chat (qlora ne-tuned)Lama3-8b-instruct (ne-tuned)Mistral-7b-instruct-v0.2 (ne-tuned)GPT-3.5-turbo (ne-tuned)Transformer (single model)<em>Transformer (single model)</em>Transformer (ensemble)<em>
aThe symbol "</em>" represented the results reported by Vaucher et al.The result in black bold is the best performance.The details of ne-tuning cost can be found in TableS3.©2024The Author(s).Published by the Royal Society of Chemistry Chem.Sci., 2024, 15, 10600-10611 | 10607</p>
<p>© 2024 The Author(s). Published by the Royal Society of Chemistry
AcknowledgementsWe thank all contributions of the open-source community on LLMs.We appreciate Yaghi's group for guiding in ChatGPT prompt engineering for chemistry tasks.This work was supported by the National Natural Science Foundation of China (T2225002 and 82273855 to M. Y. Z. and 82204278 to X. T. L.), the National Key Research and Development Program of China (2022YFC3400504 to M. Y. Z.), the SIMM-SHUTCM Traditional Chinese Medicine Innovation Joint Research Program (E2G805H to M. Y. Z.), the Shanghai Post-doctoral Excellence Program (2023693 to Z. Y. F.) and the Shanghai Municipal Science and Technology Major Project.Data availabilityAll data and code of this work are available at GitHub: https:// github.com/zw-SIMM/SFTLLMs_for_ChemText_Mining to allow replication of processing, ne-tuning and evaluation.All concrete values of performance in gures are listed in Section 5 of the ESI.†Author contributionsConflicts of interestThere are no conicts to declare.
Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, F Zipoli, J Geluykens, V H Nair, P Schwaller, T Laino, Nat. Commun. 36012020</p>
<p>Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis. M Suvarna, A C Vaucher, S Mitchell, T Laino, J Pérez-Ramírez, Nat. Commun. 79642023</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 3702020</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, Science. 22112019</p>
<p>AI-driven robotic chemist for autonomous synthesis of organic molecules. T Ha, D Lee, Y Kwon, M S Park, S Lee, J Jang, B Choi, H Jeon, J Kim, H Choi, Sci. Adv. 4612023</p>
<p>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientic literature. M C Swain, J M Cole, J. Chem. Inf. Model. 562016</p>
<p>ChemDataExtractor 2.0: autopopulated ontologies for materials science. J Mavracic, C J Court, T Isazawa, S R Elliott, J M Cole, J. Chem. Inf. Model. 612021</p>
<p>Automated chemical reaction extraction from scientic literature. J Guo, A S Ibanez-Lopez, H Gao, V Quach, C W Coley, K F Jensen, R Barzilay, J. Chem. Inf. Model. 622021</p>
<p>Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. C M Castro Nascimento, A S Pimentel, J. Chem. Inf. Model. 632023</p>
<p>Comparing the Performance of College Chemistry Students with ChatGPT for Calculations Involving Acids and Bases. T M Clark, E Anderson, N M Dickson-Karn, C Soltanirad, N Tani, J. Chem. Educ. 1002023</p>
<p>. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, 10.48550/arXiv.2108.09926arXiv:2305.18365arXiv, 2023, preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Comput. Surv. 552023</p>
<p>. Y Zhang, Y Li, L Cui, D Cai, L Liu, T Fu, X Huang, E Zhao, Y Zhang, Y Chen, 10.48550/arXiv.2309.01219arXiv:2309.01219arXiv. 2023preprint</p>
<p>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>. L Patiny, G Godin, 10.26434/chemrxiv-2023-05v1b-v2ChemRxiv. 2023</p>
<p>. Q Chen, H Sun, H Liu, Y Jiang, T Ran, X Jin, X Xiao, Z Lin, H Chen, Z Niu, An Extensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT. 5572023Bioinformatics</p>
<p>. A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, 10.48550/arXiv.2310.06825arXiv:2310.06825arXiv, 2023, preprint</p>
<p>Llama3. April 26, 2024</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, 10.48550/arXiv.2307.09288arXiv:2307.09288arXiv. 2023preprint</p>
<p>Exploring the limits of transfer learning with a unied text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 212020</p>
<p>. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, 10.48550/arXiv.1910.13461arXiv:1910.13462019preprint</p>
<p>. D F Nippa, A T Müller, K Atz, D B Konrad, U Grether, R E Martin, G Schneider, 10.26434/chemrxiv-2023-nfq7h-v2ChemRxiv. 2024preprint</p>
<p>The open reaction database. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, J. Am. Chem. Soc. 1432021</p>
<p>Data sharing in chemistry: lessons learned and a case for mandating structured reaction data. R Mercado, S M Kearnes, C W Coley, J. Chem. Inf. Model. 632023</p>
<p>SciFinder. August 29, 2023</p>
<p>Reaxys. August 29, 2023</p>
<p>aExtractor: a system for automatic extraction of chemical information from biomedical literature. J Xiong, X Liu, Z Li, H Xiao, G Wang, Z Niu, C Fei, F Zhong, G Wang, W Zhang, Sci. China: Life Sci. 672023</p>
<p>MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation. Y Qian, J Guo, Z Tu, Z Li, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. Y Qian, J Guo, Z Tu, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>ReactionDataExtractor 2.0: a deep learning approach for data extraction from chemical reaction schemes. D M Wilary, J M Cole, J. Chem. Inf. Model. 632023</p>
<p>D Lowe, 10.6084/m9.figshare.5104873.v1Chemical reactions from US patents. 1976-Sep2016. August 29, 2023</p>
<p>. D M Lowe, 2012University of CambridgePhD thesis</p>
<p>A Peng, M Wu, J Allard, L Kilpatrick, S Heidel, GPT-3.5 Turbo ne-tuning and API updates. August 22, 2023</p>
<p>. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, 10.48550/arXiv.2305.14314arXiv:2305.143142023preprint</p>
<p>W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J Gonzalez, H Zhang, I Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems PrinciplesKoblenz, Germany2023presented in part at the</p>            </div>
        </div>

    </div>
</body>
</html>
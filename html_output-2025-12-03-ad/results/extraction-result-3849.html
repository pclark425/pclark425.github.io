<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3849 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3849</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3849</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-264426756</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.543.pdf" target="_blank">Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3849.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3849.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as an automatic reviewer/judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors used GPT-4 to rate the same 100-sample subsets annotated by three human reviewers across summarisation, simplification and GEC, converting scores to rankings and comparing agreement with human judgements; GPT-4 showed reasonable alignment on summarisation and simplification but weaker alignment on GEC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge (GPT-4) for ranking model outputs on summarisation, text simplification and grammatical error correction</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Text summarisation (CNN/DailyMail), Text simplification (Newsela), Grammatical error correction (BEA-2019)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported inter-annotator agreement (Krippendorff's alpha): human-only average α = 0.88 (minimum observed 0.62). When GPT-4 is included with human annotators the average α = 0.70 (minimum observed 0.34).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>GPT-4's rankings aligned well with humans for summarisation and simplification (described as 'reasonable' / 'surprisingly good') but showed stronger disagreement on GEC; GPT-4 did not show the positional bias reported in some prior work, and the authors used rankings (rather than absolute scores) and averaging which they say dampened some differences. The paper also notes GPT-4 occasionally failed to emit strict JSON and required reruns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower and task-dependent alignment (notably poor alignment on GEC, low α down to 0.34); occasional formatting failures requiring re-running prompts; potential bias because GPT-4 evaluated outputs from sibling OpenAI models (e.g., ChatGPT) which could favor same-family outputs; the small human annotator pool (3 reviewers) and small sample size limit generalisability of agreement estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Where reported, GPT-4 matched human judgement well for summarisation and simplification and produced rankings that correlated reasonably closely with human rankings; the authors did not observe systematic positional bias when using GPT-4; GPT-4 allowed programmatic, scalable annotation (JSON outputs) when it behaved as expected.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use rankings and averaging to reduce positional and scale differences; re-run LLM evaluations when strict output format fails; combine LLM-based evaluation with human evaluation and expand human evaluation to larger, more diverse samples; be cautious when using an LLM to evaluate outputs from models in the same model family; supplement with multiple automatic/reference-less metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sottana et al., Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3849.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3849.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge vs Human comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparisons between LLM-as-a-judge evaluations and human evaluations reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares human annotations (3 expert reviewers) to GPT-4 judgements across three sequence-to-sequence tasks, finding strong human inter-annotator agreement but degraded agreement when GPT-4 is included, with task-specific variation (good for summarisation/simplification, worse for GEC) and several methodological caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Direct comparison of human evaluation (3 expert annotators) and LLM-as-a-judge (GPT-4) on the same 100-sample subsets per task; scores converted to rankings per-sample and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Summarisation, Simplification, Grammatical Error Correction (GEC)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Human-human Krippendorff α mean = 0.88 (min 0.62). Human+GPT-4 Krippendorff α mean = 0.70 (min 0.34), indicating reduced agreement when the LLM judge is included.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Humans and GPT-4 agreed on relative model rankings more often for summarisation and simplification than for GEC; GPT-4's judgments produced 'reasonable' alignment but were 'not perfect'. Humans judged gold references as worse than many model outputs, a phenomenon GPT-4's rankings broadly reflected for some tasks but diverged on others (notably GEC).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Task-dependent failures: notably lower alignment in GEC (greater disagreement); potential same-family model bias (GPT-4 evaluating ChatGPT outputs); limited annotator/sample size reduces reliability of agreement estimates; choice to convert to rankings and average may mask some types of disagreement; occasional LLM output format failures.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>GPT-4 matched human evaluators well enough to produce reasonable rankings for summarisation and simplification tasks; no evidence of observed positional bias in this study (contrasting prior reports). Human evaluators and GPT-4 both ranked ChatGPT highly on many metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Treat LLM-as-judge outputs as complementary to human evaluation; prefer aggregated rankings (vs raw absolute scores) to reduce scale effects; validate LLM evaluators per task (GEC showed notably worse alignment); expand human evaluation sample sizes; use multiple evaluation signals (including reference-less metrics) and be cautious with same-family model evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sottana et al., Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Gpteval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>ChatGPT outperforms crowd-workers for text annotation tasks <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3849",
    "paper_id": "paper-264426756",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "GPT-4 (as reviewer)",
            "name_full": "GPT-4 used as an automatic reviewer/judge",
            "brief_description": "The authors used GPT-4 to rate the same 100-sample subsets annotated by three human reviewers across summarisation, simplification and GEC, converting scores to rankings and comparing agreement with human judgements; GPT-4 showed reasonable alignment on summarisation and simplification but weaker alignment on GEC.",
            "citation_title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge (GPT-4) for ranking model outputs on summarisation, text simplification and grammatical error correction",
            "task_or_domain": "Text summarisation (CNN/DailyMail), Text simplification (Newsela), Grammatical error correction (BEA-2019)",
            "llm_model_name": "GPT-4",
            "agreement_rate": "Reported inter-annotator agreement (Krippendorff's alpha): human-only average α = 0.88 (minimum observed 0.62). When GPT-4 is included with human annotators the average α = 0.70 (minimum observed 0.34).",
            "qualitative_differences": "GPT-4's rankings aligned well with humans for summarisation and simplification (described as 'reasonable' / 'surprisingly good') but showed stronger disagreement on GEC; GPT-4 did not show the positional bias reported in some prior work, and the authors used rankings (rather than absolute scores) and averaging which they say dampened some differences. The paper also notes GPT-4 occasionally failed to emit strict JSON and required reruns.",
            "limitations_or_failure_cases": "Lower and task-dependent alignment (notably poor alignment on GEC, low α down to 0.34); occasional formatting failures requiring re-running prompts; potential bias because GPT-4 evaluated outputs from sibling OpenAI models (e.g., ChatGPT) which could favor same-family outputs; the small human annotator pool (3 reviewers) and small sample size limit generalisability of agreement estimates.",
            "counterexamples_or_strengths": "Where reported, GPT-4 matched human judgement well for summarisation and simplification and produced rankings that correlated reasonably closely with human rankings; the authors did not observe systematic positional bias when using GPT-4; GPT-4 allowed programmatic, scalable annotation (JSON outputs) when it behaved as expected.",
            "recommendations_or_best_practices": "Use rankings and averaging to reduce positional and scale differences; re-run LLM evaluations when strict output format fails; combine LLM-based evaluation with human evaluation and expand human evaluation to larger, more diverse samples; be cautious when using an LLM to evaluate outputs from models in the same model family; supplement with multiple automatic/reference-less metrics.",
            "citation": "Sottana et al., Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
            "uuid": "e3849.0",
            "source_info": {
                "paper_title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM-as-judge vs Human comparison",
            "name_full": "Comparisons between LLM-as-a-judge evaluations and human evaluations reported in this paper",
            "brief_description": "The paper compares human annotations (3 expert reviewers) to GPT-4 judgements across three sequence-to-sequence tasks, finding strong human inter-annotator agreement but degraded agreement when GPT-4 is included, with task-specific variation (good for summarisation/simplification, worse for GEC) and several methodological caveats.",
            "citation_title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
            "mention_or_use": "use",
            "evaluation_setting": "Direct comparison of human evaluation (3 expert annotators) and LLM-as-a-judge (GPT-4) on the same 100-sample subsets per task; scores converted to rankings per-sample and averaged.",
            "task_or_domain": "Summarisation, Simplification, Grammatical Error Correction (GEC)",
            "llm_model_name": "GPT-4",
            "agreement_rate": "Human-human Krippendorff α mean = 0.88 (min 0.62). Human+GPT-4 Krippendorff α mean = 0.70 (min 0.34), indicating reduced agreement when the LLM judge is included.",
            "qualitative_differences": "Humans and GPT-4 agreed on relative model rankings more often for summarisation and simplification than for GEC; GPT-4's judgments produced 'reasonable' alignment but were 'not perfect'. Humans judged gold references as worse than many model outputs, a phenomenon GPT-4's rankings broadly reflected for some tasks but diverged on others (notably GEC).",
            "limitations_or_failure_cases": "Task-dependent failures: notably lower alignment in GEC (greater disagreement); potential same-family model bias (GPT-4 evaluating ChatGPT outputs); limited annotator/sample size reduces reliability of agreement estimates; choice to convert to rankings and average may mask some types of disagreement; occasional LLM output format failures.",
            "counterexamples_or_strengths": "GPT-4 matched human evaluators well enough to produce reasonable rankings for summarisation and simplification tasks; no evidence of observed positional bias in this study (contrasting prior reports). Human evaluators and GPT-4 both ranked ChatGPT highly on many metrics.",
            "recommendations_or_best_practices": "Treat LLM-as-judge outputs as complementary to human evaluation; prefer aggregated rankings (vs raw absolute scores) to reduce scale effects; validate LLM evaluators per task (GEC showed notably worse alignment); expand human evaluation sample sizes; use multiple evaluation signals (including reference-less metrics) and be cautious with same-family model evaluations.",
            "citation": "Sottana et al., Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
            "uuid": "e3849.1",
            "source_info": {
                "paper_title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Gpteval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "gpteval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "ChatGPT outperforms crowd-workers for text annotation tasks",
            "rating": 1,
            "sanitized_title": "chatgpt_outperforms_crowdworkers_for_text_annotation_tasks"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.01143525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks</p>
<p>Andrea Sottana andrea.sottana@netmind.ai 
Department of Informatics
King's College London</p>
<p>Bin Liang bin.liang@netmind.ai 
Department of Informatics
King's College London</p>
<p>Kai Zou 
Department of Informatics
King's College London</p>
<p>Zheng Yuan zheng.yuan@kcl.ac.uk 
Department of Informatics
King's College London</p>
<p>Netmind Ai 
Department of Informatics
King's College London</p>
<p>Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks
E9884D18303EF617A0BA9BEBDB3E4692
Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models.We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation.We also explore the potential of the recently released GPT-4 to act as an evaluator.We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics.We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks.Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs), particularly Transformer based (Vaswani et al., 2017;Devlin et al., 2019), have shown remarkable abilities across a wide range of NLP tasks.With the recent advances in capabilities of general-purpose generative models (Brown et al., 2020;Touvron et al., 2023), a range of NLP tasks can be reformulated as generation tasks.</p>
<p>Robust evaluation is still an unsolved problem and established automatic evaluation metrics have been found to be poor surrogates, correlating weakly with human judgement (Coyne et al., 2023).There is often no clear consensus on how these models should be evaluated (Mousavi et al., 2022).Human evaluation has often been considered as the trusted evaluation method, though issues with human evaluation have also been widely acknowledged (Iskender et al., 2021), e.g. it can be difficult to reproduce (Cumbicus-Pineda et al., 2021).Nonetheless, a human evaluation study remains one of the best tools to sensibly assess any bias or limitation with automatic metrics (Liang et al., 2022).</p>
<p>Recent evaluation work has often focused on a single task (Zhang et al., 2023;Coyne et al., 2023), a single model (Bang et al., 2023), a single dataset (Gilardi et al., 2023) or automatic evaluation (Liang et al., 2022).In this work, we carry out a multidataset, multi-model, multi-task hybrid evaluation using automatic metrics, human evaluation, and model-to-model evaluation with GPT-4 (OpenAI, 2023). 1 We explore the open and closed-source LLMs space to sample the current landscape of available models and evaluate them on the following sequence-to-sequence tasks, reframed as text generation tasks without the requirement for task-specific fine-tuning: text summarisation, text simplification, and grammatical error correction (GEC).</p>
<p>These are our main findings: firstly, we show how traditional reference-based evaluation metrics are inadequate at predicting or replacing human judgement.It is unclear whether this is due to the limitations of the metrics or to the poor quality of references of large open source datasets, or both.While automatic metrics might have been an adequate proxy to evaluate previous models, they seem unable to reliably capture the performance of latest-generation LLMs which now generate ac-ceptable output that is significantly different from the gold reference.Secondly, we prove that even open-source models outperform the gold standard reference of large and well-established datasets according to human evaluators.This shows how data quality is now one of the main bottlenecks in evaluation research.Finally, we reveal how GPT-4 has reasonable alignment with human judgement when ranking different models on most tasks and metrics; we did however observe some variations, with lower alignment in some metrics than in others.Our code is available at https://github.com/protagolabs/seq2seq_llm_evaluation.</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>For text simplification, we used the Newsela test set (Xu et al., 2015), in particular the version used by Jiang et al. (2020).We randomly selected 3,000 samples after removing samples redundancy.2For text summarisation, experiments were run on 3,000 random samples taken from the CNN / DailyMail test set (Hermann et al., 2015;Nallapati et al., 2016).For GEC, we used the BEA-2019 Shared Task (Bryant et al., 2019) development set comprising of 4,384 samples.3</p>
<p>Models</p>
<p>All experiments were performed on a zero-shot unsupervised basis, without any additional fine-tuning or in-context learning, using a range of opensource LLMs and OpenAI commercial models. 4e experimented with the HuggingFace5 implementation of the following open-source models:6 Flan-T5 (google/flan-t5-xxl) (Chung et al., 2022); T0pp (bigscience/T0pp) (Sanh et al., 2021); OPT-IML (facebook/opt-iml-max-30b) (Iyer et al., 2022); Flan-UL2 (google/flan-ul2) (Tay et al., 2022).The OpenAI models we used were GPT-3 (text-davinci-003) (Brown et al., 2020); InstructGPT (davinci-instruct-beta) (Ouyang et al., 2022) and ChatGPT7 (gpt-3.5-turbo-0301).</p>
<p>For implementation details, prompt engineering and hyper-parameter tuning, refer to appendix B.</p>
<p>Evaluation Metrics</p>
<p>We analysed models' outputs using both automatic metrics and human evaluation, and assessed the ability of the recently released GPT-4 model to act as a reviewer.</p>
<p>Automatic Evaluation</p>
<p>We used the most widely adopted reference-based metrics for each of the tasks.For text simplification, we report the SARI score (Xu et al., 2016).For text summarisation, we report the ROUGE score (Lin, 2004); following Phang et al. (2022), we compute the geometric mean of ROUGE-{1, 2, L} F1 scores.</p>
<p>For GEC, we report the F 0.5 score computed using the ERRANT toolkit (Bryant et al., 2017).</p>
<p>Human Evaluation</p>
<p>Due to budgetary and time constraints, we recruited 3 human reviewers8 through the Prolific platform9 and asked them to review the quality of the models' outputs, as well as the gold reference on 100 randomly selected samples per dataset.All three reviewers were asked to annotate the same 100 samples for each of the three tasks.The studies were conducted on a customised version of the open-source POTATO annotation tool (Pei et al., 2022).For human evaluation of text summarisation, we followed the evaluation criteria and their definitions as adopted in Fabbri et al. ( 2021): Relevance, Fluency, Coherence and Consistency, on a 5-point Likert scale (Likert, 1932) from 1 to 5.</p>
<p>For text simplification, we followed the evaluation criteria and their definitions as adopted in Grabar and Saggion (2022): Semantics, Fluency and Simplicity, on a 5-point Likert scale.For GEC, we adopted the Over-correction criterion from Fang et al. (2023) and introduced two new criteria: Semantics and Grammaticality.The definitions and assessment scales for these GEC criteria are detailed in Appendix C. The full set of instructions given to human reviewers for all tasks can be found in our GitHub repository linked above.are shown both on the main subset and the small subset used for human evaluation.† Due to the specifics of HuggingFace implementation, a temperature of 0.0 cannot be used, we therefore used a value of 0.01 for such cases.</p>
<p>GPT-4 as a Reviewer</p>
<p>We used GPT-4 as an additional reviewer to assess whether it can be reliably deployed in place of human reviewers.The definition of the evaluation criteria and their assessment scales were included in the GPT-4 prompt together with the input text for each sample. 10GPT-4 was also asked to annotate the same 100 samples that were shown to human reviewers for each of the three tasks.The full prompts given to GPT-4 for all tasks can also be found in our GitHub repository linked above.</p>
<p>Results and Discussion</p>
<p>Automatic Evaluation Results</p>
<p>Results are shown in Table 1.In order to allow a comparison between open-source and paid-for models' performance, for each task, we report the best open-source model and two commercial models from OpenAI. 11For text summarisation, T0pp significantly outperformed GPT-3 and ChatGPT (with p &lt; 0.001).For text simplification, Flan-T5 and InstructGPT yield the best results, significantly outperforming ChatGPT (p &lt; 0.001).</p>
<p>For GEC, ChatGPT and OPT-IML perform best with a very similar distribution, significantly outperforming GPT-3 (p &lt; 0.001).</p>
<p>We also observed that for each task, the same prompt seemed to perform best for all models and temperature settings, with only one exception, suggesting that the quality of prompts is almost modelinvariant.See Appendix D for more details.</p>
<p>10 Occasionally GPT-4 returned a score of 4.5, and we converted 4.5 to 4 for evaluation purposes (6 out of 3,000 cases).</p>
<p>11 More detailed results are in Appendix D.</p>
<p>Human and GPT-4 Evaluation Results</p>
<p>Human reviewers and GPT-4 were shown 4 outputs per sample: the outputs from the models in Table 1 and the gold standard, and were asked to score each model's output on the metrics and scales described in section 3.2.We then converted their scores to rankings for each model and each reviewer from best (1) to worst (4) and took the average. 12The rankings from human evaluation and GPT-4 evaluation (in brackets) are shown in Table 2, alongside the interval Krippendorff α coefficient (Krippendorff, 2011) to express inter-annotator agreement.</p>
<p>The raw scores and a more detailed set of Krippendorff α coefficients based on individual annotator pairs are shown in Appendix E. As it can be clearly seen, there is generally very good inter-annotator agreement, with an average Krippendorff α of 0.88 across all metrics, with the lowest being 0.62.On text summarisation, most reviewers scored ChatGPT as the best for Relevance and Fluency, and all reviewers scored ChatGPT as best model for Coherence and Consistency, while ChatGPT had a worse ROUGE score compared to other models when using automatic evaluation (see Table 1).Interestingly, all human reviewers scored the gold reference summaries as the worst on all metrics.This reveals the poor quality of reference summaries when compared to most models' outputs, and therefore reference-based automatic metrics could produce unreliable results.It is therefore not surprising that ChatGPT outputs were ranked the worst by automatic metrics in text summarisation and simplification, but the best when using human evaluators.For text simplification, ChatGPT was rated the best model by all reviewers for Fluency and Simplicity, while it was rated poorly for Semantics, with the best model being Flan-T5.We observed that this was due to Flan-T5 returning a lot of outputs which were identical to the inputs, therefore the semantics was obviously fully preserved, but without any inherent text simplification.The gold standard was scored as worst according to all reviewers.</p>
<p>We had substantially different results for GEC, where ChatGPT was rated the best model by human reviewers for Grammaticality (meaning all or most errors were fixed) but was rated as worst or second worst model for Semantics and Overcorrection, for which the best model was OPT-IML.This underlines how ChatGPT tends to overcorrect, and in doing so might add information to the sentence which were not originally present, which is consistent with recent findings (Fang et al., 2023;Wu et al., 2023).The gold reference was scored mostly as second worst on most metrics and by most reviewers.</p>
<p>For both text summarisation and simplification, GPT-4 used as a reviewer produced surprisingly good results which correlate well, albeit not perfectly, with human reviewers.We observed a stronger disagreement between human reviewers and GPT-4 in GEC.It is also worth noting that we did not observe the systematic positional bias when using GPT-4 as a reviewer as reported by Wang et al. (2023).However, we postulate that averaging the scores across the samples and using rankings instead of absolute scores helped to dampen this effect.If we include GPT-4 evaluation, the average Krippendorff α is 0.70 across all metrics, with the lowest being 0.34.</p>
<p>Conclusion</p>
<p>Model evaluation is a topic which is attracting increasing interest from the community.Liang et al. (2022) have recently published an extensive evaluation report on LLMs, however they mostly focused on automatic evaluation.Prompted by the recent advances in generative capabilities of the latest LLMs, we conducted this study to explore the drift between human judgement and automatic, reference-based evaluation of zero-shot model performance.We also explored model-to-model evaluation with GPT-4.The study was conducted using large, open-source datasets often acting as benchmarks for their respective tasks.</p>
<p>Our work reveals a systematic misalignment between reference-based automatic metrics and human evaluation on a range of generative tasks, highlighting the inadequacy of the gold reference in the public NLP benchmarks.It is not clear whether this misalignment is purely due to the limitations of automatic metrics, or whether poor reference quality makes using any reference-based comparative metrics unreliable.Despite ChatGPT being rated one of the best models on most metrics by human reviewers, the best open-source LLMs also consistently outperformed the reference outputs.We also explored the potential of GPT-4 to act as a reviewer and found it has strong correlation with human judgement for summarisation and simplification tasks, and moderate correlation for GEC.</p>
<p>Future work will look at improving the quality of prompts, providing few-shot in-context learning (Brown et al., 2020), or exploring the potential of chain-of-thought prompting (Wei et al., 2022) in improving models' outputs.Given the misalignment mentioned above, extending human evaluation to larger datasets and to a wider range of model settings will also be of particular future interest, so as to minimise the bias introduced when using automatic metrics to select a subset for human evaluation.Finally, introducing multiple automatic evaluation metrics (e.g.reference-less) for each task might help deepen our understanding of the relation between such metrics and human judgement.</p>
<p>Limitations</p>
<p>This paper suffers from the following limitations:</p>
<p>• A limited amount of prompt tuning and prompt space investigation was carried out.Between 2 and 5 different prompts per task were tried, therefore a more focused study on prompt engineering could potentially bring significant improvements, however this is a stand-alone exploration topic, which we leave for future work.</p>
<p>• We did not perform any in-context learning or chain-of-thought prompting, which have been shown to significantly improve the performance of generative models.As such, there may be margin for improving the quality of models' outputs, while the quality of gold references will remain unchanged until new datasets become available.</p>
<p>• We used automatic metrics (SARI, ROUGE and F 0.5 ) to determine the best combination of settings (model, prompt, temperature) for each task.However, since this study revealed poor correlation between human judgement and such metrics, we cannot exclude that the settings we chose for human evaluation were not the most appropriate, which means the study may have suffered from some bias indirectly introduced by using automatic metrics for selection of outputs for the human evaluation study.This is further aggravated by traditional open source datasets only presenting one gold reference output per sample when multiple equally valid outputs could exist, leading to unreliable scores; for example, two summaries of the same story can be both very good but contain few common bi-grams, leading to a poor ROUGE score when doing automatic evaluation.</p>
<p>• Given the wide variety of the text corpora on which most of the models we used were pretrained on, it is very likely that at least some of the models may have been trained on some of the open-source datasets we used to evaluate them.While it is difficult to mitigate for this (for example OpenAI did not publish a list of datasets used to train their models), our results might have been affected by this, and using new unreleased datasets would have been preferable to reduce this bias.However, this was not possible due to the highly expensive and time consuming nature of the task of creating high quality large datasets from scratch, which is a well known issue across the research community.</p>
<p>• While we did not use the same model for both inference and evaluation, we used GPT-4 for evaluation of all models, including the outputs from ChatGPT.Considering they belong to the same family of OpenAI models, GPT-4 might have a bias for rating ChatGPT's outputs higher than other models.However, our results were not able to validate or refute this, as human reviewers also rated ChatGPT outputs as the best across most metrics.</p>
<p>• Due to time and budgetary constraints, we were only able to hire 3 reviewers (not including GPT-4), and asked reviewers to annotate 100 samples per dataset, which is a small proportion of each dataset.Due to the small number of reviewers and reviewed samples, the noise-to-signal ratio may affect the strength and generalisability of our findings.Furthermore, using human evaluation as gold standard is also prone to introducing bias.How-ever, we found that in most cases all annotators agreed that the gold standard was worse than the best models' outputs, so we do believe this is a valid conclusion, given how consistent it was across different tasks and annotators.</p>
<p>Ethics Statement</p>
<p>Our work makes use of LLMs, and there are known concerns associated with such models (Bender et al., 2021), including data bias, toxicity of training content or outputs, their environmental impact, the lack of explainability for their outputs, and the potential to replace human workers with resulting job losses.We did not perform any fine-tuning as part of this project, and only used open-source datasets.Some of the OpenAI's models we used are not open-source, and their overall impact on society is only starting to become apparent.Overall we believe this research does not increase the risk of harm caused by these models or datasets as we only explored their limitations and mance.We employed 3 human annotators through the Prolific platform for a 16-hour study.Reviewers were paid £13.20 per hour, not including Prolific's fees. 13We did not collect any personal information beyond demographic data provided by Prolific, including age, profession, gender amongst others.</p>
<p>While Prolific does provide such data, we did not use them as screening criteria, and only adopted the screening criteria mentioned in section 3.2.All annotators were provided with a detailed description of the study before committing to take part.</p>
<p>A Newsela Dataset Processing</p>
<p>We observed that the ACL 2020 version (Jiang et al., 2020) of the Newsela dataset (Xu et al., 2015) contains a number of samples where either the source (input) or the destination (reference) were duplicated.In such cases, based on our observations, it was appropriate to merge them into a single sample.If the source was a duplicate but the destination wasn't, we kept the source without duplication, and created the destination by merging the two original destination samples, in the order in which they appear in the dataset.Likewise if the destination was a duplicate but the source wasn't.</p>
<p>See example below</p>
<p>• Original dataset, sample 1 -Source: Ron Bee , a professor at San Diego State University , is worried that so few Americans serve in the military .-Destination: Ron Bee is a professor in California , and he is worried .</p>
<p>• Original dataset, sample 2 -Source: Ron Bee , a professor at San Diego State University , is worried that so few Americans serve in the military .-Destination: Very few people join the military now .</p>
<p>• Our merged sample -Source: Ron Bee , a professor at San Diego State University , is worried that so few Americans serve in the military .-Destination: Ron Bee is a professor in California , and he is worried .Very few people join the military now .</p>
<p>B Implementation Details</p>
<p>Due to time and budgetary constraints, the full scale experiments were performed using the most promising settings after a preliminary study conducted on a subset of each dataset (which consists of 100 samples) on a much broader range of settings.We experimented with a range of prompts and temperature values to better explore the capabilities of each model.The final settings are task dependent; for example, we empirically observed that lower temperature values always gave the best outcomes for text summarisation and simplification, whereas for GEC it was beneficial to use higher values for some models.</p>
<p>B.1 Prompt Engineering</p>
<p>The following prompts were used, where \n indicates a newline and [...] indicates the input sample; for each of the three tasks, we report the best prompt, i.e. the prompt whose output was used for our evaluation work, at the top (prompt (a)).</p>
<p>The same prompt yielded best results regardless of model and temperature, with extremely limited exceptions.</p>
<p>.]\n</p>
<p>The simplified version of the story is: (c) Simplify the following text.</p>
<p>[...] (d) Explain this to a 5 year old.</p>
<p>[...] (e) Explain this to a 5 year old.</p>
<p>[...] \n The explanation to a 5 year old could be: When using GPT-4 as a reviewer, we prompted GPT-4 to output the text following strict json format rules so its output could be processed at scale programmatically.When it failed to do so, we rerun the evaluation on that specific sample until the output was in the desired format, which happened mostly at the first attempt and occasionally after 2-3 attempts as GPT-4 output is non-deterministic.</p>
<p>B.2 Hyperparameter Tuning</p>
<p>We experimented with the following temperature values: 0.0 (we used 0.01 for HuggingFace models due to implementation requirements), 0.2, 0.5, 0.7.We observed that for text simplification and summarisation, the lowest value always yielded the best results, whereas for GEC, some combinations of models and prompts yielded better results for temperatures of 0.2 or 0.5, despite the best overall combination being at a temperature of 0.0 even for GEC.For all other hyper-parameters, we used the default settings for each model without modifications.</p>
<p>B.3 Tokenization and Truncation</p>
<p>While the Newsela and BEA-2019 dataset samples are all below 512 tokens,14 the samples from CNN / DailyMail have a broader distribution, with 80.6% exceeding 482 tokens and 9.8% exceeding 1506 tokens.Different models and implementations have different maximum sequence lengths.Furthermore, while OpenAI models count the total number of input and output tokens towards their maximum sequence length, HuggingFace models have two separate limits for input and output tokens respectively.In order to facilitate the inference process, we used the following heuristics to tailor different design decisions to each model to try to maximise performance:</p>
<p>• For GPT-3, which accepts up to 4000 combined input and output tokens, we did not perform any truncation, as the longest sample had 2,571 tokens.</p>
<p>• For InstructGPT, which accepts up to 2049 combined input and output tokens, we truncated the input after 1506 tokens.This leaves 512 tokens for the generated output, as well as a further 31 tokens for the prompt (it is imperative not to truncate the portion of the prompt at the end of the input)</p>
<p>• For HuggingFace models accepting inputs up to 512 tokens (excluding the output), we truncated at 482 tokens to leave space for the prompt; for HuggingFace models accepting inputs up to 2048 tokens we truncated at 2018 tokens.</p>
<p>C Human Evaluation Criteria for GEC</p>
<p>The criteria and their definitions and assessment scales given to reviewers for the GEC task are reported below.</p>
<p>• Semantics.This assesses whether the meaning of the text is preserved following the GEC.Semantic preservation is assessed on a 5-point Likert scale from 1 (Meaning Not Preserved) to 5 (Meaning fully preserved).NOTE: You should penalise corrections which change the meaning unnecessarily.For example, the sentence "I wentt at Rome for my birthday" should be corrected to "I went to Rome for my birthday".A correction such as "I went to Rome for my anniversary" should be penalised in this category as they introduce unnecessary changes to the meaning.</p>
<p>• Grammaticality.This assesses the quality of the correction and answers the question "How many errors are left in the corrected sentence?".Please provide a count of the remaining errors, regardless of whether they were present in the source or they were newly introduced errors in the supposed corrected version.The three options are "0", "1", "2 or more".</p>
<p>• Over-correction.Since there can be multiple ways to correct a sentence, this assesses whether the correction is unnecessarily verbose or makes unnecessary syntax changes.</p>
<p>The best correction should be done with the minimum number of edits.For example, if the sentence "I wentt at Rome for my birthday" is corrected to "I decided to go to Rome for my birthday" this should be penalised under this category because it contains unnecessary syntax changes, even though the final sentence is grammatically correct.This metric answers the question: Is the system over-correcting or making unnecessary syntax changes?The answers should be "No", "Minor over-correction", "Moderate over-correction" or "Substantial overcorrection".</p>
<p>Note that a correction which results in a change of meaning will most likely also be an over-correction.Therefore we expect that if a correction is given a poor score in the Semantics category, it will also receive a poor score in the Over-correction category, and as such there may be some overlap between these two metrics.However, the reverse is not necessarily true, as you could easily have an over-correction without a change of meaning.For example, correcting a sentence from "I wentt at Rome for my birthday" to "I decided to go to Rome for my birthday" doesn't significantly affect the meaning of the sentence, but it nonetheless represents a clear case of over-correction as "wentt at" should have been corrected to "went to" instead of "decided to go to".As such we felt there would be value in keeping these two metrics separate.</p>
<p>D Detailed Automatic Evaluation Results</p>
<p>Table 3 shows the average results of the experiments we run on the summarisation dataset, for each model, temperature and prompt.Refer to Appendix B.1 for prompt details.Table 4 shows the average results of the experiments we run on the simplification dataset.</p>
<p>Summarize the following text.[...] \n The summary is: (b) [...] \n Summarize the text above.(c) Summarize the following text.[...] \n The very short summary is: (d) This is the main story: [...] \n The summarized version of the story is: 2. Text simplification (a) Simplify the following text.[...] \n The simplified version is: (b) This is the main story: [..</p>
<p>Reply with a corrected version of the input sentence with all grammatical and spelling errors fixed.If there are no errors, reply with a copy of the original sentence.\n\n Input sentence: [...] \n Corrected sentence: (b) Correct the following to standard English: \n\n Sentence: [...] \n Correction:</p>
<p>Table 1 :
1
Automatic evaluation of the best open-source model and two commercial models from OpenAI.Results
TaskModelOpen source TemperatureScore (main subset)Score (human eval. subset)summarisation (ROUGE score)T0pp GPT-3 ChatGPTYes No No0.01  † 0 028.82 24.22 23.7631.62 27.19 25.72Simplification (SARI score)Flan-T5 InstructGPT ChatGPTYes No No0.01  † 0 044.98 44.79 37.5544.61 43.25 35.01GEC (F0.5 score)OPT-IML GPT-3 ChatGPTYes No No0.01  † 0 0.239.05 38.40 39.5444.97 41.75 37.97</p>
<p>Table 2 :
2
Average human evaluation rankings per model, task and metrics, where 1.00 means best model and 4.00 means worst model.GPT-4 rankings in brackets.When two models were ranked the same, results are shown as average between lower and upper bound (e.g. two best models are shown as 1.50 each).† α 1 represents the interval Krippendorff α coefficient based on the 3 human annotators rankings, while α 2 includes GPT-4 rankings.</p>
<p>Table 3 :
3
Table 5 shows the average results of the experiments we run on the GEC dataset.Detailed automatic evaluation results on the text summarisation task.
ModelTemp.Summ. PromptROUGE scoreChatGPT01a23.76GPT-30 0.71a 1b 1a 1b24.22 23.28 23.24 22.46InstructGPT0 0.71a 1b 1a 1b20.04 18.60 19.60 19.04T0pp0.01 0.71a 1b 1a 1b28.82 28.80 26.31 26.19Flan-UL20.01 0.71a 1b 1a 1b23.77 18.61 21.83 17.25</p>
<p>Table 4 :
4
Detailed automatic evaluation results on the text simplification task.
ModelTemp.GEC PromptF0.5 scoreChatGPT0 0.23a 3b 3a 3b39.48 22.47 39.54 22.3803a 3b38.40 33.39GPT-30.23a 3b38.31 33.280.53a 3b37.86 32.2203a 3b40.44 38.92InstructGPT0.23a 3b40.00 37.560.53a 3b36.77 31.520.013a 3b39.05 36.04OPT-IML0.3 0.63a 3b 3a 3b38.36 35.52 35.84 33.700.93a 3b29.89 27.380.013a 3b19.17 12.84Flan-T50.3 0.63a 3b 3a 3b20.64 13.83 22.33 15.540.93a 3b20.62 15.40</p>
<p>Table 5 :
5
Detailed automatic evaluation results on the GEC task.</p>
<p>When preparing the manuscript, the authors have noticed that some recent work has also explored model-to-model evaluation, e.g. Chiang and Lee (2023);Liu et al. (2023);Fu et al. (2023). This paper makes a significant contribution by using an extensive set of metrics to provide a comprehensive evaluation of each model on several different sequence-to-sequence tasks.
More details are given in Appendix A.
We did not use the test set as it was not fully disclosed.
It is worth noting that some of the models used are already fine-tuned to follow instructions on a wide range of NLP tasks, some of which include the tasks above.
http://huggingface.co
The open-source models were run on local servers with up to 6 NVIDIA GeForce RTX 3090 GPUs each.
https://openai.com/chatgpt
We only accepted reviewers based in the UK, with English as their first language and who had at least a Degree or Master's level education in English Language or English Literature, and a 100% Prolific approval rate with at least 200 prior submissions.
9 https://www.prolific.co
This is to remove subjectivity and individual differences as human reviewers and GPT-4 might employ different marking criteria.
The base pay was £12 with a 10% bonus if they completed the full 16-hour study. All 3 reviewers completed the study and were awarded this bonus.
Alexander R Fabbri, Wojciech Kryściński, Bryan Mc- Cann, Caiming Xiong, Richard Socher, and Dragomir  Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.
We used text-davinci-003 as tokenizer with the tiktoken python library, however we observed negligible differences when using HuggingFace tokenizers.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>The BEA-2019 shared task on grammatical error correction. Christopher Bryant, Mariano Felice, Øistein E Andersen, Ted Briscoe, 10.18653/v1/W19-4406Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. the Fourteenth Workshop on Innovative Use of NLP for Building Educational ApplicationsFlorence, Italy2019Association for Computational Linguistics</p>
<p>Automatic annotation and evaluation of error types for grammatical error correction. Christopher Bryant, Mariano Felice, Ted Briscoe, 10.18653/v1/P17-1074Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/ARXIV.2210.114162022Scaling instruction-finetuned language models</p>
<p>Analyzing the performance of gpt-3.5 and gpt-4 in grammatical error correction. Steven Coyne, Keisuke Sakaguchi, Diana Galvan-Sosa, Michael Zock, Kentaro Inui, arXiv:2303.143422023arXiv preprint</p>
<p>Linguistic capabilities for a checklist-based evaluation in automatic text simplification. Oscar M Cumbicus-Pineda, Itziar Gonzalez-Dios, Aitor Soroa, CTTS@ SEPLN. 2021</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Is chatgpt a highly fluent grammatical error correction system?. Tao Fang, Shu Yang, Kaixin Lan, Derek F Wong, Jinpeng Hu, Lidia S Chao, Yue Zhang, arXiv:2304.01746a comprehensive evaluation. 2023arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Chatgpt outperforms crowd-workers for textannotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, arXiv:2303.150562023arXiv preprint</p>
<p>Evaluation of automatic text simplification: Where are we now, where should we go from here. Natalia Grabar, Horacio Saggion, Actes de la 29e Conférence sur le Traitement Automatique des Langues Naturelles. s de la 29e Conférence sur le Traitement Automatique des Langues Naturellesconférence principale20221</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT Press20151</p>
<p>Reliability of human evaluation for text summarization: Lessons learned and challenges ahead. Neslihan Iskender, Tim Polzehl, Sebastian Möller, Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval). the Workshop on Human Evaluation of NLP Systems (HumEval)Online. Association for Computational Linguistics2021</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, 2022</p>
<p>Neural CRF model for sentence alignment in text simplification. Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, 10.18653/v1/2020.acl-main.709Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Klaus Krippendorff, Computing krippendorff's alpha-reliability. 2011</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>A technique for the measurement of attitudes. Archives of psychology. Rensis Likert, 1932140</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Evaluation of response generation models: Shouldn't it be shareable and replicable?. Mahed Seyed, Gabriel Mousavi, Michela Roccabruna, Simone Lorandi, Giuseppe Caldarella, Riccardi, Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM). the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Abstractive text summarization using sequence-to-sequence RNNs and beyond. Ramesh Nallapati, Bowen Zhou, Çaglar Cicero Dos Santos, Bing Gulçehre, Xiang, 10.18653/v1/K16-1028Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyAssociation for Computational Linguistics2016</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Potato: The portable text annotation tool. Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, David Jurgens, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingSystem Demonstrations2022</p>
<p>Investigating efficiently extending transformers for long input summarization. Jason Phang, Yao Zhao, Peter J Liu, arXiv:2208.043472022arXiv preprint</p>
<p>. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan TeehanStella Biderman, Leo Gao, Tali Bers, Thomas WolfRush. 2021. Multitask prompted training enables zero-shot task generalization</p>
<p>Yi Tay, Mostafa Dehghani, Xavier Vinh Q Tran, Dara Garcia, Tal Bahri, Huaixiu Schuster, Neil Steven Zheng, Donald Houlsby, Metzler, arXiv:2205.05131Unifying language learning paradigms. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, Michael Lyu, arXiv:2303.13648Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. 2023arXiv preprint</p>
<p>Problems in current text simplification research: New data can help. Wei Xu, Chris Callison-Burch, Courtney Napoles, 10.1162/tacl_a_00139Transactions of the Association for Computational Linguistics. 32015</p>
<p>Optimizing statistical machine translation for text simplification. Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, Chris Callison-Burch, 10.1162/tacl_a_00107Transactions of the Association for Computational Linguistics. 42016</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, arXiv:2301.13848Benchmarking large language models for news summarization. 2023arXiv preprint</p>
<p>Evaluation Results Table 6 shows the average scores and standard deviation for each human reviewer and GPT-4 for each task, metric and model across the three 100-sample subsets. Table 7 shows the same data by ranking, instead of absolute scores. It also shows the interval Krippendorff α coefficient expressing inter-annotator agreement for all annotator pairs. Detailed Human, Gpt-4 , including each annotator and GPT-4</p>
<p>. Std. dev. GPT-4. </p>
<p>reference 2.24 1.14 3.40 0.98 1.87 0.69 4.98 0.14 T0pp 3.44 1.20 4.66 0.78 4.30 1.28 4.93 0.26FLUENCY Gold. </p>
<p>reference 2.87 1.43 3.64 1.32 3.51 1.51 4.50 0.52 T0pp 3.90 1.18 4.60 0.88 4.20 1.18 3.72 0.60COHERENCE Gold. </p>
<p>reference 4.48 0.87 3.73 1.15 4.05 1.37 4.74 0.46 T0pp 4.90 0.41 4.38 0.85 4.83 0.69 4.21 0.57CONSISTENCY Gold. </p>
<p>Text Simplification SEMANTICS Gold reference 2. 78 1.31 3.39 1.31 1.91 1.43 4.03 1.01</p>
<p>Flat-T5. </p>
<p>Flat-T5. </p>
<p>reference 3.10 1.05 3.81 1.17 3.35 1.28 3.91 0.60SIMPLICITY Gold. </p>
<p>Flat-T5. </p>
<p>reference 4.91 0.38 4.74 0.59 4.77 0.66 4.86 0.51 OPT-IML 5.00 0.00 4.94 0.37 4.97 0.22 4.37 0.73Grammatical Error Correction SEMANTICS Gold. </p>
<p>. Grammaticality † Gold, reference 0.64 0.77 0.38 0.61 0.42 0.72 0.12 0.38 OPT-IML 1.00 0.76 0.58 0.67 0.83 0.85 0.62 0.58</p>
<p>. Overcorrection † Gold, reference 0.05 0.30 0.45 0.75 0.40 0.79 0.00 0.00 OPT-IML 0.00 0.00 0.14 0.55 0.01 0.10 0.12 0.35</p>
<p>Table 6: Average and standard deviation of the scores given by each human annotator and GPT-4 per model, task and metrics, across all analysed samples (100 per task). † All metrics on a Likert scale. 1worst) to 5 (best) except Grammaticality on a scale 0 (best) to 3 (worst) and Over-Correction on a scale 0 (best) to 2 (worst</p>            </div>
        </div>

    </div>
</body>
</html>
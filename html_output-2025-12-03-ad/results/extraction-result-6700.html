<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6700 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6700</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6700</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-cf69d946f05b1ecea27b15768701b4edec5da198</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cf69d946f05b1ecea27b15768701b4edec5da198" target="_blank">Towards Solving Text-based Games by Producing Adaptive Action Spaces</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to tackle the first task and train a model that generates the set of all valid commands for a given context and tries three generative models on a dataset generated with Textworld.</p>
                <p><strong>Paper Abstract:</strong> To solve a text-based game, an agent needs to formulate valid text commands for a given context and find the ones that lead to success. Recent attempts at solving text-based games with deep reinforcement learning have focused on the latter, i.e., learning to act optimally when valid actions are known in advance. In this work, we propose to tackle the first task and train a model that generates the set of all valid commands for a given context. We try three generative models on a dataset generated with Textworld. The best model can generate valid commands which were unseen at training and achieve high $F_1$ score on the test set.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6700.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6700.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HRED+PS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Recurrent Encoder-Decoder with Pointer-Softmax Decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical encoder-decoder that inserts a session-level GRU between encoder and pointer-softmax decoder to summarize previously decoded phrases (session memory) and condition subsequent decoding; used to generate sets of admissible commands in TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HRED+PS (hierarchical command generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encoder: bidirectional GRU over context; Entity encoder: GRU over entity token span; Decoder: two-layer GRU pointer-softmax (vocabulary + copy from context). A session-level GRU sits between encoder and decoder and consumes query representations (initial query is encoded context, subsequent queries are final decoder hidden states) to produce a session state that initializes the decoder for each phrase generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU-based hierarchical encoder-decoder with Pointer-Softmax</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (TextWorld ACG / ACGE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state (session-level RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fixed-size session vector summarizing previously decoded phrases (q^m representations: q^1 = encoded context, later q^m = final decoder hidden states), i.e., compressed summary embeddings of prior generated commands and context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Session-level GRU: session^m = GRU_ses(session^{m-1}, q^m) where q^m are query representations (final decoder hidden states or initial context encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Use session hidden state as initial decoder hidden state for next phrase generation; per-token retrieval uses attention over encoder context encodings (learned attention)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning with cross-entropy on generated phrases (maximum likelihood over token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision / Recall / F1 score on generated admissible commands (reported on ACG and ACGE test sets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ACG test F1 = 89.2 (Precision 94.1, Recall 84.7); ACGE test F1 = 94.2 (Precision 96.8, Recall 91.7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>The paper finds HRED+PS outperforms the basic pointer-softmax+beam-search baseline by a wide margin (F1 89.2 vs 35.5 on ACG), but is outperformed by the concatenation approach (PS+Cat). Authors hypothesize the gating mechanism in the hierarchical session RNN can hinder gradient flow when only a single query is present, filtering certain queries and reducing effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Session-level gating can 'filter' queries and hinder gradients from flowing to subsequent sessions, which the authors argue can be detrimental in their setting (single query per context). The hierarchical model also conditions on previous generated phrases which can cause dependencies that hurt generation in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>When only a single query exists per context, hierarchical gating may be harmful — consider alternative architectures (e.g., concatenation of targets) or ensure session-level gating does not block useful gradient flow; combine generator with a control policy and fine-tune generator for more relevant commands rather than using it as a fixed generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6700.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6700.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PS+Cat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointer-Softmax Decoder with Concatenated Targets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pointer-softmax encoder-decoder trained to generate concatenated admissible commands separated by special tokens, removing the need for an explicit session memory module while producing a variable number of commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PS+Cat (concatenation generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same encoder as other models (bidirectional GRU + entity encoder), pointer-softmax decoder is trained to output all target commands concatenated into a single sequence separated by delimiter tokens; inference yields multiple commands by splitting on delimiter tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU-based encoder-decoder with Pointer-Softmax (concatenated-target training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (TextWorld ACG / ACGE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning with cross-entropy on concatenated target sequence</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision / Recall / F1 score on generated admissible commands</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable (no explicit memory component). Reported test performance: ACG F1 = 96.5 (Precision 98.4, Recall 94.7); ACGE F1 = 97.6 (Precision 98.9, Recall 96.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>PS+Cat outperforms HRED+PS; authors attribute PS+Cat's superior performance to lack of session-level gating which can otherwise block gradients and filter queries.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>When used as a fixed generator for downstream control policies, failing to generate a mandatory command would set an upper bound on control performance; authors propose fine-tuning generator with control policy.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Concatenation-based generation is effective for producing variable-sized target sets and can avoid hierarchical gating issues; ensure delimiter handling and training targets match inference splitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6700.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6700.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PS+BS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointer-Softmax with Beam Search (fixed-k beam outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pointer-softmax decoder used with beam search to produce multiple candidate commands by taking top k beams; suffers from over-generation and mismatch between k and true number of targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PS+BS (beam-search multi-command generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pointer-softmax encoder-decoder trained on single target commands; at inference multiple commands are produced by taking the top-k beams from beam search (parameters k and beam width chosen by validation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU-based encoder-decoder with Pointer-Softmax + Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (TextWorld ACG / ACGE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning with cross-entropy on single target commands; inference uses beam search to obtain multiple outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision / Recall / F1 score on generated admissible commands</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable (no explicit memory); reported test performance: ACG F1 = 35.5 (Precision 26.6, Recall 53.3); ACGE F1 = 33.0 (Precision 20.1, Recall 93.0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Beam search increases generation of unseen actions (diversity) but causes over-generation and poor precision when the number of beams (k) does not match the true number of admissible commands.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Over-generation leading to low precision; requires choosing k close to true target count for best F1; mismatch harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use beam search diversity when target count is known or close to k; otherwise prefer models that natively handle variable number of outputs (HRED or PS+Cat).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6700.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6700.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HRED (Sordoni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work proposing hierarchical recurrent encoder-decoder architecture that uses a session-level RNN to model sequences of queries/utterances; used here as inspiration for session-level memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HRED (prior architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical model with token-level encoder/decoder and session-level RNN that summarizes previous utterances to condition future generation; provides the conceptual session memory used by HRED+PS in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hierarchical RNN (Sordoni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state (session-level RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Session embeddings summarizing previous utterances/queries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Session-level RNN (e.g., GRU) updates with final decoder states as inputs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Session state used to initialize or condition decoder; attention for token-level retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Originally supervised sequence learning (mentioned as inspiration; not trained in this paper separately)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>In settings with single or limited queries, gating in session-level RNNs can hinder gradient flow (as observed empirically in this paper when adapting HRED).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Be cautious with session-level gating when there are few session queries; monitor gradient flow and consider alternative conditioning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6700.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6700.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DQN (Language Understanding for Text-based Games Using Deep Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning approach that generates commands by learning separate Q-functions for verbs and nouns using LSTM state representations; cited as an earlier attempt at command generation in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Understanding for Text-based Games Using Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent that decomposes actions into verb and noun components and learns separate Q-value functions for each using LSTM encoders; used to generate limited structured commands (verb-noun pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM-based DQN (two Q-functions for verb and noun)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-based games (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (off-policy deep Q-learning)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Limits structure of generated commands to verb-noun pairs and cannot represent more complex multi-entity commands.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6700.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6700.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action-Eliminating Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-Eliminating Network (as used in Haroush et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL approach that assumes access to the full set of possible game commands and prunes this set per state to help the agent select correct commands; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning how not to act in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Towards Solving Text-based Games by Producing Adaptive Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Action-Eliminating Network</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Assumes a predefined large set of possible commands (permutations across the game) and applies a learned pruning mechanism per state to remove invalid or unlikely actions, simplifying RL decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-based games (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (described in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires access to (and management of) a large predefined action set; not suitable where action generation is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Solving Text-based Games by Producing Adaptive Action Spaces', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion <em>(Rating: 2)</em></li>
                <li>Language Understanding for Text-based Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Learning how not to act in text-based games <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Pointing the Unknown Words <em>(Rating: 1)</em></li>
                <li>Generating Diverse Numbers of Diverse Keyphrases <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6700",
    "paper_id": "paper-cf69d946f05b1ecea27b15768701b4edec5da198",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "HRED+PS",
            "name_full": "Hierarchical Recurrent Encoder-Decoder with Pointer-Softmax Decoder",
            "brief_description": "A hierarchical encoder-decoder that inserts a session-level GRU between encoder and pointer-softmax decoder to summarize previously decoded phrases (session memory) and condition subsequent decoding; used to generate sets of admissible commands in TextWorld.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "HRED+PS (hierarchical command generator)",
            "agent_description": "Encoder: bidirectional GRU over context; Entity encoder: GRU over entity token span; Decoder: two-layer GRU pointer-softmax (vocabulary + copy from context). A session-level GRU sits between encoder and decoder and consumes query representations (initial query is encoded context, subsequent queries are final decoder hidden states) to produce a session state that initializes the decoder for each phrase generation.",
            "model_name": "GRU-based hierarchical encoder-decoder with Pointer-Softmax",
            "model_size": null,
            "benchmark_name": "TextWorld (TextWorld ACG / ACGE datasets)",
            "memory_used": true,
            "memory_type": "recurrent hidden state (session-level RNN)",
            "memory_representation": "Fixed-size session vector summarizing previously decoded phrases (q^m representations: q^1 = encoded context, later q^m = final decoder hidden states), i.e., compressed summary embeddings of prior generated commands and context",
            "memory_update_mechanism": "Session-level GRU: session^m = GRU_ses(session^{m-1}, q^m) where q^m are query representations (final decoder hidden states or initial context encoding)",
            "memory_retrieval_method": "Use session hidden state as initial decoder hidden state for next phrase generation; per-token retrieval uses attention over encoder context encodings (learned attention)",
            "training_method": "Supervised learning with cross-entropy on generated phrases (maximum likelihood over token sequences)",
            "evaluation_metric": "Precision / Recall / F1 score on generated admissible commands (reported on ACG and ACGE test sets)",
            "performance_with_memory": "ACG test F1 = 89.2 (Precision 94.1, Recall 84.7); ACGE test F1 = 94.2 (Precision 96.8, Recall 91.7)",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "The paper finds HRED+PS outperforms the basic pointer-softmax+beam-search baseline by a wide margin (F1 89.2 vs 35.5 on ACG), but is outperformed by the concatenation approach (PS+Cat). Authors hypothesize the gating mechanism in the hierarchical session RNN can hinder gradient flow when only a single query is present, filtering certain queries and reducing effectiveness.",
            "reported_limitations": "Session-level gating can 'filter' queries and hinder gradients from flowing to subsequent sessions, which the authors argue can be detrimental in their setting (single query per context). The hierarchical model also conditions on previous generated phrases which can cause dependencies that hurt generation in some cases.",
            "best_practices_recommendations": "When only a single query exists per context, hierarchical gating may be harmful — consider alternative architectures (e.g., concatenation of targets) or ensure session-level gating does not block useful gradient flow; combine generator with a control policy and fine-tune generator for more relevant commands rather than using it as a fixed generator.",
            "uuid": "e6700.0",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "PS+Cat",
            "name_full": "Pointer-Softmax Decoder with Concatenated Targets",
            "brief_description": "A pointer-softmax encoder-decoder trained to generate concatenated admissible commands separated by special tokens, removing the need for an explicit session memory module while producing a variable number of commands.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "PS+Cat (concatenation generator)",
            "agent_description": "Same encoder as other models (bidirectional GRU + entity encoder), pointer-softmax decoder is trained to output all target commands concatenated into a single sequence separated by delimiter tokens; inference yields multiple commands by splitting on delimiter tokens.",
            "model_name": "GRU-based encoder-decoder with Pointer-Softmax (concatenated-target training)",
            "model_size": null,
            "benchmark_name": "TextWorld (TextWorld ACG / ACGE datasets)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Supervised learning with cross-entropy on concatenated target sequence",
            "evaluation_metric": "Precision / Recall / F1 score on generated admissible commands",
            "performance_with_memory": "Not applicable (no explicit memory component). Reported test performance: ACG F1 = 96.5 (Precision 98.4, Recall 94.7); ACGE F1 = 97.6 (Precision 98.9, Recall 96.3).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "PS+Cat outperforms HRED+PS; authors attribute PS+Cat's superior performance to lack of session-level gating which can otherwise block gradients and filter queries.",
            "reported_limitations": "When used as a fixed generator for downstream control policies, failing to generate a mandatory command would set an upper bound on control performance; authors propose fine-tuning generator with control policy.",
            "best_practices_recommendations": "Concatenation-based generation is effective for producing variable-sized target sets and can avoid hierarchical gating issues; ensure delimiter handling and training targets match inference splitting.",
            "uuid": "e6700.1",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "PS+BS",
            "name_full": "Pointer-Softmax with Beam Search (fixed-k beam outputs)",
            "brief_description": "Pointer-softmax decoder used with beam search to produce multiple candidate commands by taking top k beams; suffers from over-generation and mismatch between k and true number of targets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "PS+BS (beam-search multi-command generator)",
            "agent_description": "Pointer-softmax encoder-decoder trained on single target commands; at inference multiple commands are produced by taking the top-k beams from beam search (parameters k and beam width chosen by validation).",
            "model_name": "GRU-based encoder-decoder with Pointer-Softmax + Beam Search",
            "model_size": null,
            "benchmark_name": "TextWorld (TextWorld ACG / ACGE datasets)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Supervised learning with cross-entropy on single target commands; inference uses beam search to obtain multiple outputs",
            "evaluation_metric": "Precision / Recall / F1 score on generated admissible commands",
            "performance_with_memory": "Not applicable (no explicit memory); reported test performance: ACG F1 = 35.5 (Precision 26.6, Recall 53.3); ACGE F1 = 33.0 (Precision 20.1, Recall 93.0).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "Beam search increases generation of unseen actions (diversity) but causes over-generation and poor precision when the number of beams (k) does not match the true number of admissible commands.",
            "reported_limitations": "Over-generation leading to low precision; requires choosing k close to true target count for best F1; mismatch harms performance.",
            "best_practices_recommendations": "Use beam search diversity when target count is known or close to k; otherwise prefer models that natively handle variable number of outputs (HRED or PS+Cat).",
            "uuid": "e6700.2",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "HRED (Sordoni et al.)",
            "name_full": "A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion",
            "brief_description": "Prior work proposing hierarchical recurrent encoder-decoder architecture that uses a session-level RNN to model sequences of queries/utterances; used here as inspiration for session-level memory.",
            "citation_title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion",
            "mention_or_use": "mention",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "HRED (prior architecture)",
            "agent_description": "Hierarchical model with token-level encoder/decoder and session-level RNN that summarizes previous utterances to condition future generation; provides the conceptual session memory used by HRED+PS in this paper.",
            "model_name": "Hierarchical RNN (Sordoni et al.)",
            "model_size": null,
            "benchmark_name": null,
            "memory_used": true,
            "memory_type": "recurrent hidden state (session-level RNN)",
            "memory_representation": "Session embeddings summarizing previous utterances/queries",
            "memory_update_mechanism": "Session-level RNN (e.g., GRU) updates with final decoder states as inputs",
            "memory_retrieval_method": "Session state used to initialize or condition decoder; attention for token-level retrieval",
            "training_method": "Originally supervised sequence learning (mentioned as inspiration; not trained in this paper separately)",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "In settings with single or limited queries, gating in session-level RNNs can hinder gradient flow (as observed empirically in this paper when adapting HRED).",
            "best_practices_recommendations": "Be cautious with session-level gating when there are few session queries; monitor gradient flow and consider alternative conditioning strategies.",
            "uuid": "e6700.3",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM-DQN (Language Understanding for Text-based Games Using Deep Reinforcement Learning)",
            "brief_description": "A reinforcement-learning approach that generates commands by learning separate Q-functions for verbs and nouns using LSTM state representations; cited as an earlier attempt at command generation in text-based games.",
            "citation_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "mention_or_use": "mention",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "LSTM-DQN",
            "agent_description": "An RL agent that decomposes actions into verb and noun components and learns separate Q-value functions for each using LSTM encoders; used to generate limited structured commands (verb-noun pairs).",
            "model_name": "LSTM-based DQN (two Q-functions for verb and noun)",
            "model_size": null,
            "benchmark_name": "Text-based games (prior work)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Reinforcement learning (off-policy deep Q-learning)",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Limits structure of generated commands to verb-noun pairs and cannot represent more complex multi-entity commands.",
            "best_practices_recommendations": null,
            "uuid": "e6700.4",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Action-Eliminating Network",
            "name_full": "Action-Eliminating Network (as used in Haroush et al.)",
            "brief_description": "An RL approach that assumes access to the full set of possible game commands and prunes this set per state to help the agent select correct commands; cited in related work.",
            "citation_title": "Learning how not to act in text-based games",
            "mention_or_use": "mention",
            "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "agent_name": "Action-Eliminating Network",
            "agent_description": "Assumes a predefined large set of possible commands (permutations across the game) and applies a learned pruning mechanism per state to remove invalid or unlikely actions, simplifying RL decision-making.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Text-based games (prior work)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Reinforcement learning (described in cited work)",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Requires access to (and management of) a large predefined action set; not suitable where action generation is needed.",
            "best_practices_recommendations": null,
            "uuid": "e6700.5",
            "source_info": {
                "paper_title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion",
            "rating": 2
        },
        {
            "paper_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Learning how not to act in text-based games",
            "rating": 2
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Pointing the Unknown Words",
            "rating": 1
        },
        {
            "paper_title": "Generating Diverse Numbers of Diverse Keyphrases",
            "rating": 2
        }
    ],
    "cost": 0.01380525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Solving Text-based Games by Producing Adaptive Action Spaces</h1>
<p>Ruo Yu Tao<br>McGill University<br>Microsoft Research<br>ruo.tao@mail.mcgill.ca</p>
<p>Marc-Alexandre Côté<br>Microsoft Research<br>macote@microsoft.com</p>
<p>Xingdi Yuan<br>Microsoft Research<br>eric.yuan@microsoft.com</p>
<p>Layla El Asri<br>Microsoft Research<br>layla.elasri@microsoft.com</p>
<h4>Abstract</h4>
<p>To solve a text-based game, an agent needs to formulate valid text commands for a given context and find the ones that lead to success. Recent attempts at solving text-based games with deep reinforcement learning have focused on the latter, i.e., learning to act optimally when valid actions are known in advance. In this work, we propose to tackle the first task and train a model that generates the set of all valid commands for a given context. We try three generative models on a dataset generated with Textworld (Côté et al., 2018). The best model can generate valid commands which were unseen at training and achieve high $F_{1}$ score on the test set.</p>
<h2>1 Introduction</h2>
<p>Text-based games offer a unique framework to train decision-making models insofar as these models have to understand complex text instructions and interact via natural language. At each time step of a text-based game, the current environment of the player (or the 'context') is described in words. To move the game forward, a text command (or an 'action') must be issued. Based on the new action and the current game state, the game transitions to a new state and the new context resulting from the action is described to the player. This iterative process can be naturally divided into two tasks. The first task is to recognize the commands that are possible in a given context (e.g., open the door if the context contains an unlocked door), and the second task is the reinforcement learning task of learning to act optimally in order to solve the game (Narasimhan et al., 2015; Zelinka, 2018; He et al., 2015; Haroush et al., 2018). Most work on reinforcement learning has focused on training an agent that picks the best command from a given set of valid commands, i.e., pick the command that would lead to completing the game.</p>
<p>Humans who play a text-based game typically do not have access to a list of commands and a large part of playing the game consists of learning how to formulate valid commands. In this paper, we propose models that try to accomplish this task. We frame it as a supervised learning problem and train a model by giving it (input, label) pairs where the input is the current context as well as the objects that the player possesses, and the output is the list of admissible commands given this input. Similarly to Côté et al. (2018), we define an admissible command as a command that changes the game's state. We generate these (input, label) pairs with TextWorld (Côté et al., 2018), a sandbox environment for generating text-based games of varying difficulty.
In this work, we explore and present three neural encoder-decoder approaches:</p>
<ul>
<li>a pointer-softmax model that uses beam search to generate multiple commands;</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An instance of a TextWorld game. The context is the concatenation of the room's and inventory's description, and the admissible commands are the actions that would affect the state.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th>Valid</th>
<th>Test</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACG</td>
<td>33,716</td>
<td>4,243</td>
<td>4,276</td>
<td>9.37 ± 26.3</td>
</tr>
<tr>
<td>ACGE</td>
<td>214,741</td>
<td>26,904</td>
<td>27,423</td>
<td>2.47 ± 0.7</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Commands</th>
<th>Unique</th>
<th>Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>315,974</td>
<td>14,328</td>
<td>-</td>
</tr>
<tr>
<td>Valid</td>
<td>39,302</td>
<td>4,006</td>
<td>846</td>
</tr>
<tr>
<td>Test</td>
<td>40,464</td>
<td>4,243</td>
<td>921</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the datasets. Left) Number of data points in each train/valid/test split, and the average number of admissible commands (i.e., target) per data point. Right) Number of commands in each train/valid/test split, how many of those are unique, and how many are not seen in the train set.</p>
<ul>
<li>a hierarchical recurrent model with pointer-softmax generating multiple commands at once;</li>
<li>a pointer-softmax model generating multiple commands at once.</li>
</ul>
<p>The first model has the disadvantage of imposing a fixed number of actions for any given context. The two others alleviate this constraint but suffer from conditioning on the previous action generated. We compare empirical and qualitative results from those models, and pinpoint their weaknesses.</p>
<h2>2 Related Work</h2>
<p>Sequence to sequence generation Sentence generation has been studied extensively with the inception of sequence to sequence models (Sutskever et al., 2014), and attentive decoding (Bahdanau et al., 2014). Pointer-based sequence to sequence networks (Vinyals et al., 2015; Gulcehre et al., 2016; Wang et al., 2017) help dealing with out-of-vocabulary words by introducing a mechanism for choosing between outputting a word from the vocabulary or referencing an input word during decoding. Vinyals et al. (2015) studied the problem of matching input sequences to output sets, i.e., where there is no natural order between the elements. This task is challenging because there is no natural order between the sentences but there is an order between the tokens within each sentence. One of the models we try for this task is the hierarchical encoder-decoder (Sordoni et al., 2015) originally proposed to model the dialogue between two speakers. Another model is inspired by Yuan et al. (2018b) who generates concatenated target sentences with orthogonally regularized separators.</p>
<p>Reinforcement learning for text-based games Many recent attempts at solving text-based games have assumed that the agent has a predefined set of commands to choose from. For instance, the Action-Eliminating Network (Haroush et al., 2018) assumes that the agent has access to all possible permutations of commands in the entire game, and prunes that list in each state to allow the agent to better select correct commands. One attempt at command generation for a text-based game is the LSTM-DQN (Narasimhan et al., 2015). This approach generates commands by leveraging off-policy deep Q-value approximations (Mnih et al., 2013), and learns two separate Q-functions for verbs and nouns. This limits the structure of generated commands to verb-action pairs, and does not allow for more robust multi-entity commands. Yuan et al. (2018a) extends the LSTM-DQN approach with an exploration bonus to try and generalize, and beat games consisting of collecting coins in a maze.</p>
<p>Separating planning from generation in dialogue systems The task of choosing the best next utterance to generate for a given context has been extensively studied in the literature on dialogue systems (Rieser and Lemon, 2016; Pietquin et al., 2011; Fatemi et al., 2016). Historically, dialogue systems have considered separately the tasks of understanding the context, producing the available</p>
<p>next utterances and of generating the next utterance (Lemon and Pietquin, 2007). Recent attempts at learning to perform all these tasks through one end-to-end model have produced encouraging results (Li et al., 2017; Bordes and Weston, 2016) but so far, the best-performing models still separate these two tasks (Wen et al., 2016; Asadi and Williams, 2016). Inspired by these results, we decide to frame the task of solving a text-based game into an action generation and an action selection modules and we propose models for action generation in the following section.</p>
<h1>3 Methodology</h1>
<h3>3.1 Dataset and environment</h3>
<p>In this section, we introduce a dataset called TextWorld Action Command Generation (TextWorld ACG). It is a collection of game walkthroughs gathered from random games generated with TextWorld. Statistics of TextWorld ACG are shown in Table 1. Each data point in TextWorld ACG consists of:</p>
<ol>
<li>Context: concatenation of the room's and inventory's description for a game state;</li>
<li>Entities: a list of interactable object names or exits appearing within the context;</li>
<li>Commands: a list of strings that contains all the admissible commands recognized by TextWorld.</li>
</ol>
<p>We define two tasks using TextWorld ACG to learn the action space of these TextWorld games. First, without conditioning on entities, the model needs to generate all the admissible commands. Second, conditioning on one entity, the model is required to generate all valid commands that are related to that entity. In the following sections, we denote the task without conditioning on entity with ACG, and the task conditioning on entities with ACGE. The data used for the ACGE task is created by splitting each data point in TextWorld ACG by its entities, so that each data point in ACGE has a single entity. There exist commands with multiple entities (i.e., put apple on table) - in these cases we group this action with one of the entities, and expect the models to produce the other entity. We also ignore the two commands (look and inventory) that don't affect the game state. This is because the context already consists of the exact descriptions returned by look and inventory. Adding the two commands would only serve to inflate metrics.</p>
<h3>3.2 Command generation</h3>
<p>In the following sections, we denote tokenized input words from the context sequence as $w, x$ to denote embedded tokens, a subscript ( $e$ or $d$ etc.) to denote where the representations are from (encoder, decoder etc.), $h$ to represent hidden states, $s$ to represent session states and $y$ to denote output tokens. We use superscripts to represent time steps. An absence of a superscript represents multiple time-steps. We represent concatenation with angled brackets $\langle \rangle$. We also represent linear transformations with $L$, as well as linear transformations followed by an non-linear activation function $f$ as $L^{f}$. A subscript on these transformations (ie. $L_{1}, L_{2}$ ) represent transformations with different parameters.</p>
<h3>3.2.1 Context encoding</h3>
<p>Given a sequence of length $N$ in the context, we have the input sequence $w=\left(w_{e}^{1}, \ldots, w_{e}^{N}\right)$ which we embed using GloVe (Pennington et al., 2014) vectors to produce $x_{e}=\left(x_{e}^{1}, \ldots, x_{e}^{N}\right)$. We feed $x_{e}$ into a bidirectional RNN (Cho et al., 2014; Schuster and Paliwal, 1997) to retrieve forwards $\left(h_{e, f}\right)$ and backwards $\left(h_{e, b}\right)$ encodings of the source sequence:</p>
<p>$$
\begin{aligned}
h_{e, f}^{t} &amp; =\operatorname{GRU}<em e="e">{e, f}\left(x</em>\right) \
h_{e, b}^{t} &amp; =\operatorname{GRU}}^{t}, h_{e, f}^{t-1<em e="e">{e, b}\left(x</em>\right)
\end{aligned}
$$}^{t}, h_{e, b}^{t+1</p>
<p>We concatenate the two to get the resulting encoded sequence $h_{e}^{t}=\left\langle h_{e, f}^{t}, h_{e, b}^{t}\right\rangle$. Then, we take a step depending on whether we condition on entity or not. Given a sequence of $m$ word tokens $\left(w_{e n t}^{1}, \ldots, w_{e n t}^{m}\right)$ from the entity (which is also a sequence of word tokens), we find the indices $0&lt;i&lt;j&lt;N$ where the entity words appear in context, i.e., $\left(w_{e n t}^{1}, \ldots, w_{e n t}^{m}\right)=\left(w_{e}^{i}, \ldots, w_{e}^{j}\right)$. Now we take context encodings $\left(h_{e}^{t}, \ldots, h_{e}^{j}\right)$ and use them as input to a GRU, where $i \leqslant t \leqslant j$ :</p>
<p>$$
h_{e n t}^{t}=\operatorname{GRU}<em e="e">{e n t}\left(h</em>\right)
$$}^{t}, h_{e n t}^{t-1</p>
<p>We use the final hidden state of this entity RNN as an entity encoding, which we will label as $h_{e n t}$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Hierarchical decoding of multiple commands given a single context. The orange outline represents the portions of the model also used for PS + BS and PS + Cat architectures.</p>
<h3>3.2.2 Attentive decoding and Pointer Softmax</h3>
<p>The decoder is also a recurrent model that takes in the context encodings $h_{e}$ and the generated entity encoding $h_{ent}$, at every timestep $t$ it produces a probability distribution of generating the next token $p(y_{t})$. This next token can come from one of two sources - either a word in the context or a word in our shortlist vocabulary. Our shortlist vocabulary in this case is just our entire vocabulary (consisting of all possible 887 unique words in the dataset). The first part of the decoder model is an RNN that takes in the embedding of the previous output $x_{d}^{t}=\operatorname{embed}(y^{t-1})$ and previous decoder hidden state $h_{d}^{t-1}$ to produce the first hidden state:</p>
<p>$$
h_{d 1}^{t}=\operatorname{GRU}<em d="d">{d 1}\left(x</em>\right)
$$}^{t}, h_{d}^{t-1</p>
<p>Next, we concatenate this output hidden state with the entity representation to produce $u^{t}=$ $\left\langle h_{d 1}^{t}, h_{e n t}\right\rangle$. We use this as the query to an attention mechanism (Bahdanau et al., 2014) which generates annotations from this query and a value (in this case context encodings $h_{e}$ ). We generate these annotations with a two layer Feed Foward Network (FFN), and define a distribution over the context encodings. The context vector $c^{t}$ is then computed by taking the weighted sum of the context encodings $h_{e}$ :</p>
<p>$$
\begin{aligned}
\alpha^{t} &amp; =\operatorname{softmax}\left(L_{1}\left(L_{2}^{\text {tanh }}\left(\left\langle u^{t}, h_{e}\right\rangle\right)\right)\right) \
c^{t} &amp; =\sum_{i} \alpha^{t, i} h_{e}^{i}
\end{aligned}
$$</p>
<p>We now use the annotations as the distribution over the context sequence, $p_{c}\left(y^{t}\right)=\alpha^{t}$. We take the context vector and use this and the first RNN hidden state $h_{d 1}^{t}$ as input to a second RNN:</p>
<p>$$
h_{d 2}^{t}=\operatorname{GRU}<em 1="1" d="d">{d 2}\left(c^{t}, h</em>\right)
$$}^{t</p>
<p>We use this hidden state as the previous hidden state for the next time step $\left(h_{d}^{t}=h_{d 2}^{t}\right)$. We also apply dropout on the output of this RNN for regularization purposes. We now use the concatenation of $\left\langle h_{d 2}^{t}, c^{t}, x_{d}^{t}\right\rangle$ as input to both the shortlist FFN and switch FFN to generate the shortlist distribution $p_{s}\left(y^{t}\right)$ and switch distributions $s^{t}$ respectively:</p>
<p>$$
\begin{aligned}
p_{s}\left(y^{t}\right) &amp; =\operatorname{softmax}\left(L_{3}\left(L_{4}^{\text {tanh }}\left(\left\langle h_{d 2}^{t}, c^{t}, x_{d}^{t}\right\rangle\right)\right)\right) \
s^{t} &amp; =\operatorname{sigmoid}\left(L_{5}\left(L_{6}^{\text {tanh }}\left(\left\langle h_{d 2}^{t}, c^{t}, x_{d}^{t}\right\rangle\right)\right)\right)
\end{aligned}
$$</p>
<p>We generate output tokens from a combined distribution over the context words ( $p_{c}$ ), shortlist words and a switch that interpolate the probability of each distribution as per the Pointer Softmax (Gulcehre et al., 2016) decoder framework</p>
<p>$$
p\left(y^{t}\right)=s^{t} \cdot p_{s}\left(y^{t}\right)+\left(1-s^{t}\right) \cdot p_{c}\left(y^{t}\right)
$$</p>
<h1>3.2.3 Hierarchical session encoding</h1>
<p>We adopt the framework of the hierarchical recurrent encoder-decoder (Sordoni et al., 2015) as one solution to alleviate the problem of multiple phrase generation per context (Figure 2). We place the session-level RNNs in between the encoder and decoder in order to condition on and summarize the previously decoded phrases. The session-level RNN takes as input a sequence of query representations $q^{1}, \ldots, q^{M}$. We let $q^{1}=h_{e}^{S}$, and all subsequent $q^{i}$ 's will be the final decoder hidden state as per Figure 2. The session-level state becomes session $^{m}=\mathrm{GRU}<em d="d">{\text {ses }}\left(\right.$ session $\left.^{m-1}, q^{m}\right)$, which we use as initial hidden states of the decoder, $h</em>$.}^{0}=$ session $^{m</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>$F_{1}$ score</th>
<th>Unseen recall</th>
<th>Seen recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACG</td>
<td>PS + BS(11, 30)</td>
<td>26.6</td>
<td>53.3</td>
<td>35.5</td>
<td>12.0</td>
<td>54.8</td>
</tr>
<tr>
<td></td>
<td>HRED + PS</td>
<td>94.1</td>
<td>84.7</td>
<td>89.2</td>
<td>48.4</td>
<td>86.0</td>
</tr>
<tr>
<td></td>
<td>PS + Cat</td>
<td>98.4</td>
<td>94.7</td>
<td>96.5</td>
<td>83.0</td>
<td>95.1</td>
</tr>
<tr>
<td>ACGE</td>
<td>PS + BS(3, 10)</td>
<td>20.1</td>
<td>93.0</td>
<td>33.0</td>
<td>80.9</td>
<td>93.5</td>
</tr>
<tr>
<td></td>
<td>HRED + PS</td>
<td>96.8</td>
<td>91.7</td>
<td>94.2</td>
<td>59.7</td>
<td>92.8</td>
</tr>
<tr>
<td></td>
<td>PS + Cat</td>
<td>98.9</td>
<td>96.3</td>
<td>97.6</td>
<td>76.7</td>
<td>96.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Models performance on the ACG and ACGE tasks. Where $k$ in PS + BS $(k, W)$ was determined by the highest $F_{1}$ score on the respective validation sets. The recall for commands seen (resp. unseen) during training is also reported.</p>
<h1>3.2.4 Learning with command generation</h1>
<p>We employ a cross-entropy loss for all the learning objectives. The first model architecture uses the context encoder connected with a pointer-softmax decoder on single target commands (we label this as PS + BS $(k, W)$. During inference, we use the top $k$ out of $W$ beams to produce $k$ commands. With $S^{i}$ as the phrase produced at time step $i$, we try to maximize the following log-likelihood:</p>
<p>$$
\mathcal{L}\left(S^{i}\right)=\sum_{t=1}^{T} \log p\left(y^{i, t} \mid y^{i, 1: t-1}\right), \text { for } i=1, \ldots,|S|
$$</p>
<p>The second model applies hierarchical decoding, where we encode our context sequence as above and have a session state run through all the pointer-softmax decoder steps (we label this as HRED + PS). We use the same objective function as Sordoni et al. (2015) over the parameters for all the RNNs in the model. We let $S$ be all generated phrases given a context, $Q^{m}$ represents the phrase generated at session time-step $m$, so objective is to maximize the following log-likelihood:</p>
<p>$$
\mathcal{L}(S)=\sum_{m=1}^{M} \sum_{t=1}^{T} \log p\left(y^{t} \mid y^{1: t-1}, Q^{1: m-1}\right)
$$</p>
<p>The final model uses the same architecture as Yuan et al. (2018b), we train on the concatenated target commands delineated by separator tokens (we label this as PS + Cat). In this case, the objective is:</p>
<p>$$
\mathcal{L}(S)=\sum_{t=1}^{|S|} \log p\left(y^{t} \mid y^{1: t-1}\right)
$$</p>
<h2>4 Results and Discussion</h2>
<p>The empirical results (Table 2) and qualitative results (Appendix A.1) show the ability for our best model to generate valid unseen commands and achieve $F_{1}$ scores of 96.5 and 97.6 on ACG and ACGE respectively. The hierarchical and concatenation models outperform the Pointer-Softmax with Beam Search by a wide margin - largely due to the over-generation of PS + BS and the mismatch in number of targets between $k$ and actual number of target target commands (as seen in Appendix B.1). We hypothesize the PS + Cat outperforms the HRED model due to the gating mechanism between each session state. Conditioning on different queries gives HRED the ability to prevent gradients to flow through to the next session. We can see the detriment of this gating by comparing their $F_{1}$ scores. We hypothesize that as we only have a single query from our encoded context (and hence no "noisy" queries (Sordoni et al., 2015)) the gating mechanism hinders the model by "filtering" certain queries. We also observe a noticeable gap between the performances in the ACG and ACGE as expected. In the ACGE case, the models are more constrained by conditioning information. This means the scope of its generation narrows - our models generate smaller sequences on average (as shown in Table 1), which decreases the likelihood of generating missing or extra commands as shown in Appendix A.3.
Experiments for models initialized without pre-trained GloVe embeddings were also conducted on both ACG and ACGE datasets, but resulted in an almost negligible ( $\leqslant 0.2 \%$ ) decline in F1-score of the model. We postulate this is due to the mismatch in objectives between how GloVe is trained and the required entity relations in our environment.
Interestingly, the generative models are able to generate a large portion of the valid commands that are unseen during training. Added diversity from beam search seems to help in producing unseen</p>
<p>examples, but only in the case where the number of targets for a training instance is close to the number of targets we generate during inference as seen in Table 2. A large beam width is able to generate more unseen actions because of how beam search over generates actions.</p>
<p>In this work, we explored three different approaches at generating sets of text commands that are context dependent. We tested them on TextWorld ACG and ACGE, two new datasets built using TextWorld. Seeing those encouraging results, our next step would be to combine the command generation with a control policy in order to play (and solve) text-based games. While the performance of the command generation is good (on TextWorld games), using it as a fixed generator would set an upper bound on the performance of the control policy (i.e., commands, mandatory for the game progression, might never be generated in the first place). Instead, our next goal is to develop a control policy that can use the generator and fine tune it to produce more relevant commands.</p>
<h1>Acknowledgments</h1>
<p>Special thanks to Kaheer Suleman for his help and guidance in model architectures.</p>
<h2>References</h2>
<p>Asadi, K. and Williams, J. D. (2016). Sample-efficient deep reinforcement learning for dialog control. CoRR.</p>
<p>Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</p>
<p>Bordes, A. and Weston, J. (2016). Learning end-to-end goal-oriented dialog. CoRR, abs/1605.07683.
Cho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259.</p>
<p>Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler, A. (2018). Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.</p>
<p>Fatemi, M., El Asri, L., Schulz, H., He, J., and Suleman, K. (2016). Policy networks with two-stage training for dialogue systems. In Proc. of SIGDIAL.</p>
<p>Gulcehre, C., Ahn, S., Nallapati, R., Zhou, B., and Bengio, Y. (2016). Pointing the Unknown Words. ArXiv e-prints.</p>
<p>Haroush, M., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2018). Learning how not to act in text-based games.</p>
<p>He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2015). Deep Reinforcement Learning with a Natural Language Action Space. ArXiv e-prints.</p>
<p>Lemon, O. and Pietquin, O. (2007). Machine learning for spoken dialogue systems. In Interspeech, pages $2685-2688$.</p>
<p>Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jurafsky, D. (2017). Adversarial Learning for Neural Dialogue Generation. In EMNLP.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. ArXiv e-prints.</p>
<p>Narasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language Understanding for Text-based Games Using Deep Reinforcement Learning. ArXiv e-prints.</p>
<p>Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543.</p>
<p>Pietquin, O., Geist, M., Chandramohan, S., and Frezza-Buet, H. (2011). Sample-efficient batch reinforcement learning for dialogue management optimization. ACM Trans. Speech Lang. Process., 7 .</p>
<p>Rieser, V. and Lemon, O. (2016). Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning. In EACL.</p>
<p>Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681.</p>
<p>Sordoni, A., Bengio, Y., Vahabi, H., Lioma, C., Simonsen, J. G., and Nie, J.-Y. (2015). A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion. ArXiv e-prints.</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems Volume 2, NIPS'14, pages 3104-3112, Cambridge, MA, USA. MIT Press.</p>
<p>Vinyals, O., Bengio, S., and Kudlur, M. (2015). Order matters: Sequence to sequence for sets. In Proceedings of the International Conference on Learning Representations.</p>
<p>Vinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer Networks. ArXiv e-prints.
Wang, T., Yuan, X., and Trischler, A. (2017). A Joint Model for Question Answering and Question Generation. ArXiv e-prints.</p>
<p>Wen, T., Gasic, M., Mrksic, N., Rojas-Barahona, L. M., Su, P., Ultes, S., Vandyke, D., and Young, S. J. (2016). A network-based end-to-end trainable task-oriented dialogue system. In Proc. of EACL.</p>
<p>Yuan, X., Côté, M.-A., Sordoni, A., Laroche, R., Tachet des Combes, R., Hausknecht, M., and Trischler, A. (2018a). Counting to Explore and Generalize in Text-based Games. ArXiv e-prints.</p>
<p>Yuan, X., Wang, T., Meng, R., Thaker, K., He, D., and Trischler, A. (2018b). Generating Diverse Numbers of Diverse Keyphrases. ArXiv e-prints.</p>
<p>Zelinka, M. (2018). Using reinforcement learning to learn how to play text-based games. ArXiv e-prints.</p>
<h1>A Full Results</h1>
<h2>A. 1 Qualitative results from generation</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Context</th>
<th style="text-align: center;">-= attic = - you 've entered an attic . you see a closed type p box . oh wow ! is that what i <br> think it is ? it is ! it 's a workbench . you see a type p keycard and a bug on the workbench . <br> hmmm ... what else , what else ? there is an unblocked exit to the east . you do n't like <br> doors ? why not try going south, that entranceway is unblocked . you are carrying nothing .</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PS + BS</td>
<td style="text-align: center;">go bug; go east; go south; go type; open bug; open east; open type; open type p; <br> open type p box; open type p keycard'; take bug; take bug p keycard from; take east; <br> take south; take type; take type p; take type p box; take type p keycard; <br> take type p keycard from</td>
</tr>
<tr>
<td style="text-align: center;">HRED + PS</td>
<td style="text-align: center;">go east; go south; open type p box; take type p keycard from workbench;</td>
</tr>
<tr>
<td style="text-align: center;">PS + Cat</td>
<td style="text-align: center;">go east; go south; open type p box; take bug from workbench; <br> take type p keycard from workbench;</td>
</tr>
<tr>
<td style="text-align: center;">Ground Truth</td>
<td style="text-align: center;">go east; go south; open type p box; take bug from workbench; <br> take type p keycard from workbench</td>
</tr>
</tbody>
</table>
<p>Table 3: Example from ACG test set, predictions generated by 3 models. All mismatched commands are shown in bold. All italicized commands are commands that are unseen during training.</p>
<h2>A. 2 Full empirical results</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$F_{1}$ score</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PS + BS</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">35.5</td>
</tr>
<tr>
<td style="text-align: center;">HRED + PS</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">89.2</td>
</tr>
<tr>
<td style="text-align: center;">PS + Cat</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">96.5</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$F_{1}$ score</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PS + BS</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: center;">HRED + PS</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">PS + Cat</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Left: Model performance on the ACG task, with PS + BS we use the top 11 beam search generated phrases. This number was determined by highest validation $F_{1}$ score. Right: Model performance on the ACGE task. Again, PS + BS uses the top 2 commands generated by beam search, also determined by the highest validation $F_{1}$ score.</p>
<h1>A. 3 Graphical representation of results</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ratio of missing commands by number of words in predictions to total commands by number of words in both ACG and ACGE.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The count of extra commands generated by number of words in the command for both ACG and ACGE.</p>
<h1>B Dataset statistics</h1>
<h2>B. 1 Additional statistics about the datasets</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: First column shows the number of admissible commands per extracted game state (i.e., represents the target in TextWorld ACG). Second column shows the number of entities (i.e., interactable objects or exits) per game state. Third column shows the number of words per context
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: First column shows the frequencies of the verbs (i.e., first word of a command) in the dataset. Second column shows the length of the commands in the dataset. Third column shows the length of the unique commands.</p>            </div>
        </div>

    </div>
</body>
</html>
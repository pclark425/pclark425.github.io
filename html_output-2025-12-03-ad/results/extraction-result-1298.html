<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1298 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1298</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1298</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-cd6cd14021fb5262bd00016ed31f190afd5a6f06</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cd6cd14021fb5262bd00016ed31f190afd5a6f06" target="_blank">No-regret Bayesian Optimization with Unknown Hyperparameters</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> This paper presents the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters, and proposes several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees.</p>
                <p><strong>Paper Abstract:</strong> Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1298.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1298.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Gaussian Process Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization agent that adapts GP kernel hyperparameters (lengthscales) and RKHS norm bounds over time to ensure no-regret convergence when the true hyperparameters are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive GP-UCB (A-GP-UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A GP-based Bayesian optimization algorithm using a UCB acquisition: it maintains a Gaussian process surrogate (posterior mean mu_t and posterior variance sigma_t), computes acquisition x_{t+1} = argmax_x mu_t(x) + sqrt(beta_t) sigma_t(x), and adapts kernel lengthscales and the RKHS norm bound via monotone scaling functions g(t) (shortening lengthscales) and b(t) (increasing norm bound). It can be combined with online hyperparameter estimators (MAP/HMC) and enforces truncation/scale limits.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (Gaussian process surrogate with UCB acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts experimental design by progressively expanding the considered function class: lengthscales are scaled as theta_t = theta_0 / g(t) (shorter lengthscales => rougher functions) and the RKHS norm bound as B_t = b(t) g(t)^d B_0; beta_t (confidence width) is increased accordingly using mutual information I_{theta_t}(y_t; f). The paper proposes ways to set g(t), b(t) (via a combined h(t) matched to a sublinear reference regret p(t)) and to combine these scalings with MAP/HMC estimates (either min(theta_MAP, theta_0/g(t)) or theta_MAP / max(g(t),1)). The algorithm picks next experiments by maximizing the GP-UCB acquisition under the adapted hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box continuous optimization problems (unknown objective functions); evaluated on synthetic 1D bump function, GP-sampled functions on grids, and a 4D logistic-regression hyperparameter tuning task (MNIST validation loss).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown (black-box) objective function, continuous compact domain D subset R^d, noisy evaluations with sigma-sub-Gaussian noise, potentially nonstationary / irregular (e.g., functions with local narrow bumps), no gradient access.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Problem dimension d is variable: experiments include d=1 (synthetic bumped function), grid-interpolated GP samples (finite grid), and d=4 (hyperparameter tuning); action is selecting continuous input x in D per iteration; complexity measures include information capacity gamma_t which scales with g(t)^d and with kernel type (e.g., Gaussian: O(g(t)^d (log t)^{d+1})).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: provably no-regret (sublinear cumulative regret) under monotone unbounded g(t), b(t) that grow slowly enough; empirically attains close-to-optimal simple regret comparable to BO with online hyperparameter estimation and converges in the benchmarks used. In experiments, A-GP-UCB avoided getting stuck in local optima where HMC-based baselines did and achieved sublinear cumulative regret (though larger than GP-UCB with oracle hyperparameters due to extra exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines without adaptation (GP-UCB with misspecified hyperparameters or MAP/HMC alone) can get stuck in local optima or maintain constant per-step regret; GP-UCB with true hyperparameters outperforms A-GP-UCB in cumulative regret but is not available in practice when hyperparameters are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported as empirically comparable to BO with online hyperparameter estimation: A-GP-UCB reaches good simple-regret solutions within the same evaluation budgets used in experiments (initialization with 2^d random points plus tens-to-hundreds of BO iterations in the plotted experiments); exact numeric sample counts are not specified in-tabular form in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Uses UCB acquisition (mu + sqrt(beta_t) sigma) where beta_t is adapted to account for expanded function classes (beta_t includes b(t) g(t)^d B_0 and mutual information term). Exploration is increased by shortening lengthscales (increasing model complexity / uncertainty) and enlarging the norm bound; h(t) (combined scaling) is chosen to match a sublinear reference regret p(t) so adaptation increases exploration only when the current model suggests the regret has plateaued.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>GP-UCB with true (oracle) hyperparameters, GP-UCB with MAP hyperparameter estimation, GP-UCB with HMC marginalization of hyperparameters, a conservative variant of Wang & de Freitas (2014) style lengthscale shrinking, and non-adaptive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Theoretically proves first BO algorithm with provable no-regret convergence when kernel hyperparameters are unknown by slowly expanding the candidate RKHS via g(t), b(t). 2) Shows mutual information gamma_t grows with g(t)^d (kernel-dependent), so g,b must grow slowly (e.g., log t) to keep regret sublinear. 3) Proposes practical adaptive rules for h(t) (match estimated regret to a sublinear reference p(t)), and ways to combine with MAP/HMC; 4) Empirically A-GP-UCB attains close-to-optimal simple regret and avoids local optima in cases where baselines with hyperparameter inference (especially HMC) get stuck; cumulative regret is higher than an oracle GP-UCB due to deliberate extra exploration but remains sublinear.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) If g(t), b(t) grow too fast the algorithm incurs excessive exploration and may lose sublinear regret; 2) High-dimensional problems amplify the cost because gamma_t scales as g(t)^d, making aggressive scaling impractical in large d; 3) The method trades off extra cumulative regret for safety against misspecification — there is a cost to not knowing hyperparameters; 4) Performance depends on reasonable choices for p(t) and the regret estimator, and one-step estimator (16) can be noisy and overshoot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No-regret Bayesian Optimization with Unknown Hyperparameters', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1298.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1298.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO acquisition algorithm that selects points by maximizing the upper confidence bound mu_t(x) + sqrt(beta_t) sigma_t(x) using a GP surrogate; provable sublinear regret when GP hyperparameters (and RKHS norm bound) are known.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP-UCB (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard GP-based Bayesian optimization using the UCB acquisition rule; relies on fixed kernel hyperparameters and a prescribed beta_t to form high-probability confidence intervals and guide exploration vs. exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (UCB acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Not adaptive in the sense of this paper's A-GP-UCB: GP-UCB uses fixed hyperparameters (or separately estimated ones) and balances exploration with exploitation via the chosen beta_t schedule; when combined with online hyperparameter estimation it implicitly adapts but without the controlled expansion guarantees introduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same black-box continuous optimization tasks (synthetic bump, GP-sampled grids, MNIST logistic-regression hyperparameter tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous objective, noisy evaluations; when hyperparameters are correct the GP model is well-specified; if misspecified, confidence intervals may not contain f and the method can prematurely converge to local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Depends on the benchmark: in experiments includes d=1 and d=4 problems; requires solving an inner optimization (the UCB maximization) at each iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>When used with true/oracle hyperparameters: lowest cumulative regret in experiments (best-case). When hyperparameters are misspecified and not adapted, performance can be poor (stuck in local optima).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Provably sample-efficient (sublinear regret) when correct hyperparameters and beta_t bound hold; empirical sample efficiency depends strongly on correct hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by beta_t schedule; larger beta_t increases exploration; no internal mechanism to expand RKHS when hyperparameters are wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as a baseline against A-GP-UCB variants, and against hyperparameter-estimation strategies (MAP, HMC).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrated as the theoretical gold-standard when hyperparameters known; but when hyperparameters are misspecified, GP-UCB can fail to converge (constant regret) as shown in synthetic experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to guarantee convergence/no-regret when kernel hyperparameters or norm bounds are misspecified; sensitive to GP prior mismatch (e.g., functions with local narrow bumps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No-regret Bayesian Optimization with Unknown Hyperparameters', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1298.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1298.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HMC-hyper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hamiltonian Monte Carlo hyperparameter marginalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian hyperparameter inference procedure (HMC) used to marginalize GP kernel hyperparameters rather than using point estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP with HMC hyperparameter marginalization (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GP-UCB (or GP surrogate) where kernel hyperparameters are sampled from the posterior using Hamiltonian Monte Carlo and predictions/marginalizations are integrated over these samples rather than using a single MAP estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization with Bayesian hyperparameter inference (HMC marginalization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by updating posterior samples of hyperparameters as new data arrives; exploration arises from uncertainty in hyperparameters but there is no explicit mechanism to systematically expand the RKHS to guarantee eventual containment of the true function.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same BO benchmarks used in experiments (synthetic bump function, GP samples, MNIST hyperparameter tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown noisy black-box functions; HMC reflects posterior belief over stationary GP hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as the GP-UCB experiments; computational cost higher due to HMC sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically, in the synthetic bump example HMC-based hyperparameter marginalization tended to over-estimate lengthscales (favor too-smooth functions) and got stuck in a local optimum, producing constant per-step regret (poor cumulative regret) compared to A-GP-UCB and oracle GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can be sample-inefficient in presence of RKHS functions that are improbable under stationary GP priors; may require more data to shift posterior away from misleading large-lengthscale modes.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration results indirectly from posterior hyperparameter uncertainty; however, when posterior concentrates on overly-smooth hyperparameters exploration is insufficient to find narrow local maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against A-GP-UCB variants and GP-UCB with MAP and with oracle hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>HMC marginalization, while principled, can empirically lead to over-smoothing and premature convergence to local optima on functions that are in the RKHS but improbable under the GP prior; A-GP-UCB avoids this by explicitly enlarging the function class.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May concentrate posterior mass on large lengthscales based on local data, preventing exploration of narrow/global bumps; does not provide no-regret guarantees when the prior is misspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No-regret Bayesian Optimization with Unknown Hyperparameters', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1298.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1298.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAP-hyper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum a Posteriori hyperparameter estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Point-estimate approach to infer GP kernel hyperparameters by maximizing the posterior (or marginal likelihood), commonly used in BO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP with MAP hyperparameter estimation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses MAP to update GP kernel hyperparameters online; combined with GP-UCB acquisition. In A-GP-UCB the MAP estimate can be truncated or scaled by g(t) to merge with the controlled expansion strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization with online MAP hyperparameter updates</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Updates hyperparameters each iteration via MAP; A-GP-UCB combines MAP with its scaling by taking elementwise min(theta_MAP, theta_0/g(t)) or theta_MAP / max(g(t),1) to ensure controlled exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same BO benchmarks (synthetic bump, GP samples, MNIST tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown black-box continuous functions with noisy observations; MAP tends to prefer smooth functions if data is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by experiment; MAP is computationally cheap relative to sampling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>When combined with A-GP-UCB scaling, MAP-based A-GP-UCB achieves good practical performance: empirically it eventually finds correct lengthscales and attains sublinear regret, improving over plain MAP+GP-UCB which can be misled at small t.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Plain GP-UCB with MAP-only (no controlled g(t), b(t)) can be misled by MAP estimates and may fail to find narrow/global bumps, though often works well when the objective resembles a GP sample.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Good sample efficiency when the objective resembles GP prior; A-GP-UCB improves robustness in misspecified cases with similar empirical sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>MAP sets a single hyperparameter point and UCB's beta_t controls exploration; A-GP-UCB's scaling modulates MAP to encourage exploration when regret plateaus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against HMC marginalization, A-GP-UCB variants, oracle GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MAP is practical and often sufficient; combining MAP with A-GP-UCB scaling gives the empirical benefits of MAP while providing theoretical convergence guarantees under unknown hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>MAP can be overconfident and prefer overly-smooth models based on local data, causing insufficient exploration unless counteracted by A-GP-UCB-style expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No-regret Bayesian Optimization with Unknown Hyperparameters', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1298.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1298.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WDF-variant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wang & de Freitas-style lengthscale shrinking (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic that decreases GP lengthscales over time (when a lower bound is known) to encourage exploration; the paper tests a conservative variant (line search to enforce sigma >= kappa).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Modified Wang & de Freitas (2014) variant (conservative)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Heuristic BO variant that shrinks kernel lengthscales when posterior predictive standard deviation at the next UCB-selected point falls below a threshold kappa; implemented here via a line search to find the smallest scaling such that sigma_t(x_{t+1}) >= kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Heuristic Bayesian optimization via scheduled lengthscale shrinking</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Shrinks lengthscales whenever predictive uncertainty at candidate next point falls below a set threshold, thus forcing increased model roughness and exploration; in experiments the conservative variant ensures sigma >= 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same synthetic and benchmark tasks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous objectives with noisy evaluations; method assumes or enforces minimal permitted lengthscale behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as other baselines; line search adds computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically in the synthetic example this conservative WDF-style method expanded the function class too aggressively and produced constant per-step regret (failed to obtain sublinear cumulative regret) — i.e., it over-explored and did not repeatedly exploit found optima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Poor in the tested scenario because aggressive shrinking forced excessive exploration and prevented concentration on near-optimal inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Biased toward exploration via enforced minimal predictive uncertainty; lacks the principled matching to a sublinear reference regret used in A-GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to A-GP-UCB variants, GP-UCB baselines (MAP/HMC), and oracle GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Illustrates that naive or overly aggressive lengthscale shrinking can cause persistent exploration and prevent sublinear regret; motivates the need for controlled, sublinear growth of g(t), b(t) as in A-GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Over-explores if no proper lower bound on hyperparameters is known; fails to guarantee sublinear regret when enforced reference growth is not sublinear.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No-regret Bayesian Optimization with Unknown Hyperparameters', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Convergence rates of efficient global optimization algorithms <em>(Rating: 2)</em></li>
                <li>Streaming kernel regression with provably adaptive mean, variance, and regularization <em>(Rating: 2)</em></li>
                <li>High-dimensional Gaussian process bandits <em>(Rating: 2)</em></li>
                <li>Practical Bayesian optimization of machine learning algorithms <em>(Rating: 2)</em></li>
                <li>Automatic gait optimization with Gaussian process regression <em>(Rating: 1)</em></li>
                <li>Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1298",
    "paper_id": "paper-cd6cd14021fb5262bd00016ed31f190afd5a6f06",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "A-GP-UCB",
            "name_full": "Adaptive Gaussian Process Upper Confidence Bound",
            "brief_description": "A Bayesian optimization agent that adapts GP kernel hyperparameters (lengthscales) and RKHS norm bounds over time to ensure no-regret convergence when the true hyperparameters are unknown.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive GP-UCB (A-GP-UCB)",
            "agent_description": "A GP-based Bayesian optimization algorithm using a UCB acquisition: it maintains a Gaussian process surrogate (posterior mean mu_t and posterior variance sigma_t), computes acquisition x_{t+1} = argmax_x mu_t(x) + sqrt(beta_t) sigma_t(x), and adapts kernel lengthscales and the RKHS norm bound via monotone scaling functions g(t) (shortening lengthscales) and b(t) (increasing norm bound). It can be combined with online hyperparameter estimators (MAP/HMC) and enforces truncation/scale limits.",
            "adaptive_design_method": "Bayesian optimization (Gaussian process surrogate with UCB acquisition)",
            "adaptation_strategy_description": "Adapts experimental design by progressively expanding the considered function class: lengthscales are scaled as theta_t = theta_0 / g(t) (shorter lengthscales =&gt; rougher functions) and the RKHS norm bound as B_t = b(t) g(t)^d B_0; beta_t (confidence width) is increased accordingly using mutual information I_{theta_t}(y_t; f). The paper proposes ways to set g(t), b(t) (via a combined h(t) matched to a sublinear reference regret p(t)) and to combine these scalings with MAP/HMC estimates (either min(theta_MAP, theta_0/g(t)) or theta_MAP / max(g(t),1)). The algorithm picks next experiments by maximizing the GP-UCB acquisition under the adapted hyperparameters.",
            "environment_name": "Black-box continuous optimization problems (unknown objective functions); evaluated on synthetic 1D bump function, GP-sampled functions on grids, and a 4D logistic-regression hyperparameter tuning task (MNIST validation loss).",
            "environment_characteristics": "Unknown (black-box) objective function, continuous compact domain D subset R^d, noisy evaluations with sigma-sub-Gaussian noise, potentially nonstationary / irregular (e.g., functions with local narrow bumps), no gradient access.",
            "environment_complexity": "Problem dimension d is variable: experiments include d=1 (synthetic bumped function), grid-interpolated GP samples (finite grid), and d=4 (hyperparameter tuning); action is selecting continuous input x in D per iteration; complexity measures include information capacity gamma_t which scales with g(t)^d and with kernel type (e.g., Gaussian: O(g(t)^d (log t)^{d+1})).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: provably no-regret (sublinear cumulative regret) under monotone unbounded g(t), b(t) that grow slowly enough; empirically attains close-to-optimal simple regret comparable to BO with online hyperparameter estimation and converges in the benchmarks used. In experiments, A-GP-UCB avoided getting stuck in local optima where HMC-based baselines did and achieved sublinear cumulative regret (though larger than GP-UCB with oracle hyperparameters due to extra exploration).",
            "performance_without_adaptation": "Baselines without adaptation (GP-UCB with misspecified hyperparameters or MAP/HMC alone) can get stuck in local optima or maintain constant per-step regret; GP-UCB with true hyperparameters outperforms A-GP-UCB in cumulative regret but is not available in practice when hyperparameters are unknown.",
            "sample_efficiency": "Reported as empirically comparable to BO with online hyperparameter estimation: A-GP-UCB reaches good simple-regret solutions within the same evaluation budgets used in experiments (initialization with 2^d random points plus tens-to-hundreds of BO iterations in the plotted experiments); exact numeric sample counts are not specified in-tabular form in the paper.",
            "exploration_exploitation_tradeoff": "Uses UCB acquisition (mu + sqrt(beta_t) sigma) where beta_t is adapted to account for expanded function classes (beta_t includes b(t) g(t)^d B_0 and mutual information term). Exploration is increased by shortening lengthscales (increasing model complexity / uncertainty) and enlarging the norm bound; h(t) (combined scaling) is chosen to match a sublinear reference regret p(t) so adaptation increases exploration only when the current model suggests the regret has plateaued.",
            "comparison_methods": "GP-UCB with true (oracle) hyperparameters, GP-UCB with MAP hyperparameter estimation, GP-UCB with HMC marginalization of hyperparameters, a conservative variant of Wang & de Freitas (2014) style lengthscale shrinking, and non-adaptive baselines.",
            "key_results": "1) Theoretically proves first BO algorithm with provable no-regret convergence when kernel hyperparameters are unknown by slowly expanding the candidate RKHS via g(t), b(t). 2) Shows mutual information gamma_t grows with g(t)^d (kernel-dependent), so g,b must grow slowly (e.g., log t) to keep regret sublinear. 3) Proposes practical adaptive rules for h(t) (match estimated regret to a sublinear reference p(t)), and ways to combine with MAP/HMC; 4) Empirically A-GP-UCB attains close-to-optimal simple regret and avoids local optima in cases where baselines with hyperparameter inference (especially HMC) get stuck; cumulative regret is higher than an oracle GP-UCB due to deliberate extra exploration but remains sublinear.",
            "limitations_or_failures": "1) If g(t), b(t) grow too fast the algorithm incurs excessive exploration and may lose sublinear regret; 2) High-dimensional problems amplify the cost because gamma_t scales as g(t)^d, making aggressive scaling impractical in large d; 3) The method trades off extra cumulative regret for safety against misspecification — there is a cost to not knowing hyperparameters; 4) Performance depends on reasonable choices for p(t) and the regret estimator, and one-step estimator (16) can be noisy and overshoot.",
            "uuid": "e1298.0",
            "source_info": {
                "paper_title": "No-regret Bayesian Optimization with Unknown Hyperparameters",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "GP-UCB",
            "name_full": "Gaussian Process Upper Confidence Bound",
            "brief_description": "A BO acquisition algorithm that selects points by maximizing the upper confidence bound mu_t(x) + sqrt(beta_t) sigma_t(x) using a GP surrogate; provable sublinear regret when GP hyperparameters (and RKHS norm bound) are known.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GP-UCB (baseline)",
            "agent_description": "Standard GP-based Bayesian optimization using the UCB acquisition rule; relies on fixed kernel hyperparameters and a prescribed beta_t to form high-probability confidence intervals and guide exploration vs. exploitation.",
            "adaptive_design_method": "Bayesian optimization (UCB acquisition)",
            "adaptation_strategy_description": "Not adaptive in the sense of this paper's A-GP-UCB: GP-UCB uses fixed hyperparameters (or separately estimated ones) and balances exploration with exploitation via the chosen beta_t schedule; when combined with online hyperparameter estimation it implicitly adapts but without the controlled expansion guarantees introduced here.",
            "environment_name": "Same black-box continuous optimization tasks (synthetic bump, GP-sampled grids, MNIST logistic-regression hyperparameter tuning).",
            "environment_characteristics": "Unknown continuous objective, noisy evaluations; when hyperparameters are correct the GP model is well-specified; if misspecified, confidence intervals may not contain f and the method can prematurely converge to local optima.",
            "environment_complexity": "Depends on the benchmark: in experiments includes d=1 and d=4 problems; requires solving an inner optimization (the UCB maximization) at each iteration.",
            "uses_adaptive_design": null,
            "performance_with_adaptation": "When used with true/oracle hyperparameters: lowest cumulative regret in experiments (best-case). When hyperparameters are misspecified and not adapted, performance can be poor (stuck in local optima).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Provably sample-efficient (sublinear regret) when correct hyperparameters and beta_t bound hold; empirical sample efficiency depends strongly on correct hyperparameters.",
            "exploration_exploitation_tradeoff": "Controlled by beta_t schedule; larger beta_t increases exploration; no internal mechanism to expand RKHS when hyperparameters are wrong.",
            "comparison_methods": "Compared as a baseline against A-GP-UCB variants, and against hyperparameter-estimation strategies (MAP, HMC).",
            "key_results": "Demonstrated as the theoretical gold-standard when hyperparameters known; but when hyperparameters are misspecified, GP-UCB can fail to converge (constant regret) as shown in synthetic experiments.",
            "limitations_or_failures": "Fails to guarantee convergence/no-regret when kernel hyperparameters or norm bounds are misspecified; sensitive to GP prior mismatch (e.g., functions with local narrow bumps).",
            "uuid": "e1298.1",
            "source_info": {
                "paper_title": "No-regret Bayesian Optimization with Unknown Hyperparameters",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "HMC-hyper",
            "name_full": "Hamiltonian Monte Carlo hyperparameter marginalization",
            "brief_description": "A Bayesian hyperparameter inference procedure (HMC) used to marginalize GP kernel hyperparameters rather than using point estimates.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GP with HMC hyperparameter marginalization (baseline)",
            "agent_description": "GP-UCB (or GP surrogate) where kernel hyperparameters are sampled from the posterior using Hamiltonian Monte Carlo and predictions/marginalizations are integrated over these samples rather than using a single MAP estimate.",
            "adaptive_design_method": "Bayesian optimization with Bayesian hyperparameter inference (HMC marginalization)",
            "adaptation_strategy_description": "Adapts by updating posterior samples of hyperparameters as new data arrives; exploration arises from uncertainty in hyperparameters but there is no explicit mechanism to systematically expand the RKHS to guarantee eventual containment of the true function.",
            "environment_name": "Same BO benchmarks used in experiments (synthetic bump function, GP samples, MNIST hyperparameter tuning).",
            "environment_characteristics": "Unknown noisy black-box functions; HMC reflects posterior belief over stationary GP hyperparameters.",
            "environment_complexity": "Same as the GP-UCB experiments; computational cost higher due to HMC sampling.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically, in the synthetic bump example HMC-based hyperparameter marginalization tended to over-estimate lengthscales (favor too-smooth functions) and got stuck in a local optimum, producing constant per-step regret (poor cumulative regret) compared to A-GP-UCB and oracle GP-UCB.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Can be sample-inefficient in presence of RKHS functions that are improbable under stationary GP priors; may require more data to shift posterior away from misleading large-lengthscale modes.",
            "exploration_exploitation_tradeoff": "Exploration results indirectly from posterior hyperparameter uncertainty; however, when posterior concentrates on overly-smooth hyperparameters exploration is insufficient to find narrow local maxima.",
            "comparison_methods": "Compared against A-GP-UCB variants and GP-UCB with MAP and with oracle hyperparameters.",
            "key_results": "HMC marginalization, while principled, can empirically lead to over-smoothing and premature convergence to local optima on functions that are in the RKHS but improbable under the GP prior; A-GP-UCB avoids this by explicitly enlarging the function class.",
            "limitations_or_failures": "May concentrate posterior mass on large lengthscales based on local data, preventing exploration of narrow/global bumps; does not provide no-regret guarantees when the prior is misspecified.",
            "uuid": "e1298.2",
            "source_info": {
                "paper_title": "No-regret Bayesian Optimization with Unknown Hyperparameters",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "MAP-hyper",
            "name_full": "Maximum a Posteriori hyperparameter estimation",
            "brief_description": "Point-estimate approach to infer GP kernel hyperparameters by maximizing the posterior (or marginal likelihood), commonly used in BO.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GP with MAP hyperparameter estimation (baseline)",
            "agent_description": "Uses MAP to update GP kernel hyperparameters online; combined with GP-UCB acquisition. In A-GP-UCB the MAP estimate can be truncated or scaled by g(t) to merge with the controlled expansion strategy.",
            "adaptive_design_method": "Bayesian optimization with online MAP hyperparameter updates",
            "adaptation_strategy_description": "Updates hyperparameters each iteration via MAP; A-GP-UCB combines MAP with its scaling by taking elementwise min(theta_MAP, theta_0/g(t)) or theta_MAP / max(g(t),1) to ensure controlled exploration.",
            "environment_name": "Same BO benchmarks (synthetic bump, GP samples, MNIST tuning).",
            "environment_characteristics": "Unknown black-box continuous functions with noisy observations; MAP tends to prefer smooth functions if data is limited.",
            "environment_complexity": "Varies by experiment; MAP is computationally cheap relative to sampling methods.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "When combined with A-GP-UCB scaling, MAP-based A-GP-UCB achieves good practical performance: empirically it eventually finds correct lengthscales and attains sublinear regret, improving over plain MAP+GP-UCB which can be misled at small t.",
            "performance_without_adaptation": "Plain GP-UCB with MAP-only (no controlled g(t), b(t)) can be misled by MAP estimates and may fail to find narrow/global bumps, though often works well when the objective resembles a GP sample.",
            "sample_efficiency": "Good sample efficiency when the objective resembles GP prior; A-GP-UCB improves robustness in misspecified cases with similar empirical sample efficiency.",
            "exploration_exploitation_tradeoff": "MAP sets a single hyperparameter point and UCB's beta_t controls exploration; A-GP-UCB's scaling modulates MAP to encourage exploration when regret plateaus.",
            "comparison_methods": "Compared against HMC marginalization, A-GP-UCB variants, oracle GP-UCB.",
            "key_results": "MAP is practical and often sufficient; combining MAP with A-GP-UCB scaling gives the empirical benefits of MAP while providing theoretical convergence guarantees under unknown hyperparameters.",
            "limitations_or_failures": "MAP can be overconfident and prefer overly-smooth models based on local data, causing insufficient exploration unless counteracted by A-GP-UCB-style expansion.",
            "uuid": "e1298.3",
            "source_info": {
                "paper_title": "No-regret Bayesian Optimization with Unknown Hyperparameters",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "WDF-variant",
            "name_full": "Wang & de Freitas-style lengthscale shrinking (modified)",
            "brief_description": "A heuristic that decreases GP lengthscales over time (when a lower bound is known) to encourage exploration; the paper tests a conservative variant (line search to enforce sigma &gt;= kappa).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Modified Wang & de Freitas (2014) variant (conservative)",
            "agent_description": "Heuristic BO variant that shrinks kernel lengthscales when posterior predictive standard deviation at the next UCB-selected point falls below a threshold kappa; implemented here via a line search to find the smallest scaling such that sigma_t(x_{t+1}) &gt;= kappa.",
            "adaptive_design_method": "Heuristic Bayesian optimization via scheduled lengthscale shrinking",
            "adaptation_strategy_description": "Shrinks lengthscales whenever predictive uncertainty at candidate next point falls below a set threshold, thus forcing increased model roughness and exploration; in experiments the conservative variant ensures sigma &gt;= 0.1.",
            "environment_name": "Same synthetic and benchmark tasks used in the paper.",
            "environment_characteristics": "Unknown continuous objectives with noisy evaluations; method assumes or enforces minimal permitted lengthscale behavior.",
            "environment_complexity": "Same as other baselines; line search adds computational overhead.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically in the synthetic example this conservative WDF-style method expanded the function class too aggressively and produced constant per-step regret (failed to obtain sublinear cumulative regret) — i.e., it over-explored and did not repeatedly exploit found optima.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Poor in the tested scenario because aggressive shrinking forced excessive exploration and prevented concentration on near-optimal inputs.",
            "exploration_exploitation_tradeoff": "Biased toward exploration via enforced minimal predictive uncertainty; lacks the principled matching to a sublinear reference regret used in A-GP-UCB.",
            "comparison_methods": "Compared directly to A-GP-UCB variants, GP-UCB baselines (MAP/HMC), and oracle GP-UCB.",
            "key_results": "Illustrates that naive or overly aggressive lengthscale shrinking can cause persistent exploration and prevent sublinear regret; motivates the need for controlled, sublinear growth of g(t), b(t) as in A-GP-UCB.",
            "limitations_or_failures": "Over-explores if no proper lower bound on hyperparameters is known; fails to guarantee sublinear regret when enforced reference growth is not sublinear.",
            "uuid": "e1298.4",
            "source_info": {
                "paper_title": "No-regret Bayesian Optimization with Unknown Hyperparameters",
                "publication_date_yy_mm": "2019-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Convergence rates of efficient global optimization algorithms",
            "rating": 2
        },
        {
            "paper_title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
            "rating": 2
        },
        {
            "paper_title": "High-dimensional Gaussian process bandits",
            "rating": 2
        },
        {
            "paper_title": "Practical Bayesian optimization of machine learning algorithms",
            "rating": 2
        },
        {
            "paper_title": "Automatic gait optimization with Gaussian process regression",
            "rating": 1
        },
        {
            "paper_title": "Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics",
            "rating": 1
        }
    ],
    "cost": 0.01713775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>No-Regret Bayesian Optimization with Unknown Hyperparameters</h1>
<p>Felix Berkenkamp<br>Department of Computer Science<br>ETH Zurich<br>Zurich, Switzerland<br>Angela P. Schoellig<br>SCHOELLIG@UTIAS.UTORONTO.CA<br>Institute for Aerospace Studies<br>University of Toronto<br>Toronto, Canada<br>Andreas Krause<br>KRAUSEA@ETHZ.CH<br>Department of Computer Science<br>ETH Zurich<br>Zurich, Switzerland</p>
<h2>Editor:</h2>
<h4>Abstract</h4>
<p>Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.</p>
<p>Keywords: Bayesian optimization, Unknown hyperparameters, Reproducing kernel Hilbert space (RKHS), Bandits, No regret</p>
<h2>1. Introduction</h2>
<p>The performance of machine learning algorithms often critically depends on the choice of tuning inputs, e.g., learning rates or regularization constants. Picking these correctly is a key challenge. Traditionally, these inputs are optimized using grid or random search (Bergstra and Bengio, 2012). However, as data sets become larger the computation time required to train a single model increases, which renders these approaches less applicable. Bayesian optimization (BO, Mockus (2012)) is an alternative method that provably determines good inputs within few evaluations of the underlying objective function. BO methods construct a statistical model of the underlying objective function and use it to evaluate inputs that</p>
<p>are informative about the optimum. However, the theoretical guarantees, empirical performance, and data efficiency of BO algorithms critically depend on their own choice of hyperparameters and, in particular, on the prior distribution over the function space. Thus, we effectively shift the problem of tuning inputs one level up, to the tuning of hyperparameters of the BO algorithm.</p>
<p>In this paper, we use a Gaussian processes (GP, Rasmussen and Williams (2006)) for the statistical model. We present the first BO algorithm that does not require knowledge about the hyperparameters of the GP's stationary kernel and provably converges to the global optimum. To this end, we adapt the hyperparameters of the kernel and our BO algorithm, so that the associated function space grows over time. The resulting algorithm provably converges to the global optimum and retains theoretical convergence guarantees, even when combined with online estimation of hyperparameters.</p>
<p>Related work General BO has received a lot of attention in recent years. Typically, BO algorithms suggest inputs to evaluate by maximizing an acqusition function that measures informativeness about the optimum. Classical acquisition functions are the expected improvement over the best known function value encountered so far given the GP distribution (Mockus et al., 1978) and the Upper Confidence Bound algorithm, GP-UCB, which applies the 'optimism in the face of uncertainty' principle. The latter is shown to provably converge by Srinivas et al. (2012). Durand et al. (2018) extend this framework to the case of unknown measurement noise. A related method is truncated variance reduction by Bogunovic et al. (2016), which considers the reduction in uncertainty at candidate locations for the optimum. Hennig and Schuler (2012) propose entropy search, which approximates the distribution of the optimum of the objective function and uses the reduction of the entropy in this distribution as an acquisition function. Alternative information-theoretic methods are proposed by Hernández-Lobato et al. (2014); Wang and Jegelka (2017); Ru et al. (2018). Other alternatives are the knowledge gradient (Frazier et al., 2009), which is one-step Bayes optimal, and information directed sampling by Russo and Van Roy (2014), which considers a tradeoff between regret and information gained when evaluating an input. Kirschner and Krause (2018) extend the latter framework to heteroscedastic noise.</p>
<p>These BO methods have also been successful empirically. In machine learning, they are used to optimize the performance of learning methods (Brochu et al., 2010; Snoek et al., 2012). BO is also applicable more broadly; for example, in reinforcement learning to optimize a parametric policy for a robot (Calandra et al., 2014; Lizotte et al., 2007; Berkenkamp et al., 2016) or in control to optimize the energy output of a power plant (Abdelrahman et al., 2016). It also forms the backbone of Google vizier, a service for tuning black-box functions (Golovin et al., 2017).</p>
<p>Some of the previous BO algorithms provide theoretical guarantees about convergence to the optimum. These theoretical guarantees only hold when the kernel hyperparameters are known a priori. When this is not the case, hyperparameters are often inferred using either maximum a posteriori estimates or sampling-based inference (Snoek et al., 2012). Unfortunately, methods that estimate the hyperparameters online are known to get stuck in local optima (Bull, 2011). Instead, we propose to adapt the hyperparameters online in order to enlarge the function space over time, which allows us to provide guarantees in terms of convergence to the global optimum without knowing the hyperparameters. Wang and de</p>
<p>Freitas (2014) analyze this setting when a lower bound on the kernel lengthscales is known a priori. They decrease the lengthscales over time and bound the regret in terms of the known lower-bound on the lengthscales. Empirically, similar heuristics are used by Wang et al. (2016); Wabersich and Toussaint (2016). In contrast, this paper considers the case where the hyperparameters are not known. Moreover, the scaling of the hyperparameters in the previous two papers did not depend on the dimensionality of the problem, which can cause the function space to increase too quickly.</p>
<p>Considering larger function classes as more data becomes available is the core idea behind structural risk minimization (Vapnik, 1992) in statistical learning theory. However, there data is assumed to be sampled independently and identically distributed. This is not the case in BO, where new data is generated actively based on past information.</p>
<p>Our contribution In this paper, we present Adaptive GP-UCB (A-GP-UCB), the first algorithm that provably converges to the globally optimal inputs when BO hyperparameters are unknown. Our method expands the function class encoded in the model over time, but does so slowly enough to ensure sublinear regret and convergence to the optimum. Based on the theoretical insights, we propose practical variants of the algorithm with guaranteed convergence. Since our method can be used as an add-on module to existing algorithms with hyperparameter estimation, it achieves similar performance empirically, but avoids local optima when hyperparameters are misspecified. In summary, we:</p>
<ul>
<li>Provide theoretical convergence guarantees for BO with unknown hyperparameters;</li>
<li>Propose several practical algorithms based on the theoretical insights;</li>
<li>Evaluate the performance in practice and show that our method retains the empirical performance of heuristic methods based on online hyperparameter estimation, but leads to significantly improved performance when the model is misspecified initially.
The remainder of the paper is structured as follows. We state the problem in Sec. 2 and provide relevant background material in Sec. 3. We derive our main theoretical result in Sec. 4 and use insights gained from the theory to propose practical algorithms. We evaluate these algorithms experimentally in Sec. 5 and draw conclusions in Sec. 6. The technical details of the proofs are given in the appendix.</li>
</ul>
<h1>2. Problem Statement</h1>
<p>In general, BO considers global optimization problems of the form</p>
<p>$$
\mathbf{x}^{*}=\underset{\mathbf{x} \in \mathcal{D}}{\operatorname{argmax}} f(\mathbf{x})
$$</p>
<p>where $\mathcal{D} \subset \mathbb{R}^{d}$ is a compact domain over which we want to optimize inputs $\mathbf{x}$, and $f: \mathcal{D} \rightarrow \mathbb{R}$ is an objective function that evaluates the reward $f(\mathbf{x})$ associated with a given input configuration $\mathbf{x}$. For example, in a machine learning application, $f(\mathbf{x})$ may be the validation loss and $\mathbf{x}$ may be the tuning inputs (e.g., regularization parameters) of the training algorithm. We do not have any significant prior knowledge about the structure of $f$. Specifically, we cannot assume convexity or that we have access to gradient information. Moreover, evaluations of $f$ are corrupted by $\sigma$-sub-Gaussian noise, a general class of noise models that includes, for example, bounded or Gaussian noise.</p>
<p>Regret We aim to construct a sequence of input evaluations $\mathbf{x}<em t="t">{t}$, that eventually maximizes the function value $f\left(\mathbf{x}</em>=$ $\max }\right)$. One natural way to prove this convergence is to show that an algorithm has sublinear regret. The instantaneous regret at iteration $t$ is defined as $r_{t<em t="t">{\mathbf{x} \in \mathcal{D}} f(\mathbf{x})-f\left(\mathbf{x}</em>}\right) \geq 0$, which is the loss incurred by evaluating the function at $\mathbf{x<em T="T">{t}$ instead of at the a priori unknown optimal inputs. The cumulative regret is defined as $R</em>\right)$ and the algorithm converges. Thus, we aim to design an optimization algorithm that has sublinear regret.}=$ $\sum_{0<t \leq T} r_{t}$, the sum of regrets incurred over $T$ steps. If we can show that the cumulative regret is sublinear for a given algorithm, that is, $\lim _{t \rightarrow \infty} R_{t} / t=0$, then eventually the algorithm evaluates the function at inputs that lead to close-to-optimal function values most of the time. We say that such an algorithm has no-regret. Intuitively, if the average regret approaches zero then, on average, the instantaneous regret must approach zero too, since $r_{t}$ is strictly positive. This implies that there exists a $t>0$ such that $f\left(\mathbf{x}_{t}\right)$ is arbitrarily close to $f\left(\mathbf{x}^{*</p>
<p>Regularity assumptions Without further assumptions, it is impossible to achieve sublinear regret on (1). In the worst case, $f$ could be discontinuous at every input in $\mathcal{D}$. To make the optimization problem in (1) tractable, we make regularity assumptions about $f$. In particular, we assume that the function $f$ has low complexity, as measured by the norm in a reproducing kernel Hilbert space (RKHS, Christmann and Steinwart (2008)). An RKHS $\mathcal{H}<em 0="0" _geq="\geq" i="i">{k}$ contains well-behaved functions of the form $f(\mathbf{x})=\sum</em>} \alpha_{i} k\left(\mathbf{x}, \mathbf{x<em i="i">{i}\right)$, for given representer points $\mathbf{x}</em>$ that decay sufficiently quickly. The kernel $k(\cdot, \cdot)$ determines the roughness and size of the function space and the induced RKHS norm $|f|} \in \mathbb{R}^{d}$ and weights $\alpha_{i} \in \mathbb{R<em k="k">{k}=\sqrt{\langle f, f\rangle}$ measures the complexity of a function $f \in \mathcal{H}</em>$ with respect to the kernel.</p>
<p>In the following, we assume that $f$ in (1) has bounded RKHS norm $|f|<em _theta="\theta">{k</em>}} \leq B$ with respect to a kernel $k_{\theta}$ that is parameterized by hyperparameters $\theta$. We write $\mathcal{H<em k__theta="k_{\theta">{\theta}$ for the corresponding RKHS, $\mathcal{H}</em>$ by the lengthscales $\theta$,}}$. For known $B$ and $\theta$, no-regret BO algorithms for (1) are known, e.g., GP-UCB (Srinivas et al., 2012). In practice, these hyperparameters need to be tuned. In this paper, we consider the case where $\theta$ and $B$ are unknown. We focus on stationary kernels, which measure similarity based on the distance of inputs, $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k\left(\mathbf{x}-\mathbf{x}^{\prime}\right)$. The most commonly used hyperparameters for these kernels are the lengthscales $\theta \in \mathbb{R}^{d}$, which scale the inputs to the kernel in order to account for different magnitudes in the different components of $\mathbf{x}$ and effects on the output value. That is, we scale the difference $\mathbf{x}-\mathbf{x}^{\prime</p>
<p>$$
k_{\theta}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k\left(\frac{[\mathbf{x}]<em 1="1">{1}-\left[\mathbf{x}^{\prime}\right]</em>{[\theta]}<em d="d">{1}}, \ldots, \frac{[\mathbf{x}]</em>\right]}-\left[\mathbf{x}^{\prime<em d="d">{d}}{[\theta]</em>\right)
$$}</p>
<p>where $[\mathbf{x}]_{i}$ denotes the $i$ th element of $\mathbf{x}$. Typically, these kernels assign larger similarity scores to inputs when the scaled distance between these two inputs is small. Another common hyperparameter is the prior variance of the kernel, a multiplicative constant that determines the magnitude of the kernel. We assume $k(\mathbf{x}, \mathbf{x})=1$ for all $\mathbf{x} \in \mathcal{D}$ without loss of generality, as any multiplicative scaling can be absorbed by the norm bound $B$.</p>
<p>In summary, our goal is to efficiently solve (1) via a BO algorithm with sublinear regret, where $f$ lies in some RKHS $\mathcal{H}<em k__theta="k_{\theta">{\theta}$, but neither the hyperparameters $\theta$ nor the normbound $|f|</em>$ are known.}</p>
<h1>3. Background</h1>
<p>In this section, we review Gaussian processes (GPs) and Bayesian optimization (BO).</p>
<h3>3.1 Gaussian processes (GP)</h3>
<p>Based on the assumptions in Sec. 2, we can use GPs to infer confidence intervals on $f$. The goal of GP inference is to infer a posterior distribution over the nonlinear map $f(\mathbf{x}): D \rightarrow \mathbb{R}$ from an input vector $\mathbf{x} \in D$ to the function value $f(\mathbf{x})$. This is accomplished by assuming that the function values $f(\mathbf{x})$, associated with different values of $\mathbf{x}$, are random variables and that any finite number of these random variables have a joint Gaussian distribution (Rasmussen and Williams, 2006). A GP distribution is parameterized by a prior mean function and a covariance function or kernel $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$, which defines the covariance of any two function values $f(\mathbf{x})$ and $f\left(\mathbf{x}^{\prime}\right)$ for $\mathbf{x}, \mathbf{x}^{\prime} \in D$. In this work, the mean is assumed to be zero without loss of generality. The choice of kernel function is problem-dependent and encodes assumptions about the unknown function.</p>
<p>We can condition a $G P\left(0, k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right)$ on a set of $t$ past observations $\mathbf{y}<em 1="1">{t}=\left(y</em>}, \ldots, y_{t}\right)$ at inputs $\mathcal{A<em 1="1">{t}=\left{\mathbf{x}</em>}, \ldots, \mathbf{x<em t="t">{t}\right}$ in order to obtain a posterior distribution on $f(\mathbf{x})$ for any input $\mathbf{x} \in D$. The GP model assumes that observations are noisy measurements of the true function value, $y</em>}=f\left(\mathbf{x<em t="t">{t}\right)+\omega</em>$, where}$, where $\omega_{t} \sim \mathcal{N}\left(0, \sigma^{2}\right)$. The posterior distribution is again a $G P\left(\mu_{t}(\mathbf{x}), k_{t}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right)$ with mean $\mu_{t}$, covariance $k_{t}$, and variance $\sigma_{t</p>
<p>$$
\begin{aligned}
\mu_{t}(\mathbf{x}) &amp; =\mathbf{k}<em t="t">{t}(\mathbf{x})\left(\mathbf{K}</em>}+\mathbf{I} \sigma^{2}\right)^{-1} \mathbf{y<em t="t">{t} \
k</em>}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &amp; =k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)-\mathbf{k<em t="t">{t}(\mathbf{x})\left(\mathbf{K}</em>}+\mathbf{I} \sigma^{2}\right)^{-1} \mathbf{k<em t="t">{t}^{\mathrm{T}}\left(\mathbf{x}^{\prime}\right) \
\sigma</em>)
\end{aligned}
$$}^{2}(\mathbf{x}) &amp; =k_{t}(\mathbf{x}, \mathbf{x</p>
<p>The covariance matrix $\mathbf{K}<em t="t">{t} \in \mathbb{R}^{t \times t}$ has entries $\left[\mathbf{K}</em>\right]<em i="i">{(i, j)}=k\left(\mathbf{x}</em>}, \mathbf{x<em t="t">{j}\right), i, j \in{1, \ldots, t}$, and the vector $\mathbf{k}</em>}(\mathbf{x})=\left[k\left(\mathbf{x}, \mathbf{x<em t="t">{1}\right), \ldots, k\left(\mathbf{x}, \mathbf{x}</em>}\right)\right]$ contains the covariances between the input $\mathbf{x}$ and the observed data points in $\mathcal{A<em t="t">{t}$. The identity matrix is denoted by $\mathbf{I}</em>$.} \in \mathbb{R}^{t \times t</p>
<h3>3.2 Learning RKHS functions with GPs</h3>
<p>The GP framework uses a statistical model that makes different assumptions from the ones made about $f$ in Sec. 2. In particular, we assume a different noise model, and samples from a $\operatorname{GP}\left(0, k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right)$ are rougher than RKHS funtions and are not contained in $\mathcal{H}_{k}$. However, GPs and RKHS functions are closely related (Kanagawa et al., 2018) and it is possible to use GP models to infer reliable confidence intervals on $f$ in (1).</p>
<p>Lemma 1 (Abbasi-Yadkori (2012); Chowdhury and Gopalan (2017)) Assume that $f$ has bounded RKHS norm $|f|<em t="t">{k} \leq B$ and that measurements are corrupted by $\sigma$-subGaussian noise. If $\beta</em>}^{1 / 2}=B+4 \sigma \sqrt{I\left(\mathbf{y<em t="t">{t} ; f\right)+1+\ln (1 / \delta)}$, then for all $\mathbf{x} \in D$ and $t \geq 0$ it holds jointly with probability at least $1-\delta$ that $\left|f(\mathbf{x})-\mu</em>)$.}(\mathbf{x})\right| \leq \beta_{t}^{1 / 2} \sigma_{t}(\mathbf{x</p>
<p>Lemma 1 implies that, with high probability, the true function $f$ is contained in the confidence intervals induced by the posterior GP distribution that uses the kernel $k$ from Lemma 1 as a covariance function, scaled by an appropriate factor $\beta_{t}$. Here, $I\left(\mathbf{y}<em t="t">{t} ; f\right)$ denotes the mutual information between the GP prior on $f$ and the $t$ measurements $\mathbf{y}</em>$. Intriguingly, for</p>
<p>GP models this quantity only depends on the inputs $\mathbf{x}<em t="t">{t}$ and not the corresponding measurement $y</em>$, the mutual information is given by}$. Specifically, for a given set of measurements $\mathbf{y}_{\mathcal{A}}$ at inputs $\mathbf{x} \in \mathcal{A</p>
<p>$$
I\left(\mathbf{y}<em _mathcal_A="\mathcal{A">{\mathcal{A}} ; f\right)=0.5 \log \left|\mathbf{I}+\sigma^{-2} \mathbf{K}</em>\right|
$$}</p>
<p>where $\mathbf{K}<em _mathbf_x="\mathbf{x">{\mathcal{A}}$ is the kernel matrix $\left[k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right]</em>$ are about the function $f$. If the function values are independent of each other under the GP prior, they will provide large amounts of new information. However, if measurements are taken close to each other as measured by the kernel, they are correlated under the GP prior and provide less information.}, \mathbf{x}^{\prime} \in \mathcal{A}}$ and $|\cdot|$ is the determinant. Intuitively, the mutual information measures how informative the collected samples $\mathbf{y}_{\mathcal{A}</p>
<h1>3.3 Bayesian Optimization (BO)</h1>
<p>BO aims to find the global maximum of an unknown function (Mockus, 2012). The framework assumes that evaluating the function is expensive in terms of time required or monetary costs, while other computational resources are comparatively inexpensive. In general, BO methods model the objective function $f$ with a statistical model and use it to determine informative sample locations. A popular approach is to model the underlying function with a GP, see Sec. 3.1. GP-based BO methods use the posterior mean and variance predictions in (3) and (5) to compute the next sample location.</p>
<p>One commonly used algorithm is the GP-UCB algorithm by Srinivas et al. (2012). It uses confidence intervals on the function $f$, e.g., from Lemma 1, in order to select as next input the point with the largest plasuble function value according to the model,</p>
<p>$$
\mathbf{x}<em t="t">{t+1}=\underset{\mathbf{x} \in \mathcal{D}}{\operatorname{argmax}} \mu</em>)
$$}(\mathbf{x})+\beta_{t}^{1 / 2} \sigma_{t}(\mathbf{x</p>
<p>Intuitively, (7) selects new evaluation points at locations where the upper bound of the confidence interval of the GP estimate is maximal. Repeatedly evaluating the function $f$ at inputs $\mathbf{x}_{t+1}$ given by (7) improves the mean estimate of the underlying function and decreases the uncertainty at candidate locations for the maximum, so that the global maximum is provably found eventually (Srinivas et al., 2012). While (7) is also an optimization problem, it only depends on the GP model of $f$ and solving it therefore does not require any expensive evaluations of $f$.</p>
<p>Regret bounds Srinivas et al. (2012) show that the GP-UCB algorithm has cumulative regret $R_{t}=\mathcal{O}\left(\sqrt{t \beta_{t} \gamma_{t}}\right)$ for all $t \geq 1$ with the same $(1-\delta)$ probability as the confidence intervals, e.g., in Lemma 1, hold. Here $\gamma_{t}$ is the largest amount of mutual information that could be obtained by any algorithm from at most $t$ measurements,</p>
<p>$$
\gamma_{t}=\max <em _mathcal_A="\mathcal{A">{\mathcal{A} \subset D,|\mathcal{A}| \leq t} I\left(\mathbf{y}</em> ; f\right)
$$}</p>
<p>We refer to $\gamma_{t}$ as the information capacity, since it can be interpreted as a measure of complexity of the function class associated with a GP prior. It was shown by Srinivas et al. (2012) that $\gamma_{t}$ has a sublinear dependence on $t$ for many commonly used kernels such as the Gaussian kernel. As a result, $R_{t}$ has a sublinear dependence on $t$ so that $R_{t} / t \rightarrow 0$ and therefore GP-UCB converges to function evaluations close to $f\left(\mathbf{x}^{*}\right)$. These regret bounds</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A sample from the GP prior in Fig. 1a typically varies at a consistent rate over the input space. However, RKHS functions with the same kernel may be less consistent and can have bumps, as in Fig. 1b (gray). As a result, inferring the posterior lengthscales based on measurements (blue crosses in Fig. 1b) can lead to erroneous results. In Fig. 1c, most of the probability mass of the posterior lengthscales has concentrated around large lengthscales that encode smooth functions. Consequently, the GP's $2 \sigma$ confidence intervals in Fig. 1b (blue shaded) based on the posterior samples do not contain the true function.
were extended to Thompson sampling, an algorithm that uses samples from the posterior GP as the acquisition function, by Chowdhury and Gopalan (2017).</p>
<p>Online hyperparameter estimation In the previous section, we have seen that the GPUCB algorithm provably converges. However, it requires access to a RKHS norm bound $|f|_{\theta} \leq B$ under the correct kernel hyperparameters $\theta$ in order to construct reliable confidence intervals using Lemma 1. In practice, these are unknown and have to be estimated online, e.g., based on a prior distribution placed on $\theta$. Unfortunately, it is well-known that online estimation of the inputs, be it via maximum a posteriori (MAP) or sampling methods, does not always converge to the optimum (Bull, 2011). The problem does not primarily lie with the inference scheme, but rather with the assumptions made by the GP. In particular, typical samples drawn from a GP with a stationary kernel tend to have a similar rate of change throughout the input space, see Fig. 1a. In contrast, the functions inside the RKHS, as specified in Sec. 2, can have different rates of change and are thus improbable under the GP prior. For example, the grey function in Fig. 1b is almost linear but has one bump that defines the global maximum, which makes this function an improbable sample under the GP prior even though it belongs to the RKHS induced by the same kernel. This property of GPs with stationary kernels means that, for inference, it is sufficient to estimate the lengthscales in a small part of the state-space in order to make statements about the function space globally. This is illustrated in Fig. 1c, where we show samples from the posterior distribution over the lengthscales based on the measurements obtained from the GP-UCB algorithm in Fig. 1b (blue crosses). Even though the prior distribution on the lengthscales $\theta$ is suggestive of short lengthscales, most of the posterior probability mass is concentrated around lengthscales that are significantly larger than the true ones. As a result, even under model averaging over the samples from the posterior distribution of the lengthscales, the GP confidence intervals do not contain the true function in Fig. 1b. This is not a problem of the inference method applied, but rather a direct consequence of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: BO algorithms get stuck in local optima when the hyperpararameters of the model are misspecified. In Fig. 2a, the true function is not contained within the GP's confidence intervals (blue shaded), so that GP-UCB only collects data at the local optimum on the right (green arrow), see also Fig. 1. Our method expands the function class over time by scaling the hyperparameters, which encourages additional exploration in Fig. 2b. The function class grows slowly enough, so that the global optimum is provably found in Fig. 2c.
the probabilistic model that we have specified based on the stationary kernel, which does not consider functions with different rates of change to be likely.</p>
<h1>4. The Adaptive GP-UCB Algorithm</h1>
<p>In this section, we extend the GP-UCB algorithm to the case where neither the norm bound $B$ nor the lengthscales $\theta$ are known. In this case, it is always possible that the local optimum is defined by a local bump based on a kernel with small lengthscales, which has not been encountered by the data points as in Fig. 1b. The only solution to avoid this problem is to keep exploring to eventually cover the input space $\mathcal{D}$ (Bull, 2011). We consider expanding the function space associated with the hyperparameters slowly over time, so that we obtain sublinear regret once the true function class has been identified. Intuitively, this can help BO algorithms avoid premature convergence to local optima caused by misspecified hyperparameters $\theta$ and $B$. For example, in Fig. 2a, the GP-UCB algorithm has converged to a local maximum. By decreasing the lengthscales, we increase the underlying function class, which means that the GP confidence intervals on the function increase. This enables GP-UCB to explore further so that the global optimum is found, as shown in Fig. 2c.</p>
<p>Specifically, we start with an initial guess $\theta_{0}$ and $B_{0}$ for the lengthscales and norm bound on $f$, respectively. Over the iterations, we scale down the lengthscales and scale up the norm bound,</p>
<p>$$
\theta_{t}=\frac{1}{g(t)} \theta_{0}, \quad B_{t}=b(t) g(t)^{d} B_{0}
$$</p>
<p>where $g: \mathbb{N} \rightarrow \mathbb{R}<em _0="&gt;0">{&gt;0}$ and $b: \mathbb{N} \rightarrow \mathbb{R}</em>}$ with $b(0)=g(0)=1$ are functions that can additionally depend on the data collected up to iteration $t, \mathcal{A<em t="t">{t}$ and $\mathbf{y}</em>$ of the kernel become shorter, which enlarges the underlying function space:}$. As $g(t)$ increases, the lengthscales $\theta_{t</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The function $f$ in Fig. 3a has RKHS norm $|f|<em 0="0">{\theta</em>$ does not converge (constant regret). Our method scales the lengthscales and norm bound by $g(t)$ and $b(t)$, so that we eventually capture the true model. Scaling the hyperparameters beyond the true ones leads to additional exploration and thus larger cumulative regret than GP-UCB with the true, unknown hyperparameters $\theta$ and $B$. However, as long as the cumulative regret is upper bounded by a sublinear function $p$, ultimately the A-GP-UCB algorithm converges to the global optimum.}}&gt;B_{0}$. To account for this, we expand the norm ball by $b(t)$ over time. When we scale down the lengthscales by $g(t)$, the norm of $f$ in the resulting RKHS is larger, see Lemma 2. We account for this when defining the norm ball $B_{t}$ in (9). In Fig. 3b, the GP-UCB algorithm based on the misspecified hyperparameters $B_{0}$ and $\theta_{0</p>
<p>Lemma 2 (Bull, 2011, Lemma 4) If $f \in \mathcal{H}<em _theta_prime="\theta^{\prime">{\theta}$, then $f \in \mathcal{H}</em> \leq \theta$, and}}$ for all $0&lt;\theta^{\prime</p>
<p>$$
|f|<em _theta_prime="\theta^{\prime">{\mathcal{H}</em> \frac{[\theta]}}^{2}}^{2} \leq\left(\prod_{i=1}^{d<em i="i">{i}}{\left[\theta^{\prime}\right]</em>\right)|f|}<em _theta="\theta">{\mathcal{H}</em>
$$}}^{2</p>
<p>Lemma 2 states that when decreasing the lengthscales $\theta$, the resulting function space contains the previous one. Thus, as $g(t)$ increases we consider larger RKHS spaces as candidate spaces for the function $f$. In addition, as we increase $b(t)$, we consider larger norm balls within the function space $\mathcal{H}<em t="t">{\theta</em>}}$, which corresponds to more complex functions. However, it follows from (10) that, as we increase $g(t)$, we also increase the norm of any existing function in $\mathcal{H<em 0="0">{\theta</em>}}$ by at most a factor of $g(t)^{d}$. This is illustrated in Fig. 3a: as we scale up the norm ball to $b(t) B_{0}$, we capture $f$ under the initial lengthscales $\theta_{0}$. However, by shortening the lengthscales by $g(t)$, the function $f$ has a larger norm in the new function space $\mathcal{H<em t="t">{\theta</em>}}=\mathcal{H<em 0="0">{\theta</em>$ in (9).} / g(t)}$. We account for this through the additional scaling factor $g(t)^{d}$ in the norm bound $B_{t</p>
<p>Theoretical analysis Based on the previous derivations together with Lemma 2, it is clear that, if $g(t)$ and $b(t)$ are monotonically increasing functions and $f \in \mathcal{H}<em>{\theta</em>{t^{<em>}}}$ with $|f|<em>{\theta</em>{t^{</em>}}} \leq$ $B_{t^{<em>}}$ for some $t^{</em>}&gt;0$, then $f \in \mathcal{H}<em t="t">{\theta</em>$ and $|f|}<em t="t">{\theta</em>$ for all $t \geq t^{}} \leq B_{t<em>}$. That is, once the function $f$ is contained within the norm ball of $B_{t^{</em>}}$ for the lengthscales $\theta_{t^{*}}$, then, for any further increase in $b(t)$ or $g(t)$, the function $f$ is still contained in the candidate space $\left{f \in \mathcal{H}<em t="t">{\theta</em>\right}$.}} \mid f \leq B_{t</p>
<p>Based on this insight, we propose A-GP-UCB in Algorithm 1. At iteration $t$, A-GP-UCB sets the GP lengthscales to $\theta_{t}$ and selects new inputs $\mathbf{x}<em t="t">{t+1}$ similar to the GP-UCB algorithm, but based on the norm bound $B</em>$. We extend the analysis of GP-UCB and Lemma 1 to obtain our main result.
Theorem 1 Assume that $f$ has bounded RKHS norm $|f|<em _theta="\theta">{k</em>}}^{2} \leq B$ in a RKHS that is parametrized by a stationary kernel $k_{\theta}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ with unknown lengthscales $\theta$. Based on an initial guess, $\theta_{0}$ and $B_{0}$, define monotonically increasing functions $g(t)&gt;0$ and $b(t)&gt;0$ and run A-GP-UCB with $\beta_{t}^{1 / 2}=b(t) g(t)^{d} B_{0}+4 \sigma \sqrt{I_{\theta_{t}}\left(\mathbf{y<em t="t">{t} ; f\right)+1+\ln (1 / \delta)}$ and GP lengthscales $\theta</em> / g(t)$. Then, with probability at least $(1-\delta)$, we obtain a regret bound of}=\theta_{0</p>
<p>$$
R_{t} \leq 2 B \max \left(g^{-1}\left(\max <em 0="0">{i} \frac{\left[\theta</em>\right]<em i="i">{i}}{[\theta]</em>
$$}}\right), b^{-1}\left(\frac{B}{B_{0}}\right)\right)+\sqrt{C_{1} t \beta_{t} I_{\theta_{t}}\left(\mathbf{y}_{t} ; f\right)</p>
<p>where $I_{\theta_{t}}$ is the mutual information in (6) based on the GP model with lengthscales $\theta_{t}$ and $C_{1}=8 / \log \left(1+\sigma^{-2}\right)$.</p>
<p>The proof is given in the appendix. Intuitively, the regret bound in (11) splits the run of the algorithm into two distinct phases. In the first one, either the RKHS space $\mathcal{H}<em t="t">{\theta</em>$ do not necessarily contain the true function $f$, as in Fig. 1b. In these iterations, we obtain constant regret that is bounded by $2 B$, since $|f|}}(\mathcal{D})$ or the norm bound $B_{t}$ are too small to contain the true function $f$. Thus, the GP confidence intervals scaled by $\beta_{t}^{1 / 2<em _theta="\theta">{\infty} \leq|f|</em>} \leq$ $B$. After both $g$ and $b$ have grown sufficiently in order for the considered function space to contain the true function, the confidence bounds are reliable and we can apply the theoretical results of the GP-UCB algorithm. This is illustrated in Fig. 3b: If the initial hyperparameters $\theta_{0}$ and $B_{0}$ are misspecified, the confidence intervals do not contain $f$ and GP-UCB does not converge. We avoid this problem by increasing $b(t)$ and $g(t)$ over time, so that we eventually contain $f$ in our function class. However, increasing the norm ball and decreasing the lengthscales beyond the true ones causes additional exploration and thus additional cumulative regret relative to GP-UCB with the true, unknown hyperparameters. This additional regret represents the cost of not knowing the hyperparameters in advance. As long as the overall regret remains bounded by a sublinear function $p(t)$, our method eventually converges to the global optimum. The regret bound in (11) depends on the true hyperparameters $\theta$ and $B$. However, the algorithm does not depend on them. Theorem 1 provides an instance-specific bound, since the mutual information depends on the inputs in $\mathcal{A<em _theta__t="\theta_{t">{t}$. One can obtain a worst-case upper bound by bounding $I</em>}}\left(\mathbf{y<em t="t">{t} ; f\right) \leq \gamma</em>$. While Theorem 1 assumes that the noise properties are known, the results can be extended to estimate the noise similar to Durand et al. (2018).}\left(\theta_{t}\right)$, which is the worst-case mutual information as in (8), but based on the GP model with lengthscales $\theta_{t</p>
<p>For arbitrary functions $g(t)$ and $b(t)$, the candidate function space $\left{f \in \mathcal{H}<em t="t">{\theta</em>$ on the scaling factor $g(t)$.
Proposition 2 Let $k_{\theta}$ be a stationary kernel parameterized by lengthscales $\theta$ as in (2) and define $\gamma_{t}(\theta)$ for lengthscales $\theta$ as in (8). Define the lengthscales as $\theta_{t}=\theta_{0} / g(t)$ as in (9).}} \mid f \leq B_{t}\right}$ can grow at a faster rate than it contracts by selecting informative measurements $y_{t}$ according to (7). In particular, in the regret term $\sqrt{C_{1} t \beta_{t} \gamma_{t}}$ both $\beta_{t}$ and $\gamma_{t}$ depend on the scaling factors $g(t)$ and $b(t)$. If these factors grow at a faster rate than $\sqrt{t}$, the resulting algorithm does not enjoy sublinear regret. We have the following result that explicitly states the dependence of $\gamma_{t</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Adaptive</span><span class="w"> </span><span class="nt">GP-UCB</span><span class="o">(</span><span class="nt">A-GP-UCB</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Input</span><span class="w"> </span><span class="nt">space</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">G</span><span class="w"> </span><span class="nt">P</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">0</span><span class="o">,</span><span class="w"> </span><span class="nt">k</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">functions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="o">(</span><span class="nt">t</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="o">(</span><span class="nt">t</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">B_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">diam</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">all</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">0</span><span class="o">,</span><span class="nt">1</span><span class="o">,</span><span class="nt">2</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Set</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">GP</span><span class="w"> </span><span class="nt">kernel</span><span class="w"> </span><span class="nt">lengthscsales</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nt">g</span><span class="o">(</span><span class="nt">t</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">beta_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">1</span><span class="w"> </span><span class="err">/</span><span class="w"> </span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">B</span><span class="o">(</span><span class="nt">t</span><span class="o">)+</span><span class="nt">4</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma</span><span class="w"> </span><span class="err">\</span><span class="nt">sqrt</span><span class="p">{</span><span class="err">I_{\theta_{t</span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">f</span><span class="err">\</span><span class="nt">right</span><span class="o">)+</span><span class="nt">1</span><span class="o">+</span><span class="err">\</span><span class="nt">ln</span><span class="w"> </span><span class="o">(</span><span class="nt">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="nt">delta</span><span class="o">)</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">B</span><span class="o">(</span><span class="nt">t</span><span class="o">)=</span><span class="nt">b</span><span class="o">(</span><span class="nt">t</span><span class="o">)</span><span class="w"> </span><span class="nt">g</span><span class="o">(</span><span class="nt">t</span><span class="o">)^</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="w"> </span><span class="nt">B_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Choose</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">argmax</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathbf{x</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">)+</span><span class="err">\</span><span class="nt">beta_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">1</span><span class="w"> </span><span class="err">/</span><span class="w"> </span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Evaluate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">y_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="o">=</span><span class="nt">f</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)+</span><span class="err">\</span><span class="nt">epsilon_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Perform</span><span class="w"> </span><span class="nt">Bayesian</span><span class="w"> </span><span class="nt">update</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">obtain</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<ul>
<li>If $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\frac{1}{2}\left|\mathbf{x}-\mathbf{x}^{\prime}\right|_{2}^{2}\right)$ is the squared exponential (Gaussian) kernel, then</li>
</ul>
<p>$$
\gamma_{t}\left(\theta_{t}\right)=\mathcal{O}\left(g(t)^{d}(\log t)^{d+1}\right)
$$</p>
<ul>
<li>If $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left(2^{1-\nu} / \Gamma(\nu)\right) r^{\nu} B_{\nu}(r)$ is the Matérn kernel, where $r=\sqrt{2 \nu}\left|\mathbf{x}-\mathbf{x}^{\prime}\right|<em _nu="\nu">{2}, B</em>$ is the modified Bessel function with $\nu&gt;1$, and $\Gamma$ is the gamma function. Then</li>
</ul>
<p>$$
\gamma_{t}\left(\theta_{t}\right)=\mathcal{O}\left(g(t)^{2 \nu+d} t^{\frac{d(d+1)}{2 \nu+d(d+1)}} \log t\right)
$$</p>
<p>Proposition 2 explicitly states the relationship between $\gamma_{t}$ and $g(t)$. For the Gaussian kernel, if we scale down the lengthscales by a factor of two, the amount of mutual information that we can gather in the worst case, $\gamma_{t}$, grows by $2^{d}$. Given the dependence of $\gamma_{t}$ on $g(t)$, we can refine Theorem 1 to obtain concrete regret bounds for two commonly used kernels.
Corollary 3 If, under the assumptions of Theorem 1, $g(t)$ and $b(t)$ grow unbounded, then we obtain the following, high-probability regret bounds for Algorithm 1:</p>
<ul>
<li>Squared exponential kernel: $R_{t} \leq \mathcal{O}\left(b(t) \sqrt{t g(t)^{3 d} \gamma_{t}\left(\theta_{0}\right)}+g(t)^{d} \gamma_{t}\left(\theta_{0}\right) \sqrt{t}\right)$;</li>
<li>Matérn kernel: $R_{t} \leq \mathcal{O}\left(b(t) \sqrt{t g(t)^{2 \nu+3 d} \gamma_{t}\left(\theta_{0}\right)}+g(t)^{\nu+d} \gamma_{t}\left(\theta_{0}\right) \sqrt{t}\right)$.</li>
</ul>
<p>If $b(t)$ and $g(t)$ grow unbounded, the first term of the cumulative regret in (11) can be upper bounded by a constant. The remaining result is obtained by plugging in $\beta_{t}$ and the bounds from (8). Thus, any functions $g(t)$ and $b(t)$ that render the regret bounds in Corollary 3 sublinear allow the algorithm to converge, even though the true lengthscales and norm bound are unknown.</p>
<p>The specific choices of $b(t)$ and $g(t)$ matter for the regret bound in Theorem 1 in practice. Consider the one-dimensional case $d=1$ for the Gaussian kernel. Given the true hyperparameters $B$ and $\theta$, if we set $g(t)=\theta_{0} / \theta$ and $b(t)=B / B_{0}$ to be constant, we recover the non-adaptive regret bounds of GP-UCB with known hyperparameters. If $g(t)$ depends on $t$ and grows slowly, then the algorithm incurs constant regret during the initial rounds when the model is misspecified, while functions $g$ that grow to values larger than the optimal ones lead to additional exploration and incur an additional $\mathcal{O}\left(b(t) g(t)^{3 d / 2)}\right)$ factor in the cumulative regret in later rounds, as in Corollary 3. In the following section, we discuss appropriate choices for these functions in practice.</p>
<h1>4.1 Choosing the scaling functions $g(t)$ and $b(t)$</h1>
<p>It follows from Theorem 1 that A-GP-UCB achieves no-regret for any functions $g(t)$ and $b(t)$ that increase without bound and render (11) sublinear in $t$. Thus, the corresponding BO routine converges to the optimal value eventually. For example, $b(t)=g(t)=\log (t)$ satisfy this condition. However, the convergence guarantees in Theorem 1 are only meaningful once $t$ has grown sufficiently so that the true function is contained in the confidence intervals. In practice, BO is often used with objective functions $f$ that are expensive to evaluate, which imposes a hard constraint on the number of evaluations. For the regret bounds to be meaningful in this setting, we must choose functions $g$ and $b$ that grow fast enough to ensure that the constant regret period in (11) is small, yet slow enough that the effect of the sublinear regret is visible for small enough $t$. In the following, we propose two methods to choose $g(t)$ and $b(t)$ adaptively, based on the observations seen so far.</p>
<p>For convenience, we fix the relative magnitude of $g(t)$ and $b(t)$. In particular, we define $b(t)=1+\epsilon_{b}(t)$ and $g(t)^{d}=1+\epsilon_{g}(t)$ together with a weighting factor $\lambda=\epsilon_{b}(t) / \epsilon_{g}(t)$ that encodes whether we prefer to scale up the norm bound using $b(t)$ or decrease the lengthscales using $g(t)$. This allows us to reason about the overall magnitude of the scaling $h(t)=$ $\left(1+\epsilon_{g}(t)\right)\left(1+\epsilon_{b}(t)\right) \geq 1$, which can be uniquely decomposed into $g(t)$ and $b(t)$ given $\lambda$. For $\lambda=0$ we have $g(t)=h(t), b(t)=1$ and the algorithm prefers to attribute an increase in $h(t)$ to $g(t)$ and shorten the lengthscales, while for $\lambda \rightarrow \infty$ the algorithm prefers to scale up the RKHS norm. The assumptions in Corollary 3 hold for any $\lambda \in(0, \infty)$ if $h(t)$ grows unbounded. Moreover, we have that $g(t)^{d} \leq h(t)$ and $b(t) \leq h(t)$.</p>
<p>Reference regret While any function $h(t)$ that grows unbounded and renders the cumulative regret in Theorem 1 sublinear makes our method to converge to the optimum eventually, we want to ensure that our method performs well in finite time too. For fixed hyperparameters with $h(t)=1$, which implies $g(t)=b(t)=1$, our algorithm reduces to GP-UCB with hyperparameters $\theta_{0}$ and $B_{0}$ and the regret bound term $\sqrt{C_{1} \beta_{t} \gamma_{t}\left(\theta_{0}\right)}$ is sublinear, which is illustrated by the bottom curve in Fig. 3b. However, this does not imply no-regret if hyperparameters are misspecified as in Fig. 2a, since the first term in Theorem 1 is unbounded in this case. To avoid this, we must increase the scaling factor $h(t)$ to consider larger function classes.</p>
<p>We propose to define a sublinear reference regret $p(t)$, see Fig. 3b, and to scale $h(t)$ to match an estimate of the regret with respect to the current hyperparameters to this reference. As GP-UCB converges, the regret estimate with respect to the current hyperparameters levels off and drops below the reference $p(t)$. In these cases, we increase $h(t)$ to consider larger function classes and explore further. The choice of $p(t)$ thus directly specifies the amount of additional regret one is willing to incur for exploration. Specifically, given a regret estimate $\bar{R}<em t="t">{t}(h)$ that depends on the data collected so far and the selected scaling $h$, we obtain $h(t)$ from matching the reference, $\bar{R}</em>(h)=p(t)$, as</p>
<p>$$
h^{<em>}(t)=\bar{R}_{t}^{-1}(p(t)), \quad h(t)=\max \left(h^{</em>}(t), h(t-1)\right)
$$</p>
<p>Here we explicitly enforce that $h(t)$ must be an increasing function. In the following, we consider estimators $\bar{R}_{t}$ that are increasing functions of $h$, so that (14) can be solved efficiently via a line search.</p>
<p>Whether choosing $h(t)$ according to (14) leads to a sublinear function depends on the regret estimator $\bar{R}_{t}$. However, it is always possible to upper bound the $h(t)$ obtained from (14) by a fixed sublinear function. This guarantees sublinear regret eventually. In the following, we consider two estimators that upper bound the cumulative regret experienced so far with respect to the hyperparameters suggested by $h(t)$.</p>
<p>Regret bound As a first estimator for the cumulative regret, we consider the regret bound on $R_{t}$ in (11). We focus on the Gaussian kernel, but the arguments transfer directly to the case of the Matérn kernel. The term $\sqrt{C_{1} t \beta_{t} I_{\theta_{t}}\left(\mathbf{y}<em t="t">{t} ; f\right)}$ bounds the regret with respect to the current function class specified by $\theta</em>}$. In addition to the direct dependence on $b(t) g(t)^{d}$ in $\beta_{t}$, the regret bound also depends on $g(t)$ implicitly through the mutual information $I_{\theta_{t}}\left(\mathbf{y<em t="t">{t} ; f\right)$, where $\theta</em>}=\theta_{0} / g(t)$. To make the dependence on $g(t)$ more explicit, we use Theorem 2 and rewrite the mutual information as $(g(t) / g(t-1))^{d} I_{\theta_{t-1}}\left(\mathbf{y<em t="t">{t} ; f\right)$ instead. Note that the scaling factor was derived for $\gamma</em>$, but remains a good indicator of increase in mutual information in practice. With this replacement we use</p>
<p>$$
\bar{R}<em 1="1">{t}(h)=\sqrt{C</em>
$$} t \beta_{t}(b(t), g(t)) g(t)^{d} I_{\theta_{t-1}}\left(\mathbf{y}_{t} ; f\right)</p>
<p>to estimate the regret, where the term $\beta_{t}(b, g)$ is as in Theorem 1, but with the mutual information similarly replaced with the explicit dependence on $g(t)$. Solving (14) together with (15) is computationally efficient, since computing $\bar{R}_{t}$ does not require inverting the kernel matrix.</p>
<p>One step predictions While (15) is fast to compute, it requires us to know the dependence of $\gamma_{t}\left(\theta_{t}\right)$ on $h(t)$. Deriving analytic bounds can be infeasible for many kernels. As an alternative, we estimate the regret one-step ahead directly. In particular, if the considered function class is sufficiently large and our confidence intervals hold at all time steps $t&gt;0$, then the one-step ahead cumulative regret $R_{t+1}$ for our algorithm at iteration $t$ is bounded from above by</p>
<p>$$
\bar{R}<em j="1">{t}=2 \sum</em>\right)
$$}^{t} \beta_{j}^{1 / 2} \sigma_{j}\left(\mathbf{x}_{j+1</p>
<p>where each $\beta_{t}$ and $\sigma_{t}$ is based on the corresponding hyperparameters $\theta_{t}$. In Theorem 1, $R_{t+1}$ is further upper-bounded by (11). The regret estimate in (16) depends on $\mathbf{x}<em t="t">{t+1}$, which is the next input that would be evaluated based on the UCB criterion with GP hyperparameters scaled according to $h(t)$. As the hyperparameters for previous iterations are fixed, the only term that depends on $h(t)$ is the bound on the instantaneous regret, $r</em>} \leq 2 \beta_{t} \sigma_{t}\left(\mathbf{x<em t="t">{t+1}\right)$. Unlike (15), (16) is not able to exploit the known dependence of $\gamma</em>$ on $h(t)$, so that it cannot reason about the long-term effects of changing $h(t)$. This means that, empirically, the cumulative regret may overshoot the reference regret, only to settle below it later.</p>
<p>Scaling $h(t)$ according to (16) provides an interesting perspective on the method by Wang and de Freitas (2014). They decrease the kernel lengthscales whenever $\sigma_{t}\left(\mathbf{x}<em j="1">{t+1}\right) \leq \kappa$. In our framework, this corresponds to $p(t)=\sum</em>\right)\right) \geq \kappa t$, which is not sublinear. As a consequence, while they ultimately bound the cumulative regret using the smallest possible lengthscale, the choice for $p(t)$ forces too much exploration to achieve sublinear regret before the lower bound is reached. In contrast, if we choose $p(t)$ to be sublinear,}^{t} 2 \beta_{j} \max \left(\kappa, \sigma_{j}\left(\mathbf{x}_{j+1</p>
<p>then the function class grows slowly enough to ensure more careful exploration. This allows us to achieve sublinear regret in the case when a lower bound on the hyperparameters it not known.</p>
<h1>4.2 Practical Considerations and Discussion</h1>
<p>In this section, we discuss additional practical considerations and show how to combine the theoretical results with online inference of the hyperparameters.</p>
<p>Online inference and exploration strategies The theoretical results presented in the previous sections extend to the case where the initial guess $\theta_{0}$ of the GP's lengthscale is improved online using any estimator, e.g., with MAP estimation to obtain $\theta_{t}^{\mathrm{MAP}}$. Theoretically, as long as the change in $\theta_{0}$ is bounded, the cumulative regret increases by at most a constant factor. In practice, this bound can always be enforced by truncating the estimated hyperparameters. Moreover, the scaling induced by online inference can be considered to be part of $g(t)$ according to Lemma 2, in which case the norm bound can be adapted accordingly. In practice, online inference improves performance drastically, as it is often difficult to specify an appropriate relative initial scaling of the lengthscales $\theta_{0}$.</p>
<p>In more than one dimension, $d&gt;1$, there are multiple ways that MAP estimation can be combined with the theoretical results of the paper. The simplest one is to enforce an upper bound on the lengthscales based on $g(t)$,</p>
<p>$$
\theta_{t}=\min \left(\theta_{t}^{\mathrm{MAP}}, \theta_{0} / g(t)\right)
$$</p>
<p>where the min is taken elementwise. This choice is similar to the one by Wang et al. (2016). If all entries of $\theta_{0}$ have the same magnitude, this scaling can be understood as encouraging additional exploration in the smoothest direction of the input space first. This often makes sense, since MAP estimates tend to assume functions that are too smooth, see Fig. 1. However, it can be undesirable in the case when the true function only depends on a subset of the inputs. In these cases, the MAP estimate would correctly eliminate these inputs from the input space by assigning long lengthscales, but the scaling in (17) would encourage additional exploration in these directions first. Note that eventually exploring the entire input space is unavoidable to avoid getting stuck in local optima (Bull, 2011).</p>
<p>An alternative approach is to instead scale down the MAP estimate directly,</p>
<p>$$
\theta_{t}=\theta_{t}^{\mathrm{MAP}} / \max (g(t), 1)
$$</p>
<p>This scaling can be understood as evenly encouraging additional exploration in all directions. While (18) also explores in directions that have been eliminated by the MAP estimate, unlike (17) it simultaneously explores all directions relative to the MAP estimate. From a theoretical point of view, the choice of exploration strategy does not matter, as in the limit as $t \rightarrow \infty$ all lengthscales approach zero. In the one-dimensional case, the two strategies are equivalent. Both strategies use the MAP lengthscales for BO in the nominal case, but the $g(t)$ factor eventually scales down the lengthscales further. This ensures that our method only improves on the empirical performance of BO with MAP estimation.</p>
<p>In practice, maximum likelihood estimates for the inputs are often good enough when the underlying function resembles a sample from a GP. Thus, the approach presented in</p>
<p>this paper is most relevant when the underlying function has some 'nonstationarity'. In the literature, other approaches to deal with nonstationarity have been proposed. For example, Snoek et al. (2013) scale the input inputs through a beta function and infer its hyperparameters online. Our approach can easily be combined with any such method, as it works on top of any estimate provided by the underlying inference scheme. Moreover, in high-dimensional spaces one can combine our algorithm with methods to automatically identify a low-dimensional subspace of $\mathcal{D}$ (Djolonga et al., 2013; Wang et al., 2016).</p>
<p>In this paper, we have considered the kernel to be fixed, and only adapted the lengthscales and norm bound. However, often the kernel structure itself is a critical hyperparameter (Duvenaud et al., 2011). The strategy presented in this paper could be used to add rougher kernels over time or, for example, to adapt the $\nu$ input of the Matérn kernel, which determines its roughness.</p>
<p>Confidence intervals Empirically, $\beta_{t}$ is often set to a constant rather than using the theoretical bounds in Lemma 1, which leads to (point-wise) confidence intervals when $f$ is sampled from a GP model. In particular, typically measurement data is standardized to be zero mean and unit variance and $\beta_{t}$ is set to two or three. This often works well in practice, but does not provide any guarantees. However, if one were to believe the resulting confidence bounds, our method can be used to avoid getting stuck in local optima, too. In this case on can set $h(t)=g(t)$ and apply our method as before.</p>
<p>General discussion Knowing how the sample complexity of the underlying BO algorithm depends on the lengthscales also has implications in practice. For example, Wang et al. (2016) and Wabersich and Toussaint (2016) suggest to scale down the lengthscales by a factor of 2 and roughly 1.1, respectively, although not at every iteration. As shown in Sec. 4, this scales the regret bound by a factor of $g^{d}$, which quickly grows with the number of dimensions. Exponentiating their factors with $1 / d$ is likely to make their approaches more robust when BO is used in high-dimensional input spaces $\mathcal{D}$.</p>
<p>Lastly, in a comparison of multiple BO algorithms (acquisition functions) on a robotic platform, Calandra et al. (2014) conclude that the GP-UCB algorithm shows the best empirical performance for their problem. They use the theoretical version of the algorithm by Srinivas et al. (2012), in which $\beta_{t}$ grows with an additional factor of $\mathcal{O}\left(\sqrt{\log \left(t^{2}\right)}\right)$ relative to Lemma 1. In our framework with the bounds in Lemma 1, this is equivalent to scaling up the initial guess for the RKHS norm bound for $f$ by the same factor at every iteration, which increases the function class considered by the algorithm over time. We conjecture that this increase of the function class over time is probably responsible for pushing the MAP estimate of the lengthscales out of the local minima, which in turn led to better empirical performance.</p>
<h1>5. Experiments</h1>
<p>In this section, we evaluate our proposed method on several benchmark problems. As baselines, we consider algorithms based on the UCB acquisition function. We specify a strong gamma prior that encourages short lengthscales, and consider both maximum a posteriori (MAP) point-estimates of the hyperparameters and a Hamiltonian Monte Carlo (HMC) approach that samples from the posterior distribution of the hyperparameters and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Mean and standard deviation of the empirical simple and cumulative regret over ten different random initializations for the function in Fig. 2. The HMC baseline (red) gets stuck in a local optimum and obtains constant regret in Fig. 4a. GP-UCB with the true hyperparameters (gray dashed) obtains the lowest cumulative regret in Fig. 4b. However, our methods (orange/blue) increase the function class over time, see Fig. 4c, and thus obtain sublinear regret without knowing the true hyperparameters.
marginalizes them out. Unless otherwise specified, the initial lengthscales are set to $\theta_{0}=\mathbf{1}$, the initial norm bound is $B_{0}=2$, the confidence bounds hold with probability at least $\delta=$ 0.9 , and the tradeoff factor between $b(t)$ and $g(t)$ is $\lambda=0.1$.</p>
<p>We follow several best-practices in BO to ensure a fair comparison with the baselines. We rescale the input space $\mathcal{D}$ to the unit hypercube in order to ensure that both the initial lengthscales and the prior over lengthscales are reasonable for different problems. As is common in practice, the comparison baselines use the empirical confidence intervals suggested in Sec. 4.2, instead of the theoretical bounds in Lemma 1 that are used for our method. Lastly, we initialize all GPs with $2^{d}$ measurements that are collected uniformly at random within the domain $\mathcal{D}$. To measure performance, we use the cumulative regret that has been the main focus of this paper. In addition, we evaluate the different methods in terms of simple regret, which is the regret of the best inputs evaluated so far, $\max <em t_prime="t^{\prime">{x \in \mathcal{D}} f(x)-$ $\max </em>\right)$. This metric is relevant when costs during experiments do not matter and BO is only used to determine high-quality inputs by the end of the optimization procedure.}&lt;=t} f\left(\mathbf{x}_{t^{\prime}</p>
<h1>5.1 Synthetic Experiments</h1>
<p>Example function We first evaluate all proposed methods on the example function in Fig. 2, which lives inside the RKHS associated with a Gaussian kernel with $\theta=0.1$ and has norm $|f|<em _theta="\theta">{k</em>$ together with maximum a posteriori hyperparameter estimation. We compare against both GP-UCB with the fixed, correct hyperparameters and HMC hyperparameter estimation. Additionally, we consider a modified variant of the method suggested by Wang and de Freitas (2014), see Sec. 4.1. Rather than scaling the lengthscales by a fixed constant, we conduct a line search to find the smallest possible scaling factor that ren-}}=2$. We evaluate our proposed method for the sublinear reference function $p(t)=t^{0.9</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Simple and cumulative regret over 10 random seeds for samples from a GP with bounded RKHS norm. The GP-UCB algorithm with misspecified hyperparameters (magenta) fails to converge given only a wrong choice of $B_{0}$. In contrast, our methods (blue/orange) converge even though $\theta_{0}$ is misspecified in addition.
ders $\sigma_{t}\left(\mathbf{x}_{t+1}\right) \geq \kappa=0.1$. This is the most conservative variant of the algorithm. Note that we do not know a lower bound on the hyperparameters and therefore do not enforce it.</p>
<p>The results of the experiments are shown in Fig. 4. The simple regret plot in Fig. 4a shows that all methods based on hyperparameter adaptation evaluate close-to-optimal inputs eventually, and do so almost as quickly as GP-UCB based on the true hyperparameters (black, dashed). However, the method based on HMC hyperparameter estimation (red) considers functions that are too smooth and gets stuck in local optima, as in Fig. 2. This can also be seen in Fig. 4c, which plots the effective scaling $g(t)$ based on the combination of Bayesian hyperparameter estimation and hyperparameter adaptation through $h(t)$. The HMC hyperparameters consistenly over-estimate the lengthscales by a factor of roughly two. In contrast, while the MAP estimation leads to the wrong hyperparameters initially, the adaptation methods in (15) and (16) slowly increase the function class until the true lengthscales are found eventually. It can be seen that the one step estimate (16) (orange) is more noisy than the upper bound in (15) (blue).</p>
<p>While all adaptation methods determine good inputs quickly according to the simple regret, they perform differently in terms of the cumulative regret in Fig. 4b. As expected, the HMC method (red line) converges to a local optimum and experiences constant regret increase equal to the simple regret at every time step. The modified method of Wang and de Freitas (2014) (green line) expands the function class too aggressively and also achieves constant regret. Empirically, their method always explores and never repeatedly evaluates close-to-optimal inputs that would decrease cumulative regret. While the method works well in terms of simple regret, without a lower bound on the hyperparameters it never converges to sublinear regret. As expected from Theorem 1, GP-UCB based on the optimal hyperparameters achieves the lowest cumulative regret. Our two methods expand the function class over time, which allows them to converge to close-to-optimal inputs, even though MAP estimation estimates the hyperparameters wrongly initially. While the regret is sublinear, the additional exploration caused by $g(t)$ means that the cumulative regret is larger. This is the additional cost we incur for not knowing the hyperparameters in advance.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Simple and cumulative regret over 5 random seeds for a logistic regression problem. All methods determine close-to-optimal parameters. However, our methods explore more to counteract misspecified hyperparameters.</p>
<p>Samples from a GP As a second experiment, we compare GP-UCB to A-GP-UCB on samples drawn from a GP when the norm bound $B_{0}$ is misspecified. Samples from a GP are not contained in the RKHS. To avoid this technical issue, we sample function values from the posterior GP at only a finite number of discrete gridpoints and interpolate between them using the kernel with the correct lengthscales $\theta$. We rescale these functions to have RKHS norm of $B=4$, but use $B_{0}=0.25$ as an initial guess for both BO algorithms and do not use any hyperparameter estimation. Even though we use the correct kernel lengthscales for GP-UCB, $\theta_{0}=\theta=0.1$, this discrepancy means that the true function is not contained in the initial confidence intervals. As before, for our method we use the reference regret $p(t)=t^{0.9}$ and additionally misspecify the lengthscales, $\theta_{0}=1$.</p>
<p>The results are shown in Fig. 5. GP-UCB with the correct hyperparameters (black, dashed) obtains the lowest cumulative regret. However, it fails to converge when hyperparameters are misspecified (magenta), since the confidence intervals are too small to encourage any exploration. In contrast, our methods (blue/orange) converge to close-to-optimal inputs as in the previous example.</p>
<h1>5.2 Logistic Regression Experiment</h1>
<p>Lastly, we use our method to tune a logistic regression problem on the MNIST data set (LeCun, 1998). As in the experiment in Klein et al. (2016), we consider four training inputs: the learning rate, the $l_{2}$ regularization constant, the batch size, and the dropout rate. We use the validation loss as the optimization objective.</p>
<p>The results are shown in Fig. 6. Even though the input space is fairly high-dimensional with $d=4$, all algorithms determine close-to-optimal inputs quickly. In particular, MAP estimation determines that both the dropout rate and the batch size do not influence the validation loss significantly. Since the theoretical results in A-GP-UCB are compatible with MAP estimation, our approach achieves the same empirical performance, but has theoretical worst-case regret bounds. After convergence, the BO baselines repeatedly evaluate the same inputs, without gaining any new information. In contrast, our method continues to explore in order to potentially find better inputs. While it does not occur in this case, this allows us</p>
<p>to be more confident that the global optimum has been identified as $t$ increases. For standard BO methods, there is no guarantee of convergence with misspecified hyperparameters.</p>
<h1>6. Conclusion and Future Work</h1>
<p>We introduced A-GP-UCB, a BO algorithm that is provably no-regret when hyperparameters are unknown. Our method adapts the hyperparameters online, which causes the underlying BO algorithm to consider larger function spaces over time. Eventually, the function space is large enough to contain the true function, so that our algorithm provably converges. We evaluated our method on several benchmark problems, confirming that, on the one hand, it provably converges even in cases where standard BO algorithms get stuck in local optima, and, on the other hand, enjoys competitive performance as standard BO algorithms that do not have theoretical guarantees in this setting.</p>
<p>The main idea behind our analysis is that adapting the hyperparameters increases the cumulative regret bound, but we do so slowly enough to converge eventually. This idea is fairly general and could also be applied to other no-regret algorithms. Another potential future direction is to investigate alternative strategies to select the scaling factors $b(t)$ and $g(t)$ and consider adapting other parameters such as the kernel structure.</p>
<h2>Acknowledgments</h2>
<p>This research was supported in part by SNSF grant 200020_159557, ERC grant no. 815943, NSERC grant RGPIN-2014-04634, the Vector Institute, and an Open Philantropy Project AI fellowship. We would like to thank Johannes Kirschner for valueable discussions.</p>
<h2>References</h2>
<p>Yasin Abbasi-Yadkori. Online learning of linearly parameterized control problems. PhD thesis, 2012.</p>
<p>Hany Abdelrahman, Felix Berkenkamp, and Andreas Krause. Bayesian optimization for maximum power point tracking in photovoltaic power plants. In 2016 European Control Conference (ECC), pages 2078-2083, 2016.</p>
<p>James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281-305, 2012.</p>
<p>Felix Berkenkamp, Andreas Krause, and Angela P. Schoellig. Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. arXiv:1602.04450 $[c s . R O], 2016$.</p>
<p>Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance reduction: a unified approach to Bayesian optimization and level-set estimation. In Advances in Neural Information Processing Systems 29, pages 1507-1515. Curran Associates, Inc., 2016.</p>
<p>Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv:1012.2599 [cs], 2010.</p>
<p>Adam D. Bull. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning Research, 12(Oct):2879-2904, 2011.</p>
<p>Roberto Calandra, André Seyfarth, Jan Peters, and Marc Peter Deisenroth. An experimental comparison of Bayesian optimization for bipedal locomotion. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 1951-1958, 2014.</p>
<p>Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 844-853. PMLR, 2017.</p>
<p>Andreas Christmann and Ingo Steinwart. Support Vector Machines. Information Science and Statistics. Springer, New York, NY, 2008.</p>
<p>Josip Djolonga, Andreas Krause, and Volkan Cevher. High-dimensional Gaussian process bandits. In Advances in Neural Information Processing Systems 26, pages 1025-1033, 2013 .</p>
<p>Audrey Durand, Odalric-Ambrym Maillard, and Joelle Pineau. Streaming kernel regression with provably adaptive mean, variance, and regularization. Journal of Machine Learning Research, 19(17):1-34, 2018.</p>
<p>David K. Duvenaud, Hannes Nickisch, and Carl Edward Rasmussen. Additive Gaussian processes. In Advances in Neural Information Processing Systems 24, pages 226-234, 2011 .</p>
<p>Peter Frazier, Warren Powell, and Savas Dayanik. The knowledge-gradient policy for correlated normal beliefs. INFORMS Journal on Computing, 21(4):599-613, 2009.</p>
<p>Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. Google vizier: a service for black-box optimization. In Proc. of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17, pages 1487-1495, New York, NY, USA, 2017. ACM.
I. S. Gradshteïn, I. M. Ryzhik, and Alan Jeffrey. Table of integrals, series, and products. Academic Press, Amsterdam, Boston, 7th ed edition, 2007.</p>
<p>Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13(1):1809-1837, 2012.</p>
<p>José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 918-926. Curran Associates, Inc., 2014.</p>
<p>Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K. Sriperumbudur. Gaussian processes and kernel methods: a review on connections and equivalences. arXiv:1807.02582 [stat.ML], 2018.</p>
<p>Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with heteroscedastic noise. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 358-384. PMLR, 2018.</p>
<p>Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Bayesian neural network for predicting learning curves. In NIPS 2016 Bayesian Neural Network Workshop, 2016 .</p>
<p>Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.</p>
<p>Daniel J. Lizotte, Tao Wang, Michael H. Bowling, and Dale Schuurmans. Automatic gait optimization with Gaussian process regression. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI), volume 7, pages 944-949, 2007 .</p>
<p>Jonas Mockus. Bayesian approach to global optimization: theory and applications. Springer Science \&amp; Business Media, 2012.</p>
<p>Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2:117-129, 1978.</p>
<p>Carl Edward Rasmussen and Christopher K.I Williams. Gaussian processes for machine learning. MIT Press, Cambridge MA, 2006.</p>
<p>Binxin Ru, Michael A. Osborne, Mark Mcleod, and Diego Granziol. Fast informationtheoretic Bayesian optimisation. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4384-4392. PMLR, 2018.</p>
<p>Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1583-1591. Curran Associates, Inc., 2014.
M. W. Seeger, S. M. Kakade, and D. P. Foster. Information consistency of nonparametric Gaussian process methods. IEEE Transactions on Information Theory, 54(5):2376-2382, 2008 .</p>
<p>Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 29512959. Curran Associates, Inc., 2012.</p>            </div>
        </div>

    </div>
</body>
</html>
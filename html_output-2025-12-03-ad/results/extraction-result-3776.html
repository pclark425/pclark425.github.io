<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3776 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3776</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3776</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-35631fd55c2545615811fa8072015356ac8198e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/35631fd55c2545615811fa8072015356ac8198e7" target="_blank">LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities</a></p>
                <p><strong>Paper Venue:</strong> World wide web (Bussum)</p>
                <p><strong>Paper TL;DR:</strong> An exhaustive quantitative and qualitative evaluation of Large Language Models for Knowledge Graph (KG) construction and reasoning suggests that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors.</p>
                <p><strong>Paper Abstract:</strong> This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3776.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3776.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (SciERC IE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (applied to SciERC entity/relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used in this paper as a zero-shot and one-shot information extractor to perform entity and relation extraction on SciERC (a scientific-domain dataset), showing some extraction ability on scientific text but underperforming fully supervised systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4, a large autoregressive transformer language model (exact architecture and size not specified in this paper); used via interactive interface in zero-shot and one-shot prompting settings.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scientific (SciERC: scientific publications / scientific-domain abstracts used for knowledge-graph construction)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured entity–relation triples for knowledge-graph construction (i.e., extraction of factual relations from scientific text) — not explicit extraction of high-level scientific laws in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting (zero-shot and one-shot in-context demonstration prompts); instruction-driven extraction using the LLM's internal knowledge and context understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative comparison vs. fully supervised SOTA on benchmark dataset using micro F1 for entity/relation extraction (results reported in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 obtained low absolute F1 on SciERC (reported: 7.2 zero-shot, 9.1 one-shot) — better than smaller LLM variants but substantially below fine-tuned supervised SOTA; demonstrates some ability to extract structured relations from scientific text but limited by domain specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poorer performance on specialized scientific terminology and domain-specific datasets; sensitivity to prompt/instruction quality; dataset noise/label ambiguity; knowledge-cutoff and pretraining bias; the experiments extract triples rather than explicit qualitative scientific laws or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3776.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3776.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VINE (Virtual Knowledge Extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VINE: Virtual Knowledge Extraction dataset / task (introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic task/dataset created to evaluate whether LLMs can learn and extract previously unseen (fabricated) entity–relation triples from text, thereby probing generalization vs. memorization; GPT-4 extracted a high fraction of the virtual triples in the small test used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with comparison to ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (primary) and ChatGPT (comparative) used interactively with few-shot demonstrations; models benefit from instruction tuning and RLHF as noted in the paper, but no additional fine-tuning was performed for VINE.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Synthetic / constructed sentences (VINE created by replacing real entities/relations in Re-TACRED sentences with fabricated entities/relations to ensure novelty relative to model pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Evaluation reported on a small random sample (10 sentences covering all relations); full VINE construction based on Re-TACRED test set but exact size unspecified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Virtual knowledge triples (fabricated entity–relation triples) — a proxy test for the model's ability to acquire and extract novel relation patterns; not direct extraction of scientific laws, but tests for rule-like relation extraction generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting with demonstrations (paper used two demonstrations per relation in the shown experiment); instruction-based in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Extraction accuracy on the sampled test sentences; direct comparison of extraction success rates between GPT-4 and ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>On the small evaluation, GPT-4 successfully extracted 80% of virtual triples after learning two demonstrations, while ChatGPT achieved 27%, suggesting GPT-4 can rapidly learn to extract novel structured relations from instructions rather than relying solely on memorized facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Very small evaluation sample (10 sentences) limits statistical confidence; synthetic nature of VINE may not map to discovery of real scientific principles; possibility of subtle leakage from pretraining (mitigated by fabricating tokens but not eliminable); task focuses on triple extraction rather than induction of higher-level laws or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3776.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3776.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoKG (multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoKG: Autonomous Knowledge Graph construction and reasoning via multi-agent communication</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AutoKG is a proposed multi-agent framework that uses role-based LLM agents (e.g., Consultant, KG user) plus a web-searcher to construct and reason over knowledge graphs by iteratively querying LLMs and external sources; demonstrated qualitatively in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-agent LLM system (GPT-4 and ChatGPT used as role-based agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A communicative-agent architecture using off-the-shelf LLMs (GPT-4, ChatGPT) in role-playing configurations (based on CAMEL role-playing); agents exchange natural-language messages and can call a web-searcher retrieval component to augment with external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Multidomain; demonstration provided for a film-domain KG (Spider-Man: Across the Spider-Verse 2023); proposed applicability to specialized domains including scientific literature but not experimentally validated on large scholarly corpora in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Inductive construction of structured knowledge (knowledge-graph triples and subgraphs) enabling downstream reasoning and potentially supporting induction of rules; the paper does not report direct induction of explicit scientific laws or principles.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Multi-agent iterative prompting (role-based dialogues) combined with retrieval-augmentation (web searcher) to gather factual evidence and iteratively refine KG construction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative demonstration and example-based illustration (no formal quantitative evaluation); outputs require manual validation and comparison to standard answers or expert review as recommended by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>AutoKG (with multi-agent LLMs and web augmentation) produced more effective and comprehensive knowledge graphs in the demonstration example than single-agent approaches; framework shows promise for integrating LLM generalization with external evidence for KG construction and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Practical issues reported: API/token limits constrain scalability; hallucination risk from LLMs requiring human or algorithmic vetting; limited autonomy (human oversight still necessary); no quantitative validation on extracting higher-level scientific rules from large scholarly corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. <em>(Rating: 2)</em></li>
                <li>Zero-shot information extraction via chatting with chatgpt. <em>(Rating: 2)</em></li>
                <li>Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. <em>(Rating: 2)</em></li>
                <li>Large language model is not a good few-shot information extractor, but a good reranker for hard samples! <em>(Rating: 2)</em></li>
                <li>Camel: Communicative agents for "mind" exploration of large language model society. <em>(Rating: 2)</em></li>
                <li>Interactive natural language processing. <em>(Rating: 1)</em></li>
                <li>Larger language models do in-context learning differently. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3776",
    "paper_id": "paper-35631fd55c2545615811fa8072015356ac8198e7",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "GPT-4 (SciERC IE)",
            "name_full": "Generative Pre-trained Transformer 4 (applied to SciERC entity/relation extraction)",
            "brief_description": "GPT-4 is used in this paper as a zero-shot and one-shot information extractor to perform entity and relation extraction on SciERC (a scientific-domain dataset), showing some extraction ability on scientific text but underperforming fully supervised systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4, a large autoregressive transformer language model (exact architecture and size not specified in this paper); used via interactive interface in zero-shot and one-shot prompting settings.",
            "input_domain": "Scientific (SciERC: scientific publications / scientific-domain abstracts used for knowledge-graph construction)",
            "corpus_size": null,
            "law_type": "Structured entity–relation triples for knowledge-graph construction (i.e., extraction of factual relations from scientific text) — not explicit extraction of high-level scientific laws in this paper.",
            "distillation_method": "Prompting (zero-shot and one-shot in-context demonstration prompts); instruction-driven extraction using the LLM's internal knowledge and context understanding.",
            "evaluation_method": "Quantitative comparison vs. fully supervised SOTA on benchmark dataset using micro F1 for entity/relation extraction (results reported in Table 1 of the paper).",
            "results_summary": "GPT-4 obtained low absolute F1 on SciERC (reported: 7.2 zero-shot, 9.1 one-shot) — better than smaller LLM variants but substantially below fine-tuned supervised SOTA; demonstrates some ability to extract structured relations from scientific text but limited by domain specificity.",
            "limitations_or_challenges": "Poorer performance on specialized scientific terminology and domain-specific datasets; sensitivity to prompt/instruction quality; dataset noise/label ambiguity; knowledge-cutoff and pretraining bias; the experiments extract triples rather than explicit qualitative scientific laws or principles.",
            "uuid": "e3776.0",
            "source_info": {
                "paper_title": "LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "VINE (Virtual Knowledge Extraction)",
            "name_full": "VINE: Virtual Knowledge Extraction dataset / task (introduced in this paper)",
            "brief_description": "A synthetic task/dataset created to evaluate whether LLMs can learn and extract previously unseen (fabricated) entity–relation triples from text, thereby probing generalization vs. memorization; GPT-4 extracted a high fraction of the virtual triples in the small test used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with comparison to ChatGPT)",
            "model_description": "GPT-4 (primary) and ChatGPT (comparative) used interactively with few-shot demonstrations; models benefit from instruction tuning and RLHF as noted in the paper, but no additional fine-tuning was performed for VINE.",
            "input_domain": "Synthetic / constructed sentences (VINE created by replacing real entities/relations in Re-TACRED sentences with fabricated entities/relations to ensure novelty relative to model pretraining).",
            "corpus_size": "Evaluation reported on a small random sample (10 sentences covering all relations); full VINE construction based on Re-TACRED test set but exact size unspecified in the paper.",
            "law_type": "Virtual knowledge triples (fabricated entity–relation triples) — a proxy test for the model's ability to acquire and extract novel relation patterns; not direct extraction of scientific laws, but tests for rule-like relation extraction generalization.",
            "distillation_method": "Few-shot prompting with demonstrations (paper used two demonstrations per relation in the shown experiment); instruction-based in-context learning.",
            "evaluation_method": "Extraction accuracy on the sampled test sentences; direct comparison of extraction success rates between GPT-4 and ChatGPT.",
            "results_summary": "On the small evaluation, GPT-4 successfully extracted 80% of virtual triples after learning two demonstrations, while ChatGPT achieved 27%, suggesting GPT-4 can rapidly learn to extract novel structured relations from instructions rather than relying solely on memorized facts.",
            "limitations_or_challenges": "Very small evaluation sample (10 sentences) limits statistical confidence; synthetic nature of VINE may not map to discovery of real scientific principles; possibility of subtle leakage from pretraining (mitigated by fabricating tokens but not eliminable); task focuses on triple extraction rather than induction of higher-level laws or principles.",
            "uuid": "e3776.1",
            "source_info": {
                "paper_title": "LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AutoKG (multi-agent)",
            "name_full": "AutoKG: Autonomous Knowledge Graph construction and reasoning via multi-agent communication",
            "brief_description": "AutoKG is a proposed multi-agent framework that uses role-based LLM agents (e.g., Consultant, KG user) plus a web-searcher to construct and reason over knowledge graphs by iteratively querying LLMs and external sources; demonstrated qualitatively in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-agent LLM system (GPT-4 and ChatGPT used as role-based agents)",
            "model_description": "A communicative-agent architecture using off-the-shelf LLMs (GPT-4, ChatGPT) in role-playing configurations (based on CAMEL role-playing); agents exchange natural-language messages and can call a web-searcher retrieval component to augment with external evidence.",
            "input_domain": "Multidomain; demonstration provided for a film-domain KG (Spider-Man: Across the Spider-Verse 2023); proposed applicability to specialized domains including scientific literature but not experimentally validated on large scholarly corpora in this paper.",
            "corpus_size": null,
            "law_type": "Inductive construction of structured knowledge (knowledge-graph triples and subgraphs) enabling downstream reasoning and potentially supporting induction of rules; the paper does not report direct induction of explicit scientific laws or principles.",
            "distillation_method": "Multi-agent iterative prompting (role-based dialogues) combined with retrieval-augmentation (web searcher) to gather factual evidence and iteratively refine KG construction.",
            "evaluation_method": "Qualitative demonstration and example-based illustration (no formal quantitative evaluation); outputs require manual validation and comparison to standard answers or expert review as recommended by the authors.",
            "results_summary": "AutoKG (with multi-agent LLMs and web augmentation) produced more effective and comprehensive knowledge graphs in the demonstration example than single-agent approaches; framework shows promise for integrating LLM generalization with external evidence for KG construction and reasoning.",
            "limitations_or_challenges": "Practical issues reported: API/token limits constrain scalability; hallucination risk from LLMs requiring human or algorithmic vetting; limited autonomy (human oversight still necessary); no quantitative validation on extracting higher-level scientific rules from large scholarly corpora.",
            "uuid": "e3776.2",
            "source_info": {
                "paper_title": "LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction.",
            "rating": 2
        },
        {
            "paper_title": "Zero-shot information extraction via chatting with chatgpt.",
            "rating": 2
        },
        {
            "paper_title": "Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness.",
            "rating": 2
        },
        {
            "paper_title": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples!",
            "rating": 2
        },
        {
            "paper_title": "Camel: Communicative agents for \"mind\" exploration of large language model society.",
            "rating": 2
        },
        {
            "paper_title": "Interactive natural language processing.",
            "rating": 1
        },
        {
            "paper_title": "Larger language models do in-context learning differently.",
            "rating": 1
        }
    ],
    "cost": 0.0135275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities</h1>
<p>Yuqi Zhu ${ }^{1,2 \dagger}$, Xiaohan Wang ${ }^{1,2 \dagger}$, Jing Chen ${ }^{1,2 \dagger}$, Shuofei Qiao ${ }^{1,2}$,<br>Yixin $\mathrm{Ou}^{1,2}$, Yunzhi Yao ${ }^{1,2}$, Shumin Deng ${ }^{3}$, Huajun Chen ${ }^{1,2}$,<br>Ningyu Zhang ${ }^{1,2^{<em>}}$<br>${ }^{1 </em>}$ Zhejiang University, China.<br>${ }^{2}$ ZJU-Ant Group Joint Research Center for Knowledge Graphs, China.<br>${ }^{3}$ National University of Singapore, NUS-NCS Joint Lab, Singapore.</p>
<ul>
<li>Corresponding author(s). E-mail(s): zhangningyu@zju.edu.cn; Contributing authors: zhuyuqi@zju.edu.cn;
${ }^{\dagger}$ These authors contributed equally to this work.</li>
</ul>
<h4>Abstract</h4>
<p>This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.</p>
<p>Keywords: Knowledge Graph, Information Extraction, GPT-4, Large Language Model</p>
<h1>1 Introduction</h1>
<p>Knowledge Graph (KG) is a semantic network comprising entities, concepts, and relations [1-6], which can catalyse applications across various scenarios. Constructing KGs $[7,8]$ typically involves multiple tasks such as Named Entity Recognition (NER) [9, 10], Relation Extraction (RE) [11, 12], Event Extraction (EE) [13, 14], and Entity Linking (EL) [15]. Additionally, Link Prediction (LP) [16, 17] is a crucial step for KG reasoning, essential for understanding constructed KGs. These KGs also hold a central position in Question Answering (QA) tasks [18, 19], especially in conducting inference based on question context, involving the construction and application of relation subgraphs. This paper empirically investigates the potential applicability of LLMs in the KG domain, taking ChatGPT and GPT-4 [20] as examples. The research begins with an examination of the fundamental capabilities of LLMs [21-24], progressing to explore possible future developments, aiming to enhance our understanding of LLMs and introduce new perspectives and methods to the field of knowledge graphs.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 The overview of our work. There are three main components: 1) Basic Evaluation: detailing our assessment of large models (text-davinci-003, ChatGPT, and GPT-4), in both zero-shot and one-shot settings, using performance from fully supervised state-of-the-art models as benchmarks; 2) Virtual Knowledge Extraction: an examination of LLMs' virtual knowledge capabilities on the constructed VINE dataset; and 3) Automatic $\boldsymbol{K} \boldsymbol{G}$ : the proposal of utilizing multiple agents to facilitate the construction and reasoning of KGs.</p>
<p>Recent Capabilities. Entity and Relation Extraction, along with Event Extraction, are pivotal for Knowledge Graph (KG) construction tasks [25-28]. They play a critical role in organizing vast amounts of entity, relation, and event data into structured representations, forming the foundational elements that underpin the construction and enrichment of KGs. Meanwhile, Link Prediction, as a core task of KG reasoning [29], aims to uncover latent relationships between entities, thereby enriching the knowledge graph. Additionally, we delve into the utilization of LLMs in knowledgebased Question Answering tasks [30,31] to gain a thorough insight into their reasoning capabilities. Given these considerations, we select these tasks as representatives for evaluating both the construction and reasoning of KGs. As illustrated in Figure 1, our initial investigation targets the zero-shot and one-shot abilities of large language models across the aforementioned tasks. This analysis serves to assess the potential usage of such models in the field of knowledge graphs. The empirical findings reveal that LLMs like GPT-4 exhibit limited effectiveness as a few-shot information extractor, yet demonstrate considerable proficiency as an inference assistant.</p>
<p>Generalizability Analysis. To delve deeper into the behavior of LLMs in information extraction tasks, we devise a unique task termed Virtual Knowledge Extraction, targeting LLMs' ability to generalize and extract unfamiliar knowledge. This undertaking aims to discern whether the observed performance enhancements on these tasks are attributed to the extensive internal knowledge repositories of LLMs or to their potent generalization capabilities facilitated by instruction tuning [32] and Reinforcement Learning from Human Feedback (RLHF) [33]. And our experiments on a newly constructed dataset, VINE, indicate that large language models like GPT-4 can acquire new knowledge from instructions and effectively execute extraction tasks, thereby affording a more nuanced understanding of large models to a certain extent.</p>
<p>Future Opportunities. In light of the preceding experiments, we further examine prospective directions for knowledge graphs. Given the remarkable generalization capabilities of large models [34,35], we opt to employ them to aid in the construction of KG. Compared to smaller models, these LLMs mitigate potential resource wastage and demonstrate notable adaptability in novel or data-scarce situations. However, it's important to recognize their strong dependence on prompt engineering [36] and the inherent limitations of their knowledge cutoff. Consequently, researchers are exploring interactive mechanisms that allow LLMs to access and leverage external resources, aiming to enhance their performance further [37].</p>
<p>On this basis, we introduce the concept of AutoKG - autonomous KG construction and reasoning via multi-agent communication. In this framework, the human role is diminished, with multiple communicative agents each playing their respective roles. These agents interact with external sources, collaboratively accomplishing the task. We summarize our contributions as follows ${ }^{1}$ :</p>
<ul>
<li>We evaluate LLMs, including ChatGPT and GPT-4, offering an initial understanding of their capabilities by evaluating their zero-shot and one-shot performance on KG construction and reasoning on eight benchmark datasets.</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>We design a novel Virtual Knowledge Extraction task and construct the VINE dataset. By evaluating the performance of LLMs on it, we further demonstrate that LLMs such as GPT-4 possess strong generalization abilities.</li>
<li>We introduce the concept of automatic KG construction and reasoning, known as AutoKG. Leveraging LLMs’ inner knowledge, we enable multiple agents of LLMs to assist in the process through iterative dialogues, providing insights for future research.</li>
</ul>
<h1>2 Recent Capabilities of LLMs for KG Construction and Reasoning</h1>
<p>The release of large language models like GPT-4, recognized for their remarkable general capabilities, has been considered by researchers as the spark of artificial general intelligence (AGI) [38]. To facilitate an in-depth understanding of their performance in KG-related tasks, a series of evaluations are conducted. $\S 2.1$ introduces the evaluation principles, followed by a detailed analysis in $\S 2.2$ on the performance of LLMs in the construction and reasoning tasks, highlighting variations across different datasets and domains. Moreover, $\S 2.3$ delves into the reasons underlying the subpar performance of LLMs in certain tasks. And finally, $\S 2.4$ discusses whether the models' performance is genuinely indicative of generalization abilities or influenced by inherent advantages of the knowledge base.</p>
<h3>2.1 Evaluation Principle</h3>
<p>In this study, we conduct a comprehensive assessment of LLMs, represented by GPT4, and specifically analyze the performance disparities and enhancements between GPT-4 and other models in the GPT series, such as ChatGPT. A primary area of investigation is the models' performance in zero-shot and one-shot tasks, as these tasks illuminate the models' generalization capabilities under data-limited conditions. Given that some experiments in our study rely on randomly sampled subsets of datasets, it is important to note that there may be inherent variability in the results due to this sampling approach. We deliberately choose zero-shot and one-shot tasks over those requiring more examples, as they better test the models' adaptability and practical application in scenarios with sparse data. The experimental prompt used is detailed in Appendix D. Utilizing the evaluation results, our objective is to explore the reasons behind the models' exemplary performance in specific tasks and identify potential areas of improvement. Ultimately, our goal is to derive valuable insights for future advancements in such models.</p>
<h3>2.2 KG Construction and Reasoning</h3>
<h3>2.2.1 Settings</h3>
<p>Datasets. During the task of Entity and Relation Extraction, Event Extraction, we employ DuIE2.0 [39], SciERC [40], Re-TACRED [41], and MAVEN [42] datasets. For Link Prediction, we utilized FB15K-237 [43] and ATOMIC 2020 [44] datasets. Finally,</p>
<p>FreebaseQA [45] and MetaQA [16] datasets are used in the Question Answering task. The dataset used is described in detail in Appendix B.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2 Examples of ChatGPT and GPT-4 on the RE datasets. (1) Zero-shot on the SciERC dataset (2) Zero-shot on the Re-TACRED dataset (3) One-shot on the DuIE2.0 dataset</p>
<h1>2.2.2 Overall Results</h1>
<p>Entity and Relation Extraction. We conduct experiments on DuIE2.0, ReTACRED, and SciERC, each involving 20 samples in the test/valid sets, encompassing all types of relationships present within the datasets. Here we use PaddleNLP LIC2021 IE ${ }^{5}$, PL-Marker [46] and EXOBRAIN [47] as baselines on each dataset, respectively. Concurrently, for evaluation purposes, the results are reported utilizing the standard micro F1 score. As shown in Table 1, GPT-4 performs relatively well in both zero-shot and one-shot manners compared to ChatGPT, even though its performance has not yet surpassed that of fully supervised small models.</p>
<ul>
<li>Zero-shot GPT-4's zero-shot performance significantly improves across all tested datasets, especially in DuIE2.0, scoring 31.03, compared to ChatGPT's 10.3. Specifically, in the example of Re-TACRED in Figure 2, ChatGPT fails to extract the target triple, possibly due to the close proximity of head and tail entities</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and the ambiguity of predicates. In contrast, GPT-4 gives the correct answer " org:alternate_names", highlighting its superior language comprehension.</p>
<ul>
<li>One-shot Simultaneously, the optimization of text instructions has been shown to enhance the performance of LLMs. In the context of DuIE2.0 shown in Figure 2, GPT-4 discerns an implicit relation from a statement about George Wilcombe's association with the Honduras national team. This precision is attributed to GPT-4's extensive knowledge base, which facilitates the inference of George Wilcombe's nationality. However, it is also observed that GPT-4 encounters challenges with complex sentences, with factors such as prompt quality and relational ambiguity affecting the outcomes.
<img alt="img-2.jpeg" src="img-2.jpeg" /></li>
</ul>
<p>Fig. 3 Here are examples of task Event Extraction, Link Prediction and Question Answering.</p>
<p>Event Extraction. For simplification, we conduct event detection experiments on 20 random samples from MAVEN, encompassing all event types. Using the Fscore metric, GPT-4's performance is benchmarked against the existing state-of-theart (SOTA) [48] model, as well as to other models in the GPT family. Based on our results, GPT-4 shows inconsistent superiority over the SOTA, with both GPT-4 and ChatGPT outperforming each other in different scenarios.</p>
<ul>
<li>Zero-shot As shown in Table 1, GPT-4 outperforms ChatGPT. For the sentence "Now an established member of the line-up, he agreed to sing it more often.", ChatGPT generates the result Becoming_a_member, while GPT-4 identifies two more: Agree_or_refuse_to_act, Performing. It is worth noting that in this experiment, ChatGPT frequently provides answers with a single event type. In contrast, GPT-4's ability to grasp complex contextual information enables it to identify multiple event types within these sentences.</li>
<li>One-shot In this configuration, ChatGPT's performance improves notably, while GPT-4 experiences a slight decline. Figure 3 illustrates that GPT-4 incorrectly identifies five event types where the correct answers are Process_end and Come_together. Despite detecting underlying ranking and comparison information, GPT-4 misses the</li>
</ul>
<p>trigger words final and host. Simultaneously, we observe that under one-shot setup, GPT-4 tends to produce a higher number of erroneous responses when it is unable to identify the correct ones. We theorize this could stem from implicit type indications of the dataset.</p>
<p>Table 1 KG Construction and KG Reasoning tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Knowledge Graph Construction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Knowledge Graph Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DuIE2.0</td>
<td style="text-align: center;">Re-TACRED</td>
<td style="text-align: center;">SciERC</td>
<td style="text-align: center;">MAVEN</td>
<td style="text-align: center;">FB15K-237</td>
<td style="text-align: center;">ATOMIC2020</td>
<td style="text-align: center;">FreebaseQA</td>
<td style="text-align: center;">MetaQA</td>
</tr>
<tr>
<td style="text-align: center;">Fine-Tuned SOTA</td>
<td style="text-align: center;">69.42</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">11.43</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">10.26</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">31.03</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;">One-shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">30.63</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">25.86</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">41.91</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">56.0</td>
</tr>
</tbody>
</table>
<p>Link Prediction. Task link prediction involves experiments on two distinct datasets, FB15k-237 and ATOMIC2020. The former is a random sample set comprising 25 instances, whereas the latter encompasses 23 instances on behalf of all possible relations. Among various approaches, the best performing fine-tuned models are C-LMKE (BERT-base) [49] and COMET (BART) [50] for each.</p>
<ul>
<li>Zero-shot In Table 1, GPT-4 on the FB15k-237 demonstrates that its hits@1 score is nearing the SOTA level. Regarding the ATOMIC2020, while GPT-4 still exceeds the other two models, there remains a considerable discrepancy in terms of bleu1 score between GPT-4's performance and the fine-tuned SOTA achieved. In the zero-shot context, it is observable that ChatGPT often refrains from providing immediate answers when faced with link prediction ambiguity, opting instead to seek further contextual data. This cautious approach contrasts with GPT-4's propensity to offer direct responses, suggesting possible differences in their reasoning and decision-making strategies.</li>
<li>One-shot Instructional text optimization has proven beneficial in enhancing GPT series' performance in link prediction tasks. Empirical evaluations demonstrate one-shot GPT-4 improves results on both datasets, supporting accurate tail entity prediction in triples. In the example of Figure 3, the target [MASK] is Primetime Emmy Award. In zero-shot setting, GPT-4 fails to comprehend the relation, leading to an incorrect response Comedy Series. However, when the demonstration is incorporated, GPT-4 successfully identifies the target.</li>
</ul>
<p>Question Answering. We conduct the evaluation using two prevalent Knowledge Base Question Answering datasets, FreebaseQA and MetaQA, with 20 random instances sampled from each. In MetaQA, we sample proportional to their dataset representation. Yu et al. [51] and Madani and Joseph [52] represent the SOTA models employed. And for both datasets, AnswerExactMatch is adopted as the metric of evaluation.</p>
<ul>
<li>Zero-shot As shown in Table 1, ChatGPT and GPT-4 demonstrate identical performance on FreebaseQA, surpassing preceding fully supervised SOTA by $16 \%$. Yet, no advantage of GPT-4 over ChatGPT is observed. For MetaQA, there is still a large gap between LLMs and supervised SOTA, possibly due to multi-answer questions and LLM input token constraints. Nevertheless, GPT-4 outperforms ChatGPT by 11.1 points, which indicates the superiority of GPT-4 against ChatGPT on more challenging QA tasks. Specifically, in the example of Figure 3, GPT-4 correctly answers a multi-hop question from MetaQA, yielding both 1999 and 1974 release dates, highlighting its superior performance in multi-hop QA tasks over ChatGPT.</li>
<li>One-shot We also conduct experiments under one-shot setting by randomly sampling one example from the train set as the in-context exemplar. Results in Table 1 demonstrate that only text-davinci-003 benefits from the prompt, while both ChatGPT and GPT-4 encounter a performance drop. This can be attributed to the notorious alignment tax where models sacrifice some of their in-context learning ability for aligning with human feedback.</li>
</ul>
<h1>2.2.3 KG Construction vs. Reasoning</h1>
<p>Our experiments on KG construction and reasoning reveal that LLMs exhibit superior reasoning skills compared to their construction capabilities. Given the challenge of quantifying reasoning and construction abilities, we assess the comparative capabilities of LLMs in these tasks by measuring the performance differential between LLMs and the current SOTA methodologies. Larger performance disparities indicate poorer performance. Despite the exemplary performance of LLMs, they do not surpass the current state-of-the-art models in KG construction under zero-shot and one-shot settings, indicating limitations in extracting information from sparse data. Conversely, all LLMs in one-shot, and GPT-4 in zero-shot, match or near SOTA performance on the FreebaseQA and FB15K-237 datasets. Moreover, they exhibit relatively good performance across the remaining datasets, which underscores their adaptability in KG reasoning tasks as well. The intrinsic complexity of KG construction tasks may account for this discrepancy in performance. Furthermore, the robust reasoning performance of LLMs might be attributed to their exposure to relevant knowledge during pre-training.</p>
<h3>2.2.4 General vs. Specific Domain</h3>
<p>In our study, we evaluate the performance of large language models, exemplified by GPT-4, across diverse knowledge domains, ensuring a balanced assessment in both generic and specialized contexts. We employed a consistent method of evaluating relative task capabilities, similar to the performance disparity assessment described in §2.2.3. The chosen benchmarks, SciERC and Re-TACRED, represent scientific and general domains, respectively. While Re-TACRED exhibits a broader range of relation types compared to the seven in SciERC, both GPT-4 and ChatGPT underperform on the specialized SciERC dataset, indicating their limitations in domain-specific data. Interestingly, GPT-4's performance boost on SciERC is less pronounced than on ReTACRED when given one demonstration. Specifically, during our experiments, we note challenges in LLMs' recognition and understanding of specialized terms within the</p>
<p>SciERC dataset. We hypothesize that the subpar performance on specialized datasets may stem from these models being predominantly trained on vast general corpora, thereby lacking sufficient domain-specific expertise.</p>
<h1>2.3 Discussion: Why LLMs do not present satisfactory performance on some tasks?</h1>
<p>Our experiments underscore GPT-4's ability to extract knowledge across diverse domains, albeit not surpassing the performance of fine-tuned models. This observation also aligns with findings from previous research [25, 53]. Our experiment, conducted in March-April 2023, uses an interactive interface rather than an API to evaluate the GPT models on a randomly selected subset of datasets.</p>
<p>Notably, in assessing large models across eight datasets, we identify that the outcomes may be subject to various factors. Dataset Quality: Using the KG construction task as an illustration, dataset noise could lead to ambiguities. Complex contexts and potential label inaccuracies may also negatively impact model evaluation. Instruction Quality: Model performance is notably influenced by the semantic depth of instructions. Finding optimal instructions through prompt engineering ${ }^{3}$ can enhance performance. An In-context Learning [54] approach with relevant samples can further improve outcomes. Evaluation Methods: Current methods may not be entirely apt for assessing the capabilities of large models like ChatGPT and GPT-4. Dataset labels may not capture all correct responses, and answers involving synonymous terms might not be accurately recognized.</p>
<h3>2.4 Discussion: Do LLMs have memorized knowledge or truly have the generalization ability?</h3>
<p>Leveraging insights from prior studies, it is apparent that large models are adept at swiftly extracting structured knowledge from minimal information. This observation raises a question regarding the origin of the performance advantage in LLMs: is it due to the substantial volume of textual data used in pre-training phases, enabling the models to acquire pertinent knowledge, or is it attributed to their robust inference and generalization capabilities?</p>
<p>To explore this, we design the Virtual Knowledge Extraction task, targeting LLMs' ability to generalize and extract unfamiliar knowledge. Unlike conventional benchmarks, the task focuses on evaluating how models perform when confronted with information they have not previously encountered, rather than relying solely on knowledge accumulated during pre-training. Existing datasets largely comprise entities familiar to LLMs, potentially sourced from their pre-training corpora, thereby possibly including relationships already encoded within these corpora during extraction tasks. Addressing these dataset constraints, we introduce VINE, a novel dataset specifically crafted for Virtual Knowledge Extraction.</p>
<p>In VINE, we fabricate entities and relations not found in reality, structuring them into knowledge triples. We then instruct the models to extract this synthetic knowledge, using the efficiency of this process as an indicator of LLMs' capacity to manage</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4 Prompts used in Virtual Knowledge Extraction. The blue box is the demonstration and the pink box is the corresponding answer.
virtual knowledge. It is worth noting that we construct VINE based on the test set of Re-TACRED. The primary idea behind this process is to replace existing entities and relations in the original dataset with unseen ones, thereby creating unique virtual knowledge scenarios.</p>
<h1>2.4.1 Data Collection</h1>
<p>Considering the vast training datasets of large models like GPT-4, it is challenging for us to find the knowledge that they do not recognize. Using GPT-4 data up to September 2021 as a basis, we select a portion of participants' responses from two competitions organized by the New York Times in 2022 and 2023 as part of our data sources.</p>
<p>However, due to the limited number of responses in the above contests and to enhance data source diversity, we also create new words by randomly generating letter sequences. This is accomplished by generating random sequences between 7 to 9 characters in length (including 26 letters of the alphabet and the symbol "..") and appending common noun suffixes at random to finalize the construction. More details could be found in Appendix C.</p>
<h3>2.4.2 Preliminary Results</h3>
<p>In our experiment, we conduct a random selection of ten sentences for evaluation, ensuring they encompass all relationships. We assess the performance of ChatGPT and GPT-4 on these test samples after learning two demonstrations of the same relation. Notably, GPT-4 successfully extracted $80 \%$ of the virtual triples, while the accuracy of ChatGPT is only $27 \%$.</p>
<p>In Figure 4, we provide large models with a triple composed of virtual relation types and virtual head and tail entities-[Schoolnogo, decidiaster, Reptance] and [Intranguish, decidiaster, Nugculous]-along with the respective demonstrations. The results demonstrate that GPT-4 effectively completed the extraction of the virtual triple. Consequently, we tentatively conclude that GPT-4 exhibits a relatively strong generalization ability and can rapidly acquire the capability to extract new knowledge through instructions, rather than relying solely on the memory of relevant knowledge. Related work [55] has also confirmed that large models possess an exceptionally strong generalization ability concerning instructions.</p>
<h1>3 Future Opportunities: Automatic KG Construction and Reasoning</h1>
<p>In contemplating the trajectory of Knowledge Graph, the pronounced merits of large language models become evident. They not only optimize resource utilization but also outperform smaller models in adaptability, especially in varied application domains and data-limited settings. Such strengths position them as primary tools for KG construction and reasoning. Yet, while the prowess of LLMs is impressive, researchers have identified certain limitations, such as misalignment with human preferences and the tendency for hallucinations. The efficacy of models like ChatGPT heavily leans on human engagement in dialogue generation. Further refining model responses necessitates intricate user task descriptions and enriched interaction contexts, a process that remains demanding and time-intensive in the development lifecycle. Consequently, there is a growing interest in the realm of interactive natural language processing (iNLP) [37]. In parallel, research efforts concerning intelligent agents continue to proliferate [56-58]. A notable example of this advancement is AutoGPT ${ }^{4}$, which can independently generate prompts and carry out tasks such as event analysis, programming, and mathematical operations. Concurrently, Li et al. [59] delves into the potential for autonomous cooperation between communicative agents and introduces a novel cooperative agent framework called role-playing.</p>
<p>In light of our findings, we propose the use of communicative intelligent agents for KG construction, leveraging different roles assigned to multiple agents to collaborate on KG tasks based on their mutual knowledge. Considering the knowledge cutoff prevalent in large models during the pre-training phase, we suggest the incorporation of external sources to assist task completion. These sources can include knowledge bases, existing KGs, and internet retrieval systems, among others. Here we name this AutoKG.</p>
<p>For a simple demonstration of the concept, we utilize the role-playing method in CAMEL [59]. As depicted in Figure 5, we designate the $K G$ assistant agent as a Consultant and the $K G$ user agent as a $K G$ domain expert. Upon receipt of the prompt and assigned roles, the task-specifier agent provides an elaborate description to clarify the concept. Following this, the $K G$ assistant and $K G$ user collaborate in a multiparty setting to complete the specified task until the $K G$ user confirms its completion. Concurrently, a web searcher role is introduced to aid the $K G$ assistant in internet</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5 Illustration of AutoKG, that integrates KG construction and reasoning by employing GPT4 and communicative agents based on ChatGPT. The figure omits the specific operational process, providing the results directly.
knowledge retrieval. When the $K G$ assistant receives a dialogue from the $K G$ user, it initially consults the web searcher on whether to browse information online based on the content. Guided by the web searcher's response, the $K G$ assistant then continues to address the $K G$ user's command. The experimental example indicates that the knowledge graph related to the film Spider-Man: Across the Spider-Verse released in 2023 is more effectively and comprehensively constructed using the multi-agent and internet-augmented approach.</p>
<p>Remark. By combining the efforts of artificial intelligence and human expertise, AutoKG could speed up the creation of specialized KGs, fostering a collaborative environment with language models. This system leverages domain and internet knowledge to produce high-quality KGs, augmenting the factual accuracy of LLMs in domainspecific tasks, thereby increasing their practical utility. AutoKG not only simplifies the construction process but also improves LLMs' transparency, facilitating a deeper</p>
<p>understanding of their internal workings. As a cooperative human-machine platform, it bolsters the understanding and guidance of LLMs' decision-making, increasing their efficiency in complex tasks. However, it is noteworthy that despite the assistance of AutoKG, the current results of the constructed knowledge graph still necessitate manual evaluation and validation.</p>
<p>Furthermore, three significant challenges remain when utilizing AutoKG, necessitating further research and resolution: The utilization of the API is constrained by a maximum token limit. Currently, the gpt-3.5-turbo in use is subjected to a max token restriction. This constraint impacts the construction of KGs. AutoKG now exhibits shortcomings in facilitating efficient human-machine interaction. In fully autonomous machine operations, human oversight for immediate error correction is lacking, yet incorporating human involvement in every step will increase time and labor costs substantially. Hallucination problem of LLMs. Given the known propensity of LLMs to generate non-factual information, it's imperative to scrutinize outputs from them. This can be achieved via comparison with standard answers, expert review, or through semi-automatic algorithms.</p>
<h1>4 Conclusion and Future Work</h1>
<p>In this paper, we investigate LLMs for KG construction and reasoning. We question whether LLMs' extraction abilities arise from their vast pre-training corpus or their strong contextual learning capabilities. To investigate this, we conduct a Virtual Knowledge Extraction task using a novel dataset, with results highlighting the LLMs' robust contextual learning. Furthermore, we propose an innovative method of AutoKG for accomplishing KG construction and reasoning tasks by employing multiple agents. In the future, we would like to extend our work to other LLMs and explore additional KG-related tasks, such as multimodal reasoning.</p>
<h2>Declarations</h2>
<ul>
<li>Funding. This work was supported by the National Natural Science Foundation of China (No. 62206246, No. NSFCU23B2055, No. NSFCU19B2027), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent Introduction Programme (2021A-156-G), Tencent AI Lab Rhino-Bird Focused Research Program (RBFR2024003), Information Technology Center and State Key Lab of CAD\&amp;CG, Zhejiang University, and NUS-NCS Joint Laboratory (A-0008542-00-00).</li>
<li>Ethics approval and consent to participate. This work did not involve any human participants, their data, or biological materials, and therefore did not require ethical approval.</li>
<li>Data and Materials availability. Our data and materials are accessible in the repository here ${ }^{5}$.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>[1] Cai, B., Xiang, Y., Gao, L., Zhang, H., Li, Y., Li, J.: Temporal knowledge graph completion: A survey. In: Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pp. 6545-6553 (2023). https://doi.org/10.24963/IJCAI.2023/734
[2] Zhu, X., Li, Z., Wang, X., Jiang, X., Sun, P., Wang, X., Xiao, Y., Yuan, N.J.: Multi-modal knowledge graph construction and application: A survey. IEEE Trans. Knowl. Data Eng. 36(2), 715-735 (2024) https://doi.org/10.1109/TKDE. 2022.3224228
[3] Liang, K.Y., Meng, L., Liu, M., Liu, Y., Tu, W., Wang, S., Zhou, S., Liu, X., Sun, F.: A survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal. IEEE transactions on pattern analysis and machine intelligence PP (2022)
[4] Chen, X., Zhang, J., Wang, X., Wu, T., Deng, S., Wang, Y., Si, L., Chen, H., Zhang, N.: Continual multimodal knowledge graph construction. In: Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024 (2024)
[5] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X.: Unifying large language models and knowledge graphs: A roadmap. IEEE Trans. Knowl. Data Eng. 36(7), 3580-3599 (2024) https://doi.org/10.1109/TKDE.2024.3352100
[6] Pan, J.Z., Razniewski, S., Kalo, J., Singhania, S., Chen, J., Dietze, S., Jabeen, H., Omeliyanenko, J., Zhang, W., Lissandrini, M., Biswas, R., Melo, G., Bonifati, A., Vakaj, E., Dragoni, M., Graux, D.: Large language models and knowledge graphs: Opportunities and challenges. TGDK 1(1), 2-1238 (2023) https://doi. org/10.4230/.1.1.2
[7] Ye, H., Zhang, N., Chen, H., Chen, H.: Generative knowledge graph construction: A review. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 1-17 (2022). https://doi.org/10.18653/v1/2022. emnlp-main. 1
[8] Ding, L., Zhou, S., Xiao, J., Han, J.: Automated construction of theme-specific knowledge graphs. CoRR (2024) https://doi.org/10.48550/ARXIV.2404.19146
[9] Chiu, J.P.C., Nichols, E.: Named entity recognition with bidirectional lstm-cnns. Trans. Assoc. Comput. Linguistics 4, 357-370 (2016) https://doi.org/10.1162/ tacl_a_00104
[10] Gui, H., Yuan, L., Ye, H., Zhang, N., Sun, M., Liang, L., Chen, H.: Iepile:</p>
<p>Unearthing large-scale schema-based information extraction corpus. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (2024)
[11] Zeng, D., Liu, K., Chen, Y., Zhao, J.: Distant supervision for relation extraction via piecewise convolutional neural networks. In: Màrquez, L., Callison-Burch, C., Su, J., Pighin, D., Marton, Y. (eds.) Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1753-1762 (2015). https://doi.org/10.18653/ v1/d15-1203
[12] Chen, X., Zhang, N., Xie, X., Deng, S., Yao, Y., Tan, C., Huang, F., Si, L., Chen, H.: Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In: Laforest, F., Troncy, R., Simperl, E., Agarwal, D., Gionis, A., Herman, I., Médini, L. (eds.) WWW '22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pp. 2778-2788 (2022). https://doi.org/10.1145/3485447.3511998
[13] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 167-176 (2015). https://doi.org/10.3115/v1/p15-1017
[14] Deng, S., Zhang, N., Kang, J., Zhang, Y., Zhang, W., Chen, H.: Meta-learning with dynamic-memory-based prototypical network for few-shot event detection. In: Caverlee, J., Hu, X.B., Lalmas, M., Wang, W. (eds.) WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020, pp. 151-159 (2020). https://doi.org/10.1145/ 3336191.3371796
[15] Shen, W., Wang, J., Han, J.: Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Trans. Knowl. Data Eng. 27(2), 443-460 (2015) https://doi.org/10.1109/TKDE.2014.2327028
[16] Zhang, Y., Dai, H., Kozareva, Z., Smola, A.J., Song, L.: Variational reasoning for question answering with knowledge graph. In: McIlraith, S.A., Weinberger, K.Q. (eds.) Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 (2018). https://doi.org/10.1609/aaai.v32i1.12057
[17] Rossi, A., Barbosa, D., Firmani, D., Matinata, A., Merialdo, P.: Knowledge graph embedding for link prediction: A comparative analysis. ACM Trans. Knowl. Discov. Data 15(2), 14-11449 (2021) https://doi.org/10.1145/3424672</p>
<p>[18] Karpukhin, V., Oguz, B., Min, S., Lewis, P.S.H., Wu, L., Edunov, S., Chen, D., Yih, W.: Dense passage retrieval for open-domain question answering. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 (2020). https://doi.org/10.18653/v1/2020.emnlp-main. 550
[19] Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.: Retrieving and reading: A comprehensive survey on open-domain question answering. CoRR (2021)
[20] OpenAI: GPT-4 technical report. CoRR abs/2303.08774 (2023) https://doi. org/10.48550/arXiv. 2303.08774
[21] Liu, A., Hu, X., Wen, L., Yu, P.S.: A comprehensive evaluation of chatgpt's zeroshot text-to-sql capability. CoRR (2023) https://doi.org/10.48550/arXiv. 2303. 13547
[22] Shakarian, P., Koyyalamudi, A., Ngu, N., Mareedu, L.: An independent evaluation of chatgpt on mathematical word problems (MWP). In: Martin, A., Fill, H., Gerber, A., Hinkelmann, K., Lenat, D., Stolle, R., Harmelen, F. (eds.) Proceedings of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023), Hyatt Regency, San Francisco Airport, California, USA, March 27-29, 2023. CEUR Workshop Proceedings, vol. 3433 (2023)
[23] Lai, V.D., Ngo, N.T., Veyseh, A.P.B., Man, H., Dernoncourt, F., Bui, T., Nguyen, T.H.: Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023. findings-emnlp. 878
[24] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J., Wen, J.: A survey of large language models. CoRR (2023) https://doi.org/10.48550/arXiv. 2303.18223
[25] Wei, X., Cui, X., Cheng, N., Wang, X., Zhang, X., Huang, S., Xie, P., Xu, J., Chen, Y., Zhang, M., Jiang, Y., Han, W.: Zero-shot information extraction via chatting with chatgpt. CoRR abs/2302.10205 (2023) https://doi.org/10.48550/ arXiv. 2302.10205
[26] Li, B., Fang, G., Yang, Y., Wang, Q., Ye, W., Zhao, W., Zhang, S.: Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. CoRR (2023)
[27] Li, G., Wang, P., Ke, W.: Revisiting large language models as zero-shot relation extractors. In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association</p>
<p>for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023.findings-emnlp. 459
[28] Wan, Z., Cheng, F., Mao, Z., Liu, Q., Song, H., Li, J., Kurohashi, S.: GPTRE: in-context learning for relation extraction using large language models. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023.emnlp-main. 214
[29] Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., Yang, D.: Is chatgpt a general-purpose natural language processing task solver? In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023.emnlp-main. 85
[30] Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y.: Evaluating the logical reasoning ability of chatgpt and GPT-4. CoRR (2023) https://doi.org/10.48550/ ARXIV. 2304.03439
[31] Jiang, J., Zhou, K., Zhao, W.X., Song, Y., Zhu, C., Zhu, H., Wen, J.: Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. CoRR (2024) https://doi.org/10.48550/ARXIV.2402.11163
[32] Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., Tay, Y., Zhou, D., Le, Q.V., Zoph, B., Wei, J., Roberts, A.: The flan collection: Designing data and methods for effective instruction tuning. In: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. Proceedings of Machine Learning Research (2023)
[33] Christiano, P.F., Leike, J., Brown, T.B., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences. In: Guyon, I., Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.) Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA (2017)
[34] Leiter, C., Zhang, R., Chen, Y., Belouadi, J., Larionov, D., Fresen, V., Eger, S.: Chatgpt: A meta-analysis after 2.5 months. CoRR (2023) https://doi.org/10. 48550/ARXIV. 2302.13795
[35] Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Zhong, S., Yin, B., Hu, X.B.: Harnessing the power of llms in practice: A survey on chatgpt and beyond. ACM Trans. Knowl. Discov. Data 18(6), 160-116032 (2024) https://doi.org/10. $1145 / 3649506$
[36] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.H., Le,</p>
<p>Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 (2022)
[37] Wang, Z., Zhang, G., Yang, K., Shi, N., Zhou, W., Hao, S., Xiong, G., Li, Y., Sim, M.Y., Chen, X., Zhu, Q., Yang, Z., Nik, A., Liu, Q., Lin, C., Wang, S., Liu, R., Chen, W., Xu, K., Liu, D., Guo, Y., Fu, J.: Interactive natural language processing. CoRR (2023) https://doi.org/10.48550/arXiv.2305.13246
[38] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S.M., Nori, H., Palangi, H., Ribeiro, M.T., Zhang, Y.: Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR (2023) https://doi.org/10.48550/ARXIV.2303.12712
[39] Li, S., He, W., Shi, Y., Jiang, W., Liang, H., Jiang, Y., Zhang, Y., Lyu, Y., Zhu, Y.: Duie: A large-scale chinese dataset for information extraction. In: Tang, J., Kan, M., Zhao, D., Li, S., Zan, H. (eds.) Natural Language Processing and Chinese Computing - 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9-14, 2019, Proceedings, Part II. Lecture Notes in Computer Science, vol. 11839, pp. 791-800 (2019). https://doi.org/10.1007/978-3-030-32236-6_72
[40] Luan, Y., He, L., Ostendorf, M., Hajishirzi, H.: Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In: Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (eds.) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 3219-3232 (2018). https: //doi.org/10.18653/v1/d18-1360
[41] Stoica, G., Platanios, E.A., Póczos, B.: Re-tacred: Addressing shortcomings of the TACRED dataset. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. $13843-13850(2021)$
[42] Wang, X., Wang, Z., Han, X., Jiang, W., Han, R., Liu, Z., Li, J., Li, P., Lin, Y., Zhou, J.: MAVEN: A massive general domain event detection dataset. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 1652-1671 (2020). https://doi.org/10.18653/v1/2020. emnlp-main. 129
[43] Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Representing text for joint embedding of text and knowledge bases. In: Márquez, L., Callison-Burch, C., Su, J., Pighin, D., Marton, Y. (eds.) Proceedings of</p>
<p>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1499-1509 (2015). https://doi.org/10.18653/v1/d15-1174
[44] Hwang, J.D., Bhagavatula, C., Bras, R.L., Da, J., Sakaguchi, K., Bosselut, A., Choi, Y.: (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 6384-6392 (2021)
[45] Jiang, K., Wu, D., Jiang, H.: Freebaseqa: A new factoid QA data set matching trivia-style question-answer pairs with freebase. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 318-323 (2019). https://doi.org/10.18653/ v1/n19-1028
[46] Ye, D., Lin, Y., Li, P., Sun, M.: Packed levitated marker for entity and relation extraction. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 4904-4917 (2022). https://doi.org/10.18653/v1/2022.acl-long. 337
[47] Park, S., Kim, H.: Improving sentence-level relation extraction through curriculum learning. CoRR (2021)
[48] Wang, S., Yu, M., Huang, L.: The art of prompting: Event detection based on type specific prompts. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1286-1299 (2023). https://doi.org/10.18653/v1/2023.acl-short. 111
[49] Wang, X., He, Q., Liang, J., Xiao, Y.: Language models as knowledge embeddings. In: Raedt, L.D. (ed.) Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, pp. 2291-2297 (2022). https://doi.org/10.24963/ijcai.2022/318
[50] Hwang, J.D., Bhagavatula, C., Bras, R.L., Da, J., Sakaguchi, K., Bosselut, A., Choi, Y.: (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 6384-6392 (2021). https://doi.org/10.1609/aaai.v35i7.16792</p>
<p>[51] Yu, D., Zhang, S., Ng, P., Zhu, H., Li, A.H., Wang, J., Hu, Y., Wang, W.Y., Wang, Z., Xiang, B.: Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 (2023)
[52] Madani, N., Joseph, K.: Answering questions over knowledge graphs using logic programming along with language models. In: Maughan, K., Liu, R., Burns, T.F. (eds.) The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023 (2023)
[53] Gao, J., Zhao, H., Yu, C., Xu, R.: Exploring the feasibility of chatgpt for event extraction. CoRR (2023) https://doi.org/10.48550/arXiv.2303.03836
[54] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., Sui, Z.: A survey for in-context learning. CoRR (2023) https://doi.org/10.48550/ ARXIV. 2301.00234
[55] Wei, J.W., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., Ma, T.: Larger language models do in-context learning differently. CoRR (2023)
[56] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W.X., Wei, Z., Wen, J.: A survey on large language model based autonomous agents. Frontiers Comput. Sci. 18(6), 186345 (2024) https://doi.org/10.1007/S11704-024-40231-1
[57] Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong, L., Zhou, Y., Wang, W., Jiang, C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q., Qin, W., Zheng, Y., Qiu, X., Huan, X., Gui, T.: The rise and potential of large language model based agents: A survey. CoRR (2023) https://doi.org/10.48550/ arXiv. 2309.07864
[58] Zhao, P., Jin, Z., Cheng, N.: An in-depth survey of large language model-based artificial intelligence agents. CoRR (2023) https://doi.org/10.48550/arXiv. 2309. 14365
[59] Li, G., Hammoud, H.A.A.K., Itani, H., Khizbullin, D., Ghanem, B.: Camel: Communicative agents for "mind" exploration of large language model society. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)
[60] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell,</p>
<p>R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual (2020). https://proceedings.neurips. cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
[61] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models. Trans. Mach. Learn. Res. 2022 (2022)
[62] Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., Do, Q.V., Xu, Y., Fung, P.: A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In: Park, J.C., Arase, Y., Hu, B., Lu, W., Wijaya, D., Purwarianti, A., Krisnadhi, A.A. (eds.) Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1 - 4, 2023, pp. 675-718 (2023). https://doi.org/10.18653/v1/2023.ijcnlp-main. 45
[63] Nori, H., King, N., McKinney, S.M., Carignan, D., Horvitz, E.: Capabilities of GPT-4 on medical challenge problems. CoRR (2023) https://doi.org/10.48550/ ARXIV. 2303.13375
[64] Qiao, S., Ou, Y., Zhang, N., Chen, X., Yao, Y., Deng, S., Tan, C., Huang, F., Chen, H.: Reasoning with language model prompting: A survey. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 5368-5393 (2023). https://doi.org/ 10.18653/v1/2023.acl-long. 294
[65] Sánchez, R.J., Conrads, L., Welke, P., Cvejoski, K., Marin, C.O.: Hidden schema networks. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 4764-4798 (2023). https://doi.org/10.18653/v1/2023.acl-long. 263
[66] Ma, Y., Cao, Y., Hong, Y., Sun, A.: Large language model is not a good few-shot information extractor, but a good reranker for hard samples! In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 10572-10601 (2023). https: //doi.org/10.18653/v1/2023.findings-emnlp. 710
[67] Jeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., Stüber, A.T., Topalis, J., Weber, T., Wesp, P., Sabel, B.O., Ricke, J., Ingrisch, M.: Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports. CoRR (2022) https://doi.org/10.48550/arXiv.2212.14882</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The code and datasets are in https://github.com/zjunlp/AutoKG&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
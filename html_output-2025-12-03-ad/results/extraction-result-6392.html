<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-273233571</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.07985v1.pdf" target="_blank">Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6392.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6392.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source OpenAI model evaluated in this paper; achieved the best reported accuracy on the Omni-MATH Olympiad-level benchmark when using test-time enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; implied to be a large pre-trained model with strong reasoning abilities (closed-source) and likely trained on large web/code/text corpora and instruction-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH (Olympiad-level math benchmark); comparisons to GSM8K and MATH also reported</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problems (algebra, calculus, number theory, discrete math, geometry, applied math, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language olympiad-style problems with solutions; final answers typically boxed (free-response)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level; problems categorized into T1-T4 tiers (T4 = hardest; overall difficulty up to 10); this model evaluated across those tiers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Model prompts per repository guidelines; test-time scaling (Best-of-N / RM sampling) used for enhancement; evaluation judged by GPT-4o; in-context few-shot experiments performed with 4 demonstrations for some studies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (model-level exact-answer consistency as judged by GPT-4o/Omni-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>60.54% accuracy on Omni-MATH (best reported, using test-time enhancement)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Process-level assessment performed with GPT-4o splitting model solutions into steps; errors categorized into Logical, Accumulation, Calculation, Unrelated. OpenAI o1-mini showed fewer errors across categories relative to other evaluated models. The paper also analyzes in-context demonstration domain similarity and difficulty effects using GPT-4o as base for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Logical errors are the dominant error type in multi-step olympiad problems; accumulation and calculation errors also present but less frequent for this model. Best-of-N/test-time scaling improvements are limited and sometimes ineffective; no major data-leakage-driven improvements observed.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>This model (with test-time scaling) attains the highest accuracy reported in the paper, indicating that inference-time enhancements can help here; however, the paper reports that Best-of-N and reward-model based scaling do not consistently improve performance across all models, and gains plateau.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6392.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source OpenAI model evaluated on Omni-MATH; strong reasoning performance but behind o1-mini in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; treated as a strong general-purpose reasoning LLM (closed-source).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step mathematical problems (various domains)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language olympiad-style problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (T1-T4 tiers)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Prompts follow vendor guidelines; inference tokens limited to 4096 in experiments for o1-preview; evaluated with GPT-4o-based judgment; standard inference (vanilla) and some test-time scaling settings evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (GPT-4o evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>52.55% accuracy on Omni-MATH (reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Included in process-level step analysis via GPT-4o; error breakdown shows logical and calculation errors among common issues. Also part of data-leakage analysis using 5-gram matching; nearly no leakage for o1-preview according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Logical and calculation errors on step-level; limited improvements from scaling due to inference token constraints and reward model limitations; demonstrates difficulty with discrete mathematics compared to algebra/calculus.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Improves over many open-source vanilla models but remains well below perfect; increasing inference/token budget helps but has limits due to reward-model/policy-model interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6392.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (evaluation and baseline model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4o model is used both as a judged baseline and as the primary judgment/evaluation model for checking model outputs step-wise and for dataset labeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in detail in this paper; used as a strong evaluator and as a base for some labeling (difficulty/domain classification).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as evaluator for Omni-MATH; also evaluated on Omni-MATH as a solver</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Evaluation/judgment of multi-step olympiad math solutions; also solving olympiad problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Receives problem + reference answer + model-generated solution for judgment; inputs are text-only</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level; used to assign difficulty tags (1–10) to problems not in AoPS ratings</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-temperature deterministic prompts for labeling and evaluation (temperature=0, top_p=1, max tokens set), few-shot where necessary; few-shot GPT-4o-based evaluation prompt provided in Appendix; process-level step evaluation (split steps and assess correctness and error types)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy as a solver; evaluator consistency with human annotations (%), and consistency with Omni-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported solver accuracy ~30.49% (table entry); as an evaluator, GPT-4o achieved 98% agreement with human (gold) annotations on a 100-sample subset; used as gold for judging model answers.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used to perform process-level assessment: GPT-4o splits solutions into steps, classifies each step as Unrelated/Accumulation/Calculation/Logic/Other, and provides error explanations. Also used to classify difficulty and domains during dataset creation. Observed to produce more calculation and logical errors as a solver compared to math-specialized models, but very reliable as a judge.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>As a solver, shows calculation and logical errors on some olympiad problems; as an evaluator it is highly reliable (98% agreement with humans).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not directly extrapolated in the paper; GPT-4o used primarily as a robust evaluator rather than a best-performing solver in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6392.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-MATH-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-MATH-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-specialized Qwen2.5 family model with a 72B-size variant instruction-tuned for math tasks; evaluated in vanilla and test-time scaled settings in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-MATH-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only), instruction-tuned math-specialist</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Described as math-specialized instruction-finetuned model (reference to Qwen2.5-MATH technical report). Implied training includes math datasets and self-improvement procedures targeted at math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step problems across domains (algebra, number theory, discrete math, geometry, applied math etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language olympiad-style problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (T1-T4 tiers); evaluated across fine-grained difficulty buckets #D1-#D10</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla inference following repository instructions; test-time scaling experiments with Best-of-N / reward model re-ranking (RM@8, RM@256 reported), in-context few-shot experiments for domain/difficulty selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (GPT-4o-evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>36.20% accuracy in vanilla setting; test-time scaled variants reported (e.g., RM@8: 36.34%, RM@256: 35.95%)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Included in process-level step analysis showing nontrivial rates of unrelated generation for some models; data-leakage analysis via 5-gram matching indicates some contamination but low absolute numbers. Paper hypothesizes reward-model discriminative limits and policy-model search difficulty when applying Best-of-N methods to this model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tendency to produce unrelated content in some cases (relative to other models), logical and accumulation errors remain dominant; Best-of-N / reward re-ranking sometimes fails to improve or even inconsistently improves performance, possibly due to reward model discrimination parity with the strong policy model.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Test-time scaling (RM re-ranking / Best-of-N) yielded inconsistent improvements; small improvements observed on some Qwen2.5 variants but not reliably across settings, indicating non-monotonic scaling with inference-time sampling and re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6392.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-MATH-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-MATH-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B variant of the Qwen2.5 math-specialized instruction-tuned family, evaluated in vanilla and test-time scaled setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-MATH-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only), instruction-tuned math-specialist</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Math-focused instruction fine-tuning (paper references Qwen2.5 technical reports); trained to improve math reasoning via self-improvement routines per the referenced technical reports.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language problems (boxed final answers)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (T1-T4 tiers)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla prompts per repository; test-time scaling via RM@8 and RM@256 (Best-of-N re-ranking) applied in experiments; in-context demonstration experiments (4 same-domain examples) for selection studies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (GPT-4o-evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>33.22% accuracy in vanilla setting; RM@256 variant reported 35.79% (improvement observed for this smaller model in some re-ranking settings)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Part of domain/difficulty in-context experiments showing that domain-similar in-context examples and higher-difficulty examples can improve performance; included in data-leakage checks via 5-gram matching.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Logical errors dominate; reward-model re-ranking improved the 7B variant more than the 72B variant in the paper, suggesting reward-model / policy-model interactions vary with model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Smaller 7B variant sometimes benefited more from RM re-ranking than the larger 72B variant, showing non-monotonic scaling of test-time methods with base model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6392.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaLlama-3.1-70B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaLlama-3.1-70B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B Meta Llama 3.1 instruction-tuned model evaluated on Omni-MATH; one of the stronger open models in the comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaLlama-3.1-70B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only), instruction-tuned LLaMA-family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; referenced as an instruction-tuned large LLaMA-series model (general-purpose), likely trained on web and instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math tasks across domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language olympiad problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (T1-T4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Standard prompts per model repository; evaluated in vanilla mode; included in domain/difficulty in-context selection experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (GPT-4o-evaluated) and per-domain accuracy reported in table</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.16% overall accuracy reported in table (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Included in process-level error analysis showing distribution across logical/accumulation/calculation/unrelated categories; data-leakage and contamination analysis applied across models including this one.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Logical errors and accumulation errors common; weaker performance on discrete mathematics relative to algebra/number theory; in-context domain mismatch can harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>As a 70B model, shows better performance than many smaller open models but still substantially below the best closed-source models; difficulty-consistency trends improve with model capability generally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6392.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6392.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NuminaMATH-72B-COT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NuminaMATH-72B-COT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 72B model specialized for mathematics (NuminaMATH variant) using chain-of-thought or similar math-oriented training, evaluated on Omni-MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NuminaMATH-72B-COT</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only), math-specialized</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Presented as a math-specialized model (NuminaMATH family); paper references Numinamath technical report — implies math-focused training and chain-of-thought style fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step problems spanning algebra, calculus, number theory, discrete math, geometry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only natural language olympiad problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level; fine-grained difficulty buckets used in evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Vanilla inference; math-focused instruction/fine-tuning implied; evaluated using GPT-4o for judgment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (GPT-4o-evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>28.45% accuracy reported in table (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Included in step-level process analysis: logical errors common; compared per-domain performance (better in algebra/calculus/number theory, worse in discrete math).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Logical reasoning failures, accumulation errors; struggles on domain-generalization where training data is sparser (discrete math).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>72B math-specialized model performs better than many general-purpose open models but remains substantially below top closed-source test-time enhanced results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Qwen2.5-math technical report: Toward mathematical expert model via self-improvement <em>(Rating: 2)</em></li>
                <li>Qwen2 technical report <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Measuring the mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6392",
    "paper_id": "paper-273233571",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "OpenAI o1-mini",
            "name_full": "OpenAI o1-mini",
            "brief_description": "A closed-source OpenAI model evaluated in this paper; achieved the best reported accuracy on the Omni-MATH Olympiad-level benchmark when using test-time enhancements.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-mini",
            "model_family": "transformer (decoder-only, closed-source)",
            "model_size": null,
            "training_data_description": "Not specified in this paper; implied to be a large pre-trained model with strong reasoning abilities (closed-source) and likely trained on large web/code/text corpora and instruction-tuned.",
            "benchmark_name": "Omni-MATH (Olympiad-level math benchmark); comparisons to GSM8K and MATH also reported",
            "task_type": "Olympiad-level multi-step mathematical problems (algebra, calculus, number theory, discrete math, geometry, applied math, etc.)",
            "problem_format": "Text-only natural language olympiad-style problems with solutions; final answers typically boxed (free-response)",
            "difficulty_level": "Olympiad-level; problems categorized into T1-T4 tiers (T4 = hardest; overall difficulty up to 10); this model evaluated across those tiers",
            "prompting_method": "Model prompts per repository guidelines; test-time scaling (Best-of-N / RM sampling) used for enhancement; evaluation judged by GPT-4o; in-context few-shot experiments performed with 4 demonstrations for some studies",
            "performance_metric": "Accuracy (model-level exact-answer consistency as judged by GPT-4o/Omni-Judge)",
            "performance_value": "60.54% accuracy on Omni-MATH (best reported, using test-time enhancement)",
            "internal_analysis": "Process-level assessment performed with GPT-4o splitting model solutions into steps; errors categorized into Logical, Accumulation, Calculation, Unrelated. OpenAI o1-mini showed fewer errors across categories relative to other evaluated models. The paper also analyzes in-context demonstration domain similarity and difficulty effects using GPT-4o as base for experiments.",
            "failure_modes": "Logical errors are the dominant error type in multi-step olympiad problems; accumulation and calculation errors also present but less frequent for this model. Best-of-N/test-time scaling improvements are limited and sometimes ineffective; no major data-leakage-driven improvements observed.",
            "scaling_trend": "This model (with test-time scaling) attains the highest accuracy reported in the paper, indicating that inference-time enhancements can help here; however, the paper reports that Best-of-N and reward-model based scaling do not consistently improve performance across all models, and gains plateau.",
            "uuid": "e6392.0",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "OpenAI o1-preview",
            "name_full": "OpenAI o1-preview",
            "brief_description": "A closed-source OpenAI model evaluated on Omni-MATH; strong reasoning performance but behind o1-mini in this evaluation.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-preview",
            "model_family": "transformer (decoder-only, closed-source)",
            "model_size": null,
            "training_data_description": "Not specified in this paper; treated as a strong general-purpose reasoning LLM (closed-source).",
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step mathematical problems (various domains)",
            "problem_format": "Text-only natural language olympiad-style problems",
            "difficulty_level": "Olympiad-level (T1-T4 tiers)",
            "prompting_method": "Prompts follow vendor guidelines; inference tokens limited to 4096 in experiments for o1-preview; evaluated with GPT-4o-based judgment; standard inference (vanilla) and some test-time scaling settings evaluated",
            "performance_metric": "Accuracy (GPT-4o evaluation)",
            "performance_value": "52.55% accuracy on Omni-MATH (reported in paper)",
            "internal_analysis": "Included in process-level step analysis via GPT-4o; error breakdown shows logical and calculation errors among common issues. Also part of data-leakage analysis using 5-gram matching; nearly no leakage for o1-preview according to the paper.",
            "failure_modes": "Logical and calculation errors on step-level; limited improvements from scaling due to inference token constraints and reward model limitations; demonstrates difficulty with discrete mathematics compared to algebra/calculus.",
            "scaling_trend": "Improves over many open-source vanilla models but remains well below perfect; increasing inference/token budget helps but has limits due to reward-model/policy-model interactions.",
            "uuid": "e6392.1",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (evaluation and baseline model)",
            "brief_description": "OpenAI's GPT-4o model is used both as a judged baseline and as the primary judgment/evaluation model for checking model outputs step-wise and for dataset labeling tasks.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_family": "transformer (decoder-only, closed-source)",
            "model_size": null,
            "training_data_description": "Not specified in detail in this paper; used as a strong evaluator and as a base for some labeling (difficulty/domain classification).",
            "benchmark_name": "Used as evaluator for Omni-MATH; also evaluated on Omni-MATH as a solver",
            "task_type": "Evaluation/judgment of multi-step olympiad math solutions; also solving olympiad problems",
            "problem_format": "Receives problem + reference answer + model-generated solution for judgment; inputs are text-only",
            "difficulty_level": "Olympiad-level; used to assign difficulty tags (1–10) to problems not in AoPS ratings",
            "prompting_method": "Zero-temperature deterministic prompts for labeling and evaluation (temperature=0, top_p=1, max tokens set), few-shot where necessary; few-shot GPT-4o-based evaluation prompt provided in Appendix; process-level step evaluation (split steps and assess correctness and error types)",
            "performance_metric": "Accuracy as a solver; evaluator consistency with human annotations (%), and consistency with Omni-Judge",
            "performance_value": "Reported solver accuracy ~30.49% (table entry); as an evaluator, GPT-4o achieved 98% agreement with human (gold) annotations on a 100-sample subset; used as gold for judging model answers.",
            "internal_analysis": "Used to perform process-level assessment: GPT-4o splits solutions into steps, classifies each step as Unrelated/Accumulation/Calculation/Logic/Other, and provides error explanations. Also used to classify difficulty and domains during dataset creation. Observed to produce more calculation and logical errors as a solver compared to math-specialized models, but very reliable as a judge.",
            "failure_modes": "As a solver, shows calculation and logical errors on some olympiad problems; as an evaluator it is highly reliable (98% agreement with humans).",
            "scaling_trend": "Not directly extrapolated in the paper; GPT-4o used primarily as a robust evaluator rather than a best-performing solver in this benchmark.",
            "uuid": "e6392.2",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen2.5-MATH-72b-Instruct",
            "name_full": "Qwen2.5-MATH-72b-Instruct",
            "brief_description": "A math-specialized Qwen2.5 family model with a 72B-size variant instruction-tuned for math tasks; evaluated in vanilla and test-time scaled settings in the paper.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-MATH-72b-Instruct",
            "model_family": "transformer (decoder-only), instruction-tuned math-specialist",
            "model_size": "72B",
            "training_data_description": "Described as math-specialized instruction-finetuned model (reference to Qwen2.5-MATH technical report). Implied training includes math datasets and self-improvement procedures targeted at math reasoning.",
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step problems across domains (algebra, number theory, discrete math, geometry, applied math etc.)",
            "problem_format": "Text-only natural language olympiad-style problems",
            "difficulty_level": "Olympiad-level (T1-T4 tiers); evaluated across fine-grained difficulty buckets #D1-#D10",
            "prompting_method": "Vanilla inference following repository instructions; test-time scaling experiments with Best-of-N / reward model re-ranking (RM@8, RM@256 reported), in-context few-shot experiments for domain/difficulty selection",
            "performance_metric": "Accuracy (GPT-4o-evaluated)",
            "performance_value": "36.20% accuracy in vanilla setting; test-time scaled variants reported (e.g., RM@8: 36.34%, RM@256: 35.95%)",
            "internal_analysis": "Included in process-level step analysis showing nontrivial rates of unrelated generation for some models; data-leakage analysis via 5-gram matching indicates some contamination but low absolute numbers. Paper hypothesizes reward-model discriminative limits and policy-model search difficulty when applying Best-of-N methods to this model.",
            "failure_modes": "Tendency to produce unrelated content in some cases (relative to other models), logical and accumulation errors remain dominant; Best-of-N / reward re-ranking sometimes fails to improve or even inconsistently improves performance, possibly due to reward model discrimination parity with the strong policy model.",
            "scaling_trend": "Test-time scaling (RM re-ranking / Best-of-N) yielded inconsistent improvements; small improvements observed on some Qwen2.5 variants but not reliably across settings, indicating non-monotonic scaling with inference-time sampling and re-ranking.",
            "uuid": "e6392.3",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen2.5-MATH-7b-Instruct",
            "name_full": "Qwen2.5-MATH-7b-Instruct",
            "brief_description": "A 7B variant of the Qwen2.5 math-specialized instruction-tuned family, evaluated in vanilla and test-time scaled setups.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-MATH-7b-Instruct",
            "model_family": "transformer (decoder-only), instruction-tuned math-specialist",
            "model_size": "7B",
            "training_data_description": "Math-focused instruction fine-tuning (paper references Qwen2.5 technical reports); trained to improve math reasoning via self-improvement routines per the referenced technical reports.",
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step math problems",
            "problem_format": "Text-only natural language problems (boxed final answers)",
            "difficulty_level": "Olympiad-level (T1-T4 tiers)",
            "prompting_method": "Vanilla prompts per repository; test-time scaling via RM@8 and RM@256 (Best-of-N re-ranking) applied in experiments; in-context demonstration experiments (4 same-domain examples) for selection studies",
            "performance_metric": "Accuracy (GPT-4o-evaluated)",
            "performance_value": "33.22% accuracy in vanilla setting; RM@256 variant reported 35.79% (improvement observed for this smaller model in some re-ranking settings)",
            "internal_analysis": "Part of domain/difficulty in-context experiments showing that domain-similar in-context examples and higher-difficulty examples can improve performance; included in data-leakage checks via 5-gram matching.",
            "failure_modes": "Logical errors dominate; reward-model re-ranking improved the 7B variant more than the 72B variant in the paper, suggesting reward-model / policy-model interactions vary with model scale.",
            "scaling_trend": "Smaller 7B variant sometimes benefited more from RM re-ranking than the larger 72B variant, showing non-monotonic scaling of test-time methods with base model size.",
            "uuid": "e6392.4",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MetaLlama-3.1-70B-instruct",
            "name_full": "MetaLlama-3.1-70B-instruct",
            "brief_description": "A 70B Meta Llama 3.1 instruction-tuned model evaluated on Omni-MATH; one of the stronger open models in the comparisons.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "MetaLlama-3.1-70B-instruct",
            "model_family": "transformer (decoder-only), instruction-tuned LLaMA-family",
            "model_size": "70B",
            "training_data_description": "Not detailed in this paper; referenced as an instruction-tuned large LLaMA-series model (general-purpose), likely trained on web and instruction data.",
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step math tasks across domains",
            "problem_format": "Text-only natural language olympiad problems",
            "difficulty_level": "Olympiad-level (T1-T4)",
            "prompting_method": "Standard prompts per model repository; evaluated in vanilla mode; included in domain/difficulty in-context selection experiments",
            "performance_metric": "Accuracy (GPT-4o-evaluated) and per-domain accuracy reported in table",
            "performance_value": "24.16% overall accuracy reported in table (vanilla)",
            "internal_analysis": "Included in process-level error analysis showing distribution across logical/accumulation/calculation/unrelated categories; data-leakage and contamination analysis applied across models including this one.",
            "failure_modes": "Logical errors and accumulation errors common; weaker performance on discrete mathematics relative to algebra/number theory; in-context domain mismatch can harm performance.",
            "scaling_trend": "As a 70B model, shows better performance than many smaller open models but still substantially below the best closed-source models; difficulty-consistency trends improve with model capability generally.",
            "uuid": "e6392.5",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "NuminaMATH-72B-COT",
            "name_full": "NuminaMATH-72B-COT",
            "brief_description": "A 72B model specialized for mathematics (NuminaMATH variant) using chain-of-thought or similar math-oriented training, evaluated on Omni-MATH.",
            "citation_title": "OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS",
            "mention_or_use": "use",
            "model_name": "NuminaMATH-72B-COT",
            "model_family": "transformer (decoder-only), math-specialized",
            "model_size": "72B",
            "training_data_description": "Presented as a math-specialized model (NuminaMATH family); paper references Numinamath technical report — implies math-focused training and chain-of-thought style fine-tuning.",
            "benchmark_name": "Omni-MATH",
            "task_type": "Olympiad-level multi-step problems spanning algebra, calculus, number theory, discrete math, geometry",
            "problem_format": "Text-only natural language olympiad problems",
            "difficulty_level": "Olympiad-level; fine-grained difficulty buckets used in evaluation",
            "prompting_method": "Vanilla inference; math-focused instruction/fine-tuning implied; evaluated using GPT-4o for judgment",
            "performance_metric": "Accuracy (GPT-4o-evaluated)",
            "performance_value": "28.45% accuracy reported in table (vanilla)",
            "internal_analysis": "Included in step-level process analysis: logical errors common; compared per-domain performance (better in algebra/calculus/number theory, worse in discrete math).",
            "failure_modes": "Logical reasoning failures, accumulation errors; struggles on domain-generalization where training data is sparser (discrete math).",
            "scaling_trend": "72B math-specialized model performs better than many general-purpose open models but remains substantially below top closed-source test-time enhanced results.",
            "uuid": "e6392.6",
            "source_info": {
                "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Qwen2.5-math technical report: Toward mathematical expert model via self-improvement",
            "rating": 2,
            "sanitized_title": "qwen25math_technical_report_toward_mathematical_expert_model_via_selfimprovement"
        },
        {
            "paper_title": "Qwen2 technical report",
            "rating": 2,
            "sanitized_title": "qwen2_technical_report"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Measuring the mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_the_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        }
    ],
    "cost": 0.0159805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Omni-MATH OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS
11 Oct 2024</p>
<p>Bofei Gao 
Peking University</p>
<p>Feifan Song 
Peking University</p>
<p>Zhe Yang 
Peking University</p>
<p>Zefan Cai 
University of Wisconsin</p>
<p>Yibo Miao 
Shanghai Jiao Tong University</p>
<p>Qingxiu Dong 
Peking University</p>
<p>Lei Li 
The University of Hong
Kong 10 Zhongguancun Laboratory</p>
<p>Chenghao Ma 
Engineering Research Center of Information Networks</p>
<p>Liang Chen 
Peking University</p>
<p>Runxin Xu 
Peking University</p>
<p>Zhengyang Tang 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Benyou Wang 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Daoguang Zan 
Institute of Software
Chinese Academy of Sciences</p>
<p>Shanghaoran Quan 
Ge Zhang 
University of Waterloo</p>
<p>Sha Lei 
Yichang Zhang 
Xuancheng Ren 
Tianyu Liu 
Baobao Chang 
Peking University</p>
<p>-Madison 
Alibaba Group 
Github Repo 
Omni-MATH OMNI-MATH: A UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS
11 Oct 20244E73A0C7D57B63CF0E31470C6A216BAEarXiv:2410.07985v2[cs.CL]
Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities.However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models.To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level.Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation.These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning.Furthermore, we conducted an in-depth analysis based on this benchmark.Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) (OpenAI, 2023;Abhimanyu Dubey &amp; Abhishek Kadian, 2024;Yang et al., 2024a) have shown remarkable capabilities in producing human-like text (Abhimanyu Dubey &amp; Abhishek Kadian, 2024), code generation (Hui et al., 2024;Rozière et al., 2024), and dialogue (Chiang et al., 2024).Among these capabilities, mathematical ability serves as a fundamental measure of the problem-solving and complex reasoning skills of LLMs, motivating various endeavors towards improving the mathematical reasoning of LLMs (Yang et al., 2024b;Azerbayev et al., 2024;Wang et al., 2024).</p>
<p>Figure 1: Comparisons among different models on GSM8K, MATH, and Omni-MATH, where the models are ranked based on their performance on MATH, and those marked with "⋆" are closedsource models.As observed, the iterative advancements of these models show that existing benchmarks are nearing saturation.Our proposed Omni-MATH introduces a challenging benchmark to further advance mathematical intelligence in large language models.</p>
<p>To evaluate the mathematical capabilities of LLMs, researchers have introduced various mathematical benchmarks (Zhang et al., 2024), including the well-recognized GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).However, as LLMs have evolved rapidly, these benchmarks have increasingly lost their challenge (OpenAI, 2024), resulting in a diminished capacity to differentiate between model capabilities, as illustrated in Figure 1.Although some efforts have been made to establish more challenging benchmarks, most prior work has not focused specifically on evaluating mathematical capabilities (He et al., 2024).In addition, the test results are closely coupled with other LLM capabilities, such as multimodal fusion, rendering the findings to be vague for understanding the mathematical reasoning improvements (Huang et al., 2024).</p>
<p>To bridge this gap, we introduce Omni-MATH, a universal Olympiad-level benchmark specifically designed for mathematical reasoning.Our dataset exclusively focuses on text-only mathematics and encompasses an extensive collection of 4,428 competition-level problems.We curate our benchmark based on the actual selection processes of mathematical Olympiads, ensuring that our dataset includes problems from both introductory competitions and professional international contests.To further explore LLMs' performance in multiple mathematical areas, we have established a hierarchical classification of mathematical domains, including 33 sub-domains and more than 10 distinct difficulty levels (see Figure 2), allowing for a nuanced analysis of model performance across various mathematical disciplines and levels of complexity.For better evaluation, we provide a GPT-4o-based model evaluation and Omni-Judge, an open-source verifier.Our Omni-Judge provides achieves over 91% consistency with GPT-4o and 86% consistency with human judgments, providing reliable feedback.What's more, the open-source verifier provides a simple and effective way to evaluate the Olympiad-level mathematical problems.</p>
<p>We conduct a comprehensive evaluation of the currently strongest models on Omni-MATH.As shown in 3, it presents a significant challenge to the existing models.Even the most capable models OpenAI o1-mini and o1-preview, renowned for their reasoning ability, only achieve an accuracy of 60.54% and 52.55%, while the highest score among open-source models is just 36.2%.In addition, the detailed categorization of mathematical disciplines and comprehensive evaluation provide new insights into current LLMs.For instance, LLMs show a marginally greater aptitude for solving algebra, while struggling significantly with discrete mathematics.The commonly used approach for test-time scaling, Best-of-N, has proven ineffective for Olympiad-level mathematics problems.This underscores the urgent need for further research into test-time scaling techniques.</p>
<p>To summarize, our contribution includes: 1) We introduce Omni-Math, a universal Olympiad-level mathematical benchmark with over 33 subdomains and diverse difficulty levels, posing new challenges to the problem solving and complex reasoning capability of LLMs.</p>
<p>2) We introduce Omni-Judge, the first mathematical verifier designed for highly challenging prob- Table 1: Comparison of various mathematical reasoning benchmarks."# Data" denotes the total number of data in the corresponding benchmark, "# T.M" denotes the number of textual English mathematical reasoning data, "# T.O.M" indicates the number of textual English Olympiad-level mathematical reasoning data, "# Domain" refers to the number of domains in Olympiad-level mathematical data, "# Diff" represents the number of difficulty levels, "Leak Det." signifies whether data leakage detection is performed and "Evaluator" describes the evaluation methods employed.</p>
<p>OMNI-MATH BENCHMARK</p>
<p>In this section, we explore the construction process of Omni-MATH.This involves data generation, manual annotation( §E.1), and the classification of domains ( §2.3) and difficulty levels ( §2.2).Furthermore, we will introduce the evaluation ( §2.4) of the Omni-MATH, which includes GPT-4o-based evaluation and the open-source evaluator Omni-Judge.</p>
<p>Problem &amp; Solution MathPix Crawl</p>
<p>Crawl</p>
<p>GPT-4o Reformat</p>
<p>Problem:</p>
<p>Find the all the functions that satisfies ...</p>
<p>Answer:</p>
<p>The answer is xxx, because...</p>
<p>DATA COLLECTION AND ANNOTATION</p>
<p>First, we conducted a comprehensive review of mathematics competitions worldwide and classified them into five levels based on difficulty, scale, and prestige, referencing the website AoPS Wiki: Competition Ratings and other forums containing the discussions of mathematical contests.The details are presented in Table 5.It is important to note that this classification is somewhat preliminary; it serves only to determine which competitions we include in our benchmark, rather than providing definitive judgments on the difficulty level of each problem.After obtaining these target contests, we start our data collection and annotation process, as shown in Figure 3.</p>
<p>Data Collection</p>
<p>For some target competitions like IMO and IMC, we can directly crawl the publicly available problems and solutions from the official website.Then we utilized MathPix to convert the PDF documents of the problems and solutions into LaTeX format.Given that the provided solutions are guaranteed to be correct, we prioritized crawling these solutions for data collection.</p>
<p>For sections lacking open-source solutions, we extracted data from the famous AoPS website, which includes information from both the AoPS Wiki1 and the AoPS forum2 .The AoPS Wiki features recognized solutions, whereas solutions in the AoPS forum are often user-uploaded and may not be correct.We crawl the problem with multiple user-uploaded solutions following methods detailed in Appendix D. A rough consistency check is conducted by extracting final answers and checking whether they are identical to each other, leading to the exclusion of any inconsistent cases.Additionally, cases from which fewer than three answers could be scraped were also discarded.For each data source, we applied rules for initial filtering after obtaining the data, ensuring that both the questions and responses generally adhered to the specified input and output formats.</p>
<p>Table 2: The number of data in the construction process."#Data" denotes the number of data, "#R.Filter" denotes the number of data after rulebased filtering, and "#H.Filter" denotes the number of data after human annotation.Data Annotation After the automated construction of all the data, we engaged a team of professional annotators, comprised of graduate and doctoral students to verify the solutions and the answers of the dataset manually.</p>
<p>For the data from Contest Page and AoPS Wiki sections, which included official solutions, the verification process is relatively simple.We focus on ensuring consistency with the respective data sources.Conversely, we place particular emphasis on the problems sourced from the AoPS Forum.Following an initial screening, we narrow the dataset down to 1,100 problems.</p>
<p>Omni-MATH</p>
<p>At this stage, we engage a team of four annotators to assess whether the most frequent responses from the original forum align with the final outputs produced.Each annotator reviews 550 problems, ensuring that each problem is analyzed by two different personnel for greater reliability.We employ cross-validation to enhance the robustness of our findings, yielding an accuracy rate of 92.7%.After excluding inconsistent cases, we conducted a manual sampling of 150 entries, resulting in an improved accuracy rate of 97.3%.The overall data generation process is shown in Table 2, and additional annotation specifics can be found in Appendix C.</p>
<p>After the data collection and data annotation section, we obtain the overall data for Omni-MATH.</p>
<p>DIFFICULTY CLASSIFICATION</p>
<p>Through practical research, we find out that the difficulty of Olympic-level problems varies significantly.To enable a comprehensive assessment of Olympiad-level mathematical reasoning across various levels, We categorize the overall data by different difficulty levels.Relying solely on factors such as the difficulty and popularity of competitions to determine problem difficulty is insufficient.</p>
<p>To achieve a more objective assessment of the difficulty of each problem in our dataset, we consulted the AoPS: Rating3 page, which provides difficulty ratings for problems from various competitions.Specifically, the difficulty is quantified on a scale from 0 to 10, including increments such as 0.5 and 0.25.For competitions listed on this page, we directly assign the corresponding difficulty score on the problem.For those competitions not included, we utilize the existing competitions, problems, and their associated difficulties on this page as a basis for in-context learning and prompt GPT-4o to assign specific difficulty ratings.The details of this prompting procedure can be found in the Appendix E.4.Following categorization, the overall distribution of difficulty levels is presented in Figure 4.After conducting instance-level difficulty classification, we organized the difficulty distribution by competition.Our analysis revealed that the actual difficulty distribution rankings shown in Figure 4 closely align with the tiered classifications we previously surveyed in Figure 5.This consistency further validates the reliability of our domain classification.</p>
<p>DOMAIN CLASSIFICATION</p>
<p>Unlike existing benchmarks, we argue that Olympic-level mathematics competitions encompass a wide range of complex topics.The data from different domains present a greater challenge for model generalization, and it is quite likely that a single training set will yield inconsistent improvements across various fields.Therefore, it is essential to categorize the existing dataset into distinct mathematical domains to better investigate the model's performance across different math areas.</p>
<p>Specifically, we draw upon relevant competition guidebooks (Engel, 1998;Zeitz, 2017) to organize the mathematical fields into a hierarchical tree structure.This tree-based structure not only enhances</p>
<p>Omni-MATH the rationality of domain classification by aligning with the table of contents of reference materials but also allows us to compute accuracy metrics at varying levels of granularity.</p>
<p>For the domain categorization, we utilized GPT-4o to classify the problems.The prompt used for classification can be found in Appendix E.5, along with the detailed classification results.</p>
<p>EVALUATION</p>
<p>GSM8K</p>
<p>MATH</p>
<p>Omni-MATH</p>
<p>Model Answer:
\boxed{ \begin{aligned} &amp; (1,\ 0,\ 0,\ 0), \ &amp; (3,\ 0,\ 0,\ 1), \ &amp; (1,\ 1,\ 1,\ 0), \ &amp; (2,\ 2,\ 1,\ 1). \end{aligned} } Problem:
Find all positive integers $a,n\ge1$ such that..</p>
<p>Reference Answer:</p>
<p>\boxed{(2, 6), (2^k -1, 2), (1, n) \text{ for any } n \ge 1}</p>
<p>Model Answer:</p>
<p>• a = 1 for any n ≥ 1, • or n = 2 and a + 1 is a power of 2, • or a = 2 and n = 6.Compute $\arcsin \left( -\frac{1}{2} \right).$Express your answer in radians.</p>
<p>Model Answer:</p>
<p>The answer is: 260</p>
<p>Model Answer:</p>
<p>\boxed{-\frac{\pi}{6}}</p>
<p>Problem:</p>
<p>Toulouse has twice as many sheep as Charleston.Charleston has 4 times as many sheep as Seattle...</p>
<p>Reference Answer:</p>
<p>\boxed{-\frac{\pi}{6}}</p>
<p>Problem:</p>
<p>Find all nonnegative integer $(x,y,z,w)$ of the equation[2^x\cdot3^y-5^z\cdot7^w=1.]Reference Answer:</p>
<p>\boxed{(1, 1, 1, 0), (2, 2, 1, 1), (1, 0, 0, 0), (3, 0, 0, 1)} We discover that conducting answer-level assessments for Olympiad-level problems is a complex task due to the diverse formats of the final answers produced by models (Figure 5).This diversity makes it challenging to evaluate model outputs using fixed rules, as seen in other benchmarks, and we accordingly employ model-based evaluation for the assessment of Omni-MATH.Specifically, we leverage GPT-4o to verify whether the content generated by the tested model aligns with the standard answer.In our prompt, we provide problem, reference answer, model-generated solution, querying GPT-4o to determine whether the model solution is consistent with the reference answer.Detailed prompts and model outputs can be found in the Appendix H. Additionally, in Section 4.2, we assessed the evaluative capability of GPT-4o to ensure the accuracy of our results.</p>
<p>To promote public accessibility for Omni-MATH, we additionally developed an open-source evaluation model, named Omni-Judge, to assess the consistency between model solutions and reference answers at a low cost.Specifically, we first constructed a dataset for training (∼17618), validation (∼2200), and test (∼2200) based on partial results from GPT-4o evaluation, which have no overlaps of questions with each other.Then we treat this task as a similar flow of instruction following, where the extraction of the model answer, judgment, and detailed justification are all required to be generated by Omni-Judge.We accordingly test multiple instruct models while appending a case in the context to boost the performances.It is finally evaluated on the consistency between its judges and those from GPT-4o, which surprisingly exceeds 90%, demonstrating the strong feasibility of Omni-Judge.Detailed analysis can be found in Sec 4.2 and Appendix I.</p>
<p>OLYMPIAD-LEVEL MATH EVALUATION ON EXISTING LLMS</p>
<p>EXPERIMENTAL SETUP</p>
<p>We evaluate 15 models recognized for their strong mathematical reasoning capabilities.All prompts for these models are formatted according to the guidelines provided on their respective repositories.The detailed model information and prompt specifications can be found in Appendix F. To ensure a more accurate assessment, we employ GPT-4o to evaluate whether the model outputs are consistent with the correct answers.</p>
<p>We then analyze the accuracy rates across different domains and levels of difficulty.For clarity and simplicity in the presentation, we report the accuracy rates of the first-level subdomains of the domain tree.For difficulty, we categorize it into four tiers according to the difficulty tag: T1: 1-3, T2: 3-5, T3: 5-7, and T4: 7-10.Detailed accuracy statistics for each distinct difficulty level are provided in Appendix G. Domain-Specific Analysis Models demonstrate a stronger proficiency in domains such as algebra, calculus, and number theory while showing significant weaknesses in discrete mathematics.We hypothesize that this phenomenon stems from the prevalence of the former subjects in mathematical datasets; nearly all datasets reference algebra and calculus.Conversely, datasets related to discrete mathematics are scarcer, rendering this domain particularly challenging for models.</p>
<p>Omni-MATH</p>
<p>EXPERIMENTAL RESULT</p>
<p>Limitations in Best-of-N Scaling The efficient test-time scaling techniques warrant further development.We have employed the common approach in the current mathematical reasoning studies, known as Best-of-N (Wang et al., 2024;Gao et al., 2024;Yang et al., 2024b), utilizing the Qwen2.5-Math-RM-72B.However, we found that scaling the inference time did not yield consistent improvements in performance.We propose two potential reasons: (1) The reward model demonstrates a limited ability to supervise tasks related to Olympic-level mathematics.The reward model can successfully improve the Qwen2.5-MATH-7b-Instructfrom a total accuracy of 33.22 to 35.79, but fails with Qwen2.5-MATH-72b-Instruct.The reason might be the limitations of the reward model's discriminative ability, which is comparable to that of Qwen2.5-MATH-72b-Instruct,making it challenging to further improve performance.</p>
<p>(2) The policy model struggles to search the correct solutions.Moreover, with the limitations on inference tokens for OpenAI o1-preview and OpenAI o1-mini set to a maximum of 4096, which is considerably lower than the RM@256 costs, we still observe performance that greatly surpasses that of vanilla models.To eliminate the potential impact of data contamination on the conclusions of the experiments above, it is essential to conduct data leakage detection.Following Xu et al. (2024), we employed n-gram accuracy to identify any data leakage present in the existing models.Specifically, for each sample in the dataset, we concatenated the problem and its corresponding solution, then randomly selected K positions for 5-gram extraction.Whether the given sample has been contaminated corresponds to whether the 5 grams predicted by the model are identical to the ground truth 5 grams.We report the results in Table 4.</p>
<p>It can be seen that most models exhibit certain data leakage, as Omni-MATH is based on data sourced from the internet, while Qwen2.5-MATH-72b-instructhas the highest degree of data leakage with 5 grams accurately predicted in 31 samples.We also collect the samples that are contaminated and then correctly answered by the tested models, finding that the numbers of the "Contaminated" and "Contaminated &amp; Correct" samples are both extremely low.It indicates that data leakage has little impact on the conclusions before and Omni-MATH still poses significant challenges for them.</p>
<p>Moreover, we focus on the distribution of problem difficulty in these contaminated samples, where for most tested models, the average difficulty of "Contaminated &amp; Correct" samples is consistently lower than that of all contaminated samples, except GPT-4o and MetaLlama-3.1-70B-instructthat almost have no data leakage.It seems that difficulty level continues to exert a notable influence on the performance of models even in scenarios of data leakage.</p>
<p>RELIABILITY OF JUDGMENT MODELS</p>
<p>There are several studies utilizing model-based evaluation like GPT-4 to evaluate the correctness of answers (Fang et al., 2024;Huang et al., 2024), but whether such judgment models can consistently provide reliable feedback on Olympic-level problems remains inconclusive.In this part, we conduct a meta-evaluation experiment to assess the reliability of model-generated judgments.In detail, we first build a subset containing 100 samples and engage several graduate and doctoral students to annotate each sample as golden judgments, i.e. whether each solution is aligned with the standard answer for the given problem.Next, the predictions from judgment models are compared with the gold-standard annotations.We were surprised to find that GPT-4o acquires a 98% accuracy with human annotations, demonstrating the strong reliability of both the evaluation tool and the results in Omni-MATH.Additionally, Omni-Judge also shows considerable effectiveness with an accuracy of 86%, making it a cost-effective tool for quickly evaluating and iterating models.</p>
<p>Since GPT-4o shows great consistency with human annotations, we treat it as a reliable proxy of human judgment and further analyze the performance of Omni-Judge in Appendix I.</p>
<p>EMPIRICAL ANALYSIS ON DATA SELECTION</p>
<p>In this section, we aim to provide practical conclusions of the data selection in pursuit of a better Olympic-level mathematical reasoner.Here we develop a novel approach to estimating the impact of domain/difficulty distributions on the final performance, that is, for each model, we manipulate demonstrations in the input and observe variation in performance through in-context learning.</p>
<p>To balance the in-context ability of the model and its mathematical reasoning ability, we choose GPT-4o as the base model for this experiment.Due to cost constraints, we randomly sample 500 Domain To investigate the mutual influence of data from different domains, we retrieve data from different domains as in-context demonstrations.To balance reasoning cost and the impact of cases, we select four same-domain examples for in-context learning.We present the mutual influence among 5 main domains in Figure 6 (a).For the sake of presentation, we normalize the column according to the diagonal data, i.e., putting in-domain data as the baseline.From the experimental results, we observe that the mutual influence among these domains is asymmetric.For example, Discrete Mathematics has a worse influence on Number Theory, while Number Theory has a positive effect on the prediction of Discrete Mathematics.Additionally, Applied Mathematics and Algebra show a significant positive effect on Discrete Math, with an accuracy of 124% of the in-domain data.Also, Number Theory greatly supports Geometry, with accuracy reaching 150% of in-domain accuracy.Additionally, we find that Algebra interferes with applied mathematics.</p>
<p>Common Length</p>
<p>The similarity between the in-context examples and the target domain is an important factor that may influence the model's performance.To explore this, we conduct experiments to examine the impact of the fine-grained domain similarity between demonstrations and the target domain on the model's effectiveness.The results are shown in Figure 6 (b).We define domain similarity as the length of a common path on the domain tree.A longer common path indicates greater domain similarity, and we find that as the domain similarity increases, the model's performance consistency also increases.Compared to random selection, the most significant improvement is brought by the same first-level domain data, suggesting that the model has the ability to generalize the in-domain data at a broader level.</p>
<p>Difficulty We also discussed the impact of the difficulty of in-context examples on model performance.Consistent with the settings in Sections 3.1 and 4.4, we divided the difficulty into 4 intervals.We find that as the difficulty of examples in the prompt increases, the model's performance gradually improves, except for a slight decline on T4.Overall, we can conclude that more difficult data would help improve the model performance.</p>
<p>DIFFICULTY CONSISTENCY ANALYSIS</p>
<p>Recent studies (Yang et al., 2024c) have shown that models may exhibit the capability to solve difficult problems, yet fail to perform well on easier ones.Specifically, the accuracy of solving challenging problems is not consistently higher than that of simpler ones in all cases.In this section, we examine the consistency of model performance within the problem difficulty in Omni-MATH.</p>
<p>Our difficulty tag is obtained differently from the pairwise data construction employed in Yang et al. (2024c), so it is unsuitable to directly utilize the consistency score from their study.However, we can measure the trend intensity of model performance as the difficulty increases for consistency.Specifically, we use the four difficulty intervals in Table 3 for the overall assessment.The model performance is defined by the variables x 1 , x 2 , x 3 , x 4 , where 0 &lt; x i &lt; 100.Then we define the trend intensity A by simplifying the implementation of ARIMA Modeling (Box &amp; Pierce, 1970)</p>
<p>Omni-MATH and adapting it to our situation.The implementation of A with n variables is shown below:
A = n−1 i=1 K • (x i − x i+1 ) if x i+1 &gt; x i min( max(x) n , x i − x i+1 ) if x i+1 &lt;= x i (1)
This equation measures the predicted values and their relative comparison at each position based on the first difference.We assume that x decreases as i increases.For counter-trend situations, we apply a penalty coefficient that is double that of positive trends.We constrain the largest impact of each position to the max(x) n = 25.For sequences of four variables with a penalty coefficient of K = 2, the value range of A is [−200, 75].Intuitively, the more pronounced the downward trend of x, the larger the value of A; conversely, the smaller the value of A. The experimental result is shown in Figure 7.  1.The difficulty consistency represents shows that our difficulty level is generally aligned with the performance of models.</p>
<p>From the experimental results, we observe that the consistency of all models is positive, indicating that as the difficulty increases, the overall accuracy of all models declines.This finding also validates the rationality of our difficulty classification.Additionally, we note that as model capabilities improve, their consistency also increases, but exceptions also exist.This conclusion is similar to that reached in Yang et al. (2024c), even though our methods of measurement are entirely different.</p>
<p>RELATED WORK</p>
<p>MATHEMATICS BENCHMARKS</p>
<p>Measuring the mathematical reasoning abilities of language models has long been a focal point in both academia and industry.GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) are the key benchmarks in the mathematical reasoning domain.GSM8K primarily emphasizes a model's capacity to solve practical application-based math problems, while MATH presents a greater challenge, encompassing university-level and introductory competition-level mathematics questions.As models become more powerful, several works have proposed more challenging problems, such as OCWCourses (Lewkowycz et al., 2022), SAT (Azerbayev et al., 2024), JeeBench (Arora et al., 2023), and MATHOdyssey (Fang et al., 2024).However, as models rapidly improve, the challenge posed by these benchmarks is gradually diminishing, making it increasingly difficult to differentiate their mathematical reasoning abilities.Another line of research (Zheng et al., 2022;Azerbayev et al., 2023) employs formal languages to measure the models' theorem proving capabilities.</p>
<p>OLYMPIAD-LEVEL BENCHMARKS</p>
<p>To further explore the boundaries of large language model capabilities, many studies have attempted to yield Olympiad-level data as benchmarks.Among these, AlphaGeometry (Trinh et al., 2024) offers Olympiad-level geometry problems.CHAMP (Mao et al., 2024) provides high school competition-level math problems, distinctly annotated with concepts and hints relevant and helpful for each problem.OlympiadBench (He et al., 2024) and OlympicArena (Huang et al., 2024) are two comprehensive benchmarks that encompass Olympiad problems from various fields.However, within these benchmarks, the proportion of text-only mathematical reasoning data remains scarce.Furthermore, none of the aforementioned benchmarks have further differentiated and analyzed the problems from Olympiad mathematics competitions.Our proposed Omni-MATH focuses exclusively on text-only Olympiad-level mathematical reasoning to truly explore the boundaries of current LLM's mathematical capabilities.We further categorize the problems from the Olympiad mathematics competitions into various difficulty levels, providing a clear and reasonable difficulty classification setting for better diagnosing model performance.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce Omni-MATH, a comprehensive and highly challenging benchmark for Olympiad-level mathematical reasoning.Our experiments reveal that even the most advanced model in mathematical reasoning, OpenAI o1-mini, achieves a maximum accuracy of only 60.5%, while the open-source model currently achieves only 36.2%.These results demonstrate that the highest difficulty level in mathematics presents significant challenges for large models.Additionally, we conducted an in-depth analysis of model performance based on this dataset, aiming to provide valuable contributions to the mathematics community.</p>
<p>Omni-MATH A ANALYSIS OF PROCESS-LEVEL ASSESSMENT</p>
<p>In this section, we utilize GPT-4o for process-level assessment.Specifically, we provide GPT-4o of the question, the corresponding standard answer, and the model-generated answer in the prompt.We split both the reference answer and model-generated answer into steps and let GPT-4o evaluate the correctness of each step of the model-generated answer.If a step is identified as incorrect, we further query an explanation for the error.Following the Gao et al. (2024), we categorize the mathematical reasoning steps as follows: Unrelated: This indicates that the step is irrelevant and does not contribute towards deducing the final answer.Accumulation: This denotes that the step is incorrect due to a mistake in the preceding step, leading to subsequent errors.Calculation: This categorization is reserved for errors arising from incorrect calculations, which is one of the most common errors in mathematical reasoning.Logic: This applies to steps that are logically flawed in the context of solving the given problem.Other: This category encompasses steps that are erroneous for reasons not covered by the aforementioned categories.Then we compiled the proportions of step-level error types and the model performances, as illustrated in Figure 8. Logical errors represent a significant issue for the models.As shown in Figure 8 (a), among the best existing inference models, most erroneous steps can be attributed to logical errors, followed by accumulation errors and calculation errors.This highlights the challenges posed by our proposed assessment set for current models.</p>
<p>OpenAI O1-mini demonstrates significantly fewer errors across all categories.Figure 8 (b) illustrates the error distribution across different categories for various models.Given that the total number of errors varies by category, we applied normalization.Our findings indicate that the Ope-nAI O1-mini model outperforms other models across all error categories.Notably, GPT-4o exhibits a higher incidence of calculation and logical errors compared to other math-specialized models, underscoring that specialized training in mathematics is essential for Olympiad-Level mathematical reasoning.Additionally, Qwen2.5-MATH-72b-Instructshows a tendency to produce unrelated content compared with other models.</p>
<p>B DETAILED DATA LEAKAGE INFORMATION</p>
<p>We examine whether the given instance has been contaminated by testing whether the model's predicted 5 grams are identical to the ground truth 5 grams.The detailed experimental results are illustrated in the Figure 9.</p>
<p>Task Example</p>
<p>Under answer area conditional judgment, new annotation content will be automatically displayed if the conditions are met, otherwise, it will directly move to the next annotation.The detailed annotation tasks and principles are listed here.Specifically, the annotators were assigned the three tasks:</p>
<p>• Determine whether the current problem contains multiple questions or is proof-based.If it is a multi-part question or a proof question, the evaluation of this data becomes more complex, making it unsuitable for the evaluation in our benchmark; therefore, we decided to discard this part of the data.• Verify the correctness of the answers present in the existing dataset.This step is essential for assessing answer quality; different verification methods are required depending on the data source.For entries from AoPS Wiki and problems with provided solutions, it suffices</p>
<p>Omni-MATH to check for consistency with the answers in the data source.For data from the AoPS Forum, it is also necessary to refer back to the results provided in the forum.</p>
<p>• If the original answer is found to be incorrect, provide a corrected answer based on the data source.</p>
<p>C.2 ANNOTATION DETAILS</p>
<p>For the 1100 questions collected from the AoPS Forum, we employed 4 annotators to verify the consistency between the final responses and the original forum replies.Each annotator labeled 550 questions, resulting in two annotations per question.We then conducted cross-validation and sampled cases for inspection.The accuracy of the cross-validation was 92.7%(1020/1100), while the post-sampling accuracy was 97.3%(146/150).After removing discrepancies, we obtained a total of 888 questions from this set.</p>
<p>For the Contest Page questions converted from PDF to LaTeX, the quality was exceptionally high.</p>
<p>We employed rough filtering rules to eliminate some errors that occurred during the conversion process.Subsequently, a manual check was performed to ensure the consistency between answers and explanations.This part resulted in a total of 3295 questions.</p>
<p>Regarding the questions from the AoPS Wiki, comprehensive explanations were readily available.</p>
<p>We only needed to filter out the video-based explanations and exclude questions that did not fit within our collection scope, such as AMC and AIME questions.This section yielded a total of 298 questions.</p>
<p>After the labeling process, the initial dataset comprised 4481 questions.Subsequently, we hired 2 annotators to inspect the entire dataset, following a set of principles.After a final filtration, 53 questions were removed from the Contest Page source, resulting in a final dataset of 4428 questions.</p>
<p>D DATA CRAWLING DETAILS</p>
<p>To enhance the possibility of obtaining correct answers in AoPS Wiki, we specifically targeted posts that contained hidden tags indicating "solution", as well as those that included "\boxed{}" or "■".</p>
<p>Following LI et al. (2024), we assume that these posts have a higher probability of containing accurate answers.Then employed GPT-4 to reformat the user-uploaded solutions into the standardized format of solution + \boxed{Final Answer} to improve the answer format quality.</p>
<p>E.2 DIFFICULTY</p>
<p>The detailed difficulty distribution of the Omni-MATH is shown in Figure 11.The difficulty distribution roughly follows a normal distribution</p>
<p>E.3 DOMAIN CLASSIFICATION TREE</p>
<p>The overall domain tree with the problem number is shown in Figure 12.Note that the total number is larger than the total data of 4428 because the question might belong to multiple domains.</p>
<p>E.4 DIFFICULTY CLASSIFICATION PROMPTS</p>
<p>For data not included in Aops:Ratings, we need to assign a difficulty score to the problem.We collect all the problems with its solution and the data source.Then we use GPT-4o to assign a difficulty level of the problem, the prompt we use is shown in Figure 13.The relevant page information makes the prompt too long to show on this page.Instead, we upload the total prompt as the additional materials.</p>
<p>Omni-MATH</p>
<p>Figure 11: The difficulty distribution of Omni-MATH.</p>
<p>E.5 DOMAIN CLASSIFICATION PROMPTS</p>
<p>We use GPT-4o to assign a domain tag to each problem, the prompt we use is shown in Figure 14.The domain tree makes the prompt too long to show on this page.Instead, we upload the total prompt as the additional materials.</p>
<p>Omni-MATH   (DeepSeek-AI et al., 2024); Claude-3.5-SONNET(Anthropic, 2024); NuminaMATH-72B-COT (LI et al., 2024); Qwen2-MATH-7b-Instruct (Yang et al., 2024a); Qwen2.5-MATH-7b-Instruct(Yang et al., 2024a); Qwen2-MATH-72b-Instruct (Yang et al., 2024a); Qwen2.5-MATH-72b-Instruct(Yang et al., 2024a); GPT-4o (OpenAI, 2023); OpenAI o1-preview (OpenAI, 2024); OpenAI o1mini (OpenAI, 2024).</p>
<p>F.2 MODEL INFERENCE SETUP</p>
<p>For all the models under evaluation, the prompts follow the instructions provided on their respective release pages, such as DeepseekMATH-7b-RL and Qwen2.5-MATH-7b.</p>
<p>Omni-MATH</p>
<h1>CONTEXT # I am a teacher, and I have some high-level olympiad math problems.I want to evaluate the difficulty of these math problems.There are some references available regarding the difficulty of the problems: <difficulty reference> ## Examples for difficulty levels For reference, here are problems from each of the difficulty levels 1-10: ## Some known difficulty ratings of the competitions.</h1>
<p>(Too long to show...) </difficulty reference> # OBJECTIVE # A. Summarize the math problem in a brief sentence, describing the concepts involved in the math problem.B. Based on the source of the given problem, as well as the difficulty of the problems referenced in these materials and the solution to the current problem, please provide an overall difficulty score for the current problem.The score should be a number between 1 and 10, with increments of 0.5, and should align perfectly with the materials.In chat models where there are no specific instructions on how to phrase the mathematical reasoning prompts, we directly input the model with the problem along with a system prompt: "You are an experienced educator in the field of MATHEMATICS."</p>
<p>To mitigate randomness in the responses, we set the parameters as follows: temperature = 0, top p = 1, and a maximum of 2048 tokens.For the O1-preview and O1-mini, due to constraints of inference costs, we configured the maximum completion tokens to 4096.Additionally, for inference with non-API models, we employed the vllm framework.</p>
<p>G METRICS WITH DISTINCT DIFFICULTY LEVEL</p>
<p>In this section, we present the fine-grained performance of the model across varying difficulty levels in Table 6.Due to the limited number of instances at certain decimal levels, such as 4.75 and 2.25, we categorize all difficulty levels into integer buckets.For example, a score of 4.75 falls under the "#D5" category, while 2.25 falls under "#D3."</p>
<p>The results of our experiments are depicted in the figure below.We observed a consistent decline in the model's performance as difficulty increased, except for the "#D10" category, which contains very few instances (only 16), leading to significant fluctuations in results.</p>
<p>Additionally, we note that the accuracy for nearly all model responses in the "#D1" category is exceptionally high, indicating that this difficulty level does not effectively differentiate model performance.However, as the difficulty increases, the differences among the model performances begin to emerge.Overall, we find that the difficulty of the GSM8K dataset mainly corresponds to the difficulty of "#D1," while the MATH dataset aligns with the difficulty of "#D2."This further underscores that our proposed benchmark significantly exceeds the existing evaluation datasets.</p>
<p>Omni-MATH</p>
<h1>CONTEXT # I am a teacher, and I have some high-level math problems.I am tasked with evaluating the correctness of a student's answer.Below, I am provided with a problem and a reference answer.Additionally, a student's answer is provided.My job is to assess whether the student's answer captures the same meaning as the reference answer, even when expressed with different wording or format.</h1>
<h1>OBJECTIVE # I need you to judge whether the student's answer is correct given the ground truth answer.The student's answer of 216 is incorrect in the context of the problem, which asks for the total count of 4-digit numbers beginning with 1 that have exactly two identical digits.The reference answer is 432.</h1>
<p>In the student's solution, they consider only cases where the digit '1' is one of the identical digits.However, the problem also includes the scenario where the identical digits could be different from '1'.Thus, the student's calculation does not account for all valid configurations.The discrepancy in figures indicates that the student's answer does not share the same meaning as the reference answer.</p>
<p>=== report over === <example math solution> (Too long....We Omit this) </example math solution> ## Student Final Answer \text{Susana has a winning strategy.}## Equivalence Judgement FALSE ## Justification The student's answer states that "Susana has a winning strategy," which is not in agreement with the reference answer indicating "No winning strategy; the game can last indefinitely."The essence of the problem is that neither player can guarantee a win because the game can go on indefinitely without either player being able to write a polynomial with an integer root.The student's conclusion contradicts this, suggesting that a specific winning strategy exists for Susana, which is fundamentally incorrect according to the problem's conditions.Thus, the student's answer does not capture the same meaning as the reference answer.Table 7: Omni-Judge results.We implement Omni-Judge based on three base models: LLaMA-2-7b-Chat, LLaMA-3-8b-Instruct, and LLaMA-3.1-8b-Instruct, and evaluate their performance according to the rate of successfully parsing outputs and the rate of consistency between the judge of Omni-Judge and GPT-4o for each problem.</p>
<p>Model Training with Homology Data</p>
<p>LLaMA-2-7b-Chat LLaMA-3-8b-Instruct LLaMA-3.</p>
<p>I ANALYSIS OF OMNI-JUDGE</p>
<p>Omni-Judge is proposed as an economic but effective way to provide judgment for complicated mathematical problems accompanied by reference answers and tested solutions.In this part, we test Omni-Judge on the quality of generated judgment in two aspects: whether in a precise format and whether to generate correct judgment, represented by the rates of being successfully parsed (Success) and consistency rates with golden judgments (Consistency), as in Table 7.Note that judgments from GPT-4o are seen as the golden ones here.Furthermore, as discussed in Section 2.4, we experiment with various LLaMA-series models, and with upgraded versions, the comprehensive capabilities of models are generally acknowledged to improve.We find the performance of Omni-Judge also shows enhancements along this trend.In detail, we collect test data from several models tested in Table 3, 5 of which also provide data for training Omni-Judge (denoted as Training with Homology Data) and the rest 3 ones do not, while they demonstrate similar conclusions.</p>
<p>In terms of format accuracy, the Success rates are approximately 100% across different settings, while the Consistency rates are not.Omni-Judge model trained with LLaMA-3.1-8b-Instructshows the best Consistency with GPT-4o in total, followed by the version of LLaMA-3-8b-Instruct.On the other hand, Omni-Judge exhibits the lowest scores based on LLaMA-2-7b-Chat.The high Success suggests that models tend to learn formats first, aligning with findings of format learning in Wang et al. (2022).However, to provide better judgments, the base models are required to effectively capture the optimization directions during fine-tuning or possess knowledge in this domain.</p>
<p>Therefore, the highest Consistency of Omni-Judge on LLaMA-3.1-8b-Instruct is reasonable, and we consequently use it as the default version of Omni-Judge.</p>
<p>Another significant value of mathematical benchmarks is the provided ranking, which reflects the relative capability of each subject among all tested models.Therefore, it is meaningful to compare the ranking of model capabilities from Omni-Judge and GPT-4o, which suggests whether Omni-</p>
<p>Figure 2 :
2
Figure 2: An overall illustration of Omni-MATH.The left-top part presents the diverse and wellstructured data sources of Omni-MATH.The left-bottom part represents the hierarchical mathematical domains of Omni-Math.The right part presents a concrete data example of Omni-Math.</p>
<p>Figure 3 :
3
Figure 3: The overall data collection and annotation process of Omni-MATH.</p>
<p>Figure 4 :
4
Figure 4: Difficulty distribution across contests.</p>
<p>Figure 5 :
5
Figure 5: Examples of the problem, reference answer, and model-generated answer from different datasets, where the model-generated answers are all correct.On the right side, model-generated answers have issues in reformatting or requiring additional reasoning to correctly evaluate.</p>
<p>Figure 6 :
6
Figure 6: Results of data selection experiments.(a) The impact of different demonstration domains on model performance.(b) The effect of the distance within the domain tree on model performance.(c) The influence of the difficulty level of in-context examples on model performance.</p>
<p>Figure 7 :
7
Figure 7: Performance and Consistency Comparison of Different Models.Consistency is measured by Equation1.The difficulty consistency represents shows that our difficulty level is generally aligned with the performance of models.</p>
<p>Figure 8 :
8
Figure 8: Process-level result across differnec</p>
<p>(a) Number of instances of different models affected by contamination and the correct instances under contaminated conditions.(b) Average difficulty of contaminated and correct instances.</p>
<p>Figure 9 :
9
Figure 9: Data Leakage and Correct Instances for Different Models.</p>
<p>Figure 10 :
10
Figure 10: The illustration of the UI page of the annotation task.</p>
<p>Figure 12 :
12
Figure 12: The total domain tree of Omni-MATH.</p>
<p>Figure 13 :
13
Figure 13: The detailed difficulty classification prompt.</p>
<p>Figure 14 :
14
Figure 14: The detailed domain classification prompt.</p>
<p>The student's answer does not provide a final conclusion and the final answer is missing, which indicates a lack of conclusion.</p>
<p>Figure 15 :
15
Figure 15: The evaluation prompt of GPT-4o.</p>
<p>Figure 17 :
17
Figure 17: Results of predicted accuracy on different models, conducted by Omni-Judge and GPT-4o, respectively.</p>
<p>Table 3 :
3
Main Result.The results with bold text represent the best performance score.Qwen2.5-MATH-72b-Instructand OpenAI o1-mini separately perform best in the vanilla model setting and test-time scaled model setting..1239.7721.88 21.5715.7425.75 29.38 68.38 25.01 21.83 15.81 Qwen2.5-MATH-7b-Instruct33.22 39.39 37.50 31.25 26.89 16.93 28.62 30.37 66.23 29.20 24.68 20.34 Qwen2-MATH-72b-Instruct 33.68 40.27 37.50 27.34 22.53 17.50 30.01 32.96 70.10 29.06 24.71 17.98 Qwen2.5-MATH-72b-Instruct36.20 43.33 42.53 39.84 26.5718.2834.28 33.37 70.96 31.37 27.75 22.29 Test-time Scaled Models Qwen2.5-MATH-7b-InstructRM@835.70 42.12 36.78 33.59 31.8918.9629.59 30.88 67.95 31.46 27.41 24.0 Qwen2.5-MATH-7b-InstructRM@25635.79 42.54 49.43 39.06 25.79 19.75 31.66 33.13 68.24 30.48 27.81 23.71 Qwen2.5-MATH-72b-InstructRM@836.34 43.89 48.28 34.38 26.18 18.28 33.30 34.12 71.24 32.04 26.94 23.43 Qwen2.5-MATH-72b-InstructRM@25635.95 43.47 47.13 35.94 25.10 19.41 32.64 34.12 68.38 31.46 27.68 26.28
ModelAccAlg.P.CalCalGeo. D.M. Num. App.#T1#T2#T3#T4Vanilla ModelsInternLM2-MATH-mixtral8*22B14.24 18.19 12.50 10.16 8.708.03 10.09 12.36 42.78 8.01 10.35 6.74DeepSeekMATH-7b-RL16.12 21.28 20.45 12.50 9.877.719.98 13.58 49.07 9.11 11.49 7.80Mathstral-7B-v0.119.13 23.99 25.00 13.28 12.19 10.04 14.58 16.30 53.07 10.93 15.29 11.86DeepSeek-Coder-V2-Lite-Instruct19.73 24.55 23.86 13.28 13.06 8.92 15.88 16.81 55.93 13.15 12.86 9.55MetaLlama-3.1-70B-instruct24.16 29.15 27.59 18.75 14.76 11.74 17.03 24.66 62.66 16.82 16.95 13.71DeepSeek-Coder-V225.78 30.24 35.23 15.62 17.99 12.71 20.90 23.58 65.38 18.84 18.06 14.61Claude-3.5-SONNET26.23 30.30 29.55 19.53 17.70 15.74 19.51 26.70 66.23 18.91 18.27 17.41NuminaMATH-72B-COT28.45 34.74 27.27 21.88 20.41 16.95 23.47 25.06 65.63 23.70 20.33 21.08Qwen2-MATH-7b-Instruct29.36 36.08 35.23 24.22 18.68 14.41 27.04 25.93 63.52 24.30 21.52 18.54GPT-4o 30.49 36OpenAI
o1-preview  52.55 57.70 57.47 53.91 43.11 31.2649.6753.42 80.11 50.83 42.2537.71OpenAI o1-mini 60.54 67.82 68.18 60.94 51.50 37.68 61.74 60.52 82.23 63.10 49.11 42.69 Olympic-level mathematics remains far from solved Our findings indicate that Omni-MATH currently presents significant challenges to all models.The strongest model evaluated is OpenAI o1-mini, which utilizes test-time enhancement to achieve an accuracy rate of just 60.54%.OpenAI o1-preview attains an accuracy of 52.55%, while the SOTA vanilla model acquires 36.2%,leaving a substantial gap from the top two models.It should be noted that open-source models, like Qwen2.5-MATH, have surpassed GPT-4o in the field of Olympiad mathematical reasoning.</p>
<p>Table 5 :
5
The hierarchical data source of the Omni-MATH.
ContestsNumber of ProblemsHighly Influential International CompetitionsIMO75IMO Shortlist190IMO Longlist43Putnam101IMC68Notable International/National Challenging CompetitionsUSAMO133IZhO14China TST106CMO38USA TST45Intermediate International/National CompetitionsAPMO85USAJMO39Balkan MO26Balkan MO Shortlist35JBMO23JBMO Shortlits33ToT55BalticWay37Alibaba global Contest22Middle European Mathematical Olympiad21Other Olympiads269National Championships or CompetitionsHMMT 21385HMMT 11896Yau Contest7Introductory CompetitionsPascal249Cayley201Fermat232</p>
<p>This problem is straightforward and primarily involves basic fraction addition, making it suitable for early middle school students.</p>
<h1>STYLE #Data report.# TONE #Professional, scientific.# AUDIENCE #Students. Enable them to better understand the difficulty of the math problems.# RESPONSE: MARKDOWN REPORT ### Summarization[Summarize the math problem in a brief paragraph.]## Difficulty[Rate the difficulty of the math problem and give the reason.]# ATTENTION #-Add "=== report over ===" at the end of the report.<example math problem>(Too long to show)</example math problem>## SummarizationThe problem requires finding a value that makes the equation $\frac{1}{9}+\frac{1}{18}=\frac{1}{\square}$.This involves adding two fractions and determining the equivalent fraction.## DifficultyRating: 1Reason: === report over ===</example math problem>(Too long to show)</example math problem>## SummarizationThe problem asks for the possible values of $n$ for a regular n-sided polygon that can be completely triangulated into isosceles triangles using non-intersecting diagonals.The solution involves analyzing the properties of the diagonals forming isosceles triangles and deducing that $n$ can be expressed in terms of powers of 2.## DifficultyRating: 7Reason: The problem involves understanding properties of isosceles triangles in the context of polygon triangulation and requires critical reasoning to establishrelationships between the number of sides and powers of 2, making it more complex than typical undergraduate-level problems.=== report over ===<math problem>[Question]:{{Question Here}}[Solution]:{{Solution Here}}[Source]:{{Source Here}}</math problem></h1>
<p>Summarize the math problem in a brief sentence, describing the concepts involved in the math problem.B.Categorize the math problem into specific mathematical domains.Please provide a classification chain, for example: Applied Mathematics -&gt; Probability -&gt; Combinations.The following is a basic classification framework in the field of mathematics.The math problem can be categorized into multiple domains, but no more than three.Separate the classification chains with semicolons(;).-YourclassificationMUSTfall under one of the aforementioned subfields; if it really does not fit, please add "Other" to the corresponding branch.For example: Algebra -&gt; Intermediate Algebra -&gt; Other.Only the LAST NODE is allowed to be "Other"; the preceding nodes must strictly conform to the existing framework.-Themathdomain must conform to a format of classification chain, like "Applied Mathematics -&gt; Probability -&gt; Combinations".-Add "=== report over ===" at the end of the report.</p>
<h1>CONTEXT #I am a teacher, and I have some high-level olympiad math problems.I want to categorize the domain of these math problems.# OBJECTIVE #A. <math domains>(Too long to show, domain tree here..)</math domains># STYLE #Data report.# TONE #Professional, scientific.# AUDIENCE #Students. Enable them to better understand the domain of the problems.# RESPONSE: MARKDOWN REPORT ### Summarization[Summarize the math problem in a brief paragraph.]## Math domains[Categorize the math problem into specific mathematical domains, including major domains and subdomains.]# ATTENTION #-<example math problem>(Too long to show)</example math problem>## SummarizationThe problem requires finding a value that makes the equation $\frac{1}{9}+\frac{1}{18}=\frac{1}{\square}$.This involves adding two fractions and determining the equivalent fraction.## Math domainsMathematics -&gt; Algebra -&gt; Prealgebra -&gt; Fractions;=== report over ===</example math problem>(Too long to show)</example math problem>## SummarizationThe problem asks for the possible values of $n$ for a regular n-sided polygon that can be completely triangulated into isosceles triangles using non-intersecting diagonals. Thesolution involves analyzing the properties of the diagonals forming isosceles triangles and deducing that $n$ can be expressed in terms of powers of 2.=== report over ===<math problem>[Question]:{{Question Here}}[Solution]:{{Solution Here}}[Source]:{{Source Here}}</math problem></h1>
<p>Your tasks include: A. Identify Mathematical or Notational Equivalence: Pay special attention to any LaTeX expressions in both answers.Confirm that the mathematical relationships, variables, and operations conveyed are equivalent.B.Provide a Justification: Conclude with a brief explanation as to why you believe the student's output is correct or incorrect, highlighting any key differences in meaning or content.The reference answer is ALWAYS correct.You should carefully judge whether the student gives the same answer as reference answer.-TheEquivalenceJudgement is only TRUE or FALSE.The answer is FALSE even if the student's final answer almost correct with a minor mistakes.-Theanswer is contained within the "boxed" section, so you can focus solely on comparing the content in the student's answer box with the reference answer, without needing to consider the intermediate steps.-Add"=== report over ===" at the end of the report.</p>
<h1>STYLE #Teaching report.# TONE #Professional, scientific.# AUDIENCE #Students. Enable them to better understand whether the answer they produce is correct.# RESPONSE: MARKDOWN REPORT ### Student Final Answer[Extract the student's final answer, which is enclosed in "\boxed{}".]## Equivalence Judgement[Whether the student's answer share the same meaning with the reference answer. (TRUE or FALSE)]## Justification[Conclude with a brief explanation as to why you believe the student's answer is correct or incorrect.]# ATTENTION #-<example math solution>(Too long.... We Omit this)</example math solution>## Student Final Answer216## Equivalence JudgementFALSE## Justification</h1>
<p>https://artofproblemsolving.com/wiki/index.php
https://artofproblemsolving.com/community/c13 contests
https://artofproblemsolving.com/wiki/index.php/AoPS Wiki:Competition ratings
Omni-MATH H.2 CASE STUDY OF EVALUATION ISSUEProblem: Find all positive integer $ m$ if there exists prime number $ p$ such that $ n^m\minus{}m$ can not be divided by $ p$ for any integer $ n$.Reference Answer: m \neq 1Model-generated Solution:To solve the problem, we need to determine all positive integers \( m \) for which thereexists a prime number \( p \) such that \( p \) does <strong>not</strong> divide .... ###Answer: <strong>All positive integers m at least two.Every integer m ≥ 2 satisfies the stated condition.</strong>GPT-4o-Evaluation:Equivalence Judgement: TRUE, Justification: The student's answer indicates that all positive integers \( m \geq 2 \) satisfy the given condition, which is mathematically equivalent to the reference answer \( m \neq 1 \).Both answers correctly identify that \( m = 1 \) does not satisfy the condition, while all other positive integers do.Thus, the meaning conveyed by the student's answer aligns with the reference answer.Problem:Find all nonnegative integer solutions $(x,y,z,w)$ of the equation\[2^x\cdot3^y-5^z\cdot7^w=1.\]Reference Answer:(1, 1, 1, 0), (2, 2, 1, 1), (1, 0, 0, 0), (3, 0, 0, 1)Model-generated Solution:To find all nonnegative integer solutions \((x, y, z, w)\) to the equation:..GPT-4o-Evaluation: Equivalence Judgement: TRUE, Justification: The student's final answer, which consists of the solutions \((1, 0, 0, 0)\), \((3, 0, 0, 1)\), \((1, 1, 1, 0)\), and \((2, 2, 1, 1)\), matches exactly with the reference answer.Each tuple denotes a nonnegative integer solution to the given equation, \[2^x\cdot3^y-5^z\cdot7^w=1.\]The correspondence of each part of the solution shows that the student's solution is mathematically equivalent and correctly identifies all potential solutions.Figure16: The case study of GPT-4o-based evalution.In Figure16, we selected data from two Human Annotated Datasets that represent two scenarios where rule-based evaluation is impractical.However, GPT-4o demonstrates the capability to assess correctly in both situations.The first scenario is reasoning.It requires a nuanced understanding of the problem statement, which implies that m is a positive integer greater than 0. The correct answer is m ̸ = 1; however, the model generates the answer as m ≥ 2. Considering the defined domain for m and the constraints of the answer, we can infer that these two expressions are equivalent.Thus, a certain level of reasoning ability is necessary to arrive at the correct conclusion.The second scenario involves complex formatting.In this case, the order of the answers does not align with the sequence of the tuples generated by the model in LaTeX, leading to inconsistencies.This makes it challenging to employ rule-based evaluations effectively.Omni-MATHJudge can be a practical judgment model offering reliable feedback.As in Figure17, Omni-Judge provides outcomes similar to GPT-4o for each tested model, as well as an identical ranking of model capabilities according to the predicted accuracy, which proves its usability.
The llama 3 herd of models. Abhinav Pandey, Abhimanyu Dubey, Abhinav Jauhri, 2024</p>
<p>. A I Mistral, Mathstral, </p>
<p>. Anthropic, 2024</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , 2023</p>
<p>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, 2023</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2024</p>
<p>Distribution of residual autocorrelations in autoregressiveintegrated moving average time series models. E P George, David A Box, Pierce, Journal of the American statistical Association. 653321970</p>
<p>and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, 2024</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. Deepseek-Ai , Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, -Math Xuan Omni, Qinyu Lu, Yaohui Chen, Chengqi Wang, Jiashi Deng, Chenggang Li, Chong Zhao, Fuli Ruan, Wenfeng Luo, Liang, 2024</p>
<p>Problem-Solving Strategies. Arthur Engel, 10.1007/b976821998SpringerNew York, NY</p>
<p>Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou, 2024</p>
<p>Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Tianyu Liu, arXiv:2406.14024The reason behind good or bad: Towards a better mathematical verifier with natural language feedback. 2024arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu, Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. 2024</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. Xingzhang Ren, Xuancheng Ren2024</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 2022</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, Numinamath, 2024</p>
<p>Champ: A competition-level dataset for fine-grained analyses of llms' mathematical reasoning capabilities. Yujun Mao, Yoon Kim, Yilun Zhou, 2024</p>
<p>OpenAI. Gpt-4 technical report. 2023257532815</p>
<p>Learning to reason with llms. Openai, 2024</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. 2024</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. -Math Zhihong Omni, Peiyi Shao, Qihao Wang, Runxin Zhu, Junxiao Xu, Xiao Song, Haowei Bi, Mingchuan Zhang, Y K Zhang, Y Li, Daya Wu, Guo, 2024</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, Zhifang Sui, 2024</p>
<p>. Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, Sanjiv Kumar, 2022Preserving in-context learning ability in large language model fine-tuning</p>
<p>Benchmarking benchmark leakage in large language models. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu, arXiv:2404.188242024arXiv preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. 2024a</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024b</p>
<p>Can large language models always solve easy problems if they can solve harder ones?. Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, Zhifang Sui, 2024c</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin, Internlm-math: Open math large language models toward verifiable reasoning. 2024</p>
<p>The art and craft of problem solving. Paul Zeitz, 2017John Wiley &amp; Sons</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li, 2024</p>
<p>Omni-MATH Table 6: Main Result with different distinct difficulty levels. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, 2022Minif2f: a cross-system benchmark for formal olympiad-level mathematics</p>
<p>7b-RL 16.12 82.53 49.06 27.77 11.83 7.37 8.51 12.07 12.61 8.58 0.00 MetaLlama-3.1-70B-instruct 24.16 84.12 65.86 42.92 20.05 14.99 16.38 16.01 16.00 13.75 12.5Model Acc #D1 #D2 #D3 #D4 #D5 #D6 #D7 #D8 #D9 #D10 Vanilla Models DeepSeekMATH. </p>
<p>35.70 86.50 69.06 54.04 37.14 28.20 26.67 27.92 24.92 23.12 25.0Test-time Scaled Models Qwen2.5-MATH-7b-Instruct RM@8. </p>
<p>GPT-4O EVALUATION PROMPTS Figure 15 shows the detailed prompt we use in GPT-4o based evaluation. However, the few-shot prompt is too long to show in the paper. H Detailed, Evaluation Information, H , Instead, we will provide the total prompts in our additional materials</p>            </div>
        </div>

    </div>
</body>
</html>
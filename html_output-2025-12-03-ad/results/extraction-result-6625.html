<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-b2542a738b75ee9b7ce1a13d8b78f9095d212412</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b2542a738b75ee9b7ce1a13d8b78f9095d212412" target="_blank">Generate rather than Retrieve: Large Language Models are Strong Context Generators</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer, is called.</p>
                <p><strong>Paper Abstract:</strong> Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6625.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6625.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GENREAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate-then-Read</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that prompts a large autoregressive language model to generate contextual documents for a query and then conditions a reader model on those generated documents to produce answers; generation is used in place of retrieval from an external corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GENREAD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generator (InstructGPT or other LLM) produces one or multiple contextual passages d for a question q; a reader (either the same LLM zero-shot or a supervised FiD reader) conditions on the generated passages plus the question to produce the final answer. Generation can be diversified via nucleus sampling, human prompt variants, or clustering-based in‑context demonstration prompts to produce multiple perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Generator: InstructGPT (Davinci) ~175B; Reader (supervised experiments): FiD variants T5-770M and T5-3B (770M / 3B)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Parametric memory (knowledge stored in model parameters) plus ephemeral generated-context buffer (generated passages supplied as context); no external corpus index required</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages generated by the LLM (contextual documents) and implicit knowledge stored in model weights (parametric memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory is accessed by prompting/autoregressive generation conditioned on the question and optional in‑context demonstrations (including clustering-based prompts); generated passages are then provided as input to the reader (attention over the supplied passages). There is no separate persistent external read/write controller; updating knowledge requires model retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (Natural Questions, TriviaQA, WebQuestions), Fact Checking (FEVER, FM2), Dialogue (Wizard of Wikipedia / WoW)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge-intensive QA / fact verification / knowledge-grounded dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Zero-shot (GENREAD with InstructGPT generator + InstructGPT reader): NQ 28.0% EM, TriviaQA 59.0% EM, WebQ 24.6% EM (Table 1). Supervised (GENREAD producing documents; FiD-xl reader, clustering prompts): TriviaQA 71.6% EM, WebQ 54.4% EM, NQ 45.6% EM (Table 2, GENREAD (FiD-xl) (clustering)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Closed-book / no-doc baseline InstructGPT (direct answer generation, no contextual docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1). The paper treats these as the comparable ‘without generated-context’ results.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for QA; Accuracy for fact checking; F1 / Rouge-L for dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Generation avoids maintaining an external corpus and can produce more readable, focused passages, but: (1) generated documents are less diverse and exhibit diminishing marginal recall as number of generations grows; (2) generation is computationally expensive per query (FLOPs) compared to retrieval for large query volumes — crossover point estimated ~2,473 queries given the FLOP model in Appendix A.6; (3) generated text can hallucinate or be stale (temporal mismatch); (4) generating many documents is slower and costlier than retrieving many documents; clustering-based prompting mitigates diversity issues but adds preprocessing (encoding/clustering) cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on time-dependent questions due to stale parametric knowledge; hallucinations in generated passages can cause incorrect answers; lower coverage/diversity of multiple generated documents compared to retrieved documents (answer-coverage lower without clustering prompts); scaling to many distinct documents per query is challenging; updating knowledge requires model retraining rather than swapping external documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang. GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6625.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6625.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPR+FiD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense Passage Retriever + Fusion-in-Decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieve-then-read pipeline where DPR encodes and indexes passages from an external corpus into dense vectors and retrieves top-K passages via nearest-neighbor search; FiD (Fusion-in-Decoder) reads multiple retrieved passages jointly to generate an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dense passage retrieval for open-domain question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DPR + FiD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DPR (two-tower dense retriever) creates dense embeddings for each passage and stores them in an index (external memory). Retrieval is done by nearest-neighbor similarity (dot product) between query and passage embeddings; FiD fuses multiple retrieved passages in the decoder to produce an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>DPR: BERT-base (110M) in the paper's accounting; FiD reader used in experiments: T5-770M and T5-3B (770M / 3B)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External document store indexed as dense vector embeddings (vector database / index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Dense passage embeddings (fixed-length vectors) with associated raw text passages in the corpus</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Similarity search / nearest-neighbor retrieval over dense embeddings (ANN); updating memory requires re-encoding documents and reindexing</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (TriviaQA, WebQ, NQ) and used as baseline for fact checking and dialogue tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Retrieval-augmented knowledge-intensive QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported DPR baseline performance (from Table 2 / cited DPR paper): DPR (100 docs) + FiD reported in literature: TriviaQA 56.8% EM, WebQ 41.1% EM, NQ 41.5% EM (avg 46.5). In the paper's own experiments using FiD with DPR (10 documents): FiD-1 (DPR, Wikipedia, 10 docs) achieved TriviaQA 61.9% EM, WebQ 48.1% EM, NQ 46.7% EM (Table 2). Zero-shot retrieval + InstructGPT reported: DPR + InstructGPT: NQ 29.1% EM, TriviaQA 53.8% EM, WebQ 20.2% EM (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Closed-book InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1) — this serves as the comparison when external memory is absent. Note: FiD requires documents; a direct FiD 'no memory' ablation is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Indexing the entire corpus requires a large upfront cost (paper provides FLOP estimates: encoding ~21M Wikipedia docs once is expensive — see Appendix A.6). Two-tower dense retrievers encode queries and documents separately which limits deep interactions between query and passage representations; retrieval can be efficient at scale per-query but requires maintaining and re-encoding an index for updates. Retrieval + FiD with many documents increases memory and training compute (e.g., using 100 docs is costly).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Retrieved chunks may be noisy or only partially relevant (fixed chunking); retriever models can give false positives that increase recall but not answer precision; temporal mismatch if the indexed corpus is out-of-date (but easier to update by swapping corpus than retraining an LLM). Dense retrievers are limited by embedding dimensionality and fixed encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. Dense passage retrieval for open-domain question answering. (Karpukhin et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6625.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6625.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that retrieves relevant passages from an external corpus and conditions a generative model on the retrieved passages to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieves documents from an external corpus (retriever component) and conditions a generative sequence model on the retrieved material to produce answers; the retrieved material serves as external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported example in paper: RAG 400M (Table 2 row for RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External document store (Wikipedia) via retriever</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages (and typically embeddings for retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Retriever fetches top-K documents for each query; generator conditions on those documents (usually via concatenation or encoder-decoder attention)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (benchmarks used in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge-intensive QA / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline (Table 2): RAG (400M, 10 docs) – TriviaQA 56.1% EM, WebQ 45.2% EM, NQ 44.5% EM (avg 48.6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not directly ablated in this paper for RAG specifically; closed-book baselines (InstructGPT no docs) reported elsewhere are: NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>This paper cites RAG in related work but does not provide a detailed tradeoff analysis specific to RAG; general retrieval tradeoffs (indexing cost, updateability, shallow two-tower interactions) are discussed more broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly analyzed in this paper; RAG shares general retrieval limitations (dependence on corpus coverage, potential retrieval noise).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. (Lewis et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6625.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6625.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25+InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 sparse retrieval + InstructGPT reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline pipeline using BM25 (sparse term-matching) to retrieve documents from Wikipedia which are then provided to InstructGPT to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BM25 + InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>BM25 builds an inverted index and retrieves top-K documents by term-matching scores; retrieved raw passages are concatenated with the question and provided to InstructGPT for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BM25: non‑neural sparse retrieval algorithm (no model parameters); InstructGPT ~175B used as reader</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External inverted-index corpus (sparse retrieval on Wikipedia)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages indexed by term statistics (inverted index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Term-frequency / inverse-document-frequency based scoring (BM25) to rank passages; top-K retrieved for reading</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NQ, TriviaQA, WebQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Retrieval-augmented QA (sparse retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Zero-shot reported (Table 1): BM25 + InstructGPT – NQ 19.7% EM, TriviaQA 52.2% EM, WebQ 15.8% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1); note that BM25+InstructGPT performed worse than the no-doc LLM on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>BM25 is simple and cheap but relies on keyword overlap and can return noisy chunks; does not capture deeper semantics compared to dense retrieval; integration with LLMs may not always improve over closed-book generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Can underperform closed-book LLMs for some benchmarks (seen in Table 1); limited by vocabulary overlap and the fixed chunking of documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: BM25 and beyond. (Robertson et al., 2009).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6625.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6625.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contriever+InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contriever (unsupervised dense retriever) + InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised dense retrieval model (Contriever) used to fetch relevant documents via vector similarity; retrieved passages are then read by InstructGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised dense information retrieval with contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Contriever + InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Contriever encodes passages into dense embeddings using contrastive unsupervised training; retrieval is performed by nearest-neighbor search in the embedding space; retrieved raw passages are supplied to InstructGPT for answer production.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper for Contriever; InstructGPT reader ~175B</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External vector-indexed corpus (dense retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Dense passage embeddings and raw text passages</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Embedding similarity search (ANN) to retrieve top-K passages</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NQ, TriviaQA, WebQ) as baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Retrieval-augmented QA (unsupervised dense retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Zero-shot baseline (Table 1): Contriever + InstructGPT – NQ 18.0% EM, TriviaQA 51.3% EM, WebQ 16.6% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Unsupervised dense retriever avoids supervised annotation for retrieval but (as two-tower models) only captures shallow interactions between query and document representations; performance can lag supervised retrievers depending on the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In these experiments, Contriever-based pipelines performed worse than GENREAD and some supervised retriever+reader baselines on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gautier Izacard, Mathild Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave. Unsupervised dense information retrieval with contrastive learning. (Izacard et al., 2022a).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6625.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6625.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google+InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google search engine retrieval + InstructGPT reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Google search (web search engine) to obtain relevant textual documents which are then provided to InstructGPT to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Google Search + InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External web search (Google) is used to obtain relevant documents (external memory is the indexed web); returned documents are read by InstructGPT to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>InstructGPT ~175B (reader); Google search is an external service (size unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External web-index (search engine index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw web pages / snippets returned by search queries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Search engine retrieval (query to search API) returning ranked documents/snippets; documents then supplied to the reader</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NQ, TriviaQA, WebQ) as baselines</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Retrieval-augmented QA via web search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Zero-shot (Table 1): Google + InstructGPT – NQ 28.8% EM, TriviaQA 58.8% EM, WebQ 20.4% EM. Supervised (Table 2): FiD-xl (Google search, 10 docs) – TriviaQA 70.1% EM, WebQ 53.6% EM, NQ 45.0% EM (avg 56.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Web search can provide fresh/up-to-date information (mitigates temporality issues) but depends on external service; quality varies and may return noisy or irrelevant results; integration with LLM reader costs additional latency and may require query engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In some supervised experiments Google+FiD performed comparably to GENREAD but did not always outperform DPR; web search quality and recency can help temporal questions but retrieval quality still limits performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generate rather than Retrieve: Large Language Models are Strong Context Generators', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dense passage retrieval for open-domain question answering. <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Generated knowledge prompting for commonsense reasoning <em>(Rating: 1)</em></li>
                <li>Autoregressive entity retrieval <em>(Rating: 1)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6625",
    "paper_id": "paper-b2542a738b75ee9b7ce1a13d8b78f9095d212412",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "GENREAD",
            "name_full": "Generate-then-Read",
            "brief_description": "A pipeline that prompts a large autoregressive language model to generate contextual documents for a query and then conditions a reader model on those generated documents to produce answers; generation is used in place of retrieval from an external corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GENREAD",
            "agent_description": "Generator (InstructGPT or other LLM) produces one or multiple contextual passages d for a question q; a reader (either the same LLM zero-shot or a supervised FiD reader) conditions on the generated passages plus the question to produce the final answer. Generation can be diversified via nucleus sampling, human prompt variants, or clustering-based in‑context demonstration prompts to produce multiple perspectives.",
            "model_size": "Generator: InstructGPT (Davinci) ~175B; Reader (supervised experiments): FiD variants T5-770M and T5-3B (770M / 3B)",
            "memory_used": true,
            "memory_type": "Parametric memory (knowledge stored in model parameters) plus ephemeral generated-context buffer (generated passages supplied as context); no external corpus index required",
            "memory_representation": "Raw text passages generated by the LLM (contextual documents) and implicit knowledge stored in model weights (parametric memory)",
            "memory_access_mechanism": "Memory is accessed by prompting/autoregressive generation conditioned on the question and optional in‑context demonstrations (including clustering-based prompts); generated passages are then provided as input to the reader (attention over the supplied passages). There is no separate persistent external read/write controller; updating knowledge requires model retraining.",
            "task_name": "Open-domain QA (Natural Questions, TriviaQA, WebQuestions), Fact Checking (FEVER, FM2), Dialogue (Wizard of Wikipedia / WoW)",
            "task_category": "Knowledge-intensive QA / fact verification / knowledge-grounded dialogue",
            "performance_with_memory": "Zero-shot (GENREAD with InstructGPT generator + InstructGPT reader): NQ 28.0% EM, TriviaQA 59.0% EM, WebQ 24.6% EM (Table 1). Supervised (GENREAD producing documents; FiD-xl reader, clustering prompts): TriviaQA 71.6% EM, WebQ 54.4% EM, NQ 45.6% EM (Table 2, GENREAD (FiD-xl) (clustering)).",
            "performance_without_memory": "Closed-book / no-doc baseline InstructGPT (direct answer generation, no contextual docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1). The paper treats these as the comparable ‘without generated-context’ results.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for QA; Accuracy for fact checking; F1 / Rouge-L for dialogue",
            "tradeoffs_reported": "Generation avoids maintaining an external corpus and can produce more readable, focused passages, but: (1) generated documents are less diverse and exhibit diminishing marginal recall as number of generations grows; (2) generation is computationally expensive per query (FLOPs) compared to retrieval for large query volumes — crossover point estimated ~2,473 queries given the FLOP model in Appendix A.6; (3) generated text can hallucinate or be stale (temporal mismatch); (4) generating many documents is slower and costlier than retrieving many documents; clustering-based prompting mitigates diversity issues but adds preprocessing (encoding/clustering) cost.",
            "limitations_or_failure_cases": "Fails on time-dependent questions due to stale parametric knowledge; hallucinations in generated passages can cause incorrect answers; lower coverage/diversity of multiple generated documents compared to retrieved documents (answer-coverage lower without clustering prompts); scaling to many distinct documents per query is challenging; updating knowledge requires model retraining rather than swapping external documents.",
            "citation": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang. GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS.",
            "uuid": "e6625.0",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DPR+FiD",
            "name_full": "Dense Passage Retriever + Fusion-in-Decoder",
            "brief_description": "A retrieve-then-read pipeline where DPR encodes and indexes passages from an external corpus into dense vectors and retrieves top-K passages via nearest-neighbor search; FiD (Fusion-in-Decoder) reads multiple retrieved passages jointly to generate an answer.",
            "citation_title": "Dense passage retrieval for open-domain question answering.",
            "mention_or_use": "use",
            "agent_name": "DPR + FiD",
            "agent_description": "DPR (two-tower dense retriever) creates dense embeddings for each passage and stores them in an index (external memory). Retrieval is done by nearest-neighbor similarity (dot product) between query and passage embeddings; FiD fuses multiple retrieved passages in the decoder to produce an answer.",
            "model_size": "DPR: BERT-base (110M) in the paper's accounting; FiD reader used in experiments: T5-770M and T5-3B (770M / 3B)",
            "memory_used": true,
            "memory_type": "External document store indexed as dense vector embeddings (vector database / index)",
            "memory_representation": "Dense passage embeddings (fixed-length vectors) with associated raw text passages in the corpus",
            "memory_access_mechanism": "Similarity search / nearest-neighbor retrieval over dense embeddings (ANN); updating memory requires re-encoding documents and reindexing",
            "task_name": "Open-domain QA (TriviaQA, WebQ, NQ) and used as baseline for fact checking and dialogue tasks",
            "task_category": "Retrieval-augmented knowledge-intensive QA",
            "performance_with_memory": "Reported DPR baseline performance (from Table 2 / cited DPR paper): DPR (100 docs) + FiD reported in literature: TriviaQA 56.8% EM, WebQ 41.1% EM, NQ 41.5% EM (avg 46.5). In the paper's own experiments using FiD with DPR (10 documents): FiD-1 (DPR, Wikipedia, 10 docs) achieved TriviaQA 61.9% EM, WebQ 48.1% EM, NQ 46.7% EM (Table 2). Zero-shot retrieval + InstructGPT reported: DPR + InstructGPT: NQ 29.1% EM, TriviaQA 53.8% EM, WebQ 20.2% EM (Table 1).",
            "performance_without_memory": "Closed-book InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1) — this serves as the comparison when external memory is absent. Note: FiD requires documents; a direct FiD 'no memory' ablation is not provided.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Indexing the entire corpus requires a large upfront cost (paper provides FLOP estimates: encoding ~21M Wikipedia docs once is expensive — see Appendix A.6). Two-tower dense retrievers encode queries and documents separately which limits deep interactions between query and passage representations; retrieval can be efficient at scale per-query but requires maintaining and re-encoding an index for updates. Retrieval + FiD with many documents increases memory and training compute (e.g., using 100 docs is costly).",
            "limitations_or_failure_cases": "Retrieved chunks may be noisy or only partially relevant (fixed chunking); retriever models can give false positives that increase recall but not answer precision; temporal mismatch if the indexed corpus is out-of-date (but easier to update by swapping corpus than retraining an LLM). Dense retrievers are limited by embedding dimensionality and fixed encoders.",
            "citation": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. Dense passage retrieval for open-domain question answering. (Karpukhin et al., 2020).",
            "uuid": "e6625.1",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A hybrid approach that retrieves relevant passages from an external corpus and conditions a generative model on the retrieved passages to produce answers.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "agent_name": "RAG",
            "agent_description": "Retrieves documents from an external corpus (retriever component) and conditions a generative sequence model on the retrieved material to produce answers; the retrieved material serves as external memory.",
            "model_size": "Reported example in paper: RAG 400M (Table 2 row for RAG)",
            "memory_used": true,
            "memory_type": "External document store (Wikipedia) via retriever",
            "memory_representation": "Raw text passages (and typically embeddings for retrieval)",
            "memory_access_mechanism": "Retriever fetches top-K documents for each query; generator conditions on those documents (usually via concatenation or encoder-decoder attention)",
            "task_name": "Open-domain QA (benchmarks used in comparisons)",
            "task_category": "Knowledge-intensive QA / retrieval-augmented generation",
            "performance_with_memory": "Reported baseline (Table 2): RAG (400M, 10 docs) – TriviaQA 56.1% EM, WebQ 45.2% EM, NQ 44.5% EM (avg 48.6).",
            "performance_without_memory": "Not directly ablated in this paper for RAG specifically; closed-book baselines (InstructGPT no docs) reported elsewhere are: NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).",
            "has_comparative_results": false,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "This paper cites RAG in related work but does not provide a detailed tradeoff analysis specific to RAG; general retrieval tradeoffs (indexing cost, updateability, shallow two-tower interactions) are discussed more broadly.",
            "limitations_or_failure_cases": "Not directly analyzed in this paper; RAG shares general retrieval limitations (dependence on corpus coverage, potential retrieval noise).",
            "citation": "Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. (Lewis et al., 2020).",
            "uuid": "e6625.2",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "BM25+InstructGPT",
            "name_full": "BM25 sparse retrieval + InstructGPT reader",
            "brief_description": "A baseline pipeline using BM25 (sparse term-matching) to retrieve documents from Wikipedia which are then provided to InstructGPT to answer questions.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond.",
            "mention_or_use": "use",
            "agent_name": "BM25 + InstructGPT",
            "agent_description": "BM25 builds an inverted index and retrieves top-K documents by term-matching scores; retrieved raw passages are concatenated with the question and provided to InstructGPT for answer generation.",
            "model_size": "BM25: non‑neural sparse retrieval algorithm (no model parameters); InstructGPT ~175B used as reader",
            "memory_used": true,
            "memory_type": "External inverted-index corpus (sparse retrieval on Wikipedia)",
            "memory_representation": "Raw text passages indexed by term statistics (inverted index)",
            "memory_access_mechanism": "Term-frequency / inverse-document-frequency based scoring (BM25) to rank passages; top-K retrieved for reading",
            "task_name": "Open-domain QA (NQ, TriviaQA, WebQ)",
            "task_category": "Retrieval-augmented QA (sparse retrieval)",
            "performance_with_memory": "Zero-shot reported (Table 1): BM25 + InstructGPT – NQ 19.7% EM, TriviaQA 52.2% EM, WebQ 15.8% EM.",
            "performance_without_memory": "InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1); note that BM25+InstructGPT performed worse than the no-doc LLM on some benchmarks.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "BM25 is simple and cheap but relies on keyword overlap and can return noisy chunks; does not capture deeper semantics compared to dense retrieval; integration with LLMs may not always improve over closed-book generation.",
            "limitations_or_failure_cases": "Can underperform closed-book LLMs for some benchmarks (seen in Table 1); limited by vocabulary overlap and the fixed chunking of documents.",
            "citation": "Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: BM25 and beyond. (Robertson et al., 2009).",
            "uuid": "e6625.3",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Contriever+InstructGPT",
            "name_full": "Contriever (unsupervised dense retriever) + InstructGPT",
            "brief_description": "An unsupervised dense retrieval model (Contriever) used to fetch relevant documents via vector similarity; retrieved passages are then read by InstructGPT.",
            "citation_title": "Unsupervised dense information retrieval with contrastive learning.",
            "mention_or_use": "use",
            "agent_name": "Contriever + InstructGPT",
            "agent_description": "Contriever encodes passages into dense embeddings using contrastive unsupervised training; retrieval is performed by nearest-neighbor search in the embedding space; retrieved raw passages are supplied to InstructGPT for answer production.",
            "model_size": "Not specified in this paper for Contriever; InstructGPT reader ~175B",
            "memory_used": true,
            "memory_type": "External vector-indexed corpus (dense retrieval)",
            "memory_representation": "Dense passage embeddings and raw text passages",
            "memory_access_mechanism": "Embedding similarity search (ANN) to retrieve top-K passages",
            "task_name": "Open-domain QA (NQ, TriviaQA, WebQ) as baseline comparisons",
            "task_category": "Retrieval-augmented QA (unsupervised dense retrieval)",
            "performance_with_memory": "Zero-shot baseline (Table 1): Contriever + InstructGPT – NQ 18.0% EM, TriviaQA 51.3% EM, WebQ 16.6% EM.",
            "performance_without_memory": "InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Unsupervised dense retriever avoids supervised annotation for retrieval but (as two-tower models) only captures shallow interactions between query and document representations; performance can lag supervised retrievers depending on the dataset.",
            "limitations_or_failure_cases": "In these experiments, Contriever-based pipelines performed worse than GENREAD and some supervised retriever+reader baselines on several benchmarks.",
            "citation": "Gautier Izacard, Mathild Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave. Unsupervised dense information retrieval with contrastive learning. (Izacard et al., 2022a).",
            "uuid": "e6625.4",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Google+InstructGPT",
            "name_full": "Google search engine retrieval + InstructGPT reader",
            "brief_description": "Use of Google search (web search engine) to obtain relevant textual documents which are then provided to InstructGPT to answer queries.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Google Search + InstructGPT",
            "agent_description": "External web search (Google) is used to obtain relevant documents (external memory is the indexed web); returned documents are read by InstructGPT to produce answers.",
            "model_size": "InstructGPT ~175B (reader); Google search is an external service (size unspecified)",
            "memory_used": true,
            "memory_type": "External web-index (search engine index)",
            "memory_representation": "Raw web pages / snippets returned by search queries",
            "memory_access_mechanism": "Search engine retrieval (query to search API) returning ranked documents/snippets; documents then supplied to the reader",
            "task_name": "Open-domain QA (NQ, TriviaQA, WebQ) as baselines",
            "task_category": "Retrieval-augmented QA via web search",
            "performance_with_memory": "Zero-shot (Table 1): Google + InstructGPT – NQ 28.8% EM, TriviaQA 58.8% EM, WebQ 20.4% EM. Supervised (Table 2): FiD-xl (Google search, 10 docs) – TriviaQA 70.1% EM, WebQ 53.6% EM, NQ 45.0% EM (avg 56.2).",
            "performance_without_memory": "InstructGPT (no docs): NQ 20.9% EM, TriviaQA 57.5% EM, WebQ 18.6% EM (Table 1).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Web search can provide fresh/up-to-date information (mitigates temporality issues) but depends on external service; quality varies and may return noisy or irrelevant results; integration with LLM reader costs additional latency and may require query engineering.",
            "limitations_or_failure_cases": "In some supervised experiments Google+FiD performed comparably to GENREAD but did not always outperform DPR; web search quality and recency can help temporal questions but retrieval quality still limits performance.",
            "citation": "",
            "uuid": "e6625.5",
            "source_info": {
                "paper_title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dense passage retrieval for open-domain question answering.",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Generated knowledge prompting for commonsense reasoning",
            "rating": 1
        },
        {
            "paper_title": "Autoregressive entity retrieval",
            "rating": 1
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond.",
            "rating": 1
        }
    ],
    "cost": 0.024867499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS</h1>
<p>Wenhao Yu ${ }^{1 <em>}$, Dan Iter ${ }^{2}$, Shuohang Wang ${ }^{2}$, Yichong Xu ${ }^{2}$, Mingxuan Ju ${ }^{1}$, Soumya Sanyal ${ }^{3 </em>}$, Chenguang Zhu ${ }^{2}$, Michael Zeng ${ }^{2}$, Meng Jiang ${ }^{1}$<br>${ }^{1}$ University of Notre Dame ${ }^{2}$ Microsoft Cognitive Service Research<br>${ }^{3}$ University of Southern California<br>${ }^{1}$ wyu1@nd.edu; ${ }^{2}$ iterdan@microsoft.com</p>
<h4>Abstract</h4>
<p>Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GENREAD), which first prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-thenread pipeline $D P R$-FiD by +4.0 and +3.9 , without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.</p>
<h2>1 INTRODUCTION</h2>
<p>Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require access to a large amount of world or domain knowledge (Petroni et al., 2021). These tasks are even challenging for humans without access to an external knowledge source such as Wikipedia. A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from Wikipedia and then conditions the prediction of the answer on these documents along with the question (Karpukhin et al., 2020; Lewis et al., 2020; Izacard \&amp; Grave, 2021). Nevertheless, these methods mainly suffer from three drawbacks. First, candidate documents for retrieval are chunked (e.g., 100 words) and fixed, so the retrieved documents might contain noisy information that is irrelevant to the question. Second, the representations of questions and documents are typically obtained independently in modern two-tower dense retrieval models (Karpukhin et al., 2020), leading to only shallow interactions captured between them (Khattab et al., 2021). Third, document retrieval over a large corpus requires the retriever model to first encode all candidate documents and store representations for each document. These two operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this paper, we propose to leverage large language models, such as InstructGPT (Ouyang et al., 2022), to directly generate contextual documents for a given question, instead of retrieving relevant documents from an external corpus, such as Wikipedia. Our approach has two main advantages. First, we show that generated contextual documents contain the correct answer more often than the top retrieved documents. We believe this is because large language models generate contextual documents by performing deep token-level cross-attention between all the question and document contents, resulting in generated documents that are more specific to the question than retrieved documents. Second, we show that our approach significantly outperforms directly generating answers from large language models despite not incorporating any new external information. This is mainly because the task of generating document-level contexts is close to the objective of causal language modeling pre-training, so the world knowledge stored in the model parameters can be better utilized.</p>
<p>We show, on multiple datasets, that generated documents are more likely to contain correct answers than the top retrieved documents. Notably, in dense retrieval methods, as more documents are retrieved, the recall of documents containing the correct answer increases (Karpukhin et al., 2020). However, the recall performance does not scale as well with generated documents because even with sampling methods, generated documents tend to contain duplicate information. In order to improve the recall performance of generated documents, we propose a novel clustering-based prompt method. We synthesize a prompt with in-context demonstrations of question-document pairs sampled from diverse clusters. These prompts result in generated documents that cover different perspectives of the question and improve the scaling of performance as more documents are generated per question.</p>
<p>In contrast to the retrieve-then-read pipeline, our method is essentially a generate-then-read pipeline. Specifically, it first prompts a large language model to generate contextual documents based on a given question, and then reads the generated document to produce the final answer. The reader can still be a large model (e.g., InstructGPT (Ouyang et al., 2022)) used under a zero-shot setting, or a small one (e.g., FiD (Izacard \&amp; Grave, 2021)) fine-tuned with generated documents on the training split of the target dataset. We evaluate our proposed method on three different knowledge-intensive tasks and demonstrate its effectiveness on both zero-shot and supervised settings.</p>
<p>Overall, our main contributions can be summarized as follows:</p>
<ol>
<li>We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks, i.e., replacing the process of retrieving documents from Wikipedia or searching for related documents on Google, by prompting a large language model to generate relevant contextual documents.</li>
<li>We propose a novel clustering-based prompting approach to generate multiple diverse contextual documents that increases the likelihood of covering the correct answer. We demonstrate this approach can significantly improve performance on end QA and other downstream tasks.</li>
<li>We conduct extensive experiments with three knowledge-intensive NLP tasks under both zeroshot and supervised settings. Notably, our method can match or even outperform retrieve-then-read pipeline methods, without retrieving any documents from any external knowledge source.</li>
</ol>
<h1>2 Related Work</h1>
<h3>2.1 Knowledge-intensive NLP via Retrieve-then-Read Pipeline.</h3>
<p>Mainstream methods for solving knowledge-intensive NLP tasks employ a retrieve-then-read model pipeline. Given a question, this model first leverages a retriever over a large evidence corpus (e.g. Wikipedia) to fetch a set of relevant documents that may contain the answer. A reader is then used to peruse the retrieved documents and predict an answer. Recent follow-up work has mainly focused on improving the retriever (Karpukhin et al., 2020; Qu et al., 2021; Sachan et al., 2022) or the reader (Izacard \&amp; Grave, 2021; Cheng et al., 2021; Yu et al., 2022), or training the system end-toend (Lewis et al., 2020; Singh et al., 2021). Early retrieval methods mainly employed sparse retrievers, such as BM25 (Chen et al., 2017). Recently, ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020) have revolutionized the field by utilizing dense contextualized vectors for document indexing, leading to superior performance to traditional approaches. We propose an alternative approach which forgoes retrieval, instead extracting the knowledge from the model parameters of a large language model. We show that our approach is can be combine with dense retrievers to outperform both methods independently. Our method can also be combined with any reader mechanism, allowing generated context documents to be plugged into any current knowledge-intensive NLP pipelines.</p>
<h1>2.2 Generator as Retriever for Obtaining Contextual Documents.</h1>
<p>Recent works have investigated using auto-regressive language models to generate identifier strings for documents, as an intermediate target for retrievals, such as entity names (De Cao et al., 2020) or distinctive n-grams that can be mapped to full passages (Bevilacqua et al., 2022). However, one needs to create the identifiers, hence the structure was not thoroughly evaluated on a large-scale benchmark (Bevilacqua et al., 2022). Other works have demonstrated that the knowledge stored in the parameters of pre-trained language models could be "retrieved" to some extent by directly generating text (Petroni et al., 2019; Roberts et al., 2020). However, the previous work only used generation for query expansion (Mao et al., 2021), which did not exploit the potential of directly generating contextual documents for open-domain questions. Different from the above approaches that aimed to train a generator model to produce contextual document identifiers (which is still using the original Wikipedia text) or provide data augmentation to retrievers, our work directly generates contextual documents for given questions.</p>
<h3>2.3 NLP Models Enhanced by Large Language Model Outputs.</h3>
<p>A line of recent work has shown that relevant knowledge can be elicited from large language models, especially for those domains that lack appropriate knowledge bases with sufficient coverage (Liu et al., 2022b; Fang et al., 2022). For example, Liu et al. (2022b) proposed leveraging GPT-3 to generate relevant contexts, then providing the contexts as additional input when answering a commonsense question. Another line of work focused on prompting a large language model to generate a series of intermediate reasoning steps, often referred to as chain-of-thought (Wei et al., 2022b; Kojima et al., 2022; Li et al., 2022). The prompt consists of an instruction (e.g., Let's think step by step!), a few demonstrations that are fixed for each task, and a new-question placeholder. The demonstrations are human-written, and each consists of a question in the style of the task and a series of intermediate reasoning steps that is helpful for answering the question. Our work does not require any human annotation, but adds to this line of work of leveraging model generated text to guide further generations. In our case, we apply this approach to knowledge-intensive tasks, which have not been explored by previous work.</p>
<h2>3 Proposed Method</h2>
<p>In this section, we present details of our proposed novel generate-then-read (GENREAD) pipeline for solving various knowledge-intensive tasks. Specifically, it first prompts a large language model to generate contextual documents with respect to a given query, then reads the generated documents to predict the final answer. The reader can either be a large model (e.g., InstructGPT) used for the zero-shot setting, or a small one (e.g., FiD) fine-tuned with generated documents on the training split of the target dataset. We introduce the zero-shot setting in $\S 3.1$ and supervised setting in $\S 3.2$.</p>
<h3>3.1 Zero-Shot Setting</h3>
<p>Under the zero-shot setting, there is no training data - neither questions nor contextual documents. When tested on the open-domain QA task, most existing large language models directly encode the given question and predict the answer (Brown et al., 2020; Du et al., 2022; Chowdhery et al., 2022). Specifically, the question $q$, associated with some text prompt, is input to the model, which then generates the answer, denoted as $p(a \mid q, \theta)$, where $\theta$ represents the pre-trained model parameters. In practice, the maximum a posteriori estimation (MAP) is the final answer, i.e., $\hat{a}=\arg \max _{a} p(a \mid q, \theta)$. However, this way of directly asking large language models to output answers often leads to poor performance, as it leaves a considerable amount of additional world knowledge unexploited (Levine et al., 2022). On the contrary, the zero-shot retrieve-then-read pipeline first uses an off-the-shelf retriever to fetch relevant documents from an external knowledge source such as Wikipedia, then asks the large language model to read the documents and predict the answer.</p>
<p>In this work, we improve the performance by introducing an additional auxiliary generated document variable $d$, and then extend the model to have the form $p(a \mid q)=\sum_{i} p(a \mid d_{i}, q) p\left(d_{i} \mid q\right)$. In practice, we cannot sum over all possible documents $d$. Therefore, the most common approach is to compute</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overall framework of clustering-based prompting method. It leverages distinct questiondocument pairs sampled from each embedding cluster as in-context demonstrations to prompt a large language model to generate diverse documents, then read the documents to predict an answer.
the MAP estimate $\hat{d}=\arg \max \hat{p}(d)$ using beam search, and then to approximate the sum over $d$ with this single value. This two step approach, we label it as a generate-then-read pipeline.</p>
<p>STEP 1: GENERATE. In this step, we first prompt a large language model (e.g., InstructGPT (Ouyang et al., 2022)) to generate documents based on the given question. For example, the input to the language model could be "Generate a background document to answer the given question. {question placeholder}". We can use any decoding strategy (e.g., greedy decoding, beam search), but we used greedy decoding throughout the zero-shot experiments for simplicity and reproducibility.</p>
<p>STEP 2: READ. In the second step, we use generated sentence $\hat{d}$ along with the input question to produce the final answer from the large language model. This is actually the same setting as "zeroshot" reading comprehension, as widely studied in existing works (Brown et al., 2020; Lazaridou et al., 2022). We choose appropriate prompts from P3 (Bach et al., 2022), such as "Refer to the passage below and answer the following question. Passage: {background placeholder} Question: {question placeholder}". Finally, the language model is fed the prompted text to generate an answer.</p>
<h1>3.2 SUPERVISED SETTING</h1>
<p>Although large language models demonstrate impressive performance on zero-shot learning abilities, their performance still lag behind the supervised setting. Therefore, we also explore how the generated documents from large language models can benefit the supervised setting. As directly fine-tuning large language models on downstream datasets could be prohibitively expensive, we leverage a small reader model such as FiD to peruse the generated documents under the supervised setting.</p>
<p>Under the supervised setting, scaling the size of retrieved documents can lead to better performance (Karpukhin et al., 2020; Izacard \&amp; Grave, 2021). This is mainly because retrieving more documents can cover more relevant information and knowledge, i.e., a higher recall score. Nevertheless, asking a large language model to generate multiple high-quality contextual documents is a challenging task. Dense retrieval methods can fetch multiple documents covering different perspectives of the answer. Compared to dense retrievers, simply prompting a large language model to generate multiple contextual documents often leads to low knowledge coverage, since the contents generated by multiple decoding passes from the same input tend to be similar. Sampling decoding methods, such as nucleus sampling ${ }^{1}$ (Holtzman et al., 2020) can diversify the generation process to some extent, but the knowledge content of generated texts still tends to be highly repetitive when used to generate documents for a given question. We further propose two novel solutions, including diverse human prompts and clustering-based prompts, which will be elaborated on in this section.</p>
<h3>3.2.1 DIVERSE HUMAN PROMPTS</h3>
<p>In order to avoid similar token distributions under a single prompt, we ask human annotators to provide different prompts, in order to make the generated document diverse. This method is simple, but can effectively vary the token distribution during generation. In the experiments, we empirically found this method can bring improvement to the retrieval performance (Figure 2). However, this</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>method suffers from two drawbacks. On one hand, it requires human annotators to write different prompts, which cannot be easily generalized to different knowledge-intensive tasks. On the other hand, different large language models might be sensitive to different prompt words, which might cause a set of good prompt words not work on a different large language model.</p>
<h1>3.2.2 Clustering-based Prompts</h1>
<p>To increase knowledge coverage in generated documents, we propose a novel clustering-based prompt method. It first clusters the representations of a set of documents into $K$ classes ( $K=2$ in Figure 1), where the number of classes is equal to the number of documents that need to be generated in the end. Next, it randomly selects $n$ question-document pairs ( $n=5$ in Figure 1) from each cluster. Lastly, a large language model presents the different $n$ question-document pairs as in-context demonstrations for generating documents to a given question. In this way, large language models are based on different distributions of examples, hence resulting in generated documents covering different perspectives. We show this in Figure 1 and illustrate the details of each step as follows.</p>
<p>Step 1: Get One Initial Document Per Question. Similar to the zero-shot setting, we first ask a large language model to generate one contextual document $d$ for each question $q \in \mathcal{Q}$, where $\mathcal{Q}$ is the set of questions in the training split. Alternatively, we can use an unsupervised retriever (e.g., BM25) to obtain a document from Wikipedia. We now have a question-document pair set $\left{q_{i}, d_{i}\right}_{i=1}^{|\mathcal{Q}|}$.</p>
<p>Step 2: Encode each Document, Do K-means Clustering. We then use a large language model (i.e., GPT-3) to encode each question-document pair, i.e., $\mathbf{e}<em i="i">{i}=\text { GPT-3 }\left(\left[q</em>}, d_{i}\right]\right)$, resulting in a 12,288-dimensional vector per document. Then, we use K-means to cluster all embedding vectors $\left{\mathbf{e<em i="1">{i}\right}</em>$ into $K$ sets, so each question-document pair is assigned a unique cluster id $c \in{1, \ldots, K}$. We vary the number of $K$ in the experiments, which will be illustrated in Figure 2.}^{|\mathcal{Q}|</p>
<p>Step 3: Sample and Generate $K$ Documents. Lastly, we sample $n$ question-document pairs from each cluster $c$, denoted as $\left{q_{c 1}, d_{c 1} ; q_{c 2}, d_{c 2} ; \ldots ; q_{c n}, d_{c n}\right}$, in which $n$ is a hyperparameter ${ }^{2}$. Then, the $n$ sampled question-document pairs from the same cluster serve as in-context demonstrations for the large language model to generate a contextual document. For example, the input to the large language model could be " $\left{q_{c 1}\right.$ placeholder} $\left{d_{c 1}\right.$ placeholder} $\left.\ldots\left{q_{c n}\right.\right.$ placeholder} $\left{d_{c n}\right.$ placeholder} {input question placeholder}". By enumerating the sampled documents in these $K$ clusters, we can finally get $K$-generated documents. By conditioning on different sampled in-context demonstrations collected from different clusters, the large language model has been biased for different perspectives. Although these different perspectives exist in a latent manner, we empirically show it works well in practice, by comparing it with sampling methods, diverse human prompts (Figure 2 and Table 2) and randomly sampling $n$ pairs from the entire dataset (Table 11).</p>
<h2>4 EXPERIMENTS</h2>
<p>In this section, we conduct comprehensive experiments on three knowledge-intensive NLP tasks, including open-domain QA (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and WebQ (Berant et al., 2013)), fact checking (FEVER (Thorne et al., 2018) and FM2 (Eisenschlos et al., 2021)) and open-domain dialogue system (WoW (Dinan et al., 2019)). More detailed dataset information can be found in Appendix A.1. To evaluate the model performance, we use exact match (EM) score for evaluating open-domain QA (Zhu et al., 2021). An answer is considered correct if and only if its normalized form has a match in the acceptable answer list. We also employ Recall@K (R@K) as an intermediate evaluation metric, measured as the percentage of top-K retrieved or generated documents that contain the answer. This metric is commonly used in evaluations of previous works (Karpukhin et al., 2020; Izacard \&amp; Grave, 2020; Sachan et al., 2022). For other knowledge-intensive tasks, we follow the KILT benchmark (Petroni et al., 2021) to use accuracy (ACC) for fact checking and F1 / Rouge-L (R-L) score for open-domain dialogue system.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Open-domain QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fact Checking</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dialogue System</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">WebQ</td>
<td style="text-align: center;">FEVER</td>
<td style="text-align: center;">FM2</td>
<td style="text-align: center;">WoW (F1 / R-L)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">*with retriever, AND directly trained on these datasets</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DPR + InstructGPT*</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">13.7</td>
</tr>
<tr>
<td style="text-align: center;">*with retriever, BUT NOT trained on these datasets</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BM25 + InstructGPT</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">13.7</td>
</tr>
<tr>
<td style="text-align: center;">Contriever + InstructGPT</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;">Google + InstructGPT</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">13.2</td>
</tr>
<tr>
<td style="text-align: center;">*without retriever, and not using external documents</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Previous SoTA methods</td>
<td style="text-align: center;">$24.7^{1}$</td>
<td style="text-align: center;">$56.7^{2}$</td>
<td style="text-align: center;">$19.0^{1}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (no docs.)</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">GENREAD (InstructGPT)</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">14.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Zero-shot open-domain QA performance. Our proposed GENREAD with the InstructGPT reader (named GENREAD (InstructGPT)) can significantly outperform the original InstructGPT, achieving new state-of-the-art performance on three open-domain QA benchmarks (previous SoTA: ${ }^{1}$ GLaM (Du et al., 2022), ${ }^{2}$ FLAN (Wei et al., 2021)) under this setting without using any external document. Our GENREAD can achieve comparable or even better performance than zero-shot retrieve-then-read models that use a retriever or search engine to first obtain contextual documents. To ensure reproducibility, we use greedy search in decoding. All prompts used are shown in the $\S$ B.1. Note: fix numbers in v2 by adding average performance of different prompts, see details in Table 20.</p>
<h1>4.1 Zero-Shot Setting EXPERIMENTS</h1>
<p>We first compare our proposed GENREAD approach with various large language models proposed in recent years, including GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), FLAN (Wei et al., 2021), GLaM (Du et al., 2022), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022) and InstructGPT (Ouyang et al., 2022). Due to the space limitation, we only put the best performance on each dataset in Table 1, in which the line is called previous SoTA methods. In addition, their corresponding model parameters and performance are listed in Table 9 in Appendix. All of these baseline methods use the same input formats, i.e., [prompt words; question].
GENREAD is based on InstructGPT with 175B parameters. In order to fully evaluate the effectiveness of our proposed method, we also compare with InstructGPT augmented with retrieved documents from Wikipedia or Google search. The baseline methods (1) BM25 / Contriever + InstructGPT; (2) Google + InstructGPT; (3) DPR + InstructGPT have the same input format as our GENREAD , i.e., [prompt words; contextual document; question]. BM25 is a traditional sparse retrieval method. Contriever (Izacard et al., 2022a) is a state-of-the-art unsupervised dense retrieval model. DPR (Karpukhin et al., 2020) is a supervised dense retrieval model directly trained on NQ, TriviaQA and WebQ datasets. We note that comparing with above three methods is challenging because our method only relies on the large language model itself, without using any external corpus.</p>
<h3>4.1.1 EXPERIMENTAL RESULTS</h3>
<p>In the experiments, we use InstructGPT as our backbone model. As shown in Table 1, compared with state-of-the-art large language models, our proposed GENREAD with the InstructGPT reader improves its performance by generating contextual documents and conditioning on the generated documents, even though no new data is introduced, and the generator and reader have the exact same parameters. Specifically, GENREAD can improve the EM score by +6.9 on three open-domain QA benchmarks, compared to the original InstructGPT. We also make a similar observation on fact checking and open-domain dialogue system. Our proposed GENREAD can consistently outperform the baseline InstructGPT model without retrieving any contextual documents.</p>
<p>To further validate the effectiveness of GENREAD, we compare against zero-shot retrieve-then-read pipeline models, which first use a retrieval model or the Google search engine to get a relevant contextual document, then use InstructGPT to read the texts and produce the final answer. As shown in Table 1, GENREAD can achieve on-par performance with zero-shot retrieve-then-read pipeline models on the NQ and FM2 datasets, and outperform them on all other benchmarks. The knowledge learned by the large language models can be retrieved via autoregressive text generation. Without</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Recall@K on test sets, measured as the percentage of top-K documents that contain the answer. Our proposed clustering-based prompting method can outperform DPR and Google search, also two variants of using LLMs to generate documents. Exact numbers are reported in Table 6.
seeing any examples from these datasets, GENREAD can outperform using the supervised retrieval model (i.e., DPR) to recover relevant contextual documents.</p>
<h1>4.2 SUpervised Setting Experiments</h1>
<p>We compare our proposed GENREAD with retrieve-then-read models, including DPR (Karpukhin et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard \&amp; Grave, 2021). In addition, we compared with obtaining relevant documents from the internet using the Google search engine.</p>
<h3>4.2.1 EXPERIMENTAL SETUP</h3>
<p>For our proposed method, we replace the retriever with a large language model to directly generate contextual documents. In the experiments, we use InstructGPT (Ouyang et al., 2022). After contextual documents are retrieved or generated, we employ a FiD reader with 770 M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets. We note that we only use 10 documents during reading for the following reasons.</p>
<p>Why do we choose to use only 10 documents instead of 100 when reading?
As noted in Section 6.2 in DPR (Karpukhin et al., 2020) and Figure 3 in FiD (Izacard \&amp; Grave, 2021), increasing the number of documents can lead to better model performance and achieve state-of-the-art when using 100 documents. However, there are two major drawbacks to using 100 documents during the reading step. First, the operation is very expensive, leading to a significant increase in memory consumption and training time. As reported by Izacard \&amp; Grave (2021), the training process requires 64 Tesla V100 32GB running for around one day. Second, generating documents by using a large language model is slow and expensive, so only using 10 documents can be a significant cost saving in our method. Therefore, in our experiments, we choose to use 10 documents during the reading process. When using FiD-770M (i.e., FiD-large), the training process can be easily performed even on a single Tesla V100 32GB GPU. Meanwhile, when only using 10 documents, we can also increase the size of FiD model from 770 M to 3 B , which takes about the same amount of GPU memory as using 100 documents on a 770 M model, but at the same time significantly shortens the training time. We note that training T5-3B model needs a bigger cluster such as 8 Tesla V100 or A100 GPUs.</p>
<h3>4.2.2 EXPERIMENTAL ReSULTS ON OPEN-DOMAIN QA</h3>
<p>We first use Recall@K to compare the retrieval accuracy of different models. As shown in Figure 2, GENREAD can significantly outperform DPR and Google search for under 10 retrieved or generated documents. Compared to different GENREAD variants, including nucleus sampling, human written prompts, and clustering-based prompts, clustering-based prompts achieve the best performance. At the same time, we notice that the language model inevitably has the problem that the slope of the curve decreases as the number of generated documents increases. On one hand, this is due to the similarity of token distributions when large language models generate multiple documents. On the other hand, due to the shallow interaction characteristics of the dense retrieval model itself, the retrieved documents might not be completely relevant to the given question, so that the increase in recall might come from false positive documents, as also mentioned by Sachan et al. (2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;"># reader <br> parameters</th>
<th style="text-align: center;"># docu- <br> ments</th>
<th style="text-align: center;">TriviaQA <br> open test</th>
<th style="text-align: center;">WebQ <br> open test</th>
<th style="text-align: center;">NQ <br> open test</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">*baselines with retrieving from Wikipedia; all numbers reported by existing papers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: left;">RAG (Lewis et al., 2020)</td>
<td style="text-align: center;">400 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD (Izacard \&amp; Grave, 2021)</td>
<td style="text-align: center;">770 M</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">56.5</td>
</tr>
<tr>
<td style="text-align: left;">*baselines with retrieving from Wikipedia or Google; all numbers from our experiments</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD-1 (DPR, Wikipedia)</td>
<td style="text-align: center;">770 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: left;">FiD-xl (DPR, Wikipedia)</td>
<td style="text-align: center;">3 B</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: left;">FiD-xl (Google search)</td>
<td style="text-align: center;">3 B</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">56.2</td>
</tr>
<tr>
<td style="text-align: left;">*our proposed method by leveraging a large language model to generate documents</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (FiD-1) (sampling)</td>
<td style="text-align: center;">770 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">53.2</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (FiD-1) (clustering)</td>
<td style="text-align: center;">770 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (FiD-xl) (sampling)</td>
<td style="text-align: center;">3 B</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (FiD-xl) (clustering)</td>
<td style="text-align: center;">3 B</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: left;">$\vdash$ merge retrieved documents with generated documents</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{7 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Supervised open-domain QA performance. By only using generated documents from InstructGPT, our GENREAD with FiD reader (named GENREAD (FiD)) can achieve better performance than baseline methods on TriviaQA and WebQ. Through our detailed analysis of NQ, we found the performance gap mainly due to the temporality issue, which will be elaborated in $\S$ A.7.</p>
<p>As shown in Table 2, we can first observe the FiD model performs the best among all baseline models. Using FiD-xl with only 10 documents achieves comparable performance with using FiD-1 with 100 documents. The average gap is less than $1 \%$ on three benchmarks. Compared with both close-book models and Wikipedia-based retrieve-then-read pipelines, our proposed GENREAD can achieve state-of-the-art performance. Furthermore, compared with using sampling methods to generate documents, the clustering-based prompt method can improve the EM score by +2.2 on average. This indicates that the clustering-based prompt method is effectively increasing the knowledge coverage of generated documents, and also leading to better downstream QA performance. We also show that GENREAD can outperform Google search on all benchmarks. We observe both our method and Google search perform worse than DPR, mainly due to the significant portion of time-dependent questions in the dataset, which is described in the following analysis.</p>
<h1>4.2.3 EXPERIMENTAL RESULTS ON OTHER TASKS</h1>
<p>We demonstrate the experimental results in Table 3. Under the supervised setting, GENREAD can achieve on par performance on the fact checking task and superior performance on the dialogue system task, indicating that large language model can be seen as a strong knowledge generator.
The main reason that GENREAD performs worse than the dense retriever for fact checking is that the task provides sufficient semantic information to reach strong performance on this binary decision task. So, there is a smaller semantic gap between the given factual statement and contextual documents than that of question and document pairs in open-domain QA, which is an easier retrieval setting for modern dense retrieval methods that are mainly based on vector similarity.</p>
<h3>4.3 Observations and Experimental Analysis</h3>
<h3>4.3.1 COMPLEMENTARITY OF GENERATED AND RETRIEVED DOCUMENTS</h3>
<p>Generated documents can be combined with retrieved documents to outperform both. Even with a very large number of retrieved documents, including few samples of generated knowledge leads to large improvements. As shown in Table 2, merging retrieved documents with generated documents can</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Combining DPR retrieved documents and large language model (LLM) generated documents can achieve significantly better performance than using DPR retrieved documents only. For a fair comparison, instead of adding LLM generated documents to the model, we replace 10 documents retrieved by DPR with 10 documents generated by LLM so the total number of documents is the same. In this experiment, we use FiD-l (i.e., FiD-large) as the reader model because when the documents scale to more than 20, FiD-xl (i.e., FiD-3B) causes out-of-memory issues on A100 GPUs.
achieve state-of-the-art performance compared to all baseline methods listed in the table. Specifically, it can improve +5.7 averagely on three open-domain QA benchmarks compared to DPR alone, and improve +4.4 averagely compared to the large language model alone.</p>
<h1>4.3.2 Coverage Analysis over All Possible Answers</h1>
<p>The improvement in open-domain QA performance is due to the fact that correct answers are included more frequently in the generated text Recall@K is the most commonly used metric in existing works to measure the retrieval performance, which computes the percentage of top-K retrieved or generated documents that contain any possible answer at least once. than in the retrieved documents. However, as many questions contain multiple correct answers, recall@K cannot fully reflect the diversity of generated or retrieved documents. Each question in the WebQ has 2.39 correct answers, 1.79 correct answers in NQ and 14.02 (including all entity alias) in the TriviaQA. NQ and WebQ do not include alias names in the labels.</p>
<p>In this section, we also demonstrate the answer coverage performance of different models in Table 6. Answer coverage measures the percentage of the number of answers that are contained in the documents over all possible answers. Coverage analysis showed that generated text tends to have lower coverage than retrieved documents</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Documents obtained by $\downarrow$</th>
<th style="text-align: center;">NQ <br> -</th>
<th style="text-align: center;">TriviaQA <br> w. alias</th>
<th style="text-align: center;">WebQ <br> -</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BM25 (Robertson et al., 2009)</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">Google search engine ${ }^{3}$</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">$\mathbf{6 7 . 9}$</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (nucleus sampling)</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">59.8</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (10 human prompts)</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">$\underline{20.1}$</td>
<td style="text-align: center;">$\underline{74.8}$</td>
<td style="text-align: center;">$\underline{61.1}$</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (clustering prompts)</td>
<td style="text-align: center;">$\underline{61.7}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Answer coverage (\%) over 10 retrieved or generated documents. Case studies are provided in Tables 16-19 in Appendix.
because generated documents tends to have little diversity compared to retrieved documents. To improve coverage, we propose GENREAD with clustering, where we include examples in the prompt from different clusters of the training data to elicit more diverse generations.</p>
<h3>4.4 Readability Analysis of Retrieved and Generated Documents</h3>
<p>After we manually compare some retrieved documents from DPR and generated documents from InstructGPT, we observe that the readability of different documents, when they contain the correct answer string, is different. In other words, documents containing answers might also contain noisy information that is irrelevant to the question, which could affect both the model and human reading.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Documents obtained by $\downarrow$</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">WebQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (nucleus sampling)</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (clustering prompts)</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Readability study on retrieved documents and generated documents. See detailed analysis in $\S 4.4$.</p>
<p>In order to further validate the readability of retrieved documents and generated documents, we extracted a subset of data examples from NQ, TriviaQA and WebQ datasets, in which both retrieved and generated documents contain the correct answer. As shown in Table 5, when both retrieved and generated documents contain the correct answer, the FiD reader can produce more correct answers when reading the generated documents from large language models (e.g., InstructGPT).
We also provide some case studies in Tables 16-19. For example, in Table 18, the question is "What city was Zeus the patron god of?". The first document retrieved by DPR is "Like the other Panhellenic Games, the ancient Olympic Games were a religious festival, held at the sanctuary of Zeus at Olympia.". Although it contains the correct answer, it is hard to infer the answer "Olympia" from it. On the contrary, InstructGPT generates the document "Zeus was the patron god of the city of Olympia, which was located in the northwestern Peloponnese region of Greece. Olympia was the site of the Olympic Games, held every four years in honor of Zeus.", which is much easier to read.</p>
<h1>5 EPILOGUE</h1>
<p>CONCLUSION. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing the dense retrieval models with large language model generators. We call it generate-thenread, which first prompts a large language model to generate contextual documents, then read the generated document to infer the final answer. Notably, without retrieving any documents, it reaches 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the current retrieval-reader model $D P R$-FiD, as well as on other two knowledge-intensive tasks.</p>
<p>Limitation and Future Work. Despite the strong performance on the presented datasets, our approach is limited in its ability to update knowledge state and adapt to new domains. A major feature of retrieve-then-read is the ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task. Our approach relies on a large language model to contain all this knowledge and adding new knowledge would likely require some retraining. Future work will explore how to efficiently incorporate new knowledge into our generate-then-read method. Besides, generated documents might suffer from hallucination error, resulting in incorrect predictions. We demonstrated case study in Table 15. Consideration in combination with recent approaches (Creswell \&amp; Shanahan, 2022) to boost generative faithfulness is a also direction worthy of future research.</p>
<h2>ETHICS STATEMENT</h2>
<p>Large language models have a wide range of beneficial applications for society, but they also have potentially harmful applications. Previous work has shown various forms of bias, such as racial and gender bias, in large language models like GPT-3, even after explicit efforts to reduce toxic language (Chan, 2022). The importance of addressing these societal harms is acknowledged by OpenAI themselves in their 2020 paper introducing GPT-3 (Brown et al., 2020), which stated "we focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 ... and issues of bias, fairness, and representation within models like GPT-3." on page 34.</p>
<p>The goal of this paper is to utilize knowledge stored in the parameters of large language models to answer open-domain questions and solve knowledge-intensive tasks. Unlike retrieve-then-read where an external corpus can be curated to be trustworthy, the use of a model to generate contextual documents may further permeate existing biases in common models. First, our work shows that generated documents suffer from challenges of stale information from outdated documents used for training. Second, we show that generated documents tend to be less diverse, potentially biasing answers towards more common entities and terms from the training data. Finally, we conducted experiments on only three large language models. It is possible that some of our conclusions or observations may not necessarily hold for other models trained with different data or objectives.</p>
<p>Regarding ethical solutions, future work includes (i) further exploring potential bias and intentional or unintentional harm that may result from using generated contextual documents; (ii) better aligning language models with user intent to generate less biased contents and fewer fabricated facts.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>This work was supported in part by NSF IIS-2119531, IIS-2137396, IIS-2142827, CCF-1901059, and ONR N00014-22-1-2507. Wenhao is supported in part by Bloomberg Data Science Ph.D Fellowship.</p>
<h2>REFERENCES</h2>
<p>Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, et al. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. $93-104,2022$.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In EMNLP 2013, pp. 1533-1544, 2013.</p>
<p>Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers. arXiv preprint arXiv:2204.10628, 2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Anastasia Chan. Gpt-3 and instructgpt: technological dystopianism, utopianism, and "contextual" perspectives in ai ethics and industry. AI and Ethics, pp. 1-12, 2022.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870-1879, 2017.</p>
<p>Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Unitedqa: A hybrid approach for open domain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3080-3090, 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.</p>
<p>Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. In International Conference on Learning Representations, 2020.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019.</p>
<p>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547-5569. PMLR, 2022.</p>
<p>Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin Börschinger, and Jordan Boyd-Graber. Fool me twice: Entailment from wikipedia gamification. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 352-365, 2021.</p>
<p>Yuwei Fang, Shuohang Wang, Yichong Xu, Ruochen Xu, Siqi Sun, Chenguang Zhu, and Michael Zeng. Leveraging knowledge in multilingual commonsense reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 3237-3246, 2022.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.</p>
<p>Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations, 2020.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In EACL 2021, pp. 874-880, 2021.</p>
<p>Gautier Izacard, Mathild Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research (TMLR), 2022a.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL 2017, pp. 1601-1611, 2017.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769-6781, 2020.</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with colbert. Transactions of the Association for Computational Linguistics, 9:929-944, 2021.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: A benchmark for question answering research. TACL 2019, pp. 452-466, 2019.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6086-6096, 2019.</p>
<p>Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, et al. Standing on the shoulders of giant frozen language models. arXiv preprint arXiv:2204.10019, 2022.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638, 2022a.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154-3169, 2022b.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4089-4100, 2021.</p>
<p>OpenAI. Codex (available via openai api for free). https://openai.com/blog/openai-codex/, 2022.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, 2019.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2523-2544, 2021.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. $5835-5847,2021$.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418-5426, 2020.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.</p>
<p>Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. Questions are all you need to train a dense passage retriever. arXiv preprint arXiv:2206.10658, 2022.</p>
<p>Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34:25968-25981, 2021.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a largescale dataset for fact extraction and verification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 809-819, 2018.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4961-4974, 2022.</p>
<p>Michael Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. $7371-7387,2021$.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774, 2021.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Splits</th>
<th>Train</th>
<th>Valid</th>
<th>Test</th>
<th>Test labels</th>
</tr>
</thead>
<tbody>
<tr>
<td>TriviaQA (Joshi et al., 2017)</td>
<td>open domain</td>
<td>78,785</td>
<td>8,837</td>
<td>11,313</td>
<td>public</td>
</tr>
<tr>
<td></td>
<td>wikipedia split</td>
<td></td>
<td></td>
<td>7,993</td>
<td>public</td>
</tr>
<tr>
<td>WebQ (Berant et al., 2013)</td>
<td>open domain</td>
<td>3,478</td>
<td>300</td>
<td>2,032</td>
<td>public</td>
</tr>
<tr>
<td>NQ (Kwiatkowski et al., 2019)</td>
<td>open domain</td>
<td>79,168</td>
<td>8,757</td>
<td>3,610</td>
<td>public</td>
</tr>
<tr>
<td>FEVER (Thorne et al., 2018)</td>
<td>kilt challenge</td>
<td>104,966</td>
<td>10,444</td>
<td>10,100</td>
<td>hidden</td>
</tr>
<tr>
<td>FM2 (Eisenschlos et al., 2021)</td>
<td>official split</td>
<td>10,149</td>
<td>1169</td>
<td>1380</td>
<td>public</td>
</tr>
<tr>
<td>WoW (Dinan et al., 2019)</td>
<td>kilt challenge</td>
<td>63,734</td>
<td>3,054</td>
<td>2,944</td>
<td>hidden</td>
</tr>
</tbody>
</table>
<p>Table 6: Datasets splits and statistics. For FEVER and WoW, labels in the test are hidden, so the model performance should be evaluated at https://ai.facebook.com/tools/kilt/.</p>
<h1>A. 1 DATASETS AND SPLITS</h1>
<ul>
<li>TriviaQA (TQA) (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from trivia and quiz-league websites.</li>
<li>WebQUESTions (WebQ) (Berant et al., 2013) consists of questions selected using Google Suggest API, where the answers are entities in Freebase.</li>
<li>Natural Questions (NQ) (Kwiatkowski et al., 2019) were mined from real Google search queries and the answers are spans in Wikipedia articles identified by human annotators.
We explore the same train / dev / test splits for the open-domain QA setting as used by Izacard \&amp; Grave (2021); Karpukhin et al. (2020). For TriviaQA, GPT-3 / GLaM / PaLM (Brown et al., 2020; Du et al., 2022; Chowdhery et al., 2022) evaluate on the Wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with their performance.</li>
<li>FEVER (Thorne et al., 2018) is one of the largest datasets for fact checking that requires retrieving evidence from external corpus to support if a statement is supported or refuted.</li>
<li>FOOL ME Twice (FM2) (Eisenschlos et al., 2021) is a challenging fact checking dataset collected by gamification. Players write challenging claims either entailed or refuted by evidence from Wikipedia. They are then tasked to spot the refuted claim among a group.</li>
<li>WIZARD OF WIKIPEDIA (WoW) (Dinan et al., 2019) is an open-domain dialogue task for training agents that can converse knowledgeably about open-domain topics. One speaker in the conversation must ground their utterances in a specific knowledge sentence from a Wikipedia page.
We use the same train / dev / test splits in KILT challenge (Petroni et al., 2021) for the FEVER and WoW datasets. Their test labels are hidden, so the performance can only be evaluated through https://ai.facebook.com/tools/kilt. For FM2, we use its official dataset splits.</li>
</ul>
<h2>A. 2 IMPLEMENTATION DETAILS</h2>
<p>We use T5-770M (Raffel et al., 2020) and T5-3B as our backbone models to implement FiD (Izacard \&amp; Grave, 2021). We use AdamW as the optimizer, with 2,000 warm-up steps. We set the dropout probability to 0.1 and weight decay to 0.01 . We use one A100 for running T5-770M and set the batch size of 16 . We use 8 A100 for running T5-3B and set the per GPU batch as 2, leading to the total batch size as 16 . We searched different learning rates, ranging from $5 e-6$ to $4 e-5$, and we found $3 e-5$ to $6 e-5$ performed the best under the T5-3B setting and $5 e-5$ to $1 e-4$ performed the best under the T5-770M setting. We refer to more individual implementation details in Table 7.</p>
<p>We implement other baseline methods by using repositories:</p>
<ul>
<li>BM25: https://github.com/castorini/pyserini</li>
<li>DPR: https://github.com/facebookresearch/DPR</li>
<li>Contriever: https://github.com/facebookresearch/contriever</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Settings / Datasets</th>
<th style="text-align: right;">NQ</th>
<th style="text-align: right;">TriviaQA</th>
<th style="text-align: right;">WebQ</th>
<th style="text-align: right;">FEVER</th>
<th style="text-align: right;">FM2</th>
<th style="text-align: right;">WoW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Peak learning rate</td>
<td style="text-align: right;">$1 \mathrm{e}-4$</td>
<td style="text-align: right;">$1 \mathrm{e}-4$</td>
<td style="text-align: right;">$1 \mathrm{e}-4$</td>
<td style="text-align: right;">$1 \mathrm{e}-4$</td>
<td style="text-align: right;">$1 \mathrm{e}-4$</td>
<td style="text-align: right;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Total batch size</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">Total training steps</td>
<td style="text-align: right;">15,000</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">20,000</td>
</tr>
<tr>
<td style="text-align: left;">Best validation steps</td>
<td style="text-align: right;">6,000</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">8,500</td>
<td style="text-align: right;">5,000</td>
<td style="text-align: right;">6,000</td>
<td style="text-align: right;">20,000</td>
</tr>
<tr>
<td style="text-align: left;">Validation performance</td>
<td style="text-align: right;">43.27</td>
<td style="text-align: right;">69.47</td>
<td style="text-align: right;">60.33</td>
<td style="text-align: right;">88.97</td>
<td style="text-align: right;">73.57</td>
<td style="text-align: right;">18.60</td>
</tr>
<tr>
<td style="text-align: left;">Best validation $\Rightarrow$ test</td>
<td style="text-align: right;">43.50</td>
<td style="text-align: right;">70.22</td>
<td style="text-align: right;">53.33</td>
<td style="text-align: right;">87.25</td>
<td style="text-align: right;">74.21</td>
<td style="text-align: right;">18.49</td>
</tr>
<tr>
<td style="text-align: left;">Peak learning rate</td>
<td style="text-align: right;">$5 \mathrm{e}-5$</td>
<td style="text-align: right;">$6 \mathrm{e}-5$</td>
<td style="text-align: right;">$3 \mathrm{e}-5$</td>
<td style="text-align: right;">$5 \mathrm{e}-5$</td>
<td style="text-align: right;">$5 \mathrm{e}-5$</td>
<td style="text-align: right;">$3 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Total batch size</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Total training steps</td>
<td style="text-align: right;">20,000</td>
<td style="text-align: right;">15,000</td>
<td style="text-align: right;">15,000</td>
<td style="text-align: right;">15,000</td>
<td style="text-align: right;">15,000</td>
<td style="text-align: right;">20,000</td>
</tr>
<tr>
<td style="text-align: left;">Best validation steps</td>
<td style="text-align: right;">14,000</td>
<td style="text-align: right;">8,500</td>
<td style="text-align: right;">11,500</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">6,000</td>
<td style="text-align: right;">16,500</td>
</tr>
<tr>
<td style="text-align: left;">Validation performance</td>
<td style="text-align: right;">44.83</td>
<td style="text-align: right;">70.61</td>
<td style="text-align: right;">61.00</td>
<td style="text-align: right;">90.53</td>
<td style="text-align: right;">76.30</td>
<td style="text-align: right;">19.12</td>
</tr>
<tr>
<td style="text-align: left;">Best validation $\Rightarrow$ test</td>
<td style="text-align: right;">45.55</td>
<td style="text-align: right;">71.55</td>
<td style="text-align: right;">54.36</td>
<td style="text-align: right;">89.58</td>
<td style="text-align: right;">77.78</td>
<td style="text-align: right;">18.87</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparaters settings and validation performance for open-domain QA (numbers reported in Table 2), fact checking and dialogue system (numbers reported in Table 3). The upper part numbers are from GENREAD (FiD-l) and the lower part numbers are from GENREAD (FiD-xl).</p>
<h1>A. 3 REPRODUCIbility via Open Source Large Language Models</h1>
<p>We note that reproducing experiments on the OpenAI API, though publicly available, costs money. For this reason, we further add an evaluation on two open-source large language models OPT (Zhang et al., 2022) and Codex (OpenAI, 2022). As shown in Table 8, OPT performed worse than InstructGPT, but still achieved comparable performance with DPR; OpenAI Codex achieved the best performance on both TriviaQA and WebQ.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Documents obtained by $\downarrow$</th>
<th style="text-align: right;">TriviaQA</th>
<th style="text-align: right;">WebQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: right;">66.3</td>
<td style="text-align: right;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">OPT (Zhang et al., 2022)</td>
<td style="text-align: right;">62.1</td>
<td style="text-align: right;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (Ouyang et al., 2022)</td>
<td style="text-align: right;">71.3</td>
<td style="text-align: right;">54.5</td>
</tr>
<tr>
<td style="text-align: left;">Codex (OpenAI, 2022)</td>
<td style="text-align: right;">$\mathbf{7 2 . 6}$</td>
<td style="text-align: right;">$\mathbf{5 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Exact match (EM) score with using DPR and different open-source large language models such as OPT and Codex to generate contextual documents.</p>
<h2>A. 4 Scaling with Number of Large Language Model Parameters</h2>
<p>Figure 4 shows the scaling of performance with InstructGPT generator parameters, including Ada-150M, Babbage-1.3B, Curie-6.7B and Davinci-175B. We note that for both FiD and our GenRead, we use the FiD-xl with 10 input documents either retrieved from Wikipedia or generated by InstructGPT. The performance of both TriviaQA and WebQ continues to improve as the generator model parameters increase, as does the slope. Only with the largest size InstructGPT, GENREAD can outperform the $D P R$-FiD. This indicates using large language model to generate contextual documents is an "emergent ability" of scaling, which is not present in smaller models but is only present in larger language models (Wei et al., 2022a).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Model performance with different size of InstructGPT as context generators.</p>
<h2>A. 5 Additional Numbers for Tables in the Main Paper</h2>
<ul>
<li>Table 9 contains additional evaluation results for Table 1. It demonstrates zero-shot open-domain QA performance, compared to recent large language model.</li>
<li>Figure 5 contains additional retrieval performance evaluation for Figure 3 of experiments on combining DPR retrieved documents and large language model generated document.</li>
<li>Table 10 contains additional retrieval performance evaluated by Recall@K of baselines and different GENREAD variants. Some numbers in the table overlaps with those in Figure 2.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;"># total <br> parameters</th>
<th style="text-align: center;">NQ <br> open test</th>
<th style="text-align: center;">TriviaQA <br> open test</th>
<th style="text-align: center;">WabQ <br> wiki split</th>
<th style="text-align: center;">WebQ <br> open test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3 (Brown et al., 2020)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">14.4</td>
</tr>
<tr>
<td style="text-align: left;">Gopher (Rae et al., 2021)</td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FLAN (Wei et al., 2021)</td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">$\underline{56.7}$</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GLaM (Du et al., 2022)</td>
<td style="text-align: center;">64B</td>
<td style="text-align: center;">$\underline{21.5}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: left;">Chinchilla (Hoffmann et al., 2022)</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PaLM (Chowdhery et al., 2022)</td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 6 . 9}$</td>
<td style="text-align: center;">10.9</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (Ouyang et al., 2022)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$\underline{19.9}$</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (InstructGPT)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\mathbf{2 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 3}$</td>
<td style="text-align: center;">$\underline{70.3}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Additional numbers for Table 1. Zero-shot open-domain QA performance, compared to recent large language models. All models in the table do not leverage any external corpus for document retrieval. Compared to InstructGPT, our proposed GENREAD can improve the EM score by +6.9 on average. GENREAD can achieve state-of-the-art performance on open test sets.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Additional retrieval performance evaluation for Figure 3 of experiments on combining DPR retrieved documents and large language model generated documents. Merging documents from two sources achieved significantly better performance than using DPR retrieved documents only.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@20</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@20</td>
</tr>
<tr>
<td style="text-align: left;">BM25 (Robertson et al., 2009)</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">63.9</td>
</tr>
<tr>
<td style="text-align: left;">Contriever (Izacard et al., 2022a)</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">65.1</td>
</tr>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">$\mathbf{7 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Google Search engine API</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (nucleus, p=.95)</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">70.6</td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (10 human prompts)</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GENREAD (clustering prompts)</td>
<td style="text-align: center;">$\mathbf{6 9 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 0}$</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">74.5</td>
</tr>
</tbody>
</table>
<p>Table 10: Retrieval performance evaluated by Recall@K of baselines and different GENREAD variants. Some numbers in the table overlaps with those in Figure 2. The table aims to show the performance of more methods, and to provide accurate recall numbers for future research comparisons.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: left;">Sample 5 documents from entire data</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: left;">Sample 5 documents from each cluster</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">45.3</td>
</tr>
</tbody>
</table>
<p>Table 11: Ablation study on the strategy of sampling documents as in-context demonstrations.</p>
<h1>A. 6 Discussion on Inference Cost of DPR and InstructGPT</h1>
<p>We now compare the costs of using DPR and InstructGPT to retrieve or generate contextual documents. We consider DPR using the BERT-base (Devlin et al., 2019) version with 110M parameters and InstructGPT using its largest version with 175B parameters. For simplicity, we use the FLOPs-pertoken estimates for Transformer-based language models, which is introduced by Kaplan et al. (2020). It should be noted that FLOPs are not a direct measure of real-world computing costs, as latency, power consumption, and other costs can vary widely based on other factors (Liu et al., 2022a).</p>
<p>For the DPR model, all Wikipedia documents (around 21M) only need to be encoded once. Therefore, as the number of input questions increases, the marginal computational cost gradually decreases. For fair comparison, we first use DPR to encode all 21M Wikipedia documents once. Encoding all Wikipedia documents requires 110 e 6 (BERT-base parameters) $\times 21 e 6$ (total number of documents) $\times 100$ (tokens per document) $=2.3 e 17$ FLOPs. When the embedding of all candidate documents are produced, retrieving documents for a given question requires 110 e 6 (BERT-base parameters) $\times 20$ (tokens per question) $+21 e 6$ (total number of documents) $\times(768+768-1)=3.2 e 10$ FLOPs.</p>
<p>For InstructGPT, it requires 175 e 9 (InstructGPT parameters) $\times 10$ (number of documents) $\times 55$ (generated tokens per document) $=9.6 e 13$ FLOPs to generate 10 documents for a given question.</p>
<p>Therefore, the equation for the total cost $Y_{\text {DPR-cost }}$ to retrieve 10 documents using DPR versus the number of input questions $X$ is: $Y_{\text {DPR-cost }}=3.2 e 10 X+2.3 e 17$. Besides, the equation for the total cost $Y_{\text {GPT3-cost }}$ to generate 10 documents using InstructGPT versus the number of input questions $X$ is: $Y_{\text {GPT3-cost }}=9.6 e 13 X$. When $Y_{\text {DPR-cost }}=Y_{\text {GPT3-cost }}, X \approx 2473$. In conclusion, if the number of input questions is less than 2473 , the total cost of InstructGPT is lower than the DPR; if the number of input questions is greater than 2473 , the total cost of InstructGPT exceeds the DPR.</p>
<h2>A. 7 Error Analysis and Case Studies on the NQ dataset</h2>
<p>As stated in Zhang \&amp; Choi (2021), NQ contains a significant proportion, roughly 16.5\%, of questions that have time-dependent answers. Similarly, Izacard et al. (2022b) observed using the latest version of Wikipedia (12 / 2021) could lead to 4.4 drops of the EM score, compared to the Wikipedia version (12 / 2018) that the NQ questions are created from. We provide case studies in Table 13 in Appendix.</p>
<p>We did case studies of 100 examples from the NQ dataset. The results are shown in Table 12. Among these 100 examples, we found that 29 examples have data collection and annotation mistakes, mainly including the temporal question issue (13 / 29) and the incomplete answer issue (16 / 29). A typical temporal-dependent question is that no specific time condition is provided. For example, "Who won the MVP for the National League?" could have different answers in different years. In 2017, the MVP is Giancarlo Stanton, and in 2018, the MVP is Christian Yelich. Besides, some answer labels provided in the NQ dataset are not complete. For example, person names in the NQ dataset usually consist of first, middle, and last names, but most names in the generated documents are first and last names. For the question "who played lionel in as time goes by?", the labeled answer is "Geoffrey Dyson Palmer". DPR-FiD produces "Geoffrey Dyson Palmer" but GENREAD produces "Geoffrey Palmer", both of which should be considered correct. More examples are provided in Table 14.</p>
<p>Besides, GENREAD produced correct answers for 49 questions. Among the 22 incorrect predictions, 12 of them could be classified as retrieval errors (i.e., step-I error) and 12 as reading errors (i.e., step-II error). In all cases of retrieval errors, none of the generated documents contain the correct answer. In all cases of reading errors, at least one generated document contains the correct answer but the reader model failed to infer the correct answer from the documents..</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Good <br> Q\&amp;A <br> $(71 \%)$</th>
<th style="text-align: center;">Correct prediction (49\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">- Query: Who got the first Nobel Prize in Physics? <br> - Document: The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Röntgen for his discovery of the remarkable rays subsequently named after him. <br> - Predicted answer: Wilhelm Conrad Röntgen <br> - Correct answer: Wilhelm Conrad Röntgen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong retrieval (12\%)</td>
<td style="text-align: center;">Hallucinations (8\%)</td>
<td style="text-align: center;">- Query: Who died in the first episode of Stranger Things? <br> - Document: In the first episode of Stranger Things, the character Will Byers dies. He is killed by Demogorgon, a monster from the Upside Down. <br> - Predicted answer: Will Byers <br> - Correct answer: Benny Hammond</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No hit answers (4\%)</td>
<td style="text-align: center;">- Query: When was coffee first made into a drink? <br> - Document: The history of coffee goes back to the 10th century, with coffee trees native to Ethiopia. The earliest substantiated evidence of either coffee drinking or knowledge of coffee tree is from sixth century AD in Ethiopia. <br> - Predicted answer: the 10th century <br> - Correct answer: the 15th century</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong reading (10\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Query: When is the fourth movie of the Divergent series coming out? <br> - Document: The fourth movie in the Divergent series was originally scheduled to be released in June 2017, but was delayed indefinitely. <br> - Predicted answer: June 2017 <br> - Correct answer: never made</td>
</tr>
<tr>
<td style="text-align: center;">Bad <br> Q\&amp;A <br> $(29 \%)$</td>
<td style="text-align: center;">Temporal questions (13\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Query: Who won the MVP for the National League? <br> - Document: In 2017, the National League MVP was won by Giancarlo Stanton of the Miami Marlins. In 2018, the National League MVP was won by Christian Yelich of the Milwaukee Brewers. <br> - Predicted answer: Christian Yelich <br> - Correct answer: Giancarlo Stanton</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Incomplete answers (16\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Query: Where do the greasers live in the Outsiders? <br> - Document: The Outsiders is a novel by S.E. Hinton. It is about a gang of greasers in Oklahoma in the 1960s. The National League MVP was won by Christian Yelich of the Milwaukee Brewers. <br> - Predicted answer: Oklahoma <br> - Correct answer: Tulsa, Oklahoma</td>
</tr>
</tbody>
</table>
<p>Table 12: Case study on 100 GENREAD predictions in the NQ dataset. Among 100 examples, there are 49 correct predictions, i.e., $\mathrm{EM}=49 \%$. We further categorized 51 incorrect predictions of our GENREAD, including errors caused by data collection and annotation, and errors caused by model prediction. In addition, we provide more case studies in Tables 13-15 (Table 13 for the temporal question issue; Table 14 for the incomplete answer issue; Table 15 for the hallucination issue).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original question</th>
<th style="text-align: center;">NQ labels</th>
<th style="text-align: center;">Correct labels</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: When is the last time the philadelphia won the superbowl?</td>
<td style="text-align: center;">Super Bowl LII; 2017</td>
<td style="text-align: center;">2018; February 4, 2018</td>
</tr>
<tr>
<td style="text-align: center;">DPR: 2017 X; Google search: 2018 $\boldsymbol{\checkmark}$; GENREAD : February 4, 2018 $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: Who has the most big ten championships in football?</td>
<td style="text-align: center;">Michigan</td>
<td style="text-align: center;">Ohio State</td>
</tr>
<tr>
<td style="text-align: center;">DPR: Michigan $\boldsymbol{X}$; Google search: Ohio State $\boldsymbol{\checkmark}$; GENREAD : Ohio State $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: Who has the most super bowls in nfl history?</td>
<td style="text-align: center;">Pittsburgh Steelers</td>
<td style="text-align: center;">Pittsburgh Steelers; <br> New England Patriots</td>
</tr>
<tr>
<td style="text-align: center;">DPR: Pittsburgh Steelers $\boldsymbol{\checkmark}$; Google search: New England Patriots $\boldsymbol{\checkmark}$; GENREAD : New England Patriots $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: How many casinos are in atlantic city new jersey?</td>
<td style="text-align: center;">11; eleven</td>
<td style="text-align: center;">9; nine</td>
</tr>
<tr>
<td style="text-align: center;">DPR: eleven $\boldsymbol{X}$; Google search: nine $\boldsymbol{\checkmark}$; GENREAD : nine $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: When did the us not go to the olympics?</td>
<td style="text-align: center;">1980</td>
<td style="text-align: center;">1980; 1984</td>
</tr>
<tr>
<td style="text-align: center;">DPR: 1980 $\boldsymbol{\checkmark}$; Google search: 1980 and 1984 $\boldsymbol{\checkmark}$; GENREAD : 1984 $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: Largest cities in the world by population?</td>
<td style="text-align: center;">Beijing</td>
<td style="text-align: center;">Tokyo</td>
</tr>
<tr>
<td style="text-align: center;">DPR: Beijing $\boldsymbol{X}$; Google search: Tokyo $\boldsymbol{\checkmark}$; GENREAD : Tokyo $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: Who has most followers on instagram in world?</td>
<td style="text-align: center;">Selena Gomez</td>
<td style="text-align: center;">Cristiano Ronaldo</td>
</tr>
<tr>
<td style="text-align: center;">DPR: Instagram $\boldsymbol{X}$; Google search: Cristiano Ronaldo $\boldsymbol{\checkmark}$; GENREAD : Cristiano Ronaldo $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q: Who is the no. 1 ranked tennis player in the world?</td>
<td style="text-align: center;">Rafael Nadal</td>
<td style="text-align: center;">Novak Djokovic</td>
</tr>
<tr>
<td style="text-align: center;">DPR: Rafael Nadal $\boldsymbol{X}$; Google search: Novak Djokovic $\boldsymbol{\checkmark}$; GENREAD : Novak Djokovic $\boldsymbol{\checkmark}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 13: Case studies of temporality issues of the NQ dataset. All these questions are drawn from Zhang \&amp; Choi (2021), which contains a subset of NQ data examples with time-dependent questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original question</th>
<th style="text-align: left;">$D P R$-FiD predictions</th>
<th style="text-align: left;">GENREAD predictions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q: Who played lionel in as time goes by?</td>
<td style="text-align: left;">Geoffrey Dyson Palmer</td>
<td style="text-align: left;">Geoffrey Palmer</td>
</tr>
<tr>
<td style="text-align: left;">Explanation: The labeled answer is "Geoffrey Dyson Palmer", however, "Geoffrey Palmer" is also correct.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DPR retrieved documents: Geoffrey Dyson Palmer, (born 4 June 1927) is an English actor known for <br> his roles in British television sitcoms playing Jimmy Anderson in "The Fall and Rise of Reginald Perrin", <br> Ben Parkinson in "Butterflies" and Lionel Hardcastle in "As Time Goes By". His film appearances include <br> "A Fish Called Wanda", "The Madness of King George", "Mrs. Brown", and "Tomorrow Never Dies". <br> GPT generated documents: As Time Goes By is a British sitcom that aired on BBC One from 1992 to <br> 2005. The show starred Geoffrey Palmer and Judi Dench as Lionel and Jean Pargetter, a middle-aged couple <br> who reunite after many years apart. Lionel was played by Palmer, who was also a writer on the show.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Q: How many cracker barrels in the united states?</td>
<td style="text-align: left;">645</td>
<td style="text-align: left;">over 630</td>
</tr>
<tr>
<td style="text-align: left;">Explanation: The labled answer is "639" or "over 600", so "over 630" is also a reasonable answer.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Q: Where do the greasers live in the outsiders?</td>
<td style="text-align: left;">Tulsa, Oklahoma</td>
<td style="text-align: left;">Oklahoma</td>
</tr>
<tr>
<td style="text-align: left;">Explanation: The labled answer is "Tulsa, Oklahoma", but "Oklahoma" is also a correct answer. <br> DPR retrieved documents: The movie received mostly positive reviews from critics, and performed well <br> at the box office, grossing 33 million on a 10 million budget. In Tulsa, Oklahoma, greasers are a gang of <br> tough, low-income working-class teens. They include Ponyboy Curtis and his two older brothers, Sodapop <br> and Darrel, as well as Johnny. <br> GPT generated documents: The Outsiders is a novel by S.E. Hinton. It is about a gang of greasers in <br> Oklahoma in the 1960s. The greasers are from the poor side of town and constantly in trouble with the law.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Q: Where are unipolar neurons found in spinal cord?</td>
<td style="text-align: left;">the granule region</td>
<td style="text-align: left;">dorsal root ganglia</td>
</tr>
<tr>
<td style="text-align: left;">Explanation: The labled answer is "the distal dorsal root", but the output "dorsal root ganglia" is the same.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Case studies of the incomplete answers issue of the NQ dataset. Since the labels in NQ are spans identified from Wikipedia passages, it is easier for $D P R$-FiD to predict correct answers.</p>
<p>Question: Who wrote the first declaration of human rights? Answer: Cyrus Cylinder
Generated document: The first declaration of human rights was the Virginia Declaration of Rights, which was written by George Mason in 1776.
Retrieved document: John Peters Humphrey, OC (April 30, 1905 - 2013 March 14, 1995) was a Canadian legal scholar, jurist, and human rights advocate. He is most famous as the author of the first draft of the Universal Declaration of Human Rights. ... ... (Wikipedia page: John Peters Humphrey)</p>
<h1>Golden Wikipedia evidence:</h1>
<ul>
<li>George Mason's writings, including substantial portions of the Fairfax Resolves of 1774, the Virginia Declaration of Rights of 1776, and his Objections to this Constitution of Government (1787) opposing ratification, have exercised a significant influence on American political thought and events. Mason prepared the first draft of the Virginia Declaration of Rights in 1776, and his words formed much of the text adopted by the final Revolutionary Virginia Convention. (Wikipedia page: George Mason) - The Cyrus Cylinder was dubbed the "first declaration of human rights" by the pre-Revolution Iranian government, a reading prominently advanced by Shah Mohammed Reza Pahlavi, in a 1967 book, The White Revolution of Iran. (Wikipedia page: Cyrus Cylinder)
Hallucination: In GPT generated documents, first declaration of human rights was considered as Virginia Declaration of Rights, which is factually incorrect. This error might occur because Wikipedia page of George Mason contains similar descriptions, causing the model to confuse their meanings.
Model predictions: DPR-FiD John Peters Humphrey; GenReAd George Mason
Question: Who died in the first episode of stranger things Answer: Benny or Barb
Generated document: In the first episode of Stranger Things, the character Will Byers dies. He is killed by the Demogorgon, a monster from the Upside Down.
Retrieved document: ... ... was the character of Barb, the nerdy friend and classmate of Nancy who is taken and killed by the monster early in the season. According to actress Shannon Purser, Barb wasn't supposed to be a big deal, and the Duffer Brothers had not gone into great detail about the character since the focus was on finding Will. ... ... (Wikipedia page: Stranger Things)</li>
</ul>
<h2>Golden Wikipedia evidence:</h2>
<p>One area of focus from these fans was the character of Barb, the nerdy friend and classmate of Nancy who is taken and killed by the monster early in the season. According to actress Shannon Purser, Barb wasn't supposed to be a big deal, and the Duffer Brothers had not gone into great detail about the character since the focus was on finding Will. ... ... (Wikipedia page: Stranger Things)
The owner, Benny, learns from a tattoo of " 011 " on her arm that her name is Eleven. Brenner, monitoring the phone lines, sends agents to the diner after Benny calls social services. The agents kill Benny, but Eleven manages to escape using telekinetic abilities. Joyce's phone short circuits after receiving a mysterious phone call that she believes is from Will. While searching for Will in the woods, Mike, Dustin and Lucas come across Eleven. (Wikipedia page: Stranger Things)
Hallucination: In GPT generated documents, Will Byers is said to be dead, which is actually incorrect. Will Byers is actually a fake deaths in season 1 and he is still alive at the end of the season.
Model predictions: DPR-FiD Barb; GENREAD Will
Question: Who played the judge in good will hunting? Answer: James P. Flynn
Generated document: The character of the Judge in Good Will Hunting was played by actor Robert Sean Leonard. Leonard is best known for his role as Dr. James Wilson in the television series House.
Retrieved document: Janitor could show such facility with matrices applied this way. Good Will Hunting Good Will Hunting is a 1997 American drama film, directed by Gus Van Sant, and starring Robin Williams, Matt Damon, Ben Affleck, ... ... (Wikipedia page: Good Will Hunting)</p>
<h2>Golden Wikipedia evidence:</h2>
<ul>
<li>James P. Flynn appeared in many films shot in the New England area. Flynn was cast as a judge in the Boston-based film Good Will Hunting in 1997. Later, he acted in the 1999 film The Cider House Rules and What's the Worst That Could Happen? in 2001. (Wikipedia page: James P. Flynn)</li>
<li>Cast: James P. Flynn as Judge George H. Malone (Wikipedia page: Good Will Hunting)</li>
</ul>
<p>Hallucination: In GPT generated documents, Robert Sean Leonard is considered as a judge in Good Will Hunting, which is factually incorrect. This error leads to wrong answer prediction.
Model predictions: DPR-FiD Stellan Skarsgrd; GENREAD Alan Arkin
Table 15: Case studies of hallucination errors in InstructGPT generated documents. The documents contain contents that contradict to the facts and world knowledge, resulting in wrong predictions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ In the experiments, we set $n=5$ and found increasing $n$ does not bring extra improvement.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1219 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1219</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1219</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-265506227</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18473v1.pdf" target="_blank">DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, learning-based approaches have demonstrated significant promise in addressing intricate navigation tasks. Traditional methods for training deep neural network navigation policies rely on meticulously designed reward functions or extensive teleoperation datasets as navigation demonstrations. However, the former is often confined to simulated environments, and the latter demands substantial human labor, making it a time-consuming process. Our vision is for robots to autonomously learn navigation skills and adapt their behaviors to environmental changes without any human intervention. In this work, we discuss the self-supervised navigation problem and present Dynamic Graph Memory (DGMem), which facilitates training only with on-board observations. With the help of DGMem, agents can actively explore their surroundings, autonomously acquiring a comprehensive navigation policy in a data-efficient manner without external feedback. Our method is evaluated in photorealistic 3D indoor scenes, and empirical studies demonstrate the effectiveness of DGMem.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1219.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1219.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat/Gibson Scenes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Photorealistic 3D Indoor Scenes (Habitat simulator with Gibson dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Realistic indoor household environments used for image-goal visual navigation experiments; scenes vary in appearance, size and floors and are simulated in Habitat using Gibson scans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat (Gibson) indoor scenes</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic 3D household indoor scenes (multiple houses/apartments) used for embodied image-goal navigation in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Graph constructed by DGMem is intended to be sparse and representative (nodes = selected observations, edges = binary reachability between nodes); connectivity is dynamic and pruned by visit counts, not described by formal average-degree metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Evaluated across 8 scenes; per-scene node/edge counts not reported (graph size grows as agent explores; scenes vary in size and floors).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hierarchical policy: global planner (BFS on DGMem) + local goal-conditioned policy (PPO-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-level architecture: (1) global: localize current and goal observations in DGMem and run BFS on the topological graph to generate a sequence of waypoint nodes; (2) local: a goal-conditional image-based policy (ResNet18 encoder, fusion MLP, GRU) trained with RL (PPO) plus imitation learning from stored optimal trajectories in DGMem.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>coverage (proportion of traversed voxels); also uses node-visit distributions and coverage curves during training</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical, memory- and planning-based (global graph planner + local goal-conditioned reactive policy); graph-memory augmented policies.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>DGMem's topological abstraction (nodes/edges, visit counts) is used to define exploration frontiers and intrinsic rewards: (1) novelty via node visit counts encourages exploring low-visit nodes and thereby improves coverage; (2) topological distance reward r_d uses change in DGMem shortest-path length to goal (|l(prev,goal)|−|l(curr,goal)|) to shape reward and guide progress; (3) navigation trajectory reward r_n rewards visiting previously unvisited nodes along transitions. These graph-topology based signals improve exploration efficiency, coverage, and downstream navigation performance compared to curiosity-only baselines which tend to repeatedly roam local regions and fail to traverse long corridors or floors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No explicit controlled comparison across differing graph topologies; experiments show DGMem yields better coverage and navigation performance than curiosity-driven baselines and standard RL/IL baselines in the tested Habitat scenes, but the paper does not quantify how specific topological metrics (diameter, clustering, dead-end fraction) vary across scenes or directly modulate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that combine an explicit topological planner (graph search on DGMem) with a learned local controller perform better on long-horizon image-goal tasks than purely end-to-end RL; the graph memory enables long-range planning and reduces burden on the learned policy. However, forcing the agent to strictly follow waypoints can produce suboptimal routing (reduced SPL) compared to expert-planner supervised methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1219.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1219.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FourRooms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FourRooms GridWorld (didactic example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic gridworld environment (FourRooms) used as a didactic example: 256 discrete states with 4 actions, used to illustrate DGMem's exploration behavior versus RND.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>FourRooms gridworld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete 2D gridworld partitioned into four rooms; used to illustrate exploration and state distribution behavior (256 states, 4 actions).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Underlying state-space is grid-graph; DGMem constructs a topological graph over visited states producing an approximately uniform coverage graph; connectivity is determined by adjacency and reachability in the grid.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>256 discrete states (explicitly reported in the paper for the FourRooms example).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DGMem exploration agent (vs RND baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent using DGMem to build an explicit topological graph and choose novelty-driven node goals (softmax over node visit counts) to direct exploration; local policy follows learned goal-conditioned controller.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>state distribution uniformity and coverage across discrete states; reported via state distribution visualizations and coverage metric; also reported SPL for navigation generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>DGMem achieves near-uniform state distribution across the 256 states in the constrained episode setting (qualitative/visual); when trained on a subset (60 states) the goal-conditional policy generalizes to all 256 states achieving SPL >90% (reported as '90+%').</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based / planning-augmented policy: explicit topological memory with goal selection based on novelty and a local goal-conditioned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Constructing an approximately uniform, representative graph of the environment enables broad generalization of the learned goal-conditional policy: training on a subset of nodes (60 states) generalizes to the whole 256-state grid when DGMem provides coverage and planning. Curiosity-based methods (RND) tend to oscillate in limited regions and fail to discover all rooms, indicating that explicit topological structure (DGMem) improves exploration across disconnected or room-separated regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No systematic experimental sweep over different grid topologies; comparison is between DGMem and curiosity/RND in the same FourRooms topology, showing DGMem finds all rooms and yields uniform coverage while RND often fails to discover all rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>In FourRooms, a policy that leverages a topological memory planner to select novel node goals yields far better exploration coverage and generalization; local goal-conditioned policies trained with DGMem demonstrations produce high SPL when generalized beyond training nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1219.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1219.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGMem (Dynamic Graph Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Graph Memory (DGMem) topological memory module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic, undirected topological graph memory where nodes are selected observation instances (image, noisy pose, visit count, visual encoding) and edges represent binary reachability and store optimal transition trajectories; used both for planning and to generate intrinsic rewards and imitation data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DGMem graph (abstract topological memory applied to environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An abstract dynamic graph constructed online from the agent's on-board observations in an environment (maps observations to graph nodes and reachability to edges); intended to represent the partially observed topology of indoor scenes or gridworlds for planning and training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse, representative topological graph created by retaining only semantically valuable observations and by merging near-duplicate observations via pose and visual similarity thresholds; edges are binary reachability flags updated from rollouts and pruned by visit counts (c_ij).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Variable — graph grows as the agent explores; number of nodes and edges not fixed and depend on scene and thresholds (semantic threshold d_c and similarity threshold d_p).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DGMem-enabled hierarchical agent (global planner + local controller)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DGMem is used to (1) select novel target nodes via softmax over negative visit counts (p_i ∝ e^{−γ c_i}), (2) compute topological distance reward r_d from differences in DGMem shortest-path lengths to a goal, (3) compute navigation trajectory reward r_n by counting first-time node visits along trajectories, and (4) provide stored optimal transition trajectories for imitation learning (with KL regularization). Also used at test-time for BFS planning across nodes to decompose long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>node-visit counts (novelty), coverage (voxels in 3D), state distribution uniformity, and training sample-efficiency (interactions until coverage/success thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-memory + planner combined with a local goal-conditioned policy; memory-based planning is essential for long-range exploration and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Key relationships encoded/used by DGMem: (a) Node visit counts encode novelty and guide exploration to frontiers; (b) topological path lengths (shortest-path on DGMem) provide dense shaping rewards guiding progress toward goals; (c) storing efficient trajectories on edges supplies imitation learning data increasing data efficiency; (d) pruning edges by low visit counts removes redundant connections but preserves essential connectivity. Empirically, these topology-driven mechanisms increase coverage, prevent local oscillatory exploration seen in curiosity-driven agents, and improve multi-goal navigation success and distance-to-goal metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No quantitative comparison of different graph-construction parameterizations (e.g., threshold sweeps) is reported; ablations cover pose-noise robustness but not systematic topology-structure comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>A hierarchical structure (global DGMem planner + local goal-conditioned policy) is effective: topological planning alleviates the need for the learned policy to plan long horizons, while imitation learning from DGMem edges improves sample efficiency; however, rigid following of graph waypoints can reduce routing optimality (SPL) compared to an oracle planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian relational memory for semantic visual navigation <em>(Rating: 2)</em></li>
                <li>Visual graph memory with unsupervised representation for visual navigation <em>(Rating: 2)</em></li>
                <li>Learning exploration policies for navigation <em>(Rating: 1)</em></li>
                <li>Exploration by random network distillation <em>(Rating: 1)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 1)</em></li>
                <li>Learning to map for active semantic goal navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1219",
    "paper_id": "paper-265506227",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Habitat/Gibson Scenes",
            "name_full": "Photorealistic 3D Indoor Scenes (Habitat simulator with Gibson dataset)",
            "brief_description": "Realistic indoor household environments used for image-goal visual navigation experiments; scenes vary in appearance, size and floors and are simulated in Habitat using Gibson scans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Habitat (Gibson) indoor scenes",
            "environment_description": "Photorealistic 3D household indoor scenes (multiple houses/apartments) used for embodied image-goal navigation in simulation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Graph constructed by DGMem is intended to be sparse and representative (nodes = selected observations, edges = binary reachability between nodes); connectivity is dynamic and pruned by visit counts, not described by formal average-degree metrics.",
            "environment_size": "Evaluated across 8 scenes; per-scene node/edge counts not reported (graph size grows as agent explores; scenes vary in size and floors).",
            "agent_name": "Hierarchical policy: global planner (BFS on DGMem) + local goal-conditioned policy (PPO-trained)",
            "agent_description": "Two-level architecture: (1) global: localize current and goal observations in DGMem and run BFS on the topological graph to generate a sequence of waypoint nodes; (2) local: a goal-conditional image-based policy (ResNet18 encoder, fusion MLP, GRU) trained with RL (PPO) plus imitation learning from stored optimal trajectories in DGMem.",
            "exploration_efficiency_metric": "coverage (proportion of traversed voxels); also uses node-visit distributions and coverage curves during training",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Hierarchical, memory- and planning-based (global graph planner + local goal-conditioned reactive policy); graph-memory augmented policies.",
            "topology_performance_relationship": "DGMem's topological abstraction (nodes/edges, visit counts) is used to define exploration frontiers and intrinsic rewards: (1) novelty via node visit counts encourages exploring low-visit nodes and thereby improves coverage; (2) topological distance reward r_d uses change in DGMem shortest-path length to goal (|l(prev,goal)|−|l(curr,goal)|) to shape reward and guide progress; (3) navigation trajectory reward r_n rewards visiting previously unvisited nodes along transitions. These graph-topology based signals improve exploration efficiency, coverage, and downstream navigation performance compared to curiosity-only baselines which tend to repeatedly roam local regions and fail to traverse long corridors or floors.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No explicit controlled comparison across differing graph topologies; experiments show DGMem yields better coverage and navigation performance than curiosity-driven baselines and standard RL/IL baselines in the tested Habitat scenes, but the paper does not quantify how specific topological metrics (diameter, clustering, dead-end fraction) vary across scenes or directly modulate performance.",
            "policy_structure_findings": "Policies that combine an explicit topological planner (graph search on DGMem) with a learned local controller perform better on long-horizon image-goal tasks than purely end-to-end RL; the graph memory enables long-range planning and reduces burden on the learned policy. However, forcing the agent to strictly follow waypoints can produce suboptimal routing (reduced SPL) compared to expert-planner supervised methods.",
            "uuid": "e1219.0",
            "source_info": {
                "paper_title": "DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "FourRooms",
            "name_full": "FourRooms GridWorld (didactic example)",
            "brief_description": "Classic gridworld environment (FourRooms) used as a didactic example: 256 discrete states with 4 actions, used to illustrate DGMem's exploration behavior versus RND.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "FourRooms gridworld",
            "environment_description": "Discrete 2D gridworld partitioned into four rooms; used to illustrate exploration and state distribution behavior (256 states, 4 actions).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Underlying state-space is grid-graph; DGMem constructs a topological graph over visited states producing an approximately uniform coverage graph; connectivity is determined by adjacency and reachability in the grid.",
            "environment_size": "256 discrete states (explicitly reported in the paper for the FourRooms example).",
            "agent_name": "DGMem exploration agent (vs RND baseline)",
            "agent_description": "Agent using DGMem to build an explicit topological graph and choose novelty-driven node goals (softmax over node visit counts) to direct exploration; local policy follows learned goal-conditioned controller.",
            "exploration_efficiency_metric": "state distribution uniformity and coverage across discrete states; reported via state distribution visualizations and coverage metric; also reported SPL for navigation generalization.",
            "exploration_efficiency_value": "DGMem achieves near-uniform state distribution across the 256 states in the constrained episode setting (qualitative/visual); when trained on a subset (60 states) the goal-conditional policy generalizes to all 256 states achieving SPL &gt;90% (reported as '90+%').",
            "success_rate": null,
            "optimal_policy_type": "Memory-based / planning-augmented policy: explicit topological memory with goal selection based on novelty and a local goal-conditioned policy.",
            "topology_performance_relationship": "Constructing an approximately uniform, representative graph of the environment enables broad generalization of the learned goal-conditional policy: training on a subset of nodes (60 states) generalizes to the whole 256-state grid when DGMem provides coverage and planning. Curiosity-based methods (RND) tend to oscillate in limited regions and fail to discover all rooms, indicating that explicit topological structure (DGMem) improves exploration across disconnected or room-separated regions.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No systematic experimental sweep over different grid topologies; comparison is between DGMem and curiosity/RND in the same FourRooms topology, showing DGMem finds all rooms and yields uniform coverage while RND often fails to discover all rooms.",
            "policy_structure_findings": "In FourRooms, a policy that leverages a topological memory planner to select novel node goals yields far better exploration coverage and generalization; local goal-conditioned policies trained with DGMem demonstrations produce high SPL when generalized beyond training nodes.",
            "uuid": "e1219.1",
            "source_info": {
                "paper_title": "DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "DGMem (Dynamic Graph Memory)",
            "name_full": "Dynamic Graph Memory (DGMem) topological memory module",
            "brief_description": "A dynamic, undirected topological graph memory where nodes are selected observation instances (image, noisy pose, visit count, visual encoding) and edges represent binary reachability and store optimal transition trajectories; used both for planning and to generate intrinsic rewards and imitation data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "DGMem graph (abstract topological memory applied to environments)",
            "environment_description": "An abstract dynamic graph constructed online from the agent's on-board observations in an environment (maps observations to graph nodes and reachability to edges); intended to represent the partially observed topology of indoor scenes or gridworlds for planning and training.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Sparse, representative topological graph created by retaining only semantically valuable observations and by merging near-duplicate observations via pose and visual similarity thresholds; edges are binary reachability flags updated from rollouts and pruned by visit counts (c_ij).",
            "environment_size": "Variable — graph grows as the agent explores; number of nodes and edges not fixed and depend on scene and thresholds (semantic threshold d_c and similarity threshold d_p).",
            "agent_name": "DGMem-enabled hierarchical agent (global planner + local controller)",
            "agent_description": "DGMem is used to (1) select novel target nodes via softmax over negative visit counts (p_i ∝ e^{−γ c_i}), (2) compute topological distance reward r_d from differences in DGMem shortest-path lengths to a goal, (3) compute navigation trajectory reward r_n by counting first-time node visits along trajectories, and (4) provide stored optimal transition trajectories for imitation learning (with KL regularization). Also used at test-time for BFS planning across nodes to decompose long-horizon tasks.",
            "exploration_efficiency_metric": "node-visit counts (novelty), coverage (voxels in 3D), state distribution uniformity, and training sample-efficiency (interactions until coverage/success thresholds).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Graph-memory + planner combined with a local goal-conditioned policy; memory-based planning is essential for long-range exploration and sample efficiency.",
            "topology_performance_relationship": "Key relationships encoded/used by DGMem: (a) Node visit counts encode novelty and guide exploration to frontiers; (b) topological path lengths (shortest-path on DGMem) provide dense shaping rewards guiding progress toward goals; (c) storing efficient trajectories on edges supplies imitation learning data increasing data efficiency; (d) pruning edges by low visit counts removes redundant connections but preserves essential connectivity. Empirically, these topology-driven mechanisms increase coverage, prevent local oscillatory exploration seen in curiosity-driven agents, and improve multi-goal navigation success and distance-to-goal metrics.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No quantitative comparison of different graph-construction parameterizations (e.g., threshold sweeps) is reported; ablations cover pose-noise robustness but not systematic topology-structure comparisons.",
            "policy_structure_findings": "A hierarchical structure (global DGMem planner + local goal-conditioned policy) is effective: topological planning alleviates the need for the learned policy to plan long horizons, while imitation learning from DGMem edges improves sample efficiency; however, rigid following of graph waypoints can reduce routing optimality (SPL) compared to an oracle planner.",
            "uuid": "e1219.2",
            "source_info": {
                "paper_title": "DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian relational memory for semantic visual navigation",
            "rating": 2,
            "sanitized_title": "bayesian_relational_memory_for_semantic_visual_navigation"
        },
        {
            "paper_title": "Visual graph memory with unsupervised representation for visual navigation",
            "rating": 2,
            "sanitized_title": "visual_graph_memory_with_unsupervised_representation_for_visual_navigation"
        },
        {
            "paper_title": "Learning exploration policies for navigation",
            "rating": 1,
            "sanitized_title": "learning_exploration_policies_for_navigation"
        },
        {
            "paper_title": "Exploration by random network distillation",
            "rating": 1,
            "sanitized_title": "exploration_by_random_network_distillation"
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 1,
            "sanitized_title": "curiositydriven_exploration_by_selfsupervised_prediction"
        },
        {
            "paper_title": "Learning to map for active semantic goal navigation",
            "rating": 1,
            "sanitized_title": "learning_to_map_for_active_semantic_goal_navigation"
        }
    ],
    "cost": 0.01078875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory
30 Nov 2023</p>
<p>Wenzhe Cai 
Teng Wang 
Guangran Cheng 
ChangyinLele Xu 
DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic Graph Memory
30 Nov 2023432451FFC4AEF0EC5C1928795DD2A7D4arXiv:2311.18473v1[cs.RO]
In recent years, learning-based approaches have demonstrated significant promise in addressing intricate navigation tasks.Traditional methods for training deep neural network navigation policies rely on meticulously designed reward functions or extensive teleoperation datasets as navigation demonstrations.However, the former is often confined to simulated environments, and the latter demands substantial human labor, making it a time-consuming process.Our vision is for robots to autonomously learn navigation skills and adapt their behaviors to environmental changes without any human intervention.In this work, we discuss the self-supervised navigation problem and present Dynamic Graph Memory (DGMem), which facilitates training only with on-board observations.With the help of DGMem, agents can actively explore their surroundings, autonomously acquiring a comprehensive navigation policy in a data-efficient manner without external feedback.Our method is evaluated in photorealistic 3D indoor scenes, and empirical studies demonstrate the effectiveness of DGMem.</p>
<p>I. INTRODUCTION</p>
<p>Navigation, a fundamental skill for mobile robots, serves as the cornerstone for basic locomotion before engaging in high-level tasks within a home-assistance robot.Classical methods for embodied agent navigation hinge on simultaneous localization and mapping (SLAM) [1]- [3], relying on intricate hand-crafted modules and statistical analysis for robust performance.However, the traditional lidar SLAM systems are inappropriate for abstract visual navigation tasks, such as finding the green apple on the table.Visual SLAM systems, dependent on accurate state estimation, are susceptible to dynamic elements, leading to mapping inconsistency.In response to these challenges, learning-based methods offer a fresh paradigm for navigation tasks through concise end-toend networks.This shift has given rise to advanced methods that have achieved notable success across various navigation tasks, including PointGoal Navigation [4], [5], ObjectGoal Navigation [6], [7], Room Navigation [8], [9], ImageGoal Navigation [10], [11], Vision-Language Navigation [12], and Audio-Visual Navigation [13].</p>
<p>To learn a navigation policy, most works resort to imitation learning (IL) or reinforcement learning (RL).Typically, training feedback is derived from human teleoperation [14] or meta-data, such as occupancy maps or shortest-path planners, embedded in simulators [15]- [19].However, deploying a frozen pre-trained navigation policy on a real robot operating † Corresponding Author Wenzhe Cai and Guangran Cheng are with the School of Automation, Southeast University, Nanjing, China.The work is done while both are interns of PengCheng Lab.</p>
<p>Teng Wang, Lele Xu and Changyin Sun are with the School of Automation, Southeast University, Nanjing, China.(e-mail: wangteng@seu.edu.cn,xulele@seu.edu.cn,cysun@seu.edu.cn)But in the real-world, it is non-trivial to acquire such meta-data without human guidance for the computation of training objectives.Therefore, in b), the only accessible information is on-board sensor data, like the RGB images from the camera.Our DGMem must evaluate the executed actions to optimize the policy network and constantly perform navigation to collect experience.</p>
<p>in a new environment may not yield optimal performance.Variations in layouts and object placements necessitate the robot's ability to dynamically adjust its parameters.Accessing meta-data used for RL in real-world scenes is impractical, and continually collecting new demonstrations using human effort to guide the robot's learning process is cumbersome.The ideal scenario involves the robot autonomously learning through active exploration and self-supervised learning.In this work, we delve into the self-supervised visual navigation task, wherein the agent must acquire a generalizable navigation policy without external feedback, relying solely on its own on-board observations.</p>
<p>The self-supervised navigation task presents two primary challenges: first, learning the navigation policy in a dataefficient manner in the absence of external feedback; and second, preventing the robot from traversing a limited region to enhance the generalization performance of the learned navigation policy.To address these challenges, we introduce a memorizing scheme, the Dynamic Graph Memory (DGMem).DGMem dynamically iterates its stored nodes and edges as the robot roams within the house, refining itself and providing evolving feedback for both RL and IL training.The combination of RL+IL training objectives significantly enhances data efficiency.To improve generalization performance, the topological structure of DGMem reveals exploration frontiers within the scene, fostering consistent exploration behavior.This ensures the training dataset covers nearly all areas in the house.Additionally, the graph serves as a planner, aiding in the execution of long-horizon navigation trajectories and alleviating the requirements for a learning-based navigation policy.Both aspects are crucial for enhancing navigation performance.In summary, our contribution encompasses three folds:</p>
<p>• We introduce the self-supervised navigation task, a crucial skill for deploying robots in the real world.• We propose DGMem, a novel memorizing scheme that serves as both a planner and trainer for the navigation policy.</p>
<p>• We evaluate DGMem in a photorealistic indoor simulator, demonstrating that even with random weight initialization, the policy network can acquire a generalizable navigation skill in the novel scene within 250k interactions.</p>
<p>II. RELATED WORKS A. Learning-Based Navigation</p>
<p>With the success of end-to-end deep reinforcement learning for video games [20], many researchers start discussing learning-based approaches for visual navigation problems and many advanced approaches are proposed.For example, [21] accelerates navigation policy training by incorporating auxiliary learning objectives.For a better understanding of the scene, [22] introduces a scene memory transformer module for several navigation tasks.[23] utilize a mapping module and design a novel intrinsic reward for fast exploration of new scenes.To reduce the dependency on simulators, [24], [25] use massive internet video clips as training data.And some recent works directly using the teleoperation navigation demonstrations to train a general navigation skill [].A versatile navigation policy that can directly transfer to all kinds of scenes is appealing but challenging.For personal usage, we seldom move the robot to other houses but only expect it 'overfitting' in our own house.Therefore, we investigate the problem of active learning without any external feedback or human intervation and we propose the DGMem which enables the robot learning navigation policy by itself.</p>
<p>B. Self-Supervised Reinforcement Learning</p>
<p>The objective of reinforcement learning is maximizing the cumulative reward by adjusting a learnable policy.Many RLbased applications are limited in simulators because only in simulators we can monitor the system status and the environment to design an appropriate reward function to guide the agent.In most real-world scenarios, we cannot access the underlying environment states thus a well-shaped reward function is unavailable.To tackle this problem, many works discuss a self-supervised setting for the Markov Decision Process (MDP), where the environment no longer gives the reward feedback, but only transits to the next state.Under such circumstances, one possible solution is to design a general model to produce pseudo-rewards with respect to agent observations.For example, [26] use internet video clips and a self-supervised training objective to train a discriminator as the reward generator.[27] introduces forward-backward representation and utilizes this representation to estimate the reward.[28] trains a state representation with self-prediction and proposes measuring the feature similarity between state and goals as reward.Based on mutual information, DIAYN [29], [30] propose an intrinsic reward and learns diverse skills in an unsupervised manner.Our work follows the reward-free settings and trains a goal-conditional navigation policy.But different from the prior work, our proposed DGMem can composes both IL and RL training objectives which greatly increase the data efficiency.</p>
<p>C. External Memory for Navigation</p>
<p>To complete long-horizon navigation tasks, the structural spatial information about the environment is usually important for path planning.Many works store key information in different types of external memory modules and exploit them for inference.Inspired by traditional SLAM, several approaches maintain a global occupancy map as interacting with the environment.For example, Neural SLAM [31] plans a subgoal in the global map which serves as an exploration direction.[32] follows the idea and introduces semantic information to the global map.Some other works build the memory modules as an abstract topological map.For example, BGM [8] builds the topological relationship for different types of rooms.VGM [33] constructs the map with image observations and extract the topological-level knowledge with a neural network.Most of the prior works only focus on improving the policy generalization capability with the external memory as policy inputs.But they must rely on a pre-training phase with a simulator.But our scheme shows that DGMem can enables an agent learning the navigation policy from stretch or continue learning in a novel environment, which is unlikely to suffer from the sim-to-real domain gap.</p>
<p>III. APPROACH</p>
<p>A. Problem Formulation</p>
<p>We formulate the self-supervised navigation task as a Markov Decision Process (MDP).During the training stage, we assume the agent is randomly initialized in the environment and experiencing an infinite episode, mirroring the scenario of acquiring a new home-assistant robot and deploying it in the house.Reset functions are unavailable, simulating the constraint that the robot cannot teleport.The agent maneuvers through the environment by executing locomotion actions a t ∈ A according to a policy π θ .The action set A contains three primitive actions which are A = {M oveAhead, T urnLef t, T urnRight}.After each action a t , the agent receives a new observation o t+1 , forming the experience trajectory D train =</p>
<p>Agent Environment action observation</p>
<p>No reward {o 0 , a 0 , o
! ! ! " ! # ! $ ! % ! !" : # ! → # " ! !# : # ! → # # ! "$ : # " → # $ ! $% : # $ → # % …… Node Transition Trajectory Graph Representation ! ! ! " ! # ! $ ! %Path Planning Reward</p>
<p>B. Dynamic Graph Memory</p>
<p>We build the DGM as an undirected graph with selfcollected experience from the new environment.The graph G contains a node set V and an edge set E. In the next following sections, we will introduce the details of graph iteration, including the node updates and edge updates, respectively.</p>
<p>Node Iteration.In DGMem, each node represents an instance of the collected observations, comprising four components v i = (I i , Pi , c i , Φ θ (I i )).Here, I i represents the RGB image, Pi represents the noisy pose estimation, c i represents the visit count of node v i , and Φ θ (I i ) represents the visual encoding of the RGB image.For efficiency, we employ a pre-trained ResNet18 [34] as the visual encoder Φ θ .To streamline the planning algorithm's efficiency on the graph, we limit the graph's complexity by preserving only valuable observations as nodes.The value of an observation is evaluated from two perspectives.First, the semantic information conveyed in the image is considered.We favor images with more apparent objects, utilizing a pre-trained YOLO-v5 [35] to detect objects in the image and label the semantic score as C o i = K id=0 c id , where c id represents the confidence score of the detected objects.Images with C o i &lt; d c are discarded for graph iteration, where d c is a scalar threshold.This ensures that background images, like those in front of a white wall, don't confuse the agent for self-localization.</p>
<p>Second, we consider the similarity between observations and existing nodes to decrease graph redundancy.We calculate both visual and pose similarity scores for each observation.The pose similarity score C e i is the eculidean distance between the nearest node in V to the observation o i , defined as follows:
C e i = min ∀j∈0,1...N ||P i − P j || 2(1)
However, the Euclidean distance doesn't account for the house structure.For example, two adjacent bedrooms may be close in eculidean distance, but they are two distinguished instances where we need both nodes for them.To address this, we introduce the visual similarity score C s i , defined as the negative cosine similarity to the most similar node's visual feature.Observations violating the rule C e i + α • C s i &lt; d p are discarded.We append the collected observation as new nodes only if both semantic and similarity scores exceed a threshold, ensuring that the graph is sparse and representative.</p>
<p>Edge Iteration.The edges in the graph represent the transition possibility from one node to another.We employ a binary flag to indicate reachability and dynamically update the edges based on the collected observations.Therefore, the edge e ij exists only if the agent believes it is easy to traverse from the node v i to v j without bypassing any intermediate nodes.In addition to storing the connected node indexes of the edges, we attach optimal trajectory τ ij and visit count c ij as additional attributes of the edge.Thus, every edge e ij is denoted as e ij = (i, j, τ ij ).To iterate the edges, at each timestep, the agent will first locate itself on the graph based on visual and pose similarity.Denote the last localized node as v t−1 , if min ∀j∈0,1...N C e i + α • C s i &lt; d locate , the location will be updated to be v j .Otherwise, the location will keep fixed as v t−1 .The visit count c ij reflects the reachability between the node v i and v j .A higher value of c ij means the necessity of edge e ij in connecting different parts of the scene.And we periodically delete those redundant edges with less c ij values.As the robot working in the house, it may encounter multiple different trajectories leading from node v i to v j .Some of these trajectories are more efficient than others, characterized by smaller transition steps.To construct the dataset for imitation learning, we optimize τ ij by retaining the most efficient trajectory while discarding others.</p>
<p>C. Active Exploration</p>
<p>To acquire a goal-conditional policy π θ (a t |o t , g i ), the diversity of o t and g i in the training set plays a crucial role in determining the generalization performance of the navigation policy.In contrast to conventional settings where dataset diversity can be ensured through explicit dataset curation or simulation, our self-supervised navigation task presents a unique challenge.The task cannot anticipate unseen states, necessitating active exploration to enhance dataset diversity.Consequently, we encourage the agent to navigate towards novel nodes, fostering exploration of new areas.</p>
<p>Leveraging DGMem as an abstraction of the partially observed scene, the novelty of nodes is easily defined using a count-based method.A lower value of the node visit count c i signifies higher novelty.During the training stage, the agent is periodically assigned a node as the navigation goal based on a softmax probability.The probability p i of node i to be chosen as the goal is defined as follows:
p i = e −γci N j=0 e −γcj(2)
where γ is a temperature scalar and N represents the number of the existing nodes.The novelty of nodes is associated with the uncertainty of local areas, and navigating to these uncertain areas provides a better chance to enhance local topology and sequentially unlock new areas.With such an active exploration scheme, the graph memory gradually covers all areas in the house, providing more comprehensive candidates for goal-conditional policy learning.An illustration of the exploration procedure are shown in Fig 3.</p>
<p>D. Data-Efficient Training</p>
<p>As no auxiliary feedback is available in the self-supervised navigation task, a most common reward function is a binary flag distinguishing whether the agent successfully arrives at the goal g t .This can be calculated by measuring the similarity between the agent observation o t with the the image goal g t .But this binary flag alone is a sparse and delayed reward signal which makes data-efficient training infeasible.To speed up the training process, we utilize the DGMem from three aspects: First, besides the binary success flag, we introduce two additional reward signals which are topological distance reward r d and navigation trajectory reward r n .Given an image-goal g i , the agent's current observation o t and the agent's past observation o t−1 , we localize them in the DGMem graph and denote the retrieved nodes as v g t , v o t and v o t−1 .We plan two topological path on the DGMem starting from v o t and v o t−1 to v g t respectively.The difference of the path length indicates whether the robot is getting closer to the goal or not.Therefore, the topological distance reward r d is defined as follows:
r d = α(|l(v t−1 , v g )| − |l(v t , v g )|)(3)
To encourage the robot to traverse more areas, the navigation trajectory reward r n is introduced and defined as the total number count of the passing-by different nodes along the trajectory.
r e = c • I(v i )(4)
where c represents the passing-by node numbers and I represents whether it is the first time going through a node.</p>
<p>And denote the succesfully arriving target reward as r s , the entire reward function can be written as follows:
r t = r d t + r n t + r s t (5)
Second, as the optimal transition navigation trajectories between pairs of nodes are stored in the DGMem, it can serve as demonstration dataset for imitation learning.In order not to intervene with RL training process, similiar to PPG [36], during update with IL objectives, we add a KL-divergence regularization to ensure the policy won't change daramtically.Concretely, during the imitation learning phase, we sample a batch of transition trajectory from the edge set and update the following objective J IL (θ):
J ce (θ) = Êt <a href="6"> at∈A a t log π θ (â t |o t , o g )</a>J reg (θ) = Êt <a href="7">KL[π θ old (•|o t , o g ), π θ (•|o t , o g )]</a>J IL (θ) = J ce (θ) + β • J reg (θ)(8)</p>
<p>E. Hierachical Navigation Policy</p>
<p>During the training stage, we acquire a robust imagegoal navigation policy for navigating towards the recorded images in DGMem within a local area.In the testing stage, the agent will be asked to perform long-horizon navigation tasks, given arbitrary starting and goal images.To that end, we incorporate the planning method to compensate for the gap between training and testing.The entire decision framework are shown in 2. The agent first locates the goal state and its own state based on the provided images in the DGMem.To locate the images into nodes, we use the trained value functions V (o o , g) to measure whether two states are reachable.Denote the localized nodes as v g and v o 0 .Then, we use the BFS to search a shortest path leading from v o 0 to v g .And finally, the navigation actions are executed to navigate from node to node consecutively to finish the entire long-horizon task.</p>
<p>IV. EXPERIMENT</p>
<p>Environment.Our work is based on the Habitat [16] simulator and the publicly available dataset Gibson [18].We evaluate our approach on the task of image-goal navigation in eight scenes of the Habitat simulator [16].The scenes vary in appearance, size, and floors.Evaluation Metrics.We evaluate both exploration ability and navigation ability in the experiments.The reported metrics are:</p>
<p>1) coverage, we digitize the 3D scenes into voxels, coverage represents the proportion of traversed voxels.2) success rate (SR), the fraction of successful episodes, i.e., episodes in which agent arrives at the location with pose distance less than a threshold.3) success weighted by path length (SPL), the standard metric that weighs success by their adherence to the shortest path.</p>
<p>4) distance to goal (DTS), the average distance to the goal pose when episode terminates.Baselines.We compare our agent with the following four exploration methods.</p>
<p>• Random: An agent uses pure random policy.</p>
<p>• Straight: An agent always select MoveAhead but perform random rotation if collision happens.• DP: An agent uses forward dynamic prediction [37] as intrinsic reward for exploration.• RND: An agent uses random network distillation [38] to form the intrinsic rewards.And we compare our agent with the following methods for the evaluation of navigation policy.</p>
<p>• BClone: An agent uses 500 demonstration trajectories each scene for imitation learning.• DAggre: An agent that learns the optimal policy by supervised learning with an expert path planner in the simulator.• PPO: An PPO [39] agent uses the dense reward provided with the simulator.• Sparse: Similar to the former agent, but trained only with the sparse terminate reward.Didactic Example.Before introducing our method on the visual navigation tasks, we first provide an intuition example on GridWorld environment.We use FourRooms environment which contains 256 possible discrete state and 4 possible actions.An perfect exploration agent should try to travel across four rooms which lead to an uniform distribution among all 256 states.We compare our DGM exploration method with RND in this example and visualize the state distribution as well as the graph memory in Fig 5 .We limit the episode time and fix the spawn grid, so hesitation behavior (e.g move up and down) may lead to the failure arriving at all four rooms.As shown in Fig 5, our methods shows consistent exploration behavior and almost achieve uniform state distribution.We also report the navigation performance in Tab I.As the DGMem construct an uniformly distributed graph of the entire scene, the navigation policy can generalize to almost everywhere in the gridworld scenarios.With the goal-conditional policy only learns in a subset with 60 states, the policy generalize to all 256 states and achieves high spl (90+%).Navigation Results.We report the exploration efficiency among all methods in Fig 4 .Our approach can better explore an unknown environment compared with the curiosity modules.From the experiment, we observe that the curiosity-motivated agent tends to repeatedly roam over several regions.The uncertainty in the policy makes it   The middle shows the state distribution of RND method and the right shows ours.We limit the episode time and find it difficult for RND to discover all four rooms but roaming between two rooms which ours keeps a near-uniform distribution across all states.</p>
<p>difficult to perform long-range exploration skills, for example, traversing a long corridor, and walking upstairs to a different floor.In contrast, our scheme owns an explicit search direction proposed by the global policy, which is beneficial for navigation scenarios.Then, we evaluate the multi-goal navigation performance and the results are shown in Table II.BClone method learns the policy by fitting in the expert demonstration dataset, the scale of the demonstrations is the key factor.With 500 trajectories for each scene, BClone shows worse generalization capability to novel goals.DAggre method consistently correct itself by comparing the output actions with an underlying path planner.Although this method can acquire a satisfied performance, especially in SPL.But in many applications, we cannot possess an optimal controller in advance to collect unlimited labeled actions.The PPO method is trained on each scene with 1M interactions.The poor result of the PPO indicates that even if the reward signal is dense, using RL to train a vanilla endto-end architecture to finish the multi-goal image navigation tasks is difficult.The performance gap between point-goal navigation [4] and the image navigation inspires us that how to learn an image representation to capture the spatial information and match the reward signal is an important issue for visual navigation.And if we remove the heuristic reward used in PPO, the agent cannot learn the navigation skills at all.Our self-supervised method, with only onboard sensor inputs, outperforms all baseline methods in success rate and distance to goal in the image-goal navigation task.This proves the design of our approach.In many scenarios, DAggre achieves better SPL compared with our method.This is mainly caused by our planning algorithm.As we force the agent must walk through all the waypoints nodes one by one, the navigation routing becomes sub-optimcal.Therefore, how to design a better global policy, not just using the static path-planning may be valuable for further improvements.And how to incorporate such a graph memory for other tasks is one of our concerns in the future work.</p>
<p>For better illustration of the DGM module, we give an example to show the growing process of the graph in Fig 6 .As the agent interacts with the environment, the edges and nodes are dynamiclly updated and finally constructs the reasonable graph memory.Fig. 6.An example visualization of the DGM generation process.At the beginning of the training process, agent roams around the spawn points, so the nodes gather and some mistaken edges exists.As the training goes, agents gradually travels to further areas so the graph memory grows.Finally, an reasonable abstraction of the entire state spaces is generated and becomes our graph memory.</p>
<p>Ablation Study.As we use self-localization in the training stage to build the graph memory, we need to discuss whether our approach is robust with the pose estimation noise.Here, we introduce three-level scales of gaussian noise (τ = 0.1, τ = 0.2, τ = 0.3) as pose estimation error and we report the average evaluation metrics across in the Table ??.The pose estimation noise only brings a minor influence on the final navigation performance.In fact, the pose estimation only affects the quality of the graph memory.For example, with the pose estimation error, the robot may mistakenly adds some observations as nodes, and this will bring inhomogeneity to the node distribution in local areas.And this add slighty change the training dataset and influcence the training difficulty.But the agents can still distinguish which states are closer with the help of the value function, the value function is not related to the pose estimation error.Therefore, the performance of the entire hierarchical policy will not degrade severely.Implementation Details In our method, the visual encoder ResNet18 are pre-trained on the ImageNet dataset and we freeze the parameters during our policy training.The fusion module concatentate the visual features from observation image and subgoal image, and follows two fully-connected layers with size (512,256).The GRU module contains 256 units.We use the PPO with Adam optimizer to train the network.we set clip ratio=0.1,nsteps=256, nminibatch=1, nepochs=4, learning rate=1e-4, discount factor=0.99 in our experiments.The learning rate will linear decay to 1e-5.We train our approach with 1M steps interactions and save the best model to report the performance.As for the hyperparameters in our method, we set the distance threshold as d c = 1.5,d s = −0.85,de = 1.0 ,the reward coefficient as α = 0.2, c = 0.05,, the imitation learning coefficient β = 0.1.We haven't carefully searched these parameters, maybe a better choice can improve the performance a little.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we present the dynamic graph memory (DG-Mem) module for visual navigation tasks.Many advances in navigation tasks hinge on realistic simulators, which are costly and time-consuming to build.Our DGMem is one way to reuse the collected experience and form an organized memory that attempts to replace the function of simulators.Detailed experiments reveal that the proposed DGM enables consistent exploration and achieves better multi-goal navigation performance compared with those methods trained with supervision or heuristic rewards.By replacing the required attributes in simulators with DGMem, we believe our proposed method reduced the gap for real-world navigation applications.</p>
<p>Fig. 1 .
1
Fig. 1.Explaination of the self-supervised navigation task.a) When training in the simulator, the training feedbacks come from the available meta-data.But in the real-world, it is non-trivial to acquire such meta-data without human guidance for the computation of training objectives.Therefore, in b), the only accessible information is on-board sensor data, like the RGB images from the camera.Our DGMem must evaluate the executed actions to optimize the policy network and constantly perform navigation to collect experience.</p>
<p>Fig. 2 .
2
Fig.2.Framework of our approach.a) Different from traditional RL, in a self-supervised setting, no reward function is available from the environment.b) As the agent interacts with the environment, it builds a graph memory from the experience data.Graph memory is used for both imitation learning and reinforcement learning.c) By localizing the current observation and the goal observation in the DGM, the global policy can decompose the task by planning.Then, the next subgoal node will be conveyed to the local policy.d) The low-level controller will output the actions based on a trained goal-conditional policy.</p>
<p>Fig. 3 .
3
Fig. 3. Exploration strategy.a) Current graph memory.b) DGMem selects a promising node as a goal and encourages navigation towards it.c) Novel states are more likely to be found.d) The updated version of the graph memory is obtained.</p>
<p>Fig. 4 .
4
Fig. 4. The coverage metric during the training stage.Only intrinsic rewards are allowed for training here.Our method outperforms all baselines by a large margin, especially in complicated scenarios.</p>
<p>Fig. 5 .
5
Fig. 5. Visualization of the DGM results and the state distribution among all states.The middle shows the state distribution of RND method and the right shows ours.We limit the episode time and find it difficult for RND to discover all four rooms but roaming between two rooms which ours keeps a near-uniform distribution across all states.</p>
<p>Planning algorithms. S M Lavalle, 2006</p>
<p>Probabilistic robotics. S Thrun, Commun. ACM. 452002</p>
<p>Multiple view geometry in computer vision. B P Wrobel, Künstliche Intell. 15412001</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A S Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, ICLR. 2020</p>
<p>Integrating egocentric localization for more realistic point-goal navigation agents. S Datta, O Maksymets, J Hoffman, S Lee, D Batra, D Parikh, Conference on Robot Learning. 2020</p>
<p>{VTN}et: Visual transformer network for object goal navigation. H Du, X Yu, L Zheng, International Conference on Learning Representations. 2021</p>
<p>Learning hierarchical relationships for object-goal navigation. A Pal, Y Qiu, H I Christensen, Conference on Robot Learning. 2020</p>
<p>Bayesian relational memory for semantic visual navigation. Y Wu, Y Wu, A Tamar, S J Russell, G Gkioxari, Y Tian, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019</p>
<p>Seeing the un-scene: Learning amodal semantic maps for room navigation. M G Narasimhan, E Wijmans, X Chen, T Darrell, D Batra, D Parikh, A Singh, ArXiv. 2007.09841. 2020</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A K Gupta, L Fei-Fei, A Farhadi, IEEE. 2017. 2016</p>
<p>Image-goal navigation via keypoint-based reinforcement learning. Y Choi, S Oh, 2021 18th International Conference on Ubiquitous Robots (UR). 2021</p>
<p>Structured scene memory for vision-language navigation. H Wang, W Wang, W Liang, C Xiong, J Shen, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</p>
<p>Learning to set waypoints for audio-visual navigation. C Chen, S Majumder, Z Al-Halah, R Gao, S K Ramakrishnan, K Grauman, arXiv: Computer Vision and Pattern Recognition. 2020</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022248006501</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, A Kembhavi, A K Gupta, A Farhadi, abs/1712.05474ArXiv. 2017</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A X Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, ArXiv. 2021</p>
<p>igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Mart'in-Mart'in, L J Fan, G Wang, S Buch, C P D'arpino, S Srivastava, L P Tchapmi, M Tchapmi, K Vainio, L Fei-Fei, S Savarese, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020</p>
<p>0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Mart'in-Mart'in, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, A Kurenkov, K Liu, H Gweon, J Wu, L Fei-Fei, S Savarese, abs/2108.03272ArXiv. 2.2021</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M A Riedmiller, A Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 5182015</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, International Conference on Learning Representations. 2017</p>
<p>Scene memory transformer for embodied agents in long-horizon tasks. K Fang, A Toshev, L Fei-Fei, S Savarese, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019. 2019</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, International Conference on Learning Representations. 2019</p>
<p>Semantic visual navigation by watching youtube videos. M Chang, A Gupta, S Gupta, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>No RL, no simulation: Learning to navigate without navigating. M Hahn, D S Chaplot, S Tulsiani, M Mukadam, J M Rehg, A Gupta, Advances in Neural Information Processing Systems. Y Beygelzimer, P Dauphin, J W Liang, Vaughan, 2021</p>
<p>Learning generalizable robotic reward functions from "in-the-wild" human videos. A S Chen, S Nair, C Finn, ArXiv. 2103.16817, 2021</p>
<p>Learning one representation to optimize all rewards. A Touati, Y Ollivier, Advances in Neural Information Processing Systems. Y Beygelzimer, P Dauphin, J W Liang, Vaughan, 2021</p>
<p>Pretraining representations for data-efficient reinforcement learning. M Schwarzer, N Rajkumar, M Noukhovitch, A Anand, L Charlin, D Hjelm, P Bachman, A C Courville, Neural Information Processing Systems. 2021</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019</p>
<p>Dynamics-aware unsupervised discovery of skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, International Conference on Learning Representations. 2020</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, International Conference on Learning Representations. 2020</p>
<p>Learning to map for active semantic goal navigation. G Georgakis, B Bucher, K Schmeckpeper, S Singh, K Daniilidis, International Conference on Learning Representations. 2022</p>
<p>Visual graph memory with unsupervised representation for visual navigation. O Kwon, N Kim, Y Choi, H Yoo, J Park, S Oh, 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015</p>
<p>Yolov5: A state-of-the-art real-time object detection system. Ultralytics, 2021</p>
<p>Phasic policy gradient. K Cobbe, J Hilton, O Klimov, J Schulman, International Conference on Machine Learning. 2020</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A J Storkey, O Klimov, abs/1810.12894ArXiv. 2018</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, ArXiv. 2017</p>            </div>
        </div>

    </div>
</body>
</html>
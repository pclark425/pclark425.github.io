<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7447 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7447</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7447</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-277043624</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.11082v1.pdf" target="_blank">LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable performance in code completion. However, the training data used to develop these models often contain a significant amount of buggy code. Yet, it remains unclear to what extent these buggy instances influence LLMs' performance when tackling bug-prone code completion tasks. To fill this gap, this paper presents the first empirical study evaluating the performance of LLMs in completing bug-prone code. Through extensive experiments on 7 LLMs and the Defects4J dataset, we analyze LLMs' accuracy, robustness, and limitations in this challenging context. Our experimental results show that completing bug-prone code is significantly more challenging for LLMs than completing normal code. Notably, in bug-prone tasks, the likelihood of LLMs generating correct code is nearly the same as generating buggy code, and it is substantially lower than in normal code completion tasks (e.g., 12.27% vs. 29.85% for GPT-4). To our surprise, 44.44% of the bugs LLMs make are completely identical to the pre-fix version, indicating that LLMs have been seriously biased by historical bugs when completing code. Additionally, we investigate the effectiveness of existing post-processing techniques and find that while they can improve consistency, they do not significantly reduce error rates in bug-prone code scenarios. Our research highlights the limitations of current LLMs in handling bug-prone code and underscores the need for improved models and post-processing strategies to enhance code completion accuracy in real-world development environments.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7447.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7447.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bug-prone vs Normal Completion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-line Code Completion: Bug-prone vs Normal Contexts (Defects4J)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of LLM performance when completing a single next line of code given contexts drawn from historically buggy (pre-fix) code versus normal (correct) code from the same files; shows large drops in exact-match correctness in bug‑prone presentations for multiple models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-GPT4; OpenAI-GPT3.5; OpenAI-GPT4o; CodeLlama-13B-hf; Gemma-7B; StarCoder2-15B; CodeGEN-350M; DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various autoregressive large language models for code (proprietary OpenAI GPT family and several open-source code models) used zero-shot to continue code given preceding lines; training regimes differ by provider (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (proprietary / 13B / 7B / 15B / 350M as reported for specific models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-line code completion on Defects4J (bug-prone and matched normal lines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given the preceding lines of a Java file up to the buggy/fix location, produce the immediate next line of code; completions compared to both the historical buggy line and the fixed line.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Code snippet fill-in (single-line completion): the model receives the preceding lines of the file as context and is asked to output only the continuation line.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot single example per task: provide file lines up to (but not including) the target line; instruction in prompt: 'provide only the continuation of the code, without any additional comments'; no few-shot examples; comparison against both buggy (pre-fix) and fixed (post-fix) lines using LCS+LED similarity metric; threshold 0.4 to mark dissimilar outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correct-identical rate (Exact match to fixed line) measured as percentage of 546 tasks; evaluation metric uses combined normalized LCS and normalized inverse Levenshtein (LCS+LED).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples reported: GPT-4 Correct-identical: 67/546 = 12.27% on bug-prone tasks vs 163/546 = 29.85% on normal tasks; GPT-3.5 Correct-identical: 125/546 = 22.89% (bug-prone) vs 248/546 = 45.42% (normal). DeepSeek-R1 (reasoning model) completely correct rate: bug-prone 13.92% vs normal 29.85%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Normal-code completion (matched non-buggy lines) performance on same models (e.g., GPT-4: 29.85% Correct-identical on normal vs 12.27% on bug-prone).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Absolute declines of similar magnitude reported; e.g., GPT-4: -17.58 percentage points (12.27% <- 29.85%) in Correct-identical exact matches when moving from normal to bug-prone format.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>546 tasks drawn from Defects4J; five runs per model; default model parameters except temperature studies done separately; evaluation compares only the first completed line; threshold 0.4 for dissimilarity.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7447.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt: 'Only continuation'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-style Prompt Restricting Output to Code Continuation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal instruction prompt used across experiments asking LLMs to return only the code continuation (no commentary or repeated original code); used to reduce extraneous tokens and standardize output format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-GPT3.5 / GPT-4 / GPT-4o and open-source code LLMs (CodeLlama, Gemma, StarCoder, CodeGEN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLMs fine-tuned or adapted for code tasks; proprietary ones from OpenAI and open models hosted via HuggingFace.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (proprietary or model-specific sizes as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-line code completion</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complete the provided code snippet's next line; the prompt explicitly asks to provide only the continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction + code context (code-only output requested)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Instruction placed before the code context: 'I need to complete the following code snippet. Please provide only the continuation of the code, without any additional comments or repeating the original code. The code is as follows:{code}'; zero-shot, single-shot style used for all completions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirect: used as the standard prompt across experiments; performance measured by LCS+LED similarity categories (Correct-identical / Correct-close / Bug-identical / Bug-close / Non-compliant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>No direct ablation numbers isolating this prompt vs an alternative are reported; it is the baseline presentation used for all model evaluations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Applied uniformly across all evaluated models; responses constrained to code-only to simplify automated similarity scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7447.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Engineering (JSON selection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Engineering for Candidate Selection Using Structured JSON Output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection-prompt style where an LLM (used as a judge) receives multiple candidate completions and is prompted (instructions first) to return a single-letter JSON identifying the best candidate; reported to improve stability and selection quality for some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-GPT3.5 (used as selector) and other models as candidate generators</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used in a meta-evaluation role: scores or picks best candidate when given multiple completions; candidate generators include GPT variants and open models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / varying</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Candidate selection from multiple generated completions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given N candidate continuations for the same code context, prompt an LLM to return the single best candidate in structured JSON (e.g., {"answer":"B"}).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured judge prompt (JSON output) fed with candidates and code context; single-character answer in JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / meta-selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Instructions placed at the beginning of the prompt (following Guo et al. design); specify output format JSON to encourage stable, machine-parseable responses. Example provided to annotators and in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correct completion rate after applying PE selection (Correct-close + Correct-identical / total tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PE across models: average number of correct completions reported as 149 (48.9%) in one cross-model voting/selection experiment; when applied with OpenAI-GPT3.5 as selector over temperature-varied candidates, PE achieved the best reported correct rate of 57.5% on that configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Direct comparison to Majority Voting (Voting avg correct 166, 51.0%) and a learned Selection model (Sel. avg correct 114, 47.5%); PE improved peak selection (57.5%) in the OpenAI-GPT3.5-specific setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Relative to the best raw model in some settings, PE sometimes improved selection (peak 57.5% vs other post-processing averages ~51%); compared to Voting average it may be lower or similar depending on setup (PE avg 48.9% vs Voting 51.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Selection performed over multiple candidates generated by same or different models and/or temperature values; OpenAI-GPT3.5 used as selector with JSON output enforced; number of candidates not always identical across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7447.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority Voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority Voting / Consensus Selection over Multiple Candidates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-processing approach that computes pairwise similarities between generated candidate completions and selects the candidate with highest average similarity (consensus); reported to be the most effective of the three post-processing approaches on average.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to candidate outputs from the seven evaluated LLMs (OpenAI GPT family, CodeLlama, Gemma, StarCoder2, CodeGEN).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Post-processing algorithm that operates on multiple textual completions (no retraining of generator); similarity computed using same LCS+LED metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (post-processing method applied to outputs of various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selecting best completion from multiple candidate outputs (cross-model or cross-temperature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given N completions for a code context, compute average pairwise similarity per candidate and pick candidate with highest average similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Aggregation/post-processing of multiple generated code completions (each a code-line string).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>N candidates generated (method varies); similarity function sim(Ci,Cj) uses same LCS+LED combination as evaluation; choose Ci with largest mean sim to others; applied both across different models at same temperature and across temperature settings within a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number and proportion of correct completions (Correct-close + Correct-identical) after selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported averages: Voting selected results produced 166 correct completions on average (51.0% correct) in cross-model experiments; across temperatures Voting averaged 173 correct (51.0%). Voting outperformed PE (avg 149 correct, 48.9%) and learned selection model (Sel., avg 114 correct, 47.5%) on average.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to raw best-performing single model settings (in some cases a single tuned model exceeded post-processing), but Voting generally improved robustness versus individual low-performing settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Voting improved average correct counts relative to the PE and Sel. post-processing baselines by +17 and +52 correct completions respectively (absolute counts over 546 tasks), corresponding to a few-percentage-point improvement in correct rate (e.g., 51.0% vs 48.9% and 47.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Applied to outputs from 7 models or from a single model across 7 temperatures (0.1..1.6); similarity and selection use normalized LCS+LED metric.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7447.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection Model (CodeT5+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned Candidate Selection Using a Trained CodeT5+ Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised selection model (CodeT5+ fine-tuned on a bug-fix dataset) that scores candidate completions for defect-likelihood and selects the candidate with highest predicted probability of being defect-free.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeT5+ (selection model) applied to candidate completions generated by primary LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CodeT5+ (pretrained model for code) further trained on ~103,585 single-statement bug-fix patches to predict whether a line is defective; used to rank and select generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeT5+ (model size not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Candidate ranking for defect-free selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Score each candidate completion for likelihood of being non-defective and select highest-scoring candidate as final completion.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Supervised ranking / classification acting on candidate code-line strings; trained on pre/post single-line patch pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>post-processing / selection model</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Training set: Java bug-fix dataset with 103,585 single-statement patches (pre and post lines used); CodeT5+ trained to predict defectiveness; selection uses highest predicted probability of being defect-free among generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correct completion rate (Correct-close + Correct-identical) after learned selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average selection performance reported: Sel. average correct completions = 114 (47.5% correct) in one cross-model experiment; however Sel. achieved a peak correct rate of 58.0% at temperature 1.5 in a particular configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to Voting (avg 166 correct, 51.0%) and PE (avg 149 correct, 48.9%); Sel. had lower average performance but reached higher peak at a specific temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Peak Sel. at temperature 1.5: +7.0 percentage points over Voting average (58.0% vs Voting's ~51.0%) in that specific setting; average performance lower than Voting.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Selection model trained on separate bug-fix dataset; applied to candidate outputs generated under different temperature settings or across models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7447.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature (sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding Temperature Settings and Their Impact on Code Completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic investigation of sampling temperature effects: higher temperature increases diversity and non-compliant outputs, reduces exact-match correct and exact-match buggy outputs, and reduces memorization (repeating training-set lines); an intermediate temperature (~0.7) gives best quality for GPT-3.5 in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-GPT3.5 primarily (experiments also repeated across other models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive generator where temperature controls randomness of token sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-line code completion across temperatures</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complete next line given context while varying decoding temperature to observe changes in quality, memorization, and non-compliant outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same code completion prompt but with temperature varied (0.1, 0.4, 0.7, 1.0, 1.3, 1.6 etc.) during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>decoding hyperparameter / prompt sampling</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Temperatures explored include 0.1, 0.4, 0.7, 1.0, 1.3, 1.6 (and 1.5 in specific Sel. report); analysis includes counts of Correct-identical, Bug-identical, Correct-close, Bug-close, and Non-compliant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Counts/proportions of completions in the evaluation categories (Correct-identical, Bug-identical, Correct-close, Bug-close, Non-compliant); memorization rate of identical reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Observed trend: increasing temperature increases Non-compliant outputs and decreases both Correct-identical and Bug-identical completions; best overall code-completion quality for GPT-3.5 observed near temperature ≈ 0.7. Memorization (exact reproductions from training data) decreases as temperature increases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Default settings used otherwise; baseline comparative is low-temperature (more deterministic) decoding which yields higher exact matches but more memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: moving from low temp to high temp reduces exact-match rates and memorization while increasing non-compliant outputs; no per-temperature p-values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multiple temperatures tested for each model; five runs per model; other decoding parameters kept default except temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7447.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token Length (context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context Token Length (Max Context) Effects on Completion Quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variation of the provided context length (token window) influences completion alignment: a context around 600 tokens gave a 'sweet spot' for GPT-3.5 with best tradeoff between quality and cost; longer contexts increased misalignment with fixed (correct) code for bug-prone tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-GPT3.5 (primary experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive LLM where context window/token-length controls how much prior code is given to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-line completion with varied context token length</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide varying amounts of preceding code (token length) as context to the model and measure completion similarity to fixed/buggy lines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Code snippet context length varied (token length param) with same single-line completion prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Token length varied across a range (e.g., up to 1000 tokens); measurement of correctness categories recorded; reported 'sweet spot' at ~600 tokens for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correctness categories using LCS+LED (Correct-identical etc.) and counts of Non-compliant outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitative finding: context length 600 tokens achieved optimal balance between code-completion quality and efficiency for GPT-3.5; longer context increased misalignment with correct code in bug-prone tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Shorter or much longer contexts compared to 600-token baseline; exact numeric deltas not tabulated beyond plotted trends.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative change: improvement up to token length ≈ 600, then declining alignment beyond that length (no exact percentage point values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3.5 experiments varying both temperature and token length; other parameters default.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7447.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7447.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-line Completion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-line Completion vs Single-line: Format Scope of Completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extension of the task from single-line to two-line completions; results are consistent with single-line findings — models still produce similar proportions of correct vs buggy outputs and post-processing does not fundamentally alter error propensity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs evaluated (OpenAI GPT variants, Gemma-7B, CodeLlama-13B-hf, StarCoder2-15B, CodeGEN-350M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same families of autoregressive code models used for single-line tasks, asked to complete two subsequent lines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various as above</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Two-line code completion (immediate next two lines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given the code up to the bug location, produce the next two lines of code; evaluate performance categories similarly to single-line experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Code snippet fill-in with multi-line target (two lines).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / output scope</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two-line completions evaluated with same LCS+LED similarity metric and same categorical thresholds; post-processing approaches (Voting, PE, Sel.) applied similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Counts/proportions of correct/bad/non-compliant completions over the two-line tasks after selection/post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: extending to two-line completions yielded consistent patterns with single-line results; post-processing approaches did not substantially reduce error rates (specific counts reported in paper tables but overall trend described qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Single-line completion performance on same models (baseline) — two-line results consistent with single-line trends.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No major improvement or degradation relative to single-line trend; error propensity remained similar across scopes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Two-line evaluations performed on same set of models and with same post-processing pipelines; results in Tables VI/VIII/IX in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Defects4J: A database of existing faults to enable controlled testing studies for java programs <em>(Rating: 2)</em></li>
                <li>ConDefects: A complementary dataset to address the data leakage concern for llm-based fault localization and program repair <em>(Rating: 2)</em></li>
                <li>Code llama: Open foundation models for code <em>(Rating: 2)</em></li>
                <li>Code-gen2: Lessons for training llms on programming and natural languages <em>(Rating: 2)</em></li>
                <li>Prompt Engineering (Guo et al.) <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7447",
    "paper_id": "paper-277043624",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Bug-prone vs Normal Completion",
            "name_full": "Single-line Code Completion: Bug-prone vs Normal Contexts (Defects4J)",
            "brief_description": "Comparison of LLM performance when completing a single next line of code given contexts drawn from historically buggy (pre-fix) code versus normal (correct) code from the same files; shows large drops in exact-match correctness in bug‑prone presentations for multiple models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-GPT4; OpenAI-GPT3.5; OpenAI-GPT4o; CodeLlama-13B-hf; Gemma-7B; StarCoder2-15B; CodeGEN-350M; DeepSeek-R1",
            "model_description": "Various autoregressive large language models for code (proprietary OpenAI GPT family and several open-source code models) used zero-shot to continue code given preceding lines; training regimes differ by provider (not detailed in this paper).",
            "model_size": "various (proprietary / 13B / 7B / 15B / 350M as reported for specific models)",
            "task_name": "Single-line code completion on Defects4J (bug-prone and matched normal lines)",
            "task_description": "Given the preceding lines of a Java file up to the buggy/fix location, produce the immediate next line of code; completions compared to both the historical buggy line and the fixed line.",
            "problem_format": "Code snippet fill-in (single-line completion): the model receives the preceding lines of the file as context and is asked to output only the continuation line.",
            "format_category": "input modality",
            "format_details": "Zero-shot single example per task: provide file lines up to (but not including) the target line; instruction in prompt: 'provide only the continuation of the code, without any additional comments'; no few-shot examples; comparison against both buggy (pre-fix) and fixed (post-fix) lines using LCS+LED similarity metric; threshold 0.4 to mark dissimilar outputs.",
            "performance_metric": "Correct-identical rate (Exact match to fixed line) measured as percentage of 546 tasks; evaluation metric uses combined normalized LCS and normalized inverse Levenshtein (LCS+LED).",
            "performance_value": "Examples reported: GPT-4 Correct-identical: 67/546 = 12.27% on bug-prone tasks vs 163/546 = 29.85% on normal tasks; GPT-3.5 Correct-identical: 125/546 = 22.89% (bug-prone) vs 248/546 = 45.42% (normal). DeepSeek-R1 (reasoning model) completely correct rate: bug-prone 13.92% vs normal 29.85%.",
            "baseline_performance": "Normal-code completion (matched non-buggy lines) performance on same models (e.g., GPT-4: 29.85% Correct-identical on normal vs 12.27% on bug-prone).",
            "performance_change": "Absolute declines of similar magnitude reported; e.g., GPT-4: -17.58 percentage points (12.27% &lt;- 29.85%) in Correct-identical exact matches when moving from normal to bug-prone format.",
            "experimental_setting": "546 tasks drawn from Defects4J; five runs per model; default model parameters except temperature studies done separately; evaluation compares only the first completed line; threshold 0.4 for dissimilarity.",
            "statistical_significance": null,
            "uuid": "e7447.0",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prompt: 'Only continuation'",
            "name_full": "Instruction-style Prompt Restricting Output to Code Continuation",
            "brief_description": "A minimal instruction prompt used across experiments asking LLMs to return only the code continuation (no commentary or repeated original code); used to reduce extraneous tokens and standardize output format.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-GPT3.5 / GPT-4 / GPT-4o and open-source code LLMs (CodeLlama, Gemma, StarCoder, CodeGEN)",
            "model_description": "Autoregressive LLMs fine-tuned or adapted for code tasks; proprietary ones from OpenAI and open models hosted via HuggingFace.",
            "model_size": "various (proprietary or model-specific sizes as above)",
            "task_name": "Single-line code completion",
            "task_description": "Complete the provided code snippet's next line; the prompt explicitly asks to provide only the continuation.",
            "problem_format": "Natural-language instruction + code context (code-only output requested)",
            "format_category": "prompt style",
            "format_details": "Instruction placed before the code context: 'I need to complete the following code snippet. Please provide only the continuation of the code, without any additional comments or repeating the original code. The code is as follows:{code}'; zero-shot, single-shot style used for all completions.",
            "performance_metric": "Indirect: used as the standard prompt across experiments; performance measured by LCS+LED similarity categories (Correct-identical / Correct-close / Bug-identical / Bug-close / Non-compliant).",
            "performance_value": "No direct ablation numbers isolating this prompt vs an alternative are reported; it is the baseline presentation used for all model evaluations in the paper.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Applied uniformly across all evaluated models; responses constrained to code-only to simplify automated similarity scoring.",
            "statistical_significance": null,
            "uuid": "e7447.1",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prompt Engineering (JSON selection)",
            "name_full": "Prompt Engineering for Candidate Selection Using Structured JSON Output",
            "brief_description": "A selection-prompt style where an LLM (used as a judge) receives multiple candidate completions and is prompted (instructions first) to return a single-letter JSON identifying the best candidate; reported to improve stability and selection quality for some configurations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-GPT3.5 (used as selector) and other models as candidate generators",
            "model_description": "GPT-3.5 used in a meta-evaluation role: scores or picks best candidate when given multiple completions; candidate generators include GPT variants and open models.",
            "model_size": "proprietary / varying",
            "task_name": "Candidate selection from multiple generated completions",
            "task_description": "Given N candidate continuations for the same code context, prompt an LLM to return the single best candidate in structured JSON (e.g., {\"answer\":\"B\"}).",
            "problem_format": "Structured judge prompt (JSON output) fed with candidates and code context; single-character answer in JSON.",
            "format_category": "prompt style / meta-selection",
            "format_details": "Instructions placed at the beginning of the prompt (following Guo et al. design); specify output format JSON to encourage stable, machine-parseable responses. Example provided to annotators and in experiments.",
            "performance_metric": "Correct completion rate after applying PE selection (Correct-close + Correct-identical / total tasks).",
            "performance_value": "PE across models: average number of correct completions reported as 149 (48.9%) in one cross-model voting/selection experiment; when applied with OpenAI-GPT3.5 as selector over temperature-varied candidates, PE achieved the best reported correct rate of 57.5% on that configuration.",
            "baseline_performance": "Direct comparison to Majority Voting (Voting avg correct 166, 51.0%) and a learned Selection model (Sel. avg correct 114, 47.5%); PE improved peak selection (57.5%) in the OpenAI-GPT3.5-specific setup.",
            "performance_change": "Relative to the best raw model in some settings, PE sometimes improved selection (peak 57.5% vs other post-processing averages ~51%); compared to Voting average it may be lower or similar depending on setup (PE avg 48.9% vs Voting 51.0%).",
            "experimental_setting": "Selection performed over multiple candidates generated by same or different models and/or temperature values; OpenAI-GPT3.5 used as selector with JSON output enforced; number of candidates not always identical across experiments.",
            "statistical_significance": null,
            "uuid": "e7447.2",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Majority Voting",
            "name_full": "Majority Voting / Consensus Selection over Multiple Candidates",
            "brief_description": "Post-processing approach that computes pairwise similarities between generated candidate completions and selects the candidate with highest average similarity (consensus); reported to be the most effective of the three post-processing approaches on average.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to candidate outputs from the seven evaluated LLMs (OpenAI GPT family, CodeLlama, Gemma, StarCoder2, CodeGEN).",
            "model_description": "Post-processing algorithm that operates on multiple textual completions (no retraining of generator); similarity computed using same LCS+LED metric.",
            "model_size": "N/A (post-processing method applied to outputs of various sizes)",
            "task_name": "Selecting best completion from multiple candidate outputs (cross-model or cross-temperature)",
            "task_description": "Given N completions for a code context, compute average pairwise similarity per candidate and pick candidate with highest average similarity.",
            "problem_format": "Aggregation/post-processing of multiple generated code completions (each a code-line string).",
            "format_category": "prompt style / post-processing",
            "format_details": "N candidates generated (method varies); similarity function sim(Ci,Cj) uses same LCS+LED combination as evaluation; choose Ci with largest mean sim to others; applied both across different models at same temperature and across temperature settings within a single model.",
            "performance_metric": "Number and proportion of correct completions (Correct-close + Correct-identical) after selection.",
            "performance_value": "Reported averages: Voting selected results produced 166 correct completions on average (51.0% correct) in cross-model experiments; across temperatures Voting averaged 173 correct (51.0%). Voting outperformed PE (avg 149 correct, 48.9%) and learned selection model (Sel., avg 114 correct, 47.5%) on average.",
            "baseline_performance": "Compared to raw best-performing single model settings (in some cases a single tuned model exceeded post-processing), but Voting generally improved robustness versus individual low-performing settings.",
            "performance_change": "Voting improved average correct counts relative to the PE and Sel. post-processing baselines by +17 and +52 correct completions respectively (absolute counts over 546 tasks), corresponding to a few-percentage-point improvement in correct rate (e.g., 51.0% vs 48.9% and 47.5%).",
            "experimental_setting": "Applied to outputs from 7 models or from a single model across 7 temperatures (0.1..1.6); similarity and selection use normalized LCS+LED metric.",
            "statistical_significance": null,
            "uuid": "e7447.3",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Selection Model (CodeT5+)",
            "name_full": "Learned Candidate Selection Using a Trained CodeT5+ Classifier",
            "brief_description": "A supervised selection model (CodeT5+ fine-tuned on a bug-fix dataset) that scores candidate completions for defect-likelihood and selects the candidate with highest predicted probability of being defect-free.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CodeT5+ (selection model) applied to candidate completions generated by primary LLMs",
            "model_description": "CodeT5+ (pretrained model for code) further trained on ~103,585 single-statement bug-fix patches to predict whether a line is defective; used to rank and select generated candidates.",
            "model_size": "CodeT5+ (model size not specified in paper)",
            "task_name": "Candidate ranking for defect-free selection",
            "task_description": "Score each candidate completion for likelihood of being non-defective and select highest-scoring candidate as final completion.",
            "problem_format": "Supervised ranking / classification acting on candidate code-line strings; trained on pre/post single-line patch pairs.",
            "format_category": "post-processing / selection model",
            "format_details": "Training set: Java bug-fix dataset with 103,585 single-statement patches (pre and post lines used); CodeT5+ trained to predict defectiveness; selection uses highest predicted probability of being defect-free among generated candidates.",
            "performance_metric": "Correct completion rate (Correct-close + Correct-identical) after learned selection.",
            "performance_value": "Average selection performance reported: Sel. average correct completions = 114 (47.5% correct) in one cross-model experiment; however Sel. achieved a peak correct rate of 58.0% at temperature 1.5 in a particular configuration.",
            "baseline_performance": "Compared to Voting (avg 166 correct, 51.0%) and PE (avg 149 correct, 48.9%); Sel. had lower average performance but reached higher peak at a specific temperature.",
            "performance_change": "Peak Sel. at temperature 1.5: +7.0 percentage points over Voting average (58.0% vs Voting's ~51.0%) in that specific setting; average performance lower than Voting.",
            "experimental_setting": "Selection model trained on separate bug-fix dataset; applied to candidate outputs generated under different temperature settings or across models.",
            "statistical_significance": null,
            "uuid": "e7447.4",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Temperature (sampling)",
            "name_full": "Decoding Temperature Settings and Their Impact on Code Completion",
            "brief_description": "Systematic investigation of sampling temperature effects: higher temperature increases diversity and non-compliant outputs, reduces exact-match correct and exact-match buggy outputs, and reduces memorization (repeating training-set lines); an intermediate temperature (~0.7) gives best quality for GPT-3.5 in these experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-GPT3.5 primarily (experiments also repeated across other models)",
            "model_description": "Autoregressive generator where temperature controls randomness of token sampling.",
            "model_size": "proprietary",
            "task_name": "Single-line code completion across temperatures",
            "task_description": "Complete next line given context while varying decoding temperature to observe changes in quality, memorization, and non-compliant outputs.",
            "problem_format": "Same code completion prompt but with temperature varied (0.1, 0.4, 0.7, 1.0, 1.3, 1.6 etc.) during decoding.",
            "format_category": "decoding hyperparameter / prompt sampling",
            "format_details": "Temperatures explored include 0.1, 0.4, 0.7, 1.0, 1.3, 1.6 (and 1.5 in specific Sel. report); analysis includes counts of Correct-identical, Bug-identical, Correct-close, Bug-close, and Non-compliant.",
            "performance_metric": "Counts/proportions of completions in the evaluation categories (Correct-identical, Bug-identical, Correct-close, Bug-close, Non-compliant); memorization rate of identical reproductions.",
            "performance_value": "Observed trend: increasing temperature increases Non-compliant outputs and decreases both Correct-identical and Bug-identical completions; best overall code-completion quality for GPT-3.5 observed near temperature ≈ 0.7. Memorization (exact reproductions from training data) decreases as temperature increases.",
            "baseline_performance": "Default settings used otherwise; baseline comparative is low-temperature (more deterministic) decoding which yields higher exact matches but more memorization.",
            "performance_change": "Qualitative: moving from low temp to high temp reduces exact-match rates and memorization while increasing non-compliant outputs; no per-temperature p-values reported.",
            "experimental_setting": "Multiple temperatures tested for each model; five runs per model; other decoding parameters kept default except temperature.",
            "statistical_significance": null,
            "uuid": "e7447.5",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Token Length (context)",
            "name_full": "Context Token Length (Max Context) Effects on Completion Quality",
            "brief_description": "Variation of the provided context length (token window) influences completion alignment: a context around 600 tokens gave a 'sweet spot' for GPT-3.5 with best tradeoff between quality and cost; longer contexts increased misalignment with fixed (correct) code for bug-prone tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-GPT3.5 (primary experiments reported)",
            "model_description": "Transformer-based autoregressive LLM where context window/token-length controls how much prior code is given to the model.",
            "model_size": "proprietary",
            "task_name": "Single-line completion with varied context token length",
            "task_description": "Provide varying amounts of preceding code (token length) as context to the model and measure completion similarity to fixed/buggy lines.",
            "problem_format": "Code snippet context length varied (token length param) with same single-line completion prompt.",
            "format_category": "input modality / prompt context",
            "format_details": "Token length varied across a range (e.g., up to 1000 tokens); measurement of correctness categories recorded; reported 'sweet spot' at ~600 tokens for GPT-3.5.",
            "performance_metric": "Correctness categories using LCS+LED (Correct-identical etc.) and counts of Non-compliant outputs.",
            "performance_value": "Reported qualitative finding: context length 600 tokens achieved optimal balance between code-completion quality and efficiency for GPT-3.5; longer context increased misalignment with correct code in bug-prone tasks.",
            "baseline_performance": "Shorter or much longer contexts compared to 600-token baseline; exact numeric deltas not tabulated beyond plotted trends.",
            "performance_change": "Qualitative change: improvement up to token length ≈ 600, then declining alignment beyond that length (no exact percentage point values reported).",
            "experimental_setting": "GPT-3.5 experiments varying both temperature and token length; other parameters default.",
            "statistical_significance": null,
            "uuid": "e7447.6",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Multi-line Completion",
            "name_full": "Two-line Completion vs Single-line: Format Scope of Completion",
            "brief_description": "Extension of the task from single-line to two-line completions; results are consistent with single-line findings — models still produce similar proportions of correct vs buggy outputs and post-processing does not fundamentally alter error propensity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs evaluated (OpenAI GPT variants, Gemma-7B, CodeLlama-13B-hf, StarCoder2-15B, CodeGEN-350M)",
            "model_description": "Same families of autoregressive code models used for single-line tasks, asked to complete two subsequent lines.",
            "model_size": "various as above",
            "task_name": "Two-line code completion (immediate next two lines)",
            "task_description": "Given the code up to the bug location, produce the next two lines of code; evaluate performance categories similarly to single-line experiments.",
            "problem_format": "Code snippet fill-in with multi-line target (two lines).",
            "format_category": "input modality / output scope",
            "format_details": "Two-line completions evaluated with same LCS+LED similarity metric and same categorical thresholds; post-processing approaches (Voting, PE, Sel.) applied similarly.",
            "performance_metric": "Counts/proportions of correct/bad/non-compliant completions over the two-line tasks after selection/post-processing.",
            "performance_value": "Reported: extending to two-line completions yielded consistent patterns with single-line results; post-processing approaches did not substantially reduce error rates (specific counts reported in paper tables but overall trend described qualitatively).",
            "baseline_performance": "Single-line completion performance on same models (baseline) — two-line results consistent with single-line trends.",
            "performance_change": "No major improvement or degradation relative to single-line trend; error propensity remained similar across scopes.",
            "experimental_setting": "Two-line evaluations performed on same set of models and with same post-processing pipelines; results in Tables VI/VIII/IX in the paper.",
            "statistical_significance": null,
            "uuid": "e7447.7",
            "source_info": {
                "paper_title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Defects4J: A database of existing faults to enable controlled testing studies for java programs",
            "rating": 2,
            "sanitized_title": "defects4j_a_database_of_existing_faults_to_enable_controlled_testing_studies_for_java_programs"
        },
        {
            "paper_title": "ConDefects: A complementary dataset to address the data leakage concern for llm-based fault localization and program repair",
            "rating": 2,
            "sanitized_title": "condefects_a_complementary_dataset_to_address_the_data_leakage_concern_for_llmbased_fault_localization_and_program_repair"
        },
        {
            "paper_title": "Code llama: Open foundation models for code",
            "rating": 2,
            "sanitized_title": "code_llama_open_foundation_models_for_code"
        },
        {
            "paper_title": "Code-gen2: Lessons for training llms on programming and natural languages",
            "rating": 2,
            "sanitized_title": "codegen2_lessons_for_training_llms_on_programming_and_natural_languages"
        },
        {
            "paper_title": "Prompt Engineering (Guo et al.)",
            "rating": 1,
            "sanitized_title": "prompt_engineering_guo_et_al"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.021189999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code
14 Mar 2025</p>
<p>Liwei Guo liwei.glw@outlook.com 
Sixiang Ye 
Zeyu Sun zeyu.zys@gmail.com 
Xiang Chen 
Yuxia Zhang yuxiazh@bit.edu.cn. 
Bo Wang wangbocs@bjtu.edu.cn. 
Jie M Zhang jie.zhang@kcl.ac.uk 
Zheng Li lizheng@mail.buct.edu.cn. 
Yong Liu </p>
<p>Beijing University of Chemical Technology
BeijingChina</p>
<p>Beijing University of Chemical Technology
BeijingChina</p>
<p>Institute of Software
National Key Laboratory of Space Integrated Information System
Chinese Academy of Sciences
BeijingChina</p>
<p>Nantong University
NantongChina</p>
<p>Yuxia Zhang is with Beijing Institute of Technology
BeijingChina</p>
<p>Beijing Jiaotong University
BeijingChina</p>
<p>is with King's College London
LondonUK</p>
<p>Beijing University of Chemical Technology
BeijingChina</p>
<p>Beijing University of Chemical Technology
BeijingChina</p>
<p>LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code
14 Mar 2025691EEE60E9FF8B990EE17F3C9F9FF166arXiv:2503.11082v1[cs.SE]Code completionLarge language modelsBugprone code completion PathIterator iterator2 = p1.getPathIterator(null); 275
Large Language Models (LLMs) have demonstrated remarkable performance in code completion.However, the training data used to develop these models often contain a significant amount of buggy code.Yet, it remains unclear to what extent these buggy instances influence LLMs' performance when tackling bug-prone code completion tasks.To fill this gap, this paper presents the first empirical study evaluating the performance of LLMs in completing bug-prone code.Through extensive experiments on 7 LLMs and the Defects4J dataset, we analyze LLMs' accuracy, robustness, and limitations in this challenging context.Our experimental results show that completing bug-prone code is significantly more challenging for LLMs than completing normal code.Notably, in bug-prone tasks, the likelihood of LLMs generating correct code is nearly the same as generating buggy code, and it is substantially lower than in normal code completion tasks (e.g., 12.27% vs. 29.85%for GPT-4).To our surprise, 44.44% of the bugs LLMs make are completely identical to the pre-fix version, indicating that LLMs have been seriously biased by historical bugs when completing code.Additionally, we investigate the effectiveness of existing post-processing techniques and find that while they can improve consistency, they do not significantly reduce error rates in bugprone code scenarios.Our research highlights the limitations of current LLMs in handling bug-prone code and underscores the need for improved models and post-processing strategies to enhance code completion accuracy in real-world development environments.</p>
<p>token [1] or line by line [2].Code completion plays a crucial role in development, especially with increasingly complex systems and the growing demand for faster development cycles.By accelerating coding efficiency, code completion can help to save time and reduce errors, making it an important tool for developers [3].</p>
<p>At an early stage, code completion relied on static analysis and a deterministic set of rules to generate suggestions [4].With the advancement of deep learning, researchers introduce neural networks to code completion [2], [5]- [8].More recently, code completion has benefited from the advent of large language models (LLMs) [9]- [12], which becomes indispensable in modern software development.GitHub Copilot, Microsoft's AI-powered coding assistant based on GPT-4, has serviced one million paying users [13].</p>
<p>However, LLMs still face many challenges in code completion [14].In particular, the datasets used to train these models often contain a substantial number of buggy code snippets.Although many of these bugs are eventually fixed, and the corrected versions coexist with the buggy ones as part of the code's revision history, there is a risk that LLMs may learnor even memorize -the buggy versions rather than the correct ones, especially in bug-prone scenarios.This could adversely affect the accuracy and reliability of LLMs in code completion.</p>
<p>An example of bug-prone code completion is shown in Fig. 1, where OpenAI's GPT-3.5 attempts to complete the code in the Generator.javafile.In this example, the original code contains a bug where iterator2 is incorrectly assigned using p1.getPathIterator(null) instead of p2.getPathIterator(null), which should have been used to correctly access the iterator of the p2 object.This bug is later fixed by assigning iterator2 to p2.getPathIterator(null), as shown in the "fixed code" section.However, when tasked with completing this code, GPT-3.5 still generates the original buggy pattern, incorrectly using p1.getPathIterator(null).This example highlights a critical challenge regarding the reliability of LLM-based code completion tools, specifically their ability to complete code that has a history of bugs accurately.It raises important concerns about the extent to which these tools can be relied upon in professional software development environments, where accuracy and dependability are crucial.</p>
<p>In this study, we aim to investigate the capability of large language models (LLMs) in handling bug-prone code completion tasks.These tasks are constructed from code bases with a history of bugs.We also investigate the factors that influence LLMs' performance in completing bug-prone code, as well as the effectiveness of post-processing methods in improving such performance.</p>
<p>Specifically, we conduct experiments on 7 LLMs, including ChatGPT (3.5&amp;4.0&amp;4o)[15]- [19], CodeLlama [11], [20], [21], StarCoder [12], [22], CodeGEN [10], [23], and Gemma [24].To rigorously assess LLM performance in bug-prone code contexts, we construct bug-prone tasks from the Defects4J dataset [25], a widely-used benchmark for real-world Java code bugs.To further validate our findings, we also conduct additional experiments on the ConDefects dataset [26], where we observe consistent patterns, reinforcing the generalizability of our conclusions across different code contexts.</p>
<p>Our experimental results demonstrate that completing bugprone code poses significantly greater challenges for LLMs compared to standard code completion.Specifically, in bugprone tasks, LLMs exhibit nearly equal probabilities of generating correct and buggy code, with a substantially lower accuracy than in normal code completion scenarios (e.g., 12.27% vs. 29.85%for GPT-4).On average, each model generates approximately 151 correct completions and 149 buggy completions, highlighting the increased difficulty of handling bug-prone contexts.Moreover, to our surprise, on average, 44.44% of the bugs LLMs make are completely identical to the historical bugs.For GPT-4o, this number is as high as 82.61%.We also observe that the "if statement" is the most common construct in bug-prone code.However, completion accuracy is notably low for constructs such as method invocations, return statements, and variable declarations.Furthermore, we find that existing post-processing approaches do not fundamentally address the tendency of code completion models to generate incorrect outputs.While postprocessing techniques can improve consistency, they do not significantly reduce error rates in the context of bug-prone code.</p>
<p>In summary, our study makes the following contributions:</p>
<p>• We provide a systematic evaluation of the ability of seven state-of-the-art LLMs to handle bug-prone code.</p>
<p>To the best of our knowledge, this study is the first to comprehensively investigate the effectiveness of LLMs on bug-prone code completion.Our results show that, while LLMs are capable of generating correct code, they exhibit a high likelihood of generating buggy completions in bugprone code contexts, with a correct-to-buggy completion ratio close to 1:1.This finding highlights a significant limitation of current models in handling complex code dependencies.</p>
<p>• We analyze the mechanisms by which LLMs generate errors, with a specific focus on bug-prone code contexts.By examining common code constructs, such as conditionals, loops, and variable declarations, we identify specific weaknesses in the models' handling of certain coding patterns.For instance, we observe that method invocations and return statements are particularly prone to errors, suggesting areas where further training or model refinement may be beneficial.• We evaluate the effectiveness of existing post-processing approaches intended to improve the quality of code completions by LLMs.Our findings indicate that while post-processing can enhance output consistency, it does not significantly reduce the rate of incorrect completions in bug-prone code contexts.This underscores the need for more advanced approaches to address the inherent challenges of bug-prone code completion.</p>
<p>To support the open science community, the code and the data used in our study are available at https://github.com/glwhappen/llms-bug-replicators.</p>
<p>II. RELATED WORK</p>
<p>Code completion plays a crucial role in Integrated Development Environments (IDEs) [27].It enhances software development efficiency by predicting the remaining code snippets based on previously entered code [28]- [30].Code completion technology has evolved through several phases.</p>
<p>A. Early Code Completion Techniques</p>
<p>In the early stages of the research, code completion relied on heuristic rules and static type information [31], where tools would suggest completions based on the initial characters of variables and function names from a predefined list.This was followed by the adoption of templates and pattern matching to provide more complex code snippet completions.</p>
<p>A significant advancement occurred when researchers recognized the statistical properties of code.Hindle et al. [32] demonstrated that code has predictable statistical patterns, leading to the adoption of statistical language models for code modeling.N-gram models became particularly prevalent [33], marking a notable evolution in code completion methodologies.However, these methods were less effective for complex code structures and logic, often failing to complete correcct code or taking too much time to generate suggestions.</p>
<p>B. Neural Network-Based Code Completion</p>
<p>With advancements in neural network technologies, the field of code completion has experienced substantial growth.Modern neural network-based technologies can offer developers contextually relevant and accurate code suggestions.These suggestions consider not just the immediate context [34] and semantics but also the broader patterns and structures within the code.</p>
<p>The scope of code completion ranges from completing the next token or line [35] to filling in method and class names [36], and even extending to the completion of entire programs or projects [37].In development environments, code completion techniques are applied at various levels, with line completion and method completion being the most commonly utilized.</p>
<p>Here, we introduce some classic studies based on neural networks.Svyatkovskiy et al. [8] proposed an LSTM-based code completion system specifically designed to recommend Python method calls.This system is integrated into the Visual Studio Code IDE as part of the IntelliCode extension, enhancing the development environment with advanced method call recommendations.Liu et al. [6] developed a Transformerbased neural architecture for code understanding and code generation, utilizing multi-task learning in its pre-training language model.They pre-trained this model with a hybrid objective function that encompasses both code understanding and generation tasks and fine-tuned it to enhance performance in code completion tasks.Karampatsis et al. [7] introduced a large-scale open-vocabulary neural language model for source code, leveraging the Byte Pair Encoding (BPE) algorithm, beam search, and a caching mechanism.This innovative approach allows for maintaining a low vocabulary size while successfully predicting out-of-vocabulary (OOV) tokens, achieving state-of-the-art performance in token-level code completion.Guo et al. [5] presented UniXcoder, a unified cross-modal programming language pre-training model that enhances code representation through innovative mechanisms.Utilizing a masked attention matrix with prefix adapters, UniXcoder controls its behavior while leveraging cross-modal contents such as Abstract Syntax Trees (ASTs) and code comments.Additionally, it employs multimodal content and learns code snippet representations via contrastive learning, improving its code completion task capabilities.Liu et al. [2] developed SANAR, a non-autoregressive model with a syntaxaware strategy for line-level completion, which optimizes token pair selection to accelerate and improve completion quality.</p>
<p>C. Large Language Models in Code Completion</p>
<p>The recent advancements in Large Language Models (LLMs), such as ChatGPT [9], [15]- [19], CodeGEN [10], [23], CodeLlama [11], [20], [21], StarCoder [12], [22], Gemma [24], ChatGLM [38], [39] Claude [40], Grok [41] and DeepSeek-R1 [42], have shown great promise in code completion tasks.Trained on vast open-source code repositories [43], they offer developers accurate and intelligent suggestions.For example, after typing a function's initial characters, these models can predict its complete signature [44].</p>
<p>However, employing LLMs for code completion introduces also challenges [45].The performance of these models largely depends on the quality of their training data, which is primarily sourced from open-source platforms like GitHub [46].This data may include buggy code, affecting the models' reliability.Training on such data can lead models to replicate these faults [14].A robust model should guarantee high reliability and quality in results.This requires a sophisticated training strategy, including dataset filtering to remove poor coding practices and errors [10], and integrating mechanisms for models to identify and avoid such bugs [47].Considering these strategies, models can be refined to offer more reliable and best-practice-aligned suggestions [35].</p>
<p>D. Program Repair</p>
<p>Our research builds upon the same dataset used in the study of program repair [48].However, we take a distinctly different approach.Program repair involves the identification and fixing of existing code bugs, analyzing buggy code, and applying fixes based on the understanding of known errors and their specific contexts [49]- [53].In contrast, our study does not equip models with the context surrounding the original bugs.Instead, we only provide the bug-prone code snippets and hope that the models recognize these as incomplete code, attempting to complete them.Our primary focus is to assess the robustness of code completion models when dealing with bug-prone code, rather than their ability to repair known bugs.</p>
<p>The key findings of our research indicate that the likelihood of LLMs generating buggy code completions is as high as their probability of generating correct ones.</p>
<p>III. STUDY DESIGN</p>
<p>A. Overview</p>
<p>This paper aims to answer the following three research questions:</p>
<p>• RQ1: How do LLMs perform on completing bugprone code?RQ1 is designed to assess the ability of LLMs to handle bug-prone code completion tasks.We compare the likelihood of LLMs in generating correct code and buggy code.We also compare the performance of LLMs in completing bug-prone code and normal code.• RQ2: What are the characteristics of the tasks where LLMs produce bugs when completing bugprone code?RQ2 investigates the role of various code constructs (such as conditionals, loops, and variable declarations) in shaping completion accuracy.The purpose is to identify areas where LLMs may require further training or refined algorithms to improve their handling of diverse coding patterns.• RQ3: Can post-processing methods mitigate the negative effects of bug-prone code on LLM-based code completion?RQ3 evaluates the impact of postprocessing approaches on the quality of code completions provided by LLMs, specifically in the context of bugprone code.Post-processing is crucial for refining raw model outputs.This RQ aims to assess various strategies for post-processing to determine their effectiveness in reducing faults and mitigating the negative effects of bugprone code in LLM-based code completion.</p>
<p>To answer these research questions, we design our methodology, which is shown in Fig. 2. Specifically, our research methodology consists of four steps.(1) Dataset Processing: this step processes the data to facilitate subsequent code completion and evaluation (in Sec.III-B).(2) LLMs Selection: this step uses various LLMs to complete the processed input data, generating multiple completion results (in Sec.III-C).</p>
<p>(3) Evaluation: this step is performed by evaluating completions of LLMs using our adopted metric (in Sec.III-D).(4) Post-processing: this step employs three post-processing approaches to refine completion results (in Sec.III-E).</p>
<p>B. Dataset Processing and Task Construction</p>
<p>Identifying and collecting data for code that is inherently prone to bugs is challenging, as it requires a systematic way to determine and validate its bug-proneness across diverse scenarios.Instead, we leverage existing datasets of real-world software defects, where the bugs are already identified and fixed.This approach is justified because historically buggy code provides strong empirical evidence of patterns and structures that are more susceptible to defects.By analyzing the pre-fix versions of such code, we can approximate the characteristics of bug-prone code, ensuring that our study is grounded in realistic and well-documented software failures.Moreover, using established bug datasets enhances reproducibility and comparability with prior research, allowing for more robust evaluations of how code completion models handle bug-prone scenarios.</p>
<p>We use the Defects4J 2.0 dataset [25] to construct the main bug-prone code completion tasks, which comprises 697 defects from 16 real-world open-source Java projects on GitHub.This dataset is extensively used for evaluating fault localization [54]- [59] and program repair [60]- [65], underlining its reliability and relevance for bug-related research.</p>
<p>With this dataset, we systematically reconstruct code scenarios with known historical bugs.We exclude cases where fixes involve adding or deleting code and concentrate on defects that involve modifications to existing lines of code, as this allows us to accurately compare between the completed code and historically buggy code.Following this criterion, 546 defects are kept for our analysis.</p>
<p>For each bug, we retain the code preceding the buggy line as the context for LLMs to perform code completion.For example, if a defect occurs at line 50, the first 49 lines of code are provided to the LLM, allowing it to generate the remaining code while assessing its susceptibility to historical bugs.</p>
<p>We take the first line LLMs complete for analysis, because in Defects4J, 76.76% of patches are single-line [25].Moreover, single-line code completion is the most common in practical coding environments [66].The completed line is then compared with the corresponding line in both the pre-fix and fixed versions in Defects4J.In Section V, we investigate whether analyzing just one-line of completed code is a threat to our conclusions.</p>
<p>C. LLMs</p>
<p>Our next step is to select LLMs for evaluation.We consider both proprietary and open-source LLMs to guarantee the generalization of our empirical findings.As a result, we select seven state-of-the-art LLMs.For proprietary LLMs, we select OpenAI-GPT4o [9], OpenAI-GPT4 [9], OpenAI-GPT3.5 [17].For open-source LLMs, we select CodeLlama-7B-hf [11], Gemma-7B [24] and StarCoder2-7B [12].We also notice that reasoning models (e.g., DeepSeek R1 [42]) perform well but require extended reasoning time, making them less suitable for prompt code completion.For these models, the analyses are in Sec.V-D.</p>
<p>In our study, for ChatGPT, we use the official API provided by OpenAI.As for the Gemma, CodeLlama, StarCoder, and CodeGEN models, we use the API provided by Hugging-Face [67].To guide the interactive dialogue model in generating relevant responses, we use the following prompt: "I need to complete the following code snippet.Please provide only the continuation of the code, without any additional comments or repeating the original code.The code is as follows:{code}".</p>
<p>D. Evaluation</p>
<p>After data processing and model selection, we feed code snippets into LLMs for code completion.Since we are only completing code lines, the resulting code is not executable as a whole.Consequently, execution-based evaluation methods are not applicable here.Instead, we aim to evaluate the generated code against the buggy code and the fixed code.</p>
<p>To ensure a reasonable assessment of code completions, we use an evaluation metric that combines the Longest Common Subsequence (LCS) and Levenshtein Edit Distance (LED).This metric involves a comparative analysis of the completed code against both the correct and buggy code present in the Defects4J dataset [2].We do not use existing metrics like CodeBLEU [68] and CodeBERT scores [69] because they are designed to measure the similarity of multi-line code.In our study, we focus primarily on single-line code, which is often very short.Therefore, these metrics are not suitable for our study (the experimental analysis is in Section IV-B).</p>
<p>This evaluation metric assesses two main scores: the "bug proximity score" and the "fix proximity score."The "bug proximity score" measures the similarity between the completed code and the original buggy code, while the "fix proximity score" evaluates the similarity between the completed code and the fixed version.To compare, each score assesses the similarity between a single line of code s and samples of fixed or buggy code t.We combine the widely used LCS and LED.LCS computes the longest subsequence common to both sequences under comparison.We normalize the LCS by dividing its value by the length of the longer string between string s and string t.LED computes the minimum number of edits (such as insertions, deletions, and substitutions) needed to transform one string into another.We normalize the LED by calculating the inverse proportion of the LED value to the length of the longer string, as a lower edit distance indicates higher similarity.</p>
<p>Since these two metrics are equally important, we assign the same weight for both.Given a line of completed code s, fixed code t ′ , and buggy code t, the formulas for calculating the similarity of the code completion results are shown as follows: bug proximity score(s, t) = LCS(s,t)+LED(s,t)
2 and fix proximity score(s, t ′ ) = LCS(s,t ′ )+LED(s,t ′ ) 2 .
We find that when the score is below threshold, the difference between the two texts becomes very significant.Therefore, we set the threshold at 0.4 to distinguish whether the two texts are completely dissimilar in both semantics and tokens based on the experimental preliminary analysis.The evaluation metric classifies the results into the following five categories:</p>
<p>• Bug-identical: If bug proximity score(s, t) = 1, indicating that the generated code s is identical to the buggy code t.This implies that the model replicates the historical bug without any correction.• Correct-identical: If fix proximity score(s, t ′ ) = 1, indicating that the generated code s exactly matches the fixed code t ′ .This demonstrates that the model produces an error-free version that perfectly corresponds with the historical correct fix.• Non-compliant: If both bug proximity score(s, t) &lt; threshold and fix proximity score(s, t ′ ) &lt; threshold.This indicates that the generated code s does not sufficiently align with either the historical bug or the historical correct fix, potentially resulting in irrelevant or nonsensical code snippets.</p>
<p>• Bug-close:</p>
<p>If bug proximity score(s, t) &gt; fix proximity score(s, t ′ ), fix proximity score(s, t ′ ) ≥ threshold, and bug proximity score(s, t) ̸ = 1.This indicates that the generated code s is closer to the historical bug t, indicating that it resembles the historical buggy code with modifications and suggesting that the LLM may tend to replicate historical bugs.</p>
<p>• Correct-close:</p>
<p>If fix proximity score(s, t ′ ) is greater than bug proximity score(s, t), bug proximity score(s, t) meets or exceeds threshold, and fix proximity score(s, t ′ ) ̸ = 1, then the generated code s is deemed Correct-close, meaning it closely aligns with the historical correct fix despite minor discrepancies.</p>
<p>In particular, our evaluation metric is designed not only to compare the generated code with both the buggy and fixed versions but also to detect if LLMs tend to replicate historical bugs.The bugs in our dataset, appearing in official open-source releases, are often subtle and challenging for developers to identify.In contrast, more obvious bugs may not offer the same insight into the models' nuanced behavior.</p>
<p>E. Post-processing Approaches</p>
<p>Post-processing methods have been widely adopted to enhance the performance of existing models [70]- [74].It uses additional techniques after the initial code generation to refine the result and enhance its accuracy.Therefore, in this study, we also explore three widely used post-processing approaches.</p>
<p>In particular, we let LLMs generate multiple completed code snippets as candidates.The focus of post-processing is to identify and select the best result among all these candidates.</p>
<p>The post-processing approaches we have explored are: 1) Majority Voting: This approach involves generating multiple responses from the LLM and uses a voting mechanism to select the result that most closely aligns with the majority.</p>
<p>2) Prompt Engineering: This approach involves generating multiple responses from the LLM and allowing the LLM to select the best result from these generated outputs.</p>
<p>3) Candidate Selection Model: This approach involves generating multiple responses from the LLM and training a custom model to select the best result based on predefined criteria.</p>
<p>These approaches are adjusted and implemented to enhance the effectiveness of the selection process, ensuring that the most accurate and useful code snippet is identified for use.In the rest of this subsection, we introduce these post-processing approaches in more details.</p>
<p>1) Majority Voting: Majority voting employs a voting rule to select the code completion that is most similar to the others from multiple results [71], [75]- [77].The purpose of this approach is to exclude outliers and select the most representative result.For our investigated bug-prone code completion, we generated several code completion results using a LLM, and then calculated the sum of similarities between each code completion and the rest to assess the average similarity of each result.Ultimately, the code completion with the highest average similarity was selected.Below is a detailed explanation of this process:</p>
<p>Suppose we have N code completion results, each denoted as C i , where i = 1, 2, . . ., N .These results are generated by the large model, and we aim to calculate the sum of similarities between each result and all other results.The similarity function can be represented as sim(C i , C j ), quantifying the similarity between two code completions C i and C j .The calculation method for sim is the same as that used for bug proximity score and f ix proximity score in 'Evaluation Metrics,' except that it compares the similarity between two code completions.</p>
<p>Formally, for each code completion C i , its average similarity S i with all other completions is calculated via
S i = 1 N −1 N j=1 j̸ =i sim(C i , C j ).
Here, S i is the average similarity of C i with all other results.Finally, we select the code completion C i with the highest S i as the best code completion result.</p>
<p>2) Prompt Engineering: Prompt Engineering (PE) has recently gained popularity as an approach to enhancing the abilities of LLMs without altering their model weights, thereby improving their accuracy on specific tasks [78], [79].In our approach, we use LLMs to generate multiple completions and then select the best result from these outputs.Specifically, we employ a particular prompt for selection, which is designed based on the work of Guo et al. [80], who found that placing instructions at the beginning of the prompt yields the best task performance.Additionally, specifying the output format as JSON can enhance the stability of the result format, likely due to its structured nature.With this prompt, we input code snippets and their multiple generated code snippets, letting the LLM choose the optimal one.We provide an example of our used prompt as follows.A prompt example is shown in Fig. 3. Therefore, for each code completion  , its average similarity  with all other completions is calculated as follows:
𝑆𝑖 = 1 𝑁 − 1 𝑁 ∑︁ 𝑗=1 𝑗≠𝑖 𝑠𝑖𝑚(𝐶𝑖, 𝐶𝑗 )
Here,  is the average similarity of  with all other results.Finally, we select the code completion  with the highest  as the best code completion result.</p>
<p>3.6.2Prompt Engineering.Prompt Engineering (PE) has recently become a popular method to boost the reasoning abilities [49] of large language models without altering their model weights, thus enhancing their accuracy on specific tasks [48].In this paper, we utilize large language models to select results from the same temperature settings across multiple models and from different temperature settings within the same model.Specifically, we used the OpenAI-GPT3.5 with a particular prompt for selection.This prompt's design is based on Guo et al. [13], indicating that placing instructions at the beginning yields the best task performance.Additionally, specifying the output format as JSON can enhance the stability of the result format, likely due to its structured nature.With this prompt, we input code snippets and their multiple generated outcomes, letting the large language model choose the optimal one.We provide an example of prompt used in this paper as follows.</p>
<p>Example of Prompt:</p>
<p>You are tasked with assessing the quality of various code completion results provided by different models to determine the best one.Each result is intended to complete a specific code snippet.You will receive both the code snippet and the completion results generated by the models.The code snippet is : /** Computes the tangent of this complex number using the . . .public Complex tan() { and the code results are
A: if (isNaN) { . . . E: if (isNaN || Double.isInfinite(real)) {
Your evaluation should focus on the quality of these completions.After reviewing them, respond with a single character in json corresponding to the result that exhibits the highest quality.For instance, if the best completion is from model A, you should respond with {"answer":"A"}.</p>
<p>Answer to Prompt: {"answer":"B"} 3.6.3Selection Model.In this section, we train a selection model and use it to select the best result.We introduce the method of scoring the results using the CodeT5+ model.The training set we select a bug-fix dataset in Recoder method [61].This training set comprises multiple bug-fix data, each of which represents a bug-fix instance.We extract the first modified line of code from each instance of the dataset, or to extract both the pre-modified and post-modified lines.3) Candidate Selection Model: This approach involves generating multiple responses from the LLM and using a customtrained selection model to choose the best result.Specifically, we develop the selection model through additional training of the CodeT5+ model [81].The training used a bug-fix dataset from Java projects [82].The dataset contains 103,585 patches, focusing only on patches that modified or introduced a single statement.</p>
<p>These patches are processed to extract pairs of data representing pre and post modifications, consistent with the segmentation method described in Sec.III-B, regarded as a bug and its corresponding fix.The CodeT5+ model is employed for training to predict whether a line of code is defective.The trained model is subsequently used to predict the likelihood of defect-free code from completed code in large language models, assigning probability scores to evaluate and select the most promising result based on the highest score.</p>
<p>F. Running Platform</p>
<p>We run the experiments on a server with four NVIDIA A100 Tensor Core GPUs, each with 80 GB of memory.</p>
<p>IV. RESULTS AND ANALYSIS</p>
<p>A. RQ1: How do LLMs perform on completing bug-prone code?</p>
<p>To answer RQ1, we assess the quality of code completion results from the selected LLMs, focusing on single-line code completions and their evaluation compared to correct or buggy code.Each model completes 546 code snippets, which are then categorized based on our evaluation metric.For all experiments, we repeat five times for each model.</p>
<p>The results are presented in Table I, where each row corresponds to an LLM, and each column displays the results for the corresponding categories, as assessed by our evaluation metric.Each cell in the table shows both the number and the corresponding percentage for each category.Specifically, this table also includes the results for the 'normal' code (in Type), which consists of 546 code snippets selected from the correct code snippets in the same code file as the corresponding bug-prone code snippets.To ensure a fair comparison, the code structures (e.g., lines in if statements or while loops) of each selected snippet match those of the original 546 code snippets 1 .For these snippets, only the correct version of the completion exists, and we measure the similarity score of each code completion against the correct implementation (referred to as fix proximity score).</p>
<p>We analyze the results from three perspectives: 1) the quality of completions, which evaluates the performance of models in completing; 2) memorization, where models completely remember incorrect answers and repeat them; and 3) the comparison between bug-prone and normal code completion tasks, highlighting how models perform on error-prone versus correct code.</p>
<p>a) The quality of completions: To evaluate the quality of completions, from this table, we can observe that the proportion of correct identical completions is low across all models for bug-prone data.The highest is 27.66% for OpenAI-GPT4o, while the lowest is 5.49% for CodeGEN-350M.This indicates a challenge in generating perfect code snippets.However, when combined with correct close completions, the models seem to perform better, though variability is evident.For instance, OpenAI-GPT3.5 achieves a total correct rate (Correct-close + Correct-identical) of 41.39%, whereas StarCoder2-15B only reaches 17.4%.</p>
<p>Such completions show that OpenAI-GPT4o and OpenAI-GPT3.5 perform better compared to other models.However, when we consider the bug completions, a deeper analysis is required.For instance, while OpenAI-GPT4o achieves a relatively high correct completion rate of 32.42% (combining Correct-close and Correct-identical) ,it also has a significant proportion of buggy completions at 5.86% for Bug-close and 27.84% for Bug-identical.Similarly, OpenAI-GPT3.5, despite having the highest correct completions, also has a significant rate of buggy completions at 15.93% for Bug-close and 16.67% for Bug-identical.</p>
<p>These results show an issue: even models that perform well in generating correct code are also prone to generating substantially incorrect outputs.This may be due to the models' sensitivity to the noise in the training data, which can include both correct and incorrect coding patterns.</p>
<p>We also find that the ratio of correct completions to buggy completions is close to 1:1 for several models in the bugprone type (the average ratio is 1.01:1.00).This proximity in ratios indicates that while LLMs have significant potential in generating correct code in the bug-prone context, they are equally prone to generating buggy outputs, reflecting inherent challenges in model training and application.In particular, OpenAI-GPT3.5 stands out with the highest rate of correct completions at 45.42% in the normal context and 22.89% in the bug-prone context, but it is closely followed by a significant 32.6% buggy completions in the bug-prone context.</p>
<p>This indicates a critical need for improved training methods, better quality assurance in training data, and enhanced postprocessing approaches to ensure that the benefits of using LLMs for code completion outweigh the risks associated with their current limitations.Addressing these challenges is essential for advancing the reliability and efficacy of LLMs in code completion tasks.</p>
<p>b) Memorization: Further, we change the aspect to the memory of these LLMs.Within our evaluation, completions that align perfectly with either the correct or buggy code snippets provided suggest that a model may have memorized these specific sequences, hinting that its training data likely encompasses projects similar to those in the Defects4J dataset or analogous compilations.This observation implies that when LLMs generate identical results, they are likely regenerating learned patterns from their training datasets rather than applying learned coding principles to new code completion scenarios.To quantify this phenomenon, we analyze the extent to which various models memorize code from the training dataset, with our analyzed results illustrated in the subsequent Fig. 4.</p>
<p>In this figure, we observe that the memorization rates for the models range between 10% and 55%.OpenAI's GPT-4o and GPT3.5 stand out for their high levels of code memorization, with proportions reaching 55.5% and 39.56%, respectively.This suggests that the training datasets for both models likely contain code that closely resembles our experimental subjects.In contrast, GPT-4 demonstrates a lower tendency toward code memorization, likely due to its training on a more diverse collection of code.This diversity may allow GPT-4 to approach code completion tasks with a broader set of strategies rather than simply replicating specific results from the training data.The memorization rates for other models fall between 10% and 20%, highlighting significant variation in how different models handle code replication.When we specifically focus on the memorization of buggy code, we find that a significant portion of the bugs LLMs generate are directly copied from their training data.For instance, OpenAI's GPT-4o has a memorization rate of 27.84%, meaning that nearly 28% of the bugs it produces are identical to the buggy code in its training set.GPT-3.5 follows with 16.67%, while Gemma-7B has the lowest memorization rate at 3.30%.</p>
<p>Building on the findings, we further investigate how generated bug completions reflect historical buggy patterns.Among all bug completions (Bug-identical and Bug-close), the Bugidentical Ratio quantifies the proportion of generated bug completions that are exactly identical to historical buggy code.The results are listed in Table I.We find that the Bugidentical Ratio varies markedly across models, ranging from 15% to 83%.For example, OpenAI's GPT-4o exhibits a ratio of 82.61%, and GPT-3.5 follows with 51.12%, implying that a significant portion of their buggy outputs are direct copies of known errors from the training data.In contrast, Gemma-7b's notably low ratio of 15.00% suggests that its buggy completions are more often merely token-wise similar to historical bugs rather than exact reproductions.This indicates that models with higher Bug-identical Ratios are more reliant on memorizing and reproducing buggy patterns from their training data, which may hinder their ability to innovate and generate error-free code.</p>
<p>These findings further support our hypothesis that the presence of buggy codes in the training dataset can profoundly affect the quality of code completions generated by LLMs, indicating an urgency to identify and remove noises in the training datasets, which can help to improve code completion quality.</p>
<p>c) Comparison between bug-prone and normal code completion tasks.:After examining the challenges of bugprone code completion, we now focus on comparing bug-prone and normal code completion tasks.</p>
<p>As shown in Table I, the accuracy of generating correct completions is significantly higher for normal code compared to bug-prone code.For instance, OpenAI-GPT3.5 achieves a high rate of 45.42% Correct-identical completions for normal code, while OpenAI-GPT4 achieves a combined Correct-close and Correct-identical rate of 60.99%.This highlights the increased difficulty in completing bug-prone code.</p>
<p>To further explore this difference in performance, we analyze the distribution of similarity scores, as shown in Fig. 5.The violin plots illustrate the difference in performance across various models on bug-prone and normal code tasks.For all evaluated models, including OpenAI-GPT4o, OpenAI-GPT3.5, and CodeLlama-13B-hf, normal code completions consistently yield higher similarity scores (ranging from 0.8 to 1.0) compared to bug-prone code, which typically falls between 0.2 and 0.4.This pattern is particularly evident in OpenAI-GPT4o, where normal code completions exhibit higher and more concentrated similarity scores, indicating greater reliability and accuracy in error-free contexts.</p>
<p>These results underscore a key limitation of current LLMs: the presence of bugs or structural complexities significantly reduces their performance on bug-prone code.This is reflected in both lower similarity scores and wider score distributions, which highlight the challenge of generating correct code in such error-prone environments.</p>
<p>Finding 1: LLMs exhibit nearly equal probabilities of generating correct and buggy code in bug-prone tasks, with a ratio of 1.01:1.00.On average, each model produces 151 correct completions and 149 buggy completions.Among all the bugs, the memorization rates for buggy code, where models reuse errors from their training datasets, can be as high as 82.61%.LLMs also show significantly lower accuracy in bug-prone tasks compared to normal code (e.g., 12.27% vs. 29.85%for GPT-4), highlighting the increased difficulty of handling bug-prone contexts.</p>
<p>Human Evaluation.To further show the effectiveness of our evaluation metric, we conduct a human study to assess whether the evaluation of experienced software developers about the category (i.e., Correct-close) of single lines of code is consistent with our metric.In particular, as introduced in Sec.III-D.We do not use existing metrics like CodeBLEU because they are designed for multi-line code.In this experiment, we take CodeBERT score and CodeBLEU as baselines.We use their respective thresholds of 0.75 and 0.4, which are determined through experimental analysis.</p>
<p>For the evaluation, we select buggy and fixed lines from the 546 code samples in the experiment, with each sample containing two lines (one buggy and one fixed), resulting in 1,092 lines.We exclude instances where the completion results matched the selected ones, resulting in 216 distinct code samples for evaluation, comprising a total of 432 lines.</p>
<p>Participants in this experiment include graduate students majoring in computer science, information technology, and software engineering, as well as professional developers with more than three years of experience in software development, totaling 10 volunteers.Each participant in the experiment receives a random set of 30 different code samples.Each sample includes the correct completion, the buggy one, a completion generated by an LLM, and the best completion selected after post-processing.Participants need to identify which category the completed code belongs to.We then compare their choices to the results obtained from our evaluation metrics.</p>
<p>The results are shown in Table II.In this table, we find there is substantial agreement between our evaluation metrics and human judgment, confirming the reliability of our metric.The accuracy rate for Correct-close and Bug-close completions stands at 73.8%/92.7%(76/38 out of 103/41 samples).The overall accuracy rate is 83.7% (251 out of 300 samples) across all categories.When compared with baselines, we find that our metric performs better than the CodeBERT score and CodeBLEU with the highest total accuracy (83.7%).This shows the effectiveness of our metric.</p>
<p>Finding 2: The human evaluation shows that our chosen metrics align well with human evaluation across a broad spectrum of code completions.The overall accuracy rate is 83.7% across all categories.Compared with CodeBERT score and CodeBLEU, our metric still performs the best.</p>
<p>B. RQ2: What factors contribute to errors in bug-prone code completion by LLMs?</p>
<p>To answer RQ2, we conduct an analysis focusing on the statement types of the completions.Our goal is to determine whether statement types influence the vulnerability of code and to pinpoint the code statements where LLMs tend to falter.Understanding the relationship between statement types and bug generation is vital.It may lead to more effective bug detection and correction strategies in automated code completion approaches, which can improve their reliability.</p>
<p>To achieve this goal, we use the parser generator tool treesitter [83] to explore the types of these statements.We then categorized the first statement in the LLMs' completion results and their corresponding fixed code.The classification criteria rely on the root node type provided by the tree-sitter.</p>
<p>Table III shows the results of the completion of the code types for the line completed by LLMs.In this table, each line represents a code type and each column represents a LLM.The three numbers within the table represent Correct Completions (Correct-close + Correct-identical), buggy completions (Bugclose + Bug-identical), and Non-Compliant completions for each model.In particular, the "other" type includes "Bi-nary Expression," "Boolean Type," "Break Statement," "Class Body," and other types that have fewer than five instances in each model.</p>
<p>From this table, we observe that the "if statement" and "expression statement" are the most common statement types.The "if statements", appear frequently across all models, with a relatively high number of buggy completions, indicating a common area of difficulty.For example, OpenAI-GPT3.5 has a high number of correct (33), but also buggy (23) completions, suggesting challenges in accurately predicting conditional logic.The expression statements show varied performance across models.Gemma-7b tends to have a higher rate of non-compliant results (64) compared to correct (16) and buggy (15), which may indicate issues with understanding or  We also observe that method invocation and return statements show significant buggy and non-compliant completions across models.For "method invocations" and return statements, the average correct completion rates are 11.30% and 17.77%, illustrating the complexity and error-prone nature of "method invocations" and return statements in completion.</p>
<p>For the proportions of correct and buggy completions, we find that in most of the types, the performance is close to their ability to generate buggy outputs, except for "variable declarations" and "method invocations".For "variable declarations", we observe that the models generally generate a higher proportion of correct completions compared to buggy ones (average 2.62:1.00).This suggests that models may have a better grasp of the syntactic and contextual requirements needed for accurately completing "variable declarations".This can be due to the relatively straightforward nature of "variable declarations", which often involve less complex logic than other statement types.Conversely, for "method invocations", the correct to buggy completion ratio is less favorable (average 1.00:2.14),indicating a challenge for the models."method invocations" often involve complex dependencies, such as the correct identification of method names, understanding of the expected parameters, and the context in which the method is used.These complexities make "method invocations" more prone to the historical bugs, reflecting a significant challenge in the model's ability to handle intricate interactions within the code.</p>
<p>Finding 3: For the completed code by LLMs, "if statement" and "expression statement" are the most common statement types."Method invocation" and the "return statement" show significant buggy and non-compliant completions across models.In most of the types, the performance is close to their ability to generate buggy outputs, except for "variable declarations" (the proportion of correct and buggy completions is 2.62:1.00)and "method invocations" (1.00:2.14).</p>
<p>In Table IV, the three numbers represent the code type for the first line of Correct Completions, buggy completions, and Non-Compliant Completions corresponding to the fixed code in the dataset.From the table, we find that in the fixed code, the "if statement" remains the most frequent type of code, maintaining a significant presence across all models and accounting for 31.2% of the completions.However, there is a consistently high number of buggy and non-compliant completions for "if statements", which underscores a common difficulty in accurately completing conditional logic, just as in our analysis in Table III.</p>
<p>Compared to the results in Table 4, we observe a significant difference in "variable declarations".In the code completion results from LLMs, these declarations occur at an average frequency of 12.6%, while in the accurate code samples from the dataset, they make up 20.5% of statements, ranking second only to "if statements".This discrepancy indicates that accurately completing "variable declarations" poses a challenge for LLMs, likely due to their insufficient understanding of specific programming contexts and type inference capabilities.</p>
<p>For other findings, we find that the results are similar to the former ones.The performance is close to their ability to generate buggy outputs.For "variable declarations", the models generate a higher proportion of correct completions compared to buggy ones.For "method invocations", the correct-to-buggy completion ratio is still less favorable.</p>
<p>Finding 4: "if statements" remain the most common types.The "variable declarations" show significant buggy and non-compliant completions across models.For the performance and ability to generate buggy outputs, the results are similar to the former.</p>
<p>C. RQ3: How can post-processing methods mitigate the negative effects of bug-prone code on LLM-based code completion?</p>
<p>To answer RQ3, we review and analyze the methods we employ.Our objective is to select the optimal completion from multiple code completion results through post-processing approaches.To this end, We conduct a set of experiments, which involved selecting results from different parameters under the same model and from the same parameters across different models.As introduced in Sec.III-E, we consider three different post-processing approaches, namely the voting mechanism (Voting), the prompt engineering (PE), and the selection model (Sel.).For each technique, we focus on two aspects: (1) selecting the best completion from multiple code completion results generated by different models, and (2) selecting the best completion from multiple code completion results generated at different temperatures.</p>
<p>a) Models: We first conduct experiments on voting among different models, where we aim to select the most similar results from different models with the same temperature setting across the previous seven models.</p>
<p>Table V provides detailed results.In this table, each line represents a temperature and each column represents a LLM.</p>
<p>The three numbers within the table represent Correct Completions (Correct-close + Correct-identical), Correct and buggy completions (Correct-close + Correct-identical + Bug-close + Bug-identical), and their proportions.The last three columns represent the performance of three post-processing approaches on all code completion results generated by different models in a specific temperature.</p>
<p>Based on the results in Table V, we find that the Voting strategy generally achieves the best results among the three post-processing approaches.It consistently secures the highest average number of correct completions (166, compared to 149 for PE and 114 for the Sel.) and the highest average rate of correct completions (51.0%, compared to 48.9% for PE and 47.5% for the Sel.).This indicates that Voting is particularly effective in pooling insights from various models to enhance the overall accuracy of code completions.</p>
<p>When examining peak performance, the Sel.stands out by achieving the highest rate of correct completions at 58.0% at a temperature setting of 1.5.When optimally tuned to specific conditions, the Sel.can outperform other strategies in maximizing completion accuracy.</p>
<p>However, when considering the original models, except the Sel. on temperature 1.5, all other post-processing approaches fail to outperform the best-performing model in either the number of correctness or the proportions.This observation highlights a limitation of the post-processing approaches: while it does reduce the risk of extremely poor outputs and ensures a more consistent baseline performance, it may not attain the maximum potential accuracy that a single, optimally functioning model could achieve under specific conditions.We also find that the performance of post-processing approaches is still close to their ability to generate buggy outputs.This means that it still has half of the completions on the bugprone code, which may contain faults.Therefore, despite the implementation of post-processing approaches like Voting, PE, and Sel., there remains a significant challenge in fully mitigating the tendency of LLMs to generate incorrect outputs.This persistence of error generation, particularly in bug-prone code scenarios, can be problematic, since even a single mistake, when propagated through an automated system, can result in substantial bugs in the final software product.</p>
<p>b) Different Temperatures: We then focus on selecting the best completion from multiple code completion results generated by each model in different temperatures.</p>
<p>The results are also in Table V, where the last three rows represent the performance of the models at different temperatures (0.1, 0.4, 0.7, 1.0, 1.3, 1.6).We chose these 7 temperatures to keep the selection manageable and avoid overwhelming complexity.</p>
<p>From the results, we find that the Voting strategy typically yields the best overall results among the three techniques, with the highest average number of correct completions (173) and the highest average correct rate (51%).For optimal performance, PE is prominent, particularly when applied exclusively with OpenAI-GPT3.5, achieving the best correct rate of 57.5%.This high performance is likely due to the model's ability to synergize well with tailored prompts that enhance its inherent strengths, thus optimizing the quality of code completions.</p>
<p>Although the post-processing techniques reach a peak effectiveness of 57.5% in generating correct completions, their overall utility remains constrained by a strong correlation with the production of incorrect outputs.This persistent issue underscores the inherent challenges in mitigating errors through post-processing alone.</p>
<p>Finding 5: Although Majority Voting achieves the best overall results among the three post-processing approaches, the implementation of these existing techniques does not fundamentally alter the fact that the performance of code completion strategies is still close to their propensity to generate incorrect outputs.</p>
<p>V. EXTENDED ANALYSIS AND DISCUSSION</p>
<p>In this section, we extend our analysis and present a discussion of our findings through several key aspects.First, we explore the impact of hyper-parameter tuning on model performance, particularly focusing on temperature and token length settings.Second, we extend our analysis to recent code bugs using the ConDefects dataset to validate our findings' generalizability.Third, we investigate the performance of LLMs on multi-line code completion tasks to examine whether our findings hold beyond single-line completions.Additionally, we analyze emerging reasoning LLMs to assess their behavior in bug-prone contexts.Through these discussions, we aim to provide deeper insights into the challenges and opportunities in bug-prone code completion using LLMs.</p>
<p>A. Hyper-Parameter Tuning</p>
<p>We notice that the hyper-parameters in the LLMs are also important and may influence the performance of them.In this section, we explore the influence of the models with different hyper-parameters.</p>
<p>a) Temperature: For the temperature, we conduct additional experiments on all seven previously mentioned models.By altering the temperature parameter, we observe each model's code completion performance at various temperature settings.We show detailed results on OpenAI-GPT3.5 in Fig. 6.We chose OpenAI-GPT3.5 for our preliminary experiments because of its popularity and proven effectiveness in code completion tasks.The results are shown in Fig. 7.</p>
<p>From Fig. 6, we find that as the temperature increases, the overall number of Non-compliant instances rises, whereas correct and buggy completions decline.This indicates that increasing temperatures induce a greater degree of variability and a reduction in precision within code completions, alongside a diminished tendency to replicate learned patterns from the training data.The optimal code completion quality is achieved with the temperature setting at approximately 0.7, 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1. 4  This may be attributed to the model's enhanced ability to balance accuracy and creativity at this temperature, thereby achieving better code completion results.The quality of code completions began to decline when the temperature exceeded 0.7.This downward trend is caused by greater divergence and randomness in outputs as temperature increases, leading to more completions that were neither accurately correct nor buggy.The trends observed in Fig. 7 persist across the models, although the specific temperature at which peak performance is achieved varies.In Fig. 6 and Fig. 7, we also find that despite the temperature changes, the previous findings remain consistent: 1) the wellperforming models in generating correct code are also prone to generating substantial incorrect outputs; 2) their performance is closely mirrored by their ability to generate buggy outputs; and 3) existing LLMs tend to generate a large amount of offtarget, irrelevant, or completely nonsensical completions.For the finding of code memorization, we get a new observation that as the temperature increases, existing models display a decreased propensity to regenerate learned patterns from their training datasets.</p>
<p>b) Token Length: For the token length, we conduct additional experiments on OpenAI-GPT3.5,varying parameters including temperature and token length.Token length denotes the context's length given to the model, with a longer length implying more information in the provided request.The experimental results are shown in Fig. 8.</p>
<p>The results show that irrespective of token length, there is a consistent pattern of model results aligning with buggy codes, and this misalignment becomes more obvious as the token length increases.Specifically, at a token length of 600, the model finds a sweet spot, achieving an optimal balance between code completion quality and economic efficiency, with the best results achieved using fewer tokens.For the previous findings (Finding 1 to Finding 4), the results are still consistent.</p>
<p>B. Extension to Recent Code Bugs</p>
<p>To further validate our findings and explore their generalizability to more recent code issues, we conduct additional experiments using the ConDefects dataset [26], which contains coding faults from October 2021 to September 2023.This dataset is particularly interesting as it helps mitigate potential data leakage risks by including only recent faults that are less likely to appear in LLMs' training data.</p>
<p>Our results on ConDefects, as shown in Table VII, demonstrate similar patterns to those observed with Defects4J, reinforcing our earlier findings:</p>
<p>• The correct-to-incorrect completion ratio remains problematic.For example, OpenAI-GPT4o shows 28.54% correct completions (11.16%Correct-identical + 17.38% Correct-close) versus 29.86% buggy completions (6.16% Bug-identical + 23.70% Bug-close), maintaining a nearly 1:1 ratio.• Models continue to struggle with complex code structures, as evidenced by the high proportion of noncompliant outputs across all models (ranging from 24.54% to 61.00%).• Even the best-performing models show significant error rates.For instance, Gemma-7B, despite having a relatively high number of Correct-identical completions (145), still produces 577 buggy or non-compliant outputs.These consistent findings across both Defects4J and Con-Defects datasets suggest that the challenges we identified in bug code completion are not specific to older code patterns but represent fundamental limitations in current LLM approaches.The persistence of these patterns, even with more recent code examples, reinforces the need for improved methods specifically designed to handle bug-prone code contexts, regardless of the code's age or origin.</p>
<p>C. Multi-lines Completion</p>
<p>In the experiments, we focus on completing the immediate next line on the bug-prone code.In practical applications of code completion, especially in the context of integrated development environments or other software development tools, developers often focus primarily on the immediate next line suggested by the code completion feature.In this section, we extend the immediate next-line completions to two-line code completions.We conduct the same experiments as in Section IV.</p>
<p>The experimental results are shown in Table VIII   As shown, extending the scope of code completion from a single immediate line to two lines yields consistent results with previous findings.Even well-performing models continue to tend to generate substantial incorrect outputs.The performance of code completion strategies remains closely linked to their propensity to generate errors.Additionally, the application of post-processing approaches does not significantly alter these results.The consistency of these results across different settings highlights the persistent challenges in enhancing the accuracy and reliability of automated code completions.</p>
<p>D. Reasoning LLM Analysis</p>
<p>New reasoning models (e.g., ChatGPT-o1 [84], DeepSeek-R1 [42]) have attracted significant media attention due to their promising capabilities in code completion tasks.However, because code completion scenarios demand extremely high speed, these models are typically excluded from early experimental phases owing to their computational demands.To investigate whether the code completions generated by these new reasoning models behave similarly to those produced by established LLMs in bug-prone contexts, we conduct additional experiments using DeepSeek-R1.</p>
<p>The experimental results are presented in Table X, which details the code quality analysis of DeepSeek-R1 in bug-prone code completion tasks.The table is consistent with that in RQ1, which reinforces the trends observed across different LLMs.</p>
<p>From this table, we observe that the key findings still hold for reasoning models.Specifically, DeepSeek-R1 exhibits a nearly balanced distribution of correct and buggy completions in bug-prone tasks.It produces 76 completely correct completions (13.92%) and 65 completely buggy completions (11.90%), along with 116 nearly correct (21.25%) and 113 nearly buggy completions (20.70%).Notably, the completely buggy ratio stands at 36.52%, indicating a substantial error rate when handling bug-prone code.When completing normal code, DeepSeek-R1 achieves higher correctness with 163 com-pletely correct completions (29.85%) and 186 nearly correct completions (34.07%).These results show that even state-ofthe-art reasoning models still struggle with bug-prone code completion.</p>
<p>E. Implications</p>
<p>The findings from this study have important implications for both the development and application of LLMs in software engineering in the realm of code completion: a) Enhanced Understanding of LLM Capabilities and Limitations: This research deepens our understanding of LLMs' capabilities and limitations in handling bug-prone code.Despite their proficiency in generating syntactically correct code, LLMs often do not fully grasp the semantic nuances, leading to potential errors.This dual nature underscores the need for enhanced model training that goes beyond syntactic understanding to include more robust semantic analysis.</p>
<p>b) Need for Improved Error Handling: This study underscores the need for improved error handling in code completions, especially in fragile contexts.The high incidence of errors calls for the development of more sophisticated error detection and correction mechanisms within LLMs.There is a clear need for models that not only generate code but also evaluate and adapt their outputs based on potential logical and runtime errors.c) Refinement of Post-processing Approaches: This investigation highlights the need for refining post-processing approaches in LLM outputs.While techniques like majority voting, prompt engineering, and selective modeling have been employed to refine the outputs of LLMs, their effectiveness remains limited.The findings suggest that these techniques, although useful in increasing the reliability of completions, still fall short of significantly reducing errors.This highlights an opportunity for developing more advanced post-processing algorithms that can better discern and correct inaccuracies in model outputs.</p>
<p>d) Practical Integration in Development Environments: This research advocates for the practical integration of enhanced LLMs with robust post-processing tools in integrated development environments (IDEs).For practitioners, such integration can significantly boost productivity and reduce error rates in real-time coding scenarios.However, the integration process must be handled with care to avoid introducing additional complexity or overhead that can detract from the user experience.</p>
<p>VI. THREATS TO VALIDITY</p>
<p>Threats to external validity lie in LLM code completion capabilities, dataset quality, and result generalization.We selecte top-performing and popular LLMs, including seven models specifically for this study, alongside others such as Bloom et al. [85], Vicuna et al. [86], ChatGLM [38], [39] and Phi et al. [87].For data sets, we mainly used the widely recognized Defects4J benchmark [54], [55], [60], [61], and further validated our findings using the ConDefects dataset in our discussion section.To prevent data leakage in training data, we excluded projects from both datasets in the bug-fix dataset [82].Although our experiments are confined to Java code, the comprehensive and real-world nature of both datasets supports their reliability and generalization for evaluating LLM performance variations.Threats to internal validity lie in code completion, which involves selecting model parameters and comparing code lengths.In our process, we use default settings for all parameters except the temperature setting.While varying default parameters might induce result variations, limiting adjustments to the temperature parameter ensured experimental consistency and control, mitigating factors that may impact internal validity.For evaluation simplicity and to emphasize initial accuracy, we compare only the first line of generated code.As code completion generally operates on a token-by-token and lineby-line basis, the first line provides a reliable benchmark for assessing the accuracy and relevance of the completion tool.Threats to construct validity lie in the choice of evaluation metrics.Metrics like CodeBLEU and CodeBERT score, de-signed for larger code blocks, often falter in precision when applied to single lines.To enhance accuracy, we use a metric that merges the LCS with LED.We validated this metric through a human evaluation, which confirmed its alignment with human judgment, thus showing its effectiveness.</p>
<p>VII. CONCLUSION</p>
<p>In this study, we conduct the first investigation into the ability of large language models (LLMs) to handle bugprone code completion tasks.Our empirical evaluation on seven state-of-the-art LLMs, including ChatGPT (3.5, 4.0, 4o), CodeLlama, StarCoder, CodeGEN, and Gemma, reveals several critical insights.We find that completing bug-prone code is significantly more difficult than standard code completion, with models generating correct and buggy completions with nearly equal probability (approximately 151 correct versus 149 buggy completions on average) and achieving substantially lower accuracy (e.g., 12.27% versus 29.85% for GPT-4).A striking discovery was that 44.44% of the bugs produced by LLMs are identical to historical bugs, indicating a tendency to replicate past errors rather than learn from corrected versions.Additionally, certain code constructs, such as method invocations, return statements, and variable declarations, prove particularly error-prone, while existing post-processing techniques fail to significantly reduce these error rates, despite improving output consistency.These findings highlight LLM limitations in managing complex code dependencies, underscoring the need for research.High error rates and bug replication suggest new methodologies are essential.</p>
<p>Fig. 1 .
1
Fig. 1.Buggy Code Example Generated By OpenAI-GPT3.5</p>
<p>Fig. 2 .
2
Fig. 2. Overview of Our Research Methodology</p>
<p>12</p>
<p>Guo and Ye, et al.</p>
<p>Fig. 3 .
3
Fig. 3. Example of Prompt</p>
<p>Fig. 4 .
4
Fig. 4. Analysis of Code Memorization Across Different LLMs</p>
<p>Fig. 5 .
5
Fig. 5. Distribution of similarity scores between LLM completions and ground-truth code for Bug-prone vs Normal Code code lines (Experimental Results).</p>
<p>TABLE I ANALYSIS
I
OF CODE QUALITY COMPLETED BY DIFFERENT LLMS
LLMsTask TypeCorrect-identical Bug-identical Correct-closeBug-close Non-compliantBug-identical ratio (against all bugs)OpenAI-GPT4obug-prone normal151 (27.66%) 183 (33.52%)152 (27.84%) -26 (4.76%) 189 (34.62%)32 (5.86%) -185 (33.88%) 174 (31.87%)82.61% -OpenAI-GPT3.5bug-prone normal125 (22.89%) 248 (45.42%)91 (16.67%) -101 (18.50%) 145 (26.56%)87 (15.93%) -142 (25.82%) 153 (28.02%)51.12% -OpenAI-GPT4bug-prone normal67 (12.27%) 163 (29.85%)59 (10.81%) -107 (19.60%) 170 (31.14%)95 (17.40%) -218 (39.93%) 213 (39.01%)38.31% -CodeLlama-13B-hfbug-prone normal57 (10.44%) 142 (26.01%)42 (7.69%) -76 (13.91%) 105 (19.23%) 179 (32.78%) -266 (48.72%) 225 (41.21%)28.57% -Gemma-7Bbug-prone normal34 (6.23%) 101 (18.50%)18 (3.30%) -88 (16.12%) 102 (18.68%) 204 (37.36%) -304 (55.68%) 241 (44.14%)15.00% -StarCoder2-15Bbug-prone normal31 (5.68%) 85 (15.57%)27 (4.95%) -64 (11.72%) 177 (32.42%)88 (16.12%) -333 (61.00%) 284 (52.01%)23.48% -CodeGEN-350Mbug-prone normal30 (5.49%) 91 (16.67%)26 (4.76%) -97 (17.77%) 119 (21.79%) 189 (34.62%) -274 (50.18%) 266 (48.72%)17.93% -</p>
<p>TABLE IV THE
IV
CODE TYPES OF THE FIXED CODE THAT CORRESPOND TO THE COMPLETED CODE FROM DIFFERENT MODELS
Code TypeCodeGEN-350MStarCoder2-15B CodeLlama-13B-hfGemma-7B OpenAI-GPT3.5 OpenAI-GPT4 OpenAI-GPT4oExpression Statement13/13/5815/10/5919/18/4716/15/5322/23/3918/17/4922/19/43Field Declaration5/4/55/2/79/3/26/4/44/5/56/3/55/4/5For Statement1/1/90/0/112/1/81/2/81/2/81/1/91/1/9Identifier4/0/32/1/42/0/50/2/53/2/22/2/33/2/2If Statement21/10/10211/8/11427/23/8322/14/9732/19/8225/18/9028/17/88Variable Declaration11/3/9810/6/9622/10/8012/7/9327/15/7016/11/8516/11/85Method Invocation1/2/91/4/73/4/51/2/91/3/82/3/71/4/7Method Declaration2/5/62/3/82/6/51/4/83/4/61/7/51/8/4Return Statement9/17/5510/14/5718/16/4715/14/5225/16/4011/22/4814/17/50Throw Statement0/1/10/1/10/1/10/0/20/1/11/0/10/1/1While Statement0/1/30/0/40/1/31/2/10/2/22/0/21/1/2Other10/8/559/8/5611/12/506/12/5516/12/4511/6/5611/9/53TABLE VTHE CORRECT RESULTS WITH POST-PROCESSING APPROACHES</p>
<p>TABLE VI THE
VI
RESULTS ON TWO-LINES COMPLETION WITH POST-PROCESSING APPROACHES
Tem.OpenAI-GPT4oOpenAI-GPT4OpenAI-GPT3.5Gemma-7BCodeLlama-13B-hf StarCoder2-15B CodeGEN-350MVotingPESel.</p>
<p>and Table IX.These two tables represent the results of different models on different temperatures and the results selected using the majority voting (Voting), prompt engineering (PE), and selection model (Sel.)from code completions of various models under specific temperatures.In this table, each line represents a temperature and each column represents a LLM.The three numbers within the table represent Correct Completions (Nearly Correct + Correct-identical), Correct and buggy completions (Correct-close + Completely Correct + Bug-close
&amp;RUUHFWFORVH%XJFORVHRI7RWDO&amp;RUUHFWLGHQWLFDO%XJLGHQWLFDO&amp;RGH<em>(10 6WDU&amp;RGHUE&amp;RGH/ODPDEKI</em>HPPDE2SHQ$,<em>372SHQ$,</em>372SHQ$,*37R7HPSHUDWXUHFig. 7. Exploring the Impact of Temperature Settings on Code Completion for Various Large Language Models</p>
<p>TABLE VII ANALYSIS
VII
OF CODE QUALITY COMPLETED BY DIFFERENT LLMS ON CONDEFECTS DATASET Fig. 8. Exploring the Impact of Different Token Lengths on Code Completion+ Bug-identical), and its proportions.In particular, the postprocessing approaches in Table VIII select the most similar results from different models with the same temperature setting across the previous five models, whereas those in TableIXselect the best completion from multiple code completion results generated by each model in different temperatures.
LLMsCompletely CorrectCompletely FaultyNearly CorrectNearly FaultyNon-compliantOpenAI-GPT4o 212(11.16%)117(6.16%)330(17.38%) 450(23.70%)790(41.60%)Gemma-7B145(7.64%)112(5.90%)262(13.80%) 465(24.49%)915(48.18%)Number of Instances100 120 140 160 180 200Correct-close Bug-close Correct-identical Bug-identical Non-compliant60 80Token Length 200 300 400 500 600 700 800 900 1000 Token Length=600</p>
<p>TABLE VIII THE
VIII
RESULTS ON TWO-LINES COMPLETION WITH POST-PROCESSING APPROACHES (DIFFERENT MODELS)
Tem.OpenAI-GPT3.5Gemma-7BCodeLlama-13B-hf StarCoder2-15B CodeGEN-350MVotingPESel.</p>
<p>TABLE X ANALYSIS
X
OF CODE QUALITY COMPLETED BY DEEPSEEK R1
LLMsTask TypeCompletely CorrectCompletely BuggyNearly CorrectNearly BuggyNon-compliantCompletely Buggy RatioDeepSeek R1 bug-prone76 (13.92%) 65 (11.90%) 116 (21.25%)113 (20.70%)176 (32.23%)36.52%normal163 (29.85%)-186 (34.07%)-197 (36.08%)-
If structurally equivalent normal code cannot be found within the same file, we expand our search to subsequent files within the dataset.</p>
<p>OpenAI-GPT4o OpenAI-GPT4 OpenAI. Tem, GPT3.5</p>
<p>Gemma CodeLlama StarCoder2 CodeGEN Voting PE Sel. </p>
<p>TABLE IX THE RESULTS ON TWO-LINES COMPLETION WITH POST-PROCESSING APPROACHES (DIFFERENT TEMPERATURE). </p>
<p>Gemma-7B CodeLlama-13B-hf StarCoder2-15B CodeGEN-350M. </p>
<p>. Avg, </p>
<p>Codefill: multi-token code completion by jointly learning from structure and naming sequences. M Izadi, R Gismondi, G Gousios, Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. the 44th International Conference on Software Engineering, ser. ICSE '22New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Nonautoregressive line-level code completion. F Liu, Z Fu, G Li, Z Jin, H Liu, Y Hao, L Zhang, ACM Transactions on Software Engineering and Methodology. 2024</p>
<p>Toward less hidden cost of code completion with acceptance and ranking models. J Li, R Huang, W Li, K Yao, W Tan, oct 2021Los Alamitos, CA, USA</p>
<p>Code completion with statistical language models. V Raychev, M Vechev, E Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI '14. the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI '14New York, NY, USAAssociation for Computing Machinery2014</p>
<p>Unixcoder: Unified cross-modal pre-training for code representation. D Guo, S Lu, N Duan, Y Wang, M Zhou, J Yin, 2022</p>
<p>Multi-task learning based pre-trained language model for code completion. F Liu, G Li, Y Zhao, Z Jin, 2020</p>
<p>Big code!= big vocabulary: open-vocabulary models for source code. R.-M Karampatsis, H Babii, R Robbes, C Sutton, A Janes, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ser. ICSE '20. the ACM/IEEE 42nd International Conference on Software Engineering, ser. ICSE '20ACMJun. 2020</p>
<p>Pythia: Ai-assisted code completion system. A Svyatkovskiy, Y Zhao, S Fu, N Sundaresan, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining, ser. KDD '19. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining, ser. KDD '19ACMJul. 2019</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Code-gen2: Lessons for training llms on programming and natural languages. E Nijkamp, H Hayashi, C Xiong, S Savarese, Y Zhou, 2023</p>
<p>Code llama: Open foundation models for code. B Rozière, J Gehring, F Gloeckle, 2024</p>
<p>Starcoder 2 and the stack v2: The next generation. A Lozhkov, R Li, L B , 2024</p>
<p>Microsoft has over a million paying Github Copilot users: CEO Nadella. Zdnet, 2023</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, 2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Narasimhan, Karthik, Salimans, Tim, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Gpt understands, too. X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, J Tang, 2023</p>
<p>Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. P P Ray, Internet of Things and Cyber-Physical Systems. 32023</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, 2023</p>
<p>Starcoder: may the source be with you. R Li, L B Allal, Y Zi, 2023</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, 2023</p>
<p>Gemma: Open models based on gemini research and technology. G Team, T Mesnard, C Hardin, 2024</p>
<p>Defects4j: A database of existing faults to enable controlled testing studies for java programs. R Just, D Jalali, M D Ernst, Proceedings of the 2014 international symposium on software testing and analysis. the 2014 international symposium on software testing and analysis2014</p>
<p>Condefects: A complementary dataset to address the data leakage concern for llm-based fault localization and program repair. Y Wu, Z Li, J M Zhang, Y Liu, Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering. 2024</p>
<p>Learning from examples to improve code completion systems. M Bruch, M Monperrus, M Mezini, Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering. the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering2009</p>
<p>Intellicode compose: Code generation using transformer. A Svyatkovskiy, S K Deng, S Fu, N Sundaresan, Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering2020</p>
<p>Code prediction by feeding trees to transformers. S Kim, J Zhao, Y Tian, S Chandra, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Ircoco: Immediate rewards-guided deep reinforcement learning for code completion. B Li, Z Sun, T Huang, H Zhang, Y Wan, G Li, Z Jin, C Lyu, arXiv:2401.166372024arXiv preprint</p>
<p>Towards a better code completion system by api grouping, filtering, and popularity-based ranking. D Hou, D M Pletcher, Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering, ser. RSSE '10. the 2nd International Workshop on Recommendation Systems for Software Engineering, ser. RSSE '10New York, NY, USAAssociation for Computing Machinery2010</p>
<p>On the naturalness of software. A Hindle, E T Barr, Z Su, M Gabel, P Devanbu, 2012 34th International Conference on Software Engineering (ICSE). 2012</p>
<p>On the localness of software. Z Tu, Z Su, P Devanbu, Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringNew York, NY, USAAssociation for Computing Machinery2014. 2014</p>
<p>Learning from examples to improve code completion systems. M Bruch, M Monperrus, M Mezini, ser. ESEC/FSE '092009Association for Computing MachineryNew York, NY, USA</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Suggesting accurate method and class names. M Allamanis, E T Barr, C Bird, C Sutton, ser. ESEC/FSE 2015Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. the 2015 10th Joint Meeting on Foundations of Software EngineeringNew York, NY, USAAssociation for Computing Machinery2015</p>
<p>Productivity assessment of neural code completion. A Ziegler, E Kalliamvakou, X A Li, A Rice, D Rifkin, S Simister, G Sittampalam, E Aftandilian, 2022New York, NY, USA</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. 268232499</p>
<p>Grok. Grok, 2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, </p>
<p>Leveraging large language models for sequential recommendation. J Harte, W Zorgdrager, P Louridas, A Katsifodimos, D Jannach, M Fragkoulis, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems</p>
<p>Llm is like a box of chocolates: the non-determinism of chatgpt in code generation. S Ouyang, J M Zhang, M Harman, M Wang, 2023</p>
<p>Copiloting the copilots: Fusing large language models with completion engines for automated program repair. Y Wei, C S Xia, L Zhang, 2023New York, NY, USA</p>
<p>Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. J White, S Hays, Q Fu, J Spencer-Smith, D C Schmidt, 2023</p>
<p>Practical program repair in the era of large pre-trained language models. C S Xia, Y Wei, L Zhang, arXiv:2210.141792022arXiv preprint</p>
<p>Automated program repair. C L Goues, M Pradel, A Roychoudhury, Communications of the ACM. 62122019</p>
<p>Semfix: Program repair via semantic analysis. H D T Nguyen, D Qi, A Roychoudhury, S Chandra, 2013 35th International Conference on Software Engineering (ICSE). IEEE2013</p>
<p>Astor: A program repair library for java. M Martinez, M Monperrus, Proceedings of the 25th international symposium on software testing and analysis. the 25th international symposium on software testing and analysis2016</p>
<p>Neural program repair with execution-based backpropagation. H Ye, M Martinez, M Monperrus, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Copiloting the copilots: Fusing large language models with completion engines for automated program repair. Y Wei, C S Xia, L Zhang, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>A survey of challenges in spectrum-based software fault localization. Q I Sarhan, Á Beszédes, IEEE Access. 102022</p>
<p>Back to the future! studying data cleanness in defects4j and its impact on fault localization. A R Chen, M N Rafi, T.-H Chen, S Wang, 2023</p>
<p>Large language models for test-free fault localization. A Z Yang, C Le Goues, R Martins, V Hellendoorn, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Mutationbased fault localization of deep neural networks. A Ghanbari, D.-G Thomas, M A Arshad, H Rajan, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Large language models in fault localisation. Y Wu, Z Li, J M Zhang, M Papadakis, M Harman, Y Liu, 2023</p>
<p>Fault localization via efficient probabilistic modeling of program semantics. M Zeng, Y Wu, Z Ye, Y Xiong, X Zhang, L Zhang, Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. the 44th International Conference on Software Engineering, ser. ICSE '22New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Empirical review of java program repair tools: A large-scale experiment on 2,141 bugs and 23,551 repair attempts. T Durieux, F Madeiral, M Martinez, R Abreu, 2019</p>
<p>Precise condition synthesis for program repair. Y Xiong, J Wang, R Yan, J Zhang, S Han, G Huang, L Zhang, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE. 2017</p>
<p>Identifying patch correctness in test-based program repair. Y Xiong, X Liu, M Zeng, L Zhang, G Huang, Proceedings of the 40th International Conference on Software Engineering, ser. ICSE '18. the 40th International Conference on Software Engineering, ser. ICSE '18ACMMay 2018</p>
<p>Automatic repair of real bugs in java: a large-scale experiment on the defects4j dataset. M Martinez, T Durieux, R Sommerard, J Xuan, M Monperrus, Empirical Software Engineering. 224Oct. 2016</p>
<p>On reliability of patch correctness assessment. X.-B D Le, L Bao, D Lo, X Xia, S Li, C Pasareanu, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). 2019</p>
<p>Do automated program repair techniques repair hard and important bugs?. M Motwani, S Sankaranarayanan, R Just, Y Brun, Empirical Software Engineering. 2352018</p>
<p>. C Wang, J Hu, C Gao, Y Jin, T Xie, H Huang, Z Lei, Y Deng, arXiv:2301.03846Practitioners' expectations on code completion. 2023arXiv preprint</p>
<p>Huggingface transformers. Huggingface, 2024</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. S Ren, D Guo, S Lu, L Zhou, S Liu, D Tang, N Sundaresan, M Zhou, A Blanco, S Ma, arXiv:2009.102972020arXiv preprint</p>
<p>Codebertscore: Evaluating code generation with pretrained models of code. S Zhou, U Alon, S Agarwal, G Neubig, arXiv:2302.055272023arXiv preprint</p>
<p>Automatic testing and improvement of machine translation. Z Sun, J M Zhang, M Harman, M Papadakis, L Zhang, Proceedings of the ACM/IEEE 42nd international conference on software engineering. the ACM/IEEE 42nd international conference on software engineering2020</p>
<p>Cctest: Testing and repairing code completion systems. Z Li, C Wang, Z Liu, H Wang, D Chen, S Wang, C Gao, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Uncovering chatgpt's capabilities in recommender systems. S Dai, N Shao, H Zhao, W Yu, Z Si, C Xu, Z Sun, X Zhang, J Xu, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>Coder reviewer reranking for code generation. T Zhang, T Yu, T Hashimoto, M Lewis, W -T. Yih, D Fried, S Wang, International Conference on Machine Learning. PMLR2023846</p>
<p>Reranking for neural semantic parsing. P Yin, G Neubig, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Don't trust: Verify-grounding llm quantitative reasoning with autoformalization. J P Zhou, C Staats, W Li, C Szegedy, K Q Weinberger, Y Wu, arXiv:2403.181202024arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Prompt engineering for healthcare: Methodologies and applications. J Wang, E Shi, S Yu, Z Wu, C Ma, H Dai, Q Yang, Y Kang, J Wu, H Hu, arXiv:2304.146702023arXiv preprint</p>
<p>Sample design engineering: An empirical study of what makes good downstream fine-tuning samples for llms. B Guo, H Wang, W Xiao, H Chen, Z Lee, S Han, H Huang, 2024</p>
<p>Codet5+: Open code large language models for code understanding and generation. Y Wang, H Le, A D Gotmare, N D Bui, J Li, S C Hoi, arXiv:2305.079222023arXiv preprint</p>
<p>A syntax-guided edit decoder for neural program repair. Q Zhu, Z Sun, Y -A. Xiao, W Zhang, K Yuan, Y Xiong, L Zhang, Proceedings of the 29th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. the 29th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering2021</p>
<p>tree sitter. 2024tree-sitter</p>
<p>Learning to reason with llms. 2024</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. T Le Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>Y Li, S Bubeck, R Eldan, A Del Giorno, S Gunasekar, Y T Lee, arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
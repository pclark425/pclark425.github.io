<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8743 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8743</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8743</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278997020</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22823v2.pdf" target="_blank">Self-Critique and Refinement for Faithful Natural Language Explanations</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8743.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8743.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SR-NLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critique and Refinement for Natural Language Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-model, iterative generate-then-reflect framework that enables LLMs to self-critique and refine post-hoc natural language explanations (NLEs) using either natural-language feedback or explicit important-word feedback derived from attributions or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama, Mistral, Qwen, Falcon (instruction-tuned variants, <=10B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source LLMs used in experiments: Falcon3-Instruct 7B, Llama3.1-Instruct 8B, Mistral-Instruct-v0.3 7B, Qwen2.5-Instruct 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SR-NLE (iterative self-critique & refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Start from an initial answer and post-hoc NLE, then repeat K rounds (K up to 3) of: (1) generate feedback on the previous explanation (either free-form Natural Language Feedback, NLF, or Important Word Feedback, IWF), and (2) generate a refined explanation conditioned on the input, answer, previous explanation, and feedback. IWF can be prompt-based or attribution-based (attention or integrated gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ComVE, ECQA, e-SNLI (natural language reasoning with NLEs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ComVE: commonsense validation (which sentence violates commonsense); ECQA: multiple-choice commonsense QA with explanations; e-SNLI: natural language inference (premise-hypothesis) with explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Best reported: IWF-Attn after 3 refinement rounds achieves average unfaithfulness rate 36.02% (percentage of counterfactual instances whose NLEs do NOT mention the intervened word). Other IWF variants: IWF-IG 36.30%, IWF-Pmt 37.23%; NLF average unfaithfulness higher (numbers reported per-dataset/model in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Init-NLE (no refinement) average unfaithfulness rate 54.81% (baseline reported across same models/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-feedback loops inside a single LLM; feedback types are (a) NLF: model-produced textual critique, and (b) IWF: a list of top-N important input words derived either by prompting the model to score words or by feature-attribution methods (attention weights or integrated gradients). Refinement is performed by feeding feedback into a refinement prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative reductions in the counterfactual-based unfaithfulness metric: average absolute reduction of 18.79 percentage points for IWF-Attn vs Init-NLE (54.81% -> 36.02%). Improvements occur mainly in round 1 (e0->e1) with diminishing returns; positive transitions (unfaithful->faithful) substantially exceed negative transitions (faithful->unfaithful), particularly for IWF variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report limitations: framework only evaluated for post-hoc NLEs (unclear generalization to chain-of-thought or jointly generated rationales); evaluation relies solely on counterfactual tests which capture one faithfulness criterion; attribution methods have reliability issues (attention may not reflect importance; IG sensitive to baseline and expensive — IG required many integration steps, e.g., 500 or 1000 steps); computational cost and scalability concerns for IG; experiments limited to models under 10B so results may vary with model scale; some isolated cases where NLF underperforms (e.g., e-SNLI with Falcon).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against Init-NLE (no refinement) and SC-NLE (self-consistency ensemble centroid selection). SR-NLE (IWF variants) consistently outperform NLF and SC-NLE in reported unfaithfulness rates; compared to prior architecture/tuning approaches (e.g., G-TEX) SR-NLE provides a lightweight alternative without model changes or fine-tuning, though cross-paper comparisons are not fully controlled. Related work Self-Refine and Reflexion are cited as general self-refinement/self-reflection paradigms; Cross-Refine is noted as a cross-model variant (different LLMs for generation vs feedback) whereas SR-NLE is single-model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Multiple ablations reported: (1) IG integration steps: Falcon, Llama, Mistral converge around ~500 steps; Qwen required ~1000, so main experiments used 500 for three models and 1000 for Qwen. (2) SC-NLE parameters: sampling 20 candidates at temperature 1.0 chosen (more candidates reduced unfaithfulness). (3) Number of important words N: unfaithfulness decreases as N increases (significant gains up to N~9); experiments used top-5. Additional analyses show most improvement occurs in first refinement round and that IWF performance is robust even when word selection quality is imperfect (random selection baseline close to IWF in some tests), indicating iterative refinement contributes beyond perfect attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique and Refinement for Faithful Natural Language Explanations', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8743.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8743.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC-NLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for Natural Language Explanations (SC-NLE baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based baseline that samples multiple candidate explanations from an LLM and selects the most representative explanation by computing a semantic centroid in embedding space and choosing the candidate closest to that centroid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Same instruction-tuned models used in experiments (Falcon, Llama, Mistral, Qwen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a zero-shot/stochastic-sampling strategy on the same models: sample multiple explanation candidates (20) and select centroid-match via SentenceBERT embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (semantic centroid voting variant SC-NLE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate n candidate explanations with temperature sampling, encode candidates with SentenceBERT, compute centroid embedding, and pick the candidate with highest cosine similarity to the centroid as the representative explanation (no iterative refine rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ComVE, ECQA, e-SNLI (same NLE tasks as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See SR-NLE entry; SC-NLE used as a strong ensemble baseline for explanation selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>SC-NLE average unfaithfulness reported 49.74% (Table 1 aggregate across models/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Init-NLE average unfaithfulness 54.81% (same experimental setting).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Not iterative self-reflection; uses stochastic sampling (temperature 1.0) and semantic centroid selection via SentenceBERT embeddings to pick the most representative explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>SC-NLE reduces unfaithfulness compared to Init-NLE (approx. 5 percentage points on average), with ablations showing more samples (up to 20) and temperature 1.0 improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SC-NLE is an ensemble-selection method rather than an iterative correction mechanism; it can be computationally expensive to sample many candidates and depends on embedding quality (SentenceBERT) and sampling parameters; it does not explicitly address faithfulness to model reasoning, only consensus among sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared in experiments: SR-NLE (IWF) outperforms SC-NLE in unfaithfulness reduction. Ablations show SC-NLE benefits from more samples and higher temperature but still lags behind best SR-NLE variants.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations indicate unfaithfulness decreases with more candidates (significant improvements up to 20) and temperature 1.0 outperforms lower temperatures, informing the chosen configuration (20 candidates, temp=1.0).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique and Refinement for Faithful Natural Language Explanations', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8743.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8743.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published iterative self-refinement approach where an LLM critiques and revises its own outputs to improve performance across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various large language models (general method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General self-refinement approach applied to LLMs (cited work), not reimplemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative generate-critique-revise loop where the same model produces an initial output, generates critique or error signals, and then revises the output accordingly; number of iterations varies by implementation (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General NLP tasks (as reported by original Self-Refine paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited as prior art showing iterative self-refinement can improve outputs across a variety of tasks; not specifically applied to NLE faithfulness in the cited work within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting and internal critique-generation by the LLM; original work details not recreated here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited in related work as evidence that LLMs can improve outputs via self-critique and revision across tasks (no numerical results reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; cited as general inspiration. The paper notes that prior work did not target NLE faithfulness specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as prior work establishing feasibility of self-refinement; SR-NLE builds on this idea but targets NLE faithfulness and uses attribution-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique and Refinement for Faithful Natural Language Explanations', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8743.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8743.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Refine (cross-model refinement for NLEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related framework that applies iterative refinement to NLE generation by using separate models for generation and feedback (cross-model), with human judgments used for faithfulness evaluation in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross-refine: Improving natural language explanation generation by learning in tandem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cross-model setups (separate LLMs for generation and feedback) as described in the cited work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original Cross-Refine adopts a two-LLM design (generator and feedback model); not reimplemented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Cross-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>One LLM generates initial explanations and a separate LLM provides feedback and revision suggestions; evaluation in original work involved human judgments for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLE generation (explanation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Targeted at improving NLE generation but uses a cross-model architecture rather than single-model self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Cross-model feedback (generator + separate reviewer LLM) and revision based on reviewer suggestions; relies on multi-model interaction rather than sole self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as related work; authors contrast SR-NLE (single-model) with Cross-Refine (cross-model) and note that Cross-Refine primarily used human judgments for faithfulness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires multiple LLMs and human judgments for faithfulness evaluation in prior work; SR-NLE highlights single-model autonomous refinement as a lighter alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with SR-NLE: Cross-Refine uses different models for generation and feedback and relies more on human evaluation, while SR-NLE is single-model and uses automated counterfactual tests.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique and Refinement for Faithful Natural Language Explanations', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8743.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8743.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach where language agents incorporate internal verbal reflection and reinforcement signals to iteratively improve agent-level behavior and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>general LLM agent implementations (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reflexion is an agent-level method that augments LLM agents with internal reflection and feedback-driven updates; cited in related work as evidence of self-reflection capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents produce reflective verbalizations about their trajectory/behavior and use reinforcement-like signals to update future action selection; not implemented in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Agent reasoning / language-agent tasks (from cited paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent-level tasks where internal reflection and reinforcement learning are used to improve performance; cited as related work on self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Agent internal verbalizations and reinforcement-informed updates; different focus (agent control) than SR-NLE's explanation faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited in related work; original paper reports performance improvements on agent tasks but this paper only references Reflexion conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper; cited to situate SR-NLE among other self-reflection approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as complementary prior work showing the broader feasibility of self-reflection in language agents; SR-NLE is specialized for NLE faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique and Refinement for Faithful Natural Language Explanations', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Cross-refine: Improving natural language explanation generation by learning in tandem <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-amplify: Improving small language models with self post hoc explanations <em>(Rating: 1)</em></li>
                <li>Graph-guided textual explanation generation framework <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8743",
    "paper_id": "paper-278997020",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SR-NLE",
            "name_full": "Self-critique and Refinement for Natural Language Explanations",
            "brief_description": "A single-model, iterative generate-then-reflect framework that enables LLMs to self-critique and refine post-hoc natural language explanations (NLEs) using either natural-language feedback or explicit important-word feedback derived from attributions or prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama, Mistral, Qwen, Falcon (instruction-tuned variants, &lt;=10B)",
            "model_description": "Instruction-tuned open-source LLMs used in experiments: Falcon3-Instruct 7B, Llama3.1-Instruct 8B, Mistral-Instruct-v0.3 7B, Qwen2.5-Instruct 7B.",
            "reflection_method_name": "SR-NLE (iterative self-critique & refinement)",
            "reflection_method_description": "Start from an initial answer and post-hoc NLE, then repeat K rounds (K up to 3) of: (1) generate feedback on the previous explanation (either free-form Natural Language Feedback, NLF, or Important Word Feedback, IWF), and (2) generate a refined explanation conditioned on the input, answer, previous explanation, and feedback. IWF can be prompt-based or attribution-based (attention or integrated gradients).",
            "task_name": "ComVE, ECQA, e-SNLI (natural language reasoning with NLEs)",
            "task_description": "ComVE: commonsense validation (which sentence violates commonsense); ECQA: multiple-choice commonsense QA with explanations; e-SNLI: natural language inference (premise-hypothesis) with explanations.",
            "performance_with_reflection": "Best reported: IWF-Attn after 3 refinement rounds achieves average unfaithfulness rate 36.02% (percentage of counterfactual instances whose NLEs do NOT mention the intervened word). Other IWF variants: IWF-IG 36.30%, IWF-Pmt 37.23%; NLF average unfaithfulness higher (numbers reported per-dataset/model in Table 1).",
            "performance_without_reflection": "Init-NLE (no refinement) average unfaithfulness rate 54.81% (baseline reported across same models/datasets).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered self-feedback loops inside a single LLM; feedback types are (a) NLF: model-produced textual critique, and (b) IWF: a list of top-N important input words derived either by prompting the model to score words or by feature-attribution methods (attention weights or integrated gradients). Refinement is performed by feeding feedback into a refinement prompt.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative reductions in the counterfactual-based unfaithfulness metric: average absolute reduction of 18.79 percentage points for IWF-Attn vs Init-NLE (54.81% -&gt; 36.02%). Improvements occur mainly in round 1 (e0-&gt;e1) with diminishing returns; positive transitions (unfaithful-&gt;faithful) substantially exceed negative transitions (faithful-&gt;unfaithful), particularly for IWF variants.",
            "limitations_or_failure_cases": "Authors report limitations: framework only evaluated for post-hoc NLEs (unclear generalization to chain-of-thought or jointly generated rationales); evaluation relies solely on counterfactual tests which capture one faithfulness criterion; attribution methods have reliability issues (attention may not reflect importance; IG sensitive to baseline and expensive — IG required many integration steps, e.g., 500 or 1000 steps); computational cost and scalability concerns for IG; experiments limited to models under 10B so results may vary with model scale; some isolated cases where NLF underperforms (e.g., e-SNLI with Falcon).",
            "comparison_to_other_methods": "Compared against Init-NLE (no refinement) and SC-NLE (self-consistency ensemble centroid selection). SR-NLE (IWF variants) consistently outperform NLF and SC-NLE in reported unfaithfulness rates; compared to prior architecture/tuning approaches (e.g., G-TEX) SR-NLE provides a lightweight alternative without model changes or fine-tuning, though cross-paper comparisons are not fully controlled. Related work Self-Refine and Reflexion are cited as general self-refinement/self-reflection paradigms; Cross-Refine is noted as a cross-model variant (different LLMs for generation vs feedback) whereas SR-NLE is single-model.",
            "ablation_study_results": "Multiple ablations reported: (1) IG integration steps: Falcon, Llama, Mistral converge around ~500 steps; Qwen required ~1000, so main experiments used 500 for three models and 1000 for Qwen. (2) SC-NLE parameters: sampling 20 candidates at temperature 1.0 chosen (more candidates reduced unfaithfulness). (3) Number of important words N: unfaithfulness decreases as N increases (significant gains up to N~9); experiments used top-5. Additional analyses show most improvement occurs in first refinement round and that IWF performance is robust even when word selection quality is imperfect (random selection baseline close to IWF in some tests), indicating iterative refinement contributes beyond perfect attribution.",
            "uuid": "e8743.0",
            "source_info": {
                "paper_title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SC-NLE",
            "name_full": "Self-Consistency for Natural Language Explanations (SC-NLE baseline)",
            "brief_description": "An ensemble-based baseline that samples multiple candidate explanations from an LLM and selects the most representative explanation by computing a semantic centroid in embedding space and choosing the candidate closest to that centroid.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Same instruction-tuned models used in experiments (Falcon, Llama, Mistral, Qwen)",
            "model_description": "Applied as a zero-shot/stochastic-sampling strategy on the same models: sample multiple explanation candidates (20) and select centroid-match via SentenceBERT embeddings.",
            "reflection_method_name": "Self-Consistency (semantic centroid voting variant SC-NLE)",
            "reflection_method_description": "Generate n candidate explanations with temperature sampling, encode candidates with SentenceBERT, compute centroid embedding, and pick the candidate with highest cosine similarity to the centroid as the representative explanation (no iterative refine rounds).",
            "task_name": "ComVE, ECQA, e-SNLI (same NLE tasks as main experiments)",
            "task_description": "See SR-NLE entry; SC-NLE used as a strong ensemble baseline for explanation selection.",
            "performance_with_reflection": "SC-NLE average unfaithfulness reported 49.74% (Table 1 aggregate across models/datasets).",
            "performance_without_reflection": "Init-NLE average unfaithfulness 54.81% (same experimental setting).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Not iterative self-reflection; uses stochastic sampling (temperature 1.0) and semantic centroid selection via SentenceBERT embeddings to pick the most representative explanation.",
            "number_of_iterations": null,
            "evidence_for_improvement": "SC-NLE reduces unfaithfulness compared to Init-NLE (approx. 5 percentage points on average), with ablations showing more samples (up to 20) and temperature 1.0 improve performance.",
            "limitations_or_failure_cases": "SC-NLE is an ensemble-selection method rather than an iterative correction mechanism; it can be computationally expensive to sample many candidates and depends on embedding quality (SentenceBERT) and sampling parameters; it does not explicitly address faithfulness to model reasoning, only consensus among sampled outputs.",
            "comparison_to_other_methods": "Directly compared in experiments: SR-NLE (IWF) outperforms SC-NLE in unfaithfulness reduction. Ablations show SC-NLE benefits from more samples and higher temperature but still lags behind best SR-NLE variants.",
            "ablation_study_results": "Ablations indicate unfaithfulness decreases with more candidates (significant improvements up to 20) and temperature 1.0 outperforms lower temperatures, informing the chosen configuration (20 candidates, temp=1.0).",
            "uuid": "e8743.1",
            "source_info": {
                "paper_title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "A previously published iterative self-refinement approach where an LLM critiques and revises its own outputs to improve performance across tasks.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "various large language models (general method)",
            "model_description": "General self-refinement approach applied to LLMs (cited work), not reimplemented in this paper's experiments.",
            "reflection_method_name": "Self-Refine",
            "reflection_method_description": "Iterative generate-critique-revise loop where the same model produces an initial output, generates critique or error signals, and then revises the output accordingly; number of iterations varies by implementation (not specified in this paper).",
            "task_name": "General NLP tasks (as reported by original Self-Refine paper)",
            "task_description": "Cited as prior art showing iterative self-refinement can improve outputs across a variety of tasks; not specifically applied to NLE faithfulness in the cited work within this paper.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompting and internal critique-generation by the LLM; original work details not recreated here.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited in related work as evidence that LLMs can improve outputs via self-critique and revision across tasks (no numerical results reproduced in this paper).",
            "limitations_or_failure_cases": "Not evaluated in this paper; cited as general inspiration. The paper notes that prior work did not target NLE faithfulness specifically.",
            "comparison_to_other_methods": "Mentioned as prior work establishing feasibility of self-refinement; SR-NLE builds on this idea but targets NLE faithfulness and uses attribution-based feedback.",
            "ablation_study_results": null,
            "uuid": "e8743.2",
            "source_info": {
                "paper_title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Cross-Refine",
            "name_full": "Cross-Refine (cross-model refinement for NLEs)",
            "brief_description": "A related framework that applies iterative refinement to NLE generation by using separate models for generation and feedback (cross-model), with human judgments used for faithfulness evaluation in the original work.",
            "citation_title": "Cross-refine: Improving natural language explanation generation by learning in tandem",
            "mention_or_use": "mention",
            "model_name": "cross-model setups (separate LLMs for generation and feedback) as described in the cited work",
            "model_description": "Original Cross-Refine adopts a two-LLM design (generator and feedback model); not reimplemented in this paper.",
            "reflection_method_name": "Cross-Refine",
            "reflection_method_description": "One LLM generates initial explanations and a separate LLM provides feedback and revision suggestions; evaluation in original work involved human judgments for faithfulness.",
            "task_name": "NLE generation (explanation tasks)",
            "task_description": "Targeted at improving NLE generation but uses a cross-model architecture rather than single-model self-refinement.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Cross-model feedback (generator + separate reviewer LLM) and revision based on reviewer suggestions; relies on multi-model interaction rather than sole self-reflection.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as related work; authors contrast SR-NLE (single-model) with Cross-Refine (cross-model) and note that Cross-Refine primarily used human judgments for faithfulness evaluation.",
            "limitations_or_failure_cases": "Requires multiple LLMs and human judgments for faithfulness evaluation in prior work; SR-NLE highlights single-model autonomous refinement as a lighter alternative.",
            "comparison_to_other_methods": "Contrasted with SR-NLE: Cross-Refine uses different models for generation and feedback and relies more on human evaluation, while SR-NLE is single-model and uses automated counterfactual tests.",
            "ablation_study_results": null,
            "uuid": "e8743.3",
            "source_info": {
                "paper_title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A prior approach where language agents incorporate internal verbal reflection and reinforcement signals to iteratively improve agent-level behavior and reasoning.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "general LLM agent implementations (cited work)",
            "model_description": "Reflexion is an agent-level method that augments LLM agents with internal reflection and feedback-driven updates; cited in related work as evidence of self-reflection capabilities.",
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Agents produce reflective verbalizations about their trajectory/behavior and use reinforcement-like signals to update future action selection; not implemented in the present paper.",
            "task_name": "Agent reasoning / language-agent tasks (from cited paper)",
            "task_description": "Agent-level tasks where internal reflection and reinforcement learning are used to improve performance; cited as related work on self-reflection.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Agent internal verbalizations and reinforcement-informed updates; different focus (agent control) than SR-NLE's explanation faithfulness.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited in related work; original paper reports performance improvements on agent tasks but this paper only references Reflexion conceptually.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper; cited to situate SR-NLE among other self-reflection approaches.",
            "comparison_to_other_methods": "Positioned as complementary prior work showing the broader feasibility of self-reflection in language agents; SR-NLE is specialized for NLE faithfulness.",
            "ablation_study_results": null,
            "uuid": "e8743.4",
            "source_info": {
                "paper_title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Cross-refine: Improving natural language explanation generation by learning in tandem",
            "rating": 2,
            "sanitized_title": "crossrefine_improving_natural_language_explanation_generation_by_learning_in_tandem"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-amplify: Improving small language models with self post hoc explanations",
            "rating": 1,
            "sanitized_title": "selfamplify_improving_small_language_models_with_self_post_hoc_explanations"
        },
        {
            "paper_title": "Graph-guided textual explanation generation framework",
            "rating": 1,
            "sanitized_title": "graphguided_textual_explanation_generation_framework"
        }
    ],
    "cost": 0.01698425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Critique and Refinement for Faithful Natural Language Explanations
6 Sep 2025</p>
<p>Yingming Wang 
University of Copenhagen</p>
<p>Pepa Atanasova 
University of Copenhagen</p>
<p>Self-Critique and Refinement for Faithful Natural Language Explanations
6 Sep 20252D2029866FB9ED3A6AB5838742974B08arXiv:2505.22823v2[cs.CL]
With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions.However, these explanations often fail to faithfully represent the model's actual reasoning process.While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness.To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -specifically, posthoc NLEs -through an iterative critique and refinement process without external supervision.Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words.Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -an absolute reduction of 18.79%.These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.Our code is available at https://github.com/ymwangv/SR-NLE.Identify the logical relationship between premise and hypothesis.Premise: A man in a red shirt is playing guitar on stage.Hypothesis: A man is performing music.</p>
<p>Introduction</p>
<p>With the rapid development of Large Language Models (LLMs), both closed-source models (Ope-nAI et al., 2024;Gemini et al., 2025) and opensource alternatives (Qwen et al., 2025;Grattafiori et al., 2024) have demonstrated remarkable capabilities across a wide range of Natural Language Processing (NLP) tasks.Yet, despite these advancements, understanding the reasoning behind their predictions remains a critical challenge -especially in applications demanding trust and accountability.</p>
<p>Natural Language Explanations (NLEs) have emerged as a promising solution by offering human-readable justifications for model predictions without requiring access to internal model mechanisms.However, ensuring their faithfulness remains a significant challenge.Recent studies have shown that NLEs generated by LLMs often fail to reflect the actual reasoning process of the model (Atanasova et al., 2023;Turpin et al., 2023;Lanham et al., 2023).</p>
<p>While prior work has primarily relied on changes of the model architecture or additional fine-tuning (Yuan et al., 2025;Wang et al., 2023a;Atanasova et al., 2022), we instead explore whether models possess the capability to independently assess and refine their own explanations.Supporting this direction, recent studies have shown that LLMs are capable of improving their outputs through iterative self-refinement (Madaan et al., 2023;Shinn et al., 2023).Following this, we ask whether LLMs know if and when their NLEs are faithful to their own internal reasoning by providing self-critique and refining their NLEs for improved faithfulness.</p>
<p>Building on this idea, we propose Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations through an iterative critique and refinement process without external supervision, as only the model itself has access to its internal reasoning and is therefore best positioned to assess explanation faithfulness.Our framework specifically targets post-hoc NLEs, where the explanation is generated after the model makes a prediction.Starting from an initial explanation, the model receives feedback identifying potential issues and generates a refined explanation accordingly.This process can be repeated multiple times, enabling incremental improvements.</p>
<p>A central component of SR-NLE is the design of feedback mechanisms that guide the refinement process.We explore two approaches: natural language feedback (NLF), which offers self-critiques in free-form text, and a novel feedback mechanismimportant word feedback (IWF), which identifies important input words for the prediction that are overlooked in the initially generated NLE.For IWF, we implement both prompt-based and attribution-based variants, including attentionbased and gradient-based techniques.</p>
<p>We validate the effectiveness of SR-NLE through extensive experiments across three reasoning datasets and four state-of-the-art LLMs, showing consistent improvements in explanation faithfulness over prior methods and strong baselines.</p>
<p>Our main contributions are as follows:</p>
<p>• We introduce SR-NLE, a novel framework that enables models to improve the faithfulness of their explanations through iterative self-critique and refinement guided by different feedback mechanisms, without external assistance, architectural modifications or specialized training.</p>
<p>• We propose and evaluate multiple feedback strategies for faithfulness, including natural language feedback (NLF) and a novel feedback mechanism -important word feedback (IWF), that leverages feature attribution to identify important input words in generated NLEs.</p>
<p>• We empirically demonstrate that SR-NLE significantly reduces unfaithfulness rates across multiple datasets and models.Our best method (attention-based IWF) achieves an average unfaithfulness rate of 36.02% compared to 54.81% for initial NLEs -an absolute reduction of 18.79% unfaithfulness.</p>
<p>Related Works</p>
<p>Natural Language Explanations and Faithfulness Evaluation Natural Language Explanations (NLEs) provide human-readable justifications for model predictions, traditionally obtained via supervised training on annotated datasets (Camburu et al., 2018;Rajani et al., 2019;Atanasova et al., 2020b).Recently, LLMs have enabled NLE generation via in-context learning (Brown et al., 2020).</p>
<p>A prominent example is chain-of-thought reasoning (Wei et al., 2022;Kojima et al., 2022), where LLMs generate intermediate reasoning steps alongside the model prediction.In contrast, our work focuses on post-hoc NLEs, where explanations are generated after the model prediction is made.Despite these advances in NLE generation, numerous studies have identified a gap between generated NLEs and the model's actual reasoning process (Atanasova et al., 2023;Turpin et al., 2023;Lanham et al., 2023).To quantify this faithfulness gap, researchers have proposed various automatic evaluation metrics, such as counterfactual tests (Atanasova et al., 2023;Siegel et al., 2024) and association-based measures (Wiegreffe et al., 2022;Parcalabescu and Frank, 2024).In this work, we adopt counterfactual tests (Atanasova et al., 2023) as our evaluation method, as they offer instancelevel, automatic assessments of explanation faithfulness without requiring human annotations.</p>
<p>Frameworks for Improving NLE Faithfulness</p>
<p>Existing frameworks for improving the faithfulness of NLEs employ strategies that either make changes to the model architecture or require an additional NLE fine-tuning stage with newly introduced objectives.Majumder et al. (2022) proposed a knowledge-grounded approach that leverages external commonsense knowledge during fine-tuning to enrich explanations.Wang et al. (2023a) introduced a two-stage approach using counterfactual regularization to align predictions with generated explanations.Architectural modifications have shown promise in the state-of-the-art G-Tex framework (Yuan et al., 2025), which encodes highlight explanations via a graph neural network to guide NLE generation.Our SR-NLE framework distin-guishes itself from existing work by enabling models to improve explanation faithfulness through iterative self-critique and refinement -without external supervision, architectural modifications, or taskspecific training -entirely based on the model's own internal knowledge.</p>
<p>Self-Refinement Methods Recent work has shown that LLMs can improve their own outputs via iterative self-refinement.The Self-Refine approach (Madaan et al., 2023) demonstrates that models can critique and revise their own outputs, leading to improved performance across a variety of tasks.Similarly, Shinn et al. ( 2023) explore selfreflection mechanisms for agent-level reasoning.</p>
<p>While these works establish the general potential of self-improvement, they do not specifically address the challenge of improving explanation faithfulness.</p>
<p>Building on the general idea of self-refinement, Cross-Refine (Wang et al., 2024) applies this paradigm to NLE generation.Their framework adopts a cross-model design, where one LLM generates the initial explanation and another, separate LLM provides feedback and suggestions for revision.In contrast, SR-NLE operates entirely within a single model, leveraging its internal capabilities for both critique and refinement.Furthermore, SR-NLE focuses specifically on improving faithfulness using automated counterfactual tests for objective evaluation.In comparison, Cross-Refine primarily evaluates explanation quality through multiple automated metrics, while relying on human judgments for faithfulness assessment.</p>
<p>Input Feature Attribution Methods Input feature attribution methods quantify how much each input feature contributes to a model's prediction.Common approaches include Shapley values (Lundberg and Lee, 2017), integrated gradients (Sundararajan et al., 2017), and attention weights (Jain and Wallace, 2019).A newer paradigm leverages prompt-based approaches (Kroeger et al., 2023), where LLMs are prompted to directly identify influential input features.The most prevalent application of these methods is to provide post-hoc explanations, helping humans understand which parts of the input most strongly influence the model's decision-making.Beyond interpretability, these methods have also been used to construct rationales for in-context exemplars in few-shot learning to improve task accuracy.AMPLIFY (Krishna et al., 2023) trains a proxy model and applies attribution methods to extract important words, which are then converted into rationales for few-shot exemplars.Self-AMPLIFY (Bhan et al., 2024) extends this idea by removing the proxy and computing attributions directly from LMs to obtain important words, which are likewise used as rationales for exemplar construction.In the context of NLEs, G-Tex (Yuan et al., 2025) leverages attribution-derived highlights to guide the generation of NLEs through graph encoding.In this work, we propose a novel use of attribution methods to obtain feedback for improving the faithfulness of LLM-generated NLEs.Similar to SELF-AMPLIFY (Bhan et al., 2024), we apply attribution methods directly to LLMs to extract important words and use them to construct feedback to guide the model in generating more faithful NLEs.</p>
<p>Method</p>
<p>In this section, we present SR-NLE, a framework for improving the faithfulness of NLEs generated by LLMs.SR-NLE employs an iterative self-critique and refinement process, enabling LLMs to progressively identify faithfulness issues in their own NLEs and make targeted improvements thereof.This framework leverages the incontext learning and self-improvement capabilities of LLMs, without requiring human involvement or additional models for feedback.</p>
<p>Preliminary</p>
<p>The SR-NLE framework operates on the assumption that LLMs have the capability to identify and improve their own explanations when guided with appropriate prompts.Our framework relies entirely on a single model M for all components, without requiring human involvement or additional models.For an input x, the model first predicts an answer y and produces an initial explanation e 0 , then, through an iterative process of self-critique and refinement, after each round r, we obtain a progressively improved explanation e r .To direct the model in different stages of the framework, we employ four categories of prompts: p ans for answer generation, p exp for explanation generation, p fb for feedback generation, and p ref for refinement generation, where both feedback and refinement prompts have two variants corresponding to our two feedback approaches: natural language feedback (NLF) and important word feedback (IWF).</p>
<p>Throughout the framework, we use "⊕" to denote filling a prompt template with its variables.</p>
<p>SR-NLE Framework</p>
<p>Our SR-NLE framework consists of two main phases: (a) Answer and Explanation Generation, which produces the answer and its initial explanation, and (b) Iterative Critique and Refinement, which progressively improves the explanation through multiple rounds.Figure 2 illustrates this two-phase process.The algorithmic formulation is provided as Algorithm 1.</p>
<p>Answer and Explanation Generation</p>
<p>This phase produces the answer and its initial explanation (see Figure 2a):</p>
<p>Answer.Given an input x, the model first generates an answer:
y = M p ans ⊕ x (1)
Explanation.Using the answer y, the model generates an initial explanation:
e 0 = M p exp ⊕ x ⊕ y (2)
This initial explanation serves as the starting point for our iterative refinement process.</p>
<p>Iterative Critique and Refinement</p>
<p>This phase forms the core of our framework, where explanations are iteratively improved for K rounds (see Figure 2b):</p>
<p>Feedback.For each refinement round r, the model generates feedback on the preceding explanation e r−1 .We explore two distinct feedback approaches:</p>
<p>• Natural Language Feedback (NLF).With this approach, M generates detailed textual selfcritique for each round r:
f r nl = M p fb ⊕ x ⊕ y ⊕ e r−1(3)
• Important Word Feedback (IWF).This novel feedback approach leverages attribution explanations, which mark specific input tokens (DeYoung et al., 2020) or segments (Ray Choudhury et al., 2023) critical to a model's prediction.While these explanations may lack the plausibility of NLEs (Jie et al., 2024), their faithfulness is straightforward to measure and has seen significant improvements (Sun et al., 2025;Atanasova et al., 2020a).We hypothesise that such explanations can enhance NLE faithfulness by providing explicit feedback about which input elements should be emphasized in the generated explanation.Our approach identifies words in the input that are most important for the answer:
S = SCORE x, y I = SELECT S, N f iw = FORMAT I(4)
Here, we employ a method SCORE to provide a list S of the words in input x with their importance scores for answer y.From these scored words, we select the top-N most important ones -I, to form the feedback.We implement two SCORE methods:</p>
<p>-Prompt-based: Following Kroeger et al. (2023), who find that LLMs can be used with high accuracy as post-hoc explainers, we prompt the model itself to assign importance scores to input words (IWF-Pmt):
SCORE = M(p fb ⊕ x ⊕ y)(5)
-Attribution-based: Following Bhan et al.</p>
<p>(2024), we use input feature attribution methods to quantify and assign importance scores to words (IWF-Attr).We detail our method for computing the IWF-Attr SCORE in Section 3.3.</p>
<p>Refinement.Using the feedback, the model refines its explanation:
e r = M p ref ⊕ x ⊕ y ⊕ e r−1 ⊕ f ⋆ (6)
where f ⋆ is either f r nl or f iw depending on the feedback type.This process of feedback generation and refinement repeats for K rounds, with each round potentially addressing different sources of unfaithfulness in the explanation.After the final round, we obtain e K as our final NLE.</p>
<p>Attribution-Based IWF SCORE</p>
<p>While prompt-based IWF directly prompts the model to assign importance scores to input words, attribution-based IWF computes these scores using feature attribution methods.Our approach for computing the attribution-based IWF SCORE is illustrated in Figure 3 and detailed in Algorithm 2, consisting of the following steps:</p>
<p>Target Span Identification First, we identify the answer span within the model output.Given a task input x and answer generation prompt template p ans , we construct the full model input p ans ⊕ x.After running the model on this combined input, we locate the answer span y within the model output.</p>
<p>Sequential Token Attribution For each token y j in the answer span, we compute attribution scores considering the entire context available at generation time.This includes all tokens in the full model input, as well as all previously generated tokens.</p>
<p>Token-level Computation We quantify how each token in the full model input contributes to generating each token in the model output:
a i,j = |Attribution(x i , y j |context &lt;j )| (7)
where a i,j represents the attribution score of token x i from the full model input (prompt + task input) for the prediction of output token y j given all preceding context.We apply the absolute value function for two key reasons: (1) to focus on the magnitude of influence rather than its direction, as both strong positive and negative influences indicate important tokens; and (2) to prevent positive and negative attributions from cancelling each other out during aggregation steps.</p>
<p>Target-level Aggregation We aggregate the token-level attributions across the answer span for each input token:
a i = |y| j=1 a i,j(8)
where we sum (rather than average) the attribution scores to capture the total influence of each input token.</p>
<p>Word-level Aggregation</p>
<p>To obtain word-level importance, we map token attributions back to the original words in the task input (excluding prompt tokens).For words split into multiple tokens during tokenization, we combine their attribution scores:
score(w) = i∈indices(w) a i(9)
where indices(w) represents the indices of all tokens corresponding to word w in the task input.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We conducted our experiments on three widely used natural language reasoning datasets with NLEs: ComVE (Wang et al., 2020), ECQA (Aggarwal et al., 2021), and e-SNLI (Camburu et al., 2018).The task of ComVE is to identify which of the two sentences violates common sense.The task of ECQA is to answer multiple-choice questions requiring common sense reasoning.The task of e-SNLI is to determine the logical relationship (contradiction, neutral or entailment) between the premise and hypothesis.We selected 1,000 instances from each dataset for our experiments due to computational constraints.Details about dataset selection and characteristics are provided in Appendix A.</p>
<p>Models</p>
<p>We utilized four state-of-the-art open-source models for our experiments: Llama (Grattafiori et al., 2024), Mistral (Jiang et al., 2023), Qwen (Qwen et al., 2025), andFalcon (Almazrouei et al., 2023).</p>
<p>For each model, we selected its instruction-tuned version, as our framework primarily operates in a zero-shot setting, which relies heavily on the model's ability to follow instructions effectively.Additionally, we limited our selection to models with sizes under 10B parameters to balance performance and computational efficiency.Detailed model specifications are provided in Appendix B.</p>
<p>Evaluation</p>
<p>To evaluate the faithfulness of the model-generated NLEs, we employ the counterfactual test proposed by Atanasova et al. (2023).The counterfactual test works by making an intervention to the original instance to get an intervened instance.The evaluation then consists of two steps: (1) Identify counter instances: intervened instances whose prediction changes compared to the original instance.</p>
<p>(2) Identify unfaithful instances: counter instances whose NLEs (generated by baseline methods or SR-NLE) do not contain the intervened word (determined by string matching).The unfaithfulness rate is calculated as:
Unfaithfulness = N unfaithful/N counter (10)
This metric allows us to directly compare the faithfulness of NLEs generated by different methods, with lower rates of unfaithfulness being more desirable.We apply this metric consistently across all baseline methods and SR-NLE variants to ensure fair comparison.</p>
<p>Intervention Generation.In our implementation, we adopt the random approach from Atanasova et al. (2023).Specifically, we randomly select a noun or a verb from any position in the input.For nouns, we prepend a random adjective, and for verbs, we prepend a random adverb.Different from Atanasova et al. (2023), we further employ prompting GPT-4o (OpenAI et al., 2024), to ensure the generation of multiple effective, coherent, and meaningful interventions for the same instance without duplications.We generate 20 unique interventions for each original instance from each dataset.The detailed intervention generation prompt and quality checks are described in Appendix C.</p>
<p>Baselines</p>
<p>We compare our SR-NLE framework against two baselines suggested by us, as well as an existing prior method:</p>
<p>Init-NLE.The initial NLEs were generated by the model without any refinement process.This corresponds to e 0 in our framework and represents the typical approach used in most NLE generation scenarios.</p>
<p>SC-NLE.NLEs generated using the Self-Consistency method (Wang et al., 2023b), where we sample multiple explanations with temperature sampling and select the most representative explanation using the semantic centroid voting (Algorithm 3).This approach encodes all candidates using SentenceBERT (Reimers and Gurevych, 2019), computes their centroid in the embedding space, and selects the explanation with the highest cosine similarity to this centroid.This effectively identifies the explanation that best represents the consensus meaning across all samples.This baseline represents a strong ensemble-based alternative that does not require iterative refinement.The specific configuration of sampling parameters is discussed in Section 4.5.</p>
<p>Comparisons to Prior Work.We also compare our SR-NLE with G-TEX (Yuan et al., 2025), a recent state-of-the-art method that also aims to improve explanation faithfulness.While we do not implement their approach, we report their results from the original paper for reference.</p>
<p>Experimental Setups</p>
<p>Implementation Details We use greedy decoding throughout our pipeline and experiment with up to K = 3 refinement rounds.For attributionbased IWF, we compare two attribution methods:</p>
<p>(1) gradient-based attribution using Integrated Gradients (IWF-IG; Sundararajan et al. ( 2017)), identified as the most faithful post-hoc explanations (Atanasova et al., 2020a), and (2) attention-based attribution (IWF-Attn) leveraging the model's attention mechanisms.A more detailed description of these attribution methods is provided in Appendix D. For all important word feedback variants, we use the top-5 important words as feedback.</p>
<p>For the SC-NLE baseline, we sample 20 candidate explanations with temperature 1.0 and select the most representative one using semantic centroid voting, as described in Section 4.4.Detailed ablation studies on various parameters are provided in Appendix E.</p>
<p>Prompts.Our entire pipeline operates in a zeroshot setting, with stage-specific instructions designed for each dataset.Complete prompt templates are provided in Appendix I.</p>
<p>Results</p>
<p>Main Results</p>
<p>Table 1 presents our comprehensive evaluation results after 3 refinement rounds.Additional results from intermediate refinement rounds, complementary metrics, additional analysis, and detailed visualizations are provided in Appendix F.</p>
<p>SR-NLE outperforms baselines</p>
<p>The SR-NLE framework shows superior performance over baseline methods in most experimental settings.Our best implementation (IWF-Attn) reduces unfaithfulness rates by an average of 18.79% compared to Init-NLE and by an average of 13.72% compared to SC-NLE.Even our least effective method (NLF), despite underperforming in isolated cases (e.g., e-SNLI with Falcon), still achieves an average reduction of 7.23% compared to Init-NLE and 2.16% compared to the SC-NLE baseline, demonstrating the overall effectiveness of our framework.</p>
<p>IWF outperforms NLF All three IWF implementations consistently outperform NLF across all experimental settings.On average, IWF-Attn, IWF-IG, and IWF-Pmt achieve 11.56%, 11.28%, and 10.35% lower unfaithfulness rates than NLF, respectively.This performance gap demonstrates that explicit important word feedback provides more effective guidance for refinement than natural language feedback.lection strategies, with prompt-based methods offering practical advantages in terms of efficiency, accessibility, and reliability (3.75% hallucination rate; see Appendix F.5).To better understand this robustness, we also conducted additional experiments exploring the impact of word selection quality on IWF performance (detailed in Appendix F.6).</p>
<p>Comparable performance across IWF variants</p>
<p>Comparison with Prior Work.</p>
<p>While the results in Table 2 suggest that SR-NLE substantially outperforms the state-of-the-art G-TEX in terms of explanation faithfulness, this comparison is not fully controlled.The reported numbers correspond to each method's best-performing configuration on each dataset, and differences in counterfactual generation strategies or data splits may influence the outcomes.Nevertheless, the results provide a useful reference point, demonstrating the potential of SR-NLE as a lightweight and effective alternative for improving explanation faithfulness, compared to G-TEX, which requires architectural changes and additional fine-tuning.</p>
<p>Detailed Analysis</p>
<p>In this section, we conduct further in-depth analyses to better understand the effectiveness of our SR-NLE framework.methods.We can observe two consistent trends: First, unfaithfulness rates continuously decrease with additional refinement rounds, demonstrating the effectiveness of our method.Second, the most substantial reduction occurs during the first refinement round (e 0 to e 1 ), with the rate of reduction slowing down in subsequent rounds.These indicate that two or three refinement rounds may offer an optimal trade-off between performance and computational efficiency in practical applications.Therefore, we limited our experiments to a maximum of three refinement rounds.</p>
<p>Relationship Between Explanation Length and Unfaithfulness Rate Figure 6 reveals a clear pattern: as explanation length increases through successive refinement rounds, unfaithfulness rates consistently decrease for all feedback methods, indicating a generally inverse relationship between length and unfaithfulness.Initial explanations are the shortest and exhibit the highest unfaithfulness rates, while refined explanations become longer with decreasing unfaithfulness in each refinement round.However, the efficiency of this length-unfaithfulness relationship varies significantly between feedback types.NLF produces substantially longer explanations but achieves relatively modest reductions in unfaithfulness.In contrast, IWF methods reach significantly lower unfaithfulness rates with more moderate length increases.This indicates that although increased length generally reduces unfaithfulness, the focus on important words in the refinement process leads to more efficient reductions.The superior performance of IWF methods demonstrates the effectiveness of our approach in guiding models to address the most relevant aspects of explanations rather than broadly expanding content.</p>
<p>Conclusion</p>
<p>In this work, we presented SR-NLE, a framework for improving the faithfulness of NLEs through an iterative self-critique and refinement process.</p>
<p>By enabling LLMs to iteratively refine their own explanations with self-feedback, our approach significantly reduces unfaithfulness rates across multiple datasets and models without requiring external supervision, additional training or architectural changes.Our experiments demonstrate that IWF consistently outperforms NLF, with attention-based methods achieving the best results.The detailed analysis reveals that our framework efficiently targets critical reasoning components, successfully converts unfaithful explanations to faithful ones, and optimizes explanation content rather than merely increasing length.These findings suggest that self-refinement offers a promising path toward more faithful explanation generation.Future work could explore additional feedback mechanisms and investigate the applicability of SR-NLE to a broader set of domains and diverse reasoning tasks.</p>
<p>Limitations</p>
<p>While our SR-NLE framework shows promising improvements in explanation faithfulness, it has several limitations.</p>
<p>Explanation Paradigm</p>
<p>Our experiments focus only on post-hoc natural language explanations, where explanations are generated after prediction.It remains unclear whether our refinement process generalizes to other explanation paradigms, such as jointly generated rationales or chain-of-thought reasoning.Different explanation generation strategies might present unique challenges and opportunities for refinement that are not addressed in our current framework.</p>
<p>Evaluation Method We rely on counterfactual tests as the sole evaluation method for measuring explanation faithfulness.While this metric offers objective signals aligned with our goal, it reflects only one type of faithfulness criterion.Future work could explore additional automatic tests-such as consistency tests and simulatability tests-to provide a more comprehensive view of explanation faithfulness.Moreover, our evaluation approach does not capture other important aspects of explanation quality, such as plausibility, completeness, or alignment with human-annotated references.</p>
<p>Attribution Method The effectiveness of our attribution-based IWF methods depends on the reliability of the underlying attribution techniques.Attention weights may not consistently reflect true feature importance, while integrated gradients can be sensitive to baseline choices and implementation details.In practice, applying integrated gradients to large language models often requires a substantial number of integration steps to achieve convergence, which increases computational cost and may limit scalability.As a result, the quality and efficiency of the feedback depend on how accurately these methods capture the model's actual reasoning process.</p>
<p>B Models</p>
<p>Model Specifications.Table 5 presents the specific versions and parameter sizes of the instructiontuned models used in our experiments.All models were accessed through their Hugging Face1 implementations.</p>
<p>Model Version Size</p>
<p>Falcon
Falcon3-Instruct 7B Llama Llama3.1-Instruct 8B Mistral Mistral-Instruct-v0.3 7B Qwen
Qwen2.5-Instruct 7B SentenceBERT Model.We use the Sentence-BERT model all-mpnet-base-v2 as the semantic encoder for the centroid voting method in the SC-NLE baseline.This model was selected based on its strong performance in various semantic similarity tasks.</p>
<p>C Evaluation</p>
<p>This section details our process for generating interventions for the counterfactual test used in our faithfulness evaluation.</p>
<p>Prompting Strategy.We used GPT-4o (OpenAI et al., 2024) 6.</p>
<p>Intervention Quality Analysis.To verify the quality of our generated interventions, we manually examined a random sample of 50 instances from each dataset (150 total).Our analysis confirmed that the intervened instances remained meaningful and coherent, with exactly one word modified as intended.These quality checks ensured that our interventions were suitable for faithfulness evaluation, as they created meaningful variations that could potentially change model predictions.</p>
<p>D Attribution Methods</p>
<p>Integrated Gradients We implement Integrated Gradients (IG) following Sundararajan et al. (2017) to compute token importance.Since all our models are generative language models, we use the end-ofsequence (EOS) token embedding as the baseline, as it serves as a neutral and consistently defined default signal.</p>
<p>Task:</p>
<p>You will be given a sentence.Your task is to edit the sentence by inserting a random adjective before a noun or a random adverb before a verb.The noun or verb must be selected randomly from the given sentence.</p>
<p>Requirements:</p>
<p>-Generate 10 different edits.</p>
<p>-Each edit should modify only one word.</p>
<p>-Enclose only the modified word in square brackets [ ].</p>
<p>-Ensure that the sentence remains grammatically correct and natural.</p>
<p>Output format:</p>
<p>1 Attention We leverage the model's self-attention mechanism to measure token importance.Specifically, we extract attention weights from the final layer of the model and average them across all attention heads.For each target token, these weights indicate how much the model attended to each input token when generating that token.</p>
<p>E Ablation Studies</p>
<p>All ablation studies are conducted on the same 100instance subsets sampled from each dataset.For the IG integration steps (Section E.1), we directly use these 100 original instances.For the other two ablation studies (Section E.2 and E.3), we follow the same procedure as in the main experiments to compute unfaithfulness: for each of the 100 instances, we generate 20 interventions (totalling 2000 intervened instances), and then select and perform experiments on the counter instances.It's important to note that we conducted these ablation studies across all dataset and model combinations.Therefore, all metrics reported in these sections represent averaged values across the entire experimental matrix, providing a comprehensive view of our method's performance across different conditions.</p>
<p>E.1 Integration Steps for IG Attribution</p>
<p>We investigated the impact of integration steps on the convergence of IG attribution calculations by experimenting with nine different step settings ranging from 100 to 1500. Figure 7 shows that most models (Falcon, Llama, and Mistral) reach reasonable convergence around 500 integration steps, with the convergence delta showing minimal changes beyond this point.In contrast, the Qwen model exhibits higher variability and slower convergence, requiring more steps to achieve stable attribution values.This may be caused by architectural differences that affect how gradients are calculated and propagated through the model.Based on these observations, we selected 500 integration steps for Falcon, Llama, and Mistral, while using 1000 steps for Qwen in our main experiments.</p>
<p>E.2 SC-NLE Parameters</p>
<p>We investigated the impact of two key parameters for our SC-NLE baseline: candidate explanation count and sampling temperature.Figure 8 shows that increasing the number of candidates reduces unfaithfulness, with significant improvements up to 20 samples.Temperature also affects performance, with temperature 1.0 consistently outperforming lower values, especially at higher sample counts.</p>
<p>Based on these results, we selected 20 candidate explanations at temperature 1.0, balancing performance improvements with computational cost.</p>
<p>E.3 Number of Important Words</p>
<p>We investigated the optimal number of important words (top-N ) for our Important Word Feedback through two complementary analyses: examining unfaithfulness changes and analyzing attribution distribution patterns.Figure (9a) shows that unfaithfulness decreases as N increases from 1 to 9 across all feedback types, with the most significant improvements occurring between N = 1 and</p>
<p>F Additional Results</p>
<p>F.1 Prediction Accuracy</p>
<p>Table 7 shows the prediction accuracy for each model across the three datasets.While accuracy is not directly related to our evaluation focus, we can observe that models demonstrate strong performance.Across different datasets, Falcon and Qwen models generally achieve higher accuracy than Llama and Mistral models.Notably, on the ComVE dataset, all models achieve accuracy rates above 90%.</p>
<p>F.2 Counter Rates</p>
<p>Table 8 shows the number of counter instances and counter rates for each model-dataset combination out of 20,000 total intervened instances.Each model achieves counter rates of 10-15% on the ECQA and e-SNLI datasets, while on ComVE, the rates are generally below 10%, with most under 5%.</p>
<p>Average Sequence Lengths</p>
<p>Table 9 reports the average sequence lengths of counter instances for each model-dataset combination.We provide results under two settings: Full, which denotes the total word length of the input, and Unique, which denotes the word length after removing duplicate words.The lengths range from about 13 to 21 words under the Full setting, while the Unique setting is consistently shorter, around 9 to 16 words.</p>
<p>F.4 Intermediate Results</p>
<p>The unfaithfulness rates after refinement round 1 and refinement round 2 are shown in Table 10.We can observe a clear downward trend in unfaithfulness rates across successive refinement rounds, demonstrating the progressive effectiveness of our iterative approach.</p>
<p>F.5 Reliability of Prompt-based Important Words Selection Strategy</p>
<p>To assess the reliability of prompt-based important words selection, we examined whether the important words identified by IWF-Pmt are grounded in the input.Specifically, we measured the hallucination rate, defined as the proportion of top-5 selected words that do not appear in the input.Table 11 presents the results across all dataset-model combinations.While we observe some variation-with ECQA showing slightly higher rates and Qwen exhibiting more hallucination compared to other models-the overall average hallucination rate across the 12 combinations is only 3.75%.This low rate demonstrates that prompt-based word selection is highly reliable, with the selected words being wellgrounded in the input.</p>
<p>F.6 Word Selection Quality Analysis</p>
<p>To assess the impact of word selection quality on IWF performance, we conducted experiments comparing different selection methods against a random baseline.Specifically, we randomly selected five words from the input and applied three rounds of refinement on the e-SNLI dataset across all four models, averaging results over three ran- dom seeds.As shown in Table 12, while the random baseline shows slightly higher unfaithfulness rates, it achieves performance close to both prompt-based and attribution-based IWF methods.</p>
<p>To understand this result, we analyzed how often the intervened word (i.e., the true reasoning factor that made the label change) appeared in the top-N selected words.Table 13 shows that current selection methods-whether prompt-based or attribution-based-capture the true reasoning word at rates similar to random selection.These findings reveal two important insights:</p>
<ol>
<li>
<p>Robustness of IWF: Even with suboptimal word selection, IWF can effectively improve explanation faithfulness, demonstrating that the iterative refinement process in SR-NLE contributes significantly to performance improvements beyond the quality of word attribution.</p>
</li>
<li>
<p>Opportunities for improvement: Current word attribution methods have considerable room for enhancement.As better attribution techniques are developed, IWF could potentially achieve even stronger performance within our SR-NLE framework.</p>
</li>
</ol>
<p>This analysis underscores that IWF's effectiveness stems from the iterative refinement process in our SR-NLE framework rather than perfect word identification, making IWF a robust feedback mechanism that can benefit from future advances in attribution methods.</p>
<p>F.7 Performance Visualization</p>
<p>Figure 10 presents radar chart visualizations of unfaithfulness rates after 3 rounds of refinement.</p>
<p>In each chart, the four axes represent different models, while the connected areas represent different methods.The visualizations clearly show that our SR-NLE methods achieve smaller areas than the baselines in the majority of cases, aligning with our quantitative results and demonstrating the effectiveness of the framework across different datasets and models.</p>
<p>G Algorithms</p>
<p>Algorithms 1-3 present the SR-NLE framework, attribution-based IWF SCORE, and semantic centroid voting strategy for SC-NLE, respectively.</p>
<p>H Additional Case Studies</p>
<p>Tables 14 to 16 present comprehensive case studies comparing NLF and IWF-Attn (our bestperforming variant) refinement processes across three datasets.Table 15 provides the complete three-round refinement details for the ECQA example discussed in Section 5.3, while Tables 14 and  16 show representative examples from ComVE and e-SNLI datasets, respectively.</p>
<p>I Prompts</p>
<p>We present the full prompt templates used in our experiments.Each prompt is composed of two parts: a task-specific prompt part and a common instruction part, which are concatenated to form the final prompt at each stage of the SR-NLE framework.</p>
<p>These stages include answer generation, explanation generation, feedback generation, and refinement generation.For feedback and refinement, we provide two variants based on natural language feedback and important word feedback.Tables 17 to 22 list the complete prompts for each stage, including the task-specific prompt part for all three datasets and the shared common instruction part.
f iw ← FORMAT(I) 10: for r = 1 to K do ▷ Iterative refinement 11: if t = NLF then 12: f r ← M(p fb ⊕ x ⊕ y ⊕ e r−1 )
13: else 14: Initial Explanation: Sentence 0 violates common sense because leaves do not help plants absorb nutrition; rather, it is the roots that absorb nutrients from the soil.Unfaithful Natural Language Feedback (NLF) Important-Word Feedback (IWF-Attn)
f r ← f iw 15: e r ← M(p ref ⊕ x ⊕ y ⊕ e r−1 ⊕ f r
Feedback 1: The explanation accurately reflects the reasoning behind selecting Sentence 0 as the one that violates common sense.It correctly identifies that leaves do not help plants absorb nutrition, but rather it is the roots.However, to improve the explanation, it could briefly mention that while leaves do not absorb nutrients, they play a crucial role in photosynthesis, which indirectly supports the plant's overall nutrition.Overall, the explanation is clear and concise.</p>
<p>Feedback 1: The 5 most important words that contributed to your prediction are: leafs, the, useless, fallen, are.</p>
<p>Refinement 1: Sentence 0 violates common sense because leaves do not help plants absorb nutrition; instead, it is the roots that take up nutrients from the soil.Leaves, however, play a vital role in photosynthesis, which is essential for the plant's overall nutrition and growth.Unfaithful</p>
<p>Refinement 1: Leaves do not assist in the absorption of nutrition; instead, it is the roots that play this crucial role.</p>
<p>Fallen leaves, however, can become useless as they decompose and return nutrients to the soil.Faithful Feedback 2: The explanation accurately reflects the reasoning behind selecting Sentence 0 as the one that violates common sense.It correctly identifies that leaves do not help plants absorb nutrition, but rather it is the roots.However, to improve the explanation, it could briefly mention that while leaves are not directly involved in nutrient absorption, they are crucial for photosynthesis, which indirectly supports the plant's nutrition.Overall, the explanation is clear and concise.</p>
<p>Feedback 2: The 5 most important words that contributed to your prediction are: leafs, the, useless, fallen, are.</p>
<p>Refinement 2: Sentence 0 violates common sense because leaves do not help plants absorb nutrition; instead, it is the roots that take up nutrients from the soil.While leaves are not directly involved in nutrient absorption, they are essential for photosynthesis, which indirectly supports the plant's nutrition and overall growth.Unfaithful Refinement 2: Leaves do not assist in the absorption of nutrition; instead, it is the roots that play this crucial role.</p>
<p>Fallen leaves, however, can become useless as they decompose and return nutrients to the soil.Faithful Feedback 3: The explanation accurately reflects the reasoning behind selecting Sentence 0 as the one that violates common sense.It correctly identifies that leaves do not help plants absorb nutrition, and instead highlights the role of roots in nutrient absorption.However, to improve the explanation, it could briefly mention that while leaves are not directly involved in nutrient absorption, they play a crucial role in the overall health of the plant by facilitating photosynthesis, which indirectly supports the plant's nutrition.This addition would provide a more comprehensive understanding of the plant's nutritional processes.</p>
<p>Feedback 3: The 5 most important words that contributed to your prediction are: leafs, the, useless, fallen, are.</p>
<p>Refinement 3: Sentence 0 violates common sense because leaves do not help plants absorb nutrition; rather, it is the roots that take up nutrients from the soil.Leaves, however, are vital for photosynthesis, which indirectly supports the plant's nutrition and overall growth.Unfaithful Refinement 3: Leaves do not assist in the absorption of nutrition; instead, it is the roots that play this crucial role.</p>
<p>Fallen leaves, however, can become useless as they decompose and return nutrients to the soil.Faithful Feedback 1: The explanation accurately reflects the reasoning behind selecting option (A) as it highlights the motel-like amenities present in Bill's room.However, it could be improved by explicitly mentioning that the combination of a bed, fridge, stove, couch, and television is more characteristic of a motel or hotel room than other options like a school or apartment.This would make the reasoning clearer and more convincing.No other significant improvements are needed.</p>
<p>Feedback 1: The 5 most important words that contributed to your prediction are: one, a, cozy, be, there.</p>
<p>Refinement 1: Bill's room contains a bed, fridge, stove, couch, and television, which are typical amenities found in a motel or hotel room, not in a school or apartment setting.This combination of features makes option (A) Motel the most appropriate choice.Unfaithful</p>
<p>Refinement 1: Bill's room, described as cozy with amenities like a bed, fridge, stove, couch, and television, suggests a motel setting, as these features are commonly found together in such accommodations.Faithful Feedback 2: The explanation accurately reflects the reasoning behind selecting option (A) Motel.It correctly identifies the presence of typical motel amenities and contrasts them with settings like schools and apartments.However, to enhance clarity, it could be beneficial to explicitly mention that motels are often the type of accommodation where travelers might find a combination of a bed, fridge, stove, couch, and television in a single room.This would further emphasize why option (A) is the most suitable choice.Overall, the explanation is mostly clear but could be slightly refined for better understanding.</p>
<p>Feedback 2: The 5 most important words that contributed to your prediction are: one, a, cozy, be, there.</p>
<p>Refinement 2: Bill's room contains a bed, fridge, stove, couch, and television, which are typical amenities found in a motel or hotel room, not in a school or apartment setting.The presence of a bed, fridge, and stove suggests it's a place for temporary accommodation, and the inclusion of a couch and television indicates it's a comfortable space for relaxation.This combination of features makes option (A) Motel the most appropriate choice, as motels often provide such a suite of amenities in a single room for travelers.Unfaithful Refinement 2: Bill's cozy room, equipped with a bed, fridge, stove, couch, and television, indicates a motel setting, where such amenities are typically found together.Faithful Feedback 3: The explanation accurately reflects the reasoning behind selecting option (A) Motel.It correctly identifies the amenities as typical of a motel room and contrasts them with other options.However, it could be improved by explicitly mentioning that the presence of a bed, fridge, and stove suggests a living space, which is more characteristic of a motel room than a school or apartment.No improvement is needed overall.</p>
<p>Feedback 3: The 5 most important words that contributed to your prediction are: one, a, cozy, be, there.</p>
<p>Refinement 3: Bill's room, equipped with a bed, fridge, stove, couch, and television, is indicative of a motel or hotel room, where such amenities are common for temporary accommodation and comfort.The presence of a bed, fridge, and stove suggests a living space, which aligns more with a motel setting than a school or apartment.This combination of features makes option (A) Motel the most appropriate choice.Unfaithful Refinement 3: Bill's cozy room, complete with a bed, fridge, stove, couch, and television, suggests a motel setting, where such a combination of amenities is commonly found.Faithful Feedback 1: The explanation accurately reflects the reasoning process and correctly identifies that the premise does not provide information about the motorcycle's power.However, it could be slightly more explicit by mentioning that the premise also does not confirm the presence of a man specifically.To improve clarity, you might add: "The premise does not specify the power of the motorcycle or confirm the presence of a man, so the hypothesis introduces new information that cannot be confirmed."</p>
<p>Feedback 1: The 5 most important words that contributed to your prediction are: a, near, powerful, is, guy.</p>
<p>Refinement 1: The premise does not specify the power of the motorcycle or confirm the presence of a man, so the hypothesis introduces new information that cannot be confirmed.Unfaithful</p>
<p>Refinement 1: The premise does not specify the power of the motorcycle, so the hypothesis introduces new information about the motorcycle being powerful, which cannot be confirmed from the given premise.Faithful</p>
<p>Feedback 2: The explanation accurately reflects the reasoning process.It correctly identifies that the premise does not provide information about the power of the motorcycle or confirm the presence of a man.However, it could be slightly more detailed by explicitly stating that the hypothesis introduces assumptions not supported by the premise.This minor clarification would make the explanation clearer.No significant improvements are needed, but adding this detail would enhance its precision.</p>
<p>Feedback 2: The 5 most important words that contributed to your prediction are: a, near, powerful, is, guy.</p>
<p>Refinement 2: The premise does not specify the power of the motorcycle or confirm the presence of a man, so the hypothesis introduces assumptions not supported by the premise.Unfaithful Refinement 2: The premise does not specify the power of the motorcycle, so the hypothesis introduces new information about the motorcycle being powerful, which cannot be confirmed from the given premise.Faithful Feedback 3: The explanation accurately reflects the reasoning process.It correctly identifies that the premise does not provide information about the power of the motorcycle or confirm the presence of a man.However, it could be slightly more explicit about why the hypothesis is considered neutral rather than just providing a general statement.A minor revision could clarify this point.Revised Explanation: The premise does not specify the power of the motorcycle or confirm the presence of a man.Therefore, the hypothesis introduces assumptions not supported by the premise, making the relationship neutral.Overall, the explanation is good but can be made clearer with a slight revision.</p>
<p>Feedback 3: The 5 most important words that contributed to your prediction are: a, near, powerful, is, guy.</p>
<p>Refinement 3: The premise does not specify the power of the motorcycle or confirm the presence of a man.Therefore, the hypothesis introduces assumptions not supported by the premise, making the relationship neutral.Unfaithful Refinement 3: The premise does not specify the power of the motorcycle, so the hypothesis introduces new information about the motorcycle being powerful, which cannot be confirmed from the given premise.Faithful    Your output must meet the following requirements:</p>
<p>-Only include individual words in the input.</p>
<p>-Evaluate each word based on its total contribution across all occurrences in the input, but include each word only once in the output.</p>
<p>-Assign each word a score from 1 to 100 (positive integers only), based on its relative importance.</p>
<p>-Rank the words in descending order of importance (most important first).</p>
<p>-Do not include any explanations, comments, or parenthetical notes.</p>
<p>You must give your output only in the following format:</p>
<p>-Begin directly with the ranked list.</p>
<p>-Each line must be in the format: '<rank>.If the feedback indicates that no improvement is needed, you should repeat the original explanation as the refined explanation.Otherwise, please refine your explanation based on the feedback.</p>
<p>Your refined explanation should: -Be clear, complete, and concise.</p>
<p>-Ideally remain similar in length to the original explanation.</p>
<p>-Retain any correct parts of your original explanation.</p>
<p>-Address the issues identified in the feedback, if any.</p>
<p>You must give your refined explanation only in the following format: Refined Explanation: [your refined explanation here.]</p>
<p>ECQA</p>
<p>You are given a multiple-choice commonsense question, and you have selected the most appropriate answer.You then provided an explanation for your choice, and received feedback on the explanation.If the explanation already includes the important words in a natural and meaningful way, you should repeat the original explanation as the refined explanation.Otherwise, please refine your explanation based on the important words.</p>
<p>Your refined explanation should: -Be clear, complete, and concise.</p>
<p>-Ideally remain similar in length to the original explanation.</p>
<p>-Retain any correct parts of your original explanation.</p>
<p>-Integrate the important words naturally and fluently-do not list or quote them directly.</p>
<p>Provide your refined explanation only in the following format: Refined Explanation: [your refined explanation here.]</p>
<p>ECQA</p>
<p>You are given a multiple-choice commonsense question, and you have selected the most appropriate answer.You then provided an explanation for your choice, and received a list of important words that contributed significantly to your reasoning.</p>
<p>Question</p>
<p>Figure 1 :
1
Figure 1: Illustration of our framework SR-NLE improving the faithfulness of the initially generated NLE by providing self-critique of the most important words used in the prediction.</p>
<p>Figure 2 :
2
Figure 2: The SR-NLE framework.(a) Answer and Explanation Generation Phase: The framework produces the answer x and initial explanation e 0 .(b) Iterative Critique and Refinement Phase: The framework iteratively improves explanations through feedbackrefinement loops over multiple rounds.</p>
<p>Figure 7 :
7
Figure 7: Impact of number of integration steps on convergence of IG attribution methods.Convergence delta measures the approximation error in the numerical integration.Lower values indicate more stable and accurate attribution calculations.</p>
<p>Figure 8 :
8
Figure 8: Impact of sampling temperature and candidate count on SC-NLE unfaithfulness.</p>
<p>Cumulative Attribution Ratio vs. Top-N.</p>
<p>Figure 9 :
9
Figure 9: Analysis of top-N important words selection.(a) Shows how unfaithfulness decreases with increasing N across feedback types (lower is better).(b) Depicts the proportion of total attribution captured by top-N words across attribution methods (higher indicates greater coverage).</p>
<p>) 16 :
16
return y, e K Algorithm 2 Attribution-based IWF SCORE Require: Model M, prompt p ans , input x, answer y, attribution method m ∈ {IG, Attention} Ensure: Word importance scores S 1: Locate answer span y within model output 2: for each token y j in answer span do ▷ Sequential target token attribution 3: for each token x i in full model input do ▷ Token-level computation 4: a i,j ← |Attribution(x i , y j |context &lt;j )| 5: for each token x i in full model input do 6: a i ← |y| j=1 a i,j ▷ Target-level aggregation 7: for each word w in task input x do 8: S(w) ← i∈indices(w) a i ▷ Word-level aggregation 9: return S Algorithm 3 Semantic Centroid Voting for SC-NLE Require: Candidate explanations E = {e 1 , e 2 , ..., e n }, SentenceBERT model M Ensure: Most representative explanation e best 1: embeddings ← M.encode(E) ▷ Encode all explanations 2: centroid ← 1 n n i=1 embeddings i ▷ Compute centroid embedding 3: similarities ← [ ] 4: for i = 1 to n do 5: similarities i ← cosine_similarity(centroid, embeddings i ) ▷ Compute similarity to centroid 6: best_idx ← arg max(similarities) ▷ Select most similar to centroid 7: return best_idx Original Sentence 0: Leafs help plants absorb nutrition.Sentence 1: The leafs are useless.Answer options: (A) sentence 0 (B) sentence 1 Model prediction: (B) sentence 1 Intervened Sentence 0: Leafs help plants absorb nutrition.Sentence 1: The fallen leafs are useless.Answer options: (A) sentence 0 (B) sentence 1 Model prediction: (B) sentence 0</p>
<p>You are given a premise and a hypothesis, and you have selected the logical relationship between them.You then provided an explanation for your choice, and received feedback on the explanation.</p>
<p>Generation Attribution Word Level Aggregation Target Level Aggregation …… …… …… …… ……
𝒙 𝟏𝒙 𝟐𝒙 𝒏𝒚 𝟏𝒙 𝟏𝒙 𝟐𝒙 𝒏𝒚 𝟏𝒚 𝟐𝒙 𝟏𝒙 𝟐𝒙 𝒏𝒚 𝟏𝒚 𝟐𝒚 𝒎Figure 3: Illustration of attribution-based IWF SCORE.</p>
<p>Table 1 :
1
Falcon Llama Mistral Qwen Falcon Llama Mistral Qwen Falcon Llama Mistral Qwen Baseline Init-NLE 69.64 72.91 70.33 69.74 49.54 42.02 47.03 52.74 22.44 59.53 58.93 42.90 54.81 SC-NLE 63.27 71.78 63.93 68.42 44.69 39.25 44.17 51.21 19.24 43.99 47.98 38.91 49.74 Pmt 44.13 62.70 46.08 51.97 24.82 24.32 43.12 29.26 22.01 36.1637.80 24.43 37.23 IWF-Attn 46.43 60.37 44.39 50.66 27.03 24.32 42.03 26.28 18.21 34.94 38.4919.10 36.02IWF-IG 42.09 58.20 49.10 52.85 24.60 24.81 42.66 27.77 18.35 37.38 35.86 21.98 36.30Main results of SR-NLE framework reporting unfaithfulness rates (%) after three refinement rounds (R3).Best (lowest) results per dataset-model combination are bolded, second best are underlined.
MethodComVEECQAe-SNLIAvg.NLF60.71 63.67 64.29 58.99 43.77 37.76 44.72 46.79 23.01 47.04 44.18 36.04 47.58SR-NLEIWF-</p>
<p>Faithfulness state transitions from e 0 to e 3 for different feedback methods, averaged across 12 modeldataset combinations.The left group shows the proportion of initially faithful explanations that become unfaithful, while the right group shows the proportion of initially unfaithful explanations that become faithful.
Mean Percentage (%)0 5 10 15 20 25 30Faithful Unfaithful 3.92% 4.83% 4.58% 4.42% Feedback Type NLF IWF-Pmt IWF-Attn IWF-IGUnfaithful Faithful 11.25% 22.33% 23.25% 22.75%Mean Unfaithfulness (%)35 40 45 50 55303540 Average Explanation Length (Words) 45 50 556065 Feedback Type NLF IWF-Pmt IWF-Attn IWF-IG Round e 0 e 1 e 2 e 3 Round e 0 e 1 e 2 e 335 40 45 50 55 Figure 4: e 0 Mean Unfaithfulness (%)e 1Refinement Rounde 2e 3 Feedback Type NLF IWF-Pmt IWF-Attn IWF-IGFigure 6: Unfaithfulness rates versus explanation lengths across refinement rounds for feedback meth-ods, averaged across 12 model-dataset combinations.Figure 5: Unfaithfulness rates across successive refine-ment rounds for feedback methods, averaged across 12model-dataset combinations.Faithfulness State Transitions To understandthe refinement mechanism at a granular level, weanalyze how individual explanations transitionbetween faithful and unfaithful states. Figure 4presents the transition rates between two keystates: faithful→unfaithful (F→U) and unfaith-ful→faithful (U→F). For all feedback methods,positive transitions (U→F) substantially exceednegative transitions (F→U), with IWF methodsshowing a particularly favorable ratio. Thisindicates that our refinement process effectivelycorrects unfaithful explanations while rarelycompromising initially faithful ones. Among allmethods, IWF-Attn achieves the best balance ofhigh positive and low negative transition rates,which explains its lowest overall unfaithfulnessrates in our main results.
Refinement Efficiency Across Rounds Figure5illustrates unfaithfulness rates across successive refinement rounds (e 0 to e 3 ) for all feedback</p>
<p>Table 3 :
3
There was only one room in the place where Bill slept.It had a bed, a fridge, a stove, a couch, and a television.Where might he be?There was only one cozy room in the place where Bill slept.It had a bed, a fridge, a stove, a couch, and a television.Where might he be?Case study comparing NLF and IWF-Attn (our best-performing variant) on the ECQA dataset.The intervened word, highlighted in blue, successfully changes the model prediction.Faithful indicates the explanation/refinement is faithful as judged by the counterfactual test, while Unfaithful indicates the opposite.
Answer options: (A) motel (B) school (C) hotel (D) apartment (E) friend's houseModel prediction: (D) apartmentIntervenedQuestion: Answer options: (A) motel (B) school (C) hotel (D) apartment (E) friend's houseModel prediction: (A) motelInitial Explanation: Bill's room contains amenities typical of a motel, such as a bed, fridge, stove, couch, and television,which are not usually found together in a school setting. UnfaithfulNatural Language Feedback (NLF)Important-Word Feedback (IWF-Attn)Feedback 1: The explanation accurately reflects the reason-Feedback 1: The 5 most important words that contributeding behind selecting option (A) as it highlights the motel-liketo your prediction are: one, a, cozy, be, there.amenities present in Bill's room. However, it could be im-proved by explicitly mentioning that the combination of abed, fridge, stove, couch, and television is more characteris-tic of a motel or hotel room than other options like a schoolor apartment. This would make the reasoning clearer andmore convincing. No other significant improvements areneeded.
OriginalQuestion:Refinement 1: Bill's room contains a bed, fridge, stove, couch, and television, which are typical amenities found in a motel or hotel room, not in a school or apartment setting.This combination of features makes option (A) Motel the most appropriate choice.Unfaithful Refinement 1: Bill's room, described as cozy with amenities like a bed, fridge, stove, couch, and television, suggests a motel setting, as these features are commonly found together in such accommodations.Faithful ...... ......Refinement after 3 rounds: Bill's room, equipped with a bed, fridge, stove, couch, and television, is indicative of a motel or hotel room, where such amenities are common for temporary accommodation and comfort.The presence of a bed, fridge, and stove suggests a living space, which aligns more with a motel setting than a school or apartment.This combination of features makes option (A) Motel the most appropriate choice.UnfaithfulRefinement after 3 rounds: Bill's cozy room, complete with a bed, fridge, stove, couch, and television, suggests a motel setting, where such a combination of amenities is commonly found.Faithful</p>
<p>Table 4 :
4
For our experiments, we selected the first 1,000 instances from each dataset's test set.The full test set of ComVE contains 1,000 instances, while ECQA and e-SNLI have 2,194 and 9,824 instances, respectively.To verify the representativeness of these subsets, we analyzed their label distributions compared to the full test sets, as shown in Table4.The slight deviation from a 50/50 split in ComVE results from the random ordering process during dataset preparation, where we randomly positioned the sentence that violates common sense as either the first or second sentence.Label distribution comparison (%) between our experimental subsets and full test sets.
Model Scale and Architecture All of our ex-periments are conducted on LLMs in the 10B pa-rameter range. Further investigation is needed tounderstand how model scale affects both the base-line quality of explanations and the effectiveness ofself-refinement, especially for smaller open-weightmodels. The performance of SR-NLE might varysignificantly with larger, more advanced models ordifferent architectural designs.</p>
<p>Table 5 :
5
Details of the models used in our experiments.</p>
<p>Table 7 :
7
Model prediction accuracy (%).
Falcon Llama Mistral QwenComVE 96.7090.8094.5096.70ECQA77.1073.2068.3479.50e-SNLI89.6056.9058.1088.70</p>
<p>Table 8 :
8
Number of counter instances (top) and counter rates in % (bottom, in parentheses) for each modeldataset combination out of 20,000 total instances.</p>
<p>Table 9 :
9
Average sequence lengths of counter instances.Full refers to the total word length, while Unique refers to the word length after removing duplicate words.
Falcon Llama Mistral QwenFullComVE 15.7215.1314.9616.18ECQA12.9913.5313.4513.64e-SNLI21.0821.5520.7021.35UniqueComVE 10.169.359.6710.26ECQA12.2512.6612.5512.82e-SNLI15.6516.2815.7815.81</p>
<p>Table 10 :
10
Unfaithfulness rates (%) after refinement rounds 1 (R1) and refinement round 2 (R2).Best (lowest) results per dataset-model combination are bolded, second best are underlined.
MethodComVEECQAe-SNLIAvg.Falcon Llama Mistral Qwen Falcon Llama Mistral Qwen Falcon Llama Mistral QwenR1NLF65.05 67.60 67.91 63.60 46.03 39.83 45.90 49.72 22.65 51.96 48.42 38.98 50.64IWF-Pmt 51.28 63.67 51.63 56.58 22.60 24.94 40.60 32.70 18.88 38.51 39.54 27.17 39.01IWF-Attn 53.83 62.38 50.42 54.39 27.29 25.19 40.34 29.44 17.25 37.55 40.35 23.38 38.48IWF-IG48.98 61.17 54.16 57.46 24.64 25.35 40.47 30.56 17.89 39.69 39.18 26.03 38.80R2NLF61.99 64.95 65.38 59.65 44.38 38.50 45.69 47.86 22.97 48.87 45.92 36.72 48.57IWF-Pmt 46.17 62.70 46.56 53.95 23.95 24.32 42.03 29.77 20.66 36.34 38.45 24.75 37.47IWF-Attn 50.26 60.45 45.11 51.10 27.25 24.40 42.91 26.70 17.14 35.16 38.41 20.01 36.58IWF-IG43.88 58.68 49.58 54.82 24.69 25.06 42.32 28.23 18.28 37.60 36.51 22.47 36.84Falcon Llama Mistral QwenComVE0.010.010.010.05ECQA0.070.100.030.08e-SNLI0.010.010.020.05
Table11: Hallucination rates (%) of extracted important words in IWF-Pmt, measured as the proportion of top-5 words not appearing in the input.</p>
<p>Table 13 :
13
Intervened word inclusion rate (%) in the top-N selected words under the random baseline and IWF methods.
Falcon Llama Mistral QwenAvg.Random21.18 ±0.2635.32 ±0.7137.68 ±0.6322.91 ±0.6829.28 ±7.64IWF-Pmt 22.01 36.1637.8024.43 30.10IWF-Attn 18.21 34.9438.4919.10 27.69IWF-IG18.35 37.3835.8621.98 28.39Table 12: Comparison of random baseline against IWFmethods on e-SNLI dataset (unfaithfulness rates in %).Random results show mean ± standard deviation overthree seeds. All methods use three refinement rounds.Top-1 Top-2 Top-3 Top-4 Top-5Random6.5813.2820.1227.0334.00Pmt3.258.7014.8922.1730.51Attn4.5214.8425.5236.0746.25IG8.5817.1425.2232.7039.87</p>
<p>Model M, prompts {p ans , p exp , p fb , p ref }, input x, feedback type t ∈ {NLF, IWF}, IWF method m ∈ {prompt-based, attribution-based}, refinement rounds K, number of important words N Ensure: Final answer y and refined explanation e K 1: y ← M(p ans ⊕ x) 2: e 0 ← M(p exp ⊕ x ⊕ y) 3: if t = IWF then ▷ Prepare Important-Word Feedback following Eq. 4
Algorithm 1 SR-NLE Framework Require: 4: if m = prompt-based thenMistralLlama14.5829.1643.7558.33Falcon 72.91 Init-NLE SC-NLE NLF IWF-Pmt IWF-Attn IWF-IG5:S ← M(p fb ⊕ x ⊕ y)6:else7:S ← ATTRIBUTIONSCORE(M, p ans , x, y)▷ Algorithm 28:I ← SELECT(S, N )9:Qwen(a) ComVELlamaInit-NLE SC-NLE NLF IWF-Pmt IWF-Attn IWF-IG52.7442.1931.6421.1010.55MistralFalconQwen(b) ECQALlamaInit-NLE SC-NLE NLF IWF-Pmt IWF-Attn IWF-IG59.5347.6235.7223.8111.91MistralFalconQwen(c) e-SNLIFigure 10: Radar chart visualization of unfaithfulnessrates after 3 rounds of refinement. Lower values andsmaller areas indicate better performance.</p>
<p>Table 14 :
14
Complete case study on the ComVE dataset.The intervened word, highlighted in blue, successfully changes the model prediction.Faithful indicates the explanation/refinement is faithful as judged by the counterfactual test, while Unfaithful indicates the opposite.There was only one room in the place where Bill slept.It had a bed, a fridge, a stove, a couch, and a television.Where might he be?There was only one cozy room in the place where Bill slept.It had a bed, a fridge, a stove, a couch, and a television.Where might he be?
OriginalQuestion: Answer options: (A) motel (B) school (C) hotel (D) apartment (E) friend's houseModel prediction: (D) apartmentIntervenedQuestion: Answer options: (A) motel (B) school (C) hotel (D) apartment (E) friend's houseModel prediction: (A) motelInitial Explanation: Bill's room contains amenities typical of a motel, such as a bed, fridge, stove, couch, and television,which are not usually found together in a school setting. UnfaithfulNatural Language Feedback (NLF)Important-Word Feedback (IWF-Attn)</p>
<p>Table 15 :
15
Complete case study on the ECQA dataset.The intervened word, highlighted in blue, successfully changes the model prediction.Faithful indicates the explanation/refinement is faithful as judged by the counterfactual test, while Unfaithful indicates the opposite.The premise does not specify the power of the motorcycle, so the hypothesis introduces new information that cannot be confirmed from the premise.Unfaithful
OriginalPremise: A guy riding a motorcycle near junk cars.Hypothesis: A man is riding a motorcycle.Answer options: (A) contradiction (B) neutral (C) entailmentModel prediction: (C) entailmentIntervenedPremise: A guy riding a motorcycle near junk cars.Hypothesis: A man is riding a powerful motorcycle.Answer options: (A) contradiction (B) neutral (C) entailmentModel prediction: (B) neutralInitial Explanation: Natural Language Feedback (NLF)Important-Word Feedback (IWF-Attn)</p>
<p>Table 16 :
16
Complete case study on the e-SNLI dataset.The intervened word, highlighted in blue, successfully changes the model prediction.Faithful indicates the explanation/refinement is faithful as judged by the counterfactual test, while Unfaithful indicates the opposite.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences. IdentifyPlease select the most appropriate answerwhich one violates commonsense.without any explanation.Sentence 0: {sentence0}You must give your answer only in theSentence 1: {sentence1}following format:Answer Options:Answer: (X)(A) Sentence 0(B) Sentence 1ECQAYou are given a multiple-choice com-monsense question. Identify the mostappropriate answer.Question: {question}Answer Options:(A) {Option 1}(B) {Option 2}(C) {Option 3}(D) {Option 4}(E) {Option 5}e-SNLIYou are given a premise and a hypothesis.Identify the logical relationship betweenthem.Premise: {premise}Hypothesis: {hypothesis}Answer Options:(A) Contradiction(B) Neutral(C) Entailment</p>
<p>Table 17 :
17
Answer generation prompts.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences, and youYour selected answer is: ([LABEL]).have selected the one that violates com-monsense.Now, please provide an explanation foryour choice.Sentence 0: {sentence0}Sentence 1: {sentence1}Your explanation should:Answer Options:-Be clear, complete, and concise.(A) Sentence 0-Ideally within two short sentences.(B) Sentence 1ECQAYou are given a multiple-choice common-sense question, and you have selected the most appropriate answer.You must give your explanation only in the following format: Explanation: [your explanation here.]Question: {question}Answer Options:(A) {Option 1}(B) {Option 2}(C) {Option 3}(D) {Option 4}(E) {Option 5}e-SNLIYou are given a premise and a hypothesis,and you have selected the logical relation-ship between them.Premise: {premise}Hypothesis: {hypothesis}Answer Options:(A) Contradiction(B) Neutral(C) Entailment</p>
<p>Table 18 :
18
Explanation generation prompts.ECQAYou are given a multiple-choice commonsense question, and you have selected the most appropriate answer.You then provided an explanation for your choice.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences, and youYour selected answer is: ([LABEL])have selected the one that violatesYour explanation is:commonsense. You then provided an[EXPLANATION]explanation for your choice.Now, please provide feedback on thisSentence 0: {sentence0}explanation.Sentence 1: {sentence1}Answer Options:Your feedback should:(A) Sentence 0-Identify whether the explanation(B) Sentence 1accurately reflects your actual reasoning.-Point out if any key factors or importantdetails are missing, unclear, or incorrect.-Briefly describe what should be added orrevised to improve the explanation.-Clearly state that no improvement isQuestion: {question} Answer Options: (A) {Option 1} (B) {Option 2}needed when the explanation is good enough. -Be concise, avoid unnecessary repetition or irrelevant details.(C) {Option 3} (D) {Option 4} (E) {Option 5}You must give your feedback only in the following format: Feedback: [your feedback here.]e-SNLIYou are given a premise and a hypothesis,and you have selected the logical relation-ship between them. You then provided anexplanation for your choice.Premise: {premise}Hypothesis: {hypothesis}Answer Options:(A) Contradiction(B) Neutral(C) Entailment</p>
<p>Table 19 :
19
Natural language feedback generation prompts.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences, and youYour selected answer is: ([LABEL]).have selected the one that violates com-monsense.Now, please evaluate all the words in theinput and rank them by how importantSentence 0: {sentence0}they were in helping you make yourSentence 1: {sentence1}choice.Answer Options:(A) Sentence 0(B) Sentence 1</p>
<p><word>, <importance_score>'ECQAYou are given a multiple-choice commonsense question, and you have selected the most appropriate answer.
Question: {question}Answer Options:(A) {Option 1}(B) {Option 2}(C) {Option 3}(D) {Option 4}(E) {Option 5}e-SNLIYou are given a premise and a hypothesis,and you have selected the logical relation-ship between them.newlinePremise: {premise}Hypothesis: {hypothesis}Answer Options:(A) Contradiction(B) Neutral(C) Entailment</p>
<p>Table 20 :
20
Important words feedback generation prompts.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences, and youYour selected answer is: ([LABEL])have selected the one that violatesYour explanation is:commonsense. You then provided an[EXPLANATION]explanation for your choice, and receivedThe feedback you received is:feedback on the explanation.[FEEDBACK]Sentence 0: {sentence0}Sentence 1: {sentence1}Answer Options:(A) Sentence 0(B) Sentence 1</p>
<p>Table 21 :
21
Refinement generation prompts based on natural language feedback.
DatasetTask Specific Prompt PartCommon Instruction Prompt PartComVEYou are given two sentences, and youYour selected answer is: ([LABEL])have selected the one that violatesYour explanation is:commonsense. You then provided an[EXPLANATION]explanation for your choice, and receivedThe important words you received are:a list of important words that contributed[FEEDBACK]significantly to your reasoning.Sentence 0: {sentence0}Sentence 1: {sentence1}Answer Options:(A) Sentence 0(B) Sentence 1</p>
<p>Table 22 :
22
Refinement generation prompts based on important words feedback.</p>
<p>https://huggingface.co</p>
<p>Explanations for Common-senseQA: New Dataset and Models. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg, 10.18653/v1/2021.acl-long.238Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, arXiv:2311.16867Preprint</p>
<p>Faithfulness tests for natural language explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein, 10.18653/v1/2023.acl-short.25Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>A diagnostic study of explainability techniques for text classification. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, 10.18653/v1/2020.emnlp-main.263Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020a</p>
<p>Generating Fact Checking Explanations. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, 10.18653/v1/2020.acl-main.656Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020b</p>
<p>Diagnosticsguided explanation generation. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Self-amplify: Improving small language models with self post hoc explanations. Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot, arXiv:2402.120382024Preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, arXiv:2005.141652020Preprint</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. Curran Associates, Inc201831</p>
<p>ERASER: A benchmark to evaluate rationalized NLP models. Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, 10.18653/v1/2020.acl-main.408Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Gemini: A family of highly capable multimodal models. Rohan Gemini, Sebastian Anil, Jean-Baptiste Borgeaud, Jiahui Alayrac, Radu Yu, Johan Soricut, Andrew M Schalkwyk, Dai, arXiv:2312.118052025Preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024Preprint</p>
<p>Attention is not explanation. Sarthak Jain, Byron C Wallace, arXiv:1902.101862019Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>How Interpretable are Reasoning Explanations from Prompting Large Language Models?. Yeo Wei, Jie , Ranjan Satapathy, Rick Goh, Erik Cambria, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Post hoc explanations of language models can improve language models. Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju, arXiv:2305.114262023Preprint</p>
<p>Are large language models post hoc explainers?. Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju, XAI in Action: Past, Present, and Future Applications. 2023</p>
<p>Measuring faithfulness in chainof-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, ; Samuel, R Bowman, Ethan Perez, arXiv:2307.13702arXiv:1705.07874Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared KaplanPreprintScott Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Knowledgegrounded self-rationalization via extractive and natural language explanations. Prasad Bodhisattwa, Oana Majumder, Thomas Camburu, Julian Lukasiewicz, Mcauley, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022162</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, arXiv:2303.08774Gpt-4 technical report. 2024Preprint</p>
<p>On measuring faithfulness or self-consistency of natural language explanations. Letitia Parcalabescu, Anette Frank, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>An Qwen, Baosong Yang, Beichen Yang, Binyuan Zhang, Bo Hui, Bowen Zheng, Chengyuan Yu, Dayiheng Li, Fei Liu, Haoran Huang, Huan Wei, Jian Lin, Jianhong Yang, Jianwei Tu, Jianxin Zhang, Jiaxi Yang, Jingren Yang, Junyang Zhou, Kai Lin, Keming Dang, Keqin Lu, Kexin Bao, Le Yang, Mei Yu, Mingfeng Li, Pei Xue, Qin Zhang, Rui Zhu, Runji Men, Tianhao Lin, Tianyi Li, Tingyu Tang, Yang Xia, Yang Fan, Yichang Su, Yu Zhang, Yuqiong Wan, Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,Preprint</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/P19-1487Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Explaining interactions between text spans. Ray Sagnik, Pepa Choudhury, Isabelle Atanasova, Augenstein, 10.18653/v1/2023.emnlp-main.783Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, arXiv:1908.100842019Preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.113662023Preprint</p>
<p>The probabilities also matter: A more faithful metric for faithfulness of freetext explanations in large language models. Noah Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz, 10.18653/v1/2024.acl-short.49Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20242Short Papers)</p>
<p>Evaluating input feature explanations through a unified diagnostic evaluation framework. Jingyi Sun, Pepa Atanasova, Isabelle Augenstein, 10.18653/v1/2025.naacl-long.530Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew MexicoAssociation for Computational Linguistics20251</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, arXiv:1703.013652017Preprint</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>SemEval-2020 task 4: Commonsense validation and explanation. Cunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, Yue Zhang, 10.18653/v1/2020.semeval-1.39Proceedings of the Fourteenth Workshop on Semantic Evaluation. the Fourteenth Workshop on Semantic EvaluationBarcelonaInternational Committee for Computational Linguistics2020</p>
<p>PINTO: Faithful language reasoning using prompt-generated rationales. Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, Xiang Ren, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Cross-refine: Improving natural language explanation generation by learning in tandem. Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt, arXiv:2409.071232024Preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, arXiv:2010.12762Advances in Neural Information Processing Systems. Sarah Wiegreffe, Ana Marasović, and Noah A. Smith. 2022. Measuring association between labels and free-text rationales. 2022Preprint</p>
<p>Graph-guided textual explanation generation framework. Shuzhou Yuan, Jingyi Sun, Ran Zhang, Michael Färber, Steffen Eger, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)Suzhou, ChinaAssociation for Computational Linguistics2025Pepa Atanasova, and Isabelle Augenstein</p>            </div>
        </div>

    </div>
</body>
</html>
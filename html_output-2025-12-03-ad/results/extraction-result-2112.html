<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2112 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2112</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2112</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-281741939</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.03413v2.pdf" target="_blank">Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science</a></p>
                <p><strong>Paper Abstract:</strong> This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2112.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2112.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI agents for end-to-end scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of automated or agentic AI systems envisioned to participate as active collaborators in scientific software workflows, including code generation, experiment design, and real-time analysis; the workshop discusses their potential roles and the validation challenges they introduce.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic AI components that can interact with simulation and analysis tools, submit jobs to HPC facilities, assist in code generation/refactoring, and act as teammates in multi-person or multi-agent workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary (HPC-enabled domains: physics, chemistry, materials science, biology, astrophysics, engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report recommends hybrid validation combining automated test generation (including property-based and metamorphic testing), checks against analytical solutions where available, community/expert vetting (peer review of generated code), formal verification methods for critical properties, and where possible comparison against observations or experiments; it emphasizes living (continuous) validation frameworks that integrate expert feedback into model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper states current validation is insufficient: there is a reliability and validation gap for AI-generated outputs and agentic systems; domain norms often require physical/observational or high-fidelity computational verification for scientific claims, and the report calls for standards and community-integrated frameworks because ad-hoc validation is not adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy metrics provided in the report; the document emphasizes the need to develop uncertainty quantification and metrics to evaluate SciML vs traditional methods rather than reporting specific accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental protocols described in this report for general AI agents; the report recommends coupling AI outputs to experiments/observations where feasible and adding peer-review and community vetting, but does not provide experimental procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The report calls for comparative evaluation (e.g., SciML methods vs traditional numerics) and for metrics/benchmarks, but provides no empirical comparisons; it highlights the need to compare simulation-based validation, formal methods, and experimental checks within hybrid frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>The report documents conceptual failures or risks: AI-generated code may be syntactically correct yet violate domain constraints or introduce subtle numerical errors that compound over time; it highlights a general ‘‘reliability and validation gap’’ but gives no case study with quantitative failure data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No concrete success case presented for generic AI agents in this report; success is proposed as achievable via community-integrated validation, formal methods, and domain-constrained AI but no empirical demonstration is included here.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>The report recommends validation against known analytical solutions, observations, or experiments and use of benchmarks, but provides no ground-truth comparisons in this document itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The report emphasizes reproducibility as a requirement and calls for continuous integration/testing and community mechanisms to improve replicability, but does not describe any independent replication examples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>The report discusses that validation (especially experimental validation and high-fidelity verification) can be costly and time-consuming and calls for community infrastructure and funding to support such efforts, but provides no quantitative cost or time estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>States domain norms vary: many physical sciences expect adherence to physical laws and numerical stability and often require experimental or high-fidelity computational verification; life sciences typically require wet-lab validation for novel claims. The report urges domain-specific standards and formal verification where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The report calls for uncertainty quantification in AI-generated scientific code and for methods that expose confidence and potential error propagation, but provides no specific UQ algorithms or numeric uncertainty results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Identified limitations include lack of standardized validation frameworks for AI-generated scientific code, potential numerical instabilities from AI-generated code, challenges in replicability across heterogeneous computing environments, and absence of community norms or benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Recommended combination: automated generation of test cases (property/metamorphic testing), checks against analytical solutions, formal verification where possible, peer/expert review, and experimental/observational comparison when feasible; living/continuous validation loops connecting expert corrections back into model retraining are encouraged.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2112.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Science-aware code generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Science-aware AI code generation systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AI systems tailored to generate, refactor, or modernize scientific code that incorporate domain constraints (physics-informed, dimensional analysis, numerical stability) and are trained on curated scientific codebases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>science-aware AI code generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AI models and toolchains designed specifically for scientific computing contexts, combining neural methods with symbolic/physics-informed constraints to ensure generated code respects conservation laws, dimensional consistency, and numerical stability.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary scientific computing (numerical simulation domains such as fluid dynamics, molecular dynamics, materials modeling, astrophysics)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation approaches discussed include property-based testing, metamorphic testing, testing against known analytical solutions, formal verification of numerical properties, uncertainty quantification of generated code, peer review by domain experts, and comparison to observations/experimental data where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The report explicitly states traditional software testing is insufficient; sufficiency requires domain-aware tests (e.g., conservation checks), formal verification for critical properties, and community-driven validation standards; experimental validation is domain-dependent and sometimes required (e.g., when outputs propose new experimental predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No quantitative accuracy numbers provided; the paper calls for development of metrics to compare SciML approaches to traditional numerical methods and for reporting of uncertainty but gives no implemented accuracy figures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experiments described in this report for code-generation systems; the need for experimental or observational validation of scientific findings produced by such code is emphasized as domain-dependent and necessary in many fields.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper advocates comparing SciML-generated results to traditional numerical solvers and to analytical solutions/observations, but contains no empirical comparison results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Notes risk that AI-generated code can violate domain-specific constraints or introduce subtle numerical errors that compound over long simulations; suggests these represent common failure modes but gives no specific empirical cases within the report.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No concrete success cases in this report; success is framed as achievable using constrained generation, domain-knowledge integration, and community validation processes.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Advocated as a necessary practice (use of analytical solutions, benchmark problems), but no direct comparisons are reported in this workshop summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Report stresses reproducibility and the need for continuous integration / testing pipelines and community review to enable replication, but does not describe reproduced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Discusses that robust validation (including formal methods and experimental follow-up) will require resources and community coordination; no numeric estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>States scientific domains require domain-specific constraints: e.g., conservation checks in fluid dynamics, energy conservation in molecular dynamics; community norms will determine whether computational validation suffices or experimental validation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Report calls for development of uncertainty quantification methods for AI-generated code and for conveying confidence about potential error propagation, but no concrete methods or numbers are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Lack of standardized validation datasets, fragmented ecosystems, and absence of formal metrics for SciML vs traditional methods are highlighted as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Recommended mix: constrained code generation + property/metamorphic tests + formal verification for critical properties + expert review +, where needed, experimental/observational checks and uncertainty quantification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2112.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Community-integrated validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Community-integrated validation and continuous learning frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed validation frameworks that tightly couple automated testing, expert/community review, and continuous retraining to create living validation systems for AI-generated scientific outputs and code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>community-integrated validation framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frameworks that automatically generate scientifically grounded test cases, incorporate domain expert feedback via peer-review-like processes, use formal verification where possible, and feed corrections back into model refinement in continuous learning loops.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary (general infrastructure recommendation for scientific computing domains)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Includes automated test generation rooted in scientific principles (analytical solutions, conservation laws), metamorphic and property-based testing, formal methods for correctness, structured peer-review of generated code, and adaptive validation that incorporates expert corrections to retrain and refine AI models.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper argues these frameworks are necessary to reach sufficiency for scientific use: single-method validation is insufficient; community integration and formal guarantees are recommended to achieve domain-acceptable levels of confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy metrics; the framework is a recommendation rather than an evaluated system in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental validation performed in this report for such frameworks; the document calls for future pilot programs and empirical studies testing these frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Report suggests such frameworks should enable comparing methods (automated tests, formal proofs, and experiments) and harmonize them, but no comparative data is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No empirical failures of such frameworks are reported in this workshop summary; limitations and risks are discussed conceptually (e.g., resource cost, need for curated datasets, governance).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No implemented success cases presented here; framework proposed as remedy to the identified validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Framework explicitly recommends ground-truth checks where available (analytical solutions, experiments); no concrete ground-truth comparisons are in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Framework aims to improve reproducibility through standardized tests, continuous integration, and community vetting; the report calls for research and pilot programs to evaluate reproducibility improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper notes continuous, community-integrated validation will require ongoing investment and infrastructural support; no quantitative cost/time values are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Framework acknowledges domain norms vary and should be encoded into validation pipelines (e.g., experiments expected in biology; proof/benchmarks in numerical analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Framework recommends integrating UQ into validation pipelines so users understand confidence and error propagation, but specific UQ techniques are not prescribed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Challenges include curating validation datasets, automating meaningful scientific tests in domains lacking closed-form solutions, coordinating expert community input, and funding/maintaining continuous validation infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Designed as an explicit hybrid approach combining automated tests (property/metamorphic), formal verification, expert/peer review, and experimental/observational comparisons when applicable, with continuous learning loops to update AI models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2112.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Property/Metamorphic testing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Property-based testing and metamorphic testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing methodologies highlighted as especially valuable in scientific domains where exact reference solutions may not exist; they validate that outputs satisfy invariants or metamorphic relations instead of exact outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>property-based and metamorphic testing</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated testing approaches that assert domain-specific properties (e.g., conservation laws) or metamorphic relations (relations between inputs and outputs under transformations) to detect incorrect behavior in AI-generated code or models when exact ground truth is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>numerical simulation domains such as fluid dynamics, multiphysics simulation, general scientific software</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report describes using property-based tests (asserting invariants like mass/energy conservation) and metamorphic tests (checking expected relations under input transformations) for AI-generated code and models; example: checking divergence-free velocity fields and mass conservation for fluid dynamics code produced by AI.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>not applicable (testing methodology rather than simulator); used to validate high-fidelity simulation codes by checking key physical properties rather than full output-to-output fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The report positions these testing methods as particularly valuable when exact solutions are unavailable, but not alone sufficient — they should complement analytic checks, benchmarks, formal methods, and community review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy metrics; these tests are qualitative or property-level checks that detect specific classes of errors rather than provide percentage agreement measures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab or field experiments are described for these testing methods in the report; they are proposed as computational checks within CI/testing pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The report recommends these methods alongside analytical-solution checks and formal verification; no empirical comparison numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Noted limitation: property/metamorphic tests can miss errors that don't violate the tested properties and cannot guarantee overall correctness alone.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>The report gives conceptual examples where these tests can reveal violations (e.g., non-divergence in fluid solvers), but provides no implemented case studies in the document.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>These methods are specifically intended for cases where exact ground truth is absent; they provide surrogate checks instead of direct ground-truth comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Proposed to be embedded in continuous integration to improve reproducibility, but no replication studies are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Relatively low compared to experiments; report does not quantify but implies these automated tests can be integrated into CI to cheaply detect classes of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Report suggests many domains accept property-level checks (e.g., conservation laws) as necessary sanity checks but typically require more (benchmarks, experiments) for full validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Property/metamorphic testing itself does not quantify uncertainty; report recommends these be combined with UQ methods for overall confidence assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>They are necessary but not sufficient; may not detect all kinds of errors and must be designed carefully to reflect meaningful scientific invariants.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Recommended as part of hybrid validation stacks: automated property/metamorphic tests + benchmarks/analytics + formal methods + expert review + experimental comparison when applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2112.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal methods for scientific software correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of formal verification and proof techniques to numerical methods, implementations, and AI components to provide mathematical guarantees about software behavior and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>formal verification methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of theorem proving, model checking, and other formal techniques to verify correctness properties of numerical algorithms and, where feasible, AI components and solvers used in scientific computing.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>numerical analysis, computational physics, computational PDE solvers, safety-critical simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report calls for use of formal methods to verify implementations of numerical methods and AI methods (cites general need and references work on verification of PDE solvers and AI systems); formal verification is presented as complementary to testing and experimental validation to provide guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Formal verification can provide high assurance for properties amenable to formalization (e.g., stability, conservation), but the report notes that not all scientific properties are tractable for formal proof and that formal methods should be combined with empirical testing.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not applicable; formal verification establishes logical guarantees rather than probabilistic accuracy metrics. The report does not provide formal results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experiments performed in this report; formal methods are proposed as computational proofs augmenting other validation mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Report suggests formal methods should be integrated with testing and experimental checks; no quantitative comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Limitations include scalability and the difficulty of encoding complex scientific models for formal proof; report mentions these as challenges rather than specific failed proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Report references the need and cites literature (without reproducing) where formal correctness has been applied (e.g., verification of PDE solvers) but does not present new successful verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Formal methods provide internal guarantees rather than comparisons to external ground truth; recommended to be used alongside empirical ground-truth checks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Formal proofs are by nature reproducible when published, but the report does not document replication studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Formal verification can be resource- and labor-intensive; the report highlights this as a barrier but provides no numeric estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In some subfields (safety-critical computation) formal guarantees are highly valued; in broader computational science they are uncommon but recommended for critical components.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Formal methods themselves do not yield probabilistic UQ but can provide bounds or proofs of absence of particular classes of errors; the report recommends combining formal methods with UQ approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Scalability, formalizability of complex models, and resource costs are cited as primary limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Intended to be used alongside property-based testing, benchmarks, community vetting, and experimental validation to achieve acceptable scientific assurance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2112.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peer review for codegen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peer review / expert vetting for AI-generated code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-in-the-loop validation where domain experts inspect, test, and approve AI-generated code or scientific claims, proposed as an essential component of trustworthy AI-driven scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>peer review / expert vetting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Structured mechanisms (peer-review-like) for domain experts to examine AI-produced code and scientific outputs, provide corrections, and feed those corrections back into continuous learning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>all scientific domains using AI for code generation or discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Expert review complements automated testing and formal verification; the report suggests tailored peer-review processes for AI-generated artifacts that capture domain-specific checks and feed corrections into model refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The report positions expert vetting as necessary but not sufficient alone; combined with automated tests and formal methods it helps achieve community-acceptable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy metrics; effectiveness depends on expertise and thoroughness of review and on integration with automated checks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental protocols described; expert vetting may recommend experimental follow-up when outputs propose novel, testable hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Discussed conceptually as complementary to automated methods; no empirical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Human review can be labor-intensive and may miss subtle numerical issues if reviewers focus on high-level correctness; report notes workload and scalability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No empirical success cases provided in the report; peer review is recommended based on established scientific norms.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Experts compare outputs to known theory, benchmarks, and empirical knowledge as part of vetting; the report does not provide detailed examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Peer review is one mechanism to increase reproducibility by ensuring methods and code are inspectable; the report advocates for community norms that support reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human expert review is resource-intensive; the report flags this as a cost and suggests tooling to minimize reviewer workload but provides no quantitative estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Peer/expert review is a central norm across scientific domains; for AI-generated artifacts, the report recommends explicit incorporation of such review into validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Expert review contributes qualitative judgement about uncertainty; report encourages integrating these judgements into UQ pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Scalability, reviewer availability, and potential for human error are key limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Human vetting is intended to be combined with automated tests, formal methods, and experimental checks in community-integrated validation pipelines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2112.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2112.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The Virtual Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited external work (bioRxiv 2024) whose title explicitly reports that AI agents designed SARS-CoV-2 nanobodies and that these designs underwent experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The Virtual Lab (title-described system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AI-agent-based system applied to protein/biomolecule design; according to the citation title, designs were validated experimentally (wet-lab validation of AI-designed nanobodies).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / protein engineering / computational drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report lists this reference and the paper title states experimental validation was performed for designed nanobodies; the workshop report does not provide the experimental protocol details but highlights the citation as an example where AI-designed artifacts were validated in the lab.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The workshop uses this citation to illustrate that experimental validation is possible and important for biology domains; it does not assess sufficiency of that study's validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy or performance details are reported in this workshop document; the only information is that experimental validation occurred (per the cited paper's title).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Not provided in this report; only the cited paper's title indicates experimental validation of AI-designed nanobodies, but the workshop report does not reproduce methods or results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The workshop references this as an example of experimental validation but does not compare it to other validation approaches in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No failures reported in this workshop document regarding that cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as an example where AI-designed molecules proceeded to experimental validation; specifics are in the referenced paper rather than in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No details provided here; presumably lab assays served as ground truth in the cited work, but the workshop report does not elaborate.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The report does not state whether the experiments from the cited paper were reproduced independently.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed for this cited work in the report.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Report uses this citation to illustrate that in biology, wet-lab (experimental) validation is often required to confirm novel AI-generated designs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No UQ information provided in the workshop text about this cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>The workshop notes that experimental validation is resource-intensive and may not be possible for all AI-generated outputs; specific limitations of the cited study are not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Not described in this report; the cited paper's title indicates experimental validation, but whether that work combined experiments with formal or computational verification is not specified here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Robin: A multi-agent system for automating scientific discovery <em>(Rating: 2)</em></li>
                <li>Development of AI-assisted microscopy frameworks through realistic simulation with pySTED <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2112",
    "paper_id": "paper-281741939",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "AI agents (general)",
            "name_full": "AI agents for end-to-end scientific workflows",
            "brief_description": "General class of automated or agentic AI systems envisioned to participate as active collaborators in scientific software workflows, including code generation, experiment design, and real-time analysis; the workshop discusses their potential roles and the validation challenges they introduce.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "AI agents (general)",
            "system_description": "Agentic AI components that can interact with simulation and analysis tools, submit jobs to HPC facilities, assist in code generation/refactoring, and act as teammates in multi-person or multi-agent workflows.",
            "scientific_domain": "cross-disciplinary (HPC-enabled domains: physics, chemistry, materials science, biology, astrophysics, engineering)",
            "validation_type": "hybrid",
            "validation_description": "The report recommends hybrid validation combining automated test generation (including property-based and metamorphic testing), checks against analytical solutions where available, community/expert vetting (peer review of generated code), formal verification methods for critical properties, and where possible comparison against observations or experiments; it emphasizes living (continuous) validation frameworks that integrate expert feedback into model updates.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The paper states current validation is insufficient: there is a reliability and validation gap for AI-generated outputs and agentic systems; domain norms often require physical/observational or high-fidelity computational verification for scientific claims, and the report calls for standards and community-integrated frameworks because ad-hoc validation is not adequate.",
            "validation_accuracy": "No numerical accuracy metrics provided in the report; the document emphasizes the need to develop uncertainty quantification and metrics to evaluate SciML vs traditional methods rather than reporting specific accuracies.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "No experimental protocols described in this report for general AI agents; the report recommends coupling AI outputs to experiments/observations where feasible and adding peer-review and community vetting, but does not provide experimental procedures.",
            "validation_comparison": "The report calls for comparative evaluation (e.g., SciML methods vs traditional numerics) and for metrics/benchmarks, but provides no empirical comparisons; it highlights the need to compare simulation-based validation, formal methods, and experimental checks within hybrid frameworks.",
            "validation_failures": "The report documents conceptual failures or risks: AI-generated code may be syntactically correct yet violate domain constraints or introduce subtle numerical errors that compound over time; it highlights a general ‘‘reliability and validation gap’’ but gives no case study with quantitative failure data.",
            "validation_success_cases": "No concrete success case presented for generic AI agents in this report; success is proposed as achievable via community-integrated validation, formal methods, and domain-constrained AI but no empirical demonstration is included here.",
            "ground_truth_comparison": "The report recommends validation against known analytical solutions, observations, or experiments and use of benchmarks, but provides no ground-truth comparisons in this document itself.",
            "reproducibility_replication": "The report emphasizes reproducibility as a requirement and calls for continuous integration/testing and community mechanisms to improve replicability, but does not describe any independent replication examples.",
            "validation_cost_time": "The report discusses that validation (especially experimental validation and high-fidelity verification) can be costly and time-consuming and calls for community infrastructure and funding to support such efforts, but provides no quantitative cost or time estimates.",
            "domain_validation_norms": "States domain norms vary: many physical sciences expect adherence to physical laws and numerical stability and often require experimental or high-fidelity computational verification; life sciences typically require wet-lab validation for novel claims. The report urges domain-specific standards and formal verification where applicable.",
            "uncertainty_quantification": "The report calls for uncertainty quantification in AI-generated scientific code and for methods that expose confidence and potential error propagation, but provides no specific UQ algorithms or numeric uncertainty results.",
            "validation_limitations": "Identified limitations include lack of standardized validation frameworks for AI-generated scientific code, potential numerical instabilities from AI-generated code, challenges in replicability across heterogeneous computing environments, and absence of community norms or benchmarks.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Recommended combination: automated generation of test cases (property/metamorphic testing), checks against analytical solutions, formal verification where possible, peer/expert review, and experimental/observational comparison when feasible; living/continuous validation loops connecting expert corrections back into model retraining are encouraged.",
            "uuid": "e2112.0"
        },
        {
            "name_short": "Science-aware code generation",
            "name_full": "Science-aware AI code generation systems",
            "brief_description": "AI systems tailored to generate, refactor, or modernize scientific code that incorporate domain constraints (physics-informed, dimensional analysis, numerical stability) and are trained on curated scientific codebases.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "science-aware AI code generation",
            "system_description": "AI models and toolchains designed specifically for scientific computing contexts, combining neural methods with symbolic/physics-informed constraints to ensure generated code respects conservation laws, dimensional consistency, and numerical stability.",
            "scientific_domain": "cross-disciplinary scientific computing (numerical simulation domains such as fluid dynamics, molecular dynamics, materials modeling, astrophysics)",
            "validation_type": "hybrid",
            "validation_description": "Validation approaches discussed include property-based testing, metamorphic testing, testing against known analytical solutions, formal verification of numerical properties, uncertainty quantification of generated code, peer review by domain experts, and comparison to observations/experimental data where applicable.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The report explicitly states traditional software testing is insufficient; sufficiency requires domain-aware tests (e.g., conservation checks), formal verification for critical properties, and community-driven validation standards; experimental validation is domain-dependent and sometimes required (e.g., when outputs propose new experimental predictions).",
            "validation_accuracy": "No quantitative accuracy numbers provided; the paper calls for development of metrics to compare SciML approaches to traditional numerical methods and for reporting of uncertainty but gives no implemented accuracy figures.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "No experiments described in this report for code-generation systems; the need for experimental or observational validation of scientific findings produced by such code is emphasized as domain-dependent and necessary in many fields.",
            "validation_comparison": "The paper advocates comparing SciML-generated results to traditional numerical solvers and to analytical solutions/observations, but contains no empirical comparison results itself.",
            "validation_failures": "Notes risk that AI-generated code can violate domain-specific constraints or introduce subtle numerical errors that compound over long simulations; suggests these represent common failure modes but gives no specific empirical cases within the report.",
            "validation_success_cases": "No concrete success cases in this report; success is framed as achievable using constrained generation, domain-knowledge integration, and community validation processes.",
            "ground_truth_comparison": "Advocated as a necessary practice (use of analytical solutions, benchmark problems), but no direct comparisons are reported in this workshop summary.",
            "reproducibility_replication": "Report stresses reproducibility and the need for continuous integration / testing pipelines and community review to enable replication, but does not describe reproduced studies.",
            "validation_cost_time": "Discusses that robust validation (including formal methods and experimental follow-up) will require resources and community coordination; no numeric estimates provided.",
            "domain_validation_norms": "States scientific domains require domain-specific constraints: e.g., conservation checks in fluid dynamics, energy conservation in molecular dynamics; community norms will determine whether computational validation suffices or experimental validation is required.",
            "uncertainty_quantification": "Report calls for development of uncertainty quantification methods for AI-generated code and for conveying confidence about potential error propagation, but no concrete methods or numbers are presented.",
            "validation_limitations": "Lack of standardized validation datasets, fragmented ecosystems, and absence of formal metrics for SciML vs traditional methods are highlighted as limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Recommended mix: constrained code generation + property/metamorphic tests + formal verification for critical properties + expert review +, where needed, experimental/observational checks and uncertainty quantification.",
            "uuid": "e2112.1"
        },
        {
            "name_short": "Community-integrated validation",
            "name_full": "Community-integrated validation and continuous learning frameworks",
            "brief_description": "Proposed validation frameworks that tightly couple automated testing, expert/community review, and continuous retraining to create living validation systems for AI-generated scientific outputs and code.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "community-integrated validation framework",
            "system_description": "Frameworks that automatically generate scientifically grounded test cases, incorporate domain expert feedback via peer-review-like processes, use formal verification where possible, and feed corrections back into model refinement in continuous learning loops.",
            "scientific_domain": "cross-disciplinary (general infrastructure recommendation for scientific computing domains)",
            "validation_type": "hybrid",
            "validation_description": "Includes automated test generation rooted in scientific principles (analytical solutions, conservation laws), metamorphic and property-based testing, formal methods for correctness, structured peer-review of generated code, and adaptive validation that incorporates expert corrections to retrain and refine AI models.",
            "simulation_fidelity": null,
            "validation_sufficiency": "Paper argues these frameworks are necessary to reach sufficiency for scientific use: single-method validation is insufficient; community integration and formal guarantees are recommended to achieve domain-acceptable levels of confidence.",
            "validation_accuracy": "No numerical accuracy metrics; the framework is a recommendation rather than an evaluated system in this report.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experimental validation performed in this report for such frameworks; the document calls for future pilot programs and empirical studies testing these frameworks.",
            "validation_comparison": "Report suggests such frameworks should enable comparing methods (automated tests, formal proofs, and experiments) and harmonize them, but no comparative data is presented.",
            "validation_failures": "No empirical failures of such frameworks are reported in this workshop summary; limitations and risks are discussed conceptually (e.g., resource cost, need for curated datasets, governance).",
            "validation_success_cases": "No implemented success cases presented here; framework proposed as remedy to the identified validation gap.",
            "ground_truth_comparison": "Framework explicitly recommends ground-truth checks where available (analytical solutions, experiments); no concrete ground-truth comparisons are in this report.",
            "reproducibility_replication": "Framework aims to improve reproducibility through standardized tests, continuous integration, and community vetting; the report calls for research and pilot programs to evaluate reproducibility improvements.",
            "validation_cost_time": "Paper notes continuous, community-integrated validation will require ongoing investment and infrastructural support; no quantitative cost/time values are provided.",
            "domain_validation_norms": "Framework acknowledges domain norms vary and should be encoded into validation pipelines (e.g., experiments expected in biology; proof/benchmarks in numerical analysis).",
            "uncertainty_quantification": "Framework recommends integrating UQ into validation pipelines so users understand confidence and error propagation, but specific UQ techniques are not prescribed here.",
            "validation_limitations": "Challenges include curating validation datasets, automating meaningful scientific tests in domains lacking closed-form solutions, coordinating expert community input, and funding/maintaining continuous validation infrastructure.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Designed as an explicit hybrid approach combining automated tests (property/metamorphic), formal verification, expert/peer review, and experimental/observational comparisons when applicable, with continuous learning loops to update AI models.",
            "uuid": "e2112.2"
        },
        {
            "name_short": "Property/Metamorphic testing",
            "name_full": "Property-based testing and metamorphic testing",
            "brief_description": "Testing methodologies highlighted as especially valuable in scientific domains where exact reference solutions may not exist; they validate that outputs satisfy invariants or metamorphic relations instead of exact outputs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "property-based and metamorphic testing",
            "system_description": "Automated testing approaches that assert domain-specific properties (e.g., conservation laws) or metamorphic relations (relations between inputs and outputs under transformations) to detect incorrect behavior in AI-generated code or models when exact ground truth is unavailable.",
            "scientific_domain": "numerical simulation domains such as fluid dynamics, multiphysics simulation, general scientific software",
            "validation_type": "simulated",
            "validation_description": "The report describes using property-based tests (asserting invariants like mass/energy conservation) and metamorphic tests (checking expected relations under input transformations) for AI-generated code and models; example: checking divergence-free velocity fields and mass conservation for fluid dynamics code produced by AI.",
            "simulation_fidelity": "not applicable (testing methodology rather than simulator); used to validate high-fidelity simulation codes by checking key physical properties rather than full output-to-output fidelity.",
            "validation_sufficiency": "The report positions these testing methods as particularly valuable when exact solutions are unavailable, but not alone sufficient — they should complement analytic checks, benchmarks, formal methods, and community review.",
            "validation_accuracy": "No numerical accuracy metrics; these tests are qualitative or property-level checks that detect specific classes of errors rather than provide percentage agreement measures.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab or field experiments are described for these testing methods in the report; they are proposed as computational checks within CI/testing pipelines.",
            "validation_comparison": "The report recommends these methods alongside analytical-solution checks and formal verification; no empirical comparison numbers provided.",
            "validation_failures": "Noted limitation: property/metamorphic tests can miss errors that don't violate the tested properties and cannot guarantee overall correctness alone.",
            "validation_success_cases": "The report gives conceptual examples where these tests can reveal violations (e.g., non-divergence in fluid solvers), but provides no implemented case studies in the document.",
            "ground_truth_comparison": "These methods are specifically intended for cases where exact ground truth is absent; they provide surrogate checks instead of direct ground-truth comparison.",
            "reproducibility_replication": "Proposed to be embedded in continuous integration to improve reproducibility, but no replication studies are reported here.",
            "validation_cost_time": "Relatively low compared to experiments; report does not quantify but implies these automated tests can be integrated into CI to cheaply detect classes of errors.",
            "domain_validation_norms": "Report suggests many domains accept property-level checks (e.g., conservation laws) as necessary sanity checks but typically require more (benchmarks, experiments) for full validation.",
            "uncertainty_quantification": "Property/metamorphic testing itself does not quantify uncertainty; report recommends these be combined with UQ methods for overall confidence assessment.",
            "validation_limitations": "They are necessary but not sufficient; may not detect all kinds of errors and must be designed carefully to reflect meaningful scientific invariants.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Recommended as part of hybrid validation stacks: automated property/metamorphic tests + benchmarks/analytics + formal methods + expert review + experimental comparison when applicable.",
            "uuid": "e2112.3"
        },
        {
            "name_short": "Formal verification",
            "name_full": "Formal methods for scientific software correctness",
            "brief_description": "Application of formal verification and proof techniques to numerical methods, implementations, and AI components to provide mathematical guarantees about software behavior and correctness.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "formal verification methods",
            "system_description": "Use of theorem proving, model checking, and other formal techniques to verify correctness properties of numerical algorithms and, where feasible, AI components and solvers used in scientific computing.",
            "scientific_domain": "numerical analysis, computational physics, computational PDE solvers, safety-critical simulation",
            "validation_type": "computational_proof",
            "validation_description": "The report calls for use of formal methods to verify implementations of numerical methods and AI methods (cites general need and references work on verification of PDE solvers and AI systems); formal verification is presented as complementary to testing and experimental validation to provide guarantees.",
            "simulation_fidelity": null,
            "validation_sufficiency": "Formal verification can provide high assurance for properties amenable to formalization (e.g., stability, conservation), but the report notes that not all scientific properties are tractable for formal proof and that formal methods should be combined with empirical testing.",
            "validation_accuracy": "Not applicable; formal verification establishes logical guarantees rather than probabilistic accuracy metrics. The report does not provide formal results itself.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experiments performed in this report; formal methods are proposed as computational proofs augmenting other validation mechanisms.",
            "validation_comparison": "Report suggests formal methods should be integrated with testing and experimental checks; no quantitative comparisons provided.",
            "validation_failures": "Limitations include scalability and the difficulty of encoding complex scientific models for formal proof; report mentions these as challenges rather than specific failed proofs.",
            "validation_success_cases": "Report references the need and cites literature (without reproducing) where formal correctness has been applied (e.g., verification of PDE solvers) but does not present new successful verifications.",
            "ground_truth_comparison": "Formal methods provide internal guarantees rather than comparisons to external ground truth; recommended to be used alongside empirical ground-truth checks.",
            "reproducibility_replication": "Formal proofs are by nature reproducible when published, but the report does not document replication studies.",
            "validation_cost_time": "Formal verification can be resource- and labor-intensive; the report highlights this as a barrier but provides no numeric estimates.",
            "domain_validation_norms": "In some subfields (safety-critical computation) formal guarantees are highly valued; in broader computational science they are uncommon but recommended for critical components.",
            "uncertainty_quantification": "Formal methods themselves do not yield probabilistic UQ but can provide bounds or proofs of absence of particular classes of errors; the report recommends combining formal methods with UQ approaches.",
            "validation_limitations": "Scalability, formalizability of complex models, and resource costs are cited as primary limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Intended to be used alongside property-based testing, benchmarks, community vetting, and experimental validation to achieve acceptable scientific assurance.",
            "uuid": "e2112.4"
        },
        {
            "name_short": "Peer review for codegen",
            "name_full": "Peer review / expert vetting for AI-generated code",
            "brief_description": "Human-in-the-loop validation where domain experts inspect, test, and approve AI-generated code or scientific claims, proposed as an essential component of trustworthy AI-driven scientific workflows.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "peer review / expert vetting",
            "system_description": "Structured mechanisms (peer-review-like) for domain experts to examine AI-produced code and scientific outputs, provide corrections, and feed those corrections back into continuous learning systems.",
            "scientific_domain": "all scientific domains using AI for code generation or discovery",
            "validation_type": "hybrid",
            "validation_description": "Expert review complements automated testing and formal verification; the report suggests tailored peer-review processes for AI-generated artifacts that capture domain-specific checks and feed corrections into model refinement.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The report positions expert vetting as necessary but not sufficient alone; combined with automated tests and formal methods it helps achieve community-acceptable validation.",
            "validation_accuracy": "No numeric accuracy metrics; effectiveness depends on expertise and thoroughness of review and on integration with automated checks.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "No experimental protocols described; expert vetting may recommend experimental follow-up when outputs propose novel, testable hypotheses.",
            "validation_comparison": "Discussed conceptually as complementary to automated methods; no empirical comparisons provided.",
            "validation_failures": "Human review can be labor-intensive and may miss subtle numerical issues if reviewers focus on high-level correctness; report notes workload and scalability concerns.",
            "validation_success_cases": "No empirical success cases provided in the report; peer review is recommended based on established scientific norms.",
            "ground_truth_comparison": "Experts compare outputs to known theory, benchmarks, and empirical knowledge as part of vetting; the report does not provide detailed examples.",
            "reproducibility_replication": "Peer review is one mechanism to increase reproducibility by ensuring methods and code are inspectable; the report advocates for community norms that support reproducibility.",
            "validation_cost_time": "Human expert review is resource-intensive; the report flags this as a cost and suggests tooling to minimize reviewer workload but provides no quantitative estimates.",
            "domain_validation_norms": "Peer/expert review is a central norm across scientific domains; for AI-generated artifacts, the report recommends explicit incorporation of such review into validation pipelines.",
            "uncertainty_quantification": "Expert review contributes qualitative judgement about uncertainty; report encourages integrating these judgements into UQ pipelines.",
            "validation_limitations": "Scalability, reviewer availability, and potential for human error are key limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Human vetting is intended to be combined with automated tests, formal methods, and experimental checks in community-integrated validation pipelines.",
            "uuid": "e2112.5"
        },
        {
            "name_short": "The Virtual Lab",
            "name_full": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "brief_description": "A cited external work (bioRxiv 2024) whose title explicitly reports that AI agents designed SARS-CoV-2 nanobodies and that these designs underwent experimental validation.",
            "citation_title": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "mention_or_use": "mention",
            "system_name": "The Virtual Lab (title-described system)",
            "system_description": "AI-agent-based system applied to protein/biomolecule design; according to the citation title, designs were validated experimentally (wet-lab validation of AI-designed nanobodies).",
            "scientific_domain": "biology / protein engineering / computational drug discovery",
            "validation_type": "experimental",
            "validation_description": "The report lists this reference and the paper title states experimental validation was performed for designed nanobodies; the workshop report does not provide the experimental protocol details but highlights the citation as an example where AI-designed artifacts were validated in the lab.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The workshop uses this citation to illustrate that experimental validation is possible and important for biology domains; it does not assess sufficiency of that study's validation.",
            "validation_accuracy": "No numerical accuracy or performance details are reported in this workshop document; the only information is that experimental validation occurred (per the cited paper's title).",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Not provided in this report; only the cited paper's title indicates experimental validation of AI-designed nanobodies, but the workshop report does not reproduce methods or results.",
            "validation_comparison": "The workshop references this as an example of experimental validation but does not compare it to other validation approaches in detail.",
            "validation_failures": "No failures reported in this workshop document regarding that cited work.",
            "validation_success_cases": "Cited as an example where AI-designed molecules proceeded to experimental validation; specifics are in the referenced paper rather than in this report.",
            "ground_truth_comparison": "No details provided here; presumably lab assays served as ground truth in the cited work, but the workshop report does not elaborate.",
            "reproducibility_replication": "The report does not state whether the experiments from the cited paper were reproduced independently.",
            "validation_cost_time": "Not discussed for this cited work in the report.",
            "domain_validation_norms": "Report uses this citation to illustrate that in biology, wet-lab (experimental) validation is often required to confirm novel AI-generated designs.",
            "uncertainty_quantification": "No UQ information provided in the workshop text about this cited work.",
            "validation_limitations": "The workshop notes that experimental validation is resource-intensive and may not be possible for all AI-generated outputs; specific limitations of the cited study are not discussed here.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "Not described in this report; the cited paper's title indicates experimental validation, but whether that work combined experiments with formal or computational verification is not specified here.",
            "uuid": "e2112.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Robin: A multi-agent system for automating scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Development of AI-assisted microscopy frameworks through realistic simulation with pySTED",
            "rating": 1
        }
    ],
    "cost": 0.019553749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Next-Generation Ecosystems for Scientific Computing
7 Oct 2025 September 2025</p>
<p>C Mcinnes 
D Arnold 
P Balaprakash 
M Bernhardt 
B Cerny 
A Dubey 
R Giles 
D W Hood 
M A Leung 
V López- Marrero 
P Messina 
Olivia B Newton 
C Oehmen 
Stefan M Wild 
J Willenbring 
L Woodley 
T Baylis 
D E Bernholdt 
C Camaño 
J Cohoon 
C Ferenbaugh 
S M Fiore 
S Gesing 
D Gómez-Zará 
J Howison 
T Islam 
D Kepczynski 
C Lively 
H Menon 
B Messer 
M Ngom 
U Paliath 
M E Papka 
I Qualters 
Elaine M Raybourn 
Katherine Riley 
P Rodriguez 
D Rouson 
M Schwalbe 
S K Seal 
Ö Sürer 
Valerie Taylor 
L Wu 
Lois Curfman Mcinnes 
Lawrence Berkeley </p>
<p>US Dept. of Energy's Office of Scientific and Technical Information</p>
<p>U.S. Department of Commerce National Technical Information Service
5301 Shawnee Road Alexandria22312VA</p>
<p>U.S. Department of Energy Office of Scientific and Technical Information
P.O. Box 6237831-0062Oak RidgeTN</p>
<p>Next-Generation Ecosystems for Scientific Computing: Harnessing Community
Software</p>
<p>AI for Cross-Disciplinary Team Science</p>
<p>Argonne National Laboratory (retired)
Argonne National Laboratory Dorian Arnold
Emory University Prasanna Balaprakash
Oak Ridge National Laboratory Mike Bernhardt
Team Libra Beth Cerny
Argonne National Laboratory Anshu Dubey
Argonne National Laboratory Roscoe Giles
Boston University Denice Ward Hood
University of Illinois Urbana-Champaign Mary Ann Leung
Sustainable Horizons Institute Vanessa López-Marrero
Stony Brook University Paul Messina</p>
<p>University of Montana Chris Oehmen
Pacific Northwest National Laboratory</p>
<p>Center for Scientific Collaboration and Community Engagement Tony Baylis
Argonne National Laboratory
Los Alamos National Laboratory (retired)
National Laboratory Jim Willenbring
Sandia National Laboratories Lou Woodley
Lawrence Livermore National Laboratory David E. Bernholdt
Oak Ridge National Laboratory Chris Camaño
California Institute of Technology Johannah Cohoon
Lawrence Berkeley National Laboratory Charles Ferenbaugh
Los Alamos National Laboratory Stephen M. Fiore
University of Central Florida Sandra Gesing
US Research Software Engineers Association Diego Gómez-Zará
University of Notre Dame James Howison
University of Texas at Austin Tanzima Islam
Texas State University David Kepczynski, Ford Motor Company Charles Lively
Lawrence Berkeley National Laboratory Harshitha Menon
Lawrence Livermore National Laboratory Bronson Messer
Oak Ridge National Laboratory Marieme Ngom
Argonne National Laboratory Umesh Paliath
GE Aerospace Research Michael E. Papka
University of Illinois Chicago Irene Qualters</p>
<p>Sandia National Laboratories</p>
<p>Argonne National Laboratory Paulina Rodriguez
The George Washington University Damian Rouson
Lawrence Berkeley National Laboratory Michelle Schwalbe
National Academies of Sciences Sudip K. Seal
Oak Ridge National Laboratory Özge Sürer
Miami University</p>
<p>Argonne National Laboratory Lingfei Wu
University of Pittsburgh</p>
<p>Toward Next-Generation Ecosystems for Scientific Computing
7 Oct 2025 September 2025FA93AEC4D1607F2FFFF904BC14EA385310.48550/arXiv.2510.03413arXiv:2510.03413v2[cs.CE]
Cover image:The cover image represents the deflagration phase in a Type-Ia supernova in a Flash/Flash-X simulation.</p>
<p>The 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science convened in Chicago, IL, from April 29 to May 1, 2025.The event brought together more than 40 experts from high-performance computing (HPC), AI, computational science, software engineering, applied mathematics, social sciences, and community development to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems.Workshop discussions underscored a powerful truth: scientific computing is at a turning point.As AI grows more capable and scientific questions become more complex, a transformation is unavoidable.The traditional ways we compute, code, and collaborate are no longer adequate.Periods of profound transformation bring both unprecedented opportunity and inherent uncertainty.Hence, addressing the complexity of this moment requires more than incremental progress-it necessitates strategic risk-taking and foundational shifts in approach.Just as the rise of digital computing reshaped nearly every scientific and industrial domain, AI-integrated software ecosystems must undergo that same level of conceptual reinvention, driven by rigorous inquiry and a willingness to redefine established frameworks.</p>
<p>We face the urgent need for a bold, enduring transformation-one that extends beyond technology alone.We envision a new kind of agile and robust scientific computing ecosystem built through socio-technical codesign-i.e, the intentional, integrated development of social and technical components as interdependent parts of a unified strategy.This approach integrates cutting-edge advances in AI and software-such as large language models (LLMs), scientific machine learning (SciML), reasoning models, and coding agents-with strategies to foster cross-disciplinary team building, training, and collaboration.Two key themes emerged:</p>
<p>• AI is not just another tool-it can dramatically boost scientific productivity, catalyzing new discovery.Moreover, while demonstrably transformative, AI's evolution is not known.• Supporting this transformation in computational science will require a broad, cross-disciplinary community and sustained investment in preparing the next generation and empowering today's experts.</p>
<p>With these guiding themes, the workshop focused on three major challenges in future scientific computing:</p>
<p>• Advancing scientific software ecosystems for HPC and AI of today and tomorrow, while ensuring results remain scientifically valid.• Fostering collaboration among scientists, AI experts, research software engineers, educators, community leaders, and even AI systems themselves-bridging traditionally siloed roles and domains.This includes private-sector partners, key to translating scientific research into real-world applications.• Rethinking training and workforce development, to help today's and tomorrow's researchers adapt to rapid changes in tools and technology.</p>
<p>The workshop identified research directions and community actions to address these challenges.These include building modular, trustworthy, AI-powered scientific software systems; creating frameworks where humans and AI systems can develop and validate code together; overhauling training pipelines to reflect the pace and nature of modern computational science; promoting responsible innovation with guidelines for AI use; and launching pilot programs and partnerships that test new ways of learning and working.</p>
<p>In short, the workshop laid out a vision for the future: one where AI, software, hardware, and human expertise work hand in hand to accelerate scientific progress, broaden access, build the workforce, and preserve the integrity of the scientific method.This report marks the beginning of a long-term, communitydriven effort to turn that vision into reality.</p>
<p>The workshop gave us a fresh, encouraging perspective.We must recognize that-more than ever-the future of science depends on more than computational power; it depends on how we embed human insight within intelligent systems.When we align AI's capabilities with the creativity, intuition, and discernment of researchers, we open new pathways for discovery.Purposeful integration of technical and societal dimensions creates a living system-one that learns, adapts, and accelerates progress where it matters most.</p>
<p>Significant Challenges Facing Computational Science</p>
<p>Throughout history, the most profound technological advances have challenged society's capacity to adapt and respond at scale.Artificial intelligence (AI), while still immature, is already poised to influence nearly every facet of human endeavor (see, e.g., [1]).Similarly, while its full impact remains uncertain, AI's disruptive and beneficial potential in science and engineering is clear.In turn, how we integrate AI across science and engineering may shape future global leadership.</p>
<p>Scientific discovery through computing underpins U.S. innovation, economic growth, and national security.Over the past fifty years, advances in computing have revolutionized how scientists learn, experiment, and theorize.High-performance computing (HPC)-including modeling and simulation, data analytics, machine learning, and AI-is a necessary means of discovery and innovation in essentially all areas of science, engineering, technology, and society [2,3,4,5,6,7].Today, that pursuit faces formidable challenges.As scientific data, modeling, simulation, and AI grow in volume, complexity, and strategic value, they have become central to the competitiveness of the nation's industrial and manufacturing sectors such as automotive, aerospace, materials, and others.To remain a global leader, the U.S. must act with urgency and intention to elevate its scientific research infrastructure-especially through scientific software ecosystems that integrate AI and advanced computing capabilities.This is no longer a question of optional investment; it is a foundational requirement for ensuring our socioeconomic prosperity, maintaining global competitiveness, strengthening national security, and securing a higher quality of life for future generations.</p>
<p>Meeting this challenge requires broad national commitment and coordination.The scientific computing community cannot transform alone.Government, academia, national labs, and the private sector must invest together in the technical, institutional, and human systems that advance science.Scientific computing must evolve not only in tools and platforms but also in collaboration, governance, and talent development.</p>
<p>To meet this challenge, we must develop and adopt innovative approaches to the integration and application of scientific computing, learning from the past and preparing for the future.Scientific progress now hinges on our ability to integrate AI not only into our research methodologies but also into the underlying software ecosystems that power them [8,9,10,11,12,13].This integration must be safe, reliable, and agile, demanding a new generation of scientific software tools that are robust, scalable, and adaptable to rapidly evolving computing platforms-not retrofits, but purpose-built ecosystems capable of accelerating discovery in areas such as materials design, energy, biology, astrophysics, manufacturing, and security.</p>
<p>Equally important is the human dimension.Scientific advancement depends not only on tools and technology but also on the people who create and use them.We must cultivate a workforce of researchers and technologists who are proficient in developing and using emerging software ecosystems and AI-enabled research methodologies [7,14,15,16,17].These people must be empowered to collaborate across disciplines, think computationally, and adapt rapidly to evolving technologies.Building this talent pipeline is not merely about education-it is about building the intellectual foundation for the nation's scientific enterprise.This is a multidimensional challenge-requiring urgent, coordinated action.We must advance technology, reimagine software, and cultivate a highly skilled workforce simultaneously.The United States aspires to remain a global leader in science and innovation [18], and progress in scientific computing is key to that vision.Over the coming decades, state-of-the-art science will increasingly rely on the effective, pervasive, and responsible use of AI, placing new demands on our tools, methods, and practices.A key enabler of this transformation will be the development of next-generation scientific software that is not only AI-integrated but also scalable, sustainable, and trustworthy.But the transformations needed for effective scientific progress are too large and pervasive to be done piecemeal by independent actors.The scope and urgency of the challenges demand coordinated, community-driven action across institutions, disciplines, and sectors-and ultimately, across national boundaries, while keeping the flexibility required for the dynamic evolution of scientific computing ecosystems.While these early efforts may focus on national coordination, lasting progress will require globally-connected community efforts.</p>
<p>Workshop objectives</p>
<p>To address these critical needs, the workshop Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science was held in Chicago, IL, during April 29 -May 1, 2025. 1 The workshop convened a broad group of experts spanning HPC, AI, computational science, domain sciences, applied mathematics, software engineering, cognitive and social sciences, and community development.Participants collectively examined how we can co-design next-generation scientific software ecosystems to enable cutting-edge research, foster cross-disciplinary collaboration, and scale effectively across institutions and infrastructure.The workshop introduced the methodology of sociotechnical co-design for next-generation scientific computing, explained in Section 1.3, as a complement to insights from other forward-looking community reports (e.g., [8,9,11,19,20,21]).By intentionally blending technical and community factors, the workshop aimed to shape a future where thriving, crossdisciplinary communities drive the next wave of scientific discovery, with high-quality scientific software as a keystone of sustained collaboration and scientific progress.</p>
<p>Scientific computing ecosystems.In the context of this report, ecosystems for scientific computing are dynamic socio-technical systems made up of people, technologies, infrastructure, institutions, workflows, and cultural practices that collectively support the development and evolution of scientific computing.Ecosystems are role-and context-dependent-shaped by the interactions among stakeholders, researchers, developers, users, institutions, and AI systems.A successful ecosystem integrates technical components (such as software libraries, AI technologies, and computational platforms) with social dimensions (such as training, governance, and collaboration norms) to enable scientific progress.We believe that such ecosystems will emerge through the deliberate co-design of software platforms, scientific tools, community structures, and educational models-systems that support rapid change and are coordinated through well-designed, cooperative approaches rather than centralized control.Figure 1 illustrates aspects of these relationships.</p>
<p>Researchers</p>
<p>AI</p>
<p>Community &amp; Workforce Development</p>
<p>Team-based Science</p>
<p>Co-design</p>
<p>Figure 1: Toward next-generation ecosystems for scientific computing: As motivated by the needs of teambased science in an AI-driven future, we must advance software, cross-disciplinary collaboration, and pedagogy (outer ring of this figure) through co-design, all while developing workforce and community.This work demands strong collaboration among researchers (in science domains, applied math, computer science, HPC, AI, etc.), software developers, stakeholders, and computing facilities (inner rings of this figure).</p>
<p>Potential impact</p>
<p>Dynamic computational workflows, encompassing modeling, simulation, data analytics, and emerging AI approaches, as enabled by the instrumental building blocks of scientific software, are key to next-generation science [8,22,23,24,25,26].Ultimately, the promise-and eventual impact-of addressing the urgent challenges introduced above rests on the ability of distributed, cross-disciplinary teams to effectively integrate varied knowledge, perspectives, policies, requirements, and methods toward producing innovative solutions for scientific computing.Figure 2 illustrates the cross-disciplinary collaboration that is typical in advanced computational science-broadly considering simulations in physics, chemistry, materials science, biology, and so on,2 which leverage heterogeneous computing architectures.These scientific codes build on low-level programming models and runtimes, libraries for math and visualization, tools to understand and optimize performance, emerging ML/AI technologies (such as distributed training frameworks, MLOps pipelines, and federated learning systems), application-specific components, and more.Multiphysics simulation requires expertise across science domains, math, CS, and ML/AI, all encapsulated in software Single small team Figure 2: Simulations in advanced scientific computing (including materials science, astrophysics, nuclear energy, biology, engine design, weather prediction, and batteries, as represented by the science images of this figure) require collaboration across science domains, applied mathematics, computer science, ML/AI, and more, where high-quality software is a primary means of encapsulating expertise for use by others.Due to increasing science challenges and complexity, no longer can a single small team (shown on the left-hand side) independently develop all required functionality.Instead, reusable libraries and tools (shown on the right-hand side), developed by teams whose expertise spans across various topics, provide key functionalities that serve many applications.Dashed lines indicate multiple areas of work per person (e.g., the red-colored person in this diagram contributes to application components, development tools, and ML/AI capabilities).</p>
<p>Opportunities for advances via AI technologies abound, including code generation, automated design of experiments, AI agents, and real-time analysis during simulations.For example, a materials science team using AI to predict crystal structures needs seamless integration between calculations of density functional theory, ML training, and HPC scheduling, while building on programming models, systems tools, and so on.Likewise, an astrophysics team using HPC and AI to study supernova explosions needs a multiphysics simulation code with shock hydrodynamics and nuclear burning, which might rely on math libraries for adaptive mesh refinement and on AI for parameterized physics models such as flame and equation of state.</p>
<p>Complexity arises due to heterogeneity across and within the various components constituting nextgeneration scientific computing ecosystems.This heterogeneity exists at many levels, for example in (1) hardware (e.g., CPUs, AI accelerators, quantum); (2) software (e.g., multitude of programming languages, environments, functionalities); (3) algorithms (e.g., mathematical, computer science, statistical, AI, domain science specific); (4) precision of operations (e.g., mixed vs. double precision, quantization in AI models); (5) data (e.g., storage, movement, processing, analysis, visualization); (6) computing environments (e.g., leadership computing facilities, federated, cloud, edge, quantum); and (7) the workforce (e.g., researchers, developers and users of software, domain scientists, management, stakeholders) or, more generally, crossdisciplinary teams, which in next-generation scientific computing may eventually include AI agents alongside humans [34,35,36].This report considers the integration of AI into scientific software development as a transformative opportunity to accelerate discovery while addressing long-standing challenges in code quality, sustainability, and accessibility; other documents consider complementary topics (see, e.g., [37,38,39]).AI's promise lies not just in automating routine coding tasks but also in fundamentally reimagining how scientific software ecosystems evolve.</p>
<p>Collaboration is a primary mode of work in scientific computing, yet its potential is only beginning to be fully realized because of the complexity of cross-disciplinary research.The advent of AI, with its potential to influence human activities related to software development and scientific discovery, adds a transformative new dimension to this challenge.Next-generation ecosystems in scientific computing offer the promise of integrating broad domain expertise, methodological approaches, and AI-driven tools to bridge gaps between theoretical principles and practical implementations.These advances can enable more robust, maintainable, and scientifically accurate software, thereby accelerating scientific discovery.</p>
<p>We can build on experiences of innovative programs such as the U.S. Department of Energy's (DOE's) Exascale Computing Project (ECP; [27,40]), Scientific Discovery through Advanced Computing (SciDAC) program [41], and Computational Science Graduate Fellowship (CSGF) program [42,43]; the National Nuclear Security Administration's (NNSA's) Predictive Science Academic Alliance Program [44]; the National Science Foundation's (NSF's) Science and Technology Centers [45] and Engineering Research Centers [46]; Sustainable Research Pathways [47], and others.These initiatives have pioneered bold approaches for crossdisciplinary collaboration at scale and for cultivating both breadth across disciplines and depth within individual fields-countering traditional academic silos, as required for advances in scientific computing.</p>
<p>Socio-technical co-design for next-generation scientific computing</p>
<p>To tackle these urgent, complex, and intertwined challenges, the workshop introduced a novel methodology of socio-technical co-design for scientific computing, illustrated in Figure 3.The concept of co-design is widely used in many fields, referring generally to collaborative approaches to designing solutions by involving multiple stakeholders (especially those who will be using or are impacted by the outcomes) throughout the design process.For example, in the human-computer interaction community, co-design is strongly associated with participatory design.Co-design in HPC and AI has been critical to the design and implementation of contemporary computer architectures, applications, algorithms, and software, considering their interrelationships (as shown in the left-hand side of this figure) [48,49].</p>
<p>Building on these co-design experiences, the workshop introduced the broader approach of sociotechnical co-design, which embraces the intentional and integrated development of both technical components (e.g., software, AI, infrastructure) and social components (e.g., teams, institutions, practices, training) of scientific computing ecosystems.This expanded approach ensures that social and technical dimensions are addressed not in isolation but as tightly coupled elements of a unified, forward-looking strategy.</p>
<p>Socio-technical co-design for future scientific computing</p>
<p>Technical and social co-design interwoven throughout all aspects of work</p>
<p>Traditional co-design for scientific computing</p>
<p>Researchers and R&amp;D</p>
<p>Applications</p>
<p>Human-AI Collaboration</p>
<p>Pedagogy and Training</p>
<p>Community and Workforce</p>
<p>Figure 3: Socio-technical co-design for next-generation scientific computing intentionally interweaves technical and social elements throughout all aspects of work, while closely coupling cycles of R&amp;D innovation between computing technologies and driving applications.By ensuring that technical and social dimensions are addressed as tightly coupled elements of a unified, forward-looking strategy, this holistic approach accelerates transformative impact across wide-ranging application domains.</p>
<p>Report structure.We organize insights from the workshop across three complementary axes that undergird scientific computing and urgently require better understanding and advancement to prepare for AI's growing role in shaping scientific discovery (see Figure 4):</p>
<p>• Software ecosystems for AI in scientific computing • Cross-disciplinary collaboration and AI for scientific software teams • Pedagogy and workforce development in the age of AI Two crosscutting topics permeate all of these:</p>
<p>Software Ecosystems</p>
<p>FOR AI IN SCIENTIFIC COMPUTING</p>
<p>Pedagogy and Workforce</p>
<p>DEVELOPMENT IN THE AGE OF AI</p>
<p>Collaboration and AI</p>
<p>• AI as a catalyst to advance software productivity, collaboration, and pedagogy • Community engagement and capacity building for next-generation scientific computing These domains are deeply interconnected, requiring co-design of technical and social systems.Addressing them holistically presents an opportunity to foster robust and scalable research ecosystems capable of driving scientific discovery in the coming decade.We next delve into these challenges, including crosscuts and interdependencies, and we introduce priority research directions as well as actionable community strategies.</p>
<p>Breaking Down the Challenges</p>
<p>As scientific computing moves toward AI-driven, cross-disciplinary, community-integrated ecosystems, significant challenges arise in aligning technical tools with human needs and institutional capabilities.Sections 2.1 through 2.3 elaborate on challenges in the complementary topics introduced in Section 1, while Section 2.4 discusses crosscutting issues and interdependencies.</p>
<p>Software ecosystems for AI in scientific computing</p>
<p>We define a software ecosystem for AI-enhanced scientific computing as the entire software infrastructure that supports all computational steps involved in scientific discovery, ranging from system software at the lowest level to inference engines at the highest level.The inference engines rely upon data obtained from simulations, observations, archives, and other sources of scientific data.Simulation and analysis tools in turn depend upon middle layers of the software stack such as math and visualization libraries and workflow management tools.Tools to analyze, predict, and optimize performance, energy, and cost of simulations are also essential.The software ecosystems must not only integrate traditional numerical methods and scientific simulations but also accommodate rapidly evolving AI models, data pipelines, and heterogeneous computing environments (e.g., CPUs, GPUs, and quantum systems).While heterogeneity is not new to scientific computing, at present it is growing in complexity and ever-changing, raising questions about CPU-GPU memory transfers, mixed precision training, and quantum-classical hybrid algorithms.Key questions thus revolve around the urgency to identify what will be needed to design, develop, and maintain software that will successfully support next-generation science conducted under heterogeneous, dynamic, and fastevolving environments, while simultaneously building the human enterprise and collaboration protocols.Specific software needs are discussed in the recent Report of the NSF/DOE Workshop on NAIRR Software [11], while software engineering challenges are presented in [20].Creating extensible and sustainable ecosystems will require a strategic blend of technical innovation and community-driven co-design.</p>
<p>Key questions:</p>
<p>• How can we build extensible, traceable, and modular scientific software ecosystems that support both legacy and AI-generated components, and what degree of intrusion into existing code is needed and acceptable?Can AI assist in decomposing monolithic scientific codes into reusable components that support ecosystem integration?How can AI tools help with refactoring, modernization, and documentation of legacy scientific code?</p>
<p>• How can AI maintain scientific correctness while accelerating development?Scientific applications must adhere to physical laws, mathematical principles, and domain-specific constraints.How can we train AI systems to generate code that is scientifically valid and numerically stable?</p>
<p>• What validation frameworks are needed for AI-generated scientific code?Traditional software testing approaches may be insufficient for scientific applications where correctness extends beyond functional requirements to include adherence to scientific principles, uncertainty quantification, and reproducibility across different computational environments.</p>
<p>• How can standards for AI be designed and integrated into software, applications, tools, and reporting to ensure the technologies can be trusted and adopted by different segments of the scientific computing community with relative ease?Scientific challenges and opportunities:</p>
<p>Fragmented ecosystem and interoperability challenges.Scientific software ecosystems consist of wideranging codebases that already struggle with continually changing hardware paradigms and now must also integrate with modern AI frameworks.We face challenges in bridging traditional scientific computing environments with AI-driven development workflows, including deploying trained models in HPC environments and integrating scientific data formats with AI frameworks.Given that data is a critical component of AIenabled scientific computing, and that computational results are often distributed across teams and siloed in storage systems, seamless access to data remains a major barrier.To support effective collaboration, we need standardized mechanisms for data sharing that address access control, provenance, and metadata.The lack of standardized interfaces, data formats, and integration protocols-especially for deeply integrated components like math libraries-further prevents seamless adoption of AI technologies and forces scientists to navigate multiple, incompatible systems.This fragmentation creates inefficiencies, requires manual intervention between simulation and analysis stages, and limits the ability to leverage AI across entire scientific workflows.Additional challenges relate to the need for performance optimization, such as model quantization, pruning, and knowledge distillation, as well as adaptability in dynamic AI landscapes.</p>
<p>Reliability and validation gap in AI-generated scientific code.A critical challenge is the disconnect between AI's current capabilities and the requirements of scientific computing for reliability and replicability [50,51].Unlike general-purpose software, scientific applications must preserve physical principles, maintain numerical stability, and provide reproducible results across different computational environments.Current AI tools often generate syntactically correct code that may violate domain-specific constraints or introduce subtle numerical errors that compound over long simulations.This point is crucial because natural language specification is by its nature imprecise.Traditionally, programmers eliminate artifacts of imprecise specification through iterative verification and debugging.What can replace or accelerate this iterative convergence to trustworthy software in its development cycle remains an open question.This gap is exacerbated by the lack of standardized validation frameworks specifically designed for AI-generated scientific code, making it difficult to systematically assess whether generated solutions maintain both computational correctness [52] and scientific validity.Federated learning introduces additional challenges, including needs for privacy-preserving collaboration across institutions and addressing security threats such as adversarial attacks on scientific AI models and data poisoning.Additionally, human-in-the-loop and continuous learning mechanisms will remain essential for aligning AI-generated software with evolving scientific understanding.</p>
<p>Development methodologies for hybrid modeling/simulation and AI.While it will be impractical to develop entirely new scientific software ecosystems from scratch, reuse of existing software in these new modalities is likely to be challenging.The changes required to code will be deeply invasive; therefore, intrusion-aware design will become critical to minimize disruption when integrating new tools or AI systems into existing software.There is an ongoing debate in the scientific community about when and where traditional numerics should be used, where emerging scientific machine learning (SciML [53]) methods might replace them, and how the two can most effectively complement one other.We urgently need to develop (and adopt) suitable metrics when evaluating SciML methods against traditional numerical methods [54,55].Middleware to assist in the creation and execution of automated computational workflows will continue to be essential but will now also need consideration for agentic AI systems [56].To support scalable infrastructure growth, such advances require ecosystem-level funding models, broad coordination, and consortia involving national labs, academia, and the private sector.</p>
<p>2.2 Cross-disciplinary collaboration and AI for scientific software teams Because the physical world is not divided into specific domains, those who seek to understand it must also eliminate unnatural boundary lines dividing their discourse.Computational science in general, and high-performance computing in particular, have pioneered advances in cross-disciplinary teams combining computing with science, engineering, and mathematical foundations.Such scientific teamwork, in which an interdependent set of group members share a common purpose, performance goals, and mutual accountability, has grown significantly to meet the needs of organizations and tackle formidable problems [57,58,59,60].However, the growing complexity of challenges in scientific computing demands increasing collaboration among researchers and potentially AI agents-where software is a primary means of collaboration.Cross-disciplinary teams in scientific computing face communication and knowledge integration challenges due to diverse vocabularies, standards, and methodologies [60,61].These challenges are associated with the structure and norms of scientific research; the proliferation of data and computational technologies; the complexity of interactions; and the knowledge, skills, and attitudes present in teams.The rapid evolution of AI technologies complicates each of these issues and also introduces a unique set of challenges.A multipronged strategy addressing macro-, meso-, and micro-level factors (see Figure 5) that influence collaboration in scientific computing is of paramount importance; consequently, we structure the remainder of this section according to these factors.</p>
<p>Macro-level factors (science and society): Structures and norms for collaboration</p>
<p>Key questions:</p>
<p>• How can existing funding structures be adapted to encourage greater and optimal interconnectedness in the scientific computing community?What new mechanisms, and possibly new sources for funding, are needed to encourage and maintain collaborations in scientific computing?</p>
<p>• How can existing norms for recognition and credit be adapted to encourage greater and optimal sharing and cross-disciplinary collaboration in the scientific computing community?What novel norms for recognition and credit in cross-disciplinary research are needed to overcome the limitations of siloed, unidisciplinary structures in academia?</p>
<p>• How can standards for AI technologies be designed to facilitate rather than constrain collaboration and innovation?</p>
<p>Scientific challenges and opportunities: The structures and norms that shape or otherwise enable scientific research are often at odds with the approaches and goals that characterize cross-disciplinary collaboration.These include the availability and distribution of funding (often insufficient for the full scope of multi-institutional, cross-disciplinary multiteam systems [60] required for large-scale computing projects).Another macro-level challenge is norms for recognition and credit, which sometimes neglect or minimize the work of various types of contributors in projects, resulting in decreased motivation, satisfaction, and productivity.Reward systems need to adapt accordingly, reflecting a shared value system for all components of the evolving landscape, including the development and maintenance of the underlying supports such as software "plumbing," project management, and communities that support collective needs.</p>
<p>Meso-level factors (organizations, interactions): Models of governance and knowledge management Key questions:</p>
<p>• What forms of governance are most beneficial for achieving fair and defensible decision making in cross-disciplinary collaborations in scientific computing?</p>
<p>• How can these governance models enable effective collaboration, accelerate innovation, and help ensure that all relevant perspectives are represented?What resources and tools are needed to enable the adoption of these governance models in multiteam collaborations?</p>
<p>• What approaches to knowledge management are beneficial for fostering trust, mitigating knowledge loss, and reducing barriers to contribution and collaboration?</p>
<p>Scientific challenges and opportunities: The complexity of collaboration in scientific computing is associated with the number of stakeholders and the differences in their requirements, approaches, and goals.</p>
<p>Effectively managing this complexity is vital for the quality of collaboration processes and outcomes, including communication, coordination, decision-making, problem solving, performance, and knowledge transfer [60].Access to knowledge must become ubiquitous, fair, and democratic, taking advantage of AI and cloud-based approaches.This challenge introduces an opportunity to envision models of governance and knowledge management that support distributed collaboration across institutions, ensuring shared responsibility for next-generation ecosystems in scientific computing.</p>
<p>Micro-level factors (teams and constituent members): Training for expertise and productivity</p>
<p>Key questions:</p>
<p>• How can collaboration among various actors in scientific computing (including national laboratories, universities, and the private sector) be leveraged to design and provide opportunities for training in cross-disciplinary research models and teamwork competencies?</p>
<p>• How can human-centered strategies and AI technologies be designed to support the acquisition of taskwork competencies (e.g., knowledge, skills, and abilities required for writing high-quality software, evaluating and selecting software, modifying and adapting software) as well as teamwork competencies (e.g., knowledge, skills, and abilities required for effective communication, coordination, learning) in scientific computing?</p>
<p>• How can human-centered strategies and AI technologies be designed to minimize workload, reduce miscommunication, increase efficiency, and augment the capabilities of individuals and teams in scientific computing projects?How can human-centered strategies and AI technologies be designed to enhance team formation and team development across multiteam collaborations?</p>
<p>Scientific challenges and opportunities: Cross-disciplinary research teams face significant challenges in communication, which in turn can reduce their capacity for effectively utilizing collaborative member expertise, developing shared knowledge structures (i.e., shared mental models and transactive memory systems), and integrating their varied knowledge [61].Because cross-disciplinary research demands the integration of the varied, specialized expertise present in a team, its members must be adept in communication and exhibit openness toward one other-competencies and dispositions that influence the team's expertise utilization and knowledge integration.Furthermore, individuals in these teams must develop expertise in topics deemed essential for scientific computing (e.g., reproducibility [51]).Both training and research are thus needed on attitudes, behaviors, and cognition that can enhance cross-disciplinary collaboration in the scientific computing community.Also needed are advances in AI-specific collaboration tools, such as shared Jupyter environments, collaborative model development platforms (GitHub for models), and automated code review systems for AI-generated code.For example, domain scientists need to be able to validate AI-generated finite difference schemes without understanding all details of the underlying transformer architecture.</p>
<p>Multilevel factors: Trust in teammates, software, and AI.Achieving an appropriate level of trust in teammates, software, and AI represents an additional significant challenge.Developing and maintaining trust across the macro, meso, and micro levels of collaborative scientific computing requires insightful leadership, time, repeated interaction, and intervention when trust is lost.A challenge is devising alternative means for achieving trust within an ecosystem-minimizing the workload required to develop trust and manage uncertainty associated with information (e.g., by leveraging wisdom of the crowds and collective intelligence).Also needed are trust protocols for AI agents, for example for AI agents to submit computational jobs to leadership-class facilities.</p>
<p>Pedagogy and workforce development in the age of AI</p>
<p>Pedagogy and workforce development in the computing sciences have always been challenging because of the need to understand and communicate ideas that span multiple disciplines.In academia an added challenge has been the difficulty of fitting in cross-disciplinary training in traditional department structures [5,7].Mission-driven research labs have been the natural homes for such efforts.With the advent of AI and growth of computational science-spanning beyond traditional roots in the physical sciences to include life sciences, social sciences, and virtually all aspects of science and society-there is more opportunity and urgency to integrate cross-disciplinary training.Many science and engineering departments will need to extend their curricula to include developing expertise in exploiting powerful computing platforms, including AI, in much the same way they have trained students in experimental methods in the past.</p>
<p>Key questions:</p>
<p>• How do we prepare the scientific workforce for an AI-augmented future?As AI tools become more sophisticated, the roles of domain scientists and software engineers will evolve.Critical questions include the following: What fundamental skills remain essential?What specific AI/ML concepts are essential?How do we maintain critical thinking and domain expertise when AI handles routine tasks?How do we ensure that the next generation can validate and improve upon AI-generated solutions?How can we ensure that AI models and datasets reflect a wide range of perspectives grounded in real-world contexts?</p>
<p>• How can strategies be developed to train and grow workforce communities when the requirements are unknown and rapidly changing?Moreover, how can we broadly enhance and evolve education to foster critical thinking, extend access, expand cross-disciplinary collaboration and communication, and create ubiquitous and democratic access to and implementation of AI for computational science?How can we effectively address community building and workforce development among various stakeholder groups?</p>
<p>• How can we build training catalogs for community-wide use, including all relevant perspectives, so that we can broaden the pool of scientists and engineers engaged in computational science?</p>
<p>Scientific challenges and opportunities:</p>
<p>Workforce development and knowledge transfer crisis.Perhaps the most profound challenge identified in the workshop is the degree of uncertainty in how to prepare the future workforce for the disruptive changes underway in computing for science.As AI tools become more capable, there is growing concern about creating a generation of scientists who can use AI-generated code but lack the fundamental understanding needed to validate, debug, or extend it.This situation creates a paradox: AI tools designed to democratize software development may inadvertently create new barriers to deep scientific understanding.Simultaneously, the rapid pace of AI advancement makes it difficult for educational institutions and training programs to update curricula as technologies and practices change, while the shift toward AI-assisted development raises fundamental questions about what constitutes essential knowledge in the age of artificial intelligence.Building curricula and the workforce for rapidly changing technological ecosystems requires new instructional and workforce development paradigms, with higher reliance on apprenticeship models, including the social (group) learning that is supported by communities of practice [62].</p>
<p>Strategies to bridge across disciplines.The development of integrated, cross-disciplinary curricula in scientific computing has been a slow and uneven process; despite decades of progress, many academic institutions still lack programs in computational science and engineering.As next-generation scientific challenges increasingly demand complex, team-based approaches that span multiple disciplines, it is essential to accelerate efforts to design curricula that bridge scientific domains, computational methods, and technological modalities.To support this shift, there are growing opportunities to leverage cloud-based AI platforms and containerized environments that enable consistent, scalable training across diverse institutional contexts.We also need to incorporate critical professional competencies often missing from current curricula, such as project management, agile development practices, effective communication, and collaborative teamwork.</p>
<p>Gaps in workforce readiness and training.One of the most persistent challenges in computational science is the limited pool of qualified candidates for recruitment-a problem compounded by the scarcity of formal academic programs in the field.Although various approaches have been attempted, they have not fully addressed the underlying gap in training pathways.Apprenticeship models for hands-on learning have shown great promise as a complementary approach to formal instruction.These are typically well-supported by associated communities of practice, where practitioners can continue to share knowledge and tactics in an ongoing, informal manner that supports continuous learning [63].Another promising approach is to shift from solely recruiting individuals with comprehensive subject-matter expertise to also recruiting based on potential, followed by targeted, domain-specific training.However, this strategy introduces a new set of challenges: developing high-quality training content, keeping it up to date with evolving technologies, and delivering it at scale.These tasks require sustained resource investment and strong coordination across the broader scientific and educational communities.</p>
<p>Crosscuts and interdependencies</p>
<p>To fully understand the challenges and opportunities identified in Sections 2.1 through 2.3, it is essential to consider two crosscutting themes that emerged consistently throughout the workshop: (1) the transformative role of AI in improving scientific software productivity, collaboration, and pedagogy; and (2) the central importance of community engagement and capacity building in enabling and sustaining these transformations in the computing sciences.These crosscutting themes intersect with all three core challenge areas and are foundational to building resilient next-generation scientific computing ecosystems.</p>
<p>AI as a catalyst for scientific productivity and sustainability.Artificial intelligence is not just a topic unto itself-it is a force multiplier across software ecosystems, collaboration frameworks, and pedagogical approaches.Section 2.1 discusses the need for AI-integrated software ecosystems that are modular, traceable, and scientifically valid.AI can play a key role in this work by helping to advance software productivity and sustainability, such as enabling intelligent code generation, automating routine tasks, and supporting decision-making across heterogeneous computing environments.AI also presents new challenges in terms of verification, trust, and intrusion-aware integration.AI can promote collaboration by acting as a bridge across disciplinary boundaries.As discussed in Section 2.2, AI can support shared understanding by translating domain-specific terminology, managing distributed workflows, and facilitating knowledge retention.AI systems can also act as teammates-assisting with code reviews, optimizing team formation, and recommending learning pathways.These roles require careful governance to ensure they support rather than hinder team cohesion and trust.</p>
<p>As explored in Section 2.3, AI can also transform pedagogy by offering new modalities of instruction, such as AI tutors, generative feedback, and adaptive curriculum planning.However, these benefits must be balanced against the risk of over-reliance, which could erode critical thinking and domain expertise.AI-enhanced education systems must therefore be grounded in robust, fair frameworks and designed to complement (not replace) human mentorship and community support.</p>
<p>Community engagement and capacity building as a strategic imperative.As AI transforms the technical foundations of scientific computing, the long-term success of that transformation will depend equally on the strength of the communities that adopt, adapt, and sustain it.Section 2.1 highlights that successful integration of AI into software ecosystems requires more than technical proficiency-it demands fluency in interdisciplinary thinking, sound judgment, and collaborative practices.Building this capacity across communities is essential for maintaining and evolving shared software infrastructure.</p>
<p>As discussed in Section 2.2, the human dimension of collaboration is central.Scientific progress increasingly depends on teams with complementary skills, diverse perspectives, and broad participation.Capacitybuilding strategies-such as training programs, fellowships, internships, apprenticeships, and institutional partnerships-should be designed to foster cross-disciplinary fluency, resilience, and leadership.Community norms, as well as opportunities for professional development, must evolve to recognize and reward all contributors, including those in roles that have historically been undervalued.</p>
<p>Section 2.3 underscores the need to rethink education and community building considering these dynamics.Curriculum innovation, community-led training models, and broad access to computational and human resources are critical.Well-supported communities create networks of practice that encourage and reward innovation and different perspectives, share lessons learned, and mentor the next generation of scientists, developers, and educators.</p>
<p>Together, these two crosscutting themes-AI to advance scientific productivity and sustainability, and community engagement with capacity building for next-generation scientific computing-represent both the engine and the scaffolding of future scientific computing.They are deeply interdependent: AI systems must be designed, deployed, and governed by engaged, well-supported communities; and those communities must be equipped with the tools, practices, and shared knowledge that AI can help accelerate.Advancing one without the other will limit impact.Investing in both will accelerate progress and ensure that nextgeneration ecosystems for scientific computing are not only more powerful, but also widely accessible, trusted, and sustainable.</p>
<p>Required Research Directions and Community Actions</p>
<p>Considering the challenges and opportunities outlined in Section 2, the workshop identified research directions and community actions needed for accelerating progress in next-generation scientific computing.As discussed in Sections 3.1 through 3.3, each priority research area combines practical needs with aspirational goals, aiming to foster a productive interplay between human insight and AI capabilities while maintaining a strong foundation of scientific rigor and community engagement.The recommended research directions emphasize key design principles: modularity, interoperability, trustworthiness, and extensive participation.The intent is to support both near-term implementation and long-term sustainability of emerging software and collaboration models for scientific computing.Section 3.4 outlines strategic community actions that are essential for supporting the software ecosystems, collaborative teams, and educational innovations discussed throughout this report.Taken together, the priority research directions and community actions define a comprehensive agenda for building thriving, resilient, and forward-looking scientific computing ecosystems.</p>
<p>Software ecosystems for AI in scientific computing</p>
<p>Next-generation scientific computing demands a radical rethinking of how software ecosystems are designed, developed, and sustained.As AI becomes more integrated into scientific workflows, software infrastructure must evolve to support new modes of discovery, while maintaining the rigorous standards of scientific correctness, reproducibility, and performance.This section identifies research priorities for building AI-integrated scientific software ecosystems that are modular, trustworthy, and adaptable.The topics below reflect a blend of foundational challenges and forward-looking opportunities, ranging from infrastructure and interoperability, to AI-assisted code generation, to robust validation methods that incorporate community knowledge and formal guarantees.Taken together, these efforts can help create a robust, adaptive foundation for software ecosystems that meet the scale, complexity, and scientific integrity required for the AI-powered future of discovery.</p>
<p>Software and infrastructure for next-generation science.Existing software infrastructure, although rich in abstractions, libraries, and tools, is inadequate for the multipronged challenges of next-generation scientific applications.The growing demands of high-fidelity models and the fragmentation of system software across AI-driven hardware platforms have created major obstacles for scientific teams.Libraries and tools may be underutilized because of the complexity of manually navigating multiple packages, and most science teams cannot feasibly manage the complexity of hybrid HPC/AI environments without new modes of support.Similar software challenges were articulated in the National Artificial Intelligence Research Resource (NAIRR) Software Workshop [11].</p>
<p>A promising direction is to design software substrates that enable AI/ML and traditional simulation codes to interoperate scalably and adapt to evolving workflows, platforms, and algorithms.At present we have little understanding of what such substrates might look like, but we can begin with research to determine and minimize the intrusiveness required to connect legacy scientific software with new AI tools, coordinate multiple coexisting ecosystems (e.g., AI agents interfacing with scientific tools), and analyze how software components interact in real workflows.Of particular importance is exploring the use of AI agents in end-to-end scientific code development workflows, including interactions with math libraries, visualization tools, debuggers, job schedulers, and HPC performance analysis tools.Key areas of research include energy-efficient implementations of numerical and AI methods, tools for software usage analytics, AI-assisted debugging, and strategies for hybridizing traditional solvers with SciML methods.Also needed are metrics to evaluate emerging SciML approaches against traditional numerical methods.</p>
<p>Development of science-aware AI code generation systems.</p>
<p>A key research priority is the development of AI systems designed and trained specifically for scientific computing contexts.This requires more than adapting general-purpose LLMs; it calls for domain-specific training on high-quality scientific codebases, the incorporation of physics-informed constraints, and the use of validation datasets that emphasize correctness as well as functionality.Research should focus on developing hybrid approaches that combine traditional symbolic reasoning with neural methods, enabling AI systems to respect physical laws, dimensional analysis, and numerical stability requirements.Promising techniques include physics-informed neural networks (PINNs), neural operators, and differentiable programming, which offer pathways for AI-generated code to honor physical laws, conserve energy, and maintain numerical stability.Systems must be capable of constrained code generation (e.g., ensuring energy conservation in molecular dynamics) and must integrate domain-specific knowledge graphs that encode scientific principles.Formal verification methods tailored to scientific applications are also essential.Key components include training datasets curated for scientific applications with verified correctness and development of specialized architectures that can reason about mathematical relationships and physical constraints.This research direction should also investigate methods for uncertainty quantification in AI-generated scientific code, enabling users to understand confidence levels and potential error propagation.We also need advances in AI interpretability, such as understanding AI decisions in safety-critical scientific applications.To integrate existing code into new development, research is needed on AI tools that can generate, refactor, and modernize scientific code while ensuring correctness and maintainability.Work is needed to use AI to extract knowledge from existing codebases and generate useful, accurate documentation for end users and contributors.AI evaluations of code bases can also be used to inform teams and leaders of risks in a code base (fragility, limited testing, etc.) and to help teams make decisions about rewriting and refactoring.We must also explore systems where human developers interactively train AI models in context, for example, by correcting recommendations.</p>
<p>Community-integrated validation and continuous learning frameworks.Given the critical importance of correctness in scientific software, research is needed to develop validation frameworks that leverage community knowledge, expert input, and formal guarantees.These systems should support automated generation of test cases grounded in scientific principles, including testing against known analytical solutions where available.Techniques such as property-based testing and metamorphic testing are especially valuable for scientific domains, enabling validation of AI-generated code even in the absence of exact reference solutions.For example, fluid dynamics code produced by AI must satisfy divergence-free velocity fields and conserve mass properties that can be checked independently of a specific output.Research should also explore adaptive "living" validation systems that evolve with scientific understanding and incorporate domain expertise as part of the AI training feedback loop.Peer review mechanisms tailored to code generation could facilitate expert vetting of outputs and structured incorporation of corrections into model refinement.These systems may include code that is validated against observations and experiments that cannot be replicated.The use of formal methods for system correctness [64] and the verification of implementations of numerical methods [65], as well as AI methods [66,67,68], is needed.In addition, as AI-generated code and integrated workflows become more complex and widely adopted, research should address emerging security concerns, including software vulnerabilities and ecosystem-level threats.These approaches should complement continuous integration and testing practices, forming a robust set of validation methods that can sustain accuracy, trust, and progress as AI becomes increasingly integrated into scientific computing.</p>
<p>Cross-disciplinary collaboration and AI for scientific software teams</p>
<p>The challenges of next-generation scientific computing demand teams that can operate across disciplinary boundaries, institutional cultures, and evolving technological landscapes.AI adds a new dimension to this complexity, both as a source of capability and as a potential collaborator.Research in this area must explore how scientific teams can effectively integrate AI systems into their workflows, while maintaining human creativity, trust, and scientific rigor.This includes developing new frameworks for trust, formalizing human-AI team roles, and applying lessons from prior collaborations in science, social science, and the private sector to guide effective engagement throughout the software lifecycle.</p>
<p>A multilevel model of trust: Teams and technology.Trust-foundational to effective collaboration-has been studied extensively as a concept; rich literature exists describing its presence in teams, differences between trust and distrust, and its application to technologies including those endowed with autonomy [69,70,71].A multilevel, dynamic model of trust is needed to guide cross-disciplinary teams working in hybrid HPC/AI environments.This includes constructs such as propensity to trust, perceived trustworthiness, trust in AI, trust in automation and automated systems, transparency, and reliance.Such a model must account for interactions across individuals, teams, and technologies, helping uncover opportunities to build and maintain trust, while also reducing the likelihood of cascading failures caused by misplaced trust or unaddressed uncertainty.</p>
<p>Roles for AI: Teammate, trainer, tool.As AI becomes increasingly embedded in scientific research, clarifying its roles within and across teams becomes essential.We must examine modes of human-AI teaming that augment human creativity and decision-making, especially in scientific code development and debugging.For example, AI-assisted collaboration tools could assist with translation between domain vocabularies (such as between scientific notation used by physicists and programming constructs familiar to research software engineers).To effectively integrate AI in teams and mitigate the likelihood of negative outcomes associated with the adoption of AI, research is needed to formalize and classify the various forms of taskwork and teamwork [60] in scientific computing.Doing so could enable, for example, the development of an adaptive taxonomy of roles and tasks for AI that is informed by the capabilities and limitations of the technology, the needs of the team, and the particulars of the domain.This configurable knowledge base could guide team design, risk management, and human-AI interaction strategies across the software lifecycle.</p>
<p>Effective engagement within cross-disciplinary teams.We must draw on insights from past scientific computing projects and other endeavors to foster environments that support extensive participation and cross-disciplinary success in next-generation computational science [61,72,73].This includes updating institutional policies and incentives to recognize collaborative work [74] and embedding communication norms [75], governance structures, and knowledge-sharing practices that welcome contributions from a wide range of stakeholders throughout the entire software lifecycle-including funding acquisition; software design, development, and testing; developer and user (re)training; and use and maintenance of the software itself to conduct science.Efforts to codify best practices, define success metrics, and incentivize adoption are essential for building resilient, high-performing teams for the AI-augmented scientific future.</p>
<p>Pedagogy and workforce development in the age of AI</p>
<p>A perennial shortage of well-qualified scientists and engineers with the breadth of knowledge needed for today's and tomorrow's scientific computing underscores the urgent need for new pedagogical approaches, training programs, and workforce strategies.These efforts must support the development of crossdisciplinary skills, incentivize and promote ways to integrate different methodologies, ease onboarding into complex research projects, and enable continuous learning to keep pace with rapid technological evolution.</p>
<p>Workforce development and human-AI collaboration models.Research is urgently needed to understand how workforce development for scientific computing must evolve in AI-augmented environments.This includes empirical studies of how AI tools influence learning, skill development, and scientific reasoning among students and researchers at different career stages.Key questions include the following: Which fundamental skills remain essential when AI handles routine coding tasks?How can pedagogical approaches combine AI assistance with deep understanding of scientific principles?How should assessment methods evolve to evaluate comprehension in AI-assisted contexts?This research should also explore new models of human-AI collaboration in scientific software development, including optimal division of labor, interface design for effective teaming, and strategies for maintaining human expertise and critical thinking while leveraging AI's computational advantages.</p>
<p>Cross-disciplinary curriculum design.Educational programs must integrate AI, software engineering, and scientific domain knowledge-from K-12 through graduate training.Identifying key competencies (e.g., trustworthy and fair AI, reproducibility, teamwork) is essential for preparing future software de-velopers in scientific fields.Curriculum development must become more flexible and modular, enabling faster iteration and responsiveness to the changing landscape.This includes fostering partnerships across academia, national labs, and the private sector.Professional development programs should also address cross-disciplinary collaboration skills and provide transformational hands-on experiences that illustrate the impact and excitement of teamwork in scientific computing, while fostering collaborative environments that respect and adapt to different domain cultures.</p>
<p>Innovative curricula and programs incorporating socio-technical co-design.New instructional models are needed that emphasize project-based learning, especially through collaborations among the private sector, academia, and national laboratories.These models should extend beyond STEM domains to include the humanities and other relevant disciplines, ensuring a broader understanding of scientific and societal impact.The models should also include thoughtful integration of career pathways to support entry at multiple points, meeting people where they are, rather than requiring people to enter with a specific set of pre-defined prerequisites.In addition, we should capture lessons from successful efforts.For example, the collaboration model adopted by the Exascale Computing Project, in partnership with DOE leadership computing facilities, was highly successful in launching the exascale era by co-creating scientific applications, human capital, and a robust scientific software ecosystem.Not only did ECP bridge research and computing facilities in a co-design approach, ECP included early attempts at socio-technical co-design when it added talent development through the Sustainable Research Pathways [76] program and training pipelines through the Center for Scientific Collaboration and Community Engagement (CSCCE) [77].</p>
<p>Community and workforce success metrics.Clear, ambitious metrics are needed to measure progress in building the HPC+AI workforce.For example, a pilot program to advance the workforce for next-generation scientific computing could aim to increase the talent pool by at least 1,000 people over five years, averaging more than 200 new and upskilled contributors annually who are equipped to tackle emerging challenges in scientific software ecosystems.Well-defined metrics can help align national efforts, guide strategic investments, and ensure that workforce development keeps pace with evolving technical demands.</p>
<p>Required community actions</p>
<p>While technical research is a key driver of innovation in scientific computing, the workshop emphasized that progress in foundational scientific software also relies on sustained collaboration across institutions, disciplines, and sectors.Building trustworthy, capable, and sustainable software ecosystems requires more than technical breakthroughs-it calls for strategic coordination, shared infrastructure, supportive policies, and workforce investment.Various organizations, projects, and foundations-such as the Consortium for the Advancement of Scientific Software, High Performance Software Foundation, IDEAS project, NumFO-CUS foundation, Research Software Alliance, Software Sustainability Institute, and US Research Software Engineering Organization-are making meaningful strides in shifting the cultural and structural foundations of scientific software [78].Raising awareness of these efforts and contributing to them-whether directly or through complementary action-will benefit the broader scientific community.Much work remains to be done, however, particularly in advancing high-impact partnerships and addressing persistent gaps in infrastructure, recognition, and career paths.</p>
<p>Strategic public-private partnerships.Of the many strategies discussed, this emerged from the workshop as a central priority.Stronger collaboration among academic institutions, national laboratories, and the private sector is essential to the future of AI-enabled scientific software.These partnerships must go beyond short-term research goals to support co-designed solutions that align with long-term scientific needs and technological shifts.The private sector includes both commercial developers of computing technologies-such as hardware manufacturers, cloud service providers, and AI platform companies-and major industrial users of scientific software.Companies across the automotive, aerospace, energy, manufacturing, and materials sectors (e.g., Ford, GE Research, Boeing, and others) depend on high-performance computing and simulation to support product design, optimization, and predictive modeling.Their engagement brings valuable application-driven insights, real-world performance requirements, and opportunities to accelerate the translation of foundational advances in computing into practical tools.Sustained coordination is needed to shape not only the technical requirements of software tools but also the practices and training ecosystems that support their adoption.Joint efforts might include cross-sector learning communities, affinity groups, shared training initiatives, and advisory partnerships that bring together HPC practitioners, AI developers, and domain scientists.Of particular importance is the need to align the rapidly evolving commercial AI and hardware landscapes with the long-term needs of the scientific community.These shared efforts play a vital role in maintaining national competitiveness, advancing research, and ensuring that science, engineering, and manufacturing progress together for long-term socioeconomic resilience.</p>
<p>Community engagement and broad-access infrastructure.As scientific software projects grow in scale and complexity, they require engagement models that foster trust, transparency, and shared ownership across teams and institutions.Building strong, multi-institutional collaborations depends on clear frameworks that support distributed leadership and far-reaching participation.Key strategies include implementing contributor agreements, project charters, and open governance models that reflect a wide range of stakeholder voices.Equally critical is sustained effort to ensure extensive, fair access to infrastructure and participation.This means funding and scaling infrastructure projects that lower barriers for under-resourced institutions and communities-such as through low-cost cloud access, peer mentoring networks, and national initiatives.</p>
<p>Recognition, incentives, and long-term community investment.The sustainability of scientific software ecosystems depends on valuing and supporting the full range of essential contributions, including development, documentation, user support, training, maintenance, and community stewardship.These roles must be reflected in hiring, promotion, and funding decisions and reinforced through long-term investment strategies.Stable, visible support for these roles strengthens the resilience of the overall ecosystem and builds the capacity needed for lasting progress.</p>
<p>Responsible innovation guidelines and fair AI.Advancing AI-driven scientific discovery without oversight regarding fairness risks opening a Pandora's box of unintended consequences.Community-wide guidelines for the responsible integration of AI are essential and must be treated as a top priority.These guidelines should address algorithmic transparency, data governance, and reproducibility, while also establishing norms to prevent bias, protect users, and anticipate unintended consequences.A key part of this effort is ensuring that datasets are complete and representative of the full range of individuals and perspectives they aim to serve.Responsible innovation must remain a core design principle as both tools and teams evolve.These policy and collaborative efforts are not secondary to technological innovation-they are foundational.By prioritizing strategic partnerships and reinforcing them with community-driven practices, institutional reforms, and long-term investment, while considering the long-term needs of both the nation's research and industrial sectors, we can address the evolving demands of software-and AI-driven science.Ultimately, this integrated approach will foster ecosystems where technological breakthroughs and social progress advance hand in hand, reinforcing one another for a more innovative, responsible, and resilient future.approach to fully harness community, software, and AI for cross-disciplinary team science.This means not only developing new software and hardware but also creating systems of practice-collaboration norms, training models, and governance strategies-that evolve with the technology.We launched a series of workshops to begin a dialogue in the computational science community and other related disciplines on how to confront and resolve these challenges.This report summarizes the outcomes of the first workshop in the series, representing the first step in a multiyear effort to understand how to co-design next-generation ecosystems for scientific computing.We envision ecosystems that are not only technologically advanced but also grounded in shared responsibility, fair practices, and a renewed commitment to bold, visionary science.</p>
<p>The workshop identified challenges and opportunities across three core domains: software infrastructure, cross-disciplinary collaboration, and pedagogy.Two crosscutting themes emerged: the transformative role of AI and the foundational importance of workforce and community development.Addressing these requires coordinated action on several fronts: advancing modular AI-integrated software; developing practices and tools that foster effective cross-disciplinary collaboration; and reimagining training for hybrid human-AI environments.Equally critical is aligning institutional policies, governance models, and community norms to ensure the long-term sustainability and growth of scientific software ecosystems and communities.Pilot projects can catalyze progress in these areas, with priorities evolving over time:</p>
<p>• Near-term (1-2 years): Launch pilot programs on hybrid AI/HPC software infrastructure as well as cross-disciplinary collaboration and pedagogy for AI-driven scientific computing, establish responsible AI guidelines, and prototype public-private partnerships.</p>
<p>• Mid-term (3-5 years): Explore scaling approaches for modular AI/HPC software ecosystems, launch workforce training curricula, expand community development, implement and evaluate several institutional policy reforms.</p>
<p>• Long-term (5+ years): Develop and evaluate globally networked ecosystems for AI-driven scientific computing, explore frameworks for community governance at scale, seek ways to integrate AI agents as active research collaborators.</p>
<p>Evolving next-generation scientific computing ecosystems, with many technical and social requirements, may seem daunting.However, the challenges also bring opportunities to learn from past successes and plan for agile approaches to grow communities and workforces poised to evolve alongside the changing landscape of HPC technologies and AI.Times of great change are marked by both enormous promise and significant uncertainty.Meeting this moment will require not just incremental improvements but bold, risktolerant thinking.Just as the dawn of quantum physics revolutionized our understanding of the universe, today's scientific landscape demands a return to that spirit of curiosity, invention, and courage to chart new waters.Human ingenuity-coupled with emerging AI capabilities-must be placed at the center of scientific progress.By intentionally interweaving technical and social components in next-generation scientific computing, we can create feedback loops that accelerate both scientific discovery and real-world impact.</p>
<p>A Glossary</p>
<p>This report aims to communicate across a broad community, including experts in high-performance computing, AI, compuational science, social and cognitive science, team science, community and workforce development, and related topics.To support our goal of communicating clearly across disciplines, this glossary defines terms as used in this report.</p>
<p>• AI for science: The next generation of methods and scientific opportunities in computing, including the development and application of AI methods (e.g., machine learning, deep learning, statistical methods, data analytics, automated control, and related areas) to build models from data and to use these models alone or in conjunction with simulation and scalable computing to advance scientific research [13].</p>
<p>• Cross-disciplinary: Collaboration among multiple disciplines toward a shared objective [60].</p>
<p>• Ecosystems for scientific computing: Dynamic socio-technical systems made up of people, technologies, infrastructure, institutions, workflows, and cultural practices that collectively support the development and evolution of scientific computing.[page 2 of this report]:</p>
<p>• Science of team science: Research area in which scholars from various disciplines, such as psychology, organizational sciences, sociology, communication, and philosophy, contribute conceptually and empirically to understanding how science teams are organized and work together, how to best measure their effectiveness, and the implications of individual differences in team science [60].</p>
<p>• Socio-technical co-design for scientific computing: The intentional and integrated development of both technical components (e.g., software, AI, infrastructure) and social components (e.g., teams, institutions, practices, training) of ecosystems for scientific computing, ensuring that social and technical dimensions are addressed not in isolation but as tightly coupled elements of a unified, forwardlooking strategy [page 4 of this report].</p>
<p>• Taskwork: Activities associated with achieving a team's goals [60].</p>
<p>• Teamwork: Interactions among team members that are essential for effective collaboration [60].</p>
<p>• Team science: Collaborative, interdependent research conducted by more than one individual [60].-Fostering collaborative environments that tap into the strengths of a broad workforce, bridging technical and community-building efforts.</p>
<p>Acronyms</p>
<p>Workshop Objectives</p>
<p>We aim to shape a forward-looking plan for the future of team-based software in scientific computing, along with actionable strategies to bring it to life, all while cultivating a vibrant community.Key areas of focus-considering emerging AI technologies and the needs of cross-disciplinary research-will include:</p>
<p>• Understanding scientific software practices: Building a shared understanding of methodologies for team-based software in HPC, explicitly integrating technical and community perspectives.• Characterizing roles and success factors: Identifying critical roles in scientific software teams and community-based success factors that sustain effective software development and user engagement.• Identifying challenges and opportunities: Examining gaps (both technical and community-driven) that hinder team effectiveness and highlighting opportunities to address emerging research needs.• Envisioning the future: Considering bold ideas for next-generation scientific software, emphasizing synergy between technical solutions and community dynamics.• Fostering community development: Developing actionable steps to strengthen the workforce and build a vibrant software community prepared to meet urgent challenges in HPC and AI-driven scientific computing.</p>
<p>By intentionally blending technical and community factors, the workshop aims to shape a future where thriving, cross-disciplinary communities drive the next wave of scientific discovery, with high-quality scientific software as a keystone of sustained collaboration and scientific progress.</p>
<p>Workshop Outcomes</p>
<p>Participants will map the landscape of team-based scientific software, characterize key roles and success factors, prioritize focus areas, and develop strategies to shape the future of scientific computing.These integrated insights-to be documented in a post-workshop report-will inform future efforts and may drive changes in perspectives, policies, and practices throughout the scientific computing community.This workshop marks the first of a three-year series dedicated to strengthening team-based scientific software in an AI-driven future, with each year incorporating the co-design framework to ensure that both technical and community needs are addressed:</p>
<p>• Year 1 emphasis: Identifying and understanding challenges, gaps, and opportunities • Year 2 emphasis: Developing and evaluating strategies and progress • Year 3 emphasis: Coordinating ecosystem-wide advancements that meld technical solutions and community-building By intentionally weaving community considerations into technical discussions-and vice versa-this threeyear series will continuously expand the community base and help to advance next-generation scientific discovery.</p>
<p>FORFigure 4 :
4
Figure 4: Report structure.</p>
<p>Figure 5 :
5
Figure 5: Team challenges span throughout macro-, meso-, and microlevel factors.</p>
<p>•</p>
<p>AI: Artificial intelligence • HPC: high-performance computing • CPU: Central processing unit • GPU: graphics processing unit • STEM: science, technology, engineering, and mathematics • NSF: National Science Foundation • DOE: U.S. Department of Energy -Addressing how varied perspectives and AI can accelerate development, use, performance, and impact of scientific software.-Embedding community-based feedback loops to align AI-driven tools with real-world needs and foster broader engagement.• Community and workforce development -Accelerating strategies to cultivate future-generation R&amp;D teams in the computing sciences.</p>
<p>Contents Executive Summary: Reimagining Scientific Computing in the Age of AI ii 1 Significant Challenges Facing Computational Science 1.1 Workshop objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.2 Potential impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.3 Socio-technical co-design for next-generation scientific computing . . . . . . . . . . . . . . 2 Breaking Down the Challenges 2.1 Software ecosystems for AI in scientific computing . . . . . . . . . . . . . . . . . . . . . .2.2 Cross-disciplinary collaboration and AI for scientific software teams . . . . . . . . . . . . .2.3 Pedagogy and workforce development in the age of AI . . . . . . . . . . . . . . . . . . . .2.4 Crosscuts and interdependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Executive Summary: Reimagining Scientific Computing in the Age of AI4 Conclusion and Next StepsAcknowledgmentsReferencesA GlossaryB Workshop DescriptionC Workshop ParticipantsD Workshop Agenda
3 Required Research Directions and Community Actions 3.1 Software ecosystems for AI in scientific computing . . . . . . . . . . . . . . . . . . . . . .3.2 Cross-disciplinary collaboration and AI for scientific software teams . . . . . . . . . . . . .3.3 Pedagogy and workforce development in the age of AI . . . . . . . . . . . . . . . . . . . .3.4 Required community actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>3
Programming models and runtimesData analytics and visualization Systems toolsML/AI Math librariesDevelopment toolsApplication Components
Appendices B, C, and D provide the workshop description, list of participants, and agenda, respectively.
For example, see the wide range of scientific applications within the recent DOE Exascale Computing Project (ECP)[27,28].
For example, E4S[29] provides a foundational HPC-AI software ecosystem for science[30], including ECP libraries and tools[31,32,33] as well as popular AI packages.
Scientific computing is entering a new era-driven by the integration of AI, the complexity of crossdisciplinary collaboration, and the growing demand for scalable, sustainable software ecosystems. As introduced in Figure3, advancing next-generation scientific computing requires a socio-technical co-design
AcknowledgmentsThis workshop was partially supported by the U.S. Department of Energy (DOE) Office of Science Distinguished Scientist Fellows Program.We especially thank our DOE contacts: Hal Finkel and David Rabson, DOE Office of Advanced Scientific Computing Research (ASCR).We thank Katie Antypas (National Science Foundation, Office of Advanced Cyberinfrastructure), April Hanks (Team Libra) and Christina Mihaly Messina (SNL) for insightful contributions to workshop discussions on next-generation ecosystems in scientific computing, which helped shape the ideas conveyed in this report.We are grateful to Suzanne Parete-Koon (ORNL) for detailed feedback on the document; her suggestions improved the precision and perspective of the report.We thank Gail Piper for editing this manuscript.Argonne is a U.S. Department of Energy laboratory managed by UChicago Argonne, LLC under contract DE-AC02-06CH11357.The Laboratory's main facility is outside Chicago, at 9700 South Cass Avenue, Lemont, Illinois 60439.For information about Argonne and its pioneering and technology programs, see www.anl.gov.B Workshop DescriptionThe 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science engaged over 40 cross-disciplinary experts in Chicago, IL from April 29 to May 1, 2025, to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems.Workshop ChargeThe HPC community has long been a leader in advancing scientific discovery to new frontiers.High-quality scientific software-which encapsulates expertise across disciplines for use by others-is a primary means of sustained collaboration and scientific progress.Motivated by urgent issues raised in recent community reports on the state of scientific software development and AI for science, energy, and security, the workshop will bring together cross-disciplinary experts-in HPC, AI, computational science, applied math, computer science, research software engineering, cognitive and social sciences, and community development-to identify challenges, prioritize gaps, and explore opportunities that will shape next-generation ecosystems for scientific computing.The workshop will follow a co-design methodology that intentionally weaves together topics in team-based scientific software, AI in scientific computing, and community/workforce development, thereby ensuring a holistic approach.Areas of emphasis.The workshop's goal is to assess and transform team-based scientific software, with emphasis on building ecosystems to address the needs of next-generation research in scientific computing, while advancing emerging AI technologies.We will integrate technical and community considerations throughout discussions on:• Software and next-generation science -Exploring how new scientific frontiers and heterogeneous computing architectures require new approaches for software and workforce development.-Incorporating community-building strategies to ensure that new technical solutions reflect the perspectives and expertise of all stakeholders representing a broad swath of backgrounds.• Software ecosystems for AI in scientific computing -Mapping pathways to create robust, scalable software ecosystems that catalyze AI-driven discoveries in HPC contexts and produce increased innovation.-Developing more robust AI tools and resources for scientific computing that address current gaps and include wide-ranging perspectives.• Team-based software and cross-disciplinary research -Identifying roles, methodologies, and best practices for team-based scientific software.-Emphasizing community co-design, where software solutions evolve through iterative dialogue among scientists, developers, and end users.• AI for scientific software productivity and sustainabilityC Workshop ParticipantsOrganizing Committee
Anthropic Economic Index report: Uneven geographic and enterprise AI adoption. Anthropic, Sept 2025</p>
<p>ASCR@ 40: Highlights and Impacts of ASCR's Programs. Bruce Hendrickson, Buddy Bland, Jackie Chen, Phil Colella, Eli Dart, Jack Dongarra, Thom Dunning, Ian Foster, Richard Gerber, Rachel Harken, 10.2172/16318122020US DOE Office of ScienceTechnical report</p>
<p>Future directions for NSF advanced computing infrastructure to support U.S. science and engineering in 2017-2020. William D Gropp, Robert J Harrison, 2016National Academies Press</p>
<p>National Science Foundation Advisory Committee on Cyber-Infrastructure. David Keyes, Valerie Taylor, Task Force on Software for Science and Engineering. 2011final report</p>
<p>Research and education in computational science and engineering. U Rüede, K Willcox, L C Mcinnes, H De Sterck, 10.1137/16M1096840SIAM Review. 6032018</p>
<p>Transforming Science Through Cyberinfrastructure: NSF's Blueprint for a National Cyberinfrastructure Ecosystem for Science and Engineering in the 21st Century: Blueprint for Cyberinfrastructure Learning and Workforce Development. 2021NSF Office of Advanced Cyberinfrastructure, CISE</p>
<p>Bruce Hendrickson, SIAM Task Force Report: The Future of Computational Science. 2025</p>
<p>Jonathan Carter, John Feddema, Doug Kothe, Rob Neely, Jason Pruet, Rick Stevens, 10.2172/1986455Advanced Research Directions on AI for Science, Energy, and Security: Report on Summer 2022 Workshops. 2023</p>
<p>A Michael, David E Heroux, Lois Curfman Bernholdt, John R Mcinnes, Daniel S Cary, Elaine M Katz, Damian Raybourn, Rouson, 10.2172/1846009Basic Research Needs in The Science of Scientific Software Development and Use: Investment in Software is Investment in Science. 2023Report of the DOE Advanced Scientific Computing Research Workshop</p>
<p>Michael E Papka, K Dhabaleswar, Ilkay Panda, Wahid Altintas, Ewa Bhimji, Murali Deelman, Nicola Emani, Daniel S Ferrier, Lois Curfman Katz, Anita Mcinnes, Feiyi Nikolich, Jim Wang, Willenbring, Final Report of the 2024 NSF/DOE Workshop on NAIRR Software, 2025. US National Science Foundation Report. </p>
<p>Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration. 10.17226/269162023The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine</p>
<p>. Rick Stevens, Valerie Taylor, Jeff Nichols, Arthur Barney Maccabe, Katherine Yelick, David Brown, 10.2172/1604756AI for Science. DOE Report. 2020OSTI</p>
<p>B Chapman, H Calandra, S Crivelli, J Dongarra, J Hittinger, S Lathrop, V Sarkar, E Stahlberg, J Vetter, D Williams, 10.2172/1222711DOE Advanced Scientific Advisory Committee (ASCAC): Workforce Subcommittee Letter. 2014</p>
<p>Roscoe Giles, Transitioning ASCR after ECP. 2020</p>
<p>Supercharging America's AI Workforce. 2024DOE Office of Critical and Emerging Technologies</p>
<p>Response to the ASCAC charge to review the Computational Science Graduate Fellowship program. Prasanna Balaprakash, Tina Brower-Thomas, Jennifer Gaudioso, Susan Gregurick, William D Gropp, Arthur Maccabe, Irene Qualters, Mark E Segal, Valerie Taylor, David Torres, Stefan M Wild, 2025U.S. Department of Energy, ASCACReport</p>
<p>Vision for American Science and Technology (VAST): Unleashing American Potential. Sudip Parikh, AAAS Task Force. 2025</p>
<p>Post Exascale Software in the ASCR Facilities Ecosystem. Saswata Hier-Majumder, Wahid Bhimji, Brandon Cook, Graham Heyes, Kalyan Kumaran, John Macauley, Bronson Messer, Philip Roth, Jiachuan Tian, Brice Videau, December 2024</p>
<p>Architecting the Future of Software Engineering: A National Agenda for Software Engineering Research &amp; Development. Anita Carleton, Mark H Klein, John E Robert, Erin Harper, Robert K Cunningham, Dionisio De Niz, John T Foreman, John B Goodenough, James D Herbsleb, Ipek Ozkaya, Douglas Schmidt, Forrest Shull, Software Engineering Institute. 2021Carnegie Mellon UniversityTechnical report</p>
<p>Envisioning Science in 2050. James Ahrens, Amber Boehnlein, Rich Carlson, Joshua Elliot, Kjiersten Fagnan, Nicola Ferrier, Ian Foster, Lee Gimpel, John Shalf, Dan Ratner, 10.2172/1871683Advanced Scientific Computing Research. 2022United States Department of Energy</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials. O Edward, Jed W Pyzer-Knapp, Pitera, W J Peter, Seiji Staar, Teodoro Takeda, Daniel P Laino, James Sanders, John R Sexton, Alessandro Smith, Curioni, 10.1038/s41524-022-00765-z20228</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/s41586-023-06792-0Nature. 6242023</p>
<p>Foundational Research Gaps and Future Directions for Digital Twins. 10.17226/268942024The National Academies PressWashington, DCNational Academy of Engineering and National Academies of Sciences, Engineering, and Medicine.</p>
<p>Development of AI-assisted microscopy frameworks through realistic simulation with pySTED. Anthony Bilodeau, Albert Michaud-Gagnon, Julia Chabbert, Benoit Turcotte, J Orn Heine, Audrey Durand, Flavie Lavoie-Cardinal, 10.1038/s42256-024-00903-wNature Machine Intelligence. 62024</p>
<p>msiFlow: Automated workflows for reproducible and scalable multimodal mass spectrometry imaging and microscopy data analysis. Philippa Spangenberg, Sebastian Bessler, Lars Widera, Jenny Bottek, Mathis Richter, Stephanie Thiebes, Devon Siemes, Sascha D Krauß, G Lukasz, Migas, Swapna Siva, Prasad Kasarla, Jens Phapale, Dagmar Kleesiek, Lars C Führer, Heike Moeller, Raf Heuer, Matthias Van De Plas, Oliver Gunzer, Jens Soehnlein, Olga Soltwisch, Klaus Shevchuk, Daniel R Dreisewerd, Engel, 10.1038/s41467-024-55306-7Nature Communications. 162025</p>
<p>DOE Exascale Computing Project (ECP). 2024</p>
<p>Exascale applications: skin in the game. Francis Alexander, Ann Almgren, John Bell, Amitava Bhattacharjee, Jacqueline Chen, Phil Colella, David Daniel, Jack Deslippe, Lori Diachin, Erik Draeger, Anshu Dubey, Thom Dunning, Thomas Evans, Ian Foster, Marianne Francois, Tim Germann, Mark Gordon, Salman Habib, Mahantesh Halappanavar, Steven Hamilton, William Hart, ( Zhenyu, ) Henry, Aimee Huang, Daniel Hungerford, Kasen, R C Paul, Tzanio Kent, Douglas B Kolev, Andreas Kothe, Ye Kronfeld, Paul Luo, David Mackenzie, Bronson Mccallen, Sue Messer, Chris Mniszewski, Amedeo Oehmen, Danny Perazzo, David Perez, William J Richards, Rob Rider, Kenneth Rieben, Andrew Roche, Michael Siegel, Carl Sprague, Rick Steefel, Madhava Stevens, Mark Syamlal, John Taylor, Jean-Luc Turner, Artur F Vay, Theresa L Voter, Katherine Windus, Yelick, 10.1098/rsta.2019.0056Phil. Trans. R. Soc. 378201900562166. March 2020</p>
<p>E4S: An HPC-AI Software Ecosystem for Science. 2025</p>
<p>Toward a cohesive AI and simulation software ecosystem for scientific innovation. M A Heroux, S Shende, L C Mcinnes, T Gamblin, J M Willenbring, 10.48550/arXiv.2411.09507RFI response to the Frontiers in AI for Science, Security, and Technology (FASST) Initiative. 2024</p>
<p>How community software ecosystems can unlock the potential of exascale computing. Lois Curfman Mcinnes, Michael A Heroux, Erik W Draeger, Andrew Siegel, Susan Coghlan, Katie Antypas, 10.1038/s43588-021-00033-yNature Computational Science. 12021</p>
<p>Lois Curfman Michael A Heroux, James Mcinnes, Todd Ahrens, Timothy C Gamblin, Xiaoye Sherry Germann, Kathryn Li, Todd Mohror, Sameer Munson, Rajeev Shende, Jeffrey Thakur, James Vetter, Willenbring, 10.1177/10943420241271005ECP libraries and tools: An overview. The International Journal of High Performance Computing Applications. 202438</p>
<p>Transforming science through software: Improving while delivering 100×. Richard Gerber, Steven Gottlieb, Michael A Heroux, Lois Curfman Mcinnes, 10.1109/MCSE.2024.3400462Computing in Science &amp; Engineering. 26012024</p>
<p>The AI Scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2408.06292ArXiv. 2024Technical Report</p>
<p>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, 10.1101/2024.11.11.623004bioRxiv. 2024</p>
<p>Robin: A multi-agent system for automating scientific discovery. Ali Essam, Ghareeb , Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J Szostkiewicz, Jon M Laurent, T Muhammed, Andrew D Razzak, Michaela M White, Samuel G Hinks, Rodriques, 2025</p>
<p>Extreme Heterogeneity 2018: Productive Computational Science in the Era of Extreme Heterogeneity. J S Vetter, 10.2172/1473756Report for the DOE ASCR Basic Research Needs Workshop on Extreme Heterogeneity. 2018</p>
<p>Suren Byna, Stratos Idreos, Terry Jones, Kathryn Mohror, Rob Ross, Florin Rusu, 10.2172/1845707for the ASCR Workshop on the Management and Storage of Scientific Data. 2021Report</p>
<p>Pavel Lougovski, D Ojas, Joe Parekh, Mark Broz, Joseph C Byrd, Yanne Chapman, Chembo, A Wibe, Eden De Jong, Travis S Figueroa, Jeffrey Humble, Larson, 10.2172/2430035for the ASCR Workshop on Basic Research Needs in Quantum Computing and Networking -2023. 2024Report</p>
<p>Exascale computing in the United States. Douglas Kothe, Stephen Lee, Irene Qualters, 10.1109/MCSE.2018.2875366IEEE Computing in Science and Engineering. 2112019</p>
<p>DOE Scientific Discovery through Advanced Computing (SciDAC). 2025</p>
<p>The early years and evolution of the DOE Computational Science Graduate Fellowship Program. David Brown, James Hack, Robert Voigt, 10.1109/MCSE.2021.3120689Computing in Science &amp; Engineering. 2362021</p>
<p>NNSA Predictive Science Academic Alliance Program. 2025Predictive-Science-Academic-Alliance-Program</p>
<p>NSF Science and Technology Centers. 2025</p>
<p>Sustainable Horizons Institute. Sustainable Research Pathways. 2025</p>
<p>James Ang, Andrew Chien, Simon David Hammond, Adolfy Hoisie, Ian Karlin, Scott Pakin, John Shalf, Jeffrey S Vetter, 10.2172/1822199Reimagining Codesign for Advanced Scientific Computing: Report for the ASCR Workshop on Reimagining Codesign. US DOE Office of Science2022</p>
<p>Co-design in the Exascale Computing Project. Timothy C Germann, 10.1177/10943420211059380The International Journal of High Performance Computing Applications. 3562021</p>
<p>Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification. 10.17226/133952012The National Academies PressWashington, DCNational Research Council</p>
<p>. 10.17226/253032019The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine. Reproducibility and Replicability in Science</p>
<p>Maya Gokhale, Ganesh Gopalakrishnan, Jackson Mayo, Santosh Nagarakatte, Cindy Rubio-González, Stephen F Siegel, Report of the DOE/NSF Workshop on Correctness in Scientific Computing. 2023</p>
<p>Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish Parashar, Abani Patra, James Sethian, Stefan M Wild, Karen Willcox, 10.2172/1478744Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence. 2019U.S. Department of Energy, ASCRReport</p>
<p>Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations. Nick Mcgreivy, Ammar Hakim, 10.1038/s42256-024-00897-5Nature Machine Intelligence. 62024</p>
<p>Envisioning better benchmarks for machine learning PDE solvers. Johannes Brandstetter, 10.1038/s42256-024-00962-zNature Machine Intelligence. 72025</p>
<p>Empowering scientific workflows with federated agents. J Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster, 2025</p>
<p>The increasing dominance of teams in production of knowledge. Stefan Wuchty, Benjamin F Jones, Brian Uzzi, 10.1126/science.1136099Science. 3165827May 2007</p>
<p>Teams at work. H M Williams, N J Allen, 10.4135/9781849200448The SAGE Handbook of Organizational Behavior: Volume 1 -Micro Approaches. Julian Barling, Cary L Cooper, SAGE Publications LtdJuly 2008</p>
<p>Enhancing the Effectiveness of Team Science. 10.17226/190072015The National Academies PressWashington, DCNational Research Council</p>
<p>. 10.17226/290432025The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine. The Science and Practice of Team Science</p>
<p>Interdisciplinarity as teamwork: How the science of teams can inform team science. M Stephen, Fiore, 10.1177/1046496408317797Small Group Research. 393June 2008</p>
<p>Situated Learning: Legitimate Peripheral Participation. Jean Lave, Etienne Wenger, 1991Cambridge University Press</p>
<p>Etienne Wenger, Richard Mcdermott, William Snyder, Cultivating Communities of Practice: A Guide to Managing Knowledge. Cambridge University Press2002</p>
<p>Systems correctness practices at Amazon Web Services. Marc Brooker, Ankush Desai, 10.1145/3729175Commun. ACM. 686June 2025</p>
<p>Shock with confidence: Formal proofs of correctness for hyperbolic partial differential equation solvers. Jonathan Gorard, Ammar Hakim, 2025</p>
<p>LlamaFirewall: An open source guardrail system for building secure AI agents. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Dominik Beto De Paola, James Gabi, Jean-Christophe Crnkovich, Kat Testud, Rashnil He, Wu Chaturvedi, Joshua Zhou, Saxe, 2025</p>
<p>What is programming?. Sebastian Nicolajsen, Claus Brabrand, 10.1145/3713068Commun. ACM. 686June 2025</p>
<p>LLMs are greedy agents: Effects of RL fine-tuning on decision-making abilities. Thomas Schmied, Jordi Org Bornschein, Markus Grau-Moya, Razvan Wulfmeier, Pascanu, 2025</p>
<p>Measuring team trust: A critical and meta-analytical review. Jennifer Feitosa, Rebecca Grossman, William S Kramer, Eduardo Salas, 10.1002/job.2436Journal of Organizational Behavior. 415June 2020</p>
<p>Trust within the workplace: A review of two waves of research and a glimpse of the third. Kurt T Dirks, Bart De, Jong , 10.1146/annurev-orgpsych-012420-083025Annual Review of Organizational Psychology and Organizational Behavior. 91Jan. 2022</p>
<p>How and why humans trust: A meta-analysis and elaborated model. P A Hancock, Theresa T Kessler, Alexandra D Kaplan, Kimberly Stowers, J Christopher Brill, Deborah R Billings, Kristin E Schaefer, James L Szalma, 10.3389/fpsyg.2023.1081086Frontiers in Psychology. 14March 2023</p>
<p>Better together: Elements of successful scientific software development in a distributed collaborative community. Julia Koehler Leman, Brian D Weitzner, P Douglas Renfrew, Steven M Lewis, Rocco Moretti, Andrew M Watkins, 10.1371/journal.pcbi.1007507PLOS Computational Biology. 165May 2020</p>
<p>A cast of thousands: How the IDEAS productivity project has advanced software productivity and sustainability. Lois Curfman Mcinnes, Michael A Heroux, David E Bernholdt, Anshu Dubey, Elsa Gonsiorowski, Rinku Gupta, Osni Marques, J David Moulton, Hai Ah Nam, Boyana Norris, Elaine M Raybourn, Jim Willenbring, Ann Almgren, Roscoe A Bartlett, Kita Cranfill, Stephen Fickas, Don Frederick, William F Godoy, Patricia A Grubel, Rebecca Hartman-Baker, Axel Huebl, Rose Lynch, Addi Malviya-Thakur, Reed Milewicz, Mark C Miller, Miranda R Mundt, Erik Palmer, Suzanne Parete-Koon, Megan Phinney, Katherine Riley, David M Rogers, Benjamin Sims, Deborah Stevens, Gregory R Watson, 10.1109/MCSE.2024.3383799IEEE Computing in Science &amp; Engineering. 2612024</p>
<p>Incentives and integration in scientific software production. James Howison, James D Herbsleb, 10.1145/2441776.2441828Proceedings of the 2013 Conference on Computer Supported Cooperative Work. the 2013 Conference on Computer Supported Cooperative WorkSan Antonio Texas USAACM2013</p>
<p>Talk to me: A case study on coordinating expertise in large-scale scientific software projects. Reed Milewicz, Elaine Raybourn, 10.1109/eScience.2018.000102018 IEEE 14th International Conference on e-Science (e-Science). Oct. 2018</p>
<p>Ann Mary, Leung, Sustainable Horizons Institute. 2025</p>
<p>Community organizations: Changing the culture in which research software is developed and sustained. S Daniel, Katz, 10.1109/MCSE.2018.2883051IEEE CiSE. 212019</p>            </div>
        </div>

    </div>
</body>
</html>
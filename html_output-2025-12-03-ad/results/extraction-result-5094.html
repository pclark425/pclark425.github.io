<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5094 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5094</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5094</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-a9e3e5dd7b30890553b7ae1c41f932e99192bb44</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44" target="_blank">Large Language Models Are Reasoning Teachers</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper uses very large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude, and proposes Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tunes smaller models.</p>
                <p><strong>Paper Abstract:</strong> Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5094.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5094.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (OpenAI text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter instruction-tuned GPT-3 variant used as the primary teacher model to generate chain-of-thought (CoT) rationales via Zero-shot-CoT prompting and as a baseline Zero-shot-CoT reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, instruction-tuned GPT-3 family model provided via OpenAI API; used as a large teacher to produce Zero-shot-CoT rationales and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>12 reasoning datasets (arithmetic, symbolic, other, commonsense) including SingleEq, AddSub, MultiArith, GSM8K, AQUA (AquaRAT), SVAMP, Date Understanding, Tracking Shuffled Objects, Last Letter Concatenation, Coin Flip, CommonSenseQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Collection of complex multi-step reasoning benchmarks: arithmetic word problems (single/multi-step), symbolic tasks, tracking/shuffled-object reasoning, date understanding, and multiple-choice commonsense/strategy questions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (Zero-shot-CoT) to generate multi-step rationales and answer predictions; used as a teacher to produce rationale+answer samples for Fine-tune-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported Zero-shot-CoT accuracies (teacher/text-davinci-002) across datasets (Table 1): SingleEq 82.24%, AddSub 78.99%, MultiArith 78.89%, GSM8K 40.26%, AQUA 34.25%, SVAMP 64.67%, Date Understanding 73.87%, Shuffled Objects 50.22%, Last Letter 56.00%, Coin Flip 92.67%, CommonSenseQA 61.75%, StrategyQA 53.57%. (Also reported variations when varying max tokens.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite strong performance on many tasks, teacher rationales are not always correct: sometimes final answers correct with incorrect reasoning; performance on hardest datasets (GSM8K, AQUA) is limited (<50%); generated rationales can be incomplete with short max sequence lengths; generation cost and inference token usage are high due to model size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used as a reference teacher: smaller student models fine-tuned on teacher rationales sometimes approach or exceed teacher accuracy on some tasks (e.g., Shuffled Objects and Coin Flip), but teacher outperforms students on many arithmetic benchmarks. Teacher performance scales with size/variant and correlates with student performance when used in Fine-tune-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Teacher performance affected by max rationale length (longer teacher outputs can improve absolute accuracies on some tasks); teacher variants (different davinci/instruct variants) produce different student outcomes: better teacher → better student. Teacher inference cost and token budget influence choice of Zero-shot-CoT vs Few-shot-CoT for generating training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Reasoning Teachers', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5094.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5094.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 students (curie/babbage/ada)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 family student models: curie (6.7B), babbage (1.3B), ada (0.3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller GPT-3 family models used as students that are fine-tuned on teacher-generated chain-of-thought samples (Fine-tune-CoT) or evaluated with Zero-shot, Zero-shot-CoT, Few-shot-CoT, and vanilla fine-tuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (curie / babbage / ada)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer family (OpenAI GPT-3 variants) used as student models: curie ≈ 6.7B, babbage ≈ 1.3B, ada ≈ 0.3B; available for fine-tuning via OpenAI API (default fine-tune settings used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B, 1.3B, 0.3B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Same 12 datasets: SingleEq, AddSub, MultiArith, GSM8K, AQUA, SVAMP, Date Understanding, Tracking Shuffled Objects, Last Letter, Coin Flip, CommonSenseQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning tasks spanning arithmetic (single- and multi-step), symbolic string tasks, object-tracking, and multiple-choice commonsense/strategy problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under multiple paradigms: Zero-shot prompting, Zero-shot-CoT prompting, Few-shot-CoT prompting, vanilla fine-tuning on (question→answer) pairs, and Fine-tune-CoT (fine-tune on teacher-generated prompt+rationale→completion pairs). Also studied with diverse reasoning (multiple teacher rationales per sample).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Key results (Table 1) summarize that Zero-shot and Zero-shot-CoT performance on small GPT-3 students is near-random on many tasks. Examples (selected): Zero-shot-CoT (6.7B curie) often <7% on many arithmetic tasks; vanilla fine-tune (6.7B) improves some tasks (e.g., Coin Flip 72.00%). Fine-tune-CoT (student sizes): 0.3B: (MultiArith 6.11% or 23.89% with different runs), 1.3B: MultiArith 13.33% (or 27.78% with diverse reasoning), 6.7B: MultiArith 33.33% (53.33% with diverse reasoning). Fine-tune-CoT achieves very high accuracy on symbolic tasks: Coin Flip ~99-100%, Last Letter ~50-62% depending on size and diverse reasoning, Shuffled Objects up to ~64% (6.7B). Diverse reasoning (D up to 64 or 8 depending on dataset) substantially boosts accuracies (e.g., ~+26 percentage points on MultiArith for 6.7B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Students still struggle on hardest arithmetic/commonsense datasets (GSM8K, AQUA) with low absolute performance; many errors are arithmetic calculation mistakes and some semantic/commonsense failures; student rationales can be repetitive or digressive; fine-tuning can inherit teacher biases/toxicities; correct final answer sometimes comes with incorrect rationale; performance sensitive to max sequence length and templated dataset splits can cause leakages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Fine-tune-CoT substantially outperforms Zero-shot-CoT and often outperforms vanilla fine-tuning and Few-shot-CoT on many tasks for the same model size; with diverse reasoning small students can outperform the large 175B teacher on some tasks (e.g., Shuffled Objects, Coin Flip). Student performance scales with student size and benefits from better teacher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations show (1) diverse reasoning degree D improves performance monotonically up to tested maxima (D up to 64) with big gains on MultiArith and SVAMP; (2) dataset size scaling: Fine-tune-CoT benefits from more training samples whereas vanilla fine-tuning sometimes degrades with more data on some tasks; (3) teacher quality: better teacher leads to better student (teacher-student correlation); (4) max rationale/prediction length impacts both teacher completions and student performance; (5) rationale filtering: answer-based filtering keeps more samples and can outperform stricter human-filtered 'golden' rationale sets when sample count differs, indicating a quantity-vs-quality tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Reasoning Teachers', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5094.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5094.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source students (GPT-2 / T5 / Flan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source student models: GPT-2 (Small/Medium/Large), T5 (Small/Base/Large) and Flan-T5 (Small/Base/Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source decoder-only (GPT-2) and encoder-decoder (T5 / Flan-T5) models used under controlled fine-tuning experiments to validate Fine-tune-CoT trends outside closed-source GPT-3 API.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 / T5 / Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 family: decoder-only transformers (124M, 355M, 774M). T5 family: encoder-decoder transformers (Small 60M, Base 220M, Large 700M). Flan-T5 are instruction-tuned T5 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2: 124M/355M/774M; T5: 60M/220M/700M; Flan-T5: 60M/220M/700M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Subset or full set of the 12 tasks used in paper (same categories: arithmetic, symbolic, other, commonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step reasoning tasks (arithmetic, symbolic operations like last-letter concatenation, object tracking, date understanding, commonsense multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Vanilla fine-tuning and Fine-tune-CoT applied in a fully controlled open-source setting (full model updates), with hyperparameters controlled (LR=3e-4, batch=8, up to 20 epochs). Also evaluated the effect of instruction tuning (Flan-T5) prior to Fine-tune-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that trends on open-source models mirror GPT-3 findings: near-random performance for Zero-shot and Zero-shot-CoT; Fine-tune-CoT consistently outperforms prompt-based baselines and is comparable or superior to vanilla fine-tuning; Flan-T5 shows more stable performance and encoder-decoder models sometimes benefit more from vanilla fine-tuning than decoder-only GPT-2. Exact per-dataset numbers are in appendix (not fully reproduced in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported performance can be sensitive to hyperparameters (learning rate, batch size, epochs) and may be under-estimated if not tuned; decoder-only models may be bottlenecked by causal masking for reasoning; arithmetic capability still weak at these small sizes; variation across epochs can be high for small models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Encoder-decoder (T5/Flan-T5) sometimes performs better under vanilla fine-tuning than decoder-only GPT-2; Fine-tune-CoT improves all families and diverse reasoning yields further gains. Results validate that CoT distillation is applicable beyond closed-source GPT-3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Controlled experiments show sensitivity to hyperparameters; instruction tuning (Flan-T5) stabilizes training and improves generalization; Fine-tune-CoT benefits from dataset size and diverse reasoning similarly to closed-source students.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Reasoning Teachers', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5094.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5094.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tune-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tune-CoT (Chain-of-Thought Fine-Tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that uses large LLMs as 'reasoning teachers' by generating chain-of-thought rationales via Zero-shot-CoT and fine-tuning much smaller student models on curated prompt→rationale+answer examples to impart multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tune-CoT (method applied to many student models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method: (1) prompt a very large teacher with Zero-shot-CoT to generate rationales and answers, (2) filter teacher completions by matching teacher final answer to ground truth, (3) reformat successful (question, rationale, answer) into concise prompt-completion pairs and fine-tune small student models with standard autoregressive next-token prediction loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Evaluated on 12 public reasoning datasets (arithmetic, symbolic, other, commonsense) as above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: requires multi-step reasoning, symbolic manipulation, tracking, and commonsense inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Distillation-style fine-tuning on teacher-generated CoT rationales (Zero-shot-CoT); extension 'diverse reasoning' generates D stochastic rationales per sample (temperature sampling) to augment training data; uses answer-based filtering to select teacher samples; fine-tunes students with compact delimiters to minimize token length.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tune-CoT substantially improves student reasoning compared to Zero-shot-CoT and vanilla fine-tuning: examples from Table 1 — MultiArith: fine-tuned 6.7B student reaches 33.33% (vs Zero-shot-CoT 5% and vanilla fine-tune 15%); with diverse reasoning 6.7B reaches 53.33%; SVAMP: improvements also observed (e.g., 12.67% → 30.33% with diverse reasoning on 6.7B). Small students (0.3B,1.3B) obtain notable gains and can outperform Zero-shot-CoT baselines and in some cases the 175B teacher on certain tasks (e.g., Shuffled Objects, Coin Flip). Fine-tune-CoT scales with: degree of diversity D, dataset size, teacher quality, and student size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not state-of-the-art for hardest datasets (GSM8K, AQUA); many student errors are arithmetic miscalculations; teacher-generated rationales sometimes incorrect even when final answer matches (≈27.6% incorrect rationales for Date Understanding), producing a quantity-vs-quality tradeoff; diverse reasoning increases development (teacher inference) cost; potential for dataset template leakage requires template-wise splits for robust evaluation; student rationales can be repetitive or digressive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms Zero-shot and Zero-shot-CoT on small models and frequently surpasses vanilla fine-tuning and Few-shot-CoT for same student sizes; diverse reasoning often yields performance beyond stronger prompt-based methods and can enable small students to exceed teacher performance on some tasks; ablations show benefits are robust across axes (teacher quality, student size, dataset size).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations/findings: (1) Diverse reasoning degree D improves performance (e.g., +~26% on MultiArith for 6.7B), (2) dataset size: student accuracy scales with more samples for Fine-tune-CoT but not always for vanilla fine-tuning, (3) teacher performance correlates with student performance, (4) rationale filtering: answer-based filtering yields more samples and sometimes better downstream student accuracy than stricter human-filtered rationales due to quantity tradeoff, (5) max sequence length L_r and L_p impact outcomes; longer lengths can help some tasks but produce repetitive rationales and have mixed effects on fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Reasoning Teachers', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5094.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5094.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse reasoning (teacher-generated multiple CoT rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension to Fine-tune-CoT that generates multiple distinct stochastic chain-of-thought rationales per training sample (sampling with temperature) to augment the fine-tuning dataset and capture diverse solution paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diverse reasoning (data-augmentation technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generate D distinct (rationale, answer) pairs per question from the teacher using temperature sampling (T=0.7), filter by matching answers, and add all accepted rationales to fine-tuning dataset; D is called degree of reasoning diversity (tested up to D=64 for some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Primarily evaluated on MultiArith and SVAMP and also applied across the 12 datasets to augment Fine-tune-CoT training data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step arithmetic and reasoning tasks benefit from multiple possible reasoning paths and linguistic templates; diverse reasoning aims to expose student to this variability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stochastic sampling from teacher (temperature T=0.7) to produce multiple CoT rationales per question; include all accepted rationale-answer pairs in fine-tuning data; tradeoff: more teacher inference at development time for improved student inference-time performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantial improvements reported: For 6.7B student on MultiArith, diverse reasoning yields ≈+26 percentage points (from ~33% to ~59% depending on row/config) or reported 33.33% → 53.33% (Table 1) depending on exact experimental run; for SVAMP improvements ≈+17 percentage points for 6.7B. Diverse reasoning consistently boosts performance across model sizes and datasets, and enables small models to surpass larger models without diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increases development cost linearly with D and number of original samples (additional teacher inference cost); requires careful balancing of D vs budget; some datasets are large and generation of many rationales is costly or infeasible; potential diminishing returns beyond tested D.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms single-rationale Fine-tune-CoT and overtakes Few-shot-CoT and vanilla fine-tuning baselines in many conditions; offers a cheaper alternative to generating more labeled examples or using Few-shot-CoT exemplars in teacher inference budget tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Degree-of-diversity ablation (Figure 3) shows monotonic improvements up to D tested (1→64): marked gains on MultiArith and SVAMP and consistent gains across student sizes; cost analysis (Appendix F) shows Pareto frontier of data acquisition cost vs performance includes diverse reasoning as cost-effective option compared to manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Reasoning Teachers', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Explanations from large language models make small reasoners better <em>(Rating: 2)</em></li>
                <li>Teaching small language models to reason <em>(Rating: 2)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
                <li>STAR: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>On the advance of making language models better reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5094",
    "paper_id": "paper-a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "InstructGPT (text-davinci-002)",
            "name_full": "InstructGPT (OpenAI text-davinci-002)",
            "brief_description": "A 175B-parameter instruction-tuned GPT-3 variant used as the primary teacher model to generate chain-of-thought (CoT) rationales via Zero-shot-CoT prompting and as a baseline Zero-shot-CoT reasoner.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Decoder-only transformer, instruction-tuned GPT-3 family model provided via OpenAI API; used as a large teacher to produce Zero-shot-CoT rationales and final answers.",
            "model_size": "175B",
            "logical_reasoning_task": "12 reasoning datasets (arithmetic, symbolic, other, commonsense) including SingleEq, AddSub, MultiArith, GSM8K, AQUA (AquaRAT), SVAMP, Date Understanding, Tracking Shuffled Objects, Last Letter Concatenation, Coin Flip, CommonSenseQA, StrategyQA",
            "task_description": "Collection of complex multi-step reasoning benchmarks: arithmetic word problems (single/multi-step), symbolic tasks, tracking/shuffled-object reasoning, date understanding, and multiple-choice commonsense/strategy questions.",
            "method_or_approach": "Zero-shot Chain-of-Thought prompting (Zero-shot-CoT) to generate multi-step rationales and answer predictions; used as a teacher to produce rationale+answer samples for Fine-tune-CoT.",
            "performance": "Reported Zero-shot-CoT accuracies (teacher/text-davinci-002) across datasets (Table 1): SingleEq 82.24%, AddSub 78.99%, MultiArith 78.89%, GSM8K 40.26%, AQUA 34.25%, SVAMP 64.67%, Date Understanding 73.87%, Shuffled Objects 50.22%, Last Letter 56.00%, Coin Flip 92.67%, CommonSenseQA 61.75%, StrategyQA 53.57%. (Also reported variations when varying max tokens.)",
            "limitations_or_failure_cases": "Despite strong performance on many tasks, teacher rationales are not always correct: sometimes final answers correct with incorrect reasoning; performance on hardest datasets (GSM8K, AQUA) is limited (&lt;50%); generated rationales can be incomplete with short max sequence lengths; generation cost and inference token usage are high due to model size.",
            "comparison": "Used as a reference teacher: smaller student models fine-tuned on teacher rationales sometimes approach or exceed teacher accuracy on some tasks (e.g., Shuffled Objects and Coin Flip), but teacher outperforms students on many arithmetic benchmarks. Teacher performance scales with size/variant and correlates with student performance when used in Fine-tune-CoT.",
            "ablation_or_analysis_results": "Teacher performance affected by max rationale length (longer teacher outputs can improve absolute accuracies on some tasks); teacher variants (different davinci/instruct variants) produce different student outcomes: better teacher → better student. Teacher inference cost and token budget influence choice of Zero-shot-CoT vs Few-shot-CoT for generating training data.",
            "uuid": "e5094.0",
            "source_info": {
                "paper_title": "Large Language Models Are Reasoning Teachers",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3 students (curie/babbage/ada)",
            "name_full": "GPT-3 family student models: curie (6.7B), babbage (1.3B), ada (0.3B)",
            "brief_description": "Smaller GPT-3 family models used as students that are fine-tuned on teacher-generated chain-of-thought samples (Fine-tune-CoT) or evaluated with Zero-shot, Zero-shot-CoT, Few-shot-CoT, and vanilla fine-tuning baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (curie / babbage / ada)",
            "model_description": "Decoder-only transformer family (OpenAI GPT-3 variants) used as student models: curie ≈ 6.7B, babbage ≈ 1.3B, ada ≈ 0.3B; available for fine-tuning via OpenAI API (default fine-tune settings used).",
            "model_size": "6.7B, 1.3B, 0.3B",
            "logical_reasoning_task": "Same 12 datasets: SingleEq, AddSub, MultiArith, GSM8K, AQUA, SVAMP, Date Understanding, Tracking Shuffled Objects, Last Letter, Coin Flip, CommonSenseQA, StrategyQA",
            "task_description": "Multi-step reasoning tasks spanning arithmetic (single- and multi-step), symbolic string tasks, object-tracking, and multiple-choice commonsense/strategy problems.",
            "method_or_approach": "Evaluated under multiple paradigms: Zero-shot prompting, Zero-shot-CoT prompting, Few-shot-CoT prompting, vanilla fine-tuning on (question→answer) pairs, and Fine-tune-CoT (fine-tune on teacher-generated prompt+rationale→completion pairs). Also studied with diverse reasoning (multiple teacher rationales per sample).",
            "performance": "Key results (Table 1) summarize that Zero-shot and Zero-shot-CoT performance on small GPT-3 students is near-random on many tasks. Examples (selected): Zero-shot-CoT (6.7B curie) often &lt;7% on many arithmetic tasks; vanilla fine-tune (6.7B) improves some tasks (e.g., Coin Flip 72.00%). Fine-tune-CoT (student sizes): 0.3B: (MultiArith 6.11% or 23.89% with different runs), 1.3B: MultiArith 13.33% (or 27.78% with diverse reasoning), 6.7B: MultiArith 33.33% (53.33% with diverse reasoning). Fine-tune-CoT achieves very high accuracy on symbolic tasks: Coin Flip ~99-100%, Last Letter ~50-62% depending on size and diverse reasoning, Shuffled Objects up to ~64% (6.7B). Diverse reasoning (D up to 64 or 8 depending on dataset) substantially boosts accuracies (e.g., ~+26 percentage points on MultiArith for 6.7B).",
            "limitations_or_failure_cases": "Students still struggle on hardest arithmetic/commonsense datasets (GSM8K, AQUA) with low absolute performance; many errors are arithmetic calculation mistakes and some semantic/commonsense failures; student rationales can be repetitive or digressive; fine-tuning can inherit teacher biases/toxicities; correct final answer sometimes comes with incorrect rationale; performance sensitive to max sequence length and templated dataset splits can cause leakages.",
            "comparison": "Fine-tune-CoT substantially outperforms Zero-shot-CoT and often outperforms vanilla fine-tuning and Few-shot-CoT on many tasks for the same model size; with diverse reasoning small students can outperform the large 175B teacher on some tasks (e.g., Shuffled Objects, Coin Flip). Student performance scales with student size and benefits from better teacher performance.",
            "ablation_or_analysis_results": "Ablations show (1) diverse reasoning degree D improves performance monotonically up to tested maxima (D up to 64) with big gains on MultiArith and SVAMP; (2) dataset size scaling: Fine-tune-CoT benefits from more training samples whereas vanilla fine-tuning sometimes degrades with more data on some tasks; (3) teacher quality: better teacher leads to better student (teacher-student correlation); (4) max rationale/prediction length impacts both teacher completions and student performance; (5) rationale filtering: answer-based filtering keeps more samples and can outperform stricter human-filtered 'golden' rationale sets when sample count differs, indicating a quantity-vs-quality tradeoff.",
            "uuid": "e5094.1",
            "source_info": {
                "paper_title": "Large Language Models Are Reasoning Teachers",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Open-source students (GPT-2 / T5 / Flan-T5)",
            "name_full": "Open-source student models: GPT-2 (Small/Medium/Large), T5 (Small/Base/Large) and Flan-T5 (Small/Base/Large)",
            "brief_description": "Open-source decoder-only (GPT-2) and encoder-decoder (T5 / Flan-T5) models used under controlled fine-tuning experiments to validate Fine-tune-CoT trends outside closed-source GPT-3 API.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 / T5 / Flan-T5",
            "model_description": "GPT-2 family: decoder-only transformers (124M, 355M, 774M). T5 family: encoder-decoder transformers (Small 60M, Base 220M, Large 700M). Flan-T5 are instruction-tuned T5 variants.",
            "model_size": "GPT-2: 124M/355M/774M; T5: 60M/220M/700M; Flan-T5: 60M/220M/700M",
            "logical_reasoning_task": "Subset or full set of the 12 tasks used in paper (same categories: arithmetic, symbolic, other, commonsense).",
            "task_description": "Same multi-step reasoning tasks (arithmetic, symbolic operations like last-letter concatenation, object tracking, date understanding, commonsense multiple-choice).",
            "method_or_approach": "Vanilla fine-tuning and Fine-tune-CoT applied in a fully controlled open-source setting (full model updates), with hyperparameters controlled (LR=3e-4, batch=8, up to 20 epochs). Also evaluated the effect of instruction tuning (Flan-T5) prior to Fine-tune-CoT.",
            "performance": "Paper reports that trends on open-source models mirror GPT-3 findings: near-random performance for Zero-shot and Zero-shot-CoT; Fine-tune-CoT consistently outperforms prompt-based baselines and is comparable or superior to vanilla fine-tuning; Flan-T5 shows more stable performance and encoder-decoder models sometimes benefit more from vanilla fine-tuning than decoder-only GPT-2. Exact per-dataset numbers are in appendix (not fully reproduced in main text).",
            "limitations_or_failure_cases": "Reported performance can be sensitive to hyperparameters (learning rate, batch size, epochs) and may be under-estimated if not tuned; decoder-only models may be bottlenecked by causal masking for reasoning; arithmetic capability still weak at these small sizes; variation across epochs can be high for small models.",
            "comparison": "Encoder-decoder (T5/Flan-T5) sometimes performs better under vanilla fine-tuning than decoder-only GPT-2; Fine-tune-CoT improves all families and diverse reasoning yields further gains. Results validate that CoT distillation is applicable beyond closed-source GPT-3 variants.",
            "ablation_or_analysis_results": "Controlled experiments show sensitivity to hyperparameters; instruction tuning (Flan-T5) stabilizes training and improves generalization; Fine-tune-CoT benefits from dataset size and diverse reasoning similarly to closed-source students.",
            "uuid": "e5094.2",
            "source_info": {
                "paper_title": "Large Language Models Are Reasoning Teachers",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Fine-tune-CoT",
            "name_full": "Fine-tune-CoT (Chain-of-Thought Fine-Tuning)",
            "brief_description": "A method introduced in this paper that uses large LLMs as 'reasoning teachers' by generating chain-of-thought rationales via Zero-shot-CoT and fine-tuning much smaller student models on curated prompt→rationale+answer examples to impart multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fine-tune-CoT (method applied to many student models)",
            "model_description": "Method: (1) prompt a very large teacher with Zero-shot-CoT to generate rationales and answers, (2) filter teacher completions by matching teacher final answer to ground truth, (3) reformat successful (question, rationale, answer) into concise prompt-completion pairs and fine-tune small student models with standard autoregressive next-token prediction loss.",
            "model_size": null,
            "logical_reasoning_task": "Evaluated on 12 public reasoning datasets (arithmetic, symbolic, other, commonsense) as above.",
            "task_description": "As above: requires multi-step reasoning, symbolic manipulation, tracking, and commonsense inference.",
            "method_or_approach": "Distillation-style fine-tuning on teacher-generated CoT rationales (Zero-shot-CoT); extension 'diverse reasoning' generates D stochastic rationales per sample (temperature sampling) to augment training data; uses answer-based filtering to select teacher samples; fine-tunes students with compact delimiters to minimize token length.",
            "performance": "Fine-tune-CoT substantially improves student reasoning compared to Zero-shot-CoT and vanilla fine-tuning: examples from Table 1 — MultiArith: fine-tuned 6.7B student reaches 33.33% (vs Zero-shot-CoT 5% and vanilla fine-tune 15%); with diverse reasoning 6.7B reaches 53.33%; SVAMP: improvements also observed (e.g., 12.67% → 30.33% with diverse reasoning on 6.7B). Small students (0.3B,1.3B) obtain notable gains and can outperform Zero-shot-CoT baselines and in some cases the 175B teacher on certain tasks (e.g., Shuffled Objects, Coin Flip). Fine-tune-CoT scales with: degree of diversity D, dataset size, teacher quality, and student size.",
            "limitations_or_failure_cases": "Not state-of-the-art for hardest datasets (GSM8K, AQUA); many student errors are arithmetic miscalculations; teacher-generated rationales sometimes incorrect even when final answer matches (≈27.6% incorrect rationales for Date Understanding), producing a quantity-vs-quality tradeoff; diverse reasoning increases development (teacher inference) cost; potential for dataset template leakage requires template-wise splits for robust evaluation; student rationales can be repetitive or digressive.",
            "comparison": "Outperforms Zero-shot and Zero-shot-CoT on small models and frequently surpasses vanilla fine-tuning and Few-shot-CoT for same student sizes; diverse reasoning often yields performance beyond stronger prompt-based methods and can enable small students to exceed teacher performance on some tasks; ablations show benefits are robust across axes (teacher quality, student size, dataset size).",
            "ablation_or_analysis_results": "Ablations/findings: (1) Diverse reasoning degree D improves performance (e.g., +~26% on MultiArith for 6.7B), (2) dataset size: student accuracy scales with more samples for Fine-tune-CoT but not always for vanilla fine-tuning, (3) teacher performance correlates with student performance, (4) rationale filtering: answer-based filtering yields more samples and sometimes better downstream student accuracy than stricter human-filtered rationales due to quantity tradeoff, (5) max sequence length L_r and L_p impact outcomes; longer lengths can help some tasks but produce repetitive rationales and have mixed effects on fine-tuning.",
            "uuid": "e5094.3",
            "source_info": {
                "paper_title": "Large Language Models Are Reasoning Teachers",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Diverse reasoning",
            "name_full": "Diverse reasoning (teacher-generated multiple CoT rationales)",
            "brief_description": "An extension to Fine-tune-CoT that generates multiple distinct stochastic chain-of-thought rationales per training sample (sampling with temperature) to augment the fine-tuning dataset and capture diverse solution paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Diverse reasoning (data-augmentation technique)",
            "model_description": "Generate D distinct (rationale, answer) pairs per question from the teacher using temperature sampling (T=0.7), filter by matching answers, and add all accepted rationales to fine-tuning dataset; D is called degree of reasoning diversity (tested up to D=64 for some datasets).",
            "model_size": null,
            "logical_reasoning_task": "Primarily evaluated on MultiArith and SVAMP and also applied across the 12 datasets to augment Fine-tune-CoT training data.",
            "task_description": "Multi-step arithmetic and reasoning tasks benefit from multiple possible reasoning paths and linguistic templates; diverse reasoning aims to expose student to this variability.",
            "method_or_approach": "Stochastic sampling from teacher (temperature T=0.7) to produce multiple CoT rationales per question; include all accepted rationale-answer pairs in fine-tuning data; tradeoff: more teacher inference at development time for improved student inference-time performance.",
            "performance": "Substantial improvements reported: For 6.7B student on MultiArith, diverse reasoning yields ≈+26 percentage points (from ~33% to ~59% depending on row/config) or reported 33.33% → 53.33% (Table 1) depending on exact experimental run; for SVAMP improvements ≈+17 percentage points for 6.7B. Diverse reasoning consistently boosts performance across model sizes and datasets, and enables small models to surpass larger models without diversity.",
            "limitations_or_failure_cases": "Increases development cost linearly with D and number of original samples (additional teacher inference cost); requires careful balancing of D vs budget; some datasets are large and generation of many rationales is costly or infeasible; potential diminishing returns beyond tested D.",
            "comparison": "Outperforms single-rationale Fine-tune-CoT and overtakes Few-shot-CoT and vanilla fine-tuning baselines in many conditions; offers a cheaper alternative to generating more labeled examples or using Few-shot-CoT exemplars in teacher inference budget tradeoffs.",
            "ablation_or_analysis_results": "Degree-of-diversity ablation (Figure 3) shows monotonic improvements up to D tested (1→64): marked gains on MultiArith and SVAMP and consistent gains across student sizes; cost analysis (Appendix F) shows Pareto frontier of data acquisition cost vs performance includes diverse reasoning as cost-effective option compared to manual annotation.",
            "uuid": "e5094.4",
            "source_info": {
                "paper_title": "Large Language Models Are Reasoning Teachers",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Explanations from large language models make small reasoners better",
            "rating": 2
        },
        {
            "paper_title": "Teaching small language models to reason",
            "rating": 2
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1
        },
        {
            "paper_title": "STAR: Bootstrapping reasoning with reasoning",
            "rating": 1
        },
        {
            "paper_title": "On the advance of making language models better reasoners",
            "rating": 1
        }
    ],
    "cost": 0.01819325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Are Reasoning Teachers</h1>
<p>Namgyu Ho, Laura Schmid, Se-Young Yun<br>KAIST<br>{itsnamgyu, laura.schmid, yunseyoung}@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tuneCoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) have demonstrated remarkable performance in a wide range of downstream tasks. Recently, large language models (LLMs) have demonstrated in-context generalization capabilities: performing downstream tasks simply by conditioning on few in-context exemplars or plain natural language task descriptions (Brown et al., 2020; Sun et al., 2021). Despite these advancements, even the largest LLMs have been found to struggle with complex tasks which require multiple reasoning steps (Rae et al., 2021).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Fine-tune-CoT uses teacher-generated reasoning to teach students. We prompt a very large teacher model, such as GPT-3 175B, to solve complex questions via zero-shot chain-of-thought reasoning. We then use the reasoning samples to fine-tune a much smaller student model. See Figure 2 for details.</p>
<p>To solve complex tasks, recent works show that it is possible to elicit reasoning abilities by prompting LLMs to perform chain-of-thought (CoT) reasoning, i.e., generate a series of intermediate reasoning steps. This can be achieved by providing CoT demonstrations as exemplars in prompting (Wei et al., 2022b). More recently, Kojima et al. (2022) found that LLMs can be prompted to perform CoT reasoning simply by providing a natural language instruction to think step-by-step.</p>
<p>A major drawback of prompt-based CoT reasoning methods, however, is their reliance on extremely large models that span hundreds of billions of parameters (Wei et al., 2022b; Kojima et al., 2022). These models are prohibitive to deploy at scale due to overwhelming computational requirements and inference costs (Wei et al., 2022b).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Detailed overview of our proposed Fine-tune-CoT method. Step 1: a very large teacher model is prompted to solve complex questions (yellow) by generating multi-step reasoning explanations (green). Step 2: completions are filtered based on the correctness of the final prediction (red). The question, rationale, and answer are used to compose a reasoning sample comprised of the prompt and a multi-step solution. Step 3: the curated reasoning samples are used to fine-tune a small, lightweight student to exhibit reasoning capabilities. The application of an LM-based teacher enables diverse reasoning-generating multiple distinct rationales for each original sample to enrich the fine-tuning data. This boosts the performance of student models without any additional human annotation.</p>
<p>Therefore, we strive to enable complex reasoning in small models which are more feasible for largescale deployment.</p>
<p>In this light, we propose an approach named Fine-tune-CoT, which utilizes the reasoning capabilities of very large LMs to teach small models how to solve complex tasks. We apply existing zero-shot CoT prompting (Kojima et al., 2022) to generate rationales from very large teacher models, and use them to fine-tune smaller student models ${ }^{2}$. We illustrate this in Figure 2. We note that standard fine-tuning without rationales has been shown to be inadequate for solving reasoning tasks with small models (Talmor et al., 2018). While there have been attempts to fine-tune small models with hand-annotated reasoning steps (Nye et al., 2021; Cobbe et al., 2021), they often require task-specific training setups and high-quality rationales which are costly to annotate (Wei et al., 2022b). In contrast, our approach can be readily applied to novel downstream tasks without hand-crafted reasoning or task engineering.</p>
<p>We also propose a novel extension to our method, termed diverse reasoning, to maximize the teaching effects of Fine-tune-CoT. Inspired by the intuition</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>that complex tasks can have multiple solutions with distinct reasoning paths (Evans, 2010), we generate multiple reasoning solutions from teacher models using stochastic sampling to augment the training data for student models ${ }^{3}$. We find that this is a simple yet highly effective approach to maximizing student performance, which has not been explicitly recognized in concurrent works on fine-tuning with CoT reasoning (Huang et al., 2022; Li et al., 2022b; Magister et al., 2022; Fu et al., 2023).</p>
<p>We evaluate our method on 12 tasks using a wide range of publicly available models. We find that Fine-tune-CoT can elicit notable reasoning performance in small models while preserving much of the versatility of prompt-based CoT reasoning, which previously required $&gt;100 \mathrm{~B}$ parameter models (Wei et al., 2022b). Diverse reasoning enables remarkable gains in performance at the minor cost of additional teacher inference at development time, by exploiting our unique learning setup. This enables models as small as 0.3 B to outperform larger students, and even the 175B teacher model in some tasks. Our ablations show that performance is con-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>sistently scalable across all axes considered: diverse reasoning, dataset size, teacher performance, and student model size. This shows the potential of our method to enable reliable performance in small models that are feasible for use in real-world applications. Lastly, we conduct thorough sample studies and analyses which shed light on crucial details previous overlooked in fine-tuning for CoT and provide intuition on the emergence of reasoning abilities in small models.</p>
<h2>2 Related Work</h2>
<p>Downstream transfer in language models Much previous work established a "pre-train and fine-tune" paradigm for enhancing LLM performance on downstream tasks (Radford et al., 2018; Dong et al., 2019; Vaswani et al., 2017; Devlin et al., 2018). However, fine-tuning is not always easily applicable (Hendrycks et al., 2020). More recent literature exhibits a paradigm shift towards "prompting" the model to predict the desired output (Liu et al., 2021; Raffel et al., 2020). Large LMs can exhibit strong performance in this setting (Brown et al., 2020). For smaller models to be able to perform similarly, additional engineering is usually required (Gao et al., 2021; Schick and Schütze, 2021b; Schick et al., 2020). For more complex tasks, the idea of using samples with explicit reasoning steps for fine-tuning a model (Nye et al., 2021; Cobbe et al., 2021) preceded the approach of chain-of-thought (CoT) prompting (Wei et al., 2022b), which enables very large LMs to perform well.</p>
<p>Chain-of-thought reasoning In few-shot CoT prompting, the model learns to generate intermediate reasoning steps that lead to a problem solution, after being fed examples of step-by-step reasoning. This enables very good performance on a wide range of tasks. (Wang et al., 2022). Additionally, LLMs can perform well in an unsupervised task-agnostic setting, using Zero-shot-CoT (Kojima et al., 2022). This requires no fine-tuning or task specific conditioning, and substantially outperforms standard zero-shot learning and sometimes even few-shot learning on a wide number of tasks.</p>
<p>Yet, prior work has shown that CoT requires extremely large models for optimal performance (Hoffmann et al., 2022; Chowdhery et al., 2022). In our work, we contrast this by showing how to utilize CoT reasoning methods for smaller models by fine-tuning them on rationales gener-
ated by a very large model. Using various LLMgenerated explanations for fine-tuning smaller models has been successfully used in prior work ( Li et al., 2022a), with a focus on specific single tasks. Also, a similar approach to ours is mentioned in (Huang et al., 2022); however we note that this concurrent work focuses on using Few-shot-CoT to self-generate fine-tuning examples by and for very large proprietary models. There is a brief glimpse into fine-tuning on smaller distilled models, but the results are limited to one dataset and very large teacher models that are inaccessible to the general community. In contrast, we provide a rich set of results and qualitative/quantitative analysis on a wide range of datasets, using open-source models that are small and accessible to everyone.</p>
<p>Knowledge distillation Typically, knowledge distillation (KD) refers to training small models derived from large models in order to reduce model size and latency, while still preserving accuracy and capacity to generalize (Hinton et al., 2015; Sanh et al., 2019). Essentially, KD is a form of model compression, making efficient deployment to capacity-limited devices possible (Bucilua et al., 2006). We note that our work could also be considered a distant variant of KD (Gou et al., 2021), similar to works on improving prompt-based methods such as Yoo et al. (2021); Schick and Schütze (2021b,a); Zelikman et al. (2022), or works on datafree distillation (Micaelli and Storkey, 2019; Nayak et al., 2019; Shen et al., 2021), where the transfer data is synthetically generated from a large teacher model. Similarly, sequence-level distillation, i.e. training a student model on sequence distributions of a larger teacher, can make neural machine translation more efficient (Kim and Rush, 2016). Despite being similar in spirit, our method still distinguishes itself from such previous work. The role of the teacher model in our method is to teach the notion of intermediate reasoning. It is not the specific output that is the main supervising signal for reasoning, but rather the generation's structure. Hence, we do not use a standard KD loss function that reflects trying to match the teacher output. Adding to this, we note that our diverse reasoning is also unusual in the context of KD, where it is e.g. sufficient in practice to only generate one teacher sequence for sequence level distillation.</p>
<h2>3 Chain-of-Thought Fine-Tuning</h2>
<p>We propose Fine-tune-CoT, a task-agnostic approach to enable chain-of-thought reasoning in small LMs. The core idea is to generate reasoning samples from very large teacher models using CoT prompting and subsequently fine-tune small student models using the generated samples. This approach preserves the versatility of prompt-based CoT methods while overcoming their reliance on prohibitively large models. To maximize versatility and minimize teacher inference costs, we use the task-agnostic Zero-shot-CoT prompting method (Kojima et al., 2022) on teacher models, as it does not require any reasoning examples or long inference context. We discuss our choice of teacher CoT prompting method in Section 7.3. In the following, we characterize Fine-tune-CoT in three distinct steps. We also provide a visual overview in Figure 2.</p>
<p>Step 1. Reasoning generation First, we utilize a large teacher model to generate CoT reasoning explanations for a given task. Consider a standard sample $S_{i}$ consisting of a question $q_{i}$ and its true answer $a_{i}$. Using Zero-shot-CoT ${ }^{4}$. we prompt the teacher model to generate a reasoning explanation, or rationale, $\hat{r}<em i="i">{i}$ to solve question $q</em>}$ and make a final answer prediction $\hat{a<em i="i">{i}$. The resulting text sequence, including the prompt and generations, takes the following form: "Q: $<q_{i}>$. A: Let's think step by step. $&lt;\hat{r}</em>&gt;$ ".}&gt;$ Therefore, the answer is $&lt;\hat{a}_{i</p>
<p>Step 2. Curation Next, we filter the generated samples and reformat them into prompt-completion pairs. For filtering, we simply compare the final prediction of the teacher model $\hat{a}<em i="i">{i}$ with the groundtruth answer $a</em>}$, following previous works (Zelikman et al., 2022; Huang et al., 2022). Note that this filtering incurs some loss of training samples. For all instances $i$ where $\hat{a<em i="i">{i}=a</em>}$, we repackage $\left(S_{i}, \hat{r<em i="i">{i}, \hat{a}</em>&gt;$ $--&gt;<a_{i}>$ END". We note that answer-based filtering does not ensure the correctness of the rationales, especially for multi-choice questions. We provide an analysis in Appendix E. 1 regarding this important detail which has not been addressed in concurrent work.}\right)$ into a reasoning sample $S_{i}^{\prime}=\left(p_{i}, c_{i}\right)$, a prompt-completion pair. To maximize inferencetime efficiency, we use special-character based delimiters to minimize token usage. Specifically, $p_{i}$ and $c_{i}$ each take the form of " $<q_{i}>$ # # #" and " $&lt;\hat{r}_{i</p>
<p>Step 3. Fine-tune Finally, we fine-tune a small pre-trained student model on the assembled reasoning samples. We use the same training objective of that used during pre-training, i.e., autoregressive language modeling objective, or next-token prediction (Radford et al., 2018).</p>
<p>Diverse reasoning To maximize the teaching effects of Fine-tune-CoT, we can generate multiple reasoning explanations for each training sample. This approach is motivated by the intuition that multiple reasoning paths can be used to solve complex tasks, i.e., type-2 tasks (Evans, 2010). We posit that this unique feature of complex tasks, in tandem with the stochastic generation abilities of the teacher model, can enable diverse reasoning to significantly boost reasoning supervision simply through additional teacher inference. In detail, for a given sample $S_{i}$, instead of applying Zero-shot-CoT using greedy decoding to obtain a single explanation-answer pair $\left(\hat{e}<em i="i">{i}, \hat{a}</em>}\right)$, we use a stochastic sampling strategy, i.e., temperature sampling with large $T$, to obtain $D$ distinct generations $\left{\left(\hat{r<em i="i" j="j">{i j}, \hat{a}</em>$. Subsequent reasoning sample curation and fine-tuning then proceed as before. We refer to $D$ as the degree of reasoning diversity. A similar approach is used in Wang et al. (2022); Huang et al. (2022), where multiple CoT outputs are generated and marginalized to find the optimal answer. However, the effects of such diverse reasoning on teaching student models has not been acknowledged or thoroughly investigated in concurrent work (Huang et al., 2022; Li et al., 2022a; Magister et al., 2022; Fu et al., 2023). We note that diverse reasoning imposes an important tradeoff between the development cost and inference cost/quality of student models which we discuss in Section 5.3.}\right)\right}_{j}^{D</p>
<h2>4 Experiments</h2>
<p>Tasks and datasets We evaluate our method on 12 datasets pertaining to four categories of complex reasoning, following Kojima et al. (2022). These include arithmetic (SingleEq, AddSub, MultiArith, GSM8K, SVAMP), other (Date Understanding, Tracking Shuffled Objects), symbolic (Last Letter Concatenation, Coin Flip), and common sense (CommonSenseQA, StrategyQA) reasoning. We provide details and references in Appendix B.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Single <br> Eq</th>
<th style="text-align: center;">Add Sub</th>
<th style="text-align: center;">Multi <br> Arith</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">Aqua</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Date Understanding</th>
<th style="text-align: center;">Shuffled Objects</th>
<th style="text-align: center;">Last Letter</th>
<th style="text-align: center;">Coin Flip</th>
<th style="text-align: center;">Common SenseQA</th>
<th style="text-align: center;">Strategy <br> QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: center;">Teacher: InstructGPT (text-davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot-CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">82.24</td>
<td style="text-align: center;">78.99</td>
<td style="text-align: center;">78.89</td>
<td style="text-align: center;">40.26</td>
<td style="text-align: center;">34.25</td>
<td style="text-align: center;">64.67</td>
<td style="text-align: center;">73.87</td>
<td style="text-align: center;">50.22</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">92.67</td>
<td style="text-align: center;">61.75</td>
<td style="text-align: center;">53.57</td>
</tr>
<tr>
<td style="text-align: center;">Student: GPT-3 (ada, babbage, curie)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">16.54</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">9.91</td>
<td style="text-align: center;">32.89</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">56.67</td>
<td style="text-align: center;">20.23</td>
<td style="text-align: center;">52.98</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot-CoT</td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">21.26</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">46.67</td>
<td style="text-align: center;">19.98</td>
<td style="text-align: center;">51.09</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot-CoT</td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">31.93</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">15.75</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">12.84</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">24.73</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune</td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">24.34</td>
<td style="text-align: center;">25.21</td>
<td style="text-align: center;">15.00</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">15.35</td>
<td style="text-align: center;">20.67</td>
<td style="text-align: center;">14.41</td>
<td style="text-align: center;">33.78</td>
<td style="text-align: center;">32.67</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">76.17</td>
<td style="text-align: center;">65.21</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune-CoT</td>
<td style="text-align: center;">0.3B</td>
<td style="text-align: center;">7.24</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">23.62</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">49.33</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">32.68</td>
<td style="text-align: center;">52.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">19.69</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">52.44</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">43.08</td>
<td style="text-align: center;">52.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">20.39</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">6.75</td>
<td style="text-align: center;">24.02</td>
<td style="text-align: center;">12.67</td>
<td style="text-align: center;">60.36</td>
<td style="text-align: center;">64.44</td>
<td style="text-align: center;">52.67</td>
<td style="text-align: center;">98.67</td>
<td style="text-align: center;">56.76</td>
<td style="text-align: center;">55.02</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune-CoT</td>
<td style="text-align: center;">0.3B</td>
<td style="text-align: center;">9.21</td>
<td style="text-align: center;">10.08</td>
<td style="text-align: center;">23.89</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.33</td>
<td style="text-align: center;">58.56</td>
<td style="text-align: center;">61.78</td>
<td style="text-align: center;">59.33</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.21</td>
</tr>
<tr>
<td style="text-align: center;">w/ diverse reasoning</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">19.33</td>
<td style="text-align: center;">27.78</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.33</td>
<td style="text-align: center;">70.27</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">60.67</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">24.34</td>
<td style="text-align: center;">31.09</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.33</td>
<td style="text-align: center;">83.78</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.22</td>
</tr>
</tbody>
</table>
<p>Table 1: Fine-tune-CoT Performance. Accuracy (\%) of OpenAI models on 12 tasks under Fine-tune-CoT (with diverse reasoning) and baseline methods. 'Random' refers to random-guess performance derived based on the number of choices in multi-choice tasks. For diverse reasoning, we report results for maximum degree $D$ considered: $D=64$ for MultiArith and SVAMP; $D=8$ for other datasets. We omit diverse reasoning for large datasets due to resource constraints and Few-shot-CoT for Tracking Shuffled Objects due to absence of prompts.</p>
<p>Models For teacher models, we use four variants of GPT-3 175B (Brown et al., 2020), provided by the OpenAI API. Unless otherwise stated, we use text-davinci-002 based on InstructGPT 175B (Ouyang et al., 2022) as the teacher for Fine-tune-CoT. For student models, we consider four popular model families. For our main experiments, we use GPT-3 {ada, babbage, curie} as they are readily available for fine-tuning via the OpenAI API. Due to the blackbox nature of the API, we also consider various open-source models under controlled settings. We use GPT-2 {Small, Medium, Large} (Radford et al., 2019) and T5${$ Small, Base, Large} (Raffel et al., 2020) as representative model families for decoder-only and encoder-decoder architectures, respectively. We also use the instruction-tuned version of T5, Flan-T5-{Small, Base, Large} (Chung et al., 2022), to investigate the effects of instruction tuning on student models, prior to applying Fine-tune-CoT. These student models are $25-2500 \mathrm{x}$ smaller than the teacher model, thus considerably more feasible for realworld deployment. We provide details on models and API usage in Appendix C.</p>
<p>Baseline methods We provide a comparison of Fine-tune-CoT (ours) with four baseline methods: standard zero-shot prompting, vanilla fine-tuning, Zero-shot-CoT (Kojima et al., 2022), and Few-shotCoT (Wei et al., 2022b). Given a training sample $\left{\left(q_{i}, a_{i}\right)\right}_{i}$, we use a simple format "Q: $<q_{i}>$ " for</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Model <br> Updates</th>
<th style="text-align: center;">CoT <br> Output</th>
<th style="text-align: center;">Sample <br> Utilization</th>
<th style="text-align: center;">Teacher <br> Usage</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">(Radford et al., 2019)</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-CoT</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">(Kojima et al., 2022)</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot-CoT</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\triangle$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">(Wei et al., 2022b)</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\beta}$</td>
<td style="text-align: center;">(Radford et al., 2018)</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune-CoT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ours</td>
</tr>
</tbody>
</table>
<p>Table 2: Taxonomy of methods. CoT methods are more interpretable due to reasoning output. While Few-shot-CoT can utilize few in-context samples, fine-tuning can utilize any number of training samples via model updates. Fine-tune-CoT benefits from the reasoning capabilities of teacher models.
zero-shot prompting. For vanilla fine-tuning, we format the prompt and completion as " $<q_{i}># # #$ " and " $<a_{i}>$ END", respectively. We clarify the taxonomy of methods in Table 2. For text generation, we use greedy decoding following Wei et al. (2022b); Kojima et al. (2022) throughout our experiments, except for diverse reasoning. For diverse reasoning on the teacher, we use temperature sampling with $T=0.7$, following Wang et al. (2022). We provide experimental details in Appendix A.</p>
<h3>4.1 Results</h3>
<p>In this section, we present the reasoning performance of models using Fine-tune-CoT and diverse reasoning. We compare with various baselines and demonstrate the scalability of our method across four axes: degree of diverse reasoning (Figure 3), dataset size (Figure 4), performance of the teacher (Figure 5), and size of the student model (Figure 6).</p>
<p>We present our findings on GPT-3 models in the main text and defer results on open-source models to Appendix G, with a brief summary at the end of this section.</p>
<p>Fine-tune-CoT elicits complex reasoning in small models Table 1 summarizes the accuracy of student models using the proposed Fine-tuneCoT, compared to prompt-based CoT baselines as well as standard fine-tuning. While Zero-shot-CoT exhibits remarkable performance on the very large 175B model (Kojima et al., 2022), it fails to enable complex reasoning in all three smaller models, showing near-negligible performance across all tasks. We also find that small models are unable to approach these tasks under standard zero-shot prompting. On the other hand, Fine-tune-CoT elicits notable reasoning performance, demonstrating significant gains over Zero-shot-CoT when using smaller models and outperforming both fine-tuning and Few-shot-CoT in more than half of the tasks. For complex arithmetic, Fine-tune-CoT achieves a notable 33\% accuracy on MultiArith while Zero-shot-CoT only reaches 5\%. Few-shot-CoT and fine-tuning only achieve $10 \%$ and $15 \%$, respectively. For two commonsense reasoning tasks, our method outperforms the near-random performance of Zero-shot-CoT by $37 \%$ and $5 \%$, respectively. Furthermore, it surpasses Few-shot-CoT on CommonSenseQA by $32 \%$ and performs similarly on StrategyQA. We observe that Fine-tune-CoT performance is most notable for tasks that are not overly complex, which include other reasoning tasks (Date Understanding, Shuffled Objects) and symbolic reasoning (Last Letter, Coin Flip), significantly outperforming other baselines. See Appendix Table 9 for performance of all students.</p>
<p>Small models can outperform very large teachers in reasoning Table 1 also shows that Fine-tune-CoT is highly effective on small models compared to the large 175B teacher model. For the tasks Shuffled Objects and Coin Flip, Fine-tuneCoT is shown to outperform the teacher model using either 1.3B or 6.7B parameters, i.e., reducing the number of required parameters by approx. $25-100 \mathrm{x}$. We also find that Fine-tune-CoT with the very small 0.3 B model consistently outperforms the 6.7B model under Zero-shot-CoT, demonstrating that our method is able to unlock a wider range of capabilities compared to the baseline, even when model size is vastly reduced.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Diverse reasoning performance. Accuracy (\%) of GPT-3 student models under Fine-tune-CoT with varying degrees of diverse reasoning $D$. Baseline performance of the largest model under vanilla fine-tuning and Few-shot-CoT are shown for comparison. Diverse reasoning is not applicable to the baselines.</p>
<p>Diverse reasoning substantially improves Fine-tune-CoT performance. To examine the learning effects of diverse reasoning and compare it with two baselines given by fine-tuning and Few-shotCoT, we apply Fine-tune-CoT using 1-64 reasoning explanations per sample across three model scales on MultiArith and SVAMP ${ }^{5}$. Figure 3 shows that diverse reasoning can significantly improve the performance of student models using Fine-tuneCoT. For the 6.7B student model, we find a boost of around $26 \%$ on MultiArith, and around $17 \%$ on SVAMP. We also note that using diverse reasoning always leads to outperforming the baseline within the respective model size, and can even boost performance of our method beyond that of a larger model that does not use diverse reasoning. This even includes the teacher in two cases (Date Understanding, Last Letter). Moreover, we find that diverse reasoning can boost the performance of Fine-tune-CoT to surpass that of both Few-shot-CoT and vanilla fine-tuning across all model sizes. We posit that due to our focus on complex tasks, the diversity of reasoning paths and linguistic templates can substantially aid in teaching student models to reason.</p>
<p>Fine-tune-CoT consistently benefits from more data. We perform an ablation on dataset size to study the performance scalability of our method with dataset size. We see that the performance of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Effects of dataset size. Accuracy (\%) of the GPT-3 6.7B student model by dataset size under vanilla fine-tuning vs Fine-tune-CoT (with diverse reasoning). Baseline performance under Few-shot-CoT is shown for comparison. Diverse reasoning is not applicable to standard fine-tuning. We show diverse reasoning performance with $D=64$ for MultiArith and SVAMP; $D=8$ for others.
the 6.7B model clearly scales with the size of the dataset, independent of the task. In comparison, vanilla fine-tuning does not always exhibit this behavior. In fact, for Date Understanding, we find that an increase in dataset size harms the performance of fine-tuning. Furthermore, Fine-tune-CoT sees additional benefits from diverse reasoning, which is not applicable in standard fine-tuning.</p>
<p>Better reasoners are better teachers Next, we can ask the question of whether the performance of the teacher is correlated with that of their student when using Fine-tune-CoT. To test this, we use different versions of GPT-3 as teacher models, keeping the size of the student model constant at 6.7B parameters (Figure 5). We find that student performance indeed scales with teacher performance, particularly in the less complex tasks Date Understanding and Last Letter. There, the performance of the student matches the performance of the teacher very closely. This also fits with our observations in Appendix D, which show that the successes and failures of teachers are correlated with those of the students. We note that this scaling effect is in contrast not a given in knowledge distillation, where more accurate teachers do not always result in better students (Menon et al., 2021).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Effects of teacher performance on students. Accuracy (\%) of teacher models (Zero-shot-CoT) and their corresponding GPT-3 6.7B student models (Fine-tune-CoT). Baseline performance under vanilla finetuning and Few-shot-CoT are shown for comparison. Teacher models are not applicable to Few-shot-CoT which uses few human-annotated examples.</p>
<p>Fine-tune-CoT performance scales with model size for small LMs Finally, we explore the effect of scaling up student model size on our method, and compare it with the effects of increasingly larger student models in Few-shot-CoT as well as vanilla fine-tuning. We can observe that the performance of Fine-tune-CoT is consistently scalable with student size (Figure 6). In contrast, the two baselines do not always exhibit the same behavior: in Date Understanding, neither Few-shot-CoT nor vanilla fine-tuning results in scalable performance.</p>
<p>Results on open-source student models Overall, our findings on T5, Flan-T5, and GPT-2 show similar trends to those observed on GPT-3. Small models exhibit near-random performance under standard zero-shot or CoT prompting in nearly all cases. Notable, we find that encoder-decoder models, i.e., T5 and Flan-T5, show noteworthy performance under standard fine-tuning, suggesting that causal masking may be a bottleneck to reasoning in decoder-based language models in the absence of CoT output. Fine-tune-CoT consistently outperforms prompt-based baselines and is comparable or superior to vanilla fine-tuning. Diverse reasoning improves performance even further, often exhibiting significant gains. We report our full findings on open-source models in Appendix G.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Effects of student model scale. Accuracy (\%) of GPT-3 student models of various sizes under Few-shot-CoT, vanilla fine-tuning, and Fine-tune-CoT (with diverse reasoning). The hatched portion indicates the performance boost of Fine-tune-CoT when using diverse reasoning with $D=64$ on MultiArith and SVAMP; $D=8$ on others.</p>
<h3>4.2 Analysis</h3>
<p>Sample study To identify the strengths and weaknesses of our method, we perform a thorough sample study across all datasets and methods. Across all arithmetic tasks, we find that a large portion of errors arises from calculations. MultiArith and SVAMP also show many semantic errors, but these are significantly reduced with diverse reasoning. For difficult tasks such as GSM8K and AQUA, we found that all methods tend to struggle. We found that our method is highly effective in text-based tasks, excluding commonsense reasoning, as well as tasks that contain common linguistic patterns. On the other hand, we find that students under Zero-shot-CoT often repeat questions or produce incoherent repetitive statements. While Few-shotCoT elicits step-by-step sentences, the student models rarely seem to understand the semantics of the question, and generations often contain logical or commonsense errors. For details on our sample study, see Appendix D.</p>
<p>Nuances of fine-tuning on CoT reasoning We shed light on nuances that have often been overlooked in previous or concurrent work (Wei et al., 2022b; Li et al., 2022a; Magister et al., 2022). First, we acknowledge the possibility that correct samples may contain incorrect reasoning. In fact, we find that $27.6 \%$ of correct teacher completions for Date Understanding contained reasoning errors.
However, ablations on rationale filtering suggest that these incorrect rationales can aid in student supervision (Appendix E.1). Secondly, we find that common maximum sequence lengths used for CoT generations often lead to incomplete answers. We observe that reasoning length differs among datasets, and longer generations typically improve accuracy, but may not be beneficial for fine-tuning (Appendix E.2). Lastly, we find that many datasets are comprised of samples that share common templates, potentially compromising the validity of our random train-test splits. To address this, we evaluate our method on manual template-wise data splits, and confirm that students retain meaningful reasoning capabilities (Appendix E.3).</p>
<h2>5 Discussion</h2>
<h3>5.1 Accessibility of Fine-tune-CoT</h3>
<p>Owing to the versatility of the teacher generation method, i.e., Zero-shot-CoT, our method can be readily applied to any complex task without taskspecific engineering. Rationales can be readily generated using publicly available APIs such as those provided by OpenAI or Anthropic. This makes it viable to obtain CoT training data in low-resource scenarios, which not only outperforms standard fine-tuning, but elicits the student to output interpretable explanations. Fine-tuning and inference on student models can also be performed on much more accessible hardware, in contrast to very large models. This can reduce long-term inference costs and minimize environmental impact while making our method fully accessible to a wide community.</p>
<h3>5.2 Viability of Fine-tune-CoT</h3>
<p>While Fine-tune-CoT elicits notable complex reasoning capabilities in small models, performance on some difficult datasets would not be considered viable for real-world use, such as $30.33 \%$ on SVAMP. However, our findings in Section 4.1 indicates significant potential for improvement, as our method is shown to be uniquely scalable with (1) diverse reasoning, (2) dataset size, (3) teacher model performance, and (4) student model size. The use of diverse reasoning and better teacher models is especially promising, as these can benefit from improved teacher LLM performance and inference costs in the future. In addition, it is possible to incorporate recent CoT methods, which lead to significant performance improvements, in student models, which we discuss in Section 7.3.</p>
<h3>5.3 Tradeoffs of Fine-tune-CoT</h3>
<p>The aforementioned opportunities to enhance Fine-tune-CoT also pose many important tradeoffs. We leave further analysis to future work.</p>
<p>Degree of diverse reasoning The performance benefits of diverse reasoning come at the cost of additional teacher inference. Therefore, diverse reasoning poses a tradeoff between development cost vs inference cost/quality. In other words, performance gains from diverse reasoning may be utilized to enhance student performance or alleviate the need for larger student models. This must also be taken into account for fair evaluation of similar distillation methods in the future.</p>
<p>Data acquisition Data annotation and diverse reasoning can both be used to enlarge fine-tuning data, but each have their associated costs. We note that the cost of diverse reasoning is linear to the number of generated rationale and the number of original samples. Despite this, it can still be a costeffective alternative to hand-annotating additional data. A preliminary cost analysis in Appendix F shows that the pareto front of data-acquisition-cost to performance always incorporates diverse reasoning. We expect that the cost benefits of diverse reasoning will continue to improve with improvements in teacher model performance and efficiency.</p>
<h3>5.4 Emergence of CoT reasoning</h3>
<p>The emergence of abilities such as CoT reasoning has become a point of interest in recent works (Wei et al., 2022b,a; Schaeffer et al., 2023). We note that the efficacy of Fine-tune-CoT on small models does not disprove this emergence, as our method is based on fine-tuning. However, we believe our results can provide some insight into this phenomena.</p>
<p>Why does Fine-tune-CoT work in small models? In a seminal work, Wei et al. (2022b) suggests that CoT reasoning is an emergent ability of scale-more specifically, a complicated phenomena involving a variety of emergent abilities, such as semantic understanding, symbol mapping, arithmetic ability. However, our sample studies suggest that Fine-tune-CoT elicits these emergent abilities even in relatively small models (see Appendix D). We explain this from two perspectives. First, Wei et al. (2022b) demonstrated the emergence of reasoning abilties by identifying a reduction in the frequency of reasoning errors with larger model scale. Similarly, we find that more potent forms
of supervision also lead to a gradual reduction in reasoning errors. For example, we found a clear distinction between Zero-, Few-shot-CoT and Fine-tune-CoT (with diverse reasoning) in the frequency and severity of semantic errors, i.e., understanding complex questions, and calculation errors. This suggests that explicit supervision on reasoning can also lead to the emergence of reasoning abilities. Second, we qualitatively find that students show capabilities that are reminiscent of the larger teacher model. We found that students can recognize common semantics and reasoning cues of the given task, and is able to imitate the process of splitting large tasks into subtasks. This suggests that it is possible to learn reasoning abilities pertaining to a particular domain. We posit that this is possible in small models due to the limited domain of reasoning, and may not be applicable in reasoning tasks that require large domains of knowledge.</p>
<p>Distillation of emergent abilities Chain-ofthought reasoning has been recognized as a prime example of emergent abilities in very large language models (Wei et al., 2022a). Our findings show that it is possible to distill this ability, under certain domains, to much smaller models simply through fine-tuning. The potential for distillation implies that future advancements in language models may lead to emergent abilities that are not only pertinent to those larger models, but could also have a broader impact, cascading benefits to smaller models.</p>
<h2>6 Conclusion</h2>
<p>We have proposed Fine-tune-CoT, a method that uses LLMs as reasoning teachers to transfer the broad reasoning capabilities previously found in $&gt;100 \mathrm{~B}$ models to student models as small as 0.3 B . We propose diverse reasoning as a novel approach to maximize these teaching effects, exploiting the unique characteristics of this new learning setup to vastly improve performance. Our extensive experiments show that Fine-tune-CoT elicits significant reasoning performance in small models, thus demonstrating the distillation of CoT reasoning which has been considered an emergent ability of scale. By leveraging publicly available models with zero-shot prompting, we demonstrate a taskagnostic approach to elicit reasoning performance in small models, making complex reasoning feasible for real-world deployment and accessible to the broader community.</p>
<h2>7 Limitations</h2>
<h3>7.1 Towards concise answers</h3>
<p>Sample studies show that rationales output from student models may occasionally be repetitive and digressive. This is undesirable in terms of inference-time efficiency as well as interpretability. As a minor optimization to inference computation, we construct our fine-tuning sample templates using special-character based delimiters instead of natural language used in concurrent work (Huang et al., 2022) to minimize sequence length. Preliminary findings showed this had no significant impact on reasoning performance. More importantly, it is desirable to train student models to generate concise answers in terms of substance. Appendix E. 2 hints at the possibility for this, showing that finetuning on shorter reasoning samples causes the student model to also produce shorter rationales.</p>
<h3>7.2 Exploring a wider array of models</h3>
<p>We note that the performance of our method is currently not state-of-the-art. However, it can benefit from advances in teacher models as well as other prompting methods. For example, future work should include a wider array of teachers, such as the highly versatile ChatGPT, which typically generates detailed long responses that may be able to impart more knowledge to the student. More recent models such as GPT-4 have demonstrated significant advances in complex reasoning abilities, which may improve the efficacy of Fine-tuneCoT on very difficult datasets, such as GSM8K. Conversely, our method could prove even more advantageous when applied to recent models with improved efficiency, such as those based on the recent LLaMA model (Touvron et al., 2023), which has sparked a proliferation of work focused on compact language models. Both of these avenues are promising for future work.</p>
<h3>7.3 Better CoT inference methods</h3>
<p>The use of diverse reasoning and better teacher or student models is especially promising, as it is possible to leverage future improvements in model performance and decreased inference costs. However, we can also consider other ways to boost performance, such as using different prompting methods. For example, previous work shows that Few-shot-CoT (Wei et al., 2022b) can improve accuracy over Zero-shot-CoT by a wide margin, e.g., going from $78.7 \%$ to $93.0 \%$ on MultiArith (Kojima
et al., 2022). However, our choice to use Zero-shot-CoT to generate reasoning samples from the teacher model is motivated by the fact that Few-shot-CoT requires a significantly larger inference context. With the current pricing models based on token usage, the typical setup of 8 -shot CoT would cost approximately 8 times more compared to Zero-shot-CoT. Therefore, we see a tradeoff between using the inference budget for Few-shot-CoT and using it for diverse reasoning with Zero-shotCoT. On the other hand, we also note that recent works introduce various ways to improve CoT reasoning performance substantially (often to nearperfect levels), which can be applied to our student models. These include refinement over repeated inferences (Wang et al., 2022; Li et al., 2022b) and self-improvement (Zelikman et al., 2022; Huang et al., 2022). In particular, self-consistency (Wang et al., 2022) can be utilizied on unlabeled samples to maximize the teaching signal. In contrast, we aim to achieve CoT reasoning without the inference time cost incurred by very large LMs. Future work is needed to incorporate these methods into Fine-tune-CoT while minimizing development and inference costs.</p>
<h3>7.4 Connection with knowledge distillation</h3>
<p>We assume that there is a lot of potential in strengthening the connections between knowledge distillation and our method. We have already seen in this work that our method shares some characteristics with KD, such as the fact that the knowledge of intermediate reasoning imparted by using also incorrect samples can have positive effects on student accuracy, akin to "dark knowledge" (Menon et al., 2021) that is transferred by training on teacher output logits and not one-hot labels. We have seen that this leads to a quantity-quality tradeoff when it comes to the ability of the student model to generalize: having fewer but perfectly curated reasoning samples is not necessarily as helpful as having a larger amount of reasoning samples that might not always be fully correct. On the other hand, we have also found that more accurate teachers do lead to more accurate students, which is not always the case in KD (Müller et al., 2019). It would therefore be of interest for future work to formalize the connection of Fine-tune-CoT with classic KD methods, and potentially test the use of a different distillation loss function that takes the teacher's actual output into account.</p>
<h2>8 Ethics Statement</h2>
<p>Our work presents various challenges and opportunities in terms of bias and toxicity in language models. It is widely known that LLMs trained on large corpora have been shown to capture biases found in the training data (Brown et al., 2020; Chowdhery et al., 2022). Since our student models are trained on reasoning samples generated by these LLMs, it is possible that such characteristics of the teacher model can get passed along to the student. This is an important point to consider when selecting the teacher model for our method.</p>
<p>Our training setup, however, does offer a unique opportunity to minimize bias and toxicity in student models, by influencing the samples used for fine-tuning. One approach would be to augment the curating step of Fine-tune-CoT to filter out biased or toxic samples. It is possible to automate this via neural network-based verifiers, previously used to filter correct output (Cobbe et al., 2021; Li et al., 2022b). Alternatively, one may consider optimizing the CoT prompts to minimize bias and toxicity in teacher-generated rationales.</p>
<p>We note that bad actors can also potentially take advantage of our method to utilize complex reasoning for malicious purposes and deploy it at scale, using small models. This highlights the importance of safeguarding the potential capabilities of LLMs by major providers. To prevent the distillation of malicious reasoning abilities in small (or large) students, future work in identifying usage patterns involved in these distillation schemes may help providers apply more stringent precautions to these use cases.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by Institute of Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant funded by Korea government (MSIT) [No. 2021-0-00907, Development of Adaptive and Lightweight Edge-Collaborative Analysis Technology for Enabling Proactively Immediate Response and Rapid Learning, 90\%], [No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), 5\%], and the Stochastic Analysis and Application Research Center (SAARC) under the National Research Foundation of Korea grant (NRF-2019R1A5A1028324, $5 \%)$.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06, page 535-541, New York, NY, USA. Association for Computing Machinery.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. arXiv preprint, arXiv:2207.07051.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Jonathan St BT Evans. 2010. Intuition and reasoning: A dual-process perspective. Psychological Inquiry, 21(4):313-326.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</p>
<p>and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789-1819.</p>
<p>Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pages 523-533. Citeseer.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</p>
<p>Yoon Kim and Alexander M. Rush. 2016. Sequencelevel knowledge distillation. arXiv preprint, arXiv: 1606.07947.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. arXiv preprint, arXiv: 2206.14858.</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan. 2022a. Explanations from large language models make small reasoners better. arXiv preprint, arXiv: 2210.06726.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.</p>
<p>Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards zero-shot language understanding. arXiv preprint, arXiv: 2202.04538.</p>
<p>Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. 2021. A statistical perspective on distillation. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7632-7642. PMLR.</p>
<p>Paul Micaelli and Amos Storkey. 2019. Zero-Shot Knowledge Transfer via Adversarial Belief Matching, chapter -. Curran Associates Inc., Red Hook, NY, USA.</p>
<p>Rafael Müller, Simon Kornblith, and Geoffrey Hinton. 2019. When Does Label Smoothing Help? Curran Associates Inc., Red Hook, NY, USA.</p>
<p>Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, Venkatesh Babu Radhakrishnan, and Anirban Chakraborty. 2019. Zero-shot knowledge distillation in deep networks. In International Conference on Machine Learning, pages 4743-4751. PMLR.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma,</p>
<p>David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. -.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Yasaman Razeghi, Robert L. Logan, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint, arXiv:2202.07206.</p>
<p>Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Helmut Schmid, and Hinrich Schütze. 2020. Automatically identifying words that can serve as labels for few-shot text classification. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5569-5578, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021b. It's not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Chengchao Shen, Xinchao Wang, Youtan Yin, Jie Song, Sihui Luo, and Mingli Song. 2021. Progressive network grafting for few-shot knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 2541-2549.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.</p>
<p>Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo Lee, and Woomyeong Park. 2021. Gpt3mix: Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR.</p>
<h2>A Experimental Details</h2>
<h2>A. 1 Generation</h2>
<p>Maximum sequence length For the maximum sequence length of teacher-generated rationales, $\hat{r}<em r="r">{i}$, we use $L</em>=128$ is insufficient for many tasks, as discussed in Appendix E.2.}=128$, following Kojima et al. (2022), unless stated otherwise. For the maximum sequence length of the student model predictions, we use $L_{p}=1024$, unless stated otherwise. We retroactively applied $L_{p}=1024$ as the default, after discovering that $L_{p</p>
<p>Sampling temperature We apply greedy decoding for all generations, except diverse reasoning, to obtain deterministic results following (Wei et al., 2022b; Kojima et al., 2022). For diverse reasoning, we use temperature sampling with $T=0.7$ to obtain diverse samples, following a similar approach from Wang et al. (2022).</p>
<h2>A. 2 Answer cleansing</h2>
<p>We follow the method used in Kojima et al. (2022) to cleanse answers generated by models to assess their correctness.</p>
<h2>A. 3 Few-shot-CoT exemplars</h2>
<p>For Few-shot-CoT prompting, we use exemplars provided by Wei et al. (2022b), with some minor formatting adaptations for consistency with our other experiments. For Last Letter Concatenation and Coin Flip, for which Few-shot-CoT prompts are not provided, we use 8 training samples used in our 8 -shot data experiments shown in Figure 4 and adapt them for Few-shot-CoT using the format of Wei et al. (2022b). This was not applicable to Tracking Shuffled Objects, therefore it was omitted from Few-shot-CoT experiments.</p>
<h2>A. 4 Fine-tuning OpenAI models</h2>
<p>We use default hyperparameters set by the OpenAI API for both vanilla fine-tuning and Fine-tune-CoT. While the specifics of the fine-tuning API is not publicly known, some details on hyperparameters are documented in the API reference ${ }^{6}$. According to the default settings, our models are trained for 4 epochs. The batch size and learning rate determined based on the number of examples used for training. The batch size is set to $0.2 \%$ of the number of training examples capped at 256. The</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learning rate is set to $0.05,0.1$, or 0.2 times that of the learning rate used to pre-train the base model, depending on the batch size. Training loss is also applied to the prompt portion of the training examples, i.e., the question, with a small weight of 0.01 . Based on API pricing, we posit that OpenAI employs a form of parameter efficient fine-tuning such as LoRA (Hu et al., 2021) for their fine-tuning API instead of updating all model parameters.</p>
<h2>A. 5 Fine-tuning open source models</h2>
<p>For vanilla fine-tuning and Fine-tune-CoT on open source models, we strictly control for hyperparameters. Across all experiments, we fine-tune the entire model with a fixed learning rate of 3e-4 and batch size of 8 . Upon inspection of model performance under various learning rates and batch sizes, we found that optimal parameters varies among datasets, even between those with similar number of reasoning samples. We train all models for a maximum of 20 epochs, which we found to be sufficient for test accuracy to plateau. We report the best test accuracy from 20 epochs, but found that performance varies significantly between epochs. Overall, we found that performance by epoch is stable for larger models, and that instruction-tuned Flan-T5 is more stable compared to T5. Similar to learning rate and batch size, the optimal number of epochs also varies between datasets, even those with similar number of reasoning samples. Based on the above, we note that our reported performances of fine-tuned open-source models may be significantly under-estimated compared to those with optimal hyperparameters, and recommend practitioners to optimize hyperparameters using a separate validation set, per each training setting.</p>
<h2>B Datasets</h2>
<p>We provide a summary of datasets used in our experiments, including their original licenses, in Appendix Table 3. We consider the 12 datasets used in Kojima et al. (2022) to measure reasoning performance. For Last Letter Concatenation and Coin Flip, we use the publicly available data provided by Kojima et al. (2022).</p>
<p>Train-test split Contrary to previous works on prompt-based CoT such as Wei et al. (2022b); Kojima et al. (2022), our fine-tuning approach requires distinct sets of samples for training and testing. If</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Choices</th>
<th style="text-align: center;">Training Samples</th>
<th style="text-align: center;">Test Samples</th>
<th style="text-align: center;">Data Split</th>
<th style="text-align: center;">License</th>
<th style="text-align: left;">References</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">356</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">None</td>
<td style="text-align: left;">Koncel-Kedziorski et al. (2015)</td>
</tr>
<tr>
<td style="text-align: left;">AddSub</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">276</td>
<td style="text-align: center;">119</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Unspecified</td>
<td style="text-align: left;">Hosseini et al. (2014)</td>
</tr>
<tr>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Unspecified</td>
<td style="text-align: left;">Roy and Roth (2016)</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7473</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: left;">Cobbe et al. (2021)</td>
</tr>
<tr>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Custom</td>
<td style="text-align: center;">Apache-2.0</td>
<td style="text-align: left;">Ling et al. (2017)</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">MIT</td>
<td style="text-align: left;">Patel et al. (2021)</td>
</tr>
<tr>
<td style="text-align: left;">Date Understanding</td>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">258</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Apache-2.0</td>
<td style="text-align: left;">Srivastava et al. (2022)</td>
</tr>
<tr>
<td style="text-align: left;">Tracking Shuffled Objects</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">525</td>
<td style="text-align: center;">225</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Apache-2.0</td>
<td style="text-align: left;">Srivastava et al. (2022)</td>
</tr>
<tr>
<td style="text-align: left;">Last Letter Concatenation</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Unspecified</td>
<td style="text-align: left;">Wei et al. (2022b); Kojima et al. (2022)</td>
</tr>
<tr>
<td style="text-align: left;">Coin Flip</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Unspecified</td>
<td style="text-align: left;">Wei et al. (2022b); Kojima et al. (2022)</td>
</tr>
<tr>
<td style="text-align: left;">CommonSenseQA</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">9741</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Unspecified</td>
<td style="text-align: left;">Talmor et al. (2018)</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1603</td>
<td style="text-align: center;">687</td>
<td style="text-align: center;">70:30</td>
<td style="text-align: center;">Apache2.0</td>
<td style="text-align: left;">Geva et al. (2021)</td>
</tr>
</tbody>
</table>
<p>Table 3: Description of datasets used in our study.
separate subsets for training and testing (or development) are provided, we use those. Otherwise, we perform a samplewise random split with a train-test ratio of 70:30. For AQUA, due to the disproportionately large size of the original training set, we randomly sample 10,000 instances for training in our experiments. Note that due to the highly templated nature of many datasets, this naive data split may not be appropriate for evaluating reasoning capabilities. This is an important nuance of fine-tuning on CoT reasoning, which we address in Appendix E.3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Family</th>
<th style="text-align: left;">Params</th>
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Variant / Name in API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">175B</td>
<td style="text-align: left;">Teacher</td>
<td style="text-align: left;">davinci</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: left;">175B</td>
<td style="text-align: left;">Teacher</td>
<td style="text-align: left;">text-davinci-001</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: left;">175B</td>
<td style="text-align: left;">Teacher</td>
<td style="text-align: left;">text-davinci-002</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: left;">175B</td>
<td style="text-align: left;">Teacher</td>
<td style="text-align: left;">text-davinci-003</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">6.7B</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">curie</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">1.3B</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">babbage</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">0.3B</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">ada</td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">60M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Small</td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">220M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Base</td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">700M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Large</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5</td>
<td style="text-align: left;">60M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Small</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5</td>
<td style="text-align: left;">220M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Base</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5</td>
<td style="text-align: left;">700M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Large</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">124M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">(Small)</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">355M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Medium</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">774M</td>
<td style="text-align: left;">Student</td>
<td style="text-align: left;">Large</td>
</tr>
</tbody>
</table>
<p>Table 4: Description of models used in our study.</p>
<h2>C Models and API Usage</h2>
<p>Appendix Table 4 describes all teacher and student models used in our study. We use InstructGPT (Ouyang et al., 2022) as the default teacher model in our experiments, due to its superior zero-shot reasoning performance, compared to GPT-3 (Brown et al., 2020) of the same size (Kojima et al., 2022).</p>
<p>Specifically, we use text-davinci-002 at the default, as it was the best available model at the start of our experiments. We were unable to consider small InstructGPT models for fine-tuning, as it is not offered by the OpenAI API. We attach model size information based on (https://blog.eleuther.ai/gpt3-model-sizes/), following Kojima et al. (2022).</p>
<p>Our total expenditure for API usage, including all preliminary experiments, was $\$ 1,981$ USD. The majority of this expenditure occurred after September 1st, 2022, from which the pricing for inference on davinci models was $\$ 0.02 / 1 \mathrm{~K}$ tokens, among others. Between teacher model inference, student model {fine-tuning, inference}, the majority of API usage in terms of cost was focused on teacher model inference.</p>
<h2>D Sample Study</h2>
<p>To understand where our method makes mistakes, where diverse reasoning can improve performance, and where our method always performs well, we observe randomly sampled instances and analyze the reasoning performance on them. To do so, we compare its generations for these samples with (1) the output of the large teacher model, (2) a student model using Zero-shot-CoT (3) a student model using Few-shot-CoT, and (4) a student model using fine-tuning without chain of thought reasoning. Our analysis reflects our overall findings, which we exemplify with representative examples in Tables $10-13$.</p>
<h2>D. 1 Error analysis</h2>
<p>For our analysis of the most common types of errors, we take a look at datasets where we find particularly bad performance of our vanilla method, also in comparison to other students. We also dis-</p>
<p>cuss the benefits of using diverse reasoning in D.2. We summarize our observations below.</p>
<p>Difficult datasets First, we observe that the sets GSM8K and AQUA appear to be too difficult for any small student model, in particular, given that the teacher model gets below $50 \%$ accuracy on both. In fact, even correct answers are usually correct only by chance, due to the high complexity of the tasks (Appendix Tables 10a,b). For AQUA in particular, we note that while we occasionally find meaningful reasoning in the 6.7B student model, students clearly cannot sufficiently learn to solve the tasks. We do note however that of all the student methods, Fine-tune-CoT still gets the best performance in these two datasets. A similar, if less salient, issue arises for StrategyQA. Here, the teacher also performs only $3 \%$ above the random guess accuracy of $50 \%$. The smaller student models actually manage to improve on this performance as long as they do not use Zero-shot-CoT, in particular vanilla fine-tuning, but the errors arising in Fine-tune-CoT often look very similar to the ones in the large teacher model. We see that all models usually merely retrieve information related to the question, but cannot synthesize an answer from it (Appendix Tables 10c,11a).</p>
<p>Arithmetic mistakes Next, we note that small models overall exhibit weak arithmetic skills. This has already been discussed in previous literature, where calculation capability has been found to scale with model size (Wei et al., 2022a). Especially in SingleEq (Appendix Table 11b) and AddSub (Appendix Table 11c), a majority of errors in the output of student models using Fine-tune-CoT simply arise from wrong calculations, less so bad reasoning. This is also a major factor in the bad performance our method exhibits on SVAMP as well as GSM8K; even correct multistep reasoning cannot compensate for the fact that the model's arithmetic tends to be wrong on intermediate steps (Appendix Tables 11d, 12a). Only the teacher model then does better on these tasks, given its much larger size, even though it does not get perfect accuracy either. However, we note here that very large language models, such as PaLM 540B, can be trained on arithmetic and scientific data to be able to reason correctly about a wide range of mathematical tasks in a step-by-step fashion (Lewkowycz et al., 2022).</p>
<p>Problematic benchmarks, impact of commonsense reasoning errors Meanwhile, when looking at our method's performance in CommonsenseQA, we note that producing consistent multistep reasoning is not always the issue. We find that the student model utilizing Fine-tune-CoT can often generate logical reasoning paths for many of the samples that are marked as false (Appendix Table 13b). Rather, the exact answer is often subjective, making it difficult to guess the correct output from logical reasoning alone (Appendix Table 13c). CommonsenseQA thus is not always an ideal benchmark when judged on accuracy, but gives insight into how well the model can produce reasoning. We also note a difference compared to Few-shot-CoT in terms of the impact of reasoning errors: the latter only performs around 5\% above random, lacks understanding of the question in many cases, and makes more severe logical and commonsense mistakes compared to our method. In fact, Fine-tune-CoT comes close to the teacher due to the relatively lower impact of errors that do arise (Appendix Table 13d). This suggests that Fine-tune-CoT enables stronger task-solving capabilities and avoids making serious commonsense mistakes that prevent it from arriving at a reasonable conclusion.</p>
<p>Aligned failures Importantly, we note that for each dataset, there seems to be a difference between "easy" and "hard" instances. When we consider the accuracy of the teacher and other student models (using fine-tuning, Zero-shot- or Few-shot-CoT) on tasks where our method fails, we find that it is always lower than on tasks where our method is successful. That is, successes and failures tend to be aligned across the different methods. We can hypothesize that factors such as content bias may play a role here; language models have been found to fail depending on context and content of the task, in a way similar to human reasoners (Dasgupta et al., 2022). We can identify samples that hint at this issue when we look at questions that include phrasing that seems contradictory or counterintuitive to the context that the model expects (see Appendix Table 13d, where the number of movies watched is larger than the number of available movies). Additionally, previous work shows that GPT-3 exhibits a performance gap between instances including terms that are frequent in the pretraining corpus, and instances including less frequent terms (Razeghi et al., 2022). This can</p>
<p>contribute to uneven performance on a multitude of (especially numerical) tasks across different methods and model sizes. We can then surmise the observed absolute differences in accuracy to stem from the various sources of errors for each method. For example, fine-tuning has much less room for error than Fine-tune-CoT, which can additionally make mistakes on intermediate reasonings such that errors compound.</p>
<h3>1.2 Improvements from diverse reasoning</h3>
<p>Semantic issues We find that models seem sensitive to how a question is formulated. This is noticeable in all datasets, in particular in SVAMP and to a certain degree in MultiArith. Besides arithmetic mistakes, we observe that such semantic issues are one of the main factors for uneven performance of vanilla Fine-tune-CoT on these two datasets.</p>
<p>In particular, we observe this issue when there is redundant information present in the question (Appendix Table 12b). Such cases elicit wrong reasoning, or lead the model to become stuck on the question, similarly to what usually happens with Zero-shot-CoT in the student model (i.e. repeating the question, or coming up with information that only vaguely pertains to the question). Other common sources of errors are when hidden variables make up the first part of the task (i.e. those tasks that force the model to calculate a previously unknown value that is described in the first sentence (Appendix Table 12c), or when the model encounters overloaded words (e.g., "landing" in Appendix Table 12d). We also observe samples where the model gets stuck on an intermediate result (Appendix Table 13a). This observation agrees with previous findings that language models have recency bias (Zhao et al., 2021).</p>
<p>However, this source of errors can be compensated for by using diverse reasoning. When comparing the generations from Few-shot-CoT, vanilla Fine-tune-CoT and Fine-tune-CoT with diverse reasoning on MultiArith, we find that diverse reasoning enables the model to understand the question better. While calculation errors are still relatively frequent, the generations show clear advantages in terms of semantic understanding and being able to reason logically as a consequence. This is especially clear when compared to Few-shot-CoT, which exhibits problems both in understanding the question and formulating coherent expressions, especially when three or more terms
are involved in the calculation, as mentioned in Kojima et al. (2022). By contrast, Fine-tune-CoT with diverse reasoning makes for a significantly smoother reasoning performance than using Few-shot-CoT or even vanilla Fine-tune-CoT. This results in vastly improved accuracy on both MultiArith and SVAMP.</p>
<h3>1.3 Strengths</h3>
<p>Having analyzed the main sources of errors, we can now focus on the datasets that elicit good performance from our method, regardless of whether we use diverse reasoning.</p>
<p>Text-based datasets As arithmetic errors are one of the main reasons for the decrease in performance of small student models, it comes as little surprise that our vanilla method without diverse reasoning performs well on datasets that are mainly text-based and do not require actual calculation skills. This includes Date Understanding (60.4\%) (Appendix Table 14a), Last Letter Concatenation (52.67\%) (Appendix Table 14b), Coin Flip (98.7\%) (Appendix Table 14c), and Shuffled Objects (64.4\%) (Appendix Table 14d). Our methods performs significantly above random choice on these sets, and additionally beats the teacher on Shuffled Objects and Coin Flip. We find that accuracy metrics for these sets are mostly faithful: while the elicited reasoning is not always very detailed, and occasionally misses some reasoning steps (Appendix Table 14e), the model draws correct conclusions from mostly correct steps. We also note that similar to MultiArith and SVAMP, performance on these four datasets can be even further boosted with diverse reasoning, outperforming the teacher model across all four.</p>
<p>Patterns These datasets also have very clear patterns in their tasks, which helps Fine-tune-CoT to perform well by providing cues on how to solve a specific task. We note that in contrast, classic fine-tuning does not have an advantage in these datasets, and it gets significantly lower accuracy than Fine-tune-CoT on all four. The same is also true for MultiArith, which we have used as a benchmark in the main text. While arithmetic errors cause the absolute accuracy of our method to be lower than the teacher, it significantly outperforms fine-tuning on MultiArith even without using diverse reasoning. Indeed, we find that also in the presence of arithmetic errors, our model reasons correctly in many cases. We can surmise that the</p>
<p>patterned nature of the tasks in MultiArith helps the student model to understand what is asked of it, eliciting the correct reasoning. Additionally, we note that the presence of such patterns in successful datasets does not mean that our method overfits to existing templates. In our template-split analysis (Appendix E.3), we in fact show that while tasks look similar to one another in certain datasets such as Date Understanding, the student model's reasoning does not rely on simply matching templates or memorizing particular solutions. This implies that our method can generalize to previously unseen tasks; the patterns in the datasets do not produce overfitting, but can be surmised to act as cues for the model's understanding of its current task. Thus, we observe that the reasoning skills of a student using Fine-tune-CoT can overcome the smaller model capacity (which proves to be completely prohibitive, e.g., for Zero-shot-CoT to have any success on the various tasks).</p>
<h2>E Nuances of Fine-tune-CoT</h2>
<h2>E. 1 Rationale filtering</h2>
<p>We investigate whether answer-based filtering is sufficient for selecting good teacher-generated reasoning samples. It is possible for the teacher model to answer correctly despite incorrect reasoning, especially in multi-choice questions where the random-guess probability is significant. To investigate the potential impact of a better filtering scheme (as opposed to our baseline answer-based filtering) we manually annotate the correctness of rationales from the teacher model and evaluate student performance when fine-tuning on correctly reasoned samples. We use the Date Understanding dataset for this ablation, as it is comprised of well-grounded multi-choice questions for which Fine-tune-CoT achieves adequate performance. Appendix Table 6 compares the Fine-tune-CoT performance of student models on Date Understanding when using correct samples filtered based on answer predictions vs golden samples, hand-picked based on the correctness of rationales. For golden samples, we exclude samples that contain incorrect reasoning steps or irrelevant steps which are misleading. We find that $28 \%$ of correct samples have incorrect rationales-significantly more than the randomguess performance of $17.12 \%$, indicating the importance of filtering. Surprisingly, we however find that answer-based filtering outperforms the more stringent human filtering by $5-11 \%$, given the same
initial samples. When we match the number of samples post-filtering (via undersampling), we do find that fine-tuning on golden samples outperforms that on correct samples by $5-8 \%$. These results suggest that there is a tradeoff between the quality and quantity of reasoning samples which must be addressed when considering sample-filtering methods. We also note that this must be considered in tandem with diverse reasoning, which can drastically increase the quantity of reasoning samples.</p>
<h2>E. 2 Maximum sequence length</h2>
<p>Following the original setting for Zero-shot-CoT (Kojima et al., 2022), we limit the max sequence length, or max tokens, allowed for the teachergenerated rationale and student reasoning predictions, denoted $L_{r}, L_{p}$, to 128 initially. However, we find that this can be insufficient in many datasets. Allowing for longer inference, we observe that model performance improves significantly on AQUA and commonsense reasoning tasks (Appendix Table 5). Sample inspection shows that rationales with over $\sim 500$ tokens are typically repetitive or too digressive. To investigate the effect of the max length $L_{r}$ of the teacher rationale on fine-tuning, we compare student performance using $L_{r}={128,512}$ (Appendix Table 7). The effect of $L_{r}$ on student performance varies across datasets, and increased $L_{r}$ does not necessarily improve student performance on tasks that require longer rationales, such as AQUA. Finally, we examine the length distribution of the generated rationales from the teacher model and student trained on short $\left(L_{r}=128\right)$ and long $\left(L_{r}=512\right)$ reasoning samples, respectively (Appendix Figure 7). We find that the distribution is different for each dataset. Notably, we find that while the distributions from the long students were similar to that of the teacher, the generated rationale from the short students were typically limited to less than $\sim 128$ tokens. These findings are in line with the intuition that different tasks require different lengths of rationales, and suggest that careful consideration is needed in determining parameters related to sequence length.</p>
<h2>E. 3 Templated datasets</h2>
<p>Upon inspection, we found that many datasets contain groups of samples which share common templates. Therefore, naive samplewise data split has the potential to leak the same templates into the train and test sets, essentially demoting the learn-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model <br> Params</th>
<th style="text-align: center;">Max <br> Tokens</th>
<th style="text-align: center;">Single <br> Eq</th>
<th style="text-align: center;">Add <br> Sub</th>
<th style="text-align: center;">Multi <br> Arith</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">Aqua</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">Date <br> Understanding</th>
<th style="text-align: center;">Shuffled <br> Objects</th>
<th style="text-align: center;">Last <br> Letter</th>
<th style="text-align: center;">Coin <br> Flip</th>
<th style="text-align: center;">Common <br> SenseQA</th>
<th style="text-align: center;">Strategy <br> QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Teacher: InstructGPT (text-davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">81.18</td>
<td style="text-align: center;">75.72</td>
<td style="text-align: center;">76.90</td>
<td style="text-align: center;">42.42</td>
<td style="text-align: center;">29.63</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">65.89</td>
<td style="text-align: center;">54.10</td>
<td style="text-align: center;">57.43</td>
<td style="text-align: center;">89.71</td>
<td style="text-align: center;">59.86</td>
<td style="text-align: center;">53.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(84.83)</td>
<td style="text-align: center;">(90.22)</td>
<td style="text-align: center;">(95.24)</td>
<td style="text-align: center;">(69.85)</td>
<td style="text-align: center;">(44.04)</td>
<td style="text-align: center;">(86.57)</td>
<td style="text-align: center;">(98.06)</td>
<td style="text-align: center;">(97.14)</td>
<td style="text-align: center;">(99.71)</td>
<td style="text-align: center;">(97.14)</td>
<td style="text-align: center;">(82.55)</td>
<td style="text-align: center;">(71.55)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">81.18</td>
<td style="text-align: center;">75.72</td>
<td style="text-align: center;">76.48</td>
<td style="text-align: center;">47.73</td>
<td style="text-align: center;">34.77</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">63.28</td>
<td style="text-align: center;">54.10</td>
<td style="text-align: center;">57.43</td>
<td style="text-align: center;">89.71</td>
<td style="text-align: center;">59.40</td>
<td style="text-align: center;">53.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(84.83)</td>
<td style="text-align: center;">(90.22)</td>
<td style="text-align: center;">(94.29)</td>
<td style="text-align: center;">(99.34)</td>
<td style="text-align: center;">(96.42)</td>
<td style="text-align: center;">(99.00)</td>
<td style="text-align: center;">(97.14)</td>
<td style="text-align: center;">(97.14)</td>
<td style="text-align: center;">(99.71)</td>
<td style="text-align: center;">(97.14)</td>
<td style="text-align: center;">(99.92)</td>
<td style="text-align: center;">(99.69)</td>
</tr>
<tr>
<td style="text-align: center;">Student: GPT-3 (ada, babbage, curie)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0.3B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">7.24</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">16.54</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">48.89</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">30.30</td>
<td style="text-align: center;">47.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(96.05)</td>
<td style="text-align: center;">(99.16)</td>
<td style="text-align: center;">(96.11)</td>
<td style="text-align: center;">(74.75)</td>
<td style="text-align: center;">(45.67)</td>
<td style="text-align: center;">(91.33)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(86.73)</td>
<td style="text-align: center;">(87.63)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">7.24</td>
<td style="text-align: center;">6.72</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">23.62</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">49.33</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">32.68</td>
<td style="text-align: center;">52.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(98.68)</td>
<td style="text-align: center;">(99.16)</td>
<td style="text-align: center;">(97.22)</td>
<td style="text-align: center;">(99.77)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(97.33)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.71)</td>
</tr>
<tr>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">13.89</td>
<td style="text-align: center;">4.02</td>
<td style="text-align: center;">15.35</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">53.78</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">40.95</td>
<td style="text-align: center;">47.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(92.76)</td>
<td style="text-align: center;">(96.64)</td>
<td style="text-align: center;">(98.89)</td>
<td style="text-align: center;">(75.36)</td>
<td style="text-align: center;">(48.03)</td>
<td style="text-align: center;">(90.33)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.56)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(86.57)</td>
<td style="text-align: center;">(83.99)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">19.69</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">52.44</td>
<td style="text-align: center;">50.67</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">43.08</td>
<td style="text-align: center;">52.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(98.68)</td>
<td style="text-align: center;">(98.32)</td>
<td style="text-align: center;">(99.44)</td>
<td style="text-align: center;">(99.92)</td>
<td style="text-align: center;">(99.61)</td>
<td style="text-align: center;">(99.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.92)</td>
<td style="text-align: center;">(98.98)</td>
</tr>
<tr>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">21.05</td>
<td style="text-align: center;">20.17</td>
<td style="text-align: center;">34.44</td>
<td style="text-align: center;">7.20</td>
<td style="text-align: center;">16.93</td>
<td style="text-align: center;">12.67</td>
<td style="text-align: center;">60.36</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">51.27</td>
<td style="text-align: center;">47.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(92.76)</td>
<td style="text-align: center;">(97.48)</td>
<td style="text-align: center;">(99.44)</td>
<td style="text-align: center;">(76.19)</td>
<td style="text-align: center;">(55.91)</td>
<td style="text-align: center;">(93.67)</td>
<td style="text-align: center;">(99.10)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(85.26)</td>
<td style="text-align: center;">(84.28)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">20.39</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">6.75</td>
<td style="text-align: center;">24.02</td>
<td style="text-align: center;">12.67</td>
<td style="text-align: center;">60.36</td>
<td style="text-align: center;">64.44</td>
<td style="text-align: center;">52.67</td>
<td style="text-align: center;">98.67</td>
<td style="text-align: center;">56.76</td>
<td style="text-align: center;">55.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(98.68)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.92)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(100.00)</td>
<td style="text-align: center;">(99.71)</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">50.00</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation on maximum sequence length. Accuracy (\%) of Zero-shot-CoT on the teacher model and Fine-tune-CoT on GPT-3 student models, based on maximum sequence length. Values in parentheses refer to the percentage of generated rationales that were completed within the allotted maximum sequence length. Percentages lower than $90 \%$ are marked in bold. Note that the maximum sequence length is applied to the reasoning portion, i.e., step 1, of Zero-shot-CoT and to the entire output of Fine-tune-CoT.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Effects of teacher reasoning length on student reasoning length. Distribution of the length of generated reasoning sequences from the 175B teacher model and fine-tuned 6.7B student models on four datasets. Student (Short) refers to students that were fine-tuned on reasoning samples with maximum rationale sequence length of $L_{r}=128$, and Student (Long) refers to students that were fine-tuned on longer reasoning samples with $L_{r}=512$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Filter</th>
<th style="text-align: center;">Samples</th>
<th style="text-align: center;">Model Params</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.3B</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">6.7B</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot-CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.81</td>
<td style="text-align: center;">14.41</td>
<td style="text-align: center;">15.32</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune-CoT</td>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">60.36</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune-CoT</td>
<td style="text-align: center;">Golden</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">28.83</td>
<td style="text-align: center;">54.95</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune-CoT</td>
<td style="text-align: center;">Answer ${ }^{\dagger}$</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">18.92</td>
<td style="text-align: center;">50.45</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.09</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6: Effects of rationale filtering. Accuracy (\%) of GPT-3 student models under Fine-tune-CoT when using samples filtered using answer predictions (Answer), or filtered by humans based on the correctness of the rationale (Golden). Answer ${ }^{\dagger}$ refers to using a randomly sampled subset of the correct samples to match the number of golden samples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model <br> Params</th>
<th style="text-align: center;">Max <br> Tokens</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">Common <br> SenseQA</th>
<th style="text-align: center;">Strategy <br> QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.3B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">23.62</td>
<td style="text-align: center;">32.68</td>
<td style="text-align: center;">52.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">15.35</td>
<td style="text-align: center;">32.10</td>
<td style="text-align: center;">52.98</td>
</tr>
<tr>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">19.69</td>
<td style="text-align: center;">43.08</td>
<td style="text-align: center;">52.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">18.90</td>
<td style="text-align: center;">43.65</td>
<td style="text-align: center;">53.42</td>
</tr>
<tr>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">6.75</td>
<td style="text-align: center;">24.02</td>
<td style="text-align: center;">56.76</td>
<td style="text-align: center;">55.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">18.90</td>
<td style="text-align: center;">58.15</td>
<td style="text-align: center;">54.15</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">20.44</td>
<td style="text-align: center;">20.01</td>
<td style="text-align: center;">50.18</td>
</tr>
</tbody>
</table>
<p>Table 7: Effects of teacher reasoning length on student performance. Accuracy (\%) of GPT-3 students models under Fine-tune-CoT on four datasets which require longer rationales, when trained on reasoning samples with maximum rationale sequence lengths of $L_{r}=128,512$.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Cost analysis of data acquisition methods. Accuracy (\%) of GPT-3 (6.7B) student model under Fine-tune-CoT with varying degrees of diverse reasoning and dataset sizes. Values in parentheses indicate estimated total cost of data acquisition, i.e., data annotation and diverse reasoning inference.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>0.3B</td>
<td>Sample-wise</td>
<td>5.56</td>
<td>17.12</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Template-wise</td>
<td>5.35</td>
<td>22.22</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1.3B</td>
<td>Sample-wise</td>
<td>13.89</td>
<td>38.74</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Template-wise</td>
<td>7.49</td>
<td>35.19</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6.7B</td>
<td>Sample-wise</td>
<td>34.44</td>
<td>60.36</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Template-wise</td>
<td>21.39</td>
<td>49.07</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 8: Sample-wise vs template-wise split. Accuracy (\%) of GPT-3 student models under Fine-tune-CoT on two moderately templated datasets when using a samplewise vs template-wise train-test split. ing problem into simple pattern matching, rather than complex reasoning. This brings into question the validity of naive samplewise data split, as it has the potential to leak the same templates into the train and test sets. To investigate whether the student models are truly learning to reason rather than matching simple patterns, we manually group samples by template and evaluate Fine-tune-CoT using a template-wise data split. We consider MultiArith and Date Understanding as they contain a moderate number of templates. Note that all datasets excluding GSM8K, CommonsenseQA, and StrategyQA contain templates to varying degrees. Appendix Table 8 shows the performance of Fine-tune-CoT when using sample-wise vs template-wise split, using the same train-test ratio of 70:30. While student performance is typically lower with a templatewise split, it still significantly outperforms random guess performance, as well as prompt-based baselines shown in Appendix Table 1. This reaffirms that Fine-tune-CoT is able to elicit complex reasoning capabilities in small language models.</p>
<h2>F Data Annotation vs Diverse Reasoning</h2>
<p>In Appendix Figure 8, we analyze the cost of data annotation and diverse reasoning, based on current OpenAI API pricing and a low estimate of annotation cost at 30 annotations per hour at an hourly rate of $\$ 20$, i.e., $\$ 0.67$ per question-answer sample. When comparing the cost and student performance of models trained with $D=1$ and $D=64$, we can clearly see that using diverse reasoning can enhance the cost-effectiveness of data acquisition. However, as the cost of diverse reasoning correlates with the size of the dataset, it is important to consider the cost-performance tradeoffs.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://platform.openai.com/docs/api-reference/finetunes/create&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Diverse reasoning is orthogonal to existing data augmentation techniques (Yoo et al., 2021; Meng et al., 2022) which aim to augment new question-answer pairs rather than diverse reasoning solutions for complex questions.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9287 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9287</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9287</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-258840955</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-acl.216.pdf" target="_blank">Better Zero-Shot Reasoning with Self-Adaptive Prompting</a></p>
                <p><strong>Paper Abstract:</strong> Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses ("in-context examples"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9287.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9287.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt format that elicits step-by-step intermediate reasoning from an LLM in a zero-shot setting by appending a trigger phrase (e.g., "Let's think step by step") rather than providing handcrafted demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3 (code-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic & logical reasoning benchmarks (MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Word-problem arithmetic and multiple-choice commonsense/strategy reasoning tasks used to evaluate chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt: single test question + CoT trigger (e.g., "Let's think step by step") without handcrafted in-context demonstrations; decoding with nonzero temperature to sample multiple reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Few-shot CoT (handcrafted demonstrations prepended) and retrieval-based Auto-CoT; also compared with variants using self-consistency (majority voting over sampled paths).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline Zero-shot CoT performance varies by task/model; generally lower than Few-shot CoT and improved by self-consistency. (No single aggregate baseline number provided in text; used as baseline in tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot CoT + self-consistency is the baseline that COSP improves upon by ~10–15% average accuracy for several reasoning tasks (reported across PaLM-62B and GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+10–15% average accuracy improvement reported for COSP over Zero-shot CoT with self-consistency (aggregate statement in paper); per-task numbers vary.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Zero-shot CoT provides task-agnostic reasoning triggers but lacks guidance from in-context examples, causing spurious reasoning on some tasks and lower performance than well-chosen few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>When self-consistency used: temperature 0.7 (PaLM), 0.4 (GPT-3), sample m=7 paths (14 total when combining stages), majority/plurality vote over sampled answers used as final prediction; used as Stage 1 generation for demo-pooling in COSP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9287.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt format that prepends several handcrafted solved question–rationale–answer demonstrations to a test question to induce chain-of-thought style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3 (code-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning benchmarks involving arithmetic word problems and commonsense/multi-hop questions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prepended K handcrafted demonstrations (in experiments 5-shot demonstrations from Wei et al.), each demonstration includes a solved question, step-by-step rationale, and final answer; decoding optionally with self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against Zero-shot CoT, COSP (self-generated in-context demos), and Auto-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Often outperforms naive Zero-shot CoT when demonstrations are well chosen; in this paper Few-shot CoT with 5 handcrafted demos is used as a comparison baseline and is sometimes matched or exceeded by COSP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>COSP matches or exceeds Few-shot CoT on most tasks despite having no handcrafted labels; exact per-task numbers vary (reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Effect is task- and demo-selection-dependent; paper emphasizes sensitivity to demonstration choice and shows that poorly chosen demonstrations can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot CoT provides explicit templates and rationales enabling LLMs to follow a reasoning pattern, but performance is highly sensitive to which demonstrations are used and requires human effort to craft/select them.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments used 5 handcrafted demonstrations randomly chosen from Wei et al. (2022b) lists to match COSP's five generated demonstrations; self-consistency sometimes applied (14 sampled paths) for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9287.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (majority voting over sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation method that samples multiple chain-of-thought reasoning paths at nonzero temperature and uses plurality/majority voting over final answers to improve final predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic and logical/commonsense reasoning tasks; used both as a standalone format and as a component of COSP and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Probabilistic decoding at non-zero temperature to sample m reasoning paths per query, then choose the final answer by plurality/majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to single-path greedy decoding and applied across Zero-shot CoT, Few-shot CoT, Auto-CoT, and COSP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to increase performance across all models and tasks considered; baseline methods without self-consistency perform worse (details in App.C.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Used to augment baselines (e.g., Zero-shot CoT with self-consistency) and COSP; COSP improves further on top of self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Consistent, often substantial improvements observed when using self-consistency versus single-path decoding (exact effect sizes depend on task/model; paper reports self-consistency 'significantly improves' performance).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Aggregating multiple sampled reasoning paths approximates marginal distributions and reduces variance/spurious individual-path errors; majority answers are more likely to be correct.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sampling: m=7 per stage in COSP (14 total when combining stages). Temperature: 0.7 for PaLM models, 0.4 for GPT-3 when self-consistency used. Outcome entropy (normalized) computed from sampled answers to measure uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9287.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thought (Auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval/clustering-based method that uses zero-shot LLM outputs and auxiliary embeddings to select in-context demonstrations without ground-truth labels (selects k-means centroids of question embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning benchmarks used to evaluate quality of automatically selected demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Selects representative questions via clustering in question-embedding space (auxiliary sentence encoder) and uses the model's generated rationales for those selected questions as in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against COSP (consistency-based selection), Zero-shot CoT, and Few-shot CoT; paper also includes Auto-CoT variants with and without self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed: Auto-CoT sometimes improves over Zero-shot CoT but can also deteriorate performance, especially on smaller models or harder tasks; in the paper Auto-CoT sometimes underperforms or hurts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>COSP consistently outperforms or is more robust than Auto-CoT in most experiments; Auto-CoT can deteriorate performance more frequently than COSP.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Negative effects observed in some model/task combinations (no single aggregate effect size); COSP reports improvements over Auto-CoT in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Auto-CoT's selection depends only on question embeddings and thus cannot control the quality of generated rationales; when many generated demonstrations are erroneous, Auto-CoT may select misleading demos.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Auto-CoT in experiments used Sentence-T5-large embeddings (same as COSP) and 5 generated demonstrations; Auto-CoT was also tested with self-consistency for fair comparison (results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9287.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COSP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency-based Self-adaptive Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that builds in-context demonstrations from the LLM's own zero-shot chain-of-thought outputs by selecting candidate demonstrations based on outcome self-consistency (low entropy), penalizing repetitiveness, and encouraging diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic word-problem datasets and commonsense/multi-choice reasoning datasets; COSP applied in zero-shot transductive setting (access to unlabeled test queries only).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Stage 1: run Zero-shot CoT m times to generate rationale–answer pairs per test item; compute normalized outcome entropy and repetitiveness; build candidate pool P of rationale-answer pairs that match majority answer; Stage 2: greedily select K in-context demonstrations by minimizing score F = normalized entropy + λ * repetitiveness (with diversity penalty G_k), prepend S of K demos to test query, query again, and majority-vote over both stages' outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Zero-shot CoT (with/without self-consistency), Auto-CoT, Few-shot CoT (5-shot), and variants COSP+ and COSP-FS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>COSP improves substantially over Zero-shot CoT baselines: reported aggregate/average improvements of ~10–15% accuracy on several arithmetic reasoning tasks for PaLM-62B and GPT-3; in many cases matches or exceeds Few-shot CoT. Example per-task (PaLM-62B, Table 4): MultiArith 85.0%, AddSub 78.9%, SingleEq 78.7%, GSM-8K 30.2%, CSQA 60.2%, StrategyQA 64.7%, Average 66.28%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Versus Zero-shot CoT with self-consistency: COSP yields ~10–15% average accuracy improvement reported; versus Few-shot CoT: COSP performs on par or better on most tasks reported; versus Auto-CoT: COSP is more robust and often better.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+10–15% average accuracy improvement over Zero-shot CoT with self-consistency (aggregate claim); per-task increases shown in tables (e.g., PaLM-62B MultiArith to 85.0% with COSP).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Selecting demonstrations that the model is internally consistent about (low outcome entropy) yields higher-quality in-context examples; penalizing repetitive reasoning avoids creating spurious patterns; encouraging diversity reduces overfitting to a narrow reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stage-1 & Stage-2 sampling: m=7 paths each (14 total when combined); default K=5 demos selected per test question; repetitiveness penalty λ=0.2 (z-score normalized terms); auxiliary embeddings via Sentence-T5-large; heuristics to prune candidates (remove no-number responses for arithmetic tasks, remove self-questioning outputs, short or fragmented responses); temperatures: 0.7 for PaLM, 0.4 for GPT-3 when self-consistency used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9287.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COSP+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COSP with adaptive demonstration allocation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A COSP variant that adaptively allocates the number of in-context demonstrations per query proportional to the Stage-1 outcome entropy (higher-entropy questions receive more demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as COSP; COSP+ targets adaptively hard examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same two-stage COSP pipeline; Stage-2 uses an adaptive K per query proportional to normalized outcome entropy computed from Stage-1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against fixed-K COSP and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On PaLM-62B, COSP+ yields small improvements in 5 out of 6 tasks versus fixed-K COSP (example Table 4: MultiArith 86.2% vs 85.0%, Average 66.32% vs 66.28%), but deteriorated on GSM-8K in one reported case.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Small, task-dependent improvements over fixed-K COSP; can degrade when additional selected demonstrations are erroneous (observed on GSM-8K).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Generally small per-task gains; e.g., MultiArith +1.2% (86.2% vs 85.0) in table; overall average change minimal.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher uncertainty (entropy) questions benefit from more in-context examples; however, adding more demonstrations can harm performance if those extra demonstrations are erroneous.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Adaptive K proportional to Stage-1 entropy; other hyperparameters same as COSP; reported experiments on PaLM-62B in App.C.3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9287.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COSP-FS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COSP Few-shot hybrid (COSP-FS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid variant where COSP Stage-1 is seeded with a small set of labeled (handcrafted) examples before selecting additional demonstrations from unlabeled model outputs, improving robustness on very difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM-8K (primary reported case)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging arithmetic word-problem dataset where zero-shot outputs can be highly erroneous for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Stage-1 includes 5 labelled handcrafted examples (from Wei et al.) in addition to zero-shot generated outputs; Stage-2 selects further examples using COSP selection criteria and runs in-context inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Few-shot CoT (manual 5-shot) and COSP (zero-shot only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to restore/improve COSP effectiveness on GSM-8K for PaLM-62B and GPT-3 where pure COSP had degraded performance; example: COSP-FS on GSM-8K shows marked improvement versus pure COSP in App.C.4 (specific table referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>COSP-FS outperforms COSP in cases where zero-shot generated demonstrations are mostly incorrect (GSM-8K with smaller models).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified as a single number in main text; reported improvements in App.C.4 and discussed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Seeding with a few ground-truth labeled demonstrations restores the predictive value of self-consistency and prevents selection of erroneous self-generated demos.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used 5 labelled examples in Stage-1 and selected 3 additional examples in Stage-2 in reported experiments (GSM-8K).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9287.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Outcome Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Normalized Outcome Entropy (for answer distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric computed from sampled final answers across multiple sampled reasoning paths: normalized entropy of the empirical distribution of unique final answers, used as a proxy for model uncertainty and correctness likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used across arithmetic and logical reasoning datasets to estimate uncertainty per query and guide demonstration selection and adaptive allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Computed from m sampled answers per question (m=7) as H = -sum p_i log p_i / log m, yielding a normalized [0,1] score where lower entropy correlates with higher correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Used as a selection criterion (low entropy preferred) and as a basis for adaptive K in COSP+; compared qualitatively with correctness (ground-truth) and shown to correlate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not a predictive performance metric itself; reported strong correlation between low entropy and correctness across most tasks/models (statistically significant point-biserial correlations except a weak case: StrategyQA on GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Questions with low outcome entropy (consistent majority answers across sampled reasoning paths) are more likely to be correctly answered; entropy thus serves as an unsupervised proxy for demonstration quality and question difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Entropy computed from m=7 sampled paths per Stage-1; normalized by log m; used to prune candidate pool and to prioritize selection of demonstrations; correlated distributions shown in Fig.3 and App.C.1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9287.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9287.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Repetitiveness penalty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Repetitiveness scoring and penalty in demonstration selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative measure of semantic/phrase-level repetitiveness in generated rationales used as a penalty term to avoid selecting repetitive demonstrations that can induce spurious patterns in the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-62B, PaLM-540B, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B, 540B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied to candidate rationale texts to compute an average pairwise phrase similarity score that captures semantic-level repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compute repetitiveness R by splitting rationales into Q phrases, embedding phrases with an auxiliary encoder (Sentence-T5), computing pairwise cosine similarities W_ab, and averaging off-diagonal similarities; contribute as λ * R term in overall selection score F = z(H) + λ * z(R) (z-scores).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Included in COSP scoring vs. ablations that remove repetitiveness term; ablation shows repetitiveness penalty contributes positively to performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Including repetitiveness penalty improved downstream performance in ablations (Fig.4a); absence of this penalty leads to worse performance due to selection of repetitive demos.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Repetitive demonstrations can create strong but spurious patterns that LLMs overfit to, causing deterioration of performance; penalizing repetitiveness avoids selecting such demos.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>λ set to 0.2 in all experiments; phrase tokenization by punctuation delimiters; embeddings by Sentence-T5-large; repetition undefined when Q=1 handled by special rule (min repetition score).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Better Zero-Shot Reasoning with Self-Adaptive Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>STAR: Self-taught reasoner bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9287",
    "paper_id": "paper-258840955",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting",
            "brief_description": "A prompt format that elicits step-by-step intermediate reasoning from an LLM in a zero-shot setting by appending a trigger phrase (e.g., \"Let's think step by step\") rather than providing handcrafted demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3 (code-davinci-001)",
            "model_size": "62B, 540B, 175B",
            "task_name": "Arithmetic & logical reasoning benchmarks (MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA)",
            "task_description": "Word-problem arithmetic and multiple-choice commonsense/strategy reasoning tasks used to evaluate chain-of-thought reasoning.",
            "presentation_format": "Zero-shot prompt: single test question + CoT trigger (e.g., \"Let's think step by step\") without handcrafted in-context demonstrations; decoding with nonzero temperature to sample multiple reasoning paths.",
            "comparison_format": "Compared to Few-shot CoT (handcrafted demonstrations prepended) and retrieval-based Auto-CoT; also compared with variants using self-consistency (majority voting over sampled paths).",
            "performance": "Baseline Zero-shot CoT performance varies by task/model; generally lower than Few-shot CoT and improved by self-consistency. (No single aggregate baseline number provided in text; used as baseline in tables.)",
            "performance_comparison": "Zero-shot CoT + self-consistency is the baseline that COSP improves upon by ~10–15% average accuracy for several reasoning tasks (reported across PaLM-62B and GPT-3).",
            "format_effect_size": "+10–15% average accuracy improvement reported for COSP over Zero-shot CoT with self-consistency (aggregate statement in paper); per-task numbers vary.",
            "explanation_or_hypothesis": "Zero-shot CoT provides task-agnostic reasoning triggers but lacks guidance from in-context examples, causing spurious reasoning on some tasks and lower performance than well-chosen few-shot examples.",
            "null_or_negative_result": false,
            "experimental_details": "When self-consistency used: temperature 0.7 (PaLM), 0.4 (GPT-3), sample m=7 paths (14 total when combining stages), majority/plurality vote over sampled answers used as final prediction; used as Stage 1 generation for demo-pooling in COSP.",
            "uuid": "e9287.0",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Few-shot CoT",
            "name_full": "Few-shot Chain-of-Thought prompting",
            "brief_description": "A prompt format that prepends several handcrafted solved question–rationale–answer demonstrations to a test question to induce chain-of-thought style reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3 (code-davinci-001)",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Reasoning benchmarks involving arithmetic word problems and commonsense/multi-hop questions.",
            "presentation_format": "Prepended K handcrafted demonstrations (in experiments 5-shot demonstrations from Wei et al.), each demonstration includes a solved question, step-by-step rationale, and final answer; decoding optionally with self-consistency.",
            "comparison_format": "Compared against Zero-shot CoT, COSP (self-generated in-context demos), and Auto-CoT.",
            "performance": "Often outperforms naive Zero-shot CoT when demonstrations are well chosen; in this paper Few-shot CoT with 5 handcrafted demos is used as a comparison baseline and is sometimes matched or exceeded by COSP.",
            "performance_comparison": "COSP matches or exceeds Few-shot CoT on most tasks despite having no handcrafted labels; exact per-task numbers vary (reported in tables).",
            "format_effect_size": "Effect is task- and demo-selection-dependent; paper emphasizes sensitivity to demonstration choice and shows that poorly chosen demonstrations can hurt performance.",
            "explanation_or_hypothesis": "Few-shot CoT provides explicit templates and rationales enabling LLMs to follow a reasoning pattern, but performance is highly sensitive to which demonstrations are used and requires human effort to craft/select them.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments used 5 handcrafted demonstrations randomly chosen from Wei et al. (2022b) lists to match COSP's five generated demonstrations; self-consistency sometimes applied (14 sampled paths) for fair comparison.",
            "uuid": "e9287.1",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (majority voting over sampled reasoning paths)",
            "brief_description": "A decoding/aggregation method that samples multiple chain-of-thought reasoning paths at nonzero temperature and uses plurality/majority voting over final answers to improve final predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Arithmetic and logical/commonsense reasoning tasks; used both as a standalone format and as a component of COSP and baselines.",
            "presentation_format": "Probabilistic decoding at non-zero temperature to sample m reasoning paths per query, then choose the final answer by plurality/majority vote.",
            "comparison_format": "Compared to single-path greedy decoding and applied across Zero-shot CoT, Few-shot CoT, Auto-CoT, and COSP.",
            "performance": "Reported to increase performance across all models and tasks considered; baseline methods without self-consistency perform worse (details in App.C.2).",
            "performance_comparison": "Used to augment baselines (e.g., Zero-shot CoT with self-consistency) and COSP; COSP improves further on top of self-consistency.",
            "format_effect_size": "Consistent, often substantial improvements observed when using self-consistency versus single-path decoding (exact effect sizes depend on task/model; paper reports self-consistency 'significantly improves' performance).",
            "explanation_or_hypothesis": "Aggregating multiple sampled reasoning paths approximates marginal distributions and reduces variance/spurious individual-path errors; majority answers are more likely to be correct.",
            "null_or_negative_result": false,
            "experimental_details": "Sampling: m=7 per stage in COSP (14 total when combining stages). Temperature: 0.7 for PaLM models, 0.4 for GPT-3 when self-consistency used. Outcome entropy (normalized) computed from sampled answers to measure uncertainty.",
            "uuid": "e9287.2",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thought (Auto-CoT)",
            "brief_description": "A retrieval/clustering-based method that uses zero-shot LLM outputs and auxiliary embeddings to select in-context demonstrations without ground-truth labels (selects k-means centroids of question embeddings).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Same reasoning benchmarks used to evaluate quality of automatically selected demonstrations.",
            "presentation_format": "Selects representative questions via clustering in question-embedding space (auxiliary sentence encoder) and uses the model's generated rationales for those selected questions as in-context demonstrations.",
            "comparison_format": "Compared against COSP (consistency-based selection), Zero-shot CoT, and Few-shot CoT; paper also includes Auto-CoT variants with and without self-consistency.",
            "performance": "Mixed: Auto-CoT sometimes improves over Zero-shot CoT but can also deteriorate performance, especially on smaller models or harder tasks; in the paper Auto-CoT sometimes underperforms or hurts performance.",
            "performance_comparison": "COSP consistently outperforms or is more robust than Auto-CoT in most experiments; Auto-CoT can deteriorate performance more frequently than COSP.",
            "format_effect_size": "Negative effects observed in some model/task combinations (no single aggregate effect size); COSP reports improvements over Auto-CoT in tables.",
            "explanation_or_hypothesis": "Auto-CoT's selection depends only on question embeddings and thus cannot control the quality of generated rationales; when many generated demonstrations are erroneous, Auto-CoT may select misleading demos.",
            "null_or_negative_result": true,
            "experimental_details": "Auto-CoT in experiments used Sentence-T5-large embeddings (same as COSP) and 5 generated demonstrations; Auto-CoT was also tested with self-consistency for fair comparison (results reported).",
            "uuid": "e9287.3",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "COSP",
            "name_full": "Consistency-based Self-adaptive Prompting",
            "brief_description": "A method that builds in-context demonstrations from the LLM's own zero-shot chain-of-thought outputs by selecting candidate demonstrations based on outcome self-consistency (low entropy), penalizing repetitiveness, and encouraging diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Arithmetic word-problem datasets and commonsense/multi-choice reasoning datasets; COSP applied in zero-shot transductive setting (access to unlabeled test queries only).",
            "presentation_format": "Stage 1: run Zero-shot CoT m times to generate rationale–answer pairs per test item; compute normalized outcome entropy and repetitiveness; build candidate pool P of rationale-answer pairs that match majority answer; Stage 2: greedily select K in-context demonstrations by minimizing score F = normalized entropy + λ * repetitiveness (with diversity penalty G_k), prepend S of K demos to test query, query again, and majority-vote over both stages' outputs.",
            "comparison_format": "Compared to Zero-shot CoT (with/without self-consistency), Auto-CoT, Few-shot CoT (5-shot), and variants COSP+ and COSP-FS.",
            "performance": "COSP improves substantially over Zero-shot CoT baselines: reported aggregate/average improvements of ~10–15% accuracy on several arithmetic reasoning tasks for PaLM-62B and GPT-3; in many cases matches or exceeds Few-shot CoT. Example per-task (PaLM-62B, Table 4): MultiArith 85.0%, AddSub 78.9%, SingleEq 78.7%, GSM-8K 30.2%, CSQA 60.2%, StrategyQA 64.7%, Average 66.28%.",
            "performance_comparison": "Versus Zero-shot CoT with self-consistency: COSP yields ~10–15% average accuracy improvement reported; versus Few-shot CoT: COSP performs on par or better on most tasks reported; versus Auto-CoT: COSP is more robust and often better.",
            "format_effect_size": "+10–15% average accuracy improvement over Zero-shot CoT with self-consistency (aggregate claim); per-task increases shown in tables (e.g., PaLM-62B MultiArith to 85.0% with COSP).",
            "explanation_or_hypothesis": "Selecting demonstrations that the model is internally consistent about (low outcome entropy) yields higher-quality in-context examples; penalizing repetitive reasoning avoids creating spurious patterns; encouraging diversity reduces overfitting to a narrow reasoning style.",
            "null_or_negative_result": false,
            "experimental_details": "Stage-1 & Stage-2 sampling: m=7 paths each (14 total when combined); default K=5 demos selected per test question; repetitiveness penalty λ=0.2 (z-score normalized terms); auxiliary embeddings via Sentence-T5-large; heuristics to prune candidates (remove no-number responses for arithmetic tasks, remove self-questioning outputs, short or fragmented responses); temperatures: 0.7 for PaLM, 0.4 for GPT-3 when self-consistency used.",
            "uuid": "e9287.4",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "COSP+",
            "name_full": "COSP with adaptive demonstration allocation",
            "brief_description": "A COSP variant that adaptively allocates the number of in-context demonstrations per query proportional to the Stage-1 outcome entropy (higher-entropy questions receive more demonstrations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B",
            "model_size": "62B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Same benchmarks as COSP; COSP+ targets adaptively hard examples.",
            "presentation_format": "Same two-stage COSP pipeline; Stage-2 uses an adaptive K per query proportional to normalized outcome entropy computed from Stage-1.",
            "comparison_format": "Compared against fixed-K COSP and other baselines.",
            "performance": "On PaLM-62B, COSP+ yields small improvements in 5 out of 6 tasks versus fixed-K COSP (example Table 4: MultiArith 86.2% vs 85.0%, Average 66.32% vs 66.28%), but deteriorated on GSM-8K in one reported case.",
            "performance_comparison": "Small, task-dependent improvements over fixed-K COSP; can degrade when additional selected demonstrations are erroneous (observed on GSM-8K).",
            "format_effect_size": "Generally small per-task gains; e.g., MultiArith +1.2% (86.2% vs 85.0) in table; overall average change minimal.",
            "explanation_or_hypothesis": "Higher uncertainty (entropy) questions benefit from more in-context examples; however, adding more demonstrations can harm performance if those extra demonstrations are erroneous.",
            "null_or_negative_result": true,
            "experimental_details": "Adaptive K proportional to Stage-1 entropy; other hyperparameters same as COSP; reported experiments on PaLM-62B in App.C.3.",
            "uuid": "e9287.5",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "COSP-FS",
            "name_full": "COSP Few-shot hybrid (COSP-FS)",
            "brief_description": "A hybrid variant where COSP Stage-1 is seeded with a small set of labeled (handcrafted) examples before selecting additional demonstrations from unlabeled model outputs, improving robustness on very difficult tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, GPT-3",
            "model_size": "62B, 175B",
            "task_name": "GSM-8K (primary reported case)",
            "task_description": "Challenging arithmetic word-problem dataset where zero-shot outputs can be highly erroneous for smaller models.",
            "presentation_format": "Stage-1 includes 5 labelled handcrafted examples (from Wei et al.) in addition to zero-shot generated outputs; Stage-2 selects further examples using COSP selection criteria and runs in-context inference.",
            "comparison_format": "Compared to Few-shot CoT (manual 5-shot) and COSP (zero-shot only).",
            "performance": "Reported to restore/improve COSP effectiveness on GSM-8K for PaLM-62B and GPT-3 where pure COSP had degraded performance; example: COSP-FS on GSM-8K shows marked improvement versus pure COSP in App.C.4 (specific table referenced).",
            "performance_comparison": "COSP-FS outperforms COSP in cases where zero-shot generated demonstrations are mostly incorrect (GSM-8K with smaller models).",
            "format_effect_size": "Not quantified as a single number in main text; reported improvements in App.C.4 and discussed qualitatively.",
            "explanation_or_hypothesis": "Seeding with a few ground-truth labeled demonstrations restores the predictive value of self-consistency and prevents selection of erroneous self-generated demos.",
            "null_or_negative_result": false,
            "experimental_details": "Used 5 labelled examples in Stage-1 and selected 3 additional examples in Stage-2 in reported experiments (GSM-8K).",
            "uuid": "e9287.6",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Outcome Entropy",
            "name_full": "Normalized Outcome Entropy (for answer distributions)",
            "brief_description": "A metric computed from sampled final answers across multiple sampled reasoning paths: normalized entropy of the empirical distribution of unique final answers, used as a proxy for model uncertainty and correctness likelihood.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Used across arithmetic and logical reasoning datasets to estimate uncertainty per query and guide demonstration selection and adaptive allocation.",
            "presentation_format": "Computed from m sampled answers per question (m=7) as H = -sum p_i log p_i / log m, yielding a normalized [0,1] score where lower entropy correlates with higher correctness.",
            "comparison_format": "Used as a selection criterion (low entropy preferred) and as a basis for adaptive K in COSP+; compared qualitatively with correctness (ground-truth) and shown to correlate.",
            "performance": "Not a predictive performance metric itself; reported strong correlation between low entropy and correctness across most tasks/models (statistically significant point-biserial correlations except a weak case: StrategyQA on GPT-3).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Questions with low outcome entropy (consistent majority answers across sampled reasoning paths) are more likely to be correctly answered; entropy thus serves as an unsupervised proxy for demonstration quality and question difficulty.",
            "null_or_negative_result": false,
            "experimental_details": "Entropy computed from m=7 sampled paths per Stage-1; normalized by log m; used to prune candidate pool and to prioritize selection of demonstrations; correlated distributions shown in Fig.3 and App.C.1.",
            "uuid": "e9287.7",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Repetitiveness penalty",
            "name_full": "Repetitiveness scoring and penalty in demonstration selection",
            "brief_description": "A quantitative measure of semantic/phrase-level repetitiveness in generated rationales used as a penalty term to avoid selecting repetitive demonstrations that can induce spurious patterns in the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-62B, PaLM-540B, GPT-3",
            "model_size": "62B, 540B, 175B",
            "task_name": "MultiArith, AddSub, SingleEq, GSM-8K, CSQA, StrategyQA",
            "task_description": "Applied to candidate rationale texts to compute an average pairwise phrase similarity score that captures semantic-level repetition.",
            "presentation_format": "Compute repetitiveness R by splitting rationales into Q phrases, embedding phrases with an auxiliary encoder (Sentence-T5), computing pairwise cosine similarities W_ab, and averaging off-diagonal similarities; contribute as λ * R term in overall selection score F = z(H) + λ * z(R) (z-scores).",
            "comparison_format": "Included in COSP scoring vs. ablations that remove repetitiveness term; ablation shows repetitiveness penalty contributes positively to performance.",
            "performance": "Including repetitiveness penalty improved downstream performance in ablations (Fig.4a); absence of this penalty leads to worse performance due to selection of repetitive demos.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Repetitive demonstrations can create strong but spurious patterns that LLMs overfit to, causing deterioration of performance; penalizing repetitiveness avoids selecting such demos.",
            "null_or_negative_result": false,
            "experimental_details": "λ set to 0.2 in all experiments; phrase tokenization by punctuation delimiters; embeddings by Sentence-T5-large; repetition undefined when Q=1 handled by special rule (min repetition score).",
            "uuid": "e9287.8",
            "source_info": {
                "paper_title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "STAR: Self-taught reasoner bootstrapping reasoning with reasoning",
            "rating": 1,
            "sanitized_title": "star_selftaught_reasoner_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1,
            "sanitized_title": "large_language_models_can_selfimprove"
        }
    ],
    "cost": 0.018311,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Better Zero-Shot Reasoning with Self-Adaptive Prompting</p>
<p>Xingchen Wan xingchenw@google.com 
Department of Engineering Science
University of Oxford</p>
<p>Ruoxi Sun ruoxis@google.com 
Department of Engineering Science
University of Oxford</p>
<p>Hanjun Dai hadai@google.com 
Department of Engineering Science
University of Oxford</p>
<p>Sercan Ö Arık soarik@google.com 
Department of Engineering Science
University of Oxford</p>
<p>Tomas Pfister tpfister@google.com 
Department of Engineering Science
University of Oxford</p>
<p>Google Cloud 
Department of Engineering Science
University of Oxford</p>
<p>A I Research 
Department of Engineering Science
University of Oxford</p>
<p>Google Deepmind 
Department of Engineering Science
University of Oxford</p>
<p>Better Zero-Shot Reasoning with Self-Adaptive Prompting
E4196347AEF3E58D8A51CD1EA3F179A8
Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans.This is made possible by their strong few and zero-shot abilities -they can effectively learn from a handful of handcrafted, completed responses ("incontext examples"), or are prompted to reason spontaneously through specially designed triggers.Nonetheless, some limitations have been observed.First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort.Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels.Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs.To address these limitations, we propose Consistency-based Selfadaptive Prompting (COSP), a novel prompt design method for LLMs.Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition.In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have led to state-of-the-art performance in existing natural language processing (NLP) tasks (Wang et al., 2018(Wang et al., , 2019;;Thoppilan et al., 2022) and to exciting emergent abilities (Wei et al., 2022a).A prominent example of the latter is the strong performance on tasks that require analytical reasoning and/or methodical planning, which were previously * Work done during internship at Google.thought to be difficult even for massive LLMs (Rae et al., 2021).This has been made possible by scaling the model size and training corpus, the strong few-shot and zero-shot abilities of modern LLMs, and new techniques such as the chain-of-thought (CoT) methods: Few-shot CoT (Wei et al., 2022b) prepends the test queries with solved input-output pairs as in-context examples to prompt the LLM to generate rationales.Zero-shot CoT (Kojima et al., 2022), on the other hand, appends trigger phrases after test queries to elicit reasoning.These methods constitute a competitive alternative to conventional fine-tuning which can be very costly for modern LLMs due to their sheer size, and is not possible when the LLMs are available as inference-only APIs -an increasingly popular paradigm for serving models like ChatGPT (Schulman et al., 2022).Despite these promising advances, numerous open challenges remain.Although Zero-shot CoT is task-agnostic (and thus the most generally applicable in any scenario) and does not require human effort for labeling, it often underperforms its fewshot counterparts, with LLMs that are not shown with "template rationale" often producing spurious reasoning steps.On the other hand, for Fewshot CoT, the performance has been shown to be sensitive to the choice of the demonstrations (Wei et al., 2022b), and thus improving the performance might require significant trial-and-errors and/or specific relevant expertise.Given the fact that modern LLMs are often used for very diverse downstream tasks, selecting even a few useful per-task examples can quickly become more laborious as the number of tasks increase.This is particularly true for the CoT tasks where both rationales and answers need to provided in the demonstrations.In some other cases, it could be difficult or simply impossible even to handcraft a few examples beforehand: for example, there might be tasks involving tedious and complicated reasoning or planning and/or novel test-time tasks unseen previously.</p>
<p>In this work, our focus is to improve LLM reasoning in the general zero-shot setup with access to input queries but not labels (i.e., transductive zero-shot (Xian et al., 2017)).We are particularly interested in this setup due to its aforementioned generality, but it is also a very challenging one given the lack of external guidance.To address this, instead of relying on simple triggers or handcrafted examples, we propose prompting the LLMs with the outputs generated by their own.To achieve this, we first collect a pool of rationales and answers to the test questions that are generated by the LLM using Zero-shot CoT, and then select the most suitable questions and answers for in-context learning, similar to Few-shot CoT.The choice of which selfgenerated rationales to include, however, is highly non-trivial, as illustrated in Fig. 1: not only does this problem inherit all of the challenges identified above for the few-shot scenario, it also carries the additional difficulty that the self-generated demonstrations themselves are imperfect.Retrieval methods assuming perfect labels (Rubin et al., 2022;Liu et al., 2022) are shown to perform worse than random in this setup (Zhang et al., 2022), and previous works bypass this issue by focusing only on surface text similarity and diversity of the question embeddings (Zhang et al., 2022), which as we show, in cases when most of the initial generated outputs are wrong, can be misleading.</p>
<p>To address these challenges, we propose COSP (Consistency-based Self-adaptive Prompting), an algorithm requiring only unlabelled samples (which are typically cheap to obtain, for example, via continuous, on-the-fly collections of user queries) and no labelled examples for strong performance, while also providing the capability of further improving the performance when a few labelled examples become available.The algorithm consists of two stages.In the first stage, COSP collects the pool of LLM responses to test questions via Zero-shot CoT.We use these responses to compute the outcome entropy of each question, a metric inspired by self-consistency (Wang et al., 2022a).This metric is then used to identify suitable question-reasoning pairs in an unsupervised way.</p>
<p>In the second stage, we perform another pass over all test samples, but with the selected questionreasoning pairs as in-context demonstrations.</p>
<p>We experiment with COSP with three different LLMs on a range of tasks.We demonstrate a 10-15% improvement in average accuracy for 6 arith- All of these are achieved with a negligible additional computational cost.In many cases, COSP performs on par or better than the few-shot baselines with handcrafted in-context examples.</p>
<p>Preliminaries</p>
<p>Chain-of-thought (CoT) Prompting.Few-shot CoT prompting (Wei et al., 2022b) prepends test questions with a series of in-context demonstrations of related solved questions as prompts.Generalizing beyond the few-shot setup, Kojima et al.  j and predicted answer ŷ(i) j ) and assign a score to each.Note that different rationales leading to the same final answer are marked the same color in "Outputs" blocks (described in §3.2).In Stage 2 (Right, §3.3), we augment the current test question for which we are interested in finding in-context examples (boxes shaded in blue) with a number of in-context demonstrations.These consist of test questions and selected Stage 1 outputs whose predicted answers ŷ(i) j are the majority predictions for that question that minimize the score defined in Eq. ( 8) with the procedure described in §3.2.The augmented test question is used to query the LLM a second time.Finally, a majority vote over outputs from both stages forms the final prediction.</p>
<p>(2022) propose Zero-shot CoT and shows that even by simply concatenating a trigger phrase (e.g., "Let's think step by step") to the test question, LLMs can be prompted to output intermediate steps in absence of handcrafted examples (we show an illustration of Few-shot and Zero-shot CoT in Fig. 5 in App.A).Our work builds upon on these works and improves upon them: we use the same zero-shot setup as Kojima et al. (2022), while enabling in-context learning in the format of Wei et al. (2022b) through an innovative use of the LLM's own outputs.</p>
<p>Self-consistency.Self-consistency (Wang et al., 2022a;Li et al., 2022b), which approximates the marginal distributions of the LLM via majority voting of multiple decoded reasoning paths, has been shown to lead to significant performance boosts across various tasks.It is compatible with either Zero-or Few-shot CoT by first introducing probabilistic decoding in the LLM decoding using a non-zero temperature, and then sampling m reasoning paths, each consisting a rationale-answer pair (r (i) , ŷ(i) ) from the LLM posterior:
{(r (i) j , ŷ(i) j )} m j=1 ∼ P(r (i) , ŷ(i) |x (i) , c, θ),(1)
where x (i) are the test questions, c denote the general format of the prompt template (e.g. the choice of in-context demonstrations in Few-shot CoT or the trigger phrase in Zero-shot CoT) and θ denote the weights that parameterize the LLM.The prediction with the majority (or more precisely, the plurality) vote is then chosen as the final prediction:
ŷ(i) = arg max ŷ(i) j m k=1 I(ŷ (i) j = ŷ(i) k ),(2)
where I(•, •) is the indicator function.We use and extend self-consistency as an integral part of our algorithm.Instead of simply taking the majority vote, we also use it as a key criterion for selecting the in-context demonstrations, which we discuss in detail in §3.</p>
<p>In this section, we present the problem setup, followed by detailed descriptions of the two stages and the key components of the algorithm.</p>
<p>Notations &amp; Problem Setup</p>
<p>We assume a test set D with |D| = N test examples.Given an LLM and a test query x (i) ∈ D (the test label, y (i) is not available to the LLM), we seek to generate a set of K demonstrations S, where each demonstration s k is a concatenation of another test question x (i k ) ∈ D \i , the generated rationale (r (i k ) ) and final prediction (ŷ (i k ) ) from the LLM (typically via Zero-shot CoT.The details are provided in §3.2):
s k = Concatenate(x (i k ) , r (i k ) , ŷ(i k ) ). (3)
The set S is then prepended as the "context" to the test question x i , serving as a guide to the LLM:
x(i) = Concatenate(s 1 , ..., s K , x (i) ),(4)
and x(i) is queried again (i.e., in-context learning).The objective is to build the set of in-context demonstrations S that maximize the LLM performance without accessing the ground-truth labels.This selection process is highly non-trivial -not only is in-context learning known to be sensitive to the choices of the demonstrations (Wei et al., 2022b;Liu et al., 2022), we also have the additional challenges that (1) we have to select K (typically small and ≤ 10) from a large set of candidates P, and that (2) the candidate pool is imperfect: in the absence of access to ground-truth labels, the responses in P are potentially erroneous, and could be misleading to the LLM.</p>
<p>3.2</p>
<p>To address these problems, we propose to use self-consistency both (1) to prune the candidate pool P, and (2) to select the demonstrations in absence of ground-truth labels.Drawing upon the insight from Wang et al. (2022a) that "majority predictions are more likely to be correct", for each test question x (i) we first compute their majority vote prediction(s) ŷ(i) from all predictions {ŷ
(i) j } m j=1
with Eq. ( 2) and retain only the rationales that lead to the majority vote prediction (which are more likely to be correct) and prune the rest of the rationale-prediction pairs (which are more likely to be wrong) from P. Following previous work (Wei et al., 2022b), we also use further heuristics, which we detail in App.B, to remove obviously bad candidates from P (e.g.responses containing no numbers for arithmetic tasks, or overly short and/or fragmented responses).Formally, after pruning, for test question x (i) , the set P is given by:
P = N i=1 m(i) j=1 x (i) , r (i) j , ŷ(i) ,(5)
where j indexes over all m(i) reasoning paths in the i-th question that led to the majority vote prediction ŷ(i) and are not otherwise excluded.</p>
<p>The second usage of self-consistency draws upon the insight that it approximates the amount of uncertainty of the model for its prediction.It is conceptually related to pseudo-labelling (Lee et al., 2013;Shi et al., 2018;Rizve et al., 2021) and entropy minimization (Grandvalet and Bengio, 2004).Intuitively, if the LLM outputs the same prediction repeatedly even under different reasoning paths, it is expected to be more confident in its prediction, and we should assign a stronger belief that the prediction is correct or at least plausible ("consistent predictions are more likely to be correct").To capture the model uncertainty, for a question x (i)  with m final answers from which we assume that there are n ≤ m unique answers ŷ(i) 1 , ..., ŷ(i) n , we compute the normalized entropy as:
H(x (i) |{ŷ (i) j } m j=1 ) = n α=1 p(ŷ (i) α ) log p(ŷ (i) α ) log m , (6(i) } N i=1 . 3: for i ∈ [1, N ] do 4:
Query the LLM with Zero-shot CoT to obtain m rationales and predictions {r
(i) j , ŷ(i) j } m i=1 to x (i) . 5:
Add candidate demos (Eq.( 3)) that led to the majority vote prediction to P. 6: end for 7: Build the set of demonstrations S using the procedure described in §3.2 and Eq. ( 8) &amp; (9).8:
for i ∈ [1, N ] do 9:
Concatenate the S to x (i) (Eq.( 4)) and query the LLM again to obtain m new rationales and answers (COSP) / an adaptive number of new rationales and answers proportional to the entropy of x (i) (COSP + ).10:</p>
<p>Compute the majority vote over 2m answers from both stages as the final answer ŷ(i) .11: end for ŷ(i) α in all m answers, and the entropy is normalized by log m, the (negative) maximum entropy if all predictions are different from each other, to [0, 1].</p>
<p>As shown in Fig. 3, we indeed find that the normalized entropy is a good proxy over a number of different tasks where low entropy is positively correlated with correctness.</p>
<p>GSM8K</p>
<p>MultiArith SingleEq 0.00 0.25 0.50 0.75 1.00</p>
<p>Outcome Entropy Correct Wrong</p>
<p>Figure 3: Entropy over outcomes approximates correctness.Distributions of entropy (Eq.( 6)) vs. the groundtruth accuracy using Zero-shot CoT of the majority vote predictions out of 7 sampled paths on GSM8K, Multi-Arith and SingleEq using PaLM-62B (Chowdhery et al., 2022).Questions answered correctly have significantly lower entropy.Further analyses are provided in App.C.1.</p>
<p>Penalizing Repetitions.Eq. ( 6) scores the test questions x (i) , but for each x (i) , especially for those with a low outcome entropy, there are multiple reasoning paths r (i) j for the majority prediction.While they lead to the same final answer, we find that their suitability as in-context demonstrations nonetheless differs.A key observation is that repetitive demonstrations often lead to worse performance, as exemplified in Fig. 1 (see Demo2).We hypothesize that repetitive responses acting as demonstrations create a strong but spurious pattern on which LLMs are prone to overfit, often leading to catastrophic deterioration of performance.</p>
<p>To address this, we first filter out the responses that involve self-questioning which we observe often leads to repetition or prompts the LLM to answer the generated questions instead of the original questions.We also introduce a quantitative measure by splitting demonstrations into phrases delimited by punctuation marks ("[.,?!]").Assuming that we have Q phrases * , we compute repetitiveness as: R r (r
(i) j ) = 2 Q(Q − 1) Q a=1 Q b=a+1 W ab , (7)
where W ∈ R Q×Q + is the similarity matrix over all pairs of phrases with W ab = S c (ϕ(q a ), ϕ(q b )), where S c (•, •) computes the cosine similarity and ϕ(q a ) and ϕ(q b ) denote the vector embedding of a-th and b-th phrases, typically obtained through an auxiliary, small language model.From Eq. ( 7), the matrix is summarized by averaging across offdiagonal elements, which gives an average similarity between two arbitrary phrases in the response.We note that it is possible for other techniques that penalize repetition at decoding-time to be applied in lieu (like hard or soft n-gram blocking (Klein et al., 2017;Paulus et al., 2017)).We use the proposed method as it also captures semantic-level repetitions, which we find to be more common than verbatim repetitions, especially when a non-zero temperature is used.</p>
<p>Overall Design &amp; Building S. With measures of consistency (Eq.( 6)) and repetitiveness (Eq.( 7)) defined, for a candidate demonstration p ∈ P given by the concatenation of question x (i) , rationale r (i) j and the (majority) prediction ŷ(i) , the score is proposed as:
F(p|x (i) , r (i) , {ŷ (i) j } m j=1 ) = H(x (i) ) + λR r (r (i) j ),(8)
where λ is a trade-off hyperparameter that is set to 0.2 in all experiments.To ensure that the different terms of Eq. ( 8) are of a comparable scale, we Algorithm 2 Building S for K ≥ 2.</p>
<p>1: Initialize S with the minimizer of Eq. ( 8
): S ← {p * 0 = arg minp∈P F(p)} 2: for k ∈ [2, K] do 3:
Find the minimizer of the modified objective (Eq.( 9)):
p * k = arg minp∈P G k (p).</p>
<p>4:</p>
<p>Add p * k to S and remove p * k from candidate pool P. 5: end for employ z-score normalization commonly used in reinforcement learning (van Hasselt et al., 2016) and NLP (Deng et al., 2022) by replacing each term with their respective z-score, with the mean and standard deviation computed over P. To select a single in-context demonstration (K = 1), we utilize the minimizer of the scoring function p * = arg min p∈P F(p).More commonly, though, we would like to select multiple demonstrations, leading to a combinatorial selection problem.For computational feasibility, we use a greedy forward selection procedure (Caruana et al., 2004) as described in Algorithm 2. We initialize S with p * and in a greedy way and select the minimizer of the following modified objective function:
G k (p) = F(p) + λR q (p, S k−1 ) ∀k ∈ [2, K],(9)
where S k−1 is the partially built demonstration set S with k − 1 elements already selected, and
R q (p, S k−1 ) = max {S c (ϕ(p), ϕ(s k ′ ))} k−1 k ′ =1
(10) is a term to encourage diversity in question types and reasoning patterns in S by penalizing demonstrations that are similar to previous ones.We additionally constrain that S should not contain multiple reasoning paths to the same question.The algorithm is repeated until all K demonstrations of S are selected.</p>
<p>Stage 2: Query with Generated In-context Demonstrations</p>
<p>With S selected, in Stage 2, we concatenate and prepend the demonstrations to the test question (Eq.( 3) &amp; ( 4)) and query the LLM again with m more repetitions.The final prediction for each question is then output as the majority vote across the predictions from both stages.</p>
<p>Outcome Entropy as a Proxy to Difficulty.We use the entropy (Eq.( 6)) to select demonstrations in §3.2.However, it is also a natural gauge of difficulty of a test question to the LLM, as a higher entropy (thus a higher uncertainty) implies that the LLM may require additional demonstrations for this question.To this end, we also experiment with a variant of COSP termed COSP + (Algorithm 1) in §5 which features an adaptively allocated number of in-context demonstrations per question that is proportional to its zero-shot entropy in Stage 1, with higher-entropy questions given more demonstrations.It is worth noting that while it is easy to select more demonstrations in COSP as we only have to greedily optimize (8) until a higher K is reached, for methods like Few-shot CoT, where the number of demonstrations is also the number of "shots", increasing it requires handcrafting more completed examples.</p>
<p>Related Works</p>
<p>There have been several recent works that also aim to improve LLM reasoning using the models' own outputs.STaR (Zelikman et al., 2022) (Chowdhery et al., 2022).Methods in the 0-shot columns use no ground-truth label guidance, whereas the 5-shot results use 5 manually labelled in-context demonstrations, randomly selected from the provided examples in Wei et al. (2022b) nale.More recently, Huang et al. (2022) relies on self-consistency to improve LLMs by fine-tuning on a large number of generated questions and rationales.However, both works rely on fine-tuning of massive models, which requires access to model weights and is much more expensive.In contrast, our method can treat the model as a black box and only requires forward passes.Other concurrent related works include Honovich et al. (2022) and Wang et al. (2022b).However, while related methodologically to our method, these papers focus on fundamentally different problems of data generation and instruction following.</p>
<p>For the in-context learning setup (Brown et al., 2020;Wei et al., 2022a;Radford et al.), various methods have been proposed, typically to retrieve relevant demonstrations from a relatively large number of labelled examples (Rubin et al., 2022;Su et al., 2022).However, Zhang et al. (2022) shows that with imperfect labels, nearest neighborstyle retrieval underperforms random selection and propose Auto-CoT, one of the few existing works that are fully comparable to COSP.Auto-CoT also uses models' zero-shot outputs as in-context demonstration by selecting the responses to the questions that are the k-means centroids in the embedding space from an auxiliary language model (a concurrent work, Li et al. (2022a) uses a similar clustering approach).However, by selecting based on question embedding only, Auto-CoT has no control over the quality of the generated rationales, and is thus prone to generating erroneous and misleading demonstrations; we empirically compare to it in §5.Another concurrent work (Weng et al., 2022) proposes self-verification, which is a zero-shot reasoning verifier to recover conditions from the predictions.However, it only works when there is a unique correct condition that results in the correct answer (such as arithmetic questions), but is not applicable when there are multiple plausible conditions.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Datasets &amp; Tasks.We consider the following arithmetic and logical reasoning tasks: AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2016), GSM-8K (Cobbe et al., 2021), SingleEq (Koncel-Kedziorski et al., 2015), CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021).The licenses of these datasets are listed in App.B.</p>
<p>LLMs.We run experiments with 3 LLMs: PaLM with 62 billion parameters (PaLM-62B) and 540 billion parameters (PaLM-540B) (Chowdhery et al., 2022) (both quantized to int8 precision) and GPT-3 with 175 billion parameters (code-davinci-001) (Brown et al., 2020;Chen et al., 2021).The readers are referred to App.B for further details.Baselines.We compare against Zero-shot CoT (Kojima et al., 2022) with and without selfconsistency (Wang et al., 2022a), and Auto-CoT (Zhang et al., 2022).Auto-CoT does not use selfconsistency originally but for a fair comparison and given that self-consistency significantly improves LLM performance, we also include an Auto-CoT baseline augmented with self-consistency (the results of original Auto-CoT without self-consistency, which have worse performance, are reported in App.C.2).Besides the zero-shot baselines, we also include Few-shot CoT (Wei et al., 2022b) with self-consistency as a comparison, where we use 5 handcrafted demonstrations randomly chosen from the provided lists of examples in Wei et al. (2022b) for all datasets to match the 5 generated demonstrations in COSP and Auto-CoT.For COSP, we sample m = 7 reasoning paths in the Stage 1 and Stage 2, respectively, and for Zero-shot CoT, Few-shot CoT and Auto-CoT experiments with self-consistency, equivalently we use 14 reasoning paths for each question.In addition to the reproduced baselines, we also reference previously published baselines under Prev columns in Tables 1 &amp; 2 that use more labelled examples and/or sampling paths, both of which lead to further performance gains but at an additional cost.We show in §5.2 that COSP may also take advantage of more sampling paths when resources are available.As mentioned in §3.2, we use λ = 0.2 (Eq.( 8)) for all experiments without further hyperparameter tuning -this value is chosen to reflect our a-priori belief of the relative importance of the objective function terms.While it is possible to achieve even further performance improvements by tuning this term, it is not investigated in the present work for a fair comparison against the baselines.</p>
<p>Results</p>
<p>Discussions on Main Results.We show the results on PaLM-62B, PaLM-540B and GPT-3 in Tables 1, and 2, respectively, and give some examples of the found demonstrations in App.D. Since we find that self-consistency improves performance in all cases, we only report the results with self-consistency in these tables; the readers are referred to App.C.2 for results without selfconsistency.In all cases, COSP delivers improvements over Zero-shot CoT baselines with particularly large gains seen on PaLM-62B and GPT-3, achieving 10% ∼ 15% average improvement over the Zero-shot CoT with self-consistency baseline.Remarkably, even though COSP operates under a much more challenging setup without ground-truth labels, COSP performs on par or better than Fewshot CoT in almost all tasks that use labels.We argue the large gains seen in the smaller models that significantly reduced their performance gap to the very large models (e.g.PaLM-540B), can be particularly practically impactful, given the former's strong advantages in computational costs and general accessibility.Furthermore, we find COSP to be robust by consistently improving over the zero-shot CoT baseline in all but one case, whereas Auto-CoT sometimes deteriorates the performance.We perform a failure analysis in App.C.5, and show that by using self-consistency as a proxy of correctness, in most cases COSP is capable of identifying outputs with sound reasoning as in-context examples even when the task is very challenging to the LLM under the zero-shot setup.In contrast, Auto-CoT is more prone to selecting erroneous examples; while Zhang et al. (2022) claims these errors minimally affect performance, we find their effect to be very model and task-specific, holding true for larger models (e.g.PaLM-540B) and easier tasks.However, on more difficult tasks or smaller models, we find erroneous demonstrations to often lead to performance deterioration.COSP + .We test COSP + on the PaLM-62B model (see App. C.3).In 5 out of 6 cases, COSP + leads to further performance gain.Across experiments, we use a fixed K which is shown to perform well overall, while COSP + results show promise in adaptively setting K; we defer a thorough investigation to future work.</p>
<p>COSP-FS.</p>
<p>We test COSP-FS in the few-shot setting as described in §3.3.We focus on GSM-  8K, the most challenging task for PaLM-62B and GPT-3 where we hypothesize that labelled samples would help more.We start with 5 labelled examples provided in Wei et al. (2022b) in Stage 1.In Stage 2, we use COSP-FS to select 3 further examples.We show the results in Table 5 in App.C.4.It is worth noting that while in Table 2 COSP reduced the performance compared to Zero-shot CoT due to the difficulty of this task and selecting correct demonstrations from the zero-shot outputs (Table 7, App.D), the addition of labelled examples restored the effectiveness of COSP, primarily through the restoration of the predictive value of consistency (Eq 6) to correctness.We analyze this in App.C.5.Ablation Studies.We perform ablation studies on COSP, using the PaLM-62B model on the Mul-tiArith dataset.We first analyze the relative importance of various components of COSP and different terms in the scoring function Eq. ( 8) &amp; 9.As shown in Fig. 4a, the results in the following settings demonstrate that all components of COSP positively contribute to the performance:</p>
<ol>
<li>
<p>S1: Majority vote on Stage 1 outputs only.This is essentially Zero-shot CoT over m paths.</p>
</li>
<li>
<p>S2,C: Majority vote on Stage 2 outputs only (i.e. the m predictions from the LLM after in-context learning with S).Only the entropy term (Eq.( 6)) is used as the scoring function.</p>
</li>
</ol>
<p>COSP(S1+2,C+D):</p>
<p>Full COSP with majority votes over predictions from both stages.</p>
<ol>
<li>COSP + (S1+2,C+D): COSP + from §3.3.Same as (iv) but with an adaptive number of demonstrations per sample.</li>
</ol>
<p>Another possible baseline is to select demonstrations randomly from the Stage 1 outputs instead of using any demonstration selection heuristic described in §3.2; we do not include this as this is included in and outperformed by AutoCoT in Zhang et al. ( 2022) -we outperform even AutoCoT as shown in Tables 1 and 2. Lastly, we also analyze the effect of the number of sampling paths in Fig. 4b, where it is clear that the performance of all methods increases with number of paths.This demonstrate that there is potential for further performance gain by scaling COSP when additional computational resources are available.</p>
<p>Conclusion</p>
<p>We propose COSP, a prompting algorithm to improve zero-shot reasoning abilities of LLMs.It achieves this by selecting in-context demonstrations from its own outputs using a novel scoring function that incorporates consistency, diversity and repetitiveness.Across multiple LLMs and tasks requiring complex reassigning, we demonstrate large improvements over the state of the art.We believe there are multiple promising directions for future work: first, the present work focuses on improving zero-shot reasoning, but we argue that the key principle used in this paper may also be extended to more general NLP task types.Second, beyond solving reasoning problems per se, LLMs' reasoning capabilities are increasingly used in other scenarios for planning &amp; interactions with external environments (Yao et al., 2023) -extending COSP to these setups could also be promising.We defer thorough investigations to a future work.</p>
<p>Limitations</p>
<p>One limitation is that the efficacy of our method is shown with massive LLMs in this paper.However, we note that our method is based on only model inference with them and is already considerably cheaper than other adaptation methods.Furthermore, our method seeks to improve LLMs: while the technology itself is ethically neutral, we acknowledge that there are various social and ethical risks of potential misuse, especially given the powerful generative capabilities of the LLMs that have become increasingly accessible to a broader audience (Weidinger et al., 2021).We argue that both the prospective end-users and researchers should be aware of these concerns when using our method in order to mitigate these risks.</p>
<p>Methodologically, we note that an integral component of our algorithm is self-consistency.We rely on the expectation that it reliably predicts accuracy, which essentially places an expectation that the model uncertainty should be reasonably wellcalibrated.While we have indeed found this to be the case for almost all considered tasks and models, additional investigations might be required to ascertain their general applicability.Given our reliance on self-consistency, for tasks where selfconsistency does not lead to significant gains, the performance improvements with our method may be limited.An example of this could be tasks with very small label spaces (e.g.binary classification) where "consistency" in outcomes may be achieved much more easily even if the model simply outputs random predictions.A potential remedy, for which we defer a thorough investigation to future work, is to not only consider the consistency over outcomes but also over the intermediate rationales which are generated but not currently used for selfconsistency computation.</p>
<p>Second, while COSP improves performance in an overwhelming majority of cases and is significantly less sensitive to the original zero-shot model performance compared to baseline methods like AutoCoT, there still exist exceptional cases where it fails to improve, especially when the tasks are too challenging in zero-shot setup -we argue that this is also due to the general, inherent limitations of the LLMs.However, both continual improvements on the foundational models and provision of some human guidance (e.g., using COSP-FS) should alleviate this issue.</p>
<p>A Illustrations of Zero-shot and Few-shot CoT</p>
<p>We show additional illustrations of CoT prompting techniques described in §2 in Fig. 5.
5</p>
<p>B Additional Implementation Details</p>
<p>In this section, we describe the additional implementation details for the experiments: for the PaLM models, we follow Wang et al. (2022a) to use a temperature of 0.7 during decoding to extract different reasoning paths when self-consistency is used.On GPT-3 model, we use a temperature of 0.4 (for experiments without self-consistency, we use the default greedy decoding procedure with a temperature of 0).Both COSP and Auto-CoT involve the use of an auxiliary language model to extract sentence embeddings of questions and/or rationales.We use Sentence-T5-large (Ni et al., 2022) for COSP.It is worth noting that Auto-CoT (Zhang et al., 2022) originally uses SentenceBERT (Reimers and Gurevych, 2019); for consistency we use Sentence-T5 for Auto-CoT as well.Given that Ni et al. (2022) show that Sentence-T5 outperforms SentenceBERT for most sentence-level tasks, there should only be a positive impact on the baseline performances as a result of this change.For all tasks except for GSM-8K, we set a maximum decoding step of 128 tokens.On GSM-8K, we found that often long responses are generated and we increase the maximum decoding step to 256 tokens.</p>
<p>We also describe the additional heuristics used to prune the pool of candidates as described in §3.2.For demonstration selection, we reject the following questions or rationales which are observed to be inappropriate for our goals:</p>
<p>(i) Responses that feature no numbers (implemented via regular expression matching) for arithmetic tasks.(ii) Responses that involve generated questions.</p>
<p>While sometimes useful, we find that in general self-questioning is confusing, and often mislead the LLMs to answer the generated questions instead of the actual questions being asked.(iii) Overly short rationales: responses containing fewer than 5 tokens.(iv) Overly fragmented responses: after separating the rationales into phrases with punctuation marks as delimiters ("[?!.,]"), we filter out the responses that feature more than 10 fragments.This is similar to the heuristic used in Wei et al. (2022b) and Zhang et al. (2022) that remove overly long questions and responses.</p>
<p>We also conduct post-processing of the LLM outputs for all methods by, for example, cutting off outputs at stop tokens like "\n\n\n" and "Q:" (the latter is to prevent the LLM from generating further questions).For other pre-and post-processing steps and for answer extraction procedures, we follow Kojima et al. (2022)  MultiArith, Sin-gleEq and AddSub datasets are licensed under the CC:BY 4.0 license (https: //www.cs.washington.edu/nlp/arithmetic).We note that our use of these assets are consistent with their intended use (for research and developments), and the use of them for the purpose of evaluating LLMs is well-documented in the literature.For all datasets, we use the same data split as previous works like Kojima et al. (2022).</p>
<p>Since no training data is used, we only use the test split (when the test labels are publicly available) or the validation/dev split (otherwise).</p>
<p>C Additional Results &amp; Analyses</p>
<p>C.1 Outcome Entropy vs Correctness to Fig. 3, in this section we provide additional experiments that demonstrate the link of the outcome entropy (Eq.( 6)) and groundtruth correctness over more tasks and models.We show the full results in Fig. 6.We show that for a vast majority of cases, the outcome entropy is correlated with correctness in a statistically significant way (in all cases except for StrategyQA on GPT-3, the p-value testing the significance of the Pointbiserial correlation (pbc) coefficient between entropy and accuracy is below 10 −5 .The pbc coefficient on StrategyQA/GPT-3 is 0.04), with correctly answered questions featuring lower entropy.We find a stronger separation in entropy distribution in arithmetic tasks compared to the logic reasoning tasks (i.e., CSQA and StrategyQA).We argue that this is partly due to logic reasoning tasks considered in the paper being feature much smaller label spaces -while the answer to the arithmetic tasks can be any real number (essentially an infinitelylarge label space), CSQA and StrategyQA feature multiple choice questions with 5 (choices A-E) and 2 (Yes/No) classes only, respectively.This is consistent with our discussions in §7 where we argue that the outcome entropy is more useful for large label space where it is very unlikely for random guessing to lead to correct answers, as opposed to classification tasks with few possible labels.As discussed in §7, a promising future direction is to address this issue and to improve the effectiveness of consistency on these types of tasks.</p>
<p>We further investigate the impact of the incontext demonstrations on the entropy by separating and comparing between Stage 1 and Stage 2 entropy and we show the comparison in Fig. 7, and we find that along with the improvement of test accuracy as reported §5, the addition of the demonstrations significantly also reduces the outcome entropy across all tasks and models.</p>
<p>C.2 Results without Self-consistency</p>
<p>In this section, we report baseline results on Zeroshot CoT, Few-shot CoT and Auto-CoT without self-consistency on PaLM models, which are omitted in Table 1 in the main text.As shown in Table  6)) vs groundtruth correctness using Zero-shot CoT for more tasks on PaLM and GPT-3.</p>
<p>3, we find self-consistency to increase performance across all models and tasks considered; due to this, we always use self-consistency in COSP, and only perform experiments with self-consistency in the GPT models.</p>
<p>C.3 COSP + Results</p>
<p>We perform preliminary experiments of COSP + on PaLM-62B model, and we report the results in Table 4.In 5 out of 6 tasks, COSP + leads to small improvements but it deteriorates the performance on GSM-8K as the additional demonstrations in this case contain more errors and became misleading to the LLM.We argue that the results show promise in more adaptively selecting demonstrations, and we defer a thorough exploration in a future work.</p>
<p>C.4 COSP-FS Results</p>
<p>We compare COSP-FS against Few-shot CoT with self-consistency using PaLM-62B and GPT-3 on GSM-8K in Table 5.Table 3: Baseline results on PaLM-62B (Left) and PaLM-540B (Right) (Chowdhery et al., 2022) without using self-consistency (i.e.argmax sampling).Except for not using self-consistency and the use of the zero temperature, all experimental setup is otherwise identical to that in Table 1.</p>
<p>C.5 Failure Analysis</p>
<p>In this section we conduct analysis in cases where demonstrations do not lead to performance improvement or even leads to performance deterioration.The only case where COSP performs worse than the Zero-shot CoT is on the GSM-8K task on GPT-3.We show some of the identified demonstrations on this particular task-model combination in Table 7 in App.D, and we find that in this case, due to the extreme difficulty of the task to the model most of the generated demonstrations are wrong.On the other hand, competing methods like Auto-CoT lead to such performance deterioration more frequently.6)) and uses a minimum of 5 and up to 8 demonstrations per sample.The COSP results are lifted from Table 1.</p>
<p>we use self-consistency to approximate correctness, the decision of Auto-CoT on which question and rationale to include entirely hinges on the question embedding, and when the initial Zero-shot performance is low, most of the generated responses would be erroneous.We find that such deterioration indeed happens more frequently in the smaller models and/or more difficult tasks where the LLMs rely more on in-context examples for guidance (we give an example in Table 6.On the other hand, in larger models like PaLM-540B with strong Zeroshot performance and whose errors are more likely due to minor flaws in reasoning (e.g.arithmetic mistakes in a single step) rather than major issues (e.g.repetitions, gibberish outputs &amp; major semantic misunderstanding), we find that consistent with the arguments in §5.5 of Zhang et al. (2022), the results are indeed less sensitive to flawed demonstrations.</p>
<p>D Examples of Generated Demonstrations</p>
<p>We show some of the generated demonstrations in various datasets in Table 7 (GPT-3) and Table 8 (PaLM).</p>
<p>COSP Q: Irwin 's family went on a camping trip in the mountains .On the first day , they hiked from their car to the campsite .First , they hiked 0.2 mile from the car to a stream , and 0.4 mile from the stream to a meadow .Then they hiked 0.1 mile from the meadow to the campsite .How many miles did Irwin 's family hike in all ?A: Let's think step by step.First, they hiked 0.2 miles from their car to the stream.Then, they hiked 0.4 miles from the stream to the meadow.Finally, they hiked 0.1 miles from the meadow to the campsite.So, the total distance is 0.2 + 0.4 + 0.1 = 0.7 miles.Therefore, the answer (arabic numerals) is 0. Q: There are 33 oak trees currently in the park .Park workers had to cut down 18 oak trees that were damaged .How many oak trees will be in the park when the workers are finished ?A: Let's think step by step.-First, we know that 18 trees were cut down.-18 trees were cut down, so there were originally 15 trees.-15 -18 = -3 trees, so there will be 3 new oak trees in the park.-33 -3 = 30 trees, so there will be 30 oak trees in the park when the workers are finished.Therefore, the answer (arabic numerals) is 30.× Q: When Jake had 1 cat , he needed to serve 0.5 can of cat food each day .Now that Jake has adopted a second cat , he needs to serve a total of 0.9 can each day .How much extra food is needed to feed the second cat ?A: Let's think step by step.-First, we need to find the total number of cans needed to feed both cats.</p>
<p>-Then, we need to find the difference between the total number of cans and the number of cans needed to feed one cat.-Finally, we need to find the number of cans needed to feed the second cat.-The answer is the number of cans needed to feed the second cat.Therefore, the answer (arabic numerals) is 0.9.× Therefore, the answer (arabic numerals) is 6. × Q: If one person can paint half a house in 5 days, how many hours would it take for 5 people to paint a whole house?A: Let's think step by step. 1. Half a house can be painted in 5 days by 1 person.2. 5 people can paint half a house in 5 days.3. 5 people can paint a whole house in 5 days.So the answer is 5 days.Therefore, the answer (arabic numerals) is 5. × Q: The local firefighters are doing a "fill the boot" fundraiser.Their goal is to raise $6300.After the first 3 hours, they have raised $2100.For how many hours do they have to fundraise in total to reach their goal, assuming an equal amount raised in every hour?A: Let's think step by step.We know that the total amount raised is equal to the total number of hours multiplied by the amount raised per hour.Total amount raised = Total number of hours * amount raised per hour $6300 = 3 * $2100 $6300 / $2100 = 3 So, the total number of hours is 3. Therefore, the answer (arabic numerals) is 3.</p>
<p>3511</p>
<p>Dataset Demonstrations GSM-8K Q: Bruce works for 5 hours on Tuesday.On Wednesday he works twice the time he works on Tuesday.On Thursday he works 2 hours less than the time he works on Wednesday.How many hours does Bruce work in all these three days?A: Let's think step by step.On Tuesday, Bruce works for 5 hours.On Wednesday, Bruce works for 2 * 5 = 10 hours.On Thursday, Bruce works for 10 -2 = 8 hours.So, in all three days, Bruce works for 5 + 10 + 8 = 23 hours.Therefore, the answer (arabic numerals) is 23.✓ Q: Mark decides to buy packs of old magic cards and open them to sell.He buys 3 packs for $1800 each.He gets 1 card that is worth $4000 and another card worth $1000.There are 30 more cards worth an average of $50 each.How much money profit did he make?A: Let's think step by step.He spent $1800 * 3 = $5400.He got $4000 + $1000 + $50 * 30 = $5500.So he made $100 profit.D3.Did you discuss whether and how consent was obtained from people whose data you're using/curating?For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?Not applicable.Left blank.</p>
<p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?Not applicable.Left blank.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?Not applicable.Left blank.</p>
<p>[Figure 1 :
1
Figure 1: Selecting in-context demonstrations for reasoning tasks can be a delicate art.LLM output is sensitive to in-context demos and their reasoning, especially when they are generated and imperfect.Example inputs &amp; outputs shown from top to bottom (MultiArith dataset &amp; PaLM-62B model): (1) zero-shot CoT with no demo: correct logic but wrong answer; (2) correct demo (Demo1) and correct answer; (3) correct but repetitive demo (Demo2) leads to repetitive outputs; (4) erroneous demo (Demo3) leads to a wrong answer, but (5) combining Demo3 and Demo1 again leads to a correct answer.This motivates the need for a carefully-designed selection procedure for in-context demos, which is the key objective of this paper.</p>
<p>Figure 2 :
2
Figure2: The overall procedure of COSP.In Stage 1 (Left, §3.2), we run Zero-shot CoT multiple times on each question to generate a pool of demonstrations (each consisting of the question x (i) , generated rationale r (i)</p>
<p>Figure 4 :
4
Figure 4: Ablation results on MultiArith dataset using the PaLM-62B LLM.</p>
<p>3.</p>
<p>S2,C+D: Majority vote on Stage 2 outputs only with the full scoring function (Eq.(8) &amp; 9).</p>
<p>Figure 5 :
5
Figure 5: Few-shot (left) and zero-shot (right) CoT.Both prompts the LLM to output intermediate steps: in few-shot (1-shot shown in the figure, with one provided completed question-rationale-answer set), handcrafted in-context examples are provided and prepended to the test questions.In the zero-shot setting, chain-of-thought output is prompted by trigger phrases ("Let's think step by step." in the figure).</p>
<p>Figure 6 :
6
Figure 6: Distribution of entropy (Eq.(6)) vs groundtruth correctness using Zero-shot CoT for more tasks on PaLM and GPT-3.</p>
<p>Figure 7 :
7
Figure 7: Distribution of entropy (Eq.(6)) in Stage 1 (i.e.first m sampling paths with Zero-shot CoT without demonstrations) and in Stage 2 (i.e. the second m sampling paths after demonstrations are prepended) in PaLM models.</p>
<p>Table 1 :
1
Madaan and Yazdanbakhsh (2022).b Wang et al. (2022a): Significantly more (40) paths sampled.Accuracy on PaLM-62B (Left) and PaLM-540B (Right)
Model Setting MethodPaLM-62B 5-shot Prev 0-shot Auto-COSP 5-shot 8-shot 0-shot Auto-COSP 5-shot 8-shot PaLM-540B 0-shot 0-shot 5-shot PrevCoTCoT (Ours)CoTCoTCoTCoT (Ours)CoTCoT# Paths14147+71414147+714MultiArith AddSub SingleEq GSM-8K CSQA StrategyQA (Average)67.2 69.1 74.4 20.9 46.5 57.2 55.88 49.55 66.28 9.4 85.0 73.2 78.9 77.8 78.7 9.2 30.2 68.2 60.2 59.4 64.781.0 72.4 79.8 30.3 66.8 67.9 66.37---27.4 a ---95.2 88.9 88.6 68.5 74.2 66.0 80.25 83.37 83.77 99.0 98.8 89.1 89.9 85.6 90.4 71.4 71.9 79.4 76.4 75.7 75.296.0 86.6 89.2 64.3 80.7 81.4 83.0399.3 b 93.7 b -74.4 b 80.7 b 81.6 b -a
is one of the first works to bootstrap reasoning from LLMs: starting from a small number of labelled data, STaR prompts the LLMs to generate rationales over the large corpus of untrained data.However, STaR requires human verification on the generated ratio-</p>
<p>. Prev columns show results previously published under similar setups but often with significantly more sampling paths and/or labelled examples."# Paths" denotes the number of reasoning paths sampled per test question.COSP uses m = 7 paths per stage and 2m = 14 paths in total.We also report baseline results without self-consistency (which are worse) in App.C.2. COSP and Auto-CoT use 5 generated in-context examples per test question.The top two results for each model are bolded and ranked by color: best and second-best.</p>
<p>Table 2 :
2
(Brown et al., 2020;Chen et al., 2021)hen et al., 2021).Refer to Table1for further explanations.</p>
<p>Intuitively, unlike COSP where
Model Setting MethodPaLM-62B 0-shot COSP COSP +(Ours) (Ours)# Paths7+77+7MultiArith AddSub SingleEq GSM-8K CSQA StrategyQA (Average)85.0 78.9 78.7 30.2 60.2 64.7 66.2886.2 79.2 78.9 28.2 60.3 65.1 66.32Table 4: Comparison between COSP and COSP + on PaLM-62B. While COSP uniformly uses 5 in-context demonstrations per question, COSP + uses an adaptive number of demonstrations per sample proportional to the outcome entropy of the test question in Stage 1 (Eq. (</p>
<p>Table 5 :
5
COSP-FS results.
Model SettingGPT-35-shotPaLM-62BMethodManual COSP-5-shot COSP-CoTFSCoTFS# Paths147+7147+7GSM-8K16.717.730.330.8</p>
<p>Joan decided to sell all of her old books .She gathered up 33 books to sell .She sold 26 books in a yard sale .How many books does Joan now have ?A: Let's think step by step. 1. 33 books -26 books = 7 books 2. Therefore, the answer (arabic numerals) is 7. ✓ Q: Keith grew 6 turnips .Alyssa grew 9 turnips .How many turnips did they grow in all ?A: Let's think step by step. 1. Keith grew 6 turnips.2. Alyssa grew 9 turnips.3.So in total, they grew 15 turnips.Therefore, the answer (arabic numerals) is 15.✓ Auto-CoT Q: Sally picked 7 lemons and Mary picked 9 lemons from the lemon tree .How many lemons were picked in total ?A: Let's think step by step.Sally picked 7 lemons.Mary picked 9 lemons.So, 14 lemons were picked in total.Therefore, the answer (arabic numerals) is 14. × Q: Sally had 27 Pokemon cards .Dan gave her 41 new Pokemon cards .Sally bought 20 Pokemon cards .How many Pokemon cards does Sally have now ?A: Let's think step by step.Sally had 27 Pokemon cards.Dan gave her 41 new Pokemon cards.Sally bought 20 Pokemon cards.How many Pokemon cards does Sally have now?Therefore, the answer (arabic numerals) is 27.×
7. ✓Q: Joan purchased a basketball game for $ 5.20 , and a racing game for $ 4.23 . How much did Joan spend on video games ? A: Let's think step by step. 1. Joan purchased a basketball game for $ 5.20 , and a racinggame for $ 4.23 . 2. $ 5.20 + $ 4.23 = $ 9.43 3. $ 9.43 is the total amount spent on video games. Therefore, the answer (arabic numerals) is 9.43. ✓Q: Jason has 7 violet balloons and 4 red balloons . He lost 3 of the violet balloons . How many vio-let balloons does Jason have now ? A: Let's think step by step. 7 -3 = 4 4 = 4 So, he has 4 violet balloons now. Therefore, the answer (arabic numerals) is 4. ✓Q: Q: Mary had 21 dimes and 38 pennies in her bank . Her dad borrowed 18 pennies from Mary . How many pennies does she have now ? A: Let's think step by step. Step 1: 21 dimes + 38 pennies = 59 pennies Step2: 18 pennies -18 pennies = 0 pennies Step 3: 59 -0 = 59 pennies Step 4: 59 pennies = 59 Step 5: 59 pennies =59 pennies Step 6: 59 pennies = 59 Step 7: 59 = 59 Step 8: Therefore, Mary has 59 pennies. Therefore, the answer (arabic numerals) is 59. ×</p>
<p>Table 6 :
6
Comparison between the demonstrations identified by COSP and Auto-CoT on the AddSub task using GPT-3.In this case, the COSP demonstrations lead to 18% improvement over Zero-shot CoT with self-consistency whereas AutoCoT demonstration led to performance deterioration.Reasoning steps are highlighted: correct, questionable/incomplete/repetitive/irrelevant and erroneous, and the ground-truth correctness of the demonstrations (unavailable to the LLM) marked by ✓ or × at the end of the responses.Debby's class is going on a field trip to the zoo.If each van can hold 4 people and there are 2 students and 6 adults going, how many vans will they need?A: Let's think step by step.First, we'll figure out how many people are going.2 students and 6 adults.So, 8 people.Then, we divide the number of people by the number of people in a van.8 people divided by 4 people per van = 2 vans.Therefore, the answer (arabic numerals) is 2. ✓ Q: Nancy had 80 files on her computer.She deleted 31 of them and put the rest into folders with 7 files in each one.How many folders did Nancy end up with?A: Let's think step by step.She deleted 31 files, so she had 80 -31 = 49 files.She put 7 files in each folder, so she had 49 / 7 = 7 folders.Therefore, the answer (arabic numerals) is 7. ✓ Q: At the arcade, Tom won 32 tickets playing 'whack a mole' and 25 tickets playing 'skee ball'.If he spent 7 of his tickets on a hat, how many tickets does Tom have left?A: Let's think step by step.He has 32 + 25 = 57 tickets.He spends 7 on a hat, so he has 50 left.Therefore, the answer (arabic numerals) is 50 ✓ Q: The schools debate team had 5 boys and 40 girls on it.If they were split into groups of 9 how many groups could they make?A: Let's think step by step.5 boys and 40 girls = 45 students Let's take 9 students per group.Therefore, the answer (arabic numerals) is 5. ✓ SingleEq Q: Mike has 87 baseball cards.Sam bought 13 of Mike's baseball cards.How many baseball cards does Mike have now ?A: Let's think step by step.13 of Mike's baseball cards are with Sam.So, 87 -13 = 74.Then, Mike has 74 baseball cards now.Therefore, the answer (arabic numerals) is 74.✓Q: Sam had 98 pennies in his bank.He spent 93 of his pennies.How many pennies does he have now ?A: Let's think step by step. 1. Sam had 98 pennies in his bank.2.He spent 93 pennies, so the amount of pennies he has left is 5. 3. So, Sam has 5 pennies now.""" # Solution # Sam had 98 pennies in his bank.# He spent 93 pennies, so the amount of pennies he has left is 5. # So, Sam has 5 pennies now.# 2. Therefore, the answer (arabic numerals) is 5. ✓
Dataset Demonstrations Multi Arith Q: s think step by step. 16 pecan pies + 14 apples pies = 30 pies. 30 pies / 5 pies per row = 6 rows. So, 6 rows. Therefore, the answer (arabic numerals) is 6. ✓ GSM-8K
Q: Nancy goes fishing with Joan.They catch 18 trout.If they equally split up the trout, how many will each one get ?A: Let's think step by step.First, we divide the total number of trout by the number of people.The answer is 9. So, each person catches 9 trout.Therefore, the answer (arabic numerals) is 9. ✓ Q: Mrs. Hilt baked pies last weekend for a holiday dinner.She baked 16 pecan pies and 14 apples pies.If she wants to arrange all of the pies in rows of 5 pies each, how many rows will she have?A: Let'Q: Two thirds of Jana's puppies are Pomeranians.One third of the Pomeranians are girls.If there are 6 Pomeranian girls, how many puppies does Jana have?A: Let's think step by step.First, we'll look at the full set of options for Jana's puppies: Puppies: Jana's puppes are Pomeranians and are either girls or boys.Pomeranians: Jana's puppies are Pomeranians.Girls: Jana's puppies are girls.Boys: Jana's puppies are boys.Let's look at the information we have, and see what we know for sure.Jana has 6 puppies.Jana's puppies are all Pomeranians.</p>
<p>Table 7 :
7
What are our bodies doing after having food?Answer Choices: (A) falling down (B) digesting (C) gas (D) weight gain (E) not hungry A: Let's think step by step.After having food, our bodies are digesting it.Digestion is the process by which food is broken down into simpler substances.Therefore, among A through E, the answer is B. ✓ Q: The wood was still rough to the touch, what did the woodworker have to do? Answer Choices: (A) petrify (B) sanded (C) warp (D) composted (E) clean A: Let's think step by step.First, the woodworker had to petrify the rough wood.Then, the woodworker had to sand the petrified wood.Finally, the woodworker had to warp the sanded wood.Therefore, among A through E, the answer is B. ✓ What do people aim to do at work? Answer Choices: (A) complete job (B) learn from each other (C) kill animals (D) wear hats (E) talk to each other A: Let's think step by step.People aim to do their jobs.Therefore, among A through E, the answer is A. ✓ Strategy QA Q: Has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?A: Let's think step by step.The Indian Ocean garbage patch is approximately 10% of the size of the North Pacific garbage patch.This means that even if the Indian Ocean garbage patch has completed full rotations of debris, it will still only be 1/10th of the size of the North Pacific garbage patch.Therefore, the answer (Yes or No) is no.×Q:Was Christina Aguilera born in the forgotten borough?A: Let's think step by step. 1. Christina 2. Aguilera 3. was 4. born 5. in 6. the 7. forgotten 8. Therefore, the answer (Yes or No) is yes.×Q:Karachi was a part of Alexander the Great's success?A: Let's think step by step. 1. Alexander the Great had a huge army.2.Karachi was a part of the empire of Alexander the Great.3.Therefore, Alexander the Great had a huge army.4.Therefore, Karachi was a part of Alexander the Great's success.Therefore, the answer (Yes or No) is no.✓Q:Was Jean Valjean imprisoned due to hunger?A: Let's think step by step.yes 1. Jean Valjean was imprisoned for stealing bread.2.He was imprisoned for stealing bread because he was hungry.3.Therefore, the answer (Yes or No) is yes.Therefore, the answer (Yes or No) is yes.✓Annotatedexamples of generated demonstrations selected by COSP on GPT-3.Similar to Table6, reasoning steps are highlighted: correct, questionable/incomplete/repetitive/irrelevant and erroneous, and the ground-truth correctness of the demonstrations (unavailable to the LLM) marked by ✓ or × at the end of the responses.
×</p>
<p>Table 8 :
8
Therefore, the answer (arabic numerals) is 100.✓CSQA Q: Where would someone purchase an upright piano?Answer Choices: (A) music class (B) college (C) music store (D) music room (E) music band A: Let's think step by step.You can't buy a piano in a music class, college, or room.And an upright piano is not in a music band.The correct answer is a music store.Therefore, among A through E, the answer is C. ✓ Q: John got his tax refund back.He treated it like it was what?Answer Choices: (A) candy (B) death and (C) free money (D) discount (E) credit A: Let's think step by step.The first thing John did when he got his tax refund was to spend it on his girlfriend.The second thing he did was to spend it on a big-screen TV.The correct answer is C. Therefore, among A through E, the answer is C. ✓ Q: How are the conditions for someone who is living in a homeless shelter?Answer Choices: (A) sometimes bad (B) happy (C) respiration (D) growing older (E) death A: Let's think step by step.The answer is choice A because the passage describes that homeless people have poor health.Therefore, among A through E, the answer is A. ✓ Annotated examples of generated demonstrations selected by COSP on PaLM-540B.See explanations of the annotations in Table 7 C2.Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?Section 5 &amp; Appendix B C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?Section 5 &amp; Appendix C. C4.If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?Section 5 &amp; Appendix B D Did you use human annotators (e.g., crowdworkers) or research with human participants?D1.Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?Not applicable.Left blank.D2.Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?Not applicable.Left blank.
Left blank.
Consistency-based Self-adaptive Prompting (COSP)As discussed in the Introduction, our goal is to develop a method that improves zero-shot reasoning abilities of LLMs and reduces the need for human effort by automatically selecting in-context demonstrations from the LLM's own generated outputs. To effectively achieve this, we propose COSP (overall procedure shown in Fig.2and Algorithm 1).
* Repetitiveness is undefined when Q = 1. In this case, we assign the minimum self-repetition score over all demonstrations where it is defined.
ACL 2023 Responsible NLP ChecklistA For every submission: A1.Did you describe the limitations of your work?Section 7 is dedicated for discussions of limitations.We also include Failure Analysis in Appendix C.5.A2. Did you discuss any potential risks of your work?We discuss potential ethical and social risks in Section 7. B3.Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?Appendix B B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?Not applicable.Left blank.B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 5 and Appendix B. We gave high-level information about these assets: e.g.arithmetic reasoning, logical reasoning, etc. B6.Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.C1.Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?Section 5 &amp; Appendix BThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Ensemble selection from libraries of models. Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, Alex Ksikes, Proceedings of the twentyfirst international conference on Machine learning. the twentyfirst international conference on Machine learning200418</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>RLPrompt: Optimizing discrete text prompts with reinforcement learning. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, Zhiting Hu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Semisupervised learning by entropy minimization. Advances in neural information processing systems. Yves Grandvalet, Yoshua Bengio, 200417</p>
<p>Unnatural instructions: Tuning language models with (almost) no human labor. Or Honovich, Thomas Scialom, Omer Levy, Timo Schick, arXiv:2212.096892022arXiv preprint</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/D14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>OpenNMT: Opensource toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander Rush, Proceedings of ACL 2017, System Demonstrations. ACL 2017, System DemonstrationsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems (NeurIPS). 202235</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , 10.1162/tacl_a_00160Transactions of the Association for Computational Linguistics. 32015</p>
<p>Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 20133896</p>
<p>Self-prompting large language models for opendomain qa. Junlong Li, Zhuosheng Zhang, Hai Zhao, arXiv:2212.086352022aarXiv preprint</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022barXiv preprint</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. Aman Madaan, Amir Yazdanbakhsh, arXiv:2209.076862022arXiv preprint</p>
<p>Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, Yinfei Yang, 10.18653/v1/2022.findings-acl.146Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304A deep reinforced model for abstractive summarization. 2017arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, </p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, Mubarak Shah, arXiv:2101.063292021arXiv preprint</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Chatgpt: Optimizing language models for dialogue. Schulman, Zoph, Kim, Hilton, Menick, Weng, Uribe, Fedus, Metz, Pokorny, 2022</p>
<p>Transductive semi-supervised deep learning using min-max features. Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng Maxiaoyu Tao, Nanning Zheng, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, arXiv:2209.01975Selective annotation makes language models better fewshot learners. 2022arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Learning values across many orders of magnitude. Arthur Hado P Van Hasselt, Matteo Guez, Volodymyr Hessel, David Mnih, Silver, Advances in neural information processing systems. 201629</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 201932</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 3503 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 3503 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022aarXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022barXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, Advances in Neural Information Processing Systems (NeurIPS). 2022b</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, arXiv:2112.04359Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. Borja BallearXiv preprint</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Zero-shot learning-the good, the bad and the ugly. Yongqin Xian, Bernt Schiele, Zeynep Akata, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, React: Synergizing reasoning and acting in language models. International Conference on Learning Representations. 2023</p>
<p>Star: Self-taught reasoner bootstrapping reasoning with reasoning. Eric Zelikman, Jesse Mu, Yuhuai Tony Noah D Goodman, Wu, Advances in neural information processing systems. 2022</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
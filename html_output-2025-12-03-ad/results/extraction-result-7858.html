<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7858 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7858</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7858</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-270285708</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.04244v1.pdf" target="_blank">Benchmark Data Contamination of Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7858.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7858.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreeEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark-free LLM-as-judge method that uses a tree-planning strategy to generate evaluation questions dynamically and run an irreproducible evaluation session with a high-performance LLM; reported to correlate well with an existing automatic evaluator (AlpacaEval2.0) in the surveyed work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Benchmark Data Contamination of Large Language Models: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General LLM evaluation / instruction-following evaluation (benchmark-free assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Tree-generated dynamic question sets (tree-planned topics); compared against AlpacaEval2.0</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>High-performance LLM(s) used by TreeEval (six models of varying sizes were tested)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Survey reports TreeEval was tested with six models of varying sizes; TreeEval uses an (unnamed) high-performance LLM to run irreproducible evaluation sessions via tree planning</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>N/A (comparison reported is to AlpacaEval2.0 automatic evaluator rather than a human study in this survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation (unspecified; reported as "high correlation" with AlpacaEval2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>susceptibility to contamination if judge LLM was trained on contaminated/biased data; potential evaluator bias inherited from judge's training data</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TreeEval can produce robust automatic evaluations and reportedly correlates highly with AlpacaEval2.0 given ~45 questions; however, the survey cautions that LLM-based judges still inherit risks from their pretraining (contamination and bias).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can perform irreproducible sessions to reduce leakage risk; scalable and automated; enables benchmark-free, dynamic evaluation without releasing static test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Tree-planning prompt strategy to generate evaluation trees; survey reports ~45 questions used to achieve high correlation with AlpacaEval2.0; tested across six models of varying sizes (details reported in Li et al. [87]).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmark Data Contamination of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7858.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7858.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge (survey comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative observations of LLM-as-judge evaluations vs human evaluations (survey-level summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-level synthesis comparing LLMs used as automated judges to human evaluators: highlights advantages (scalability, speed, potential to avoid static-benchmark leakage) and limitations (training-data contamination, bias, lack of human nuance, subjectivity differences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Benchmark Data Contamination of Large Language Models: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multiple downstream tasks (dialogue, instruction following, summarization, code generation) as discussed across cited works</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Various (survey cites Chatbot Arena human votes, TreeEval dynamic sets, and other benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various LLM judges referenced (e.g., ChatGPT/GPT-4 and other commercial/open models cited in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Judge models are drawn from commercially available and research LLMs; the survey emphasizes that their pretraining makes them vulnerable to benchmark data contamination and inherit dataset biases</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdsourced human preferences and human judges (examples include Chatbot Arena's crowdsourced voters; human evaluations following principles like the 3H rule)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>subjectivity and background biases in human judges; LLM-judges: contamination-driven overconfidence, inherited biases from pretraining, reduced sensitivity to contamination-induced artifacts, potential unfairness</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Humans provide nuanced, context-sensitive judgments but are costly and subjective; LLM-as-judge is cheaper and scalable but can reproduce training-set biases, be contaminated by benchmark data, and may not match human nuance or domain expertise across all criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Speed, cost-effectiveness, scalability, ease of automation, ability to run irreproducible sessions (claimed mitigation against leakage)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Survey-level descriptions: human evaluation examples include Chatbot Arena pairwise comparisons with ≈240K votes; LLM-as-judge examples include tree-planning (TreeEval) or automatic evaluators (AlpacaEval family); no single uniform protocol in survey — settings vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmark Data Contamination of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7858.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7858.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlpacaEval / AlpacaEval2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlpacaEval (and AlpacaEval2.0): Automatic Evaluator(s) for Instruction-following Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic LLM-based evaluation frameworks (AlpacaEval family) used as baselines/comparators in LLM-as-judge research; cited in the survey as a comparison target for TreeEval and as an example of automatic evaluators that require debiasing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AlpacaEval: An Automatic Evaluator of Instruction-following Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Benchmark Data Contamination of Large Language Models: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following / preference-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Not a single dataset; AlpacaEval is used to evaluate instruction-following model outputs (survey reports TreeEval correlation with AlpacaEval2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>AlpacaEval (automatic evaluator framework / evaluator model)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Automatic evaluator that simulates human preference judgments; later variants and debiasing methods (e.g., length-corrected AlpacaEval) are referenced in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>N/A (AlpacaEval is an automatic evaluator used as proxy for human preference in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Automatic evaluators can be biased (e.g., length bias), require debiasing corrections, and may not perfectly reflect human judgments without calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Automatic evaluators like AlpacaEval are useful for scalable evaluation and have been compared (and correlated) with other LLM-judge methods, but they require careful debiasing and validation versus human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Automated, reproducible, fast, and scalable evaluation for instruction-following outputs; less human labor required.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Referenced as a comparator in TreeEval experiments (TreeEval reported "high correlation" with AlpacaEval2.0 using ~45 questions); other AlpacaEval studies introduce debiasing (e.g., length-correction) to better align with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmark Data Contamination of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning <em>(Rating: 2)</em></li>
                <li>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference <em>(Rating: 2)</em></li>
                <li>FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An Automatic Evaluator of Instruction-following Models <em>(Rating: 2)</em></li>
                <li>Length-Corrected AlpacaEval: A Simple Debiasing of Automatic Evaluators <em>(Rating: 2)</em></li>
                <li>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Large Language Models are not Fair Evaluators <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7858",
    "paper_id": "paper-270285708",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "TreeEval",
            "name_full": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "brief_description": "A benchmark-free LLM-as-judge method that uses a tree-planning strategy to generate evaluation questions dynamically and run an irreproducible evaluation session with a high-performance LLM; reported to correlate well with an existing automatic evaluator (AlpacaEval2.0) in the surveyed work.",
            "citation_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "mention_or_use": "mention",
            "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "evaluation_task": "General LLM evaluation / instruction-following evaluation (benchmark-free assessment)",
            "dataset_name": "Tree-generated dynamic question sets (tree-planned topics); compared against AlpacaEval2.0",
            "judge_model_name": "High-performance LLM(s) used by TreeEval (six models of varying sizes were tested)",
            "judge_model_details": "Survey reports TreeEval was tested with six models of varying sizes; TreeEval uses an (unnamed) high-performance LLM to run irreproducible evaluation sessions via tree planning",
            "human_evaluator_type": "N/A (comparison reported is to AlpacaEval2.0 automatic evaluator rather than a human study in this survey summary)",
            "agreement_metric": "correlation (unspecified; reported as \"high correlation\" with AlpacaEval2.0)",
            "agreement_score": null,
            "reported_loss_aspects": "susceptibility to contamination if judge LLM was trained on contaminated/biased data; potential evaluator bias inherited from judge's training data",
            "qualitative_findings": "TreeEval can produce robust automatic evaluations and reportedly correlates highly with AlpacaEval2.0 given ~45 questions; however, the survey cautions that LLM-based judges still inherit risks from their pretraining (contamination and bias).",
            "advantages_of_llm_judge": "Can perform irreproducible sessions to reduce leakage risk; scalable and automated; enables benchmark-free, dynamic evaluation without releasing static test sets.",
            "experimental_setting": "Tree-planning prompt strategy to generate evaluation trees; survey reports ~45 questions used to achieve high correlation with AlpacaEval2.0; tested across six models of varying sizes (details reported in Li et al. [87]).",
            "uuid": "e7858.0",
            "source_info": {
                "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-as-judge (survey comparison)",
            "name_full": "Comparative observations of LLM-as-judge evaluations vs human evaluations (survey-level summary)",
            "brief_description": "Survey-level synthesis comparing LLMs used as automated judges to human evaluators: highlights advantages (scalability, speed, potential to avoid static-benchmark leakage) and limitations (training-data contamination, bias, lack of human nuance, subjectivity differences).",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "evaluation_task": "Multiple downstream tasks (dialogue, instruction following, summarization, code generation) as discussed across cited works",
            "dataset_name": "Various (survey cites Chatbot Arena human votes, TreeEval dynamic sets, and other benchmarks)",
            "judge_model_name": "Various LLM judges referenced (e.g., ChatGPT/GPT-4 and other commercial/open models cited in the survey)",
            "judge_model_details": "Judge models are drawn from commercially available and research LLMs; the survey emphasizes that their pretraining makes them vulnerable to benchmark data contamination and inherit dataset biases",
            "human_evaluator_type": "Crowdsourced human preferences and human judges (examples include Chatbot Arena's crowdsourced voters; human evaluations following principles like the 3H rule)",
            "agreement_metric": "",
            "agreement_score": null,
            "reported_loss_aspects": "subjectivity and background biases in human judges; LLM-judges: contamination-driven overconfidence, inherited biases from pretraining, reduced sensitivity to contamination-induced artifacts, potential unfairness",
            "qualitative_findings": "Humans provide nuanced, context-sensitive judgments but are costly and subjective; LLM-as-judge is cheaper and scalable but can reproduce training-set biases, be contaminated by benchmark data, and may not match human nuance or domain expertise across all criteria.",
            "advantages_of_llm_judge": "Speed, cost-effectiveness, scalability, ease of automation, ability to run irreproducible sessions (claimed mitigation against leakage)",
            "experimental_setting": "Survey-level descriptions: human evaluation examples include Chatbot Arena pairwise comparisons with ≈240K votes; LLM-as-judge examples include tree-planning (TreeEval) or automatic evaluators (AlpacaEval family); no single uniform protocol in survey — settings vary by cited work.",
            "uuid": "e7858.1",
            "source_info": {
                "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AlpacaEval / AlpacaEval2.0",
            "name_full": "AlpacaEval (and AlpacaEval2.0): Automatic Evaluator(s) for Instruction-following Models",
            "brief_description": "Automatic LLM-based evaluation frameworks (AlpacaEval family) used as baselines/comparators in LLM-as-judge research; cited in the survey as a comparison target for TreeEval and as an example of automatic evaluators that require debiasing.",
            "citation_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "mention_or_use": "mention",
            "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "evaluation_task": "Instruction-following / preference-based evaluation",
            "dataset_name": "Not a single dataset; AlpacaEval is used to evaluate instruction-following model outputs (survey reports TreeEval correlation with AlpacaEval2.0)",
            "judge_model_name": "AlpacaEval (automatic evaluator framework / evaluator model)",
            "judge_model_details": "Automatic evaluator that simulates human preference judgments; later variants and debiasing methods (e.g., length-corrected AlpacaEval) are referenced in the survey",
            "human_evaluator_type": "N/A (AlpacaEval is an automatic evaluator used as proxy for human preference in cited works)",
            "agreement_metric": "",
            "agreement_score": null,
            "reported_loss_aspects": "Automatic evaluators can be biased (e.g., length bias), require debiasing corrections, and may not perfectly reflect human judgments without calibration.",
            "qualitative_findings": "Automatic evaluators like AlpacaEval are useful for scalable evaluation and have been compared (and correlated) with other LLM-judge methods, but they require careful debiasing and validation versus human judgments.",
            "advantages_of_llm_judge": "Automated, reproducible, fast, and scalable evaluation for instruction-following outputs; less human labor required.",
            "experimental_setting": "Referenced as a comparator in TreeEval experiments (TreeEval reported \"high correlation\" with AlpacaEval2.0 using ~45 questions); other AlpacaEval studies introduce debiasing (e.g., length-correction) to better align with human judgments.",
            "uuid": "e7858.2",
            "source_info": {
                "paper_title": "Benchmark Data Contamination of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "rating": 2,
            "sanitized_title": "treeeval_benchmarkfree_evaluation_of_large_language_models_through_tree_planning"
        },
        {
            "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "rating": 2,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
            "rating": 2,
            "sanitized_title": "freeeval_a_modular_framework_for_trustworthy_and_efficient_evaluation_of_large_language_models"
        },
        {
            "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Length-Corrected AlpacaEval: A Simple Debiasing of Automatic Evaluators",
            "rating": 2,
            "sanitized_title": "lengthcorrected_alpacaeval_a_simple_debiasing_of_automatic_evaluators"
        },
        {
            "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
            "rating": 2,
            "sanitized_title": "an_empirical_study_of_llmasajudge_for_llm_evaluation_finetuned_judge_models_are_taskspecific_classifiers"
        },
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Large Language Models are not Fair Evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        }
    ],
    "cost": 0.01875675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmark Data Contamination of Large Language Models: A Survey
6 Jun 2024</p>
<p>Cheng Xu cheng.xu1@ucdconnect.ie 
University College Dublin
Ireland</p>
<p>SHUHAO GUAN
University College Dublin
Ireland</p>
<p>DEREK GREENE
University College Dublin
Ireland</p>
<p>M-TAHAR KECHADI
University College Dublin
Ireland</p>
<p>School of Computer Science
University College Dublin
Dublin 4BelfieldIreland</p>
<p>Benchmark Data Contamination of Large Language Models: A Survey
6 Jun 202469F9A3B7BAC205A2B5FA6D8EAF42C8BBarXiv:2406.04244v1[cs.CL]CCS Concepts:Computing methodologies → Natural language generation; Language resources LLMsdata contaminationbenchmarkevaluationlabel leakage
The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing.However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC).This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process.This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks.The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.</p>
<p>INTRODUCTION</p>
<p>The field of natural language processing (NLP) has undergone a significant transformation in recent years, thanks to the rapid advancement of Large Language Models (LLMs) like GPT-4 [107], Claude-3 [4], and Gemini [137].These models, built on deep learning architectures such as Transformers [142], have revolutionized various domains, including content generation, summarization, machine translation, and question-answering.By demonstrating remarkable capabilities in understanding and generating human-like text, they have gained widespread interest and acceptance in both academia and industry.</p>
<p>Amid the excitement surrounding the progress of LLMs, a critical issue has emerged: Benchmark Data Contamination (BDC).This refers to the phenomenon where language models incorporate information related to the evaluation benchmark from their training data, leading to skewed or unreliable performance during the evaluation phase.The challenge at hand involves both the evaluation process of LLMs and their privacy and security considerations [17,18,53,60,73].While some studies see this phenomenon as beneficial [12] or do not consider it to be a problem [16], the majority of studies in the academic community agree that BDC poses significant challenges to the reliability and validity of LLM evaluations, undermining trust in their outputs and hindering their real-world applications [69,83,98,119,126,178].</p>
<p>Traditional evaluation methodologies for LLMs often rely on benchmark datasets as gold standards for measuring model performance.Although these benchmarks are crucial for evaluating, validating, and comparing different models, they are not immune to the issue of BDC.With the rise of AI-generated content (AIGC), this issue is becoming more complex and difficult to detect.The datasets used for training and fine-tuning LLMs may contain benchmark-related information, such as metadata, label distributions, and contextual data, which can inadvertently impact the models' behavior and evaluation performance.Therefore, assessments based on traditional benchmarks may not accurately represent the true capabilities of LLMs and can lead to misguided conclusions about their performance.</p>
<p>In response to the widespread challenges around BDC, researchers have started to explore alternative assessment methods to reduce the risks associated with traditional benchmarks.Some promising approaches have been proposed, such as regenerating benchmark data [158,180,181], which mitigates BDC by reconstructing the original benchmarks using LLMs, and benchmark-free evaluation [24,87,166], which tries to avoid relying on predefined benchmarks altogether.These approaches aim to evaluate LLMs in a more flexible, adaptive, and reliable manner.</p>
<p>Along with the rapid development of LLMs, the issue of BDC has become increasingly important and observed in the research community.However, there is currently no comprehensive and systematic research that thoroughly discusses and defines this problem.This paper aims to fill this gap by providing a comprehensive survey on BDC in LLMs.In this survey, we define the BDC problem and organize the existing research into two main categories: Detection Techniques and Mitigation Strategies.The first category focuses on how to identify and detect BDC risks, while the second category focuses on mitigating the BDC problem in the current evaluation process of LLMs.By conducting this survey, we provide a comprehensive understanding of BDC in LLMs and offer insights into the detection and mitigation of this critical issue.</p>
<p>This paper is organized as follows.Section 2 provides relevant background information about LLMs, and we define and discuss the BDC problem and provide some examples.Sections 3 and 4 comprehensively review existing methods for detecting BDC during the evaluation of LLMs and strategies for mitigating BDC risks, respectively.The detection methods are divided into two subcategories: Matching-based and Comparison-based methods.The mitigation strategies are further divided into three subcategories: Curating New Data, Refactoring Existing Data, and Benchmark-free Evaluation.Within each category, key approaches are discussed.Subsequently, Section 5 examines the challenges and future directions for mitigating BDC risks, acknowledging the inherent complexities and trade-offs involved in developing robust evaluation strategies for LLMs.</p>
<p>BACKGROUND</p>
<p>In this section, we provide an in-depth review of LLMs and BDC.Initially, we explore the current state of research on LLMs in Section 2.1.We then elucidate the concept of BDCs and provide a formal definition in Section 2.2.In Section 2.3, we investigate the origins of BDC issues and their potential implications.Lastly, in Section 2.4, we identify a selection of critical tasks that are susceptible to the effects of BDC.</p>
<p>Large Language Models</p>
<p>An LLM is a language model notable for its ability to achieve general-purpose language understanding and generation.Such models have evolved significantly, leveraging advancements like Transformers and self-attention, which have enabled them to process longer sequences effectively.</p>
<p>Examples of LLMs include GPT [15,107,116], PaLM [25], and LLaMA [140].They serve as the backbone of many NLP applications, such as text generation, translation, question answering, and summarization.Research on LLMs has advanced in both academia and industry, and remarkable progress has been made with the launch of ChatGPT [106], which has attracted widespread attention.</p>
<p>Earlier LLMs typically made use of encoder-decoder or encoder-only architectures, which are effective for various NLP applications.For instance, models like BERT (encoder-only) [31] and T5 (encoder-decoder) [117] have shown strong performance in tasks such as text classification and machine translation.However, the most advanced and largest LLMs use a decoder-only transformerbased architecture [120], while some recent variations are based on other architectures, such as recurrent neural network variants and Mamba [49].The later LLM architectures [120] can use unlabeled data for unsupervised pre-training and exhibit better generalization across different tasks compared to encoder-decoder and encoder-only architectures [148].There are four key hyperparameters that characterize an LLM: the cost of pre-training, the model size, the dataset size, and the performance after pre-training.</p>
<p>The performance after (pre-)training is influenced by three other hyper-parameters: 1) model size, 2) training data size, and 3) cost of training.Consequently, some studies have explored the relationship between these hyper-parameters and the model's cross-entropy loss to understand how these factors affect model efficiency and performance.The KM scaling law [74] focuses on the benefits of increasing model size and dataset size, while the Chinchilla scaling law [56] highlights the importance of balancing model size with the appropriate amount of data for optimal performance.Additional research also indicates that scaling can significantly enhance the capacity of LLMs [15,25].This improvement occurs because larger models can capture more complex patterns and relationships within the data.Additionally, increasing the amount of training data exposes the model to a broader range of information, further improving its generalization abilities and enabling it to handle diverse and challenging tasks more effectively.</p>
<p>LLMs primarily possess three basic abilities: language generation, knowledge utilization, and complex reasoning.Current mainstream LLMs perform language generation by proposing the next token based on previous tokens [10].LLMs can also generate specialized languages, such as programming code, via code synthesis [50].The ability of knowledge utilization refers to LLMs that can accomplish knowledge-intensive tasks through knowledge provided during pre-training and within prompts.This ability is primarily evaluated through question-answering (QA) tasks [71] and knowledge graph completion tasks [139].Complex reasoning refers to the ability to understand and use supporting evidence or logic to derive conclusions or make decisions [59].This can be assessed through tasks, such as knowledge reasoning [125] and symbolic reasoning [154].</p>
<p>LLMs also have some advanced abilities, such as interacting with external environments or user tools.Some studies have enabled LLMs to perform specific tasks like autonomous driving through external interfaces [22] or control characters in games to achieve specific goals [151].When solving complex problems, LLMs can use external tools if deemed necessary, such as a search engine [102], image generation models [11], and compilers [43].Such tools are used to enhance the performance of LLMs in various applications.These capabilities stem from LLMs' proficiency in understanding context, generating relevant output, and interacting with other systems through well-defined interfaces, thereby enhancing their performance in various applications.</p>
<p>Emergent abilities manifest primarily in three ways: in-context learning [33], instruction following [127], and step-by-step reasoning [113].In-context learning abilities were first observed in GPT-3 [15] The instruction following ability enables the model to comprehend and execute tasks based on directives provided directly within the input prompt.For example, when provided with a prompt that includes specific instructions, such as "summarize the following text" or "translate the following sentences into French, " the model can understand and act upon these instructions, leveraging its pre-existing knowledge base and generalization capabilities [153].</p>
<p>Step-by-step reasoning refers to the ability of models to break down complex problems or queries into smaller steps, processing each one sequentially to arrive at a final answer.This is often accomplished through chain-of-thought (CoT) prompting strategy [155], in this approach, the model sequentially processes each step, building upon prior steps to construct a comprehensive answer, and research has shown that chain-of-thought prompts can bring performance gains [155].</p>
<p>The abilities mentioned above enable LLMs to exhibit strong performance but also face several issues.A frequently-reported problem is the generation of so-called hallucinations [8], where LLMs generate text that superficially appears to be correct but is actually inaccurate.This problem is difficult to resolve completely, although it can be mitigated through alignment tuning strategies [109].While LLMs have learned general language patterns, they underperform in specialized domains, such as medicine or engineering.This may be related to catastrophic forgetting [76] or a scarcity of relevant training data.Furthermore, enabling LLMs to quickly learn the latest knowledge by updating weights remains an unresolved challenge [163].</p>
<p>Benchmark Data Contamination</p>
<p>Benchmark Data Contamination refers to a critical issue encountered during the training and evaluation of LLMs.It arises when an LLM inadvertently encounters test data (or benchmark data) during its training and fine-tuning process.This exposure can significantly impact the model's performance scores, leading to inflated results that do not accurately reflect its true capabilities.We formally define this phenomenon as follows.</p>
<p>Definition 1 (Benchmark Data Contamination).Exposure of a large language model to benchmark data during the training process leads to distorted evaluation results.</p>
<p>Depending on the severity of the contamination, we categorize the BDC problem into four types:</p>
<p>(1) Semantic Level: Exposure of identical and/or derivative content of the benchmark.Typically, the content pertains to the same topic or comes from the same source as the benchmark.This form of contamination introduces biases related to specific topics, affecting the model's generalization capabilities.(2) Information Level: Exposure to benchmark-related information leads to models with tendencies and biases during evaluation.Information such as metadata, time distributions, label distributions, and external reviews of the benchmark can inadvertently influence the model's evaluation process.(3) Data Level: The benchmark data exposure excludes labels.Examples include the data content of the test set and data sequences without associated labels.Data-level contamination affects the model's understanding of the underlying patterns and relationships within the data.(4) Label Level: The complete exposure of benchmark data, including labels.When labels are made available during training, the model may directly memorize them, leading to overfitting and compromised generalization.</p>
<p>As we move from the semantic level to the label level, the severity of BDC increases, posing greater challenges to the evaluation of models.However, the complexity of detecting and preventing contamination inversely correlates with the proximity to full exposure to the benchmark.While complete exposure at the label level facilitates relatively straightforward detection and prevention</p>
<p>Criteria Metrics Accuracy</p>
<p>Exact match, Quasi-exact match, F1 score, ROUGE score [168] Calibrations Expected calibration error [51], Area under the curve [44] Fairness Demographic parity difference [167], Equalized odds difference [54] Robustness Attack success rate [144], Performance drop rate [182] measures, the more abstract and intricate nature of contamination at the semantic and information levels renders it inherently more challenging to detect and mitigate.</p>
<p>Sources and Impact</p>
<p>The training processes of LLMs can be divided into two main types.Pre-training refers to the process of training language models on a large-scale corpus with the aim of equipping LLMs with general-purpose language comprehension, where the trained model is called Pre-trained Language Models (PLMs) [115].In contrast, fine-tuning refers to the targeted continuation of training for various downstream natural language processing tasks, in order to make the LLMs better able to carry out the downstream tasks.</p>
<p>The BDC problem stems from the inherent complexity and diversity of the pre-training data used to train LLMs in NLP tasks.Excluding deliberate human attacks, one of the primary sources of BDC is the composition of large-scale pre-training datasets themselves.These datasets are often compiled from a wide range of sources, including news articles, online forums, social media posts, and other publicly available text data.While this diversity is essential for training robust and generalizable models, it also introduces the risk of unexpected exposure to test data during the model training process.Such a risk, on the other hand, is much less in the fine-tuning process, which generally uses relatively small datasets for targeted training, and is a much more controlled process with more predictable results.</p>
<p>If BDC is introduced during the training phase, we need to consider how this might impact on the evaluation process of LLMs.Such evaluations typically involve three methods: traditional benchmark testing, automatic evaluation, and human evaluation.Each method has its own metrics and processes, which can be significantly affected by BDC.</p>
<p>In traditional benchmark testing, the model's performance is assessed by training or fine-tuned on a training set and then testing it on a separate test set.However, the presence of BDC can lead to overestimated performance metrics, as the model might inadvertently "learn" from test data that was leaked into the training set.This compromises the integrity of the evaluation, making it difficult to gauge the model's true capabilities.</p>
<p>Automatic evaluation uses algorithms and pre-defined metrics to assess LLMs, reducing the need for human labor and enabling faster, more standardized assessments.Key aspects of automatic evaluation include: Accuracy, Calibration, Fairness, Robustness, which are presented in Table 1.Accuracy measures the correctness of the model's outputs against a ground truth.Calibration assesses how well the model's confidence aligns with its accuracy.Fairness evaluates the bias in the model's outputs across different demographic groups.Robustness tests the model's resilience to adversarial attacks and perturbations.This method is also currently recognized as the most promising evaluation strategy.For example, Jain et al. [67] introduced a self-supervised method to streamline the evaluation of models in real-world deployments by eliminating the need for labeling new data.Additionally, in some studies, LLMs have been used as judge models to evaluate the performance of other LLMs [58,150].Automatic evaluation can mitigate some effects of BDC by employing self-supervised methods that reduce reliance on labeled data, but this approach cannot completely eliminate the risk of contamination, for example, the LLM used for automatic evaluation may also suffers from the BDC problem.The effects on the evaluation metrics are as follows:</p>
<p>• Accuracy: Directly impacted as the model might have already seen the test data, leading to inflated accuracy scores.• Calibration: Can be skewed if the model's confidence scores are based on contaminated data, giving a false sense of reliability.• Fairness: Potentially affected if the contaminated data introduces or reinforces biases that the model learns and propagates.• Robustness: Compromised because the model's resilience to unseen data and adversarial conditions is not accurately tested with contaminated data.</p>
<p>Human evaluation is considered to be the most rigorous and nuanced method, where one or more human judges assess the model's performance on various criteria [104].For example, the Chatbot Arena created by Chiang et al. [24] has gathered numerous human votes.Human evaluations often follow principles like the 3H rule (Helpfulness, Honesty, Harmlessness [5]) or specific criteria Chang et al. [21] such as accuracy [131], relevance [177], fluency [141], transparency [157], safety [68], and human alignment [110].While human evaluation can provide a more realistic assessment of an LLM's performance, it is also susceptible to biases from evaluators' backgrounds and experiences.BDC poses a unique challenge to human evaluation.While human judges might be able to recognize and mitigate some effects of data contamination, their subjective judgments can still be influenced by familiarity with the data.Moreover, if the human evaluators themselves are biased by prior exposure to the contaminated data, their assessments might not fully reflect the model's true performance on genuinely novel inputs.</p>
<p>In conclusion, the sources and impact of BDC are critical considerations in the training and evaluation of LLMs.BDC arises primarily from the diverse and extensive pre-training datasets, potentially leading to overestimated performance metrics in traditional benchmark testing and skewed results in both automatic and human evaluations.While automatic evaluation offers a promising approach to mitigate some BDC effects through standardized and self-supervised methods, it cannot fully eliminate the risk.Human evaluation, despite being the most nuanced and rigorous method, is also vulnerable to biases introduced by BDC.Therefore, addressing BDC is essential for ensuring the integrity and reliability of LLM assessments across all evaluation methods.</p>
<p>Related Tasks</p>
<p>To gain a clearer understanding of the prevalence of the BDC problem in NLP tasks, we systematically select seven common LLM tasks and delineate specific instances where BDC vulnerability can frequently manifest:</p>
<p>• Code Generation [23,52,66,97,124,158]: In code generation tasks, large-scale pre-training data may include code snippets and corresponding programming ideas from online forums or repositories about the benchmarks, which may lead to a high risk of contamination.For example, if content related to the benchmark is included in the pre-training process, while it is not difficult to mitigate BDC by directly filtering answers that match the test set, excluding semantic-level related content, such as problem-solving tutorials, is challenging.Thus the model may still be at risk of contamination, leading to distorted evaluation results.• Machine Translation [9,55,70,101,143,169,184]: In machine translation tasks, benchmarks are often composed of translated texts from various common sources, which can naturally lead to contamination.For example, news articles or official announcements are usually available in a variety of languages, and testing a model using benchmarks that cover that topic can make it biased towards certain topics or narrative structures, leading to an inaccurate assessment of the model's translation capabilities.• Question Answering [38,77,84,93,110,111,122,123,132,136]: Benchmarks for QA tasks usually contain question and answer pairs.However, if relevant content, like discussions on Github Issues1 , is introduced during the pre-training process, models trained on such data may have difficulty recognizing correct answers, resulting in inflated performance scores that do not reflect the true functionality of the model.• Sentiment Analysis [1,2,27,99,152,161,[170][171][172][173]: In sentiment analysis tasks, common benchmarks consist of text samples labeled with sentiment under a certain topic.If the contextual information of the topic is contained in the pre-training data, it gives the model subjective biases and tendencies, which can lead to distorted results in sentiment prediction evaluation.For example, in the sentiment prediction task about COVID, if the background information of this topic is present in the pre-training information, it will let the model know in advance that it is a worldwide infection, resulting in a pre-determined distribution of predominantly negative sentiment labels from the model.• Named Entity Recognition [13,28,39,63,92,94,103,134,138,147]: Benchmark for the NER task consist of text annotated with named entities (e.g., names of people, organizations, and locations).However, if the pre-training material contains content related to these entities, it can let the model learn about the prior background knowledge, which will lead to a distorted evaluation of the entity recognition performance.• Fake News Detection [3,12,40,57,112,130,145,149,160,179]: Articles and comments associated with news events that constitute a benchmark for the fake news detection task might be used as pre-training data, leading to a risk of BDC.An event is usually covered by more than one media outlet, and different media outlets may have different positions and languages.Such large-scale relevant information for model training may lead to multiple BDC problems, from the semantic level to the label level.• Text Reconstruction [20,42,75,100,105,133,159,175]: The benchmark for text reconstruction tasks generally consists of incomplete or fragmented text passages.If the pre-training dataset contains complete benchmark texts, e.g., the original text in an antiquarian book restoration task is already present in the pre-training data, this can lead to serious distortions in the evaluation results.</p>
<p>From the above, we see that BDC poses a significant challenge in the training and evaluation of LLMs in many different contexts.In each case, contamination can potentially lead to distorted performance scores that do not accurately reflect the model's true capabilities.The severity of BDC, as categorized into Semantic Level, Information Level, Data Level, and Label Level, increases as we move closer to full exposure of the benchmark data.The complexity of detecting and mitigating BDC inversely correlates with the severity of exposure, making it a challenging problem to address.The primary sources of BDC are the large-scale pre-training datasets used in training LLMs, which due to their diversity and complexity, can inadvertently introduce the risk of BDC.Highlighting the potential scenarios of BDC occurrence across seven prevalent LLM tasks underscores the critical necessity of addressing this issue for precise model evaluation and performance enhancement in the domain of NLP. 3 BDC DETECTION TECHNIQUES Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively.Note that certain investigations incorporate elements from both strategies.Such instances are allocated to the category deemed more comprehensive or preferred by the authors in question.All reviewed work on BDC detection is summarized in Table 2.</p>
<p>Matching-based Methods</p>
<p>These methods focus on detecting BDC by examining the overlap and inclusion of pre-training data in the evaluation datasets.This typically involves dataset inspection, membership inference, and example generation.We now discuss seven representative works in this area.</p>
<p>The predominant decontamination technique in NLP is n-gram overlap.Specifically, the work by Brown et al. [15] associated with GPT-3 defines a 13-gram overlap as being indicative of contamination.In contrast, the more recent GPT-4 model [107] identifies a 50-character overlap as a contamination signal.N-gram overlap detection is favored for its simplicity and computational efficiency.However, it is essential to recognize that this approach may yield a higher false negative rate when dealing with subtle differences in text segments.</p>
<p>Li and Flanigan [85] investigated the phenomenon of task contamination in LLMs, like GPT-3, which may compromise their zero-shot and few-shot learning capabilities.The study revealed that LLMs perform better on datasets released before their training data creation date, suggesting the presence of contamination.The authors employed four methods: training data inspection (Find examples of task training by inspecting the training data), task example extraction (Extraction of task data from existing models), membership inference (Check that the model-generated content of the input instance is identical to the original dataset.), and chronological analysis (Measuring performance on datasets with known release dates and checking the evidence of contamination using chronological evidence) to provide evidence of this contamination.The paper also finds that for classification tasks without task contamination, LLMs show no significant improvement over simple majority baselines.</p>
<p>Ranaldi et al. [118] introduced another method for detecting BDC in GPT models.They assessed GPT-3.5'sperformance using the well-known Spider Dataset [165] and a novel dataset called Termite.Additionally, they employed an adversarial table disconnection (ATD) approach, which complicates Text-to-SQL tasks by removing structural pieces of information from the database.This method allowed them to analyze GPT-3.5'sefficacy on databases with modified information and assess the impact of BDC on the model's performance.</p>
<p>Pretrain Corpus</p>
<p>The Pile Similarly, as shown in Figure 1, Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps.Their findings indicated that commercial LLMs, including ChatGPT and GPT-4, can guess missing options in benchmark tests with a high level of accuracy.</p>
<p>Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training.Tested on GPT-3.5/4, the DCQ demonstrated higher contamination , Vol. 1, No. 1, Article .Publication date: June 2024.levels than other methods and effectively bypassed safety filters designed to prevent the generation of copyrighted content.</p>
<p>Golchin and Surdeanu [47] presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection.The approach achieved high accuracy rates, between 92% and 100%, across seven datasets.The authors also found specific cases of contamination in popular datasets, such as AG News, WNLI, and XSum, when tested with GPT-4.</p>
<p>Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models.Their research uncovered varying degrees of contamination, ranging from 1% to 45%, and demonstrated that contamination does not always correlate with improved model performance.Interestingly, larger models may benefit more from contaminated test sets than smaller ones, with significant accuracy boosts observed on certain benchmarks.</p>
<p>Similar findings have been made in the field of historical book archaeology, Chang et al. [20] employed a membership inference query method called name cloze to deduce the books known to these models.By removing the names from the work and retaining only the contextual information, the model is made to fill in the names in the form of cloze test.Based on the context alone, it should be almost impossible for the model to fill in the correct name, which requires not knowledge of English but specific knowledge of the work.The authors use this detection method in order to detect the severity of the BDC problem.Their findings reveal that the degree of memorization correlates with the frequency of book passages appearing on the web.This memorization affects the validity of cultural analytics assessments, as models perform better on memorized books than non-memorized ones in downstream tasks.</p>
<p>In this section, we have reviewed matching-based strategies employed to identify BDC within benchmarks.These strategies aim to uncover direct evidence, such as identifying matches between training data or LLMs-generated content, and the evaluation dataset.Common methods include inspecting the training set, generating LLMs' content related to the evaluation dataset for membership inference, and adjusting prompts to align with the evaluation dataset's content.This strategy offers the advantage of intuitively detecting BDC, enabling the development of mitigation strategies based on these detection techniques.However, it does have drawbacks.For instance, accessing the training set for data inspection is often impractical for commercially proprietary LLMs like GPT-4 [107] and Claude-3 [4].Additionally, the computational requirements and expertise needed for data matching pose challenges to widespread adoption, especially in resource-constrained settings.Notably, there are studies questioning the effectiveness of this detection scheme.Yang et al. [162], Ippolito et al. [62], and Jiang et al. [69] criticize the use of string matching methods like n-gram overlap for decontamination, demonstrating that simple test data variations such as paraphrasing can bypass these measures.They show that a 13B model can overfit a test benchmark and achieve high performance comparable to GPT-4 when such test data variations are not eliminated.Similar findings were reported by Dekoninck et al. [29], who developed a technique called Evasive Augmentation Learning (EAL).This method involves rephrasing benchmark samples during the fine-tuning stage to evade detection by current contamination detection methods.They categorized model providers and contamination detection methods, uncovering vulnerabilities that EAL exploits.The technique proved highly effective, allowing significant improvements in benchmark performance (up to 15%) while remaining undetected by existing contamination detection methods.comparing the similarity [82,96], distribution [34], perplexity [89], and generation order [108] of the generated content with that of the evaluated dataset.Additionally, comparing the performance differences of LLMs on datasets across different time periods can serve as a comparison-based method for detecting BDC [61,121].We have identified six representative works that adopt this approach, which we have categorized into three subcategories: content comparison, sequential analysis, and chronological analysis.We discuss these below.</p>
<p>Comparison-based Methods</p>
<p>Magar and Schwartz [96] presented a method to detect contaminated data in downstream tasks, they tested pre-training BERT models on corpora that include Wikipedia and labeled downstream datasets, then fine-tuning them on relevant tasks, and then detected BDC by comparing the performance of model-generated content from "seen" and "unseen" evaluation datasets.Their experiments reveal that while some models do exploit contaminated data, others merely memorize them without exploitation.The study shows that the level of memorization and exploitation is influenced by factors such as the number of data duplications and model size.</p>
<p>Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC.They also introduce TED (Trustworthy Evaluation via output Distribution), which corrects the output distribution to mitigate the effects of contamination.Through extensive experiments, they demonstrate that CDD can significantly improve contamination detection over existing methods and TED can reduce performance inflation due to contamination.The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods.Their findings reveal that popular models like ChatGPT are susceptible to BDC, emphasizing the need for more reliable evaluation methods.</p>
<p>Different from the distribution, Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination, providing evidence of significant memorization in recent foundation models across various benchmarks.The study reveals that while reading comprehension and summarisation benchmarks show signs of contamination, multiple-choice benchmarks appear less affected.This method allows for a more accessible and less computationally intensive way to audit language models for contamination, ensuring more reliable evaluations.</p>
<p>An alternative interesting perspective is to focus on the order of content generated by LLMs.Oren et al. [108] presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version.Their findings suggest that it is possible to provide provable guarantees of test set contamination, which is significant for models trained on vast internet data.They successfully applied this test to audit popular language models and found minimal evidence of widespread contamination.</p>
<p>It is also possible to examine differences in performance based on data over time.Huang et al. [61] explored the reasoning capabilities of LLMs by using competition-level programming problems from Codeforces 2 .The study provided a comprehensive evaluation of GPT-4's performance on these problems, considering aspects such as release time, difficulty, and error types.The results showed a significant decline in GPT-4's performance on problems released after September 2021, indicating potential BDC and the challenges LLMs face with complex reasoning tasks.Despite exploring various approaches, like fine-tuning and Chain-of-Thought prompting, none consistently mitigated these challenges.The study underscores the value of competition-level problems as a resource for assessing LLMs' reasoning abilities and encourages the development of models with Table 3.An overview of some of the main strategies for mitigating data contamination.We list their categories, as well as the corresponding short description and some representative references.</p>
<p>Type</p>
<p>Content Filtering</p>
<p>Improving data reliability by identifying and removing contaminated elements.[32,62] Benchmark-free LLM-as-judge LLMs evaluate themselves without relying on traditional benchmarks.[87,166] Human Participation</p>
<p>Human participant methods involve leveraging human evaluators to assess LLMs performance.</p>
<p>[ 24,166] stronger reasoning skills and better generalization, and the study by Yang et al. [162] had a similar suggested scheme.Similarly, Roberts et al. [121] focused on two code/mathematical problem-solving datasets, Codeforces and Project Euler.They found significant trends that suggest contamination, such as LLM pass rates, correlating with GitHub popularity and release dates of benchmarks.Their work contributes to the field by providing an open-source dataset, raw results, and an evaluation framework, which facilitates further research on BDC.</p>
<p>We now consider six representative comparison-based studies.Comparison-based strategies offer a robust approach to detecting BDC by scrutinizing model-generation performance against evaluation datasets.These methods, exemplified by various techniques such as content similarity, distribution analysis, perplexity estimation, and temporal performance comparisons, provide valuable insights into the presence and extent of contamination.Comparison-based strategies enable a more comprehensive detection of BDC, offering flexibility in selecting comparison perspectives to identify potential issues.However, akin to matching-based approaches, these methods encounter similar limitations, such as the requirement for substantial computational resources during testing.Additionally, unlike match-based strategies, certain comparison-based strategies may exhibit a restricted scope of detection, concentrating on specific contamination types or datasets.This specificity can hinder generalizability across diverse scenarios; for instance, datasets lacking temporal information impede the application of chronological analysis techniques.</p>
<p>The pursuit of practical solutions for detecting BDC within evaluation datasets is highly important, especially in the context of LLMs.Matching-based methods, focusing on tangible evidence such as dataset inspection and content generation analysis, offer actionable insights into the presence of contamination.However, accessibility to training data and computational demands pose practical challenges to their widespread implementation.Conversely, comparison-based methods provide robust detection mechanisms by scrutinizing model performance against evaluation datasets, offering flexibility in detection perspectives.Nonetheless, these methods may have limited detection scopes and require substantial computational resources.In conclusion, both approaches contribute significantly to understanding and mitigating BDC risks, highlighting the critical need for practical solutions that balance effectiveness with feasibility in real-world applications.Continued research and development in this area are essential for advancing the field of NLP and ensuring the trustworthiness of LLMs.</p>
<p>BDC MITIGATION STRATEGIES</p>
<p>After conducting an extensive survey of research on BDC detection, we now move on to consider the challenge of mitigating BDC.We categorize mitigation strategies into three distinct approaches: data curation, data refactoring, and benchmark-free.These strategies are discussed in Sections 4.1, 4.2, and 4.3, respectively.We summarize all investigated mitigation strategies in Table 3.</p>
<p>Curating New Data</p>
<p>Employing new data is the most straightforward way to mitigate the BDC problem [45,78].However, this is often an impractical solution.Furthermore, new data is only uncontaminated until it is incorporated into a pre-trained corpus of a future LLM.Addressing the continued availability of new benchmarks is a challenge that has received considerable attention.Along this line of thought, it is easy to associate the use of private datasets to carry out the evaluation of the performance of LLMs so that the benchmarks are less likely to appear in the pre-training data domain.Chandran et al. [19] proposed a novel approach to benchmarking where test datasets remain private, as shown in Figure 2, preventing contamination and ensuring more accurate evaluations of LLMs.The authors described various scenarios and solutions, including the use of confidential computing and cryptography, to maintain the integrity of benchmarking processes.However, this approach necessitates a degree of trust in both the model provider and the entities responsible for maintaining the benchmark's integrity.Similarly, Jacovi et al. [65] also managed to isolate the evaluation data from the public network, using three practical strategies to mitigate BDC: encrypting test data with a public key, demanding training exclusion controls from API holders, and avoiding data that appears with its solution on the Internet.</p>
<p>In contrast, Ma et al. [95] focused on a different strategy, developing dynamic benchmarks.They introduced Dynaboard, a new platform for evaluating NLP models.Unlike traditional methods that rely on self-reported metrics or predictions on a single dataset, Dynaboard evaluates models  [158] directly in the cloud.This approach addresses challenges such as reproducibility and accessibility, allowing for real-time interaction with models and collection of additional metrics like memory use and robustness.Models are ranked on the Dynascore, a utility-based aggregation of these metrics, which can be customized to reflect user preferences.</p>
<p>In a similar vein, Li et al. [90] introduced LatestEval, an automated method for creating uncontaminated reading comprehension evaluations.LatestEval combats BDC by using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models.The authors developed an automated pipeline to gather the latest texts, identify key information, and construct questions that require models to infer answers from the context rather than copy-pasting.Their experiments showed that language models exhibit minimal memorization behaviors on LatestEval compared to previous benchmarks, suggesting a more robust evaluation and a reduced risk of BDC.</p>
<p>Jain et al. [66] described the same idea to the code generation task by proposing LiveCodeBench, a benchmark that evaluates LLMs for coding by using a contamination-free dataset of coding problems from competitive programming platforms.Their results show that LiveCodeBench can effectively measure the generalization capabilities of LLMs and highlight the potential overfitting issues in existing benchmarks.The core of this work is to achieve mitigation of BDC by continuously updating the test cases in the benchmarks.Similarly, Fan et al. [41] proposed a dynamic benchmark with monthly updates to test the reasoning ability of LLMs.</p>
<p>Curating new data represents a direct and widely-adopted strategy for mitigating BDC.Within this strategy, we classify the primary methods into two distinct types: private benchmark and dynamic benchmark.The former approach circumvents inclusion in the pre-training dataset of LLMs by isolating newly collected evaluation data from the public network.The key advantage lies in its effective prevention of BDC through straightforward isolation.Encryption and stringent control measures further ensure data integrity.However, the limited accessibility of private benchmarks introduces opacity, necessitating heightened ethical considerations for both model providers and benchmark custodians.On the other hand, dynamic benchmarks offer an intriguing avenue for realtime and adaptive assessment of LLMs.By ensuring data freshness and minimizing contamination risk, they improved model evaluation.Nevertheless, dynamic benchmarks introduce bias and lack the guarantee of consistent results across successive assessments.Note that both approaches require substantial additional resources to facilitate evaluations, potentially constraining their scalability.</p>
<p>Refactoring Existing Data</p>
<p>Efforts to address the BDC challenge in LLM evaluations extend beyond curating new data.Strategies now include refactoring existing data, aiming to enhance evaluation reliability and effectiveness by restructuring and augmenting established benchmarks.In this section, we look at innovative methodologies proposed in recent literature, drawing insights from studies such as EvoEval and DyVal 2. These methodologies leverage diverse techniques to refactor existing evaluation datasets.Additionally, content filtering of existing datasets, as demonstrated by Dodge et al. [32], contributes valuable mitigation against BDC risk.</p>
<p>Xia et al. [158] proposed a scheme called EvoEval to mitigate the BDC problem, which focuses on the coding capabilities of LLMs, and which essentially creates five corresponding new prompts based on five dimensions (Difficult, Creative, Subtle, Combine, Tool use) for existing test questions, and then uses them to evaluate the consistency of the answers obtained by the model dealing with different prompts about the same question, and the code pass rate to assess its performance.The results show that on 51 LLMs, after applying the component, the models achieved an average of 39.4% reduction in performance on the HumanEval [23] benchmark.</p>
<p>Language understanding Problem solving Domain knowledge</p>
<p>Original benchmark</p>
<p>Probing benchmark</p>
<p>An astronomer observes that a planet rotates faster after a meteorite impact.Which is the most likely effect of this increase in rotation?A: Planetary density will decrease.B: Planetary years will become longer.C: Planetary days will become shorter.(Correct answer) D: Planetary gravity will become stronger.</p>
<p>Basic cognitive ability</p>
<p>Probing principles In a distant solar system, astronomers detect a planet similar to Earth in terms of mass and composition.Following a significant event where the planet was struck by a rogue meteorite, which was noted to have a sizeable mass relative to the planet, the celestial body is now observed to have a quicker spin on its axis.Considering the laws of conservation of angular momentum, what is the probable consequence of this accelerated rotational speed for the planet?</p>
<p>A: The duration of a single rotation on its axis will be reduced.(Correctanswer) B: The planet's mass will be distributed more widely.C: It will take longer for the planet to complete an orbit around its star.D: The planet will emit more heat due to increased rotational energy.E: The force exerted by the planet's mass will intensify.</p>
<p>Principle 𝒑 𝒊</p>
<p>Judge Agent Probing Agent</p>
<p>Multi-round probing and judge Fig. 4. The Meta Probing Agent (MPA) [181] process that transforms an original benchmark into a new one.</p>
<p>Principle 𝒑 𝒋</p>
<p>Judge agent Probing agent</p>
<p>The principles here can be combined to create various probing benchmarks for multifaceted analysis.In (c) we see how MPA generates a new sample, given an existing sample from ARC-C [26].</p>
<p>Similarly, the DyVal 2 [181] study introduced a new dynamic evaluation protocol called Meta Probing Agents (MPA), which is designed to assess LLMs more effectively.MPA, as a part of DyVal 2, extends the previous DyVal [180] framework and focuses on three basic cognitive abilities: language understanding, problem-solving, and domain knowledge, the framework and an example of which are demonstrated in Figure 4.The protocol dynamically configures these abilities to provide a multifaceted analysis of LLMs.The extensive evaluations conducted using MPA revealed that most LLMs have room for improvement.The study also found a strong correlation between the basic abilities and model size, indicating that larger models have stronger abilities.Additionally, MPA can serve as a data augmentation method to enhance the capabilities of LLMs.</p>
<p>Ying et al. [164] proposed an innovative approach to maintaining the reliability and timeliness of dataset evaluations for LLMs.In Figure 5, we see that the authors introduced two strategies: a mimicking strategy that uses LLMs to generate new, stylistically similar samples to existing ones, and an extending strategy that adjusts the difficulty of samples based on cognitive levels.Their experiments demonstrated that these strategies can effectively mitigate data leakage issues and provide a more nuanced evaluation of LLM capabilities.In other work, Yang et al. [162] proposed a more robust LLM-based decontamination method and apply it to popular pre-training and fine-tuning datasets.They advocate for the adoption of stronger decontamination approaches and the development of fresh one-time exams for accurate model evaluation.</p>
<p>Additionally, Dodge et al. [32] examined the Colossal Clean Crawled Corpus (C4, Raffel et al. [117]), a dataset used to train LLMs.The authors provide a detailed documentation of C4, revealing unexpected sources like patents and US military websites.They also discover machine-generated text and evaluation examples from other NLP datasets within C4.The study evaluates the impact of filters used to create C4, showing that blocklist filtering disproportionately removes text related to minority individuals.</p>
<p>Methods for refactoring existing data, specifically data regeneration and content filtering, represent promising avenues for addressing the challenges posed by BDC in language model evaluations.Data Regeneration, exemplified by approaches such as EvoEval and DyVal 2, emphasizes the restructuring and augmentation of existing datasets to provide multifaceted assessments of LLMs capabilities.By dynamically configuring evaluation protocols and introducing new prompts, these methods enhance the granularity and depth of model evaluations.However, they may require substantial computational resources and expertise to implement effectively.On the other hand, Content Filtering strategies, as demonstrated by the work of Yang et al. [162], focus on identifying and mitigating sources of contamination within datasets.These approaches offer more targeted solutions and can provide immediate improvements in data quality.Nonetheless, they may overlook nuanced aspects of model performance and require ongoing adjustments to adapt to evolving challenges.Overall, both Data Regeneration and Content Filtering methodologies contribute valuable insights and tools to the broader endeavor of refining evaluation datasets, underscoring the importance of multifaceted approaches in addressing the issue of BDC in language model evaluations.</p>
<p>Benchmark-free Evaluation</p>
<p>To provide more flexible evaluation methods for LLMs with a reduced risk of contamination, researchers have looked at a more radical strategy: benchmark-free evaluation.This strategy aims to circumvent the BDC risk associated with traditional benchmark assessments.Our review of the current research landscape reveals a nascent yet significant direction within this area.Specifically, we categorize it into two subcategories: LLM-as-judge and human participation.These novel approaches offer promising avenues for addressing the pervasive issue of BDC in LLMs evaluations while fostering greater adaptability and reliability.</p>
<p>LLM-as-judge was first used to measure human preference for content generated by LLMs [6,86,146,150,174,176,183], and then Li et al. [87] suggested that it could be used to mitigate the BDC problem of the LLMs benchmark.They introduced TreeEval, a novel method for evaluating Fig. 6.TreeEval [87] system with an illustrative tree for evaluation.The left section contains the components and their workflow in TreeEval.The right section displays a constructed tree within topic Technology and Communication for evaluation (the leaf nodes are shown in red boxes), where each node denotes a question annotated with its topic and valuation score.</p>
<p>LLMs without relying on traditional benchmarks.It allows a high-performance LLMs to conduct an irreproducible evaluation session, effectively preventing data leakage.As shown in the Figure 6, this method uses a tree planning strategy to generate a series of questions based on the current evaluation status, ensuring a comprehensive and efficient assessment.The study tested six models of varying sizes and found that TreeEval achieved a high correlation with AlpacaEval2.0[36,37,88] using approximately 45 questions, demonstrating its robustness and reliability.Similarly, Chiang et al. [24] reduced the idea of LLM-as-judge to Human-as-judge, i.e., the use of human participation to evaluate the performance of LLMs while also mitigating the BDC problem, and then they introduced a novel platform called Chatbot Arena3 that uses crowdsourced human preferences to evaluate LLMs.It employs a pairwise comparison method and has collected over 240K votes, establishing it as a widely-referenced LLMs leaderboard.</p>
<p>Notably, FreeEval proposed by Yu et al. [166], integrates multiple methods into one comprehensive framework, including traditional dataset assessment, data regeneration, LLM-as-judge, human participation, and other means which is shown in Figure 7. FreeEval is designed to enable trustworthy and efficient automatic evaluations of LLMs, and the key features of FreeEval include unified abstractions for diverse evaluation methodologies, integration of meta-evaluation techniques, and a high-performance infrastructure.</p>
<p>The emergence of benchmark-free evaluation methods presents a new way of achieving more adaptable and robust language model assessments, particularly in mitigating the risks associated with BDC.In this section, we survey relevant studies and categorize them into LLM-as-judge and human participation.The former minimizes the risk of inadvertently incorporating biased or contaminated data by enabling LLMs to self-evaluate without relying on conventional benchmarks.However, when implementing this evaluation paradigm, careful consideration is necessary regarding whether the LLMs controlling the evaluation process were trained on contaminated or biased pre-training datasets, as this can significantly impact the method's effectiveness.On the other hand, the human participant approach offers valuable insights into LLMs performance within real-world contexts and applications.Nevertheless, human evaluation inherently introduces subjectivity, influenced by personal preferences, potentially leading to discrepancies and biases in the evaluation results.Moreover, collecting and analyzing assessments from human participants is resource-intensive and time-consuming, demanding rigorous efforts to ensure reliability and validity.Both approaches share a common foundation: utilizing external evaluators to test LLMs.However, they diverge in their choice of evaluators-LLMs or humans.Consequently, both approaches face similar challenges, including uncertainty regarding external evaluators and the computational resource demands inherent to this strategy.</p>
<p>We have presented a comprehensive overview of strategies for mitigating BDC in LLM evaluations.We see that these strategies fall into three categories: data curation, data refactoring, and benchmarkfree evaluation approaches.Each category offers different solutions to tackle the challenges posed by BDC.Data curation methods, such as private and dynamic benchmarks, focus on isolating evaluation data to prevent contamination, albeit with considerations regarding accessibility and biases.Refactoring existing data through techniques like data regeneration and content filtering enhances evaluation reliability but may require substantial resources.Benchmark-free evaluation, including LLM-as-judge and human participation methodologies, presents radical yet promising alternatives, with each approach offering unique insights into LLM performance while posing challenges related to biases, subjectivity, and resource requirements.Notably, these strategies are not immune to secondary contamination, as newly collected or refactored data may still be influenced by LLMs trained on previously contaminated data.Furthermore, semantic-level contamination remains unavoidable, and simple content filtering may not suffice.Even LLMs acting as evaluators draw on the knowledge of their training data, introducing inherent risks of BDC.Collectively, these multifaceted strategies underscore the complex nature of the BDC challenge and emphasize the need for robust and adaptable LLM evaluations in real-world contexts.</p>
<p>CHALLENGES AND FUTURE DIRECTIONS</p>
<p>In current research on LLMs and BDC, it is deemed impracticable to fully remove the risks associated with contamination.This is based on two main reasons:</p>
<p>(1) Imperative of Large-Scale Pre-training: The core of LLMs' capabilities is unlocked through extensive pre-training exercises, which necessitate the use of a substantial level of training data.This content invariably encompasses information pertinent to benchmark datasets.While strategies such as data filtration and regeneration offer some mitigation of BDC risks, they fall short in addressing the semantic and informational dimensions of BDC.Furthermore, any prospective techniques capable of addressing these concerns would likely mandate prohibitive computational resources, rendering them impractical for widespread application.(2) Ascendancy of AIGC: Coinciding with the increasing maturity and ubiquity of LLM technologies, AI models are increasingly instrumental in generating new content.These models are predicated on large-scale, obscure pre-training datasets.The recursive nature of AIGC enables the evolution of BDC towards semantic dimensions, expanding the seriousness of this problem and making the human identification of BDC risks more challenging.These factors collectively highlight the complexity of the challenges faced in dealing with BDC risks, underscoring the need for innovative solutions that balance performance with practicality.We outline here several promising future directions for mitigating these problems:</p>
<p>• Human Evaluation: The inclusion of human evaluators in evaluating LLMs, as per Chiang et al. [24], potentially represents an ideal approach.However, this strategy is not without challenges.Such evaluation processes are resource-intensive and susceptible to various individual background influences, including political affiliations, cultural perspectives, personal beliefs, and educational backgrounds.These contextual factors introduce inherent subjectivity into the evaluation process, potentially leading to unintended biases.• Dynamic System: The development of dynamic systems for adaptive evaluation of LLMs is also a promising direction.Adaptive evaluation systems, exemplified by the work of Li et al. [87], offer an innovative approach.They operate beyond the confines of traditional benchmark training and testing paradigms, leveraging dynamic scheduling to assess model performance.By doing so, they can reveal the true capabilities of LLMs, thereby mitigating BDC risks to a significant extent and enhancing overall model evaluation.However, a key consideration lies in the data source underpinning the dynamic evaluator.While work such as that of Li et al. [90] introduces fresh data streams and the evaluation process is adaptive, the data used for evaluation remains inherently static, as well as potentially AIGC data.Consequently, residual BDC risks persist.Therefore, careful scrutiny of the data origin and composition is required to ensure the integrity and reliability of dynamic evaluation systems.• Benchmark Content Tags: There have been calls for the establishment of Benchmark Content Tags.Analogous to the robots.txt4protocol employed by search engines or Google's Fact Check Tools API5 in the fact-checking field, this protocol aims to improve transparency and facilitate the identification of content associated with benchmark datasets.Specifically, we advocate for the inclusion of standardized tags when posting content relevant to these benchmarks.These fixed tags serve as indicators that model evaluation is implicated.By adopting such a protocol, we mitigate the burden of filtering pre-training data, thereby promoting more effective and efficient model development and evaluation processes.• Adversarial Evaluation: The exploration of adversarial evaluation methodologies presents a promising avenue for mitigating the BDC problem in LLMs.This involves the development of generative models, incorporating diverse technological paradigms such as reinforcement learning [72,114,135], adversarial generative networks [48], and variational autoencoders [79,80], to synthesize new data representative of natural language while evading potential BDC risks.By harnessing adversarial techniques, these models can generate data that challenges the robustness and generalization capabilities of LLMs, facilitating more rigorous evaluation of model performance in the presence of BDC.Moreover, the integration of a BDC detector within the adversarial evaluation framework enables the supervision and validation of generated data, ensuring its integrity and minimizing the likelihood of BDC contamination.</p>
<p>• Comprehensive Evaluation Systems: The concept of a Comprehensive Evaluation System emerges as a natural response to the BDC challenge.Existing mitigation strategies often adopt singular viewpoints, potentially overlooking critical aspects.By integrating multiple perspectives, we can holistically address BDC risks, thereby enhancing evaluation reliability.</p>
<p>Frameworks, like the one proposed by Yu et al. [166], make it more complicated to assess the performance of LLMs.However, systems that integrate multiple BDC mitigation options, including LLM-as-judge, Human Participation, and other tools, can minimize BDC risks.</p>
<p>CONCLUSION</p>
<p>In this paper, we have explored the complex issue of BDC in LLMs and the wide variety of strategies that have been proposed for mitigating it.We reviewed the detection methods of the BDC problem and categorized them into two classes: Matching-based and Comparison-based methods, each come with their own set of challenges.However, they represent necessary steps towards ensuring the validity of LLM evaluations.Subsequently, we have categorized existing BDC mitigation strategies into three main groups: data curation, data refactoring, and benchmark-free evaluation approaches.Each of these strategies offers unique solutions to the challenges posed by BDC, but none are immune to secondary contamination or semantic-level contamination.</p>
<p>We have also highlighted the challenges and future directions in mitigating BDC risks.The necessity of large-scale pre-training and the ascendancy of AIGC make it nearly impossible to fully eliminate BDC risks.However, several promising future directions have been outlined, including human evaluation, dynamic systems, benchmark content tags, adversarial evaluation, and comprehensive evaluation systems.Each of these approaches has its own set of challenges and considerations, but they all contribute to the ongoing effort to balance performance with practicality in LLM evaluations.</p>
<p>In conclusion, the issue of BDC in LLMs is a multifaceted problem that requires a multifaceted solution.While the strategies and directions discussed in this paper offer promising avenues for mitigating BDC risks, it is clear that more work is needed in this area.As LLMs continue to evolve and become more integrated into our daily lives, the importance of robust and reliable evaluation methods will only increase.We hope that this paper serves as a valuable resource for researchers and practitioners in the field as they navigate the complex landscape of LLM evaluation in the face of BDC.</p>
<p>. The model can adjust its responses based on the examples or instructions included within the same input prompt, and in-context learning does not require the model to undergo additional training or change its weights., Vol. 1, No. 1, Article .Publication date: June 2024.</p>
<p>Fig. 1 .
1
Fig.1.An illustration of the method developed by Deng et al.[30] for identifying BDC in modern benchmarks.The figure on the left shows the workflow of an information retrieval system, which aims to detect potentially contaminated data within a benchmark by utilizing a pre-trained corpus.The figure on the right introduces TS-Guessing, an approach for detecting potential contamination.This technique involves concealing parts of the information in the test set and prompting LLMs to infer the missing elements.If the LLMs can accurately predict the same missing option as the one in the test set, it raises the suspicion that they may have encountered the benchmark data during their training.</p>
<p>Another strategy to detecting BDC involves comparing the performance of model-generation on evaluation datasets.Common examples such as comparing the similarity Common methods include , Vol. 1, No. 1, Article .Publication date: June 2024.</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: June 2024.</p>
<p>Fig. 2 .
2
Fig. 2. The scheme proposed by Chandran et al. [19].</p>
<p>Fig. 3 .
3
Fig.3.Overview of EVOEVAL evolving problem generation pipeline proposed by Xia et al.[158]</p>
<p>, Vol. 1 ,
1
No. 1, Article .Publication date: June 2024.BenchmarkData Contamination of Large Language Models: A Survey</p>
<p>𝑝 1 : 4 : 5 :
145
Paraphrasing questions  2 : Paraphrasing choices  3 : Permuting choices  Adding extra context into questions  Adding a new choice</p>
<p>Fig. 5 .
5
Fig. 5. Auto-dataset update framework proposed by Ying et al. [164], who deployed two strategies: mimicking and extending to update.</p>
<p>AFig. 7 .
7
Fig. 7. FreeEval framework proposed by Yu et al. [166].</p>
<p>Table 1 .
1
Key metrics used in different aspects of automatic evaluation of LLMs.</p>
<p>Table 2 .
2
An overview of the main methods for detecting data contamination, listing their categories, together with a short description and representative references.
TypeMethodShort DescriptionReferencesDataset InspectionDetect overlapping content be-tween pre-training and evaluation datasets.[7, 15, 39, 62, 85, 107, 178]Matching-basedMembership InferenceMake the model generate content based on test prompts for checking the inclusion of pre-training data.[20, 30, 35, 46, 47, 64, 81, 85, 91, 118, 128, 129]Example GenerationMake the model generate task-checking. relevant examples for overlap[85]Content ComparisonCompare the difference between model-generated content or with evaluation dataset, e.g. similarity, distribution, perplexity.[30, 34, 47, 82, 89, 96, 119, 128]Comparison-basedSequential AnalysisAssess the sequence alignment of evaluation dataset. model-generated content with the[73, 108]Chronological AnalysisModel performance is gauged on a dated dataset, assessing the impact of varying training data collection times.[14, 61, 85, 118, 121, 156, 162]</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: June 2024.
questionExaminertopics sampled𝑄 0𝑆 0 : (0, 0) 𝐶 0 : Technology and CommunicationLLM 1 vs LLM 2candidate questionsController Eval𝑄 1𝐶 1 : AI 𝑆 1 : (1, 1)𝑄 2𝐶 2 : 5G 𝑆 2 : (1, 1)response 1 response 2Judgescore𝐶 3 : AI Ethics 𝑆 3 : (2, 0) 𝑄 3𝑄 4𝐶 4 : Accessibility Tools 𝑆 4 : (1, 1)𝐶 5 : Human Machine Interaction 𝑄 5 𝑆 5 : (2, 0)𝐶 6 : applications 𝑄 6 𝑆 6 : (0, 2)TreeEval systemScore Aggregator𝑄 7𝑆 7 : (2, 0) 𝐶 7 : Enhancing Accessibility in Communication
, Vol. 1, No. 1, Article . Publication date: June 2024.
https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues , Vol. 1, No. 1, Article . Publication date: June
.
https://codeforces.com/ , Vol. 1, No. 1, Article . Publication date: June 2024.
https://chat.lmsys.org/ , Vol. 1, No. 1, Article . Publication date: June 2024.
https://www.robotstxt.org/
https://toolbox.google.com/factcheck/apis , Vol. 1, No. 1, Article . Publication date: June 2024.
All authors contributed significantly to the conception, design, and execution of this paper.CX played a pivotal role in shaping the core ideas and conceptual framework, leading the research effort and contributing substantially to most section.SG contributed notably to Section 2, providing valuable context and background information.DG and MTK provided supervision and guidance.All authors have read and approved the final version of the manuscript.
Opinion Mining and Sentiment Analysis. C Charu, Aggarwal, 10.1007/978-3-319-73531-3_132018Springer International PublishingCham</p>
<p>Evaluating ChatGPT and Bard AI on Arabic Sentiment Analysis. Abdulmohsen Al-Thubaity, Sakhar Alkhereyf, Hanan Murayshid, Nouf Alshalawi, Maha Omirah, Raghad Alateeq, Rawabi Almutairi ; Hassan Sawaf, Samhaa El-Beltagy, Wajdi Zaghouani, Walid Magdy, 10.18653/v1/2023.arabicnlp-1.27Proceedings of ArabicNLP 2023. Ahmed Abdelali, Nadi Tomeh, Ibrahim Abu Farha, Nizar Habash, Salam Khalifa, Amr Keleg, Hatem Haddad, Imed Zitouni, Khalil Mrini, Rawan Almatham, ArabicNLP 2023SingaporeAssociation for Computational Linguistics2023Razan Alsuwailem, Manal Alhassoun, and Imaan Alkhanen</p>
<p>. Article . Publication date. 11June 2024</p>
<p>Large Language Model Based Fake News Detection. Mussa Aman, 10.1016/j.procs.2023.12.14414th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare. EUSPN/ICTH2024. 2024. 2023231</p>
<p>Introducing the next generation of Claude. Anthropic, 2024</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Jared Kaplan, arXiv:2112.00861[cs.CL]General Language Assistant as a Laboratory for Alignment. 2021</p>
<p>Benchmarking Foundation Models with Language-Model-as-an-Examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.18653/v1/2023.ijcnlp-main.45Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Jong C Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, Adila Alfa Krisnadhi, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational Linguistics20231</p>
<p>Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM. Rachel Bawden, François Yvon, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Alvarez Sergi, Nora Vidal, Mara Aranberri, Carla Nunziatini, Mikel Parra Escartín, Maja Forcada, Carolina Popovic, Helena Scarton, Moniz, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>A Neural Probabilistic Language Model. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Advances in Neural Information Processing Systems. T Leen, T Dietterich, V Tresp, MIT Press200013</p>
<p>Improving image generation with better captions. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, 2023</p>
<p>Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models. Terra Blevins, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.233Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Language (Technology) is Power: A Critical Survey of "Bias" in NLP. Lin Su, Solon Blodgett, Hal Barocas, Iii Daumé, Hanna Wallach, 10.18653/v1/2020.acl-main.485Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana, arXiv:2404.06209[cs.LG]Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models. 2024</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Jialun Cao, Wuqi Zhang, Shing-Chi Cheung, arXiv:2403.16898[cs.SE]Concerned with Data Contamination? Assessing Countermeasures in Code Language Model. 2024</p>
<p>Extracting Training Data from Diffusion Models. Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, 32nd USENIX Security Symposium (USENIX Security 23). USENIX Association. Anaheim, CA2023. June 20241Publication date. Xu et al. usenixsecurity23/presentation/carlini</p>
<p>Extracting Training Data from Large Language Models. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, Colin Raffel, 30th USENIX Security Symposium (USENIX Security 21). USENIX Association. 2021</p>
<p>Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma, Kashish Mittal, Manohar Swaminathan, arXiv:2403.00393[cs.CR]Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs. 2024</p>
<p>Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. Kent Chang, Mackenzie Cramer, Sandeep Soni, David Bamman, 10.18653/v1/2023.emnlp-main.453Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>A Survey on Evaluation of Large Language Models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 15452024. mar 2024</p>
<p>Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton, arXiv:2310.01957[cs.RO]Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. 2023</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374[cs.LG]Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, arXiv:2403.04132[cs.AI]Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. 2024</p>
<p>PaLM: Scaling Language Modeling with Pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, Journal of Machine Learning Research. David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck242023. 2023</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457[cs.AI]Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018</p>
<p>Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa. Junqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, Xipeng Qiu, 10.18653/v1/2021.naacl-main.146Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline2021</p>
<p>Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models. Daniel De, Vassimon Manela, David Errington, Thomas Fisher, Boris Van Breugel, Pasquale Minervini, 10.18653/v1/2021.eacl-main.190Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Jorg Tiedemann, Reut Tsarfaty, the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumePaola MerloOnline2021</p>
<p>Evading Data Contamination Detection for Language Models is (too) Easy. Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev, arXiv:2402.02823[cs.LG]Article. 112024. June 2024Publication date</p>
<p>Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, arXiv:2311.09783[cs.CL]Investigating Data Contamination in Modern Benchmarks for Large Language Models. 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, arXiv:2301.00234[cs.CL]A Survey on In-context Learning. 2023</p>
<p>Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li, arXiv:2402.15938[cs.CL]2024</p>
<p>V André, Xuandong Duarte, Arlindo L Zhao, Lei Oliveira, Li, arXiv:2402.09910[cs.CL]DE-COP: Detecting Copyrighted Content in Language Models Training Data. 2024</p>
<p>Yann Dubois, Balazs Galambosi, Percy Liang, Tatsunori B Hashimoto, Length-Corrected AlpacaEval: A Simple Debiasing of Automatic Evaluators. 2024</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, arXiv:2305.14387[cs.LG]AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. 2023</p>
<p>SearchQA: A New Q&amp;A Dataset Augmented with Context from a Search Engine. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, Kyunghyun Cho, arXiv:1704.05179[cs.CL]2017</p>
<p>Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation. Aparna Elangovan, Jiayuan He, Karin Verspoor, 10.18653/v1/2021.eacl-main.113Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Jorg Tiedemann, Reut Tsarfaty, the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumePaola MerloOnline2021</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, Yoav Goldberg, arXiv:2207.14251[cs.CL]Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions. 2023</p>
<p>Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang, arXiv:2312.14890[cs.AI]NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. 2024</p>
<p>IIRC: A Dataset of Incomplete Information Reading Comprehension Questions. James Ferguson, Matt Gardner, Hannaneh Hajishirzi, Tushar Khot, Pradeep Dasigi, 10.18653/v1/2020.emnlp-main.86Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>PAL: Program-aided Language Models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine Learning ( Machine Learning ResearchPMLR2023202</p>
<p>Selective Classification for Deep Neural Networks. Yonatan Geifman, Ran El-Yaniv, ; I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S , Advances in Neural Information Processing Systems. R Vishwanathan, Garnett, Curran Associates, Inc201730</p>
<p>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?. Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban, arXiv:2404.06644[cs.CL]2024</p>
<p>Shahriar Golchin, Mihai Surdeanu, arXiv:2311.06233[cs.CL]Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. 2024</p>
<p>Time Travel in LLMs: Tracing Data Contamination in Large Language Models. Shahriar Golchin, Mihai Surdeanu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Generative Adversarial Nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. 2014. June 20241Publication date</p>
<p>. Z Ghahramani, M Welling, C , Cortes, N. Lawrence, and K.Q. WeinbergerCurran Associates, Inc27</p>
<p>Albert Gu, Tri Dao, arXiv:2312.00752[cs.LG]Mamba: Linear-Time Sequence Modeling with Selective State Spaces. 2023</p>
<p>. Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, 10.1561/25000000102017</p>
<p>On Calibration of Modern Neural Networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, PMLR, 1321-1330Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine Learning ( Machine Learning Research201770</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, arXiv:2401.14196[cs.SE]DeepSeek-Coder: When the Large Language Model Meets Programming -The Rise of Code Intelligence. 2024</p>
<p>From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj, 10.1109/ACCESS.2023.3300381IEEE Access. 112023. 2023</p>
<p>Equality of Opportunity in Supervised Learning. Moritz Hardt, Eric Price, Eric Price, Nati Srebro, Advances in Neural Information Processing Systems. D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Curran Associates, Inc201629</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young , Jin Kim, Mohamed Afify, Hany Hassan Awadalla, arXiv:2302.09210[cs.CL]How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation. 2023</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, arXiv:2203.15556[cs.CL]Training Compute-Optimal Large Language Models. Erich Elsen, Jack W Rae, Oriol Vinyals, Laurent Sifre, 2022</p>
<p>Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi, 10.1609/aaai.v38i20.30214Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. Mar. 202438</p>
<p>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao, arXiv:2403.02839[cs.CL]2024</p>
<p>Towards Reasoning in Large Language Models: A Survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen, -Chuan Chang, 10.18653/v1/2022.findings-emnlp.148Findings of the Association for Computational Linguistics: EMNLP 2022. Zornitsa Goldberg, Yue Kozareva, Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, Weizhu Chen, arXiv:2312.02143[cs.CL]Competition-Level Problems are Effective LLM Evaluators. 2023</p>
<p>Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy. Daphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, Nicholas Carlini, 10.18653/v1/2023.inlg-main.3Proceedings of the 16th International Natural Language Generation Conference. C , Maria Keet, Hung-Yi Lee, Sina Zarrieß, the 16th International Natural Language Generation ConferencePrague, CzechiaAssociation for Computational Linguistics2023</p>
<p>Nicos Isaak, arXiv:2308.15235[cs.CL]PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences. 2023</p>
<p>Training Data Extraction From Pre-trained Language Models: A Survey. Shotaro Ishihara, 10.18653/v1/2023.trustnlp-1.23Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing. Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop Kumar, Rahul Gupta, the 3rd Workshop on Trustworthy Natural Language ProcessingToronto, CanadaAssociation for Computational Linguistics2023. TrustNLP 2023</p>
<p>Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg, 10.18653/v1/2023.emnlp-main.308Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.07974[cs.SE]Article . Publication date. 112024. June 2024Models for CodeArmando Solar-Lezama, Koushik Sen, and Ion Stoica</p>
<p>Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, arXiv:2306.13651[cs.CL]2023</p>
<p>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, Yaodong Yang, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Does Data Contamination Make a Difference? Insights from Intentionally Contamination Pre-training Data For Language Models. Minhao Jiang, Ken Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen Tse Huang, Xing Wang, Shuming Shi, Zhaopeng Tu, arXiv:2301.08745[cs.CL]Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. 2023</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, arXiv:2207.05221[cs.CL]Chris Olah, and Jared Kaplan. 2022. Language Models (Mostly) Know What They Know. Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, 10.1613/jair.301Journal of artificial intelligence research. 41996. 1996</p>
<p>Deduplicating Training Data Mitigates Privacy Risks in Language Models. Nikhil Kandpal, Eric Wallace, Colin Raffel, Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, Sivan Sabato, the 39th International Conference on Machine Learning ( Machine Learning ResearchPMLR2022162</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361[cs.LG]Scaling Laws for Neural Language Models. 2020</p>
<p>Cultural entrenchment of folktales is encoded in language. Folgert Karsdorp, Lauren Fonteyn, 10.1057/s41599-019-0234-9Palgrave Communications. 512019. 2019</p>
<p>Measuring Catastrophic Forgetting in Neural Networks. Ronald Kemker, Marc Mcclure, Angelina Abitino, Tyler Hayes, Christopher Kanan, 10.1609/aaai.v32i1.11651Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2018. Apr. 201832</p>
<p>UNIFIEDQA: Crossing Format Boundaries with a Single QA System. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Online2020</p>
<p>FABLES: Evaluating faithfulness and content selection in book-length summarization. Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer, arXiv:2404.01261[cs.CL]2024</p>
<p>Semi-supervised Learning with Deep Generative Models. Shakir Durk P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, Advances in Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N Lawrence, K Q Weinberger, Curran Associates, Inc201427</p>
<p>P Diederik, Max Kingma, Welling, arXiv:1312.6114stat.ML]Auto-Encoding Variational Bayes. 2022</p>
<p>Understanding Black-box Predictions via Influence Functions. Pang Wei, Koh , Percy Liang, Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine Learning ( Machine Learning Research201770PMLR, 1885-1894</p>
<p>Platypus: Quick, Cheap, and Powerful Refinement of LLMs. Ariel Lee, Cole Hunter, Nataniel Ruiz, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Deduplicating Training Data Makes Language Models Better. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini, 10.18653/v1/2022.acl-long.577Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. June 20241Publication date</p>
<p>Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets. Patrick Lewis, Pontus Stenetorp, Sebastian Riedel, 10.18653/v1/2021.eacl-main.86Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Jorg Tiedemann, Reut Tsarfaty, the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumePaola MerloOnline2021</p>
<p>Task Contamination: Language Models May Not Be Few-Shot Anymore. Changmao Li, Jeffrey Flanigan, 10.1609/aaai.v38i16.29808Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. Mar. 202438</p>
<p>Generative Judge for Evaluating Alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Xiang Li, Yunshi Lan, Chao Yang, arXiv:2402.13125[cs.CL]TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning. 2024</p>
<p>AlpacaEval: An Automatic Evaluator of Instruction-following Models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yucheng Li, arXiv:2309.10677[cs.CL]Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. 2023</p>
<p>LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction. Yucheng Li, Frank Guerin, Chenghua Lin, 10.1609/aaai.v38i17.29822Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. Mar. 202438</p>
<p>Yucheng Li, Frank Guerin, Chenghua Lin, arXiv:2310.17589[cs.CL]An Open Source Data Contamination Report for Large Language Models. 2024</p>
<p>Towards Understanding and Mitigating Social Biases in Language Models. Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov, Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning ( Machine Learning Research2021139</p>
<p>WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, Jie Tang, 10.1145/3580305.3599931Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLong Beach, CA, USA; New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Gender and Representation Bias in GPT-3 Generated Stories. Li Lucy, David Bamman, 10.18653/v1/2021.nuse-1.5Proceedings of the Third Workshop on Narrative Understanding. Nader Akoury, Faeze Brahman, Snigdha Chaturvedi, Elizabeth Clark, Mohit Iyyer, Lara J Martin, the Third Workshop on Narrative UnderstandingAssociation for Computational Linguistics2021</p>
<p>Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking. Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, Douwe Kiela, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Data Contamination: From Memorization to Exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Short Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20222</p>
<p>Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, arXiv:2402.09299[cs.SE]Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code. 2024</p>
<p>Timothy R Mcintosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N Halgamuge, arXiv:2402.09880[cs.AI]Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence. 2024</p>
<p>Sentiment analysis algorithms and applications: A survey. Walaa Medhat, Ahmed Hassan, Hoda Korashy, 10.1016/j.asej.2014.04.011Ain Shams Engineering Journal. 52014. 2014</p>
<p>NarrativeXL: a Large-scale Dataset for Long-Term Memory Models. Arsenii Moskvichev, Ky-Vinh Mai, 10.18653/v1/2023.findings-emnlp.1005Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Adaptive Machine Translation with Large Language Models. Yasmin Moslem, Rejwanul Haque, John D Kelleher, Andy Way ; Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Alvarez Sergi, Nora Vidal, Mara Aranberri, Carla Nunziatini, Mikel Parra Escartín, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. Maja Forcada, Carolina Popovic, Helena Scarton, Moniz, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023. June 20241</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2112.09332[cs.CL]WebGPT: Browser-assisted question-answering with human feedback. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R Bowman, 10.18653/v1/2020.emnlp-main.154Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberAssociation for Computational Linguistics2020</p>
<p>Why We Need New Evaluation Metrics for NLG. Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser, 10.18653/v1/D17-1238Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Martha Palmer, Rebecca Hwa, Sebastian Riedel, the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Who did What: A Large-Scale Person-Centered Cloze Dataset. Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David Mcallester, 10.18653/v1/D16-1241Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Jian Su, Kevin Duh, Xavier Carreras, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>OpenAI. 2022. Introducing ChatGPT. </p>
<p>Proving Test Set Contamination for Black-Box Language Models. Yonatan Oren, Nicole Meister, Niladri S Chatterji, Faisal Ladhak, Tatsunori Hashimoto, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe, Advances in Neural Information Processing Systems. S Koyejo, A Mohamed, D Agarwal, K Belgrave, A Cho, Oh, Curran Associates, Inc202235</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe, Advances in Neural Information Processing Systems. S Koyejo, A Mohamed, D Agarwal, K Belgrave, A Cho, Oh, Curran Associates, Inc202235</p>
<p>emrQA: A Large Corpus for Question Answering on Electronic Medical Records. Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng, 10.18653/v1/D18-1258Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingEllen Riloff, David Chiang; Brussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, Jean-François Godbout, Reihaneh Rabbany, 10.18653/v1/2023.emnlp-main.395Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Why think step by step? Reasoning emerges from the locality of experience. Ben Prystawski, Michael Li, Noah Goodman, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Markov decision processes: discrete stochastic dynamic programming. Martin L Puterman, 10.1002/97804703168872005John Wiley &amp; Sons</p>
<p>Pre-trained models for natural language processing: A survey. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, 10.1007/s11431-020-1647-3Science China Technological Sciences. 632020. 2020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Article. 112019. June 2024Publication date</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 212020. 2020</p>
<p>Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation. Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto, arXiv:2402.08100[cs.CL]2024</p>
<p>Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models. Martin Riddell, Ansong Ni, Arman Cohan, arXiv:2403.04811[cs.SE]2024</p>
<p>How Powerful are Decoder-Only Transformer Neural Models?. Jesse Roberts, arXiv:2305.17026[cs.CL]2024</p>
<p>Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley, arXiv:2310.10628[cs.CL]Data Contamination Through the Lens of Time. 2023</p>
<p>Leveraging Large Language Models for Multiple Choice Question Answering. Joshua Robinson, David Wingate, The Eleventh International Conference on Learning Representations. 2023</p>
<p>QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. Anna Rogers, Matt Gardner, Isabelle Augenstein, 10.1145/3560260ACM Comput. Surv. 55197452023. feb 2023</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, arXiv:2308.12950[cs.CL]Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024. Code Llama: Open Foundation Models for Code. </p>
<p>ScienceQA: a novel resource for question answering on scholarly articles. Tanik Saikh, Tirthankar Ghosal, Amish Mittal, 10.1007/s00799-022-00329-yInt. J. Digit. Libr. 2332022. sep 2022Asif Ekbal, and Pushpak Bhattacharyya</p>
<p>NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, 10.18653/v1/2023.findings-emnlp.722Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023Oier Lopez de Lacalle, and Eneko Agirre</p>
<p>Multitask Prompted Training Enables Zero-Shot Task Generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, International Conference on Learning Representations. Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M RushTeven Le Scao2022</p>
<p>Detecting Pretraining Data from Large Language Models. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Membership Inference Attacks Against Machine Learning Models. Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov, 10.1109/SP.2017.412017 IEEE Symposium on Security and Privacy (SP). 2017</p>
<p>Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, Huan Liu, 10.1089/big.2020.0062Big data. 82020. 2020</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, 10.1038/s41586-023-06291-2Nature. 6202023. 2023</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S Sara Mahdavi, Joelle Barral, Dale Webster, Greg S Corrado, Yossi Matias, arXiv:2305.09617[cs.CL]Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with Large Language Models. </p>
<p>What has ChatGPT read? The origins of archaeological citations used by a generative artificial intelligence application. Dirk Hr Spennemann, arXiv:2308.03301[cs.AI]Article. 112023. June 2024Publication date</p>
<p>Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data. Dominik Stammbach, Maria Antoniak, Elliott Ash, 10.18653/v1/2022.wnu-1.6Proceedings of the 4th Workshop of Narrative Understanding (WNU2022). Faeze Brahman, Mohit Iyyer, the 4th Workshop of Narrative Understanding (WNU2022)Elizabeth Clark; Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 10.1109/TNN.1998.712192IEEE Transactions on Neural Networks. 91998. 1998</p>
<p>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805[cs.CL]Gemini: A Family of Highly Capable Multimodal Models. 2023</p>
<p>Introduction to the CoNLL-2003 shared task: language-independent named entity recognition. Erik F Tjong, Kim Sang, Fien De Meulder, 10.3115/1119176.1119195Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 -Volume. the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 -VolumeEdmonton, Canada; USAAssociation for Computational Linguistics20034CONLL '03)</p>
<p>Observed versus latent features for knowledge base and text inference. Kristina Toutanova, Danqi Chen, 10.18653/v1/W15-4007Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. Edward Grefenstette, Karl Moritz Hermann, Hugo Larochelle, Scott , Wen-Tau Yih, the 3rd Workshop on Continuous Vector Space Models and their CompositionalityBeijing, ChinaAssociation for Computational Linguistics2015</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.13971[cs.CL]LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Best practices for the human evaluation of automatically generated text. Chris Van Der Lee, Albert Gatt, Sander Emiel Van Miltenburg, Emiel Wubben, Krahmer, 10.18653/v1/W19-8643Proceedings of the 12th International Conference on Natural Language Generation, Kees van Deemter. Chenghua Lin, Hiroya Takamura, the 12th International Conference on Natural Language Generation, Kees van DeemterTokyo, JapanAssociation for Computational Linguistics2019</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Prompting PaLM for Translation: Assessing Strategies and Performance. David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster, 10.18653/v1/2023.acl-long.859Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsAnna Rogers; Toronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li, arXiv:2111.02840[cs.CL]Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. 2022</p>
<p>ArchivalQA: A Large-scale Benchmark Dataset for Open-Domain Question Answering over Historical News Collections. Jiexin Wang, Adam Jatowt, Masatoshi Yoshikawa, 10.1145/3477495.3531734Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalMadrid, Spain; New York, NY, USAAssociation for Computing Machinery2022SIGIR '22)</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.17926[cs.CL]Large Language Models are not Fair Evaluators. 2023</p>
<p>Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, Guoyin Wang, arXiv:2304.10428[cs.CL]GPT-NER: Named Entity Recognition via Large Language Models. 2023</p>
<p>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?. Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel, arXiv:2204.05832[cs.CL]2022</p>
<p>Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection. William Yang, Wang , 10.18653/v1/P17-2067Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20172Short Papers)</p>
<p>for LLM Instruction Tuning Optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Wenjin Yao, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, The Twelfth International Conference on Learning Representations. 2024. June 20241PandaLM: An Automatic Evaluation Benchmark</p>
<p>Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Yitao Xiaojian (shawn) Ma, Liang, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Zengzhi Wang, Qiming Xie, Yi Feng, Zixiang Ding, Zinong Yang, Rui Xia, arXiv:2304.04339[cs.CL]Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. 2024</p>
<p>Finetuned Language Models are Zero-Shot Learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang, arXiv:2402.11924[cs.CL]MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition. 2024</p>
<p>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. Tongshuang Wu, Michael Terry, Carrie , 10.1145/3491102.3517582Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsNew Orleans, LA, USA; New York, NY, USA, ArticleAssociation for Computing MachineryJun Cai. 2022385CHI '22)</p>
<p>Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang, arXiv:2403.19114[cs.SE]Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM. 2024</p>
<p>Large-scale Cloze Test Dataset Created by Teachers. Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy, 10.18653/v1/D18-1257Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Fuzzy Deep Hybrid Network for Fake News Detection. Cheng Xu, M-Tahar Kechadi, 10.1145/3628797.3628971Proceedings of the 12th International Symposium on Information and Communication Technology (Ho Chi Min, Vietnam) (SOICT '23). the 12th International Symposium on Information and Communication Technology (Ho Chi Min, Vietnam) (SOICT '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Sentiment analysis using deep learning architectures: a review. Ashima Yadav, Dinesh Kumar, Vishwakarma , 10.1007/s10462-019-09794-5Artificial Intelligence Review. 532020. 2020</p>
<p>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, Ion Stoica, arXiv:2311.04850[cs.CL]2023</p>
<p>Editing Large Language Models: Problems, Methods, and Opportunities. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang, 10.18653/v1/2023.emnlp-main.632Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan, arXiv:2402.11894[cs.CL]Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation. 2024</p>
<p>Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev, 10.18653/v1/D18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang, arXiv:2404.06003[cs.CL]FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models. 2024</p>
<p>Learning Fair Representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, Proceedings of the 30th International Conference on Machine Learning (Proceedings of Machine Learning Research. Sanjoy Dasgupta, David Mcallester, the 30th International Conference on Machine Learning ( Machine Learning ResearchAtlanta, Georgia, USAPMLR2013. June 20241</p>
<p>Evaluating the Problem Solving Abilities of ChatGPT. Fankun Zeng, 10.7936/7vz0-dr082023</p>
<p>Prompting Large Language Model for Machine Translation: A Case Study. Biao Zhang, Barry Haddow, Alexandra Birch, PMLR, 41092-41110Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine Learning ( Machine Learning Research2023202</p>
<p>Deep learning for sentiment analysis: A survey. Lei Zhang, Shuai Wang, Bing Liu, 10.1002/widm.1253Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 8e12532018. 2018</p>
<p>Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, arXiv:2305.15005[cs.CL]Sentiment Analysis in the Era of Large Language Models: A Reality Check. 2023</p>
<p>Towards Generative Aspect-Based Sentiment Analysis. Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam, 10.18653/v1/2021.acl-short.64Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Short Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212</p>
<p>A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges. Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam, 10.1109/TKDE.2022.3230975IEEE Transactions on Knowledge and Data Engineering. 352023. 2023</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, arXiv:2308.01862[cs.CL]Wider and Deeper LLM Networks are Fairer LLM Evaluators. 2023</p>
<p>ChID: A Large-scale Chinese IDiom Dataset for Cloze Test. Chujie Zheng, Minlie Huang, Aixin Sun, 10.18653/v1/P19-1075Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Anna Korhonen, David Traum, Lluís Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.07197[cs.CL]Towards a Unified Multi-Dimensional Evaluator for Text Generation. 2022</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.01964[cs.CL]Don't Make Your LLM an Evaluation Benchmark Cheater. 2023</p>
<p>Xinyi Zhou, Reza Zafarani, 10.1145/3395046A Survey of Fake News: Fundamental Theories. 2020. sep 202053Article</p>
<p>DyVal: Graph-informed Dynamic Evaluation of Large Language Models. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, The Twelfth International Conference on Learning Representations. 2024</p>
<p>DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents. Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie, arXiv:2402.14865[cs.CL]2024</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, arXiv:2306.04528[cs.CL]Neil Zhenqiang Gong, and Xing Xie. 2023. PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. </p>
<p>JudgeLM : Fine-tuned Large Language Models are Scalable Judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, 2024</p>
<p>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li, arXiv:2304.04675[cs.CL]Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
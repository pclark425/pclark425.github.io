<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-081edae651e709e448bdd8a1f1b5760c7c7e1f53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/081edae651e709e448bdd8a1f1b5760c7c7e1f53" target="_blank">Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> This is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot, using a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM).</p>
                <p><strong>Paper Abstract:</strong> Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLATO-LTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PLATO with Long-Term Memory (PLATO-LTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play long-term persona memory module added to a large pre-trained generative dialogue model (PLATO-2/PLATO-FT) that extracts, stores, updates, and retrieves explicit persona sentences for both user and chatbot to improve long-term conversational consistency and engagingness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PLATO-LTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generative dialogue agent (based on PLATO-2 / PLATO-FT) augmented with two explicit long-term persona memories (user memory and chatbot memory). A Persona Extractor (PE) identifies persona sentences to write into memory; persona sentences are stored along with their dense vector encodings. At generation time a Context-Persona Matching (CPM) retriever uses learned encoders to score and retrieve top-k persona entries which are concatenated with the dialogue context for the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32L transformer (32-layer variant used in main experiments; exact parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit external persona memory / retrieval-augmented memory (dense vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>explicit persona sentences (raw text) stored together with pooled dense vector embeddings (encoder [CLS] representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: persona extractor (ERNIE-CNN) extracts persona clauses from conversation, de-duplicates by cosine similarity against stored persona vectors (replace if sim > s_dup=0.95) and stores {text, E_rho(text)}. Read: efficient dense-vector similarity search (cosine) using context encoder E_c(c) and persona encoder E_rho(·), scoring via cosine(sim), then top-k retrieval (k=5) and threshold filtering (s_c=0.7); retrieved persona strings are concatenated to context for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LeMon (Long-term Memory Conversation) task evaluated on the DuLeMon dataset (Chinese long-term mutual-persona dialogue dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term persona consistency / open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Human evaluation (self-chat setting): Consistency = 0.87 (scale 0-2), Coherence = 1.67, Engagingness = 1.54; CPM retrieval: AUC = 0.76, recall@5 = 0.83.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>PLATO-FT (no long-term memory, fine-tuned baseline) Human evaluation: Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40. (Used as direct baseline in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale); CPM retrieval metrics AUC and recall@k; also automatic metrics (PPL, BLEU, F1, DISTINCT) reported for generative variants.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>No numeric latency or memory-footprint benchmarks reported. Reported qualitative trade-offs: adding LTM substantially improves persona consistency and engagingness but finetuning on the small-scale DuLeMon dataset can slightly hurt overall coherence (generation appropriateness) compared to a larger pre-trained model; storing all history without filtering (no PE) yields weaker gains than using PE (i.e., need filtering to avoid noise). The system sets no hard memory capacity limit (relies on persona sparsity), which could imply unbounded growth if persona sparsity assumptions break.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Finetuning on a relatively small domain-specific dataset can reduce coherence on very open topics; using memory without persona extraction (i.e., writing all history) yields lower improvements (PLATO-LTM w/o PE has lower consistency and engagingness than PLATO-LTM with PE). The paper notes persona sparsity in dialogue (necessitating thresholding) and that retrieval relies on similarity thresholds (s_c) which may filter out weakly signaled but relevant persona items. No cross-session real-world deployment limitations (latency, privacy, or very large memory scaling) are quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022. Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (DuLeMon / LeMon task paper; ACL2022 submission / Baidu Research).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long Time No See! Open-Domain Conversation with Long-Term Persona Memory', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6595.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6595.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLATO-LTM w/o PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PLATO-LTM without Persona Extractor (w/o PE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of PLATO-LTM that writes all historical utterances (separately for user and bot) into memory without applying the Persona Extractor filter; uses same dense-vector retrieval and generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PLATO-LTM w/o PE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same generative backbone as PLATO-LTM but the write procedure stores all historical utterances into the long-term memory (no PE-based filtering). Retrieval and generation are identical to PLATO-LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32L transformer (same model family as PLATO-LTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit external persona memory / dense vector store (but populated with all history utterances)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw historical utterance texts plus their dense encodings</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Dense-vector similarity read (cosine) using E_c and E_rho; write inserts all utterances (deduplication still applied via similarity threshold s_dup=0.95 per description).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LeMon task on DuLeMon dataset (same evaluation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term persona consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Human evaluation: Consistency = 0.49 (scale 0-2), Coherence = 1.57, Engagingness = 1.43.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared against PLATO-FT (no memory) Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Writing all history (no PE) increases noise in memory which reduces the benefit compared to using PE; still better than PLATO-FT for consistency but inferior to PLATO-LTM with PE. No quantitative latency/memory-cost numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without persona extraction, stored memory contains irrelevant or noisy lines, leading to smaller gains in consistency/engagingness compared to PE-enabled variant; highlights need for filtering/selection before writing to memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022. Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (DuLeMon / LeMon task paper; ACL2022 submission / Baidu Research).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long Time No See! Open-Domain Conversation with Long-Term Persona Memory', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6595.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6595.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLATO-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PLATO fine-tuned on DuLeMon (PLATO-FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PLATO-2 generative dialogue model fine-tuned on the DuLeMon dataset (no long-term memory component); used as a baseline to measure the effect of adding LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PLATO-2: towards building an open-domain chatbot via curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PLATO-FT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PLATO-2 pre-trained generative transformer model further fine-tuned on DuLeMon data; includes role embedding and role token strategies in some experiments but does not include an external long-term memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Experiments run with 12L and 32L PLATO variants (32L reported for main comparisons); exact parameter counts not stated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LeMon task on DuLeMon dataset (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term persona consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (no memory used). Automatic metrics for PLATO-FT 32L + role_embed + role_token: PPL = 9.380, BLEU-1/2 = 0.194 / 0.087, DISTINCT-1/2 = 0.068 / 0.296, F1 = 22.61 (automatic metrics reported for generative variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Human evaluation (no-memory baseline): Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale) and standard automatic metrics (PPL, BLEU, DISTINCT, F1).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine-tuning on the smaller DuLeMon dataset improved persona usage metrics when combined with LTM but slightly degraded coherence relative to the large pre-trained PLATO-2 (authors attribute this to domain shift from pretraining to small finetuning data).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Finetuning without external memory does not sufficiently improve long-term persona consistency (low Consistency score); finetuning may reduce general coherence on wide-ranging open-domain topics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. PLATO-2: towards building an open-domain chatbot via curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long Time No See! Open-Domain Conversation with Long-Term Persona Memory', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6595.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6595.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic memory architectures (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based / episodic memory architectures for dialogue (e.g., Kim et al. 2014; Bang et al. 2015; Elvir et al. 2017; Campos et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior dialogue systems (mostly rule-based or agent architectures) that use episodic conversational memory to extract, store, and manage facts from prior interactions to be reused in future dialogs; mentioned in related work but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>episodic memory dialogue architectures (prior systems)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Rule-based or engineered memory modules that store user-related facts or shared history (episodic memory) and use retrieval or rewriting strategies to incorporate them into generated responses; approaches vary across papers cited.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not applicable / not specified (prior works are not large pre-trained LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory / structured memory (task-specific rule-based stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>explicit facts or structured entries (e.g., user attributes, summary facts, documented history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>hand-crafted read/write processes, rule-based retrieval and rewriting (not deep dense-retrieval based in the cited examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Personalized dialogue / long-term conversation (general prior dialogue tasks and agent evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Cited works note challenges leveraging shared history at scale and accommodating conversational coordination patterns; older rule-based systems lack large-scale pretraining advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior works are mostly rule-based, hard to scale or to integrate with large pre-trained generative models; authors note these systems found it challenging to leverage shared history with individuals and to scale to diverse conversational patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Mentioned prior works: Yonghee Kim et al. 2014; Jeesoo Bang et al. 2015; Miguel Elvir et al. 2017; Joana Campos et al. 2018 (see references in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long Time No See! Open-Domain Conversation with Long-Term Persona Memory', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6595.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6595.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSC / retrieval-augmented models (Xu et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Session Chat (MSC) dataset and retrieval-augmented generative dialogue approach (Xu et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced retrieval-augmented generative dialogue approach trained on the MSC multi-session dataset that summarizes and recalls previous conversations; cited as related prior work with different memory properties from DuLeMon/PLATO-LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond goldfish memory: Long-term open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>retrieval-augmented generative models (MSC line)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained generative dialogue models augmented by retrieval of stored conversation summaries/documents from multi-session datasets; MSC stores documents summarizing past sessions and retrieves them for generation but those stored documents are not dynamically modified in their formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies in cited work (not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (document store of session summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored documents / session summaries (text fragments) that accumulate across sessions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval of stored documents/summaries and conditioning generation on retrieved documents; stored documents in MSC are static (not dynamically modified per the cited description)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MSC multi-session conversation task / long-session conversation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors note MSC-style stored documents will increase infinitely as the conversation progresses and are not dynamically modified, and training such retrieval-augmented models requires long-session datasets which are costly to collect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited limitations: stored documents are static (no dynamic updates), unbounded growth of stored items, and reliance on long-session datasets for training which are expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long Time No See! Open-Domain Conversation with Long-Term Persona Memory', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Beyond goldfish memory: Long-term open-domain conversation <em>(Rating: 2)</em></li>
                <li>Example-based chat-oriented dialogue system with personalized long-term memory <em>(Rating: 1)</em></li>
                <li>Remembering a conversation - a conversational memory architecture for embodied conversational agents <em>(Rating: 1)</em></li>
                <li>Challenges in exploiting conversational memory in human-agent interaction <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6595",
    "paper_id": "paper-081edae651e709e448bdd8a1f1b5760c7c7e1f53",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "PLATO-LTM",
            "name_full": "PLATO with Long-Term Memory (PLATO-LTM)",
            "brief_description": "A plug-and-play long-term persona memory module added to a large pre-trained generative dialogue model (PLATO-2/PLATO-FT) that extracts, stores, updates, and retrieves explicit persona sentences for both user and chatbot to improve long-term conversational consistency and engagingness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PLATO-LTM",
            "agent_description": "Generative dialogue agent (based on PLATO-2 / PLATO-FT) augmented with two explicit long-term persona memories (user memory and chatbot memory). A Persona Extractor (PE) identifies persona sentences to write into memory; persona sentences are stored along with their dense vector encodings. At generation time a Context-Persona Matching (CPM) retriever uses learned encoders to score and retrieve top-k persona entries which are concatenated with the dialogue context for the generator.",
            "model_size": "32L transformer (32-layer variant used in main experiments; exact parameter count not specified)",
            "memory_used": true,
            "memory_type": "explicit external persona memory / retrieval-augmented memory (dense vector store)",
            "memory_representation": "explicit persona sentences (raw text) stored together with pooled dense vector embeddings (encoder [CLS] representations)",
            "memory_access_mechanism": "Write: persona extractor (ERNIE-CNN) extracts persona clauses from conversation, de-duplicates by cosine similarity against stored persona vectors (replace if sim &gt; s_dup=0.95) and stores {text, E_rho(text)}. Read: efficient dense-vector similarity search (cosine) using context encoder E_c(c) and persona encoder E_rho(·), scoring via cosine(sim), then top-k retrieval (k=5) and threshold filtering (s_c=0.7); retrieved persona strings are concatenated to context for generation.",
            "task_name": "LeMon (Long-term Memory Conversation) task evaluated on the DuLeMon dataset (Chinese long-term mutual-persona dialogue dataset)",
            "task_category": "dialogue / long-term persona consistency / open-domain conversation",
            "performance_with_memory": "Human evaluation (self-chat setting): Consistency = 0.87 (scale 0-2), Coherence = 1.67, Engagingness = 1.54; CPM retrieval: AUC = 0.76, recall@5 = 0.83.",
            "performance_without_memory": "PLATO-FT (no long-term memory, fine-tuned baseline) Human evaluation: Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40. (Used as direct baseline in paper.)",
            "has_comparative_results": true,
            "performance_metric": "Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale); CPM retrieval metrics AUC and recall@k; also automatic metrics (PPL, BLEU, F1, DISTINCT) reported for generative variants.",
            "tradeoffs_reported": "No numeric latency or memory-footprint benchmarks reported. Reported qualitative trade-offs: adding LTM substantially improves persona consistency and engagingness but finetuning on the small-scale DuLeMon dataset can slightly hurt overall coherence (generation appropriateness) compared to a larger pre-trained model; storing all history without filtering (no PE) yields weaker gains than using PE (i.e., need filtering to avoid noise). The system sets no hard memory capacity limit (relies on persona sparsity), which could imply unbounded growth if persona sparsity assumptions break.",
            "limitations_or_failure_cases": "Finetuning on a relatively small domain-specific dataset can reduce coherence on very open topics; using memory without persona extraction (i.e., writing all history) yields lower improvements (PLATO-LTM w/o PE has lower consistency and engagingness than PLATO-LTM with PE). The paper notes persona sparsity in dialogue (necessitating thresholding) and that retrieval relies on similarity thresholds (s_c) which may filter out weakly signaled but relevant persona items. No cross-session real-world deployment limitations (latency, privacy, or very large memory scaling) are quantified.",
            "citation": "Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022. Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (DuLeMon / LeMon task paper; ACL2022 submission / Baidu Research).",
            "uuid": "e6595.0",
            "source_info": {
                "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PLATO-LTM w/o PE",
            "name_full": "PLATO-LTM without Persona Extractor (w/o PE)",
            "brief_description": "A variant of PLATO-LTM that writes all historical utterances (separately for user and bot) into memory without applying the Persona Extractor filter; uses same dense-vector retrieval and generation pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PLATO-LTM w/o PE",
            "agent_description": "Same generative backbone as PLATO-LTM but the write procedure stores all historical utterances into the long-term memory (no PE-based filtering). Retrieval and generation are identical to PLATO-LTM.",
            "model_size": "32L transformer (same model family as PLATO-LTM)",
            "memory_used": true,
            "memory_type": "explicit external persona memory / dense vector store (but populated with all history utterances)",
            "memory_representation": "raw historical utterance texts plus their dense encodings",
            "memory_access_mechanism": "Dense-vector similarity read (cosine) using E_c and E_rho; write inserts all utterances (deduplication still applied via similarity threshold s_dup=0.95 per description).",
            "task_name": "LeMon task on DuLeMon dataset (same evaluation protocol)",
            "task_category": "dialogue / long-term persona consistency",
            "performance_with_memory": "Human evaluation: Consistency = 0.49 (scale 0-2), Coherence = 1.57, Engagingness = 1.43.",
            "performance_without_memory": "Compared against PLATO-FT (no memory) Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40.",
            "has_comparative_results": true,
            "performance_metric": "Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale).",
            "tradeoffs_reported": "Writing all history (no PE) increases noise in memory which reduces the benefit compared to using PE; still better than PLATO-FT for consistency but inferior to PLATO-LTM with PE. No quantitative latency/memory-cost numbers provided.",
            "limitations_or_failure_cases": "Without persona extraction, stored memory contains irrelevant or noisy lines, leading to smaller gains in consistency/engagingness compared to PE-enabled variant; highlights need for filtering/selection before writing to memory.",
            "citation": "Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022. Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (DuLeMon / LeMon task paper; ACL2022 submission / Baidu Research).",
            "uuid": "e6595.1",
            "source_info": {
                "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PLATO-FT",
            "name_full": "PLATO fine-tuned on DuLeMon (PLATO-FT)",
            "brief_description": "PLATO-2 generative dialogue model fine-tuned on the DuLeMon dataset (no long-term memory component); used as a baseline to measure the effect of adding LTM.",
            "citation_title": "PLATO-2: towards building an open-domain chatbot via curriculum learning",
            "mention_or_use": "use",
            "agent_name": "PLATO-FT",
            "agent_description": "PLATO-2 pre-trained generative transformer model further fine-tuned on DuLeMon data; includes role embedding and role token strategies in some experiments but does not include an external long-term memory module.",
            "model_size": "Experiments run with 12L and 32L PLATO variants (32L reported for main comparisons); exact parameter counts not stated in paper.",
            "memory_used": false,
            "memory_type": "",
            "memory_representation": "",
            "memory_access_mechanism": "",
            "task_name": "LeMon task on DuLeMon dataset (baseline)",
            "task_category": "dialogue / long-term persona consistency",
            "performance_with_memory": "N/A (no memory used). Automatic metrics for PLATO-FT 32L + role_embed + role_token: PPL = 9.380, BLEU-1/2 = 0.194 / 0.087, DISTINCT-1/2 = 0.068 / 0.296, F1 = 22.61 (automatic metrics reported for generative variants).",
            "performance_without_memory": "Human evaluation (no-memory baseline): Consistency = 0.40, Coherence = 1.59, Engagingness = 1.40.",
            "has_comparative_results": true,
            "performance_metric": "Human evaluation scores (Coherence, Consistency, Engagingness on 0-2 scale) and standard automatic metrics (PPL, BLEU, DISTINCT, F1).",
            "tradeoffs_reported": "Fine-tuning on the smaller DuLeMon dataset improved persona usage metrics when combined with LTM but slightly degraded coherence relative to the large pre-trained PLATO-2 (authors attribute this to domain shift from pretraining to small finetuning data).",
            "limitations_or_failure_cases": "Finetuning without external memory does not sufficiently improve long-term persona consistency (low Consistency score); finetuning may reduce general coherence on wide-ranging open-domain topics.",
            "citation": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. PLATO-2: towards building an open-domain chatbot via curriculum learning.",
            "uuid": "e6595.2",
            "source_info": {
                "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Episodic memory architectures (prior work)",
            "name_full": "Rule-based / episodic memory architectures for dialogue (e.g., Kim et al. 2014; Bang et al. 2015; Elvir et al. 2017; Campos et al. 2018)",
            "brief_description": "Prior dialogue systems (mostly rule-based or agent architectures) that use episodic conversational memory to extract, store, and manage facts from prior interactions to be reused in future dialogs; mentioned in related work but not evaluated in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "episodic memory dialogue architectures (prior systems)",
            "agent_description": "Rule-based or engineered memory modules that store user-related facts or shared history (episodic memory) and use retrieval or rewriting strategies to incorporate them into generated responses; approaches vary across papers cited.",
            "model_size": "not applicable / not specified (prior works are not large pre-trained LMs)",
            "memory_used": true,
            "memory_type": "episodic memory / structured memory (task-specific rule-based stores)",
            "memory_representation": "explicit facts or structured entries (e.g., user attributes, summary facts, documented history)",
            "memory_access_mechanism": "hand-crafted read/write processes, rule-based retrieval and rewriting (not deep dense-retrieval based in the cited examples)",
            "task_name": "Personalized dialogue / long-term conversation (general prior dialogue tasks and agent evaluations)",
            "task_category": "dialogue / personalization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Cited works note challenges leveraging shared history at scale and accommodating conversational coordination patterns; older rule-based systems lack large-scale pretraining advantages.",
            "limitations_or_failure_cases": "Prior works are mostly rule-based, hard to scale or to integrate with large pre-trained generative models; authors note these systems found it challenging to leverage shared history with individuals and to scale to diverse conversational patterns.",
            "citation": "Mentioned prior works: Yonghee Kim et al. 2014; Jeesoo Bang et al. 2015; Miguel Elvir et al. 2017; Joana Campos et al. 2018 (see references in paper).",
            "uuid": "e6595.3",
            "source_info": {
                "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "MSC / retrieval-augmented models (Xu et al., 2021)",
            "name_full": "Multi-Session Chat (MSC) dataset and retrieval-augmented generative dialogue approach (Xu et al., 2021)",
            "brief_description": "Referenced retrieval-augmented generative dialogue approach trained on the MSC multi-session dataset that summarizes and recalls previous conversations; cited as related prior work with different memory properties from DuLeMon/PLATO-LTM.",
            "citation_title": "Beyond goldfish memory: Long-term open-domain conversation",
            "mention_or_use": "mention",
            "agent_name": "retrieval-augmented generative models (MSC line)",
            "agent_description": "Pretrained generative dialogue models augmented by retrieval of stored conversation summaries/documents from multi-session datasets; MSC stores documents summarizing past sessions and retrieves them for generation but those stored documents are not dynamically modified in their formulation.",
            "model_size": "varies in cited work (not specified in this paper)",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (document store of session summaries)",
            "memory_representation": "stored documents / session summaries (text fragments) that accumulate across sessions",
            "memory_access_mechanism": "retrieval of stored documents/summaries and conditioning generation on retrieved documents; stored documents in MSC are static (not dynamically modified per the cited description)",
            "task_name": "MSC multi-session conversation task / long-session conversation evaluation",
            "task_category": "dialogue / long-term memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Authors note MSC-style stored documents will increase infinitely as the conversation progresses and are not dynamically modified, and training such retrieval-augmented models requires long-session datasets which are costly to collect.",
            "limitations_or_failure_cases": "Cited limitations: stored documents are static (no dynamic updates), unbounded growth of stored items, and reliance on long-session datasets for training which are expensive.",
            "uuid": "e6595.4",
            "source_info": {
                "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Beyond goldfish memory: Long-term open-domain conversation",
            "rating": 2
        },
        {
            "paper_title": "Example-based chat-oriented dialogue system with personalized long-term memory",
            "rating": 1
        },
        {
            "paper_title": "Remembering a conversation - a conversational memory architecture for embodied conversational agents",
            "rating": 1
        },
        {
            "paper_title": "Challenges in exploiting conversational memory in human-agent interaction",
            "rating": 2
        }
    ],
    "cost": 0.0137305,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</h1>
<p>Xinchao $\mathbf{X u}^{1 <em>}$, Zhibin Gou ${ }^{1,2 </em>}$, Wenquan $\mathbf{W u}^{1}$, Zheng-Yu Niu ${ }^{1}$, Hua Wu ${ }^{1}$, Haifeng Wang ${ }^{1}$ and Shihang Wang ${ }^{3}$<br>${ }^{1}$ Baidu Inc., China<br>${ }^{2}$ School of Computer Science, Beijing University of Posts and Telecommunications<br>${ }^{3}$ Columbia University<br>{xinchaoxu, wuwenquan01,niuzhengyu, wu_hua,wanghaifeng}@baidu.com zebgou@gmail.com, sw3275@columbia.edu</p>
<h4>Abstract</h4>
<p>Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework PLATO-LTM with a Long-Term Memory (LTM) mechanism. This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Persona is crucial for open-domain dialogue systems to establish long-term intimacy with users (Huang et al., 2020). Existing persona dialogue datasets such as PersonaChat (Zhang et al., 2018; Dinan et al., 2019) and models (Li et al., 2016a; Zhang et al., 2017; Qian et al., 2018) have greatly facilitated the chatbot with configurable and persistent personalities.</p>
<p>Nevertheless, current open-domain dialogue systems still cannot build a long-term connection with humans. The possible reason is that they lack the capability of understanding and memorizing longterm dialogue history information, which we called</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A sample of long-term conversation with memory. At first, the chat partner is not familiar with each other, so the goal is to get to know each other; Then, after multiple sessions, the chatbot already has a certain understanding and memory of the user's persona and its own persona, making the deep chat possible.
long-term persona ability. Remembering and actively utilizing the user's persona increases engagingness and contributes to long-term friendships between chatbot and user (Campos et al., 2018). Without this ability, the current state-of-the-art models, such as Meena (Adiwardana et al., 2020), Blender (Roller et al., 2021), and PLATO (Bao et al., 2020), tend to talk to people like strangers in long-term conversations.</p>
<p>Despite the importance and challenge of utilizing long-term persona in open-domain dialogue, as far as we know, the long-term persona ability of large-scale models is less studied due to a lack of both task design and corresponding dataset. Previous long-term persona dialogue systems (Kim et al., 2014; Bang et al., 2015) are mainly rule-based systems without large-scale pre-training models, in which researchers proposed various episodic memory architectures to extract, store and manage rel-</p>
<p>evant facts in prior interactions for use in future dialogs (Campos et al., 2018).</p>
<p>In addition, existing persona conversation datasets (Zhang et al., 2018; Dinan et al., 2019; Zheng et al., 2019) focus only on the consistency of the chatbot's own persona and ignore the memory and utilization of the user's persona. And they all set fixed persona that cannot be updated during the chat. Recently, Xu et al. (2021) proposed MSC dataset as a multi-session extension of PersonaChat, and its sessions are additionally annotated with summaries of important personal points. Similar to the previous episodic memory architecture, Xu et al. (2021) summarize and recall previous conversations for future dialogue generation. The stored documents in MSC will not be dynamically modified and will increase infinitely as the conversation progresses. Furthermore, the retrieval-augmented generative models rely on a long-session conversation dataset for training, which is expensive and difficult to annotate.</p>
<p>To address the limitations of existing models and the above issues, we defines the LeMon (Longterm Memory Conversation) task and propose a new dataset named DuLeMon, which focuses not only on the consistency of the bot's own persona but also on the active construction and utilization of the user's persona in a long-term interaction (ie. mutual persona). We demonstrate an example dialogue in DuLeMon in Figure 1. In DuLeMon, we assume that the two speakers have previously interacted with each other and that the chatbot remembers part of the user's persona. Besides, both the user and chatbot grounding persona are annotated in each utterance.</p>
<p>Based on our collected dataset, we carefully design a novel PLATO-LTM framework for the longterm persona dialogue setting by adding a plug-andplay long-term memory (LTM) to the state-of-theart open-domain dialogue model (Bao et al., 2020). It enables us to study long-term persona conversations without relying on the long-session dataset. PLATO-LTM can extract both parties' persona information from the conversation in real time, write it to persona memory respectively, and retrieve both parties' persona information from memory to generate responses. The PLATO-LTM framework consists of three modules: (1) Persona Extractor (PE): The memory is updated by filtering irrelevant information and extracting persona sentences through a classifier. (2) Long-Term Memory (LTM): Two
separated long-term memories store the explicit persona information of interlocutors. (3) Generation Module: We use the large-scale model and the retrieved persona sentences of the user and chatbot are directly concatenated with dialogue context as model input.</p>
<p>Our major contributions are as follows:
(1) We firstly propose the long-term persona chat task LeMon for Chinese long-term conversations. Our proposed DuLeMon dataset is also the largest multi-turn Chinese mutual persona chat dataset currently available.
(2) We proposed a PLATO-LTM framework that extracts and remembers both user's and the chatbot's persona in real time, enabling the chatbot to have long-term persona dialogue without training on long-session data.
(3) Automatic and human evaluation show that our method significantly improves the consistency of the state-of-the-art in long conversations, making the response more engaging while ensuring coherency.</p>
<h2>2 Related Work</h2>
<p>Persona Dialogue: As described in Huang et al. (2020), there is much work related to persona dialogue. Generally speaking, these works can be divided into implicit persona models and explicit persona models. In the implicit model, the persona is represented in the form of the semantic persona vector. Kim et al. (2014) proposed a retrieval-based method to integrate persona and user interests into the dialogue system. Because these models are implicit methods, they are not easy to interpret and control in target response generation. In Qian et al. (2018), an explicit persona model is proposed to generate consistent responses for given persona information. The persona information of the machine includes name, gender, hobbies, and so on. In this way, the given persona information can be better used for model generation. There are also many persona chat datasets that have been constructed to develop models, as shown in Table 1. In particular, the introduction of the PersonaChat (Zhang et al., 2018; Dinan et al., 2019) dataset has extensively promoted the development of this field where the crowd-workers are simply asked to "chat with the other person naturally and try to get to know each other." However, the user's persona was unknown</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Persona</th>
<th style="text-align: left;">Mutual</th>
<th style="text-align: left;"># Dialogues</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">Multi-turn</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PersonaChat (Zhang et al., 2018)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">10,907</td>
<td style="text-align: left;">English</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">PersonaIDialog (Zheng et al., 2019)</td>
<td style="text-align: left;">Structure</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$20,830,000$</td>
<td style="text-align: left;">Chinese</td>
<td style="text-align: left;">part</td>
</tr>
<tr>
<td style="text-align: left;">XPersona (Lin et al., 2020)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">16,878</td>
<td style="text-align: left;">Multilingual</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">PEC (Zhong et al., 2020)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">355,000</td>
<td style="text-align: left;">English</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">PCR (Mazaré et al., 2018)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$700,000,000$</td>
<td style="text-align: left;">English</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">MSC (Xu et al., 2021)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">5,001</td>
<td style="text-align: left;">English</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">DuLeMon (Ours)</td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">27,501</td>
<td style="text-align: left;">Chinese</td>
<td style="text-align: left;">Yes</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of our dataset DuLeMon with other datasets.
to the bot, so the dialogue was like strangers exchanging information. In contrast, our proposed DuLeMon dataset requires the chatbot to actively remember and use the user's persona to improve conversational engagements and increase the intimacy between interlocutors in long-term interactions.
Dialogue Model with External Memory: As described in Lim (2012), there are various memory models used by the rule-based dialogue systems. In Bang et al. (2015), user-related information is memorized and used to rewrite the response. In Elvir et al. (2017), a unified episodic memory architecture for Embodied Conversational Agents (ECAs) is proposed. They describe a process that determines the prevalent contexts in the conversations obtained from the interactions. In Campos et al. (2018), the authors introduce an agent that uses its conversational memory to revisit shared history with users to maintain a coherent social relationship over time. However, they find it challenging to leverage the shared history with individual users and hard to accommodate expected conversational coordination patterns. Apart from studies in rulebased dialogue systems mentioned above, Xu et al. (2021) shows how large-scale pre-training generative dialogue models trained on existing datasets perform poorly in the long-term conversation setting and proposes a new extended English conversation dataset, entitled Multi-Session Chat (MSC). Different from them, our novel dataset DuLeMon does not rely on long sessions with high collection costs to study long-term memory problems in the persona chat, with significant differences in task design and data collection.</p>
<h2>3 Data Collection</h2>
<p>Task Definition. Given dialogue context $c=$ $\left{u_{1}, s_{1}, u_{2}, s_{2}, \ldots, u_{t-1}, s_{t-1}, u_{t}\right}$, where $u$ and $s$ represent the user and the chatbot respectively. Each speaker has its corresponding persona descrip-
tion that consists of a set of sentences, we define the user persona as $\rho^{u}=\left{\rho_{1}^{u}, \rho_{2}^{u}, \ldots, \rho_{m}^{u}\right}$, and the chatbot persona as $\rho^{s}=\left{\rho_{1}^{s}, \rho_{2}^{s}, \ldots, \rho_{n}^{s}\right}$. Given the dialogue context $c$, user persona $\rho^{u}$ and chatbot persona $\rho^{s}$, we are interested in finding the corresponding persona and predicting the chatbot response $s_{t}$.</p>
<p>To support our task, we collect and release a new dataset, entitled DuLeMon. In DuLeMon, the chatbot actively remembers and reasonably uses what the user has said about their persona while maintaining consistency in its persona, allowing the conversation to proceed more deeply. In a nutshell, our DuLeMon dataset has two essential features: During the conversation, the chatbot can see the persona of both parties; the other is that the persona associated with the response is explicitly annotated in our dataset. Unlike the PersonaChat dataset, the setting in DuLeMon is that one speaker plays the role of a chatbot, and the other plays the user's role. We elaborate on the construction process of the dataset as the following.
(1) Persona collection: The persona is mainly from the translation and rewriting of persona in PersonaChat. The chatbot's persona is only visible to itself, and the chatbot can use its persona information to chat with the user, as shown in Figure 2. The user's persona contains two parts: persona that the chatbot already knows and persona that the chatbot does not know. The first part is the user's persona that the chatbot has learned through historical conversations. This part is randomly selected from multiple personas of each user. The chatbot needs to use this information to guide the conversation during the chat process. It should be noted that in order to simulate the situation at the beginning of the chat, this part may be empty.
(2) Dialogue collection: For each dialogue, two crowd-workers (one plays the chatbot, the other plays the user) are randomly paired and given random persona. They are required to organize a di-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of our proposed DuLeMon dataset with both chatbot's and user's persona. It has two important features: one is that during the conversation, the chatbot can see the persona of both parties; the other is that the persona information associated with the response is explicitly labeled in our dataset which is shown as the $\rho^{a}$ and $\rho^{a}$ in the figure.
alogue based on the given persona. The chatbot should think more about chatting to make it go on. It should utilize the known user's persona to conduct the in-depth chat. The user will act as an ordinary user to cooperate with the conversation. The content of the chat can be selected from the given persona. It must not be irrelevant for the given information, nor can it conflict with the given persona.
(3) Persona Grounding Labeling: This part annotates whether the current response uses the given persona information and whether the current sentence is a persona sentence. For each utterance, we first let the annotators label whether it uses persona or not. Furthermore, the annotator should label the grounding persona (from chatbot or user) being used in the response. Therefore, through this process, the direct relationship between the response and the persona can be given. Then, for sentences that use the persona, we further annotate whether the utterance is a persona sentence or not.</p>
<p>To scale the amount of data, we also collected conversations where the user's persona was not visible to the bot, following the PersonaChat (Zhang et al., 2018). Finally, our DuLeMon dataset consists of two parts. In DeLeMon-SELF, the bot only knows its own persona, while in DuLeMonBOTH, it also knows part of the user's persona (as described above). The overall statistics of the DuLeMon are shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">SELF</th>
<th style="text-align: left;">BOTH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Dialogues</td>
<td style="text-align: left;">24500</td>
<td style="text-align: left;">3001</td>
</tr>
<tr>
<td style="text-align: left;"># Utterances</td>
<td style="text-align: left;">400472</td>
<td style="text-align: left;">48522</td>
</tr>
<tr>
<td style="text-align: left;">Avg. # turns</td>
<td style="text-align: left;">16.3</td>
<td style="text-align: left;">16.2</td>
</tr>
<tr>
<td style="text-align: left;">Avg. length of utterances</td>
<td style="text-align: left;">19.7</td>
<td style="text-align: left;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">Avg. # bot persona</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">Avg. # user persona (seen)</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">4.4</td>
</tr>
<tr>
<td style="text-align: left;">Avg. # user persona (unseen)</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">1.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of DuLeMon.</p>
<h2>4 Model Architecture</h2>
<p>In this work, we propose a long-term memory dialogue system based on an explicit memory readwrite mechanism. It includes three parts: persona extractor, long-term persona memory, and generation module. Through the read and write operations of the long-term memory module, the user's and chatbot's persona can be stored, updated, and read. The overall framework is shown in Figure3.</p>
<h3>4.1 Persona Extractor</h3>
<p>Given an utterance or text span as input, our persona extractor can assign each input a label to indicate if it contains persona information. Here we train an ERNIE-CNN network architecture in a supervised way on an annotated persona-utterance dataset as this persona extractor. Specifically, the ERNIE-CNN network employs a pre-trained ERNIE $^{2}$ (Sun et al., 2019) network for sentence representation, and another CNN model (Kim, 2014)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of our system PLATO-LTM. (a) shows the dialogue flow. (b) describes the modules and pipeline of our system. It consists of a persona extractor (PE), a long-term persona memory, a retriever, and a generator. (1) The long-term memory contains both user persona and chatbot persona extracted from the dialogue history by PE. (2) The retriever uses context as query to retrieve related personas in memory (3) concatenates the retrieved text to the context and use the generator to produce the generated response. (c) details our generator PLATO-2 and ranker CPM (Context Persona Matching).
for classification.
Training procedure. First, we collect the firstversion training dataset, in which there are 6 k utterances (from the DuLeMon corpus and Chinese social forum corpus) being human-annotated with positive or negative class labels. Second, using the aforementioned dataset, we train five ERNIE-CNN network (with different pre-training parameter versions) based models (called pc-stage1). Third, we employ these five models to automatically annotate 1.4 million utterances with labels, where these utterances are collected from the DuLeMon and the online Chinese social forum. We then refine this augmented dataset as the final-version dataset with the following steps: (a) Given an utterance, if there are at least two of the above five models identifying it as a positive sample, then it is attached with a positive label, (b) otherwise it is attached with a negative label. Finally, we train the five models on the final-version dataset and select the one with the best performance as our persona extractor (named pc-stage2).</p>
<p>Inference procedure. First, given an utterance, we segment it into clauses with the use of punctuation marks. Second, we use the persona extractor
mentioned above to classify each clause with a label and then collect the clause with a positive label as persona sentences.</p>
<h3>4.2 Long-Term Memory</h3>
<p>The long-term memory (LTM) module maintains memories to store the historical persona information from the user and the chatbot, respectively. The most critical operations are reading and writing based on the context persona matching (CPM) model. We use context encoder $E_{c}(\cdot)$ to encode the current context $c$, and use persona encoder $E_{\rho}(\cdot)$ to encode the persona $\rho_{i} . E(\cdot)$ is the encoder's output on the first input token ([CLS]), corresponding to the input's pooled representation.</p>
<p>The encoder $E_{c}$ and $E_{\rho}$ is initialized with the ERNIE model and then trained on our DuLeMon corpus. For each training sample, we define the positive persona as the persona used in the current user's utterance and the bot's response (including bot persona and user persona seen by bot), and the negative persona as the remaining persona of the current session. Given context $c$, a positive persona $\rho^{+}$, and a negative persona $\rho^{-}$, we use triplet loss</p>
<p>to tune the network as:</p>
<p>$$
\max \left(\operatorname{sim}\left(c, \rho^{+}\right)-\operatorname{sim}\left(c, \rho^{-}\right)+\alpha, 0\right)
$$</p>
<p>We set the margin $\alpha=0.2$ in our experiments. Below we describe the specific read and write process of the long-term memory module.</p>
<p>Write: We use the PE module to identify the persona in the dialogue history as the candidate information to be written. It needs to eliminate duplicates before writing. Specifically, calculate the cosine similarity with the persona in memory to get the most approximate persona $\rho_{j}$. When the similarity between $\rho_{i}$ and $\rho_{j}$ exceeds the given duplication threshold $s_{\text {dup }}$, replace $\rho_{j}$ in memory with $\rho_{i}$; otherwise, write $\rho_{i}$ directly into the memory. When writing to memory, save $\left{\rho_{i}, E_{\rho}\left(\rho_{i}\right)\right}$ pair for the subsequent reading. We measure the distance with the cosine similarity as:</p>
<p>$$
\operatorname{sim}\left(\rho_{i}, \rho_{j}\right)=\cos \left(E_{\rho}\left(\rho_{i}\right), E_{\rho}\left(\rho_{j}\right)\right)
$$</p>
<p>Read: The reading process can be regarded as the retrieval process from memory. First, we use the efficient similarity search of dense vectors to select candidates. Then a matching model is utilized to score the relevance of the candidates to the current context. The similarity between the context and the persona using cosine similarity:</p>
<p>$$
\operatorname{sim}\left(c, \rho_{i}\right)=\cos \left(E_{c}(c), E_{\rho}\left(\rho_{i}\right)\right)
$$</p>
<p>The top $k$ persona candidates $\rho^{u}$ in the user memory and top $k$ candidates $\rho^{s}$ in the chatbot memory are used for response generation. To model persona sparsity in dialogue, we filter out the persona, whose similarity score is lower than the similarity threshold $s_{c}$.</p>
<h3>4.3 Generation Module</h3>
<p>We trained our model on the basis of the PLATO2 (Bao et al., 2020) architecture which adopts the generic transformer language model (Vaswani et al., 2017) and leverages a stack of masked multi-head self-attention layers to train on massive dialogue data ${ }^{3}$.</p>
<p>Given the conversation context $c=$ $\left{u_{1}, s_{1}, u_{2}, s_{2}, \ldots, u_{t-1}, s_{t-1}, u_{t}\right}$, the corresponding user persona $\rho^{u}$ and chatbot persona $\rho^{s}$, the ground truth response as</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$r=\left{x_{m+1}, x_{m+2}, \ldots, x_{N}\right}$, the conditional probability of $p\left(r \mid c, \rho^{u}, \rho^{s}\right)$ can be written as the product of a series of conditional probabilities:</p>
<p>$$
p\left(r \mid c, \rho^{u}, \rho^{s}\right)=\prod_{t}^{N} p\left(r_{t} \mid c, \rho^{u}, \rho^{s}, r_{&lt;t}\right)
$$</p>
<p>Therefore, we need to minimize the following negative log-likelihood (NLL) loss:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em t="1">{N L L} &amp; =-\mathbb{E} \log p\left(r \mid c, \rho^{u}, \rho^{s}\right) \
&amp; =-\mathbb{E} \sum</em>\right)
\end{aligned}
$$}^{T} \log p\left(r_{t} \mid c, \rho^{u}, \rho^{s}, r_{&lt;t</p>
<p>where $T$ is the length of the target response $r$ and $r_{&lt;t}$ denotes previously generated words. Since the response generation is a uni-directional decoding process, each token in the response only attends to those before it. As for the context, bi-directional attention is enabled for better natural language understanding.</p>
<p>We added two strategies to distinguish different roles in the dialogue and prevent the confusing use of persona information.</p>
<ul>
<li>Role Embedding (Bao et al., 2021): different role embedding is used to distinguish the persona of different chat parties, abbreviated role_embed.</li>
<li>Role Token: splicing "system persona" before the chatbot persona and "user persona" before the user persona, abbreviated role_token.</li>
</ul>
<h2>5 Experiments</h2>
<p>In this section, we present the baselines, experiment settings, model comparisons, and results of experiments.</p>
<h3>5.1 Compared Methods</h3>
<p>As baselines, we select state-of-the-art methods to compare with our method.</p>
<ul>
<li>PLATO-2 (Bao et al., 2020): The SOTA opendomain dialogue model.</li>
<li>PLATO-FT: The PLATO-2 model fine-tuned on our proposed DuLeMon dataset.</li>
<li>
<p>PLATO-LTM: The PLATO-FT model with our proposed long-term memory (LTM).</p>
</li>
<li>
<p>PLATO-LTM w/o PE: PLATO-LTM without the persona extractor (PE) module, which stores all history utterances (user and bot separately) into memory without persona extraction.</p>
</li>
</ul>
<h3>5.2 Experiment Settings</h3>
<p>Automatic Evaluation Metrics. We use Precision, Recall and F1 to evaluate the persona classification model. For the long-term memory module, we use the AUC and recall@k to evaluate the ranking model. We evaluate responses generated by the models using PPL, BLEU (Papineni et al., 2002), and F1 with reference to the human-annotated responses and DISTINCT-1/2 (Zhao et al., 2017). More recently, Adiwardana et al. (2020) has shown the correlation between perplexity and human judgment in open-domain chit-chat models.
Human Evaluation Metrics. In human evaluation, we employ three utterance-level metrics, including coherence, consistency, engagingness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of $[0,1,2]$. The higher score, the better. These criteria are discussed as follows:</p>
<ul>
<li>Coherence: an utterance-level metric, measuring whether the response is relevant and consistent with the context.</li>
<li>Consistency: an utterance-level metric, evaluating whether the response is consistent with the persona in the dialogue history.</li>
<li>Engagingness: an utterance-level metric, assessing whether the annotator would like to talk with the speaker for each response in the long-term conversation.</li>
</ul>
<h3>5.3 Results</h3>
<p>In this part, we first analyze the effects of each module and then analyze the results of the manual evaluation of our entire system, PLATO-LTM.</p>
<h3>5.3.1 Results of Persona Extractor</h3>
<p>We measure the performance of the persona extractor. To measure the performance of different models, we manually annotated the test set (the number of test sets is 200). We select the best of the first and second stages. The result is shown in Table 3. The pc-stage2 model is better than that of the pc-stage1 model. The F1 of the model exceeds</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">ACC</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">pc-stage1</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.96</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">pc-stage2</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.87</td>
<td style="text-align: left;">0.91</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of two-stage models of our persona classifier.
0.9 , which shows that our model can effectively recognize the persona information from the dialogue history and ensure that the persona information can be correctly stored in the long-term memory. Therefore, the pc-stage2 model is adopted in our system to recognize the persona in the dialogue history.</p>
<h3>5.3.2 Selection of Generative Models</h3>
<p>The generative model utilizes the current context and persona information retrieved from long-term memory to generate the response. We first evaluate the effect of the CPM model on retrieval persona information. The AUC on the automatic test set is 0.76 , recall@ 5 is 0.83 , which shows that our model can efficiently retrieve relevant persona from the long-term memory.</p>
<p>The effect of the generative model reflects the model's ability to use the content of long-term memory to generate the response. Therefore, we select the best generative model to utilize better the retrieved persona information to generate. The result is shown in Table 4. We use the 12L model to conduct experiments to compare different models. The experiment results show that PLATO-FT + role_embed + role_token is the best. Compared to PLATO-FT, the PPL can decrease to 13.377, showing that both strategies are effective. In order to further improve the model, we increased the model size and further trained with the 32L model. Experiment results have shown that the PPL of the 32L model is lower than the 12L model by 4.4 and F1 increased by 2.5 , which can further improve the generative model. Therefore, PLATO-FT 32L + role_embed + role_token model is adopted in our system.</p>
<h3>5.3.3 Human Evaluation</h3>
<p>Self-chat has been widely used in the evaluation of dialogue systems (Li et al., 2016b; Roller et al., 2021; Bao et al., 2020), where the model plays the roles of both parties in the dialogue. To better control variables, we use our proposed PLATOLTM as a user simulator in our experiments and ask all chatbots (including PLATO-LTM) to chat sepa-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">PPL</th>
<th style="text-align: center;">BLUE-1/2</th>
<th style="text-align: center;">DISTINT-1/2</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PLATO-FT 12L</td>
<td style="text-align: center;">13.641</td>
<td style="text-align: center;">$0.190 / 0.081$</td>
<td style="text-align: center;">$0.061 / 0.277$</td>
<td style="text-align: center;">21.02</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-FT 12L + role_embed</td>
<td style="text-align: center;">13.387</td>
<td style="text-align: center;">$0.180 / 0.080$</td>
<td style="text-align: center;">$0.062 / 0.274$</td>
<td style="text-align: center;">20.98</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-FT 12L + role_token</td>
<td style="text-align: center;">13.553</td>
<td style="text-align: center;">$0.193 / 0.081$</td>
<td style="text-align: center;">$0.060 / 0.272$</td>
<td style="text-align: center;">21.28</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-FT 12L + role_embed + role_token</td>
<td style="text-align: center;">$\mathbf{1 3 . 3 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 4 / 0 . 0 8 1}$</td>
<td style="text-align: center;">$0.060 / 0.267$</td>
<td style="text-align: center;">$\mathbf{2 1 . 5 9}$</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-FT 32L + role_embed + role_token</td>
<td style="text-align: center;">9.380</td>
<td style="text-align: center;">$0.194 / 0.087$</td>
<td style="text-align: center;">$0.068 / 0.296$</td>
<td style="text-align: center;">22.61</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of automatic evaluation metric results among different generative models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;">Engagingness</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PLATO-2</td>
<td style="text-align: center;">$\mathbf{1 . 7 0}$</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">1.46</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-FT</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">1.40</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-LTM</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">$\mathbf{0 . 8 7}$</td>
<td style="text-align: center;">$\mathbf{1 . 5 4}$</td>
</tr>
<tr>
<td style="text-align: left;">PLATO-LTM w/o PE</td>
<td style="text-align: center;">1.57</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">1.43</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of human evaluation metric results on self-chat dialogues among our model and baselines. All the above generation models are 32L. The PLATO-FT is with role embedding and role token strategies.
rately with the user simulator. After that, the crowdsourcing workers evaluate only the responses generated by the chatbots other than the simulator. The details are as follows.</p>
<p>Each chatbot chats with the user simulator for 10 episodes, each containing 4 long sessions, and each session contains 16 rounds. As in Bao et al. (2020), we do not impose any restrictions on the chats except for specifying session openings. We pre-select some session openings from the DuLeMon test set, start the interactive conversation with these openings, and ask the two bots to perform chats given the context.</p>
<p>The results are shown in Table 5, from which we can get the following key results:
(1) The long-Term Memory mechanism can significantly improve dialogue consistency. As shown in Table 5, in terms of dialogue consistency, our two models, PLATO-LTM and PLATO-FT, can achieve scores of 0.87 and 0.40 , respectively, which is significantly better than the baseline model PLATO-2. Furthermore, when we compare the performance of PLATO-LTM with PLATO-FT, it can be seen that the use of Long-Term Memory and persona extractor can boost the performance of PLATO-FT with a relative improvement of $118 \%$. Moreover, the model of PLATO-LTM w/o PE can achieve a score of 0.49 , which is still better than the PLATO-FT model. It indicates that long-term memory without a persona extractor is still effective in improving persona consistency.
(2) With the long-term memory mechanism, the use of persona extractor can significantly improve persona consistency and dialogue engagingness. As shown in Table 5, in terms of dia-
logue consistency, the two models, PLATO-LTM (using PE) and PLATO-LTM w/o PE, can achieve scores of 0.87 and 0.49 respectively, indicating that the use of persona extractor can significantly improve dialogue consistency. In terms of dialogue engagingness, PLATO-LTM can obtain a score of 1.54, outperforming the baseline model PLATO-2. In addition, when we remove PE from PLATOLTM, its performance drops from 1.54 (the score of PLATO-LTM) to 1.43 (that of PLATO-LTM w/o PE), indicating that the use of persona extractor can improve the performance of PLATO-FT.
(3) Fine-tuning on the small-scale dataset will slightly hurt the performance of pre-trained dialogue models in dialogue coherence. In terms of dialogue coherence, the PLATO-FT model (finetuned on our dataset) achieve a score of 1.59 , which is lower than that of the baseline model PLATO (not finetuned on our dataset). The possible reason is that during the self-play procedure for system evaluation, their dialogs usually cover a wide range of topics, and then it is challenging to generate appropriate or coherent responses when given these open-domain topics in contexts. The finetuning procedure might hurt the capability of the pre-trained dialogue model in terms of response appropriateness or dialogue coherence, leading to the inferior performance of PLATO-LTM and its variants.</p>
<h2>6 Conclusion</h2>
<p>In this paper, We present a novel LeMon (Longterm Memory Conversation) task and then build the corresponding dataset DuLeMon, introducing longterm persona modelling into large-scale generative</p>
<p>dialogue models. We further propose a Long-Term Memory (LTM) as a plug-in component of state-of-the-art large-scale generative dialogue models. LTM consists of user memory and chatbot memory, where the user memory is for understanding and memorizing persona information mentioned by the user, and the chatbot memory attempts to keep its persona information to be continuously updated over time. Experiment results show that our system PLATO-LTM can make effective use of both parties' persona information from dialogue history to enhance dialogue consistency and engagingness when conducting a long-term conversation. In the future, we will further study the possibility of using reinforcement learning with human feedback signals to help long-term conversation.</p>
<h2>7 Ethical Considerations</h2>
<p>We are sure that DuLeMon has been collected in a manner that is consistent with the terms of use of any sources and the intellectual property and privacy rights of the original authors of the texts. Meanwhile, our project is approved by an IRB. Finally, we also provide details on the characteristics of DuLeMon and steps taken to ensure the potential problems with the quality of the dataset do not create additional risks.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. 2020. Towards a human-like opendomain chatbot. CoRR, abs/2001.09977.</p>
<p>Jeesoo Bang, Hyungjong Noh, Yonghee Kim, and Gary Geunbae Lee. 2015. Example-based chatoriented dialogue system with personalized longterm memory. In 2015 International Conference on Big Data and Smart Computing (BIGCOMP), pages 238-243.</p>
<p>Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. PLATO-2: towards building an open-domain chatbot via curriculum learning. CoRR, abs/2006.16779.</p>
<p>Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, and Zhengyu Niu. 2021. Plato-xl: Exploring the large-scale pre-training of dialogue generation.</p>
<p>Joana Campos, James Kennedy, and Jill F. Lehman. 2018. Challenges in exploiting conversational memory in human-agent interaction. In Proceedings of
the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '18, page 1649-1657, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.</p>
<p>Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander H. Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W. Black, Alexander I. Rudnicky, Jason Williams, Joelle Pineau, Mikhail S. Burtsev, and Jason Weston. 2019. The second conversational intelligence challenge (convai2). CoRR, abs/1902.00098.</p>
<p>Miguel Elvir, Avelino J. Gonzalez, Christopher Walls, and Bryan Wilder. 2017. Remembering a conversation - a conversational memory architecture for embodied conversational agents. Journal of Intelligent Systems, 26(1):1-21.</p>
<p>Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open-domain dialog systems. ACM Trans. Inf. Syst., 38(3).</p>
<p>Yonghee Kim, Jeesoo Bang, Junhwi Choi, Seonghan Ryu, Sangjun Koo, and Gary Geunbae Lee. 2014. Acquisition and use of long-term memory for personalized dialog systems. In Multimodal Analyses enabling Artificial Agents in Human-Machine Interaction - Second International Workshop, MA3HMI 2014, Held in Conjunction with INTERSPEECH 2014, Singapore, Singapore, September 14, 2014, Revised Selected Papers, volume 8757 of Lecture Notes in Computer Science, pages 78-87. Springer.</p>
<p>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1746-1751. ACL.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, and William B. Dolan. 2016a. A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016b. Deep reinforcement learning for dialogue generation.</p>
<p>Mei Yii Lim. 2012. Memory Models for Intelligent Social Companions, pages 241-262. Springer Berlin Heidelberg, Berlin, Heidelberg.</p>
<p>Zhaojiang Lin, Zihan Liu, Genta Indra Winata, Samuel Cahyawijaya, Andrea Madotto, Yejin Bang, Etsuko Ishii, and Pascale Fung. 2020. Xpersona: Evaluating multilingual personalized chatbot. CoRR, abs/2003.07568.</p>
<p>Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison, and Antoine Bordes. 2018. Training millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775-2779, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Qiao Qian, Minlie Huang, Haizhou Zhao, Jingfang Xu , and Xiaoyan Zhu. 2018. Assigning personality/profile to a chatting machine for coherent conversation generation. In Proceedings of the TwentySeventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4279-4285. International Joint Conferences on Artificial Intelligence Organization.</p>
<p>Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 300-325. Association for Computational Linguistics.</p>
<p>Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2019. Ernie 2.0: A continual pre-training framework for language understanding.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. 2021. Beyond goldfish memory: Long-term open-domain conversation.</p>
<p>Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204-2213, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Weinan Zhang, Ting Liu, Yifa Wang, and Qingfu Zhu. 2017. Neural personalized response generation as domain adaptation. CoRR, abs/1701.02073.</p>
<p>Tiancheng Zhao, Ran Zhao, and Maxine Eskénazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 654-664. Association for Computational Linguistics.</p>
<p>Yinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu, and Xuan Zhu. 2019. Personalized dialogue generation with diversified traits. CoRR, abs/1901.09672.</p>
<p>Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, and Chunyan Miao. 2020. Towards persona-based empathetic conversational models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6556-6566, Online. Association for Computational Linguistics.</p>
<h2>A Details of Data Collection</h2>
<p>The collection processes of DuLeMon are as follows.</p>
<ul>
<li>The crowdworkers enter the chat interface in pairs, and role 1 initiates a conversation;</li>
<li>The chat content can include opening greetings, self-introduction, chatting content that conforms to the persona information, asking the other party's questions, answering the other's questions, and so on. The information used in the chat must be consistent with the given personal information;</li>
<li>The dialogue contains at least 8 turns (each person speaks at least 8 utterances);</li>
</ul>
<p>At the same time, we also let the crowdworkers pay attention to the follows: 1 . Use as many words as possible, and do not repeat them. The overall dialogue strives to be natural, smooth, and not embarrassing. 2. Do not simply copy and paste the sentences in the personal information and express them as richly as possible. If it is found that $50 \%$ of the fragments of any given sentence appear in the conversation, it is a non-compliant conversation. 3. When using persona information, do not copy it entirely, and talk about relevant content around the persona. For example, if the persona setting contains the sentence "I am a painter", the response can be that "I have painted many beautiful paintings and held several exhibitions"; 4. If the question raised by the other speaker is not covered in the given personal information, the reply can be freely used; if there is any reference or related information in the given personal information, reply according it.</p>
<h1>B Details of Models</h1>
<p>Generation Model For the Generation model, We follow PLATO-2 (Bao et al., 2020). The maximum length of context, user persona, and chatbot persona are set to 384,76 , and 52 , respectively. The vocabulary contains 30 K Chinese BPE tokens. We optimize all models using Adam (Kingma and Ba, 2015) with every batch of $B=16384$ tokens and learning rate of $l r=5 e-5$. We conduct all experiments on NVIDIA V100 32GB and A100 48GB GPUs.
Long-term Memory For both user memory and chatbot memory, we set duplication threshold $s_{\text {dup }}=0.95$, number of candidates $K=5$, and similarity threshold $s_{c}=0.7$. Due to the persona sparsity of dialogue and the efficiency of our persona storage, we do not limit the memory capacity.</p>
<h2>C Cases of PLATO-LTM</h2>
<p>To concretely demonstrate the long-term persona ability in a long-term conversation, we further provide a cherry-picked example of one episode conversation (between PLATO-LTM and PLATO-2) in Figure 4.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A cherry-picked example of one episode conversation between PLATO-LTM and PLATO-2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ There are two stages within the PLATO-2 model, the first stage conduct candidate responses generation and the second stage conduct responses selection. We only implement our work on the first stage of PLATO-2.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
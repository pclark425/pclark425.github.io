<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9007 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9007</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9007</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-275581582</p>
                <p><strong>Paper Title:</strong> How to evaluate the cognitive abilities of LLMs</p>
                <p><strong>Paper Abstract:</strong> Language models have become an essential part of the burgeoning field of artificial intelligence (AI) psychology. I discuss 14 methodological considerations that can be used to design more robust, generalizable studies that evaluate the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9007.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9007.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WSC / WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Winograd Schema Challenge and WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Winograd Schema Challenge (WSC) is a benchmark of minimally differing sentence pairs for pronoun disambiguation that probes commonsense/world knowledge; WinoGrande is a much larger crowdsourced dataset with automatic filtering intended to reduce simple co-occurrence heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language models evaluated on WSC-style pronoun resolution tasks; the paper does not specify particular architectures or training corpora for the models achieving the cited results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) / WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Sets of minimally differing sentence pairs where a pronoun's referent must be inferred from commonsense/world knowledge (domains: physical properties, biology, social situations, etc.). Designed to avoid trivial syntactic or frequency cues.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>>90% accuracy on the original WSC by 2020 (as reported in this paper); performance fell substantially on the larger/filtered WinoGrande dataset (no precise numbers reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Initial reports of LLMs reaching >90% on the original (small, public) WSC overstated models' commonsense ability; performance declined on larger/filtered datasets, indicating earlier successes were inflated by dataset artifacts and memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper highlights issues that affect evaluation: selectional restrictions and frequency heuristics as shortcuts, dataset contamination (famous items seen during training), and WinoGrande's approach of crowdsourcing followed by automatic filtering using LM embeddings to reduce co-occurrence associations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High performance on the original WSC can reflect memorization or shallow heuristics (selectional restrictions, frequency effects). Larger, filtered datasets reduce these artifacts but introduce quality-control problems (typos, grammatical errors, unwarranted assumptions). The paper cites further work showing that even stringent baselines reveal association biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to evaluate the cognitive abilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9007.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9007.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>General LLM cognitive assessments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluations of large language models on diverse cognitive psychology tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that many studies have applied language-based cognitive assessments to LLMs, including tests of working memory, logical reasoning, planning, social reasoning, creativity and personality, and notes vision-language tests are becoming possible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT and other LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based large language model interfaces (example given: ChatGPT) and other open/closed LLMs used opportunistically by researchers to run cognitive-style assessments; specific model architectures, datasets and sizes are not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Working memory tests; logical reasoning tasks; planning tasks; social reasoning assessments; creativity and personality questionnaires; vision-language tasks incorporating images/video</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A broad collection of language-based and multimodal cognitive assessments adapted from human psychology to probe analogous capabilities in LLMs (domains include working memory, reasoning, planning, social cognition, creativity, personality traits).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not specified in numeric terms; the paper emphasizes mixed results across tasks and warns against overinterpreting isolated high or low scores without careful methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>The paper recommends checking for training-data contamination, recreating human testing conditions when comparing to humans, designing control conditions to detect shortcut heuristics, avoiding famous/minimally-changed items, and being explicit about prompts, language/dialect, and experimental dates (closed models may change). It also cautions about automated item generation and LLM-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Many studies lack rigorous controls: models may have seen test items in training, may exploit statistical shortcuts different from humans, cultural/language biases in training data complicate human-model comparisons, closed models hinder reproducibility, and automated scoring can be circular if done with similar models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to evaluate the cognitive abilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9007.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9007.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elazar et al. baselines (WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline analyses on Winograd-style items (Elazar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Elazar et al. introduced stricter baseline conditions (e.g., removing candidate referents or the first half of sentences) to detect raw-frequency/association biases and found LLMs performed above chance even on these reduced baselines, indicating previously undetected association heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Winograd Schema Challenge baseline conditions</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Control items derived from WSC sentences where candidate referents are excluded or sentence halves are removed; intended to reveal whether simple frequency associations drive model choices.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to be above-chance on these baseline (reduced) conditions (no numeric values given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs' above-chance baseline performance implies reliance on association biases rather than robust commonsense reasoning comparable to human understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Baseline conditions: (1) sentences with candidate referents excluded, and (2) sentences with the first half excluded; expectation is that reduced sentences should not exhibit consistent referent preference if solutions require world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>These baseline findings demonstrate that apparently robust model performance on WSC can be driven by dataset artifacts; thus simple accuracy on the original items is insufficient evidence of deep commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to evaluate the cognitive abilities of LLMs', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Winograd schema challenge <em>(Rating: 2)</em></li>
                <li>How reasonable are common-sense reasoning tasks: a case-study on the Winograd schema challenge and SWAG <em>(Rating: 2)</em></li>
                <li>Back to square one: zrtifact detection, training and commonsense disentanglement in the Winograd schema <em>(Rating: 2)</em></li>
                <li>WinoGrande <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9007",
    "paper_id": "paper-275581582",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "WSC / WinoGrande",
            "name_full": "Winograd Schema Challenge and WinoGrande",
            "brief_description": "The Winograd Schema Challenge (WSC) is a benchmark of minimally differing sentence pairs for pronoun disambiguation that probes commonsense/world knowledge; WinoGrande is a much larger crowdsourced dataset with automatic filtering intended to reduce simple co-occurrence heuristics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various, unspecified)",
            "model_description": "Large language models evaluated on WSC-style pronoun resolution tasks; the paper does not specify particular architectures or training corpora for the models achieving the cited results.",
            "model_size": null,
            "test_battery_name": "Winograd Schema Challenge (WSC) / WinoGrande",
            "test_description": "Sets of minimally differing sentence pairs where a pronoun's referent must be inferred from commonsense/world knowledge (domains: physical properties, biology, social situations, etc.). Designed to avoid trivial syntactic or frequency cues.",
            "llm_performance": "&gt;90% accuracy on the original WSC by 2020 (as reported in this paper); performance fell substantially on the larger/filtered WinoGrande dataset (no precise numbers reported here).",
            "human_baseline_performance": null,
            "performance_comparison": "Initial reports of LLMs reaching &gt;90% on the original (small, public) WSC overstated models' commonsense ability; performance declined on larger/filtered datasets, indicating earlier successes were inflated by dataset artifacts and memorization.",
            "experimental_details": "Paper highlights issues that affect evaluation: selectional restrictions and frequency heuristics as shortcuts, dataset contamination (famous items seen during training), and WinoGrande's approach of crowdsourcing followed by automatic filtering using LM embeddings to reduce co-occurrence associations.",
            "limitations_or_caveats": "High performance on the original WSC can reflect memorization or shallow heuristics (selectional restrictions, frequency effects). Larger, filtered datasets reduce these artifacts but introduce quality-control problems (typos, grammatical errors, unwarranted assumptions). The paper cites further work showing that even stringent baselines reveal association biases.",
            "uuid": "e9007.0",
            "source_info": {
                "paper_title": "How to evaluate the cognitive abilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "General LLM cognitive assessments",
            "name_full": "Evaluations of large language models on diverse cognitive psychology tests",
            "brief_description": "The paper documents that many studies have applied language-based cognitive assessments to LLMs, including tests of working memory, logical reasoning, planning, social reasoning, creativity and personality, and notes vision-language tests are becoming possible.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT and other LLMs (unspecified)",
            "model_description": "Chat-based large language model interfaces (example given: ChatGPT) and other open/closed LLMs used opportunistically by researchers to run cognitive-style assessments; specific model architectures, datasets and sizes are not enumerated in this paper.",
            "model_size": null,
            "test_battery_name": "Working memory tests; logical reasoning tasks; planning tasks; social reasoning assessments; creativity and personality questionnaires; vision-language tasks incorporating images/video",
            "test_description": "A broad collection of language-based and multimodal cognitive assessments adapted from human psychology to probe analogous capabilities in LLMs (domains include working memory, reasoning, planning, social cognition, creativity, personality traits).",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": "Not specified in numeric terms; the paper emphasizes mixed results across tasks and warns against overinterpreting isolated high or low scores without careful methodology.",
            "experimental_details": "The paper recommends checking for training-data contamination, recreating human testing conditions when comparing to humans, designing control conditions to detect shortcut heuristics, avoiding famous/minimally-changed items, and being explicit about prompts, language/dialect, and experimental dates (closed models may change). It also cautions about automated item generation and LLM-based scoring.",
            "limitations_or_caveats": "Many studies lack rigorous controls: models may have seen test items in training, may exploit statistical shortcuts different from humans, cultural/language biases in training data complicate human-model comparisons, closed models hinder reproducibility, and automated scoring can be circular if done with similar models.",
            "uuid": "e9007.1",
            "source_info": {
                "paper_title": "How to evaluate the cognitive abilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Elazar et al. baselines (WSC)",
            "name_full": "Baseline analyses on Winograd-style items (Elazar et al.)",
            "brief_description": "Elazar et al. introduced stricter baseline conditions (e.g., removing candidate referents or the first half of sentences) to detect raw-frequency/association biases and found LLMs performed above chance even on these reduced baselines, indicating previously undetected association heuristics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various, unspecified)",
            "model_description": "",
            "model_size": null,
            "test_battery_name": "Winograd Schema Challenge baseline conditions",
            "test_description": "Control items derived from WSC sentences where candidate referents are excluded or sentence halves are removed; intended to reveal whether simple frequency associations drive model choices.",
            "llm_performance": "Reported to be above-chance on these baseline (reduced) conditions (no numeric values given in this paper).",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs' above-chance baseline performance implies reliance on association biases rather than robust commonsense reasoning comparable to human understanding.",
            "experimental_details": "Baseline conditions: (1) sentences with candidate referents excluded, and (2) sentences with the first half excluded; expectation is that reduced sentences should not exhibit consistent referent preference if solutions require world knowledge.",
            "limitations_or_caveats": "These baseline findings demonstrate that apparently robust model performance on WSC can be driven by dataset artifacts; thus simple accuracy on the original items is insufficient evidence of deep commonsense reasoning.",
            "uuid": "e9007.2",
            "source_info": {
                "paper_title": "How to evaluate the cognitive abilities of LLMs",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Winograd schema challenge",
            "rating": 2,
            "sanitized_title": "the_winograd_schema_challenge"
        },
        {
            "paper_title": "How reasonable are common-sense reasoning tasks: a case-study on the Winograd schema challenge and SWAG",
            "rating": 2,
            "sanitized_title": "how_reasonable_are_commonsense_reasoning_tasks_a_casestudy_on_the_winograd_schema_challenge_and_swag"
        },
        {
            "paper_title": "Back to square one: zrtifact detection, training and commonsense disentanglement in the Winograd schema",
            "rating": 2,
            "sanitized_title": "back_to_square_one_zrtifact_detection_training_and_commonsense_disentanglement_in_the_winograd_schema"
        },
        {
            "paper_title": "WinoGrande",
            "rating": 2,
            "sanitized_title": "winogrande"
        }
    ],
    "cost": 0.0114875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>nature human behaviour</p>
<p>Anna A Ivanova 
nature human behaviour
307BC64FBD015C8B082F4E5BABF2471010.1038/s41562-024-02096-z
Language models have become an essential part of the burgeoning field of artificial intelligence (AI) psychology.I discuss 14 methodological considerations that can be used to design more robust, generalizable studies that evaluate the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.Ever since the Turing test, the idea of having a dialogue with a machine to probe its cognitive abilities ('thought') has been inextricably associated with the field of AI.In addition to its intuitive simplicity, this idea naturally aligns with everyday practice in psychology: language-based assessments are the bread and butter of many psychologists' toolkits.If researchers want to know what is happening in the mind of a human, the easiest approach is to ask.Today, advances in linguistic abilities of large language models (LLMs) -and AI systems that might incorporate LLMs as one of their components -make it possible to seamlessly test these models on language-based assessments originally designed for people.This is an unprecedented advance: to date, the only entities who could flexibly use human language were human.Now, however, we are faced with artificial systems that can process linguistic information, generate novel texts and respond to questions, which raises the question of how we might assess the cognitive capabilities of these systems.Easy access to chat-based LLM interfaces (such as ChatGPT) makes it possible for anyone to run a 'cognitive test' on an AI system.This advance has led to an explosive growth of AI psychology, with papers that assess LLMs' working memory capacity, logical reasoning, planning abilities, social reasoning, creativity and even personality traits.Advances in vision-language models now also make it possible to test AI systems on cognitive assessments that incorporate pictures and videos.Although running such assessments can be fairly straightforward, interpreting the results is not.In fact, AI psychology today is faced with a plethora of methodological questions, including what factors should be considered when assessing model performance; how we can adapt our stimuli to reduce the prevalence of 'hacks' (that is, heuristics that the model might use to achieve high task performance without using the cognitive skill being assessed); and whether, if a model passes a human test for a cognitive ability 'X', this means that it indeed possesses X.To begin answering these questions, I provide a list of 14 'dos and do nots' to consider when designing, conducting and interpreting the results of an AI psychology study (see refs.1,2 for additional considerations) (Table1).I do not touch on the philosophical question of whether it is at all appropriate to ascribe mental capacities to a machine; my goal here is simply to clarify the methodological criteria that determine the inferences we can and cannot make on the basis of a model's responses to a questionnaire or a cognitive test.</p>
<p>Experimental design</p>
<p>When designing or adapting a cognitive test for evaluating an AI model, it is important to consider the following points.</p>
<p>Do determine what the model might have learned about your test during training.</p>
<p>The two most important issues to consider are whether the model was directly trained on your task and whether the model 'saw' examples from your test during its training.In the worst case, these issues might occur together -that is, the model was trained on the task of interest using the same items as those in the current study.</p>
<p>Do consider alternative strategies that a model might use to arrive</p>
<p>at the correct answer.Even if the model has not directly learned your task during training or finetuning, it might still use a different strategy from the strategy you, the experimenter, have presupposed.The most prominent shortcuts for LLMs include word associations based on their statistical co-occurrence (for example, 'race car' and 'fast') -although there might be many others, such as grammatical cues or more abstract structural patterns in text.Humans, too, routinely use task strategies that are not initially considered by the experimenters, but the kinds of alternative strategies available to humans and to AI systems are not necessarily the same.Considering possible shortcut paths to solutions can help to both design shortcut-proof items and identify edge cases in which shortcut-based models are likely to fail 3 .</p>
<p>Do incorporate careful control conditions.</p>
<p>Once you identify alternative strategies by which a model might solve the task, the next step is to design control conditions to help to distinguish these possible strategies: for instance, whether the model will choose answer 'a' over</p>
<p>Check for updates</p>
<p>Comment</p>
<p>Do not overly trust crowdsourced or automatically generated items.The ability of AI models to process large amounts of queriesfar more than any human -creates the temptation to evaluate their performance on thousands of test items, obtained from online human workers, created by automatic item-generation scripts or even generated by AI models themselves.However, if the quality of these items is not rigorously evaluated, such tests may be meaningless.AI-generated items present their own set of challenges: a model might generate test items that resemble samples from the training data or, more broadly, generate patterns that have a high likelihood under the model and might therefore facilitate its performance.A systematic assessment of such biases in AI-generated materials remains to be done.'b' simply because 'a' is more frequent, or whether the model will show the same preference for answer 'a' over 'b' even when the question is omitted.</p>
<p>Do not rely on well-known (or minimally changed) test items.When tested on a famous test item (such as 'The trophy did not fit the suitcase because it was too small') (Box 1), the model has probably seen it multiple times during training and therefore has a high chance of relying on answer memorization; thus, its performance may not generalize to new, non-famous test items.Even if the experimenter makes a minor change to the original item (such as replacing 'trophy' with 'prize'), a model might still be able to complete the test on the basis of simple association between the old and the new item.</p>
<p>BOX 1</p>
<p>The Winograd schema challenge</p>
<p>In 2012, Levesque and colleagues proposed a test for diverse kinds of basic world knowledge 6 (named after an initial example by T. Winograd).The test includes minimally different item pairs with pronouns whose referent can be inferred on the basis of general world knowledge:</p>
<p>(1) Question: the trophy doesn't fit into the brown suitcase because it's too small.What is too small?Answer: the suitcase.(2) Question: the trophy doesn't fit into the brown suitcase because it's too large.What is too large?Answer: the trophy.</p>
<p>A set of such minimally differing sentence pairs that tackle various world knowledge phenomena -physical properties, biology, social situations and so on -was then compiled into the Winograd schema challenge (WSC).Although initially challenging, the WSC was largely solved by 2020 and LLMs achieved over 90% accuracy 7 .However, it turned out that this success did not reflect models' deep knowledge of the world, but rather the flaws in the test itself.</p>
<p>In the original paper that introduced the challenge 6 , the authors cautioned against using examples that could be solved via simple heuristics.They cite two such heuristics: selectional restrictions and frequency effects.The selectional restrictions heuristics can be illustrated with the example: "The women stopped taking the pills because they were pregnant/carcinogenic".Here, only an animate entity can be pregnant and only an inanimate entity can be carcinogenic, which leads to an unambiguous association between the adjectives and the corresponding nouns without the need to tap into deeper world knowledge.The frequency heuristics applies to cases such as: "The race car zoomed by the school bus because it was going so fast/slow".A simple association between 'race car' and 'fast' will suffice to determine the referent.Finally, the authors note that the examples need to be, in their words, 'Google-proof' -that is, not present in online text corpora used to train the models.</p>
<p>Despite the field's awareness of these heuristics, constructing a heuristics-free dataset was hard.In the original WSC dataset, over 10% of the items turned out to have simple association-based solutions 8 .To address this issue, Sakaguchi et al. 7 constructed a large dataset called WinoGrande, in which the examples were first crowdsourced online and then automatically filtered to reduce cooccurrence-based associations as estimated through language model embeddings (rather than human intuition).This filtering substantially reduced model performance, which suggests that the association heuristic indeed inflates model accuracy.</p>
<p>Soon after, Elazar et al. 9 showed that adding even more-stringent quality controls leads to substantial decreases in model performance on the WSC.The authors introduced two baseline cases to account for the raw frequency of possible continuations: sentences with candidate referents excluded ("doesn't fit into because it's too large/ small") and sentences with the first half excluded ("because it's too large/small").The assumption is that for these reduced sentences, there should be no consistent preference for 'suitcase' versus 'trophy' and -if there is -this reflects an association bias.It turned out that LLMs performed above chance on the WSC even in those baseline conditions, which indicated previously undetected association biases.</p>
<p>The final issue raised by the WSC story is the quality versus quantity trade-off in test design.The initial WSC set includes fewer than 300 examples, all of which were handcrafted by scholars.Now that these examples are freely available online, performance on this set is no longer reflective of the models' general capabilities.The authors of WinoGrande used a different approach: to obtain thousands of novel items, they asked human workers to come up with many different examples and then used automatic filtering techniques.This approach is more scalable but suffers from numerous quality issues, for example, typos ('wit' instead of 'with'), grammatical errors ('more brighter color') and unwarranted assumptions ('good at math' means 'likely to be a professor').Overall, the trade-off between result generalizability (which is harder for small datasets) and quality control (which is harder for large datasets) remains an important issue to consider in test design.</p>
<p>Kocijan et al. 10 formulate several lessons from the WSC saga, the most important of which is perhaps: "[w]e need to be careful not to rely on a perceived connection between tasks and methods".Just because we think that a certain task requires a cognitive ability 'X', does not mean that it actually does.</p>
<p>Comment</p>
<p>Do evaluate models under conditions similar to humans when comparing their performance.When conducting direct model-to-human comparisons, the default should be to recreate the same testing conditions for humans and models 4 .If a human needs instructions to perform well on a test, the model should also receive those instructions.If a human performs well on a task with no in-the-moment training, a model with human-level performance would also need to perform the task with no in-context training.If divergences during evaluation occur (for either theoretical or practical reasons), they need to be justified and highlighted when presenting the results.Do examine the effect of culture-specific aspects of the prompt on model behaviour.The language in which the test is presented, the dialect, the vocabulary used and even the use of capitalization and punctuation might all affect AI model performance, as it leverages these cues to mimic specific users online.</p>
<p>Interpreting the results</p>
<p>When interpreting the results of a cognitive assessment, the following factors are important.Do compare model and human performance.We often assume humans will perform well on a specific test even when we do not have direct evidence for it.If these exact test items and question or prompt wordings have not been tested on humans before, they should be.Do be explicit about the experimental settings when reporting the results.Computational work should, ideally, have perfect replicability.For that to be achievable, all the materials and code needed to run the experiment should be made available online.For models offered by commercial third parties, there is no guarantee that they will continue to offer access to the model, so some backup arrangements may need to be made.Finally, a third party may roll out a model update that would change experimental results; in such cases, the date of the experiment needs to be reported as well.</p>
<p>Do not overly trust LLM item scoring.The flip side of testing a model on thousands of items is the need to evaluate responses to all these items.In some cases, determining correct answers is straightforward (for example, a-d for a multiple-choice question); however, scoring free response items is hard work.Many studies are opting for LLM-based item evaluation; however, such approaches might be circular because the items that are hard for the model being evaluated might also be hard for the model doing the evaluation.At a minimum, the model being tested and the model doing the evaluation should belong to different classes; and even then, evaluator model performance needs to be rigorously checked by comparing it with human evaluations across a range of item difficulties.</p>
<p>Do not assume that model responses reflect 'universal' human behaviour.Even when directly comparing AI models and humans, it is important to be specific about which human population serves as a reference.Human psychology suffers from over-reliance on WEIRD (Western, educated, industrialized, rich and democratic) participant pools; AI psychology suffers from similar issues 5 because WEIRD individuals disproportionately contribute to the training data.Thus, calls to replace human data in psychology studies with AI-generated responses need to account for the strong demographic and cultural biases that these models bring.Similarly, model performance in English (the most represented language on the Internet) might not be reflective of model performance in other languages.</p>
<p>Do not assume that models solve the task in the same way as humans.Even if both humans and models do well on a particular test, it does not mean that they solve it in the same way.Some approaches to evaluating similarities and differences in the mechanisms that underlie human and model task performance include comparing their error patterns, generalization to new tasks, and the internal (neural) representations required to accomplish the task.</p>
<p>Do check whether a model generalizes beyond a single test.Even when we control for possible shortcuts, a model might still find a loophole that allows it to perform well on a particular test.However, the more diverse tests we include (and the more we control for the shortcuts in each), the harder it will be to attribute model performance to serendipitous factors.If a model does well on 20 logical reasoning tasks that vary in their content and format, it is more plausible that it possesses logical reasoning than if it performs well on one such task.</p>
<p>Do not jump to conclusions.The discourse around AI models has become very polarized, with both extreme enthusiasm based on a few isolated examples and extreme scepticism based on isolated failures.What the field needs is careful evaluation of specific model capabilities with a clear acknowledgement of advances and limitations.Moreover, results reported for one model may not generalize to other models, so conclusions from a single study should be qualified accordingly.</p>
<p>Testing open versus closed models</p>
<p>Some of the 'dos' discussed above -checking the training data for contamination with test items, verifying that the model has not been finetuned on the exact task being tested, and creating conditions that will allow the study to be replicated in the future on the exact same model -are essentially impossible in the case of closed models.</p>
<p>Comment</p>
<p>Thus, there is an argument to stop running AI psychology assessments on closed models altogether, given that -from a scientific perspective -the results are potentially non-reproducible and difficult to interpret.</p>
<p>And yet, it is unlikely that researchers will stop running cognitive tests on closed AI models.Although scientifically questionable, evaluating the behaviour of closed, industry-standard models has important practical implications that include understanding which real-life tasks they can (and cannot) be reliably used for; what AI models are in principle capable of achieving (given that closed models often exhibit state-of-the-art performance); and what safety risks they may present.Thus, going forward, cognitive evaluations of AI systems might be used in two separate settings: basic scientific inquiry into the cognitive capacities of AI systems (which should prioritize open models) and applied studies that tackle questions related to model performance, safety and user impact (which can be applied both to open and to closed models, with the caveat that results from closed models might not replicate or generalize).</p>
<p>As with any rapidly growing research area, the scientific practices and norms in AI psychology are constantly being defined and redefined, often through trial and error.I hope that this discussion of the 'dos and do nots' of AI psychology will help to distil some of the lessons learned and improve the robustness and validity of future work that aims to probe the cognitive capacities of AI systems.</p>
<p>CREDIT: SEAN GLADWELL/MOMENT/GETTY nature human behaviour Volume 9 | February 2025 | 230-233 | 233</p>
<p>Table 1 | The 'dos and do nots' of AI psychology Do Do not
1Determine what the model might haveRely on well-known (or minimallylearned about your test during trainingchanged) test itemsConsider alternative strategies that aOverly trust crowdsourced ormodel might use to arrive at the correctautomatically generated itemsanswerIncorporate careful control conditionsOverly trust LLM item scoringEvaluate models under conditionsAssume that model responsessimilar to humansreflect 'universal' humanbehaviourExamine the effect of culture-specificAssume that models solve theaspects of the prompt on modeltask in the same way as humansbehaviourCompare model and humanperformanceBe explicit about the experimental settings when reporting the resultsJump to conclusionsCheck whether a model generalizesbeyond a single test
Anna A. Ivanova Georgia Institute of Technology, Atlanta, GA, USA.e-mail: a.ivanova@gatech.eduPublished online: 15 January 2025 Acknowledgements I gratefully acknowledge funding support from the University System of Georgia.I thank A. Sathe, B. Lipkin, C. Kauf, G. Tuckute and K. Mahowald for their constructive comments.Competing interestsThe author declares no competing interests.Additional informationPeer review information Nature Human Behaviour thanks Michael Frank, Adele Goldberg and Yoshua Bengio for their contribution to the peer review of this work.
. M C Frank, Nat, Rev. Psychol. 22023</p>
<p>. M Mitchell, Science. 38159572023</p>
<p>. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, Proc. Natl Acad. Sci. USA. 121e23224201212024</p>
<p>. A K Lampinen, 10.48550/arXiv.2210.153032022Preprint at</p>
<p>. M Atari, M J Xue, P S Park, D Blasi, J Henrich, 10.31234/osf.io/5b26t2023</p>
<p>The Winograd schema challenge. H Levesque, E Davis, L Morgenstern, Proc. Thirteenth International Conf. on the Principles of Knowledge Representation and Reasoning. G Brewka, Thirteenth International Conf. on the Principles of Knowledge Representation and ReasoningAAAI Press2012</p>
<p>. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Commun. ACM. 642021</p>
<p>How reasonable are common-sense reasoning tasks: a case-study on the Winograd schema challenge and SWAG. P Trichelair, A Emami, A Trischler, K Suleman, J C K Cheung, K Inui, Proc. 2019 Conf. on Empirical Methods in Natural Language Processing and the 9th International Joint Conf. on Natural Language Processing. 2019 Conf. on Empirical Methods in Natural Language essing and the 9th International Joint Conf. on Natural Language essingACL2019</p>
<p>Back to square one: zrtifact detection, training and commonsense disentanglement in the Winograd schema. Y Elazar, H Zhang, Y Goldberg, D Roth, 10486-10500Proc. 2021 Conf, on Empirical Methods in Natural Language Processing. M.-F Moens, 2021 Conf, on Empirical Methods in Natural Language essingACL2021</p>
<p>. V Kocijan, E Davis, T Lukasiewicz, G Marcus, L Morgenstern, Artif. Intell. 3251039712023</p>            </div>
        </div>

    </div>
</body>
</html>
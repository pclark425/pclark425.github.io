<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9618 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9618</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9618</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-278769611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14599v2.pdf" target="_blank">Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9618.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9618.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthHypo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Truthful Hypothesis Generation Benchmark (TruthHypo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical benchmark introduced in this paper to evaluate LLMs' ability to generate truthful scientific hypotheses by simulating temporal knowledge splits using PubTator 3.0 and PubMed-derived corpora and offering link- and relation-level evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-4o, GPT-4o-mini, Llama-3.1-70B, Llama-3.1-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark is evaluated with a set of LLMs including open-source Llama-3.1 family (open-source) and proprietary GPT-4 family; models were used in prompt-based settings with or without external knowledge augmentation (KG, literature, or both). All models were trained on knowledge available before 2024 per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Structured knowledge from PubTator 3.0 (biomedical knowledge graph with annotated relations) with a temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) subsets; unstructured literature retrieval from PubMed (articles with PMID ≤ 36600000) via BM25; tasks focus on Chemical–Gene, Disease–Gene, and Gene–Gene entity pairs with added negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Given two biomedical entities (e.g., chemical and gene), the LLM must hypothesize the potential relation between them (labels: specific relation types or 'no relation'), simulating discovery of relations absent from 'seen' knowledge (i.e., future/unseen discoveries).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-driven hypothesis generation under four settings: (1) parametric knowledge only; (2) parametric + knowledge graph (multi-hop link chains mapped to textual descriptions); (3) parametric + literature (retrieval-augmented generation using BM25 over PubMed); (4) combined parametric + KG + literature. Models produce rationales and hypotheses which are decomposed into atomic claims for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated scientific hypotheses (relation predictions between entity pairs) plus model rationales; dataset-level metrics (link-level predictions) and groundedness scores when combined with KnowHD.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example (template style): "Can we hypothesize the potential relation between Chemical X (ID) and Gene Y (ID)? Final hypothesis: [positive correlate | negative correlate | inhibit | stimulate | no relation]"</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Link-level precision/recall/F1 and relation-level accuracy computed against held-out 'unseen' relations from PubTator; groundedness verification via KnowHD (atomic claim retrieval + entailment judgments); human expert annotation for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Most LLMs struggled to generate truthful hypotheses; only GPT-4o exceeded ~60% mean accuracy in aggregate. Link-level F1 > relation-level accuracy, indicating models often detect potential connections but mislabel precise relations. Groundedness scores from KnowHD correlated positively with accuracy and could be used to filter higher-truthfulness hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides temporally realistic benchmark using structured KG + literature; enables controlled evaluation across parametric and retrieval-augmented settings; supports multi-faceted evaluation (link-level, relation-level, groundedness).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on PubTator/PubMed temporal split and multi-article filtering which may limit generality beyond biomedical domain; input corpus size unspecified; benchmark focuses on specific relation types and pairwise queries (Chemical–Gene, Disease–Gene, Gene–Gene), so broader theory synthesis tasks are not directly benchmarked.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>LLMs sometimes produce false negatives (miss true associations) or merely echo/paraphrase provided context without substantive new reasoning; smaller models occasionally degrade when external knowledge is added due to integration difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9618.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9618.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-based Hallucination Detector (KnowHD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced here that decomposes LLM-generated hypotheses and their rationale into atomic claims, retrieves supporting evidence from knowledge graphs and/or literature, and computes a groundedness score indicating the fraction of atomic claims supported by retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-assisted verification pipeline (uses the same LLMs for decomposition and entailment judgments; retrieval via BM25 over PubMed and graph lookup in PubTator 3.0).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>KnowHD uses prompt-based LLM parsing to split rationales into atomic claims, then retrieves evidence from (a) PubMed via BM25 and (b) PubTator knowledge graph multi-hop edges; the LLM is prompted again to judge whether retrieved context entails each claim, producing a groundedness score (fraction of entailed claims).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Top-k documents retrieved from PubMed corpus (PMID ≤ 36600000) using BM25, and relevant edges from PubTator 3.0 knowledge graph; retrieval thresholds (τ) and k used to control relevance and efficiency (k=8 for claim verification in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Verification of atomic claims comprising the reasoning steps and claims that support a candidate scientific hypothesis produced by an LLM, to determine groundedness relative to existing literature and knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Decompose rationales into atomic claims via prompting; for each claim, retrieve top-k PubMed chunks via BM25 and/or extract relevant KG edges from PubTator; prompt LLM to judge entailment of claim by concatenated context; compute groundedness as mean of claim entailment indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Groundedness score per hypothesis (0–100%), annotated per-claim entailment judgments, and identification of unsupported (hallucinated) reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example output: {"claims": ["Chemical X upregulates Gene Y"], "context_docs": [...], "entailment": [true], "groundedness": 100}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Correlation analysis between groundedness scores and hypothesis truthfulness (accuracy on TruthHypo); human expert annotation on open-ended tasks comparing high- vs low-groundedness hypotheses; reporting of percentage of claims verified under KG, literature, and combined contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>KnowHD with literature supported ~76.3% of claims for literature-augmented Chemical–Gene rationales; KG-only supported ~51.08% in some settings; combined KG+Literature produced highest groundedness. Higher groundedness scores correlated with higher hypothesis accuracy (e.g., hypotheses with groundedness >80% had substantially higher mean accuracy). Groundedness-based selection improved final hypothesis accuracy over baselines when external knowledge was present.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Granular, claim-level verification allows isolation of hallucinated reasoning steps; flexible to use KG, literature, or both; provides a quantitative groundedness signal that correlates with truthfulness and improves selection of candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on retrieval quality (BM25 exact-term matching) and on the coverage of PubTator/PubMed; entailment judgments rely on LLM prompts which can themselves hallucinate; computational cost due to many retrievals and LLM queries for claim decomposition and checking.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Claims that are true but unsupported in the used corpus (false negatives) are marked ungrounded; LLM-based entailment judgments can be mistaken, especially for nuanced or implicit claims; KG-only contexts often failed to verify many claims that literature could.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9618.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9618.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG_BM25_PubMed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using BM25 over PubMed (as implemented in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's retrieval-augmented generation pipeline uses BM25 (MedRAG-processed PubMed chunks) to retrieve document context for LLM hypothesis generation and claim verification, providing literature grounding for generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG pipeline with BM25 retriever (BM25 + MedRAG toolkit) and LLMs for generation/verification</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A retrieval-augmented workflow in which BM25 (selected due to robustness for biomedical term matching) ranks PubMed chunks produced by the MedRAG toolkit; top-k documents (k=32 for hypothesis generation; k=8 for claim verification) are concatenated into context appended to prompts for LLM generation or entailment checks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>PubMed corpus (articles with PMID ≤ 36600000 to match 'seen' knowledge); text is chunked and indexed by MedRAG for BM25 retrieval; retrieval threshold τ set to 0.0 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Retrieve literature evidence relevant to atomic claims or to entity-pair hypothesis generation queries to ground LLM outputs in pre-2023 biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>BM25-based retrieval of top-k PubMed chunks for each claim/query, concatenation of retrieved snippets as context in LLM prompts (RAG); used both for forward hypothesis generation (provide docs then ask model to hypothesize) and for backward claim verification (provide docs then ask model to judge entailment).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>LLM-generated hypotheses/rationales that include or are conditioned on retrieved literature context; retrieved evidence lists used by KnowHD for groundedness scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example retrieval-assisted prompt context: "[Top relevant PubMed snippets about Gene Y and Chemical X] ... Based on the above, hypothesize the potential relation between Chemical X and Gene Y."</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare LLM performance (F1, accuracy) across settings with and without literature augmentation; measure percent of atomic claims verified by literature via KnowHD; human evaluation on open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Literature augmentation improved groundedness and, for larger models, improved accuracy; literature-based KnowHD verification supported ~76.3% of claims in some settings. BM25 chosen due to its effectiveness with biomedical terms over dense retrievers in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Simple, robust retrieval that captures term-specific biomedical relevance (effective for gene names etc.); effective in improving groundedness when combined with KG and larger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>BM25 relies on lexical overlap and can miss semantically relevant documents; retrieval quality determines verification results; k and τ choices impact compute and coverage; corpus limited to pre-2023 articles in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When retrieved documents do not contain explicit support for a true claim, KnowHD marks claims ungrounded (false negative); smaller LLMs sometimes cannot integrate retrieved context effectively, degrading performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9618.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9618.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o and GPT-4o-mini (GPT-4 family variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary GPT-4 family models used in experiments; GPT-4o achieved the highest overall accuracy on TruthHypo and demonstrated improved ability to use external KG and literature context compared to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (large proprietary model) and GPT-4o-mini (smaller variant of GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language models from the GPT-4 family; in the paper they are described as trained on knowledge available before 2024 and used in prompt-based generation and for entailment judgments; GPT-4o-mini is a smaller variant with reduced capabilities relative to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used with parametric-only prompts and with external augmentations: PubTator KG multi-hop textualized chains, and BM25-retrieved PubMed documents (pre-2023).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Entity-pair hypothesis generation across Chemical–Gene, Disease–Gene, and Gene–Gene relation prediction tasks; open-ended hypothesis generation dataset (Qi et al.) for human evaluation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based hypothesis generation with variants: parametric-only; parametric+KG (textualized KG chains); parametric+Lit (BM25 RAG); combined parametric+KG+Lit. Also used in KnowHD to parse claims and make entailment judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Candidate scientific hypotheses with rationales; for multi-candidate generation, selection of final hypothesis via highest KnowHD groundedness score.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Representative (format) output: "Hypothesis: Chemical X stimulates Gene Y (rationale: ... citations/snippets ...)"</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured via TruthHypo link-level F1 and relation-level accuracy; groundedness correlation analysis and human expert selection studies comparing high vs low-groundedness hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o achieved mean accuracies exceeding 60% and showed the largest improvements when augmented with KG + literature (e.g., +5.14% accuracy improvement cited for larger models vs smaller variants). Groundedness-based selection with GPT-4o-mini improved accuracy substantially when external knowledge was included.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Higher precision and better ability to leverage external knowledge sources; greater benefit from KG + literature augmentation than smaller models; groundedness selection brings performance closer to larger model baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still prone to hallucination; accuracy not perfect (many hypotheses remain untruthful); evaluation dependent on external corpus coverage; GPT-4o-mini has limited gains relative to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even GPT-4o sometimes mislabels the specific relation despite detecting a link (higher F1 but lower relation-level accuracy); examples include incorrect inference of 'no relation' when evidence exists, and parroting context without genuine reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9618.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9618.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B and Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Llama-3.1 family models evaluated in the benchmark; larger Llama-3.1-70B outperformed the 8B variant and benefitted more from external KG and literature augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B and Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs from the Llama-3 family; used in prompt-based hypothesis generation experiments across the four knowledge settings; models trained on knowledge prior to 2024 per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same as other models: parametric-only and augmented with PubTator KG (textualized chains) and BM25-retrieved PubMed (pre-2023) chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Entity-pair hypothesis generation tasks (Chemical–Gene, Disease–Gene, Gene–Gene) on the TruthHypo benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based generation with the same templates as other models; external KG and literature provided as textual context; candidate selection performed with KnowHD groundedness in multi-candidate settings.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated hypotheses and rationales; model outputs used to compute link-level and relation-level metrics and groundedness evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Template-style output: "Hypothesis: [relation label] — Rationale: ..."</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>TruthHypo F1 and accuracy, precision/recall breakdowns, groundedness verification via KnowHD, and comparison across sizes and knowledge settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Llama-3.1-70B outperformed the 8B variant, showing higher accuracy and better utilization of KG and literature; smaller Llama (8B) sometimes experienced decreased performance when external KG and literature were added (integration difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Larger open-source Llama variant shows capability gains and benefits from external knowledge; provides an open reference point for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller variant (8B) has high recall but low precision, producing many false positives; some models' integration of external knowledge can disrupt internal reasoning and lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High false positive rate in smaller model outputs; failures include misintegration of KG/literature causing degraded relation-level accuracy and hallucinated reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge. <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models. <em>(Rating: 2)</em></li>
                <li>Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment. <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research. <em>(Rating: 1)</em></li>
                <li>Improving scientific hypothesis generation with knowledge grounded large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9618",
    "paper_id": "paper-278769611",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "TruthHypo",
            "name_full": "Truthful Hypothesis Generation Benchmark (TruthHypo)",
            "brief_description": "A biomedical benchmark introduced in this paper to evaluate LLMs' ability to generate truthful scientific hypotheses by simulating temporal knowledge splits using PubTator 3.0 and PubMed-derived corpora and offering link- and relation-level evaluation metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-4o, GPT-4o-mini, Llama-3.1-70B, Llama-3.1-8B)",
            "model_description": "Benchmark is evaluated with a set of LLMs including open-source Llama-3.1 family (open-source) and proprietary GPT-4 family; models were used in prompt-based settings with or without external knowledge augmentation (KG, literature, or both). All models were trained on knowledge available before 2024 per the paper.",
            "model_size": null,
            "input_corpus_description": "Structured knowledge from PubTator 3.0 (biomedical knowledge graph with annotated relations) with a temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) subsets; unstructured literature retrieval from PubMed (articles with PMID ≤ 36600000) via BM25; tasks focus on Chemical–Gene, Disease–Gene, and Gene–Gene entity pairs with added negative examples.",
            "input_corpus_size": null,
            "topic_query_description": "Given two biomedical entities (e.g., chemical and gene), the LLM must hypothesize the potential relation between them (labels: specific relation types or 'no relation'), simulating discovery of relations absent from 'seen' knowledge (i.e., future/unseen discoveries).",
            "distillation_method": "Prompt-driven hypothesis generation under four settings: (1) parametric knowledge only; (2) parametric + knowledge graph (multi-hop link chains mapped to textual descriptions); (3) parametric + literature (retrieval-augmented generation using BM25 over PubMed); (4) combined parametric + KG + literature. Models produce rationales and hypotheses which are decomposed into atomic claims for verification.",
            "output_type": "Generated scientific hypotheses (relation predictions between entity pairs) plus model rationales; dataset-level metrics (link-level predictions) and groundedness scores when combined with KnowHD.",
            "output_example": "Example (template style): \"Can we hypothesize the potential relation between Chemical X (ID) and Gene Y (ID)? Final hypothesis: [positive correlate | negative correlate | inhibit | stimulate | no relation]\"",
            "evaluation_method": "Link-level precision/recall/F1 and relation-level accuracy computed against held-out 'unseen' relations from PubTator; groundedness verification via KnowHD (atomic claim retrieval + entailment judgments); human expert annotation for open-ended tasks.",
            "evaluation_results": "Most LLMs struggled to generate truthful hypotheses; only GPT-4o exceeded ~60% mean accuracy in aggregate. Link-level F1 &gt; relation-level accuracy, indicating models often detect potential connections but mislabel precise relations. Groundedness scores from KnowHD correlated positively with accuracy and could be used to filter higher-truthfulness hypotheses.",
            "strengths": "Provides temporally realistic benchmark using structured KG + literature; enables controlled evaluation across parametric and retrieval-augmented settings; supports multi-faceted evaluation (link-level, relation-level, groundedness).",
            "limitations": "Relies on PubTator/PubMed temporal split and multi-article filtering which may limit generality beyond biomedical domain; input corpus size unspecified; benchmark focuses on specific relation types and pairwise queries (Chemical–Gene, Disease–Gene, Gene–Gene), so broader theory synthesis tasks are not directly benchmarked.",
            "failure_cases": "LLMs sometimes produce false negatives (miss true associations) or merely echo/paraphrase provided context without substantive new reasoning; smaller models occasionally degrade when external knowledge is added due to integration difficulties.",
            "uuid": "e9618.0",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "KnowHD",
            "name_full": "Knowledge-based Hallucination Detector (KnowHD)",
            "brief_description": "A framework introduced here that decomposes LLM-generated hypotheses and their rationale into atomic claims, retrieves supporting evidence from knowledge graphs and/or literature, and computes a groundedness score indicating the fraction of atomic claims supported by retrieved context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-assisted verification pipeline (uses the same LLMs for decomposition and entailment judgments; retrieval via BM25 over PubMed and graph lookup in PubTator 3.0).",
            "model_description": "KnowHD uses prompt-based LLM parsing to split rationales into atomic claims, then retrieves evidence from (a) PubMed via BM25 and (b) PubTator knowledge graph multi-hop edges; the LLM is prompted again to judge whether retrieved context entails each claim, producing a groundedness score (fraction of entailed claims).",
            "model_size": null,
            "input_corpus_description": "Top-k documents retrieved from PubMed corpus (PMID ≤ 36600000) using BM25, and relevant edges from PubTator 3.0 knowledge graph; retrieval thresholds (τ) and k used to control relevance and efficiency (k=8 for claim verification in experiments).",
            "input_corpus_size": null,
            "topic_query_description": "Verification of atomic claims comprising the reasoning steps and claims that support a candidate scientific hypothesis produced by an LLM, to determine groundedness relative to existing literature and knowledge graphs.",
            "distillation_method": "Decompose rationales into atomic claims via prompting; for each claim, retrieve top-k PubMed chunks via BM25 and/or extract relevant KG edges from PubTator; prompt LLM to judge entailment of claim by concatenated context; compute groundedness as mean of claim entailment indicators.",
            "output_type": "Groundedness score per hypothesis (0–100%), annotated per-claim entailment judgments, and identification of unsupported (hallucinated) reasoning steps.",
            "output_example": "Example output: {\"claims\": [\"Chemical X upregulates Gene Y\"], \"context_docs\": [...], \"entailment\": [true], \"groundedness\": 100}",
            "evaluation_method": "Correlation analysis between groundedness scores and hypothesis truthfulness (accuracy on TruthHypo); human expert annotation on open-ended tasks comparing high- vs low-groundedness hypotheses; reporting of percentage of claims verified under KG, literature, and combined contexts.",
            "evaluation_results": "KnowHD with literature supported ~76.3% of claims for literature-augmented Chemical–Gene rationales; KG-only supported ~51.08% in some settings; combined KG+Literature produced highest groundedness. Higher groundedness scores correlated with higher hypothesis accuracy (e.g., hypotheses with groundedness &gt;80% had substantially higher mean accuracy). Groundedness-based selection improved final hypothesis accuracy over baselines when external knowledge was present.",
            "strengths": "Granular, claim-level verification allows isolation of hallucinated reasoning steps; flexible to use KG, literature, or both; provides a quantitative groundedness signal that correlates with truthfulness and improves selection of candidate hypotheses.",
            "limitations": "Depends on retrieval quality (BM25 exact-term matching) and on the coverage of PubTator/PubMed; entailment judgments rely on LLM prompts which can themselves hallucinate; computational cost due to many retrievals and LLM queries for claim decomposition and checking.",
            "failure_cases": "Claims that are true but unsupported in the used corpus (false negatives) are marked ungrounded; LLM-based entailment judgments can be mistaken, especially for nuanced or implicit claims; KG-only contexts often failed to verify many claims that literature could.",
            "uuid": "e9618.1",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RAG_BM25_PubMed",
            "name_full": "Retrieval-Augmented Generation using BM25 over PubMed (as implemented in this paper)",
            "brief_description": "The paper's retrieval-augmented generation pipeline uses BM25 (MedRAG-processed PubMed chunks) to retrieve document context for LLM hypothesis generation and claim verification, providing literature grounding for generated hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RAG pipeline with BM25 retriever (BM25 + MedRAG toolkit) and LLMs for generation/verification",
            "model_description": "A retrieval-augmented workflow in which BM25 (selected due to robustness for biomedical term matching) ranks PubMed chunks produced by the MedRAG toolkit; top-k documents (k=32 for hypothesis generation; k=8 for claim verification) are concatenated into context appended to prompts for LLM generation or entailment checks.",
            "model_size": null,
            "input_corpus_description": "PubMed corpus (articles with PMID ≤ 36600000 to match 'seen' knowledge); text is chunked and indexed by MedRAG for BM25 retrieval; retrieval threshold τ set to 0.0 in experiments.",
            "input_corpus_size": null,
            "topic_query_description": "Retrieve literature evidence relevant to atomic claims or to entity-pair hypothesis generation queries to ground LLM outputs in pre-2023 biomedical literature.",
            "distillation_method": "BM25-based retrieval of top-k PubMed chunks for each claim/query, concatenation of retrieved snippets as context in LLM prompts (RAG); used both for forward hypothesis generation (provide docs then ask model to hypothesize) and for backward claim verification (provide docs then ask model to judge entailment).",
            "output_type": "LLM-generated hypotheses/rationales that include or are conditioned on retrieved literature context; retrieved evidence lists used by KnowHD for groundedness scoring.",
            "output_example": "Example retrieval-assisted prompt context: \"[Top relevant PubMed snippets about Gene Y and Chemical X] ... Based on the above, hypothesize the potential relation between Chemical X and Gene Y.\"",
            "evaluation_method": "Compare LLM performance (F1, accuracy) across settings with and without literature augmentation; measure percent of atomic claims verified by literature via KnowHD; human evaluation on open-ended tasks.",
            "evaluation_results": "Literature augmentation improved groundedness and, for larger models, improved accuracy; literature-based KnowHD verification supported ~76.3% of claims in some settings. BM25 chosen due to its effectiveness with biomedical terms over dense retrievers in this domain.",
            "strengths": "Simple, robust retrieval that captures term-specific biomedical relevance (effective for gene names etc.); effective in improving groundedness when combined with KG and larger LLMs.",
            "limitations": "BM25 relies on lexical overlap and can miss semantically relevant documents; retrieval quality determines verification results; k and τ choices impact compute and coverage; corpus limited to pre-2023 articles in experiments.",
            "failure_cases": "When retrieved documents do not contain explicit support for a true claim, KnowHD marks claims ungrounded (false negative); smaller LLMs sometimes cannot integrate retrieved context effectively, degrading performance.",
            "uuid": "e9618.2",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o family",
            "name_full": "GPT-4o and GPT-4o-mini (GPT-4 family variants)",
            "brief_description": "Proprietary GPT-4 family models used in experiments; GPT-4o achieved the highest overall accuracy on TruthHypo and demonstrated improved ability to use external KG and literature context compared to smaller models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (large proprietary model) and GPT-4o-mini (smaller variant of GPT-4o)",
            "model_description": "Proprietary large language models from the GPT-4 family; in the paper they are described as trained on knowledge available before 2024 and used in prompt-based generation and for entailment judgments; GPT-4o-mini is a smaller variant with reduced capabilities relative to GPT-4o.",
            "model_size": null,
            "input_corpus_description": "Used with parametric-only prompts and with external augmentations: PubTator KG multi-hop textualized chains, and BM25-retrieved PubMed documents (pre-2023).",
            "input_corpus_size": null,
            "topic_query_description": "Entity-pair hypothesis generation across Chemical–Gene, Disease–Gene, and Gene–Gene relation prediction tasks; open-ended hypothesis generation dataset (Qi et al.) for human evaluation experiments.",
            "distillation_method": "Prompt-based hypothesis generation with variants: parametric-only; parametric+KG (textualized KG chains); parametric+Lit (BM25 RAG); combined parametric+KG+Lit. Also used in KnowHD to parse claims and make entailment judgments.",
            "output_type": "Candidate scientific hypotheses with rationales; for multi-candidate generation, selection of final hypothesis via highest KnowHD groundedness score.",
            "output_example": "Representative (format) output: \"Hypothesis: Chemical X stimulates Gene Y (rationale: ... citations/snippets ...)\"",
            "evaluation_method": "Measured via TruthHypo link-level F1 and relation-level accuracy; groundedness correlation analysis and human expert selection studies comparing high vs low-groundedness hypotheses.",
            "evaluation_results": "GPT-4o achieved mean accuracies exceeding 60% and showed the largest improvements when augmented with KG + literature (e.g., +5.14% accuracy improvement cited for larger models vs smaller variants). Groundedness-based selection with GPT-4o-mini improved accuracy substantially when external knowledge was included.",
            "strengths": "Higher precision and better ability to leverage external knowledge sources; greater benefit from KG + literature augmentation than smaller models; groundedness selection brings performance closer to larger model baselines.",
            "limitations": "Still prone to hallucination; accuracy not perfect (many hypotheses remain untruthful); evaluation dependent on external corpus coverage; GPT-4o-mini has limited gains relative to GPT-4o.",
            "failure_cases": "Even GPT-4o sometimes mislabels the specific relation despite detecting a link (higher F1 but lower relation-level accuracy); examples include incorrect inference of 'no relation' when evidence exists, and parroting context without genuine reasoning.",
            "uuid": "e9618.3",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1 family",
            "name_full": "Llama-3.1-8B and Llama-3.1-70B",
            "brief_description": "Open-source Llama-3.1 family models evaluated in the benchmark; larger Llama-3.1-70B outperformed the 8B variant and benefitted more from external KG and literature augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B and Llama-3.1-70B",
            "model_description": "Open-source LLMs from the Llama-3 family; used in prompt-based hypothesis generation experiments across the four knowledge settings; models trained on knowledge prior to 2024 per paper.",
            "model_size": "8B / 70B",
            "input_corpus_description": "Same as other models: parametric-only and augmented with PubTator KG (textualized chains) and BM25-retrieved PubMed (pre-2023) chunks.",
            "input_corpus_size": null,
            "topic_query_description": "Entity-pair hypothesis generation tasks (Chemical–Gene, Disease–Gene, Gene–Gene) on the TruthHypo benchmark.",
            "distillation_method": "Prompt-based generation with the same templates as other models; external KG and literature provided as textual context; candidate selection performed with KnowHD groundedness in multi-candidate settings.",
            "output_type": "Generated hypotheses and rationales; model outputs used to compute link-level and relation-level metrics and groundedness evaluations.",
            "output_example": "Template-style output: \"Hypothesis: [relation label] — Rationale: ...\"",
            "evaluation_method": "TruthHypo F1 and accuracy, precision/recall breakdowns, groundedness verification via KnowHD, and comparison across sizes and knowledge settings.",
            "evaluation_results": "Llama-3.1-70B outperformed the 8B variant, showing higher accuracy and better utilization of KG and literature; smaller Llama (8B) sometimes experienced decreased performance when external KG and literature were added (integration difficulty).",
            "strengths": "Larger open-source Llama variant shows capability gains and benefits from external knowledge; provides an open reference point for reproducibility.",
            "limitations": "Smaller variant (8B) has high recall but low precision, producing many false positives; some models' integration of external knowledge can disrupt internal reasoning and lower performance.",
            "failure_cases": "High false positive rate in smaller model outputs; failures include misintegration of KG/literature causing degraded relation-level accuracy and hallucinated reasoning steps.",
            "uuid": "e9618.4",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge.",
            "rating": 2,
            "sanitized_title": "pubtator_30_an_aipowered_literature_resource_for_unlocking_biomedical_knowledge"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment.",
            "rating": 2,
            "sanitized_title": "scientific_hypothesis_generation_by_a_large_language_model_laboratory_validation_in_breast_cancer_treatment"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "rating": 1,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Improving scientific hypothesis generation with knowledge grounded large language models.",
            "rating": 1,
            "sanitized_title": "improving_scientific_hypothesis_generation_with_knowledge_grounded_large_language_models"
        }
    ],
    "cost": 0.0147485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 2025</p>
<p>Guangzhi Xiong aidong@virginia.edu 
University of Virginia</p>
<p>Eric Xie 
University of Virginia</p>
<p>Corey Williams 
University of Virginia</p>
<p>Myles Kim 
University of Virginia</p>
<p>Amir Hassan Shariatmadari 
University of Virginia</p>
<p>Sikun Guo 
University of Virginia</p>
<p>Stefan Bekiranov 
University of Virginia</p>
<p>Aidong Zhang 
University of Virginia</p>
<p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 202527206A855BB799669A959D59A0E3FCB5arXiv:2505.14599v2[cs.CL]
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions.However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources.Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability.To facilitate the systematic study of these challenges, we introduce Truth-Hypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge.Our results show that LLMs struggle to generate truthful hypotheses.By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs.Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery.Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have transformed the landscape of artificial intelligence, demonstrating remarkable capabilities across diverse applications, from natural language understanding to creative content generation [Karanikolas et al., 2023;Franceschelli and Musolesi, 2024;Raiaan et al., 2024].These models, trained on extensive corpora of text, demonstrate an ability to analyze, summarize, and generate human-like text, enabling advancements across diverse domains.Recently, there has been a growing interest in leveraging LLMs for scientific discovery [Zhong et al., 2023;Yang et al., 2023;Kumar et al., 2023;Liu et al., 2024;Baek et al., 2024;Guo et al., 2024].Their capacity to process and synthesize vast amounts of scientific literature positions them as valuable tools in aiding researchers, particularly for tasks such as literature reviews, summarization, and even generating new hypotheses [Qi et al., 2023;Zhou et al., 2024;M. Bran et al., 2024;Wright et al., 2022;Zeng et al., 2023;D'Arcy et al., 2024;Ifargan et al., 2025;Yang et al., 2025].</p>
<p>One particularly promising application of LLMs is their use in scientific hypothesis generation, where they can assist in identifying promising research directions [Park et al., 2024;Si et al., 2024;Guo et al., 2025].By analyzing extensive scientific literature, LLMs can uncover gaps in existing knowledge and propose novel hypotheses that may not be immediately apparent to human researchers.For instance, LLMs have been successfully applied to propose novel drug combinations for breast cancer treatment, some of which were later validated in laboratory experiments, showcasing their potential to accelerate biomedical discoveries [Abdel-Rehim et al., 2024].</p>
<p>Despite these advancements, there are substantial challenges that limit the practical utility of LLMs in scientific hypothesis generation.A critical concern is the inability to evaluate the truthfulness of generated hypotheses.While LLMs can generate hypotheses that seem plausible, it remains uncertain whether these hypotheses are valid and grounded in existing knowledge or merely hallucinated and scientifically invalid.This issue is further exacerbated by the welldocumented "hallucination" problem, where LLMs confidently produce information that is factually inaccurate or unsupported, posing challenges to their reliability in scientific contexts [Jin et al., 2024].While current research has largely focused on improving the novelty and diversity of LLM-generated hypotheses, their truthfulness and grounding in established knowledge remain underexplored [Baek et al., 2024;Hu et al., 2024;Si et al., 2024].</p>
<p>To address these challenges, we introduce TruthHypo, a comprehensive benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detection framework designed to assess the groundedness of these hypotheses.Truth-Hypo, built on a biomedical knowledge graph along with a domain-specific corpus, provides a controlled environment to evaluate how well LLM-generated hypotheses align with established scientific knowledge.KnowHD focuses on analyzing the reasoning processes of LLMs to identify hypotheses that are likely hallucinated or untruthful.Our findings reveal that LLMs face significant challenges in generating truthful hypotheses.By analyzing hallucinations in the reasoning processes behind generated hypotheses, we demonstrate that groundedness scores from KnowHD serve as an effective signal for identifying truthful hypotheses from the diverse outputs of LLMs.</p>
<p>Human evaluations on open-ended hypothesis generation tasks further confirm the utility of KnowHD in identifying scientifically valid hypotheses.</p>
<p>Our main contributions are summarized as follows:</p>
<p>• We introduce TruthHypo, a comprehensive benchmark designed to evaluate the ability of LLMs to generate truthful scientific hypotheses.</p>
<p>• We propose KnowHD, a knowledge-based hallucination detection framework that assesses the groundedness of LLM-generated hypotheses and identifies hallucinated claims by analyzing the rationale behind the hypothesis generation.</p>
<p>• We provide an extensive analysis of existing LLMs on TruthHypo, highlighting their limitations and challenges in generating truthful hypotheses.</p>
<p>• Our evaluation further reveals the connection between hallucination and truthfulness of generated hypotheses, showing the effectiveness of using KnowHD to select truthful and grounded hypotheses.</p>
<p>Truthful Hypothesis Generation Benchmark</p>
<p>To systematically evaluate the ability of large language models (LLMs) to generate truthful scientific hypotheses, we introduce TruthHypo, a benchmark tailored for biomedical hypothesis generation.TruthHypo is designed to simulate realworld conditions by employing rigorous dataset construction, task formulation, and truthfulness evaluation metrics.An overview of the dataset construction, task formulation, and evaluation framework is depicted in Figure 1.</p>
<p>Dataset Construction</p>
<p>The dataset for TruthHypo is derived from PubTator 3.0 [Wei et al., 2024], a comprehensive biomedical knowledge graph that includes annotated relations (also called edges) extracted from scientific articles.To simulate the temporal progression of scientific discovery, we partitioned the graph into "seen" and "unseen" subsets based on the publication years of the corresponding articles.Relations in the "seen" subset were extracted from papers published before 2023, identified by PMIDs ≤ 366000001 .The "unseen" subset, designed to represent new discoveries, comprises relations extracted from papers published after 2024, identified by PMIDs ≥ 38200000.</p>
<p>To ensure no overlap between the two subsets, we removed the edges in the unseen subset that shared head and tail entities with those in the seen subset.In addition, to maintain quality and validity, only relations discovered by multiple articles in the test data were retained.This filtering process guarantees that the unseen subset exclusively contains knowledge unavailable before 2024, simulating the conditions of future scientific research.</p>
<p>In building the dataset, we focused on three key relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".These relation types were chosen for their complementary nature, detailed annotations, and potential for objective evaluation.To construct comprehensive classification tasks for evaluating different LLMs, we augment the dataset with negative test cases to assess whether LLMs tend to make false-positive predictions on entity pairs that lack a direct relationship in the existing knowledge base.The number of negative samples (labeled as "no relation") for each relation type is controlled to align with the average number of instances across other labels of the same relation type.The final dataset has 1209 instances for the "Chemical &amp; Gene" task, 268 instances for the "Disease &amp; Gene" task, and 547 instances for the "Gene &amp; Gene" task.A summary of the dataset statistics is presented in Table 1</p>
<p>Task Formulation</p>
<p>The TruthHypo benchmark includes three tasks, corresponding to the selected relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".For each task, the input is a hypothesis generation query with two entities (see Figure 5 in Appendix D for the template), and the LLM is required to hypothesize the potential relationship between them based on available knowledge and reasoning.</p>
<p>To comprehensively assess LLM performance, we evaluate their ability to generate hypotheses under various knowledge augmentation settings.In the first setting, LLMs rely solely on their parametric knowledge -information encoded in their parameters during pretraining on large corpora.This evaluates the model's intrinsic understanding and reasoning capabilities.</p>
<p>To enhance hypothesis generation, we introduce a second setting in which LLMs are augmented with structured knowledge from the "seen" knowledge graph.In this approach, key entities from the input are mapped to nodes in the graph, and multi-hop link chains connecting these nodes are explored.These chains, representing relevant relationships, are transformed into textual descriptions and provided as context for the model to use during hypothesis generation.</p>
<p>Another setting leverages information from biomedical literature using a retrieval-augmented generation (RAG) pipeline.Relevant documents are retrieved from the PubMed corpus 2 using BM25 [Robertson et al., 2009].To maintain consistency with the knowledge graph's temporal split, only articles with PMIDs ≤ 36600000 are included in the retrieval.This simulates the process of generating hypotheses based on literature available at a given point in time.</p>
<p>Finally, we consider a combined setting, where both structured knowledge from the graph and unstructured information from retrieved literature are used to support hypothesis generation.This comprehensive approach provides a more holistic context, enabling the model to reason across both sources.The LLM prompt templates we used to combine the external information with the original user instructions can be found in Figures 6, 8, 7, and 9 (Appendix D).</p>
<p>2 https://pubmed.ncbi.nlm.nih.gov/</p>
<p>Evaluation Metrics</p>
<p>To evaluate the quality of generated scientific hypotheses, we employ a set of complementary metrics tailored to different aspects of hypothesis generation.These metrics assess the performance of LLMs in identifying valid connections between entities (link-level evaluation) and predicting specific relations (relation-level evaluation).</p>
<p>For link-level evaluation, we focus on precision, recall, and F1 score.Precision measures the proportion of correctly identified connections among all hypothesized connections, emphasizing the reduction of false positives.Recall evaluates the model's ability to comprehensively identify all valid connections, capturing its sensitivity to true positives.The F1 score, as the harmonic mean of precision and recall, provides a balanced measure of performance, combining both the accuracy of predictions and the coverage of valid connections.These link-level metrics are critical for assessing the LLM's ability to hypothesize plausible relationships between entities, regardless of the specific relation type.</p>
<p>For relation-level evaluation, we employ accuracy to measure how often the generated hypotheses match the correct relation labels in the ground truth.Accuracy captures the overall correctness of hypotheses by considering both the existence of a connection and the predicted relation type.While precision, recall, and F1 focus on identifying potential connections, accuracy provides a finer-grained assessment of the model's capability to generate accurate relation labels.</p>
<p>By combining link-level and relation-level evaluations, the TruthHypo benchmark comprehensively measures the truthfulness of LLM-generated hypotheses, assessing the ability of LLMs to produce scientifically valid outputs.</p>
<p>Knowledge-based Hallucination Detection</p>
<p>As discussed earlier, a critical concern regarding the truthfulness of LLM-generated hypotheses is the occurrence of hallucinations, where models generate plausible-sounding but unsupported claims.To address this, we introduce KnowHD, a knowledge-based hallucination detection framework that evaluates the groundedness of LLM-generated hypotheses by analyzing the rationale behind their generation.KnowHD operates using scientific literature, knowledge graphs, or a combination of both as the knowledge base.An overview of the framework is presented in Figure 2.</p>
<p>To evaluate groundedness, each hypothesis and its reasoning chain are first decomposed into a set of atomic claims.This step is critical because hypotheses often consist of compound reasoning steps, some of which may be supported by existing knowledge while others may not.Parsing these into atomic claims allows a more granular evaluation of groundedness and isolates unsupported components.This step is implemented by prompting LLMs with the template shown in Figure 10 (Appendix D).</p>
<p>When using scientific literature as the knowledge base, relevant documents for each atomic claim are retrieved from the PubMed corpus, limited to articles published before 2023 (PMID ≤ 36600000).BM25 is employed to rank documents based on their relevance to the claim.To ensure computational efficiency and focus on the most relevant information, only the top-k documents are retained.The context retrieved from the literature corpus D for a claim p is defined as:</p>
<p>Rationale of Generated Hypothesis
context D (p) = {d 1 , d 2 , • • • , d k | d i ∈ D, BM25(p; d i ) ≥ τ, rank(d i ) ≤ k},(1)
where
d i represents a document in the corpus, BM25(p; d i )
is the relevance score assigned to the document for the claim p. τ is a threshold ensuring relevance, and rank(d i ) denotes the rank of d i in the BM25-retrieved list.When using a knowledge graph G as the knowledge base, the context for a claim is derived from the graph structure.For a claim p, relevant knowledge is extracted as:
context G (p) = {(e h , r, e t ) ∈ G |{e h , e t } ⊆ V(p) } , (2)
where (e h , r, e t ) represents an edge in the knowledge graph with head entity e h , tail entity e t , and relation r.The set V(p) contains all entities mentioned in the claim p.</p>
<p>The groundedness of a claim is determined based on whether the given context information (context D , context G , or context D ∪ context G ) can fully support the claim, which is implemented by prompting LLMs to provide a judgment using the template in Figure 12 (Appendix D).If the concatenated context collectively entails the claim, it is considered grounded.The overall groundedness of a hypothesis h is computed as:
groudedness(h) = 1 |C(h)| p∈C(h) 1[context(p) |= p], (3)
where C(h) represents the set of atomic claims for hypothesis h, and 1[x |= y] returns 1 if x entails y and 0 otherwise.The
context(p) can be context D (p), context G (p), or context D (p)∪ context G (p).
By offering both literature-based and graph-based contexts, KnowHD provides a robust framework for hallucination detection, offering flexibility to adapt to the available knowledge sources.This systematic evaluation of atomic claims enables a detailed assessment of the groundedness of hypotheses, identifying unsupported components and improving the reliability of LLM-generated outputs.</p>
<p>Benchmark Analysis on TruthHypo</p>
<p>Experiment Settings</p>
<p>To assess the ability of existing LLMs to generate truthful scientific hypotheses, we selected a diverse range of models varying in type and size.The Llama-3 family [Dubey et al., 2024] represents open-source LLMs, while the GPT-4 family [Achiam et al., 2023] exemplifies proprietary models.From each family, we evaluated two LLMs of different sizes (Llama-3.1-8B&amp; Llama-3.1-70B,GPT-4o-mini &amp; GPT-4o) to investigate size-related differences in performance.All LLMs were trained on the knowledge available before 2024, preventing recall of the exact knowledge for hypothesis generation.More implementation details are in Appendix A.</p>
<p>The TruthHypo benchmark evaluates LLMs across four distinct settings: (1) parametric knowledge only, (2) parametric knowledge with knowledge graphs (KG), (3) parametric knowledge with literature (Lit.), and (4) parametric knowledge with both KG and literature.These settings allow us to explore the impact of external knowledge sources on hypothesis generation.The F1 and accuracy scores of different models are reported in this section.More detailed results on the precision and recall can be found in Appendix C.</p>
<p>Comparison of LLMs in Truthful Hypothesis Generation</p>
<p>Table 2 presents the evaluation results for different LLMs and knowledge settings on TruthHypo.Across all tasks, the results indicate that most LLMs struggle to generate truthful scientific hypotheses, with only GPT-4o achieving mean accuracies exceeding 60%.Additionally, we can observe that link-level F1 scores are higher than relation-level accuracy scores, which indicates that LLMs can identify potential connections between entities but often fail to accurately predict the specific relationships.</p>
<p>For models from the same family with different sizes, larger LLMs tend to generate scientific hypotheses more likely to be truthful.This can be attributed to two main factors.First, larger LLMs generally perform better because they can store and leverage more knowledge in their parameters, as shown by the results of parametric knowledge-only setting.Second, LLMs of different sizes have diverse capabilities to process external knowledge for hypothesis generation.For example, GPT-4o-mini shows a modest 1.14% accuracy improvement when augmented with KG and literature, whereas GPT-4o achieves a more substantial 5.14% increase under the same conditions.This suggests that larger LLMs can better utilize additional context to reason about truthful scientific hypotheses.Similar trends are observed when comparing Llama-3.1-8B and Llama-3.1-70B.Interestingly, smaller models, such as Llama-3.1-8B,sometimes experience decreased performance when information from KG and literature is introduced.This degradation may stem from challenges in effectively integrating internal and external information, which can disrupt the model's reasoning processes.Performance differences are also observed across the three relation types: "Chemical &amp; Gene", "Disease &amp; Gene" and "Gene &amp; Gene".Notably, all larger models, including GPT-4o, GPT-4o-mini, and Llama-3.1-70B,tend to perform better on "Chemical &amp; Gene" tasks than on the other two types.This trend suggests that the "Chemical &amp; Gene" task may be more aligned with the pre-trained knowledge or reasoning capabilities of these models.In contrast, the smaller Llama-3.1-8Bshows a more inconsistent pattern, with performance varying across tasks and settings, likely reflecting its more limited parametric capacity and reasoning abilities.These variations in performance across relation types may be attributed to differences in training data distributions or the complexity of the relation types themselves.The relatively stronger performance on the "Chemical &amp; Gene" task highlights potential domain-specific biases or strengths in the LLMs, offering insights into their suitability for targeted applications in real-world scientific discovery.</p>
<p>Hallucination Detection on LLM-generated Hypotheses</p>
<p>To assess the groundedness of the generated hypotheses, we evaluated their rationales using KnowHD under various knowledge settings.KnowHD measures how well a hypothesis is supported by structured knowledge (KG), unstructured knowledge (literature), or both combined.The groundedness evaluation results for hypotheses generated by GPT-4o-mini are presented in Table 3.</p>
<p>The results demonstrate distinct contributions of KG and literature to grounding hypotheses.For example, KnowHD with the literature as the support knowledge base can verify 76.30% claims in the rationales of literature-augmented 'Chemical &amp; Gene" hypotheses.However, the hallucination detector can hardly verify the rationale generated based on adding KG information to parametric knowledge with only 51.08% of the claims being grounded.Combining KG and literature yields the highest groundedness scores, effectively leveraging the complementary strengths of both sources to identify grounded claims and detect hallucinations.</p>
<p>To further explore the relationship between hallucination and truthfulness, Figure 3 examines mean accuracy as a function of groundedness scores.Hypotheses were grouped based on their groundedness scores, and the average accuracy for each group was calculated.The figure reveals a positive correlation between groundedness scores and hypothesis truthfulness.As groundedness scores increase, the likelihood  of the hypothesis being truthful also increases.For example, GPT-4o-mini achieves a mean accuracy of 60.96% on "Chemical &amp; Gene" tasks under the combined KG + Literature setting, but this rises to 72.77% for hypotheses with groundedness scores above 80%.These findings underscore the potential of KnowHD to identify hypotheses with a higher probability of being truthful, particularly in contexts enriched with external knowledge.</p>
<p>Improving Generation of Truthful Hypotheses with KnowHD</p>
<p>To validate the utility of KnowHD on enhancing hypothesis generation, we prompted LLMs to generate five candidate hypotheses for each input and selected the one with the highest groundedness score as the final output.This approach was compared to two baselines: the greedy search method, where the hypothesis is generated using greedy next-token selection by the LLM, and the self-consistency method [Wang et al., 2022], which selects hypotheses based on majority voting across multiple predictions.As shown in Figure 4, groundedness-based hypothesis selection generally outperforms both the greedy search and majority-voting methods across most knowledge settings.In the parametric knowledge-only setting, the majority-voting method achieves slightly higher accuracy (61.86%) compared to groundedness-based selection (59.83%).However, as external knowledge is introduced, groundedness-based selection demonstrates consistent improvements over both baselines.For example, in the combined parametric + KG + Literature setting, GPT-4o-mini achieves an average accuracy of 63.44% when groundedness-based selection is used, approaching the performance of the larger GPT-4o model.</p>
<p>These results highlight the effectiveness of groundedness scores in scenarios where external knowledge is incorporated, as they help identify hypotheses that are more likely to be truthful.By detecting hallucinations in reasoning steps and focusing on grounded hypotheses, KnowHD provides a robust mechanism for enhancing the reliability and truthfulness of LLM-generated scientific hypotheses.</p>
<p>Human Study on Open-ended Tasks</p>
<p>To further assess the generalizability of KnowHD's effectiveness in selecting truthful hypotheses, we conducted experiments on open-ended hypothesis generation tasks.These tasks were designed to evaluate whether KnowHD could reliably identify hypotheses with a higher likelihood of truthfulness across broader and less structured generation scenarios.</p>
<p>For this analysis, we utilized the publicly available hypothesis generation dataset introduced by Qi et al. [2024], which involves generating free-form hypotheses based on given background information.We selected GPT-4o-mini as the tested LLM and enhanced its hypothesis generation process by incorporating external knowledge from scientific literature and knowledge graphs (KG).The model was prompted to generate five distinct scientific hypotheses for each input.These hypotheses were then evaluated by KnowHD, which assessed their groundedness based on their alignment with both structured (KG) and unstructured (literature) knowledge sources.</p>
<p>To analyze the relationship between groundedness scores and hypothesis truthfulness, we filtered generated hypotheses to create pairs with contrasting groundedness levels.For each input, we identified one hypothesis with the highest ground-  edness score and another with the lowest.We retained pairs where the higher groundedness score was at 30% greater than the lower score.This filtering resulted in 54 pairs of hypotheses with significant differences in groundedness levels.To validate KnowHD's effectiveness, we involved two domain experts to annotate each pair (80% agreement), selecting the hypothesis they deemed more likely truthful based on the given information.Additionally, GPT-4o was prompted to analyze the same pairs and provide its judgment.Results of this annotation study, summarized in Table 4, report the selection ratio for each group, defined as the proportion of hypotheses in each group identified as more truthful.</p>
<p>The results demonstrate a significant relationship between groundedness scores and the perceived truthfulness of hypotheses.Hypotheses with higher groundedness scores were consistently more likely to be selected as truthful by both human experts and GPT-4o, as indicated by the substantial differences in selection ratios.These findings highlight the utility of KnowHD in distinguishing truthful hypotheses, even in unstructured, open-ended generation tasks.By effectively leveraging groundedness as a criterion, KnowHD provides a robust mechanism for improving the reliability of LLMgenerated hypotheses, reinforcing its potential for facilitating real-world scientific discovery processes.</p>
<p>6 Related Work</p>
<p>Scientific Hypothesis Generation</p>
<p>The use of LLMs for scientific hypothesis generation is a rapidly growing field, leveraging the ability of these models to process and synthesize vast amounts of scientific literature [Qi et al., 2023;Yang et al., 2023;Zhou et al., 2024;Ciucȃ et al., 2023;Skarlinski et al., 2024;Radensky et al., 2024;Xiong et al., 2024c;Guo et al., 2024].LLMs have been applied in identifying research gaps and generating novel hypotheses, with notable successes in areas such as drug discovery, where generated hypotheses have led to experimentally validated drug combinations [Abdel-Rehim et al., 2024].Despite these advancements, most existing studies emphasize the novelty and diversity of hypotheses without addressing the critical aspect of truthfulness [Qi et al., 2024;Baek et al., 2024;Wang et al., 2023;Hu et al., 2024;Li et al., 2024].The prevalent hallucination problem exacerbates this issue, as LLMs often generate hypotheses that appear plausible but lack factual support [Huang et al., 2023].This gap motivates the development of TruthHypo, a benchmark explicitly designed to assess the ability of LLMs to generate truthful and grounded scientific hypotheses.</p>
<p>Knowledge Graph Reasoning</p>
<p>Knowledge graph reasoning involves inferring missing facts or relationships within a knowledge graph, with tasks such as link prediction, entity classification, and relation extraction being extensively studied [Nickel et al., 2015;Lin et al., 2015;Ji et al., 2021;Shu et al., 2024].Traditional link prediction focuses on predicting edges between entities based on graph structure.These tasks primarily target structured graph completion, emphasizing pattern detection rather than creative reasoning [Zhang and Chen, 2018;Krenn et al., 2023;Liu et al., 2023;Wu et al., 2023;Gu and Krenn, 2024].TruthHypo introduces a novel benchmark that centers on LLM-driven scientific hypothesis generation, leveraging LLMs' ability to flexibly integrate external knowledge through contextual inputs.Unlike static graph reasoning, TruthHypo evaluates how well LLMs generate grounded and truthful hypotheses.This shift highlights the growing role of LLMs in scientific discovery and bridges the gap between symbolic graph reasoning and natural language creativity.</p>
<p>Retrieval-augmented Generation</p>
<p>Retrieval-augmented generation (RAG) has emerged as a powerful approach for improving the factual accuracy and relevance of LLM outputs by integrating external knowledge during the generation process.This technique has been applied with literature retrieval, as demonstrated by [Lewis et al., 2020], to dynamically incorporate up-to-date information into model outputs.Retrieval-augmented generation methods enhance the ability of LLMs to ground their outputs in external knowledge, making them particularly valuable in tasks requiring factual accuracy, such as scientific text generation [Lála et al., 2023;Munikoti et al., 2023].In addition to literature retrieval, retrieval-augmented generation using knowledge graphs has gained attention for its potential to provide structured, domain-specific knowledge during text generation [Peng et al., 2024;Ma et al., 2024;Wang et al., 2025].Truth-Hypo builds on this paradigm by integrating both literature and knowledge graph retrieval to provide a robust evaluation of LLMs' ability to generate truthful scientific hypotheses.This dual approach enables a comprehensive analysis of the role of external knowledge in mitigating hallucinations and ensuring the groundedness of generated hypotheses.</p>
<p>Conclusion</p>
<p>We presented TruthHypo, a benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a framework for detecting hallucinations by assessing groundedness in reasoning.Through extensive evaluation, we highlighted the limitations of existing LLMs and demonstrated that selecting highly grounded hypotheses improves truthfulness.These contributions offer valuable insights for improving the reliability and utility of LLMs in scientific discovery.</p>
<p>A Implementation Details</p>
<p>For the retrieval of external knowledge from scientific literature, we implemented the information retrieval system by adopting the BM25 retriever [Robertson et al., 2009] for processed PubMed chunks provided by the MedRAG toolkit [Xiong et al., 2024a,b].BM25 (Best Matching 25) is a probabilistic retrieval model that ranks documents based on term frequency, document length normalization, and the specificity of terms through inverse document frequency (IDF).We selected BM25 as our text retriever because it is particularly effective for the biomedical domain, where dense retrievers often struggle to encode the nuanced semantics of biomedical terms such as gene names [Luo et al., 2022].BM25's reliance on exact term matching with statistical weighting makes it well-suited for capturing term-specific relevance in structured biomedical text.In our experiments, τ in Equation ( 1) is set as 0.0.The number of retrieved documents is set as k = 32 for hypothesis generation, and k = 8 for claim verification.</p>
<p>To identify biomedical entities in a given claim, we used a two-step process.In the first step, we prompted LLMs to extract the entity mentions directly from the claim (Figure 11).This step focused on identifying relevant biomedical terms, such as gene names, proteins, or diseases, without additional processing or complex workflows.The extracted entity mentions were then used in the second step, where each mention was matched to its unified representation in the PubTator 3.0 knowledge graph.This matching was implemented using a BM25 retriever.For constructing the BM25 index, each piece of text, or "chunk", was designed by concatenating all possible mentions of a given entity stored in PubTator 3.0.By leveraging BM25's ranking capabilities, we retrieved the most relevant chunk corresponding to each entity mention, ensuring accurate alignment with PubTator's unified entities.</p>
<p>B Computational Cost</p>
<p>Table 5 shows the number of all tokens used in experiments for Table 2.It shows that the additional knowledge from either the knowledge graph (KG) or literature (Lit.) will significantly increase the number of input tokens.In particular, the literature brings more tokens than KG, as the knowledge in KG is always structured and summarized.While input lengths vary across different settings, output lengths are relatively stable, a consistent pattern shown in different LLMs.</p>
<p>C Additional Quantitative Results on TruthHypo</p>
<p>Table 2 presents the F1 score of various LLMs on the Truth-Hypo benchmark, evaluating their ability to identify the existence of a new relation given current knowledge.To provide a more granular analysis, Table 6 breaks down the results into precision and recall for different tasks, offering insights into the strengths and weaknesses of each model and knowledge augmentation setting.</p>
<p>From the results in Table 6, we observe that smaller LLMs, such as Llama-3.1-8B,tend to achieve higher recall scores across all tasks, along with relatively lower precision.This indicates that while these models can generate a comprehensive set of hypotheses, they are prone to a high false positive rate, which could pose challenges in real-world applications, such as scientific hypothesis generation, where precision is often critical.High false positive rates could result in wasted time and resources when pursuing hypotheses that are unlikely to hold upon experimental validation.</p>
<p>Given that validating new biomedical hypotheses often requires months or even years of research, ensuring high precision in hypothesis generation is of paramount importance.Among the tested models, GPT-4o with external knowledge from the literature achieved the highest precision across all tasks, demonstrating its ability to generate hypotheses with fewer false positives.However, this precision came at the expense of lower recall, especially when compared to GPT-4o with knowledge augmentations from both literature and knowledge graphs (KG).This trade-off highlights the importance of balancing precision and recall based on the specific requirements of a given application.</p>
<p>When comparing different knowledge settings, we found that the improvements provided by external knowledge sources varied across tasks and models.For example, knowledge graph (KG) information significantly enhanced the precision of all LLMs on tasks involving "Disease &amp; Gene" and "Gene &amp; Gene" relations, but it did not notably improve the precision of GPT-4o on the "Chemical &amp; Gene" task.In contrast, the literature knowledge augmentation slightly improved the precision of all LLMs except GPT-4o-mini.Interestingly, the setting that combined both knowledge sources provided a more balanced precision improvement, offering a middle ground between the individual benefits of KG and literature-based augmentations.</p>
<p>Additionally, Table 6 reveals that larger models such as GPT-4o consistently outperformed smaller models in precision, regardless of the knowledge setting, reflecting their ability to integrate complex external information effectively.This highlights the potential of larger models to better utilize structured and unstructured knowledge sources for hypothesis generation.However, smaller models, with their higher recall, may still serve as useful tools for exploratory or broad hypothesis generation tasks where exhaustive coverage is prioritized over precision.</p>
<p>Overall, the analysis demonstrates that the choice of LLM and knowledge augmentation strategy should be guided by the specific trade-offs between precision and recall that align with the requirements of the downstream task.For biomedi-cal applications, where precision is often paramount, leveraging models like GPT-4o with literature-based augmentations appears to be the most effective approach.</p>
<p>To further understand the limitations of hypothesis generation with high groundedness scores, we conducted an indepth analysis of the error patterns.We identified two representative types of errors: (1) cases where the LLM incorrectly infers that there is no association between the given entities, despite supporting evidence; and (2) cases where the model simply echoes or paraphrases the provided context without engaging in substantive reasoning or hypothesis formation.These findings highlight the need for more robust mechanisms to ensure both accurate association detection and genuine reasoning in hypothesis generation, enhancing the interpretability and trustworthiness of the overall system [Doshi-Velez and Kim, 2017;Loh et al., 2022;Miller, 2023;Sinha et al., 2024a,b].</p>
<p>D Prompt Templates for LLMs in Experiments</p>
<p>Figure 5 shows the template we used to construct a hypothesis generation query given two different entities.The prompt templates for the use of LLMs in the "Parametric", "Parametric + KG", "Parametric + Lit.", and "Parametric + KG + Lit." settings are presented in Figures 6, 7, 8, 9, respectively.These templates were designed to guide the LLMs in effectively leveraging various sources of knowledge while maintaining a pre-determined structure in the model output to facilitate consistent parsing and downstream analysis.</p>
<p>Figure 10 shows the template for LLMs to extract scientific claims from the entire rationale.For the identification of biomedical entities and the use of LLMs for claim verification, we employed the templates shown in Figures 11 and 12, respectively.The entity identification templates (Figure 11) were crafted to enable the LLMs to extract precise mentions of biomedical entities such as genes or diseases from textual claims.These prompts were carefully designed to minimize ambiguity, ensuring that entities sharing the same mention could be properly distinguished using their unique IDs.</p>
<p>Knowledge</p>
<p>Figure 1 :
1
Figure 1: Overview of the TruthHypo benchmark, including dataset construction, task formulation, and truthfulness evaluation.</p>
<p>Figure 2 :
2
Figure 2: Overview of the KnowHD hallucination detection framework.Hypotheses are parsed into atomic claims, which are then evaluated for groundedness using a knowledge graph, scientific literature or both as knowledge sources.</p>
<p>Figure 3 :
3
Figure 3: Mean accuracy corresponding to different levels of groundedness.Hypotheses are grouped based on their groundedness scores provided by KnowHD (KG + Literature).Only groups with no less than 10 hypotheses are shown in the plots.The dot size reflects the number of samples in each level of groundedness.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Prompt template for constructing user input with given entities.</p>
<p>Table 1 :
1
. Statistics of various tasks in the TruthHypo benchmark.
TaskLabel# Instancepositive correlate328Chemical &amp; Genenegative correlate478no relation403stimulate104Disease &amp; Geneinhibit75no relation89positive correlate247Gene &amp; Genenegative correlate118no relation182</p>
<p>Table 2 :
2
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings.The metrics reported are link-level F1 and relation-level accuracy (Acc) for each task (Chemical &amp; Gene, Disease &amp; Gene, Gene &amp; Gene), as well as their averages."Param."denotes parametric knowledge, while "KG" and "Lit."refer to knowledge graphs and literature, respectively.All scores are percentages (%).
KnowledgeLLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene F1 Acc F1 Acc F1 AccAverage F1 AccLlama-3.1-8B80.1642.4379.3741.0479.19 46.07 66.90 43.23ParametricLlama-3.1-70B 81.3652.4483.2954.4876.66 49.91 71.54 52.03[Wei et al., 2022]GPT-4o-mini83.3161.2981.8459.3379.32 53.02 75.49 58.79GPT-4o80.7466.1775.3854.8571.56 55.58 73.17 61.81Llama-3.1-8B81.3740.6179.5948.1379.61 48.45 70.65 43.73Parametric + KGLlama-3.1-70B 87.8562.8667.6252.2478.29 58.14 79.10 60.18[Baek et al., 2024]GPT-4o-mini86.4257.6574.1755.6081.65 62.34 79.40 58.65GPT-4o88.6663.8579.5056.7282.73 61.06 81.62 62.15Llama-3.1-8B80.7846.0780.4643.2879.91 42.60 68.58 44.76Parametric + Lit.Llama-3.1-70B 82.5656.7484.1652.9979.18 51.55 73.37 54.84[Lewis et al., 2020]GPT-4o-mini85.2859.8085.7153.7381.50 51.19 77.08 56.67GPT-4o79.5265.9275.8455.9764.69 51.92 71.84 60.82Llama-3.1-8B75.9836.4877.5841.4279.19 45.70 65.37 39.62Parametric + KGLlama-3.1-70B 84.8059.3177.6456.3481.24 55.76 77.37 57.95+ LiteratureGPT-4o-mini88.3460.9684.4758.2184.17 58.50 81.42 59.93GPT-4o89.7169.3182.8662.3185.91 63.99 83.55 66.95</p>
<p>Table 4 :
4
Resultsof analysis on open-ended hypothesis generation tasks."GPT" and "Human" denote the selection ratios by GPT-4o and human experts, respectively.All scores are percentages (%).pvalues were calculated using Wilcoxon signed-rank test and Z-test.</p>
<p>Table 5 :
5
Summary of #tokens used for all experiments in Table 2.
LLMTypeParam.+KGSetting +Lit.+KG+Lit.LlamaInput295.8k1.7M25.8M27.2M-3.1-8BOutput 811.1k1.0M782.5k1.2MLlamaInput295.8k1.7M25.8M27.2M-3.1-70BOutput 813.7k 881.2k 777.6k767.8kGPT-4oInput295.8k1.7M25.8M27.2M-miniOutput 751.6k 684.0k 787.2k707.1kGPT-4oInput Output 909.9k 839.1k 891.5k 295.8k 1.7M 25.8M27.2M 875.3k</p>
<p>Table 6 :
6
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings, with precision and recall as the evaluation metrics."Prec"denotes the link-level precision, while "Recall" represents the link-level recall.Prompt template for constructing user input with given entitiesCan we hypothesize the potential relation between {{entity type 1}} {{entity name 1}} ({{entity ID 1}}) and {{entity type 2}} {{entity name 2}} ({{entity ID 2}})?The final hypothesis can be one of [{{relation label 1}}, {{relation label 2}}, 'no relation'].
LLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene Prec Recall Prec Recall Prec Recall Prec Recall AverageLlama-3.1-8B 67.5798.5166.7997.7766.92 96.99 67.29 98.00ParametricLlama-3.1-70B 74.6989.3375.2393.3073.13 80.55 74.37 87.48Wei et al. [2022]GPT-4o-mini83.0083.6279.4784.3675.50 83.56 80.37 83.70GPT-4o90.5972.8382.6769.2783.27 62.74 87.60 69.63Llama-3.1-8B 71.4294.5474.0486.0372.16 88.77 71.93 91.85Parametric + KGLlama-3.1-70B 90.6385.2493.1453.0788.58 70.14 90.34 76.89Baek et al. [2024]GPT-4o-mini86.3786.4891.0662.5792.39 73.15 88.27 79.70GPT-4o86.2791.1991.3070.3987.96 78.08 87.21 84.89Llama-3.1-8B 68.8297.7768.3697.7768.49 95.89 68.67 97.26Parametric + Lit.Llama-3.1-70B 74.9291.9475.5694.9774.58 84.38 74.92 90.30Lewis et al. [2020]GPT-4o-mini78.1893.8080.1092.1874.94 89.32 77.55 92.37GPT-4o92.7369.6083.7869.2789.37 50.68 90.62 64.44Llama-3.1-8B 68.2185.7370.6486.0369.96 91.23 69.01 87.26Parametric + KGLlama-3.1-70B 84.1385.4887.4169.8380.05 82.47 83.33 82.59+ LiteratureGPT-4o-mini82.6194.9182.4586.5983.16 85.21 82.73 91.19GPT-4o86.6193.0584.8081.0187.50 84.38 86.61 89.11
PMID is the unique identifier of the paper where the edge was extracted.
AcknowledgementsThis work is supported in part by the US National Science Foundation under grants 2217071, 2213700, 2106913,  2008208, and NIH grant 1R01LM014012.Prompt template for hypothesis generation with knowledge from parameters and KG You are a scientist.Your task is to generate a scientific hypothesis following given instructions.Prompt template for claim identification ### Statement {{statement}}Summarize the statement as a list of claims which will be further verified by external resources.Output the summarized claims in the JSON format: '''json{"claims": ["claim1", ...]}'''Prompt template for entity recognition ### Background {{background}}Extract key entities from the background statement that will be used to search for relevant information in an external knowledge graph.Each entity should be extracted as "entity type (e.g., Disease/Chemical/-Gene/Mutation) entity name (entity id if presented)".Output the extracted entities in the JSON format: '''json{"entities": ["entity1", ...]}'''
Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment. Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross J Collins, Elizabeth Bourne, Gareth W Fearnley, Emma Tate, Holly X Smith, Larisa N Soldatova, arXiv:2405.122582024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. Ioana Ciucȃ, Yuan-Sen, Sandor Ting, Kartheik Kruk, Iyer, arXiv:2306.116482023arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.086082017arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>On the creativity of large language models. Giorgio Franceschelli, Mirco Musolesi, AI &amp; SOCIETY. 2024</p>
<p>Forecasting high-impact research topics via machine learning on evolving knowledge graphs. Xuemei Gu, Mario Krenn, arXiv:2402.086402024arXiv preprint</p>
<p>Embracing foundation models for advancing scientific discovery. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Aidong Xiong, Zhang, 2024 IEEE International Conference on Big Data (BigData). IEEE2024</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Myles Huang, Corey M Kim, Stefan Williams, Aidong Bekiranov, Zhang, 31st SIGKDD Conference on Knowledge Discovery and Data Mining -Datasets and Benchmarks Track. 2025</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>Autonomous llm-driven research-from data to human-verifiable research papers. Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, Roy Kishony, NEJM AI. 21AIoa2400555, 2025</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip Yu, IEEE transactions on neural networks and learning systems. 202133</p>
<p>Demystifying large language models for medicine: A primer. Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, arXiv:2410.188562024arXiv preprint</p>
<p>Large language models versus natural language understanding and generation. Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, Michael Vassilakopoulos, Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics. the 27th Pan-Hellenic Conference on Progress in Computing and Informatics2023</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, Nima João P Moutinho, Sanjabi, Nature Machine Intelligence. 5112023</p>
<p>Mycrunchgpt: A chatgpt assisted framework for scientific machine learning. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, George Em Karniadakis, arXiv:2306.155512023arXiv preprint</p>
<p>Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.07559Paperqa: Retrieval-augmented generative agent for scientific research. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024arXiv preprint</p>
<p>Learning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>A survey on graph classification and link prediction based on gnn. Xingyu Liu, Juan Chen, Quan Wen, arXiv:2307.008652023arXiv preprint</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Application of explainable artificial intelligence for healthcare: A systematic review of the last decade. Wen Hui, Loh, Ping Chui, Silvia Ooi, Prabal Seoni, Filippo Datta Barua, Molinari, Acharya Rajendra, Computer methods and programs in biomedicine. 2011-2022. 2022226107161</p>
<p>Improving biomedical information retrieval with neural retrievers. Man Luo, Arindam Mitra, Tejas Gokhale, Chitta Baral, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 2024</p>
<p>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo, Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. arXiv e-prints. 20242407</p>
<p>Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai. Tim Miller, Proceedings of the 2023 ACM conference on fairness, accountability, and transparency. the 2023 ACM conference on fairness, accountability, and transparency2023</p>
<p>Evaluating the effectiveness of retrievalaugmented large language models in scientific document reasoning. Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana, arXiv:2311.043482023arXiv preprint</p>
<p>A review of relational machine learning for knowledge graphs. Maximilian Nickel, Kevin Murphy, Evgeniy Volker Tresp, Gabrilovich, Proceedings of the IEEE. 10412015</p>
<p>Can chatgpt be used to generate scientific hypotheses. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, Journal of Materiomics. 1032024</p>
<p>Graph retrieval-augmented generation: A survey. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang, arXiv:2408.089212024arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, First Conference on Language Modeling. 2024</p>
<p>Scideator: Humanllm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Most Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain, Kaniz Mukta, Nur Fatema, Sadman Mohammad Fahad, Sakib, 2024IEEE Access</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Knowledge graph large language model (kg-llm) for link prediction. Dong Shu, Tianle Chen, Mingyu Jin, Chong Zhang, Mengnan Du, Yongfeng Zhang, arXiv:2403.073112024arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Colidr: Concept learning using aggregated disentangled representations. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>A selfexplaining neural architecture for generalizable concept learning. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. Sam Michael D Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Hinks, J Michael, Manvitha Hammerling, Ponnapati, Andrew D Samuel G Rodriques, White, arXiv:2409.137402024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023arXiv preprint</p>
<p>Knowledge graph retrievalaugmented generation for llm-based recommendation. Shijie Wang, Wenqi Fan, Yue Feng, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, arXiv:2501.022262025arXiv preprint</p>
<p>Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Nucleic Acids Research. 35e2352022. 2024Chain-ofthought prompting elicits reasoning in large language models</p>
<p>Generating scientific claims for zero-shot scientific fact checking. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu, Wang , arXiv:2203.129902022arXiv preprint</p>
<p>Dynamic link prediction using graph representation learning with enhanced structure and temporal information. Chaokai Wu, Yansong Wang, Tao Jia, 2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD). IEEE2023</p>
<p>Benchmarking retrieval-augmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, Findings of the Association for Computational Linguistics: ACL 2024. 2024</p>
<p>Improving retrievalaugmented generation in medicine with iterative follow-up questions. Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang, Biocomputing 2025: Proceedings of the Pacific Symposium. World Scientific2024</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2025</p>
<p>Meta-review generation with checklist-guided iterative introspection. Qi Zeng, Mankeerat Sidhu, Pong Hou, Lu Chan, Heng Wang, Ji, arXiv:2305.146472023arXiv preprint</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 201831</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202336</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
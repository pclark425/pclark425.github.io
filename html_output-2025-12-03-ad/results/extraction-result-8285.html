<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8285 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8285</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8285</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-267412517</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03268v3.pdf" target="_blank">Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we pro-pose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8285.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8285.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (124M) trained from scratch on KG random-walk paths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomly initialized GPT-2 (124M) model trained from scratch on text formed by concatenating random-walk paths sampled from knowledge graphs to evaluate logical (KG) reasoning and compare LM behavior to path-aggregation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (124M, random init)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 architecture (decoder-only Transformer) with ~124M parameters; trained from scratch on synthetic text where each KG triple is a token and paragraphs are generated by uniform random walks on the KG (max path length L_max varied).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['random-walk pre-training on KG (next-token prediction)', 'path-aggregation analysis (weighted P_w and unweighted P_s used as explicit baselines)', 'LM next-token marginalization (P_LM) used as model predictor']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is pre-trained with next-token prediction on concatenated sequences of triples produced by uniform random walks on the KG. The paper constructs two explicit aggregation baselines: (1) weighted aggregation P_w which scores a target by a weighted sum of probabilities of all random-walk paths (rules) connecting head->tail, and (2) unweighted aggregation P_s which sums path probabilities without learned weights. The LM's output distribution P_LM(e2|e1,r) is compared to these aggregations via KL divergence and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (LM aggregates multiple diverse random-walk paths; comparisons include an unweighted 'similar' aggregation and a weighted 'diverse-weighted' aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Computed KL[P_w, P_LM] and KL[P_s, P_LM] across pre-training random-walk length (L_max) and aggregation maximum rule length (N_max); compared prediction accuracy (argmax) of PLM, P_w, and P_s on KG test sets; ablated L_max from 1..20 and N_max from 1..10 to study path-length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Knowledge-graph link prediction / logical reasoning on five KGs (Countries (S3), UMLS, Kinship, NELL-995, FB15K-237) where the model predicts missing tails given (e1, r).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative/relative results reported: PLM (GPT-2) performs on par with or better than weighted aggregation P_w and significantly better than unweighted aggregation P_s on Countries; on UMLS PLM consistently outperforms both P_w and P_s. KL results: KL[P_w, P_LM] is generally smaller than KL[P_s, P_LM], indicating PLM more closely matches a weighted aggregation than an unweighted one. Pre-training with longer L_max improves PLM accuracy up to an optimal path length per dataset, after which too-long paths introduce noise and performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LM behavior resembles a weighted aggregation over multiple reasoning (random-walk) paths rather than treating all paths equally; LM appears to learn rule importance (weights) and in complex KGs assigns weights that depend on head entity and relation (i.e., more fine-grained than the relation-only weights in P_w). There is an optimal pre-training path length that matches intrinsic reasoning lengths of datasets; PLM can generalize to longer reasoning lengths than seen in pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Language models trained with next-token prediction on random-walk paths behave similarly to aggregating multiple reasoning paths (weighted sum), and they learn better, more contextual rule-weighting schemes than classical path-aggregation methods; including multi-step reasoning paths in pre-training is necessary (L_max>1) to enable non-trivial logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8285.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8285.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought latent-graph random-walk continued pre-training on Llama 2 / Yi / Gemma family</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of experiments where pre-trained LMs (Llama 2 7B/13B, Yi 6B, Gemma 2B) are continually pre-trained on unlabeled random-walks constructed from chain-of-thought (CoT) step embeddings (latent reasoning graph) and then supervised fine-tuned, showing consistent gains over SFT baselines on multiple reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (7B, 13B); Yi (6B); Gemma (2B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only LLMs used as base models (Llama 2 7B and 13B; Yi 6B; Gemma 2B). Fine-tuning method: LoRA parameter-efficient continual pre-training (first M steps on random-walk CoT paths generated from a latent reasoning graph) followed by supervised fine-tuning (SFT). Default base model in experiments: Llama 2 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['latent-graph random-walk pre-training (unlabeled CoT paths)', 'chain-of-thought fine-tuning (supervised CoT SFT baseline)', 'clustering-based latent reasoning graph construction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>They encode intermediate CoT steps by averaging model hidden states up to each step, cluster these step embeddings to form latent reasoning-graph nodes (K-means clusters), generate random-walks on that graph by sampling CoT segments from nodes, continue pre-training on the concatenated CoT-step random walks for M steps, then perform supervised fine-tuning on labeled CoT datasets. Ablations vary L_max (random-walk length), M (number of pre-training steps on random-walks), and number of clusters K.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (the augmentation produces many novel combinations of CoT steps — varied reasoning paths — compared to the more similar supervised CoTs seen in SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared 'Ours' (M = 500 steps of random-walk pre-training followed by SFT) vs supervised fine-tuning (SFT) baseline with equal total steps (N=2500). Ablations: varied random-walk length L_max (observed dataset-specific optima), number of pre-training steps M (0, 500, 1000, ...), and cluster count K (including 0 = SFT baseline). Evaluated across GSM8K, AQUA, SVAMP (math MWPs), StrategyQA (multihop QA), and LogicalDeduction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math word problems (GSM8K, AQUA, SVAMP), multihop QA (StrategyQA), and logical deduction (LogicalDeduction from BIG-bench).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported numeric improvements (selected numbers from Table 1): Gemma 2B SFT vs Ours (GSM8K 24.8 -> 26.1; AQUA 31.4 -> 33.9; SVAMP 56.4 -> 60.3; StrategyQA 54.2 -> 56.3; LogicalDeduction 50.7 -> 51.6; Avg 43.5 -> 45.6). Yi 6B SFT vs Ours (avg 52.6 -> 54.6). Llama 2 7B SFT vs Ours (GSM8K 26.8 -> 28.5; AQUA 30.0 -> 34.6; SVAMP 53.3 -> 55.8; StrategyQA 58.4 -> 63.7; LogicalDeduction 53.3 -> 56.1; Avg 44.8 -> 47.7). Llama 2 13B SFT vs Ours (GSM8K 37.1 -> 41.2; AQUA 35.0 -> 37.4; SVAMP 66.4 -> 69.0; StrategyQA 69.5 -> 71.2; LogicalDeduction 55.7 -> 57.7; Avg 52.7 -> 55.3). Across models, 'Ours' (random-walk pretrain + SFT) consistently outperforms SFT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Unlabeled random-walk CoT augmentation reliably improves downstream reasoning accuracy; each dataset shows an optimal random-walk length L_max correlated with the dataset's intrinsic CoT length (e.g., GSM8K and AQUA peak at L_max≈10; SVAMP peaks at 5). Too-long random walks introduce noise and reduce gains. Ablations indicate optimal M (pretraining steps) ≈ 500 and optimal number of clusters K ≈ 100 for the small datasets used; larger K and more data hypothesized to further help.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Continued pre-training on unlabeled random-walks constructed from CoT step clusters augments reasoning diversity and consistently improves CoT reasoning performance across models and datasets; there exists an optimal augmentation path length and pretraining amount, and the LM benefits from exposure to diverse reordered reasoning paths beyond the limited supervised CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8285.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8285.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-Aggregation Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted (P_w) and Unweighted (P_s) Random-Walk Path Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two explicit aggregation schemes derived from random-walk probabilities on KGs: P_w is an exponential of a weighted sum of rule/path probabilities (weights learned per relation), and P_s is the unweighted version that sums path probabilities equally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Path aggregation baselines (P_w, P_s)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>P_w: scores candidate tails using S_w(e2|e1,r)=sum_{h in H_r} w_r(h) * P(path following rule h from e1 to e2) and normalizes with softmax (temperature T); weights w_r(h) learned via logistic regression. P_s: same scoring but with all w_r(h)=1 (unweighted). Both computed directly from KG random-walk probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['weighted-rule aggregation (P_w)', 'unweighted-rule aggregation (P_s)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Both use enumerated logical rules (conjunctive sequences of relations) and compute the probability of reaching e2 from e1 along paths that follow each rule under a uniform random-walk. P_w learns relation-dependent rule weights (regularized logistic regression) to combine rule probabilities; P_s treats all rules equally.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>P_w: diverse-weighted (aggregates multiple possible paths with learned weights); P_s: similar (treats all paths equally)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Directly compared P_w and P_s to the LM distribution P_LM via KL divergence and prediction accuracy across varying maximum rule/path length N_max and varying LM pre-training path length L_max on KG datasets (Countries, UMLS, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>KG logical reasoning link-prediction tasks (same KG datasets as LM experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>P_w generally achieves lower KL to P_LM and higher accuracy than P_s. In Countries, PLM converges to P_w performance as L_max increases; in UMLS, P_w slightly outperforms P_s but PLM still outperforms both. P_s performance degrades when including many long paths (noise).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Learning per-relation rule weights (P_w) yields better alignment with LM behavior and higher accuracy than equal-weighted aggregation; however, P_w is still typically inferior to PLM because PLM appears to learn more context-sensitive (head-entity dependent) weighting and other generalization capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Weighted aggregation (P_w) is a better explanatory baseline for LM behavior than an unweighted sum, but LM usually surpasses both, indicating that LM learns more nuanced, context-dependent weighting and generalization beyond simple rule-weighting schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Path Ranking Algorithm <em>(Rating: 2)</em></li>
                <li>Why think step by step? reasoning emerges from the locality of experience <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Triggering multi-hop reasoning for question answering in language models using soft prompts and random walks <em>(Rating: 1)</em></li>
                <li>Markov logic networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8285",
    "paper_id": "paper-267412517",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT-2 (KG)",
            "name_full": "GPT-2 (124M) trained from scratch on KG random-walk paths",
            "brief_description": "A randomly initialized GPT-2 (124M) model trained from scratch on text formed by concatenating random-walk paths sampled from knowledge graphs to evaluate logical (KG) reasoning and compare LM behavior to path-aggregation baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (124M, random init)",
            "model_description": "GPT-2 architecture (decoder-only Transformer) with ~124M parameters; trained from scratch on synthetic text where each KG triple is a token and paragraphs are generated by uniform random walks on the KG (max path length L_max varied).",
            "reasoning_methods": [
                "random-walk pre-training on KG (next-token prediction)",
                "path-aggregation analysis (weighted P_w and unweighted P_s used as explicit baselines)",
                "LM next-token marginalization (P_LM) used as model predictor"
            ],
            "reasoning_methods_description": "The model is pre-trained with next-token prediction on concatenated sequences of triples produced by uniform random walks on the KG. The paper constructs two explicit aggregation baselines: (1) weighted aggregation P_w which scores a target by a weighted sum of probabilities of all random-walk paths (rules) connecting head-&gt;tail, and (2) unweighted aggregation P_s which sums path probabilities without learned weights. The LM's output distribution P_LM(e2|e1,r) is compared to these aggregations via KL divergence and accuracy.",
            "reasoning_diversity": "both (LM aggregates multiple diverse random-walk paths; comparisons include an unweighted 'similar' aggregation and a weighted 'diverse-weighted' aggregation)",
            "reasoning_diversity_experimental_setup": "Computed KL[P_w, P_LM] and KL[P_s, P_LM] across pre-training random-walk length (L_max) and aggregation maximum rule length (N_max); compared prediction accuracy (argmax) of PLM, P_w, and P_s on KG test sets; ablated L_max from 1..20 and N_max from 1..10 to study path-length effects.",
            "task_or_benchmark": "Knowledge-graph link prediction / logical reasoning on five KGs (Countries (S3), UMLS, Kinship, NELL-995, FB15K-237) where the model predicts missing tails given (e1, r).",
            "performance_results": "Qualitative/relative results reported: PLM (GPT-2) performs on par with or better than weighted aggregation P_w and significantly better than unweighted aggregation P_s on Countries; on UMLS PLM consistently outperforms both P_w and P_s. KL results: KL[P_w, P_LM] is generally smaller than KL[P_s, P_LM], indicating PLM more closely matches a weighted aggregation than an unweighted one. Pre-training with longer L_max improves PLM accuracy up to an optimal path length per dataset, after which too-long paths introduce noise and performance drops.",
            "qualitative_findings": "LM behavior resembles a weighted aggregation over multiple reasoning (random-walk) paths rather than treating all paths equally; LM appears to learn rule importance (weights) and in complex KGs assigns weights that depend on head entity and relation (i.e., more fine-grained than the relation-only weights in P_w). There is an optimal pre-training path length that matches intrinsic reasoning lengths of datasets; PLM can generalize to longer reasoning lengths than seen in pre-training.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Language models trained with next-token prediction on random-walk paths behave similarly to aggregating multiple reasoning paths (weighted sum), and they learn better, more contextual rule-weighting schemes than classical path-aggregation methods; including multi-step reasoning paths in pre-training is necessary (L_max&gt;1) to enable non-trivial logical reasoning.",
            "uuid": "e8285.0",
            "source_info": {
                "paper_title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoT-augmented LMs",
            "name_full": "Chain-of-Thought latent-graph random-walk continued pre-training on Llama 2 / Yi / Gemma family",
            "brief_description": "A family of experiments where pre-trained LMs (Llama 2 7B/13B, Yi 6B, Gemma 2B) are continually pre-trained on unlabeled random-walks constructed from chain-of-thought (CoT) step embeddings (latent reasoning graph) and then supervised fine-tuned, showing consistent gains over SFT baselines on multiple reasoning benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 (7B, 13B); Yi (6B); Gemma (2B)",
            "model_description": "Open-source decoder-only LLMs used as base models (Llama 2 7B and 13B; Yi 6B; Gemma 2B). Fine-tuning method: LoRA parameter-efficient continual pre-training (first M steps on random-walk CoT paths generated from a latent reasoning graph) followed by supervised fine-tuning (SFT). Default base model in experiments: Llama 2 7B.",
            "reasoning_methods": [
                "latent-graph random-walk pre-training (unlabeled CoT paths)",
                "chain-of-thought fine-tuning (supervised CoT SFT baseline)",
                "clustering-based latent reasoning graph construction"
            ],
            "reasoning_methods_description": "They encode intermediate CoT steps by averaging model hidden states up to each step, cluster these step embeddings to form latent reasoning-graph nodes (K-means clusters), generate random-walks on that graph by sampling CoT segments from nodes, continue pre-training on the concatenated CoT-step random walks for M steps, then perform supervised fine-tuning on labeled CoT datasets. Ablations vary L_max (random-walk length), M (number of pre-training steps on random-walks), and number of clusters K.",
            "reasoning_diversity": "diverse (the augmentation produces many novel combinations of CoT steps — varied reasoning paths — compared to the more similar supervised CoTs seen in SFT)",
            "reasoning_diversity_experimental_setup": "Compared 'Ours' (M = 500 steps of random-walk pre-training followed by SFT) vs supervised fine-tuning (SFT) baseline with equal total steps (N=2500). Ablations: varied random-walk length L_max (observed dataset-specific optima), number of pre-training steps M (0, 500, 1000, ...), and cluster count K (including 0 = SFT baseline). Evaluated across GSM8K, AQUA, SVAMP (math MWPs), StrategyQA (multihop QA), and LogicalDeduction.",
            "task_or_benchmark": "Math word problems (GSM8K, AQUA, SVAMP), multihop QA (StrategyQA), and logical deduction (LogicalDeduction from BIG-bench).",
            "performance_results": "Reported numeric improvements (selected numbers from Table 1): Gemma 2B SFT vs Ours (GSM8K 24.8 -&gt; 26.1; AQUA 31.4 -&gt; 33.9; SVAMP 56.4 -&gt; 60.3; StrategyQA 54.2 -&gt; 56.3; LogicalDeduction 50.7 -&gt; 51.6; Avg 43.5 -&gt; 45.6). Yi 6B SFT vs Ours (avg 52.6 -&gt; 54.6). Llama 2 7B SFT vs Ours (GSM8K 26.8 -&gt; 28.5; AQUA 30.0 -&gt; 34.6; SVAMP 53.3 -&gt; 55.8; StrategyQA 58.4 -&gt; 63.7; LogicalDeduction 53.3 -&gt; 56.1; Avg 44.8 -&gt; 47.7). Llama 2 13B SFT vs Ours (GSM8K 37.1 -&gt; 41.2; AQUA 35.0 -&gt; 37.4; SVAMP 66.4 -&gt; 69.0; StrategyQA 69.5 -&gt; 71.2; LogicalDeduction 55.7 -&gt; 57.7; Avg 52.7 -&gt; 55.3). Across models, 'Ours' (random-walk pretrain + SFT) consistently outperforms SFT baseline.",
            "qualitative_findings": "Unlabeled random-walk CoT augmentation reliably improves downstream reasoning accuracy; each dataset shows an optimal random-walk length L_max correlated with the dataset's intrinsic CoT length (e.g., GSM8K and AQUA peak at L_max≈10; SVAMP peaks at 5). Too-long random walks introduce noise and reduce gains. Ablations indicate optimal M (pretraining steps) ≈ 500 and optimal number of clusters K ≈ 100 for the small datasets used; larger K and more data hypothesized to further help.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Continued pre-training on unlabeled random-walks constructed from CoT step clusters augments reasoning diversity and consistently improves CoT reasoning performance across models and datasets; there exists an optimal augmentation path length and pretraining amount, and the LM benefits from exposure to diverse reordered reasoning paths beyond the limited supervised CoTs.",
            "uuid": "e8285.1",
            "source_info": {
                "paper_title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Path-Aggregation Baselines",
            "name_full": "Weighted (P_w) and Unweighted (P_s) Random-Walk Path Aggregation",
            "brief_description": "Two explicit aggregation schemes derived from random-walk probabilities on KGs: P_w is an exponential of a weighted sum of rule/path probabilities (weights learned per relation), and P_s is the unweighted version that sums path probabilities equally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Path aggregation baselines (P_w, P_s)",
            "model_description": "P_w: scores candidate tails using S_w(e2|e1,r)=sum_{h in H_r} w_r(h) * P(path following rule h from e1 to e2) and normalizes with softmax (temperature T); weights w_r(h) learned via logistic regression. P_s: same scoring but with all w_r(h)=1 (unweighted). Both computed directly from KG random-walk probabilities.",
            "reasoning_methods": [
                "weighted-rule aggregation (P_w)",
                "unweighted-rule aggregation (P_s)"
            ],
            "reasoning_methods_description": "Both use enumerated logical rules (conjunctive sequences of relations) and compute the probability of reaching e2 from e1 along paths that follow each rule under a uniform random-walk. P_w learns relation-dependent rule weights (regularized logistic regression) to combine rule probabilities; P_s treats all rules equally.",
            "reasoning_diversity": "P_w: diverse-weighted (aggregates multiple possible paths with learned weights); P_s: similar (treats all paths equally)",
            "reasoning_diversity_experimental_setup": "Directly compared P_w and P_s to the LM distribution P_LM via KL divergence and prediction accuracy across varying maximum rule/path length N_max and varying LM pre-training path length L_max on KG datasets (Countries, UMLS, etc.).",
            "task_or_benchmark": "KG logical reasoning link-prediction tasks (same KG datasets as LM experiments).",
            "performance_results": "P_w generally achieves lower KL to P_LM and higher accuracy than P_s. In Countries, PLM converges to P_w performance as L_max increases; in UMLS, P_w slightly outperforms P_s but PLM still outperforms both. P_s performance degrades when including many long paths (noise).",
            "qualitative_findings": "Learning per-relation rule weights (P_w) yields better alignment with LM behavior and higher accuracy than equal-weighted aggregation; however, P_w is still typically inferior to PLM because PLM appears to learn more context-sensitive (head-entity dependent) weighting and other generalization capabilities.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Weighted aggregation (P_w) is a better explanatory baseline for LM behavior than an unweighted sum, but LM usually surpasses both, indicating that LM learns more nuanced, context-dependent weighting and generalization beyond simple rule-weighting schemes.",
            "uuid": "e8285.2",
            "source_info": {
                "paper_title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Path Ranking Algorithm",
            "rating": 2,
            "sanitized_title": "path_ranking_algorithm"
        },
        {
            "paper_title": "Why think step by step? reasoning emerges from the locality of experience",
            "rating": 2,
            "sanitized_title": "why_think_step_by_step_reasoning_emerges_from_the_locality_of_experience"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Triggering multi-hop reasoning for question answering in language models using soft prompts and random walks",
            "rating": 1,
            "sanitized_title": "triggering_multihop_reasoning_for_question_answering_in_language_models_using_soft_prompts_and_random_walks"
        },
        {
            "paper_title": "Markov logic networks",
            "rating": 1,
            "sanitized_title": "markov_logic_networks"
        }
    ],
    "cost": 0.01585775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
20 Jun 2024</p>
<p>Xinyi Wang wang@ucsb.edu&gt;. 
Department of Computer Science
University of Califor-nia
Santa Barbara</p>
<p>Alfonso Amayuelas 
Department of Computer Science
University of Califor-nia
Santa Barbara</p>
<p>Kexun Zhang 
Language Technologies Institute
Carnegie Mellon University</p>
<p>Liangming Pan 
Department of Computer Science
University of Califor-nia
Santa Barbara</p>
<p>Wenhu Chen 
Cheriton School of Computer Science
University of Waterloo</p>
<p>William Yang Wang 
Department of Computer Science
University of Califor-nia
Santa Barbara</p>
<p>Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
20 Jun 202489DDEBF774E039B582B12F0E16D292EFarXiv:2402.03268v3[cs.LG]
Pre-trained language models (LMs) are able to perform complex reasoning without explicit finetuning.To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time.We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning.More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs.Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason.Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance. 1.</p>
<p>Introduction</p>
<p>Recently, pre-trained large language models (LLMs) (Touvron et al., 2023a;b;Brown et al., 2020) have demonstrated remarkable capabilities in performing intricate reasoning tasks (Kojima et al., 2022).These tasks include problemsolving with world knowledge (Hendrycks et al., 2020;Suzgun et al., 2022), logical reasoning (Pan et al., 2023), and solving mathematical problems (Cobbe et al., 2021; Figure 1.We hypothesize that the pre-training corpus can be viewed as generated from random walks on a reasoning graph over world knowledge/concepts.With each node si representing concepts, pj can be viewed as arguments that connect them.Then we hypothesize that a language model (LM) training on such a corpus can be viewed as reasoning by a weighted aggregation of random walk paths that connect the entities in interest.PLM denote the LM distribution while PD denotes the random walk probability from the pre-training corpus.w 1 i denotes the weight assigned to the first random walk path by the LM for argument pi, and w 2 i denotes the weight assigned to the second random walk path.Hendrycks et al., 2021).These models are typically not explicitly fine-tuned to solve these tasks.Recent research (Jain et al., 2023) also suggests that the supervised fine-tuning process following pre-training only learns a wrapper on top of the already existing model capabilities, instead of learning new ones.It is intriguing to understand how next-token prediction pre-training contributes to the emergence of such reasoning capability.A better understanding of this matter can also inspire new pre-training/fine-tuning techniques to improve these important abilities of LLMs.</p>
<p>It is well-known that LLMs acquire emergent abilities through extensive pre-training (Wei et al., 2022a).In this paper, we focus on elucidating the emergence of reasoning ability -the capacity to draw novel conclusions from existing knowledge, which has been less studied.Many recent works also attempt to understand this phenomenon.Some works focus on understanding Transformers' reasoning capability by construction (Liu et al., 2023;Chi et al., 2023;Feng et al., 2023).Others try to provide post hoc mechanistic explanations (Geiger et al., 2021;Wu et al., 2023;Hanna et al., 2023) or understanding inference time in-context learning reasoning (Li et al., 2023;Razeghi et al., 2022;Wang et al., 2023).Our study is more relevant to the line of work analyzing the contribution of pre-training data to LM reasoning (Bi et al., 2023;Chen et al., 2023;Xiao &amp; Liu, 2023;Zhou et al., 2023;Ramesh et al., 2023).</p>
<p>In contrast to these works, we adopt a Bayesian view and try to understand why next-token-prediction pre-training can unlock LMs' reasoning ability.More specifically, we hypothesize that LMs can aggregate the indirect reasoning paths seen at pre-training time, through the next-tokenprediction training objective.In a real-world scenario, the reasoning path can be a piece of text argument connecting two concepts.We hypothesize that, at inference time, this enables an LM to jump from one concept to another during its reasoning process, which could be verbalized by generating chain-of-thought (CoT) solutions (Wei et al., 2022b), or silent without generating outputs.Prystawski et al. (2023) propose a different hypothesis that localized structure on dependencies between variables in training data is important for LM reasoning, especially CoT reasoning.Our hypothesis implies a similar property of the pre-training data: when two concepts are related by a reasoning path, they are highly likely to cooccur in the data and thus form a graph-like localized structure.One drawback of Prystawski et al. (2023)'s work is that their experiments equate reasoning to conditional probability estimation of boolean variables with intermediate variables, which can be considered overly simplified compared to realworld reasoning processes.In our paper, we aim to produce a more realistic analysis of the effect of training data by closely examining two predominant types of reasoning: logical reasoning and mathematical reasoning.In these two reasoning scenarios, we first construct unsupervised random walk paths, which are used to (continually) pre-train the LM with next-token loss.Then we adopt the pre-trained LM to perform reasoning tasks on unseen examples.</p>
<p>For logical reasoning, we analyze a straightforward yet general reasoning scenario: reasoning over knowledge graphs.A knowledge graph (KG) stores facts in the form of triples (e 1 , r, e 2 ), where e 1 and e 2 represent entities connected by the relationship r.KGs can be incomplete, lacking certain relations between existing entities.These missing relations can typically be inferred from the known triples stored in the KG by employing logical rules.For instance, the relation (A, isGrandChildof, C) can be derived from the triples (A, isSonOf, B) and (B, isSonOf, C).We formalize a reasoning path as a random walk path on the KG, which enables us to accurately compute its probability.We show that an LM pre-trained from scratch on random walk paths generated from a given KG can accurately deduce missing relation connections.We also analyze the KL divergence between LM output distributions and weighted/unweighted sums of random walk path probabilities, which are variances of the classic path ranking algorithm (PRA) (Lao et al., 2011).Our analysis suggests that the LM distribution shares many similarities with aggregating the probabilities of possible random walk paths in a logical-rule-aware manner, and is usually superior to them.</p>
<p>For mathematical reasoning, we focus on a more complex case of reasoning: solving math word problems (MWPs).Since it is very challenging to pre-train an LM from scratch to perform well on MWPs, which require both math deduction and language understanding, we propose to continue training on a pre-trained base LM.Based on the insights obtained from the KG reasoning analysis, We propose to create random walk reasoning paths from existing CoT training data, and test the effectiveness of next-token-prediction training on these unlabeled reasoning paths.More specifically, we construct a reasoning graph by regarding the reasoning state at each CoT step as the graph node.Then we reorder and reconnect the existing CoT steps to form the random walk paths on the graph.Experiment results on three MWP datasets, GSM8K (Cobbe et al., 2021), AQUA (Ling et al., 2017), SVAMP (Patel et al., 2021), show consistent improvement compared to vanilla supervised fine-tuning, and a similar effect of random walk path length as in the KG reasoning case is observed.</p>
<p>Our findings can be summarized as follows: (a) We show in both reasoning scenarios that our weighted random walk reasoning paths aggregation hypothesis is one (of many) valid ways to explain how LMs may gain their reasoning ability; (b) We show that LMs can utilize unlabeled reasoning paths highly efficiently and show the potential of incorporating the random walk idea to real-world (continue) pre-training.</p>
<p>Logical Reasoning</p>
<p>We first analyze a well-controlled case of logic reasoning, knowledge graph (KG) reasoning, by pre-training a small Transformer over random walk paths from KGs.The KL divergence between aggregated random walk path probabilities and LM distribution shows that LM is very close to a weighted aggregation.We also show that KL divergence reflects how LMs assign weights to logical rules.We find that there is usually an optimal random walk path length for training LMs.These observations support our reasoning paths aggregation hypothesis.</p>
<p>Problem setting</p>
<p>Consider a knowledge graph G = {(e i 1 , r i , e i 2 )} N i=1 consisting of N triples, such that the head entity e i 1 and tail entity e i 2 are related by r i for all i.Let R denote the set of all pos-sible relations and E denote the set of all entities.Our goal is to predict a set of unseen triples T = {(e j 1 , r j , e j 2 )} m j=1 , e j</p>
<p>1 , e j 2 ∈ E, r j ∈ R, by training a Transformer based generative language model (LM) from scratch on the given knowledge graph G.To translate a triple into a sentence (i.e. a sequence of tokens), We add each entity e i and relation r i as a new token (<e i> and <r i>) to the Transformer's vocabulary and translate each triple into a three-token sentence "<e i> <r j> <e k>.".In this way, we avoid using any natural language thus no semantic meaning of the entity or relation name will affect the LM prediction.</p>
<p>Language Model Pre-training</p>
<p>We construct the training data by performing random walks on the given KG G.More specifically, we randomly sample a start entity e ∼ U (E), where U (•) denotes the uniform distribution.Then we perform a random walk on G from e by sampling the next node with e ′ ∼ U (C(e)), and stop at a maximum path length L max .</p>
<p>Then we translate each triple into a sentence and concatenate all the sentences in the sampled random walk path to become a paragraph.The paragraphs are then concatenated together and separated by the special end-of-sequence token to form text chunks of the same length.The training loss function is the next-token prediction loss:
L LM (θ) = D T t=1 log exp (f θ (w t+1 |w 1:t ))
w∈V exp (f θ (w|w 1:t ))</p>
<p>(1)</p>
<p>Here, θ denotes the LM parameters2 .w i ∈ V represents a token in the LM vocabulary V, and w 1:T is a token sequence in the training data D, where T is the length of a text chunk.</p>
<p>To test the reasoning ability of a pre-trained LM, we format the testing triples as sentence completion tasks.For example, the triple (e 1 , r, e 2 ) will be translated to the prompt "<e 1> <r>) ", and let the LM predict the next token, then verify the prediction with the ground truth e 2 .Note that, here the raw LM output distribution is over all entities and relations.</p>
<p>To make the LM distribution more well-defined and simplify the following analysis, we take the LM output logits over all entities and define the LM output distribution as:
P LM (e 2 |e 1 , r) = exp (f θ (e 2 |e 1 , r)) e∈E exp (f θ (e|e 1 , r))
(2)</p>
<p>Random Walk Paths Aggregation</p>
<p>Recall that our hypothesis is LM can aggregate the reasoning paths seen at the pre-training time.In the KG setting, we can explicitly define how the reasoning/random walk paths are aggregated.Inspired by the classic path ranking algorithm PRA (Lao et al., 2011), we define the aggregation of random walk paths P w as the exponential of a weighted sum of the probabilities of all appropriate random walk paths connecting the two target entities.More specifically, we are interested in a distribution P w (e 2 |e 1 , r) for unseen (e 1 , r, e 2 ) in the form of:
P w (e 2 |e 1 , r) = exp(S w (e 2 |e 1 , r)/T ) e∈E exp(S w (e|e 1 , r)/T )(3)
Here S w (e 2 |e 1 , r) is a score/logits of e 2 .T &gt; 0 is a temperature to rescale the weighted logits S w so that it can match the scale of LM logits f θ3 , and that P w (e 2 |e 1 , r) and P LM (e 2 |e 1 , r) are more comparable.The score S w (e 2 |e 1 , r) is defined to be a weighted sum of the probability of following all possible logical rules going from e 1 to e 2 :
S w (e 2 |e 1 , r) = h∈H w r (h)P (e 2 |e 1 , h)
Here H denotes the set of all possible logical rules, and h ∈ H is a specific logical rule.w r (h) is the weight assigned to rule h when inferring relation r.For example, a rule for inferring the locatedIn relation can be h : (e 1 , neighborOf, e 3 ) ∧ (e 3 , locatedIn, e 2 ).Formally, for a target relation r, we consider logic rules with conjunctive form.∀{e i } n i=0 ⊂ E, (e 0 , r, e n ) ← (e 0 , r 1 , e 1 ) ∧ ... ∧ (e n−1 , r n , e n )</p>
<p>where (e i−1 , r i , e i ) ∈ G.We abbreviate such rule by h = [r 1 , r 2 , ..., r n ].We can formalize the set of all possible logic rules by
H = {[r 1 , r 2 , ..., r n ]|n ≥ 1, r i ∈ R}.
Then the probability of following a specific logic rule h ∈ H between e 1 and e 2 during the random walk would be the sum of the probability of all possible random walk paths from e 1 to e 2 following the rule h = [r 1 , r 2 , ..., r n ]:
P (e n |e 0 , h) = (e0,r1,e1)...(en−1,rn,en)∈P h n i=1 P (e i |e i−1 , r i )
where P h denotes all paths from the KG following h.Following the pre-training data generation, we perform a uniform random walk.i.e.P (e
i |e i−1 , r i ) = 1/|C(e i−1 )|.
Then the rule probability P (e 2 |e 1 , h) can be computed directly from the KG.</p>
<p>To learn the rule weights w r , we first observe that
P w (e 2 |e 1 , r) = P w (r|e 1 , e 2 ) e∈E P w (r|e 1 , e)
, if we sample e 1 and e 2 independently and uniformly.Recall Equation (3), we can instead model P w (r|e 1 , e 2 ) ∝ exp S w (e 2 |e 1 , r).We can even further simplify it into a binary classification problem p i = P w (1 r i =r |e i 1 , e i 2 ).Then we can use w r to parameterize a logistic regression model with a loss function:
L r (w) = − i [y i ln p i + (1 − y i ) ln (1 − p i )] + λ|w|, where p i = exp Sw(e i 2 |e i 1 ,r) 1+exp Sw(e i 2 |e i 1 ,r)
, and the binary label y i = 1 r i =r .λ|w| is a regularization term, and we can take any appropriate norm on w.</p>
<p>At training time, we sample positive triples with relation r and negative triples with other relations from G as training data.We search over the graph to compute their probability of being reached by each rule P (e 2 |e 1 , h) to compute p i .</p>
<p>For computation efficiency, we only want to search for a subset of more possible logical reasoning rules H r in the test set for each relation r, and assign w r (h) = 0 for h / ∈ H r .Note that a rule can be infinitely long, so we set a maximum rule length n ≤ N max .To obtain H r , we search over G, and record all paths between any two entities that are connected with the relation r, and shorter than N max .We then collect the rules that have more than m valid paths.</p>
<p>A simplified version of P w would be letting w r (h) = 1 for all h and r.And we define this unweighted aggregation distribution to be P s :
P s (e 2 |e 1 , r) = exp( h∈Hr P (e 2 |e 1 , h)/T ) e∈E exp( h∈Hr P (e|e 1 , h)/T ) (4)</p>
<p>KL Divergence and Prediction Accuracy</p>
<p>To better understand the similarity between LM and the random walk aggregation algorithm as described in the previous section, we propose to compute and analyze the KL divergence between them:
KL[P w (e|e 1 , r), P LM (e|e 1 , r)],
where e is a random variable taking values in E. To better understand the meaning of the computed KL divergence, we derive an upper bound of it by writing P LM (e 2 |e 1 , r) as marginalization over rules:
P LM (e 2 |e 1 , r) = h∈H P (e 2 |e 1 , h)P LM (h|e 1 , r) (5)
Similarly, we can write
P w (e 2 |e 1 , r) = h∈H P (e 2 |e 1 , h)P w (h|e 1 , r)(6)
Then by the Log sum inequality, we can see that the KL divergence of the rule importance is an upper bound of the computed KL divergence 4 :</p>
<p>4 Proof available in Appendix A.</p>
<p>Proposition 2.1.If LM effectively learned the random walk data distribution through pre-training, we have
KL[P w (e|e 1 , r), P LM (e|e 1 , r)] ≤KL[P w (h|e 1 , r), P LM (h|e 1 , r)]
Here h is a random variable taking values in H.This means the KL divergence reflects how LM assigns probabilities to possible logical rules based on the given prompt, which implies how the LM learns to do logical reasoning.</p>
<p>KL computation</p>
<p>We compute the KL divergence between the weighted aggregation distribution P w (e 2 |e 1 , r) as defined in Equation ( 3) and the LM distribution P LM (e 2 |e 1 , r) as defined in Equation ( 2), abbreviated as KL[P w , P LM ].</p>
<p>We then compare it with the KL divergence between the unweighted aggregation distribution P s (e 2 |e 1 , r) as defined in Equation ( 4) and the LM distribution, abbreviated as KL[P s , P LM ].To better understand the effect of random walk length, we consider maximum random walk path length ranging from 1 to 10 (i.e. 1 ≤ L max ≤ 10 and 1 ≤ N max ≤ 10), for computing both the aggregation distribution and the LM distribution.We then compute a pairwise KL between each of them and show the results as a heatmap.To better anchor the computed KL divergence, we also compute the KL divergence KL[P * , P LM ] between a reference distribution P * and the LM distribution P LM , and KL divergence KL[P u , P LM ] between the uniform distribution P u and the LM distribution P LM .Here P * is uniform over all correct answers, and P u is uniform over all possible answers.The described KL divergences for Countries (top) and UMLS (bottom) testing sets are shown in heatmaps in Figure 2.More interpretations of these quantities can be found in the caption.</p>
<p>Accuracy We also compute the prediction accuracy using each method and plot it w.r.t to path length (1 ≤ L max ≤ 10).Note that there could be more than one correct answer for a query (e 1 , r).We say the prediction is correct as long as it is one of the correct answers.The described testing accuracy for Countries (left) and UMLS (right) is shown in Figure 3, where LM is arg max P LM , Weighted is arg max P w , and Unweighted is arg max P s .In general, LM predictor P LM performs on par/better than weighted aggregation P w , and significantly better than the unweighted aggregation P s .This shows that LM likely learns a better logical rule weighting scheme than P w .</p>
<p>Results and Analysis</p>
<p>We consider five KG datasets in total: Countries (Bouchard et al., 2015), UMLS (Kok &amp; Domingos, 2007), Kinship (Denham, 2020), NELL-995 (Xiong et al., 2017), and FB15K-237 (Toutanova et al., 2015) 5 .We take the smallest two for KL divergence analysis for their lower time complexity.We show LM prediction accuracy for all datasets with different pre-training path lengths.</p>
<p>KL divergence with Countries</p>
<p>In Figure 2 (top), we can see that when the maximum path length for computing the aggregated distribution (columns) is three, there is a sudden drop in KL[P w , P LM ].This is because the ground truth path length to reach the correct answers in the testing set is three (fixed when constructing the dataset).Both the weighted and unweighted aggregation of random walk paths have low accuracy with path lengths less than three as shown in Figure 3 (left).The behavior of the path aggregation method is not well-defined at this stage and thus can result in an abnormal KL trend.On the other hand, LM yields a non-trivial accuracy when trained with a path length smaller than three, which shows LM's ability to generalize beyond the pre-training reasoning length.This echoes the findings in Xiao &amp; Liu (2023); Zhou et al. (2023), that Transformers can generalize to longer sequences than training sequences.</p>
<p>As shown in Figure 2 (top left), the weighted aggregation scheme P w converges to a stable distribution, likely by putting most weights on shorter rules when using long random walk paths.The LM distribution P LM becomes closer to P w when the pre-training path length becomes longer.On the other hand, KL[P s , P LM ] stably increases when the path length for P s becomes larger.This echoes the accuracy trends as shown in Figure 3 (left).For the countries dataset, since it only has two relations, longer random walk paths introduce more noise than useful information.Thus by increasing the path length the unweighted aggregation scheme P s becomes less and less effective.Both P w and P LM learn to assign a small weight to the long/noisy paths, and thus do not experience an accuracy drop.</p>
<p>KL divergence with UMLS In Figure 2 (bottom), we can see that when the maximum path length for computing the aggregated distribution (columns) is larger than 3, the  Each line corresponds to a different KG dataset and thus is not directly comparable.We want to highlight the common trend here that each line peaks at some optimal path length.</p>
<p>weighted aggregation scheme P w also converges to a stable distribution.To investigate why path length 3 is unique, we find the average path length corresponding to the largest number of valid paths for each relation in the testing set is 3.14.We find the average path length corresponding to the largest weight assigned by P w when N max = 10 is 2.75.This confirms that path length three is likely a good rule length for many relations.However, from Figure 3 (right), we can see that both weighted (P w ) and unweighted (P s ) aggregation peaked at path length two instead of three.We believe this is because when the rule length becomes larger (i.e.larger than two), the validity of a rule would be more head entity (e 1 ) dependent.Using only relation-dependent weight w r (h) as in P w is likely insufficient.This also explains why LM constantly outperforms both path aggregation methods: LM likely learns a rule importance function that depends both on the head entity and the relation.</p>
<p>Different from the Countries dataset, UMLS' KL[P s , P LM ] does not increase when the path length for P s increases.Instead, KL[P s , P LM ] follows a similar trend as KL[P w , P LM ], while in general KL[P w , P LM ] is smaller than KL[P s , P LM ].Similarly, in Figure 3 (right), the weighted (P w ) and unweighted (P s ) aggregation has a similar performance, while P w is slightly better.This shows that the logical rule weights learned by P w are similar between different rules, so it has similar effects (KL and accuracy) as the unweighted version P s .The LM also has a flatter distribution, as we can see for UMLS KL[P * , P LM ] &lt; KL[P u , P LM ] while for Countries KL[P * , P LM ] &gt; KL[P u , P LM ].This is likely because UMLS is more complex than Countries (49 v.s. 2 relations), thus many longer paths and rules are similarly useful for prediction, making the LM distribution flatter.</p>
<p>Prediction accuracy v.s. pre-training path length</p>
<p>We briefly touched on how the pre-training random walk path length L max affects the LM distribution in the analysis above.In general, a longer path length improves the prediction accuracy and decreases KL[P w , P LM ].This shows that LM can improve the logical rule weight assignment when trained with a longer path length.To further investigate this problem, we pre-train LM on longer random walk path lengths with more KG datasets.</p>
<p>In Figure 4, we show the LM prediction accuracy v.s. the maximum pre-training random path length of 1, 5, 7, 10, 15, and 20, trained on five different KG datasets.In general, there is a large performance gain from a path length of 1 to 5. Note that when the path length is equal to one, we randomly sample individual triples from a KG.i.e.There are no reasoning paths in the training data.So it is important to have reasoning paths with a non-trivial length in the pre-training data, to enable the LM's reasoning ability.By extending the maximum length from 10 to 20, we can see that there is a slight drop in the Countries dataset.Similarly, in most datasets, there is a small decrease after an optimal path length.This is likely because a too-long random walk path would contain more noise/unrelated triples for reasoning.i.e.It is less likely to be useful for predicting the head and tail entity relation in a path aggregation sense.On the other hand, we can understand this from a localized data structure perspective (Prystawski et al., 2023): a sufficiently long random walk path makes any two entities similarly possible to appear in the same path, thus hurting the local dependency in the training data.</p>
<p>Chain-of-thoughts Reasoning</p>
<p>After carefully analyzing the logical reasoning on KGs, we want to apply and verify the obtained insights on a more general and realistic case of reasoning: chain-of-thoughts (CoT) reasoning (Wei et al., 2022b) with textual descriptions and step-by-step solutions.We continue training a pre-trained LM with random walk reasoning paths and show that these unlabeled paths consistently benefit CoT reasoning performance across multiple datasets of various tasks, including math reasoning, multihop question answering (QA), and logical deduction.We also observe a similar optimal random walk path length effect as in the KG logical reasoning case, which is associated with the intrinsic reasoning length of different datasets.These results support our reasoning path aggregation hypothesis and imply principles for constructing/augmenting pre-training data.</p>
<p>Problem Setting</p>
<p>Suppose we have a set of training data D = {(x i , r i 1 , r i 2 , ..., r i n i , y i )} i , where x i is a question described in the text that needs to be answered.r i 1 , r i 2 , ..., r i n i is a chain-of-thought (CoT) solution, where r i j is one reasoning step.y i is the ground truth answer to the question.Since CoT datasets are hard to collect and usually small in size, a model is not likely to generalize to new questions by aggregating reasoning paths over this small set of CoT reasoning paths.Fine-tuning on a pre-trained LM can effectively mitigate this problem since the LM has already seen many other reasoning paths at the pre-training time, but more unlabeled reasoning paths specific to this task would likely improve the testing performance if the path aggregation hypothesis still holds for this task.</p>
<p>Random Walk on Latent Reasoning Graph</p>
<p>We assume that CoT paths r i 1 , r i 2 , ..., r i n i can be regarded as random walk paths sampled from a reasoning graph G, where the nodes are the reasoning states at each step r i j .The reasoning state can be regarded as a belief that will be updated after each reasoning step.Denote the last hidden state of the pre-trained LM we are going to tune by f θ .To represent the reasoning state for each step r i j , we propose to use f θ to cumulatively encode all the steps before r i j , and then average over the sequence dimension, to obtain a fixed dimensional vector s i j :
s i j = avg f θ (x i , r i 1 , r i 2 , ..., r i j )
Assuming similar s i j 's are sampled from the same node of the latent reasoning graph, we propose to cluster 6 similar s i j 's together to form a node.Suppose we have constructed a graph G from the CoT dataset D, with nodes A 1 , A 2 , ..., A K , 6 In practice we use K-meanings clustering.</p>
<p>Algorithm 1 Random Walk on Latent Graph</p>
<p>Experiments</p>
<p>Datasets.We conduct experiments on three math word problem (MWP) datasets: GSM8K (Cobbe et al., 2021), AQUA (Ling et al., 2017), SVAMP (Patel et al., 2021), a multihop QA dataset StrategyQA (Geva et al., 2021), and a logical deduction dataset LogicalDeduction from the BIG-bench (Srivastava et al., 2023).GSM8K, AQUA, and SVAMP are math questions with annotated CoT steps.StrategyQA is annotated with decomposed questions, which we used as the Chain-of-thought (CoT) path of the question.As there is no CoT annotation in LogicalDeduction, we use GPT4 to generate CoTs for the training set, which on average requires Training Because of computation limits, we do LoRA (Hu et al., 2021) parameter efficient training in 8 bits with Llama 2 7B and 13B models (Touvron et al., 2023b), Yi 6B model (Young et al., 2024) and Gemma 2B model (Team et al., 2024).If not specified, we default to using the Llama 2 7B model.</p>
<p>Results.</p>
<p>In Table 1, we demonstrate the effectiveness of our proposed method against the supervised fine-tuning (SFT) baseline.We train both our method and SFT with N = 2500 steps in total.The first M = 500 steps of our method are continually pre-trained on random walk data, and then we do 2000 steps of SFT on the original dataset.Experiment results show that our method can notably improve on math, multihop QA, and logical reasoning.The improvement is especially significant on StrategyQA, likely because of the relative simplicity of the reasoning, as only 3 subquestions per example on average are needed.</p>
<p>Then we investigate the effect of random walk path length L max by plotting accuracy v.s.path lengths.In Figure 5, we observe that each dataset has a performance peak at a certain random walk length.While both AQUA and GSM8K peak at path length 10, the SVAMP dataset peaks at path length 5.This is likely related to the different intrinsic reasoning lengths for different datasets.The average length of CoTs 7 More dataset details can be found in Appendix C.1.</p>
<p>in AQUA, GSM8, and SVAMP training sets are 4.79, 3.72, and 1.36, respectively.The reasoning length required for SVAMP is significantly shorter than the other two datasets, thus explaining the earlier peaking.As we discussed in the logical reasoning case, a long random walk may introduce more noise than useful information.Note that even the LM performance can drop after the optimal path length, it is always better than training with path length one.i.e. multi-step random walk always helps.</p>
<p>We also do ablation studies on two critical hyperparameters of our method: the number of steps training on random walk paths M and the number of clusters/nodes K.In the upper half of Table 2, we show that the optimal number of training steps M is 500 for all three datasets.Since the generated random walk reasoning paths are not natural within small corpora, e.g. the subject might be suddenly changed from one step to another, training too many steps might make the LM overfit the unwanted artifacts.In the lower half of Table 2, we show that the optimal number of clusters is 100 for all three datasets.Here 0 clusters mean the SFT baseline.</p>
<p>Since the datasets we use are small in scale, clustering with a large number of clusters may introduce more noise than useful matchings.We hypothesize that this may be solved by using a larger dataset and more number of clusters/nodes K: in this case, the steps within each node will be more intrinsically similar.This also hints at the potential of our method in the actual pre-training stage: we can view each example in the pre-training corpus as a reasoning path and apply our method.</p>
<p>Latent reasoning graph analysis.To give a better understanding of the discovered latent reasoning graph, we show some discovered reasoning patterns through the graph in Figure 6.We  decomposes the question into two parallel subquestions of size/weight/subject area, and then uses the third question to compare the answers to the first two questions. 8</p>
<p>Related Work</p>
<p>Many recent works have investigated LM's reasoning ability.Geiger et al. (2021); Wu et al. (2023) ai to find the causal abstraction of an LM.(Hanna et al., 2023) tries to find circuit for year-span-prediction. more realistic data and tasks.Hou et al. (2023) confirm with attention probing that LMs perform multi-step reasoning internally, which echos our KG logical reasoning results.9</p>
<p>Conclusion</p>
<p>In conclusion, we aim to understand reasoning abilities in language models (LMs), from the perspective of aggregating reasoning paths from pre-training data.The findings shed light on the origins of LLMs' remarkable reasoning capabilities, showcasing the importance of pre-training in acquiring these skills.The construction of the pre-training sequence, such as organizing it as "chains" or random walks on the graph, was found to significantly impact the effectiveness of reasoning.The study also revealed that LM behavior is similar to reason over known facts by aggregating relevant reasoning paths.These insights contribute to our understanding of the underlying mechanisms behind LLMs' reasoning abilities and lead to a potential pre-training data augmentation technique to boost reasoning performance.</p>
<p>Impact Statement</p>
<p>Understanding the reasoning processes of large language models (LLMs) through the lens of aggregating indirect reasoning paths holds potential implications for identifying and mitigating potential biases within LLMs.By formalizing reasoning as random walk paths on knowledge and reasoning graphs, this approach not only elucidates the mechanisms through which LLMs derive conclusions but also sheds light on data and reasoning paths that contribute to their outputs.This insight is crucial for recognizing biases embedded in the training data or in the reasoning process itself.Recognizing these biases is the first step toward developing more equitable and transparent models.By augmenting models with unbiased, unlabeled random walk reasoning paths, we can potentially reduce the influence of biased reasoning patterns and improve the fairness and reliability of LLMs in real-world applications.This research advances our understanding of LLM reasoning capabilities and their implications for bias, paving the way for more responsible AI development and deployment.</p>
<p>(2023) that reasoning paths in training data enable supervision on intermediate steps with next-token-prediction objective, and also increase the length complexity, thus reducing time/sample complexity at training time.Prystawski et al. (2023) propose a different hypothesis that localized structure on dependencies between variables in training data is important for LM reasoning, especially CoT reasoning.Our proposed hypothesis echoes theirs and can be shown effective on more realistic data and tasks.</p>
<p>Logic/knowledge graph reasoning Existing methods can be divided into three categories: rule-based, GNN-based (Gori et al., 2005), and LM-based.Markov Logic Network (MLN) (Richardson &amp; Domingos, 2006) and path ranking algorithm (PRA) (Lao et al., 2011) are two classical methods that assign weights to different logical rules.Neural Logic Programming (Yang et al., 2017) and RNN-logic (Qu et al., 2020) are two neural methods that combine the explainability of learned logical rules and the high performance of neural networks.R-GCN (Schlichtkrull et al., 2018) and NBFNet (Zhu et al., 2021) are two GNN-based methods that train a GNN on the KG and use the obtained triple embeddings.These two category methods either rely on random walks to find paths or use random walks to train GNNs.Recently, LM-based methods are shown to be highly effective on not only KG reasoning (Misra et al., 2023), but more general logical reasoning problems with text descriptions (Pan et al., 2023).</p>
<p>Chain-ot-thought (CoT) reasoning Recently, LLMs have shown to be highly effective in complex reasoning tasks, like math reasoning (Azerbayev et al., 2023;Yang et al., 2023).Chain-of-thought (CoT) (Wei et al., 2022b) prompting/finetuning has been the major way to invoke/improve LLMs' reasoning capabilities.Many variants of CoT prompting have been proposed to improve upon the vanilla CoT prompting (Chen et al., 2023;Yao et al., 2024).On the other hand, many works have focused on fine-tuning LLMs on generated high-quality CoT training data (Wang et al., 2022;Nye et al., 2022;Yuan et al., 2023).However, they all rely on the annotated Q-A pairs to generate corresponding paths with LM, which limits the size of augmented data and requires large LMs to do the CoT generation.Our proposed method does not need supervised seed data and thus can be extended to the vast amount of pre-training data.Our method is also lightweight, which only requires a small/medium LM to produce the step embeddings and then do clustering on them.</p>
<p>C. Experiment Details</p>
<p>C.1.Datasets Knowledge graph datasets For KL analysis, we focus on two KGs: Countries (Bouchard et al., 2015) and UMLS (Kok &amp; Domingos, 2007), as they have a reasonable time complexity to compute the aggregated probabilities for long paths.The Countries (Bouchard et al., 2015) contains two relations (locatedIn and neighborOf) and 227 entities, including countries, regions, and subregions.We use the hardest version (S3) of the Countries.The Unified Medical Language System (UMLS) (Kok &amp; Domingos, 2007) is a more complex KG built from biomedicine knowledge, containing 49 relations and 135 entities.Example entities are diseases and antibiotics, and example relations are treats and diagnoses.</p>
<p>We add three more datasets for computing the prediction accuracy v.s.different random walk path lengths: Kinship (Denham, 2020), NELL-995 (Xiong et al., 2017), and FB15K-237 (Toutanova et al., 2015).The Kinship dataset contains 104 entities and 26 kinship relationships among members of the Alyawarra tribe from Central Australia.The NELL-995 dataset contains 75,492 entities and 200 relations, which is built from the Web via an intelligent agent called Never-Ending Language Learner.The FB15K-237 dataset contains 14,505 entities and 237 relations derived from Freebase.We adopt a processed version of these datasets from Das et al. (2017).</p>
<p>Math word problem datasets We conduct experiments on three math word problem (MWP) datasets: GSM8K (Cobbe et al., 2021), AQUA (Ling et al., 2017), SVAMP (Patel et al., 2021).The Grade School Math dataset (GSM8K) contains 8.5K examples of linguistically diverse grade school math world problems.The AQUA-RAT dataset contains 100K samples of mathematical problems, along with sequences of human-readable mathematical expressions in natural language.The SVAMP dataset is a testing set consisting of elementary-level MWPs.The training set is a combination of simpler MWPs: MAWPS (Koncel-Kedziorski et al., 2016) and ASDiv-A (Miao et al., 2020)  StrategyQA is annotated with decomposed questions, which we used as the Chain-of-thought (CoT) path of the question.Since the test set labels are not publicly released and the testing set predictions are only allowed to be verified every 7 days, we split the original training set into a new training and testing set.</p>
<p>C.2. Training Details</p>
<p>Logical reasoning We train randomly initialized GPT-2 (Radford et al., 2019) (124M parameters) with batch size 16 and learning rate 5e-4 using AdamW optimizer (Loshchilov &amp; Hutter, 2017) on one 24G Titan GPU.</p>
<p>CoT reasoning We continually (LORA) train all base LLMs with batch size 16 and learning rate 2e-4 using AdamW optimizer (Loshchilov &amp; Hutter, 2017) on one 40G A100 GPU.</p>
<p>C.3. Additional Latent Reasoning Graph Examples</p>
<p>With the GSM8K examples in Figure 7, we show that our method discovers a pattern that first computes money for parallel items/individuals and then sums them up, within 3 and 4 steps respectively.With the StrategyQA example in Figure 7, we show a 2-step pattern that first asks about an emotion/psychology fact and then asks the applicability to an individual in the second question.</p>
<p>D. Limitations</p>
<p>While the scope of this project is to provide a plausible understanding of how language models obtain reasoning abilities from next-token pre-training, we acknowledge that there are other possible ways of understanding this phenomenon.While our empirical results show our hypothesis is also effective in real-world reasoning tasks, our experiments remain on a small scale with specific tasks, limited by our computation resources and project scope.An important future work is to apply our proposed random walk training method to a large and diverse reasoning corpus with more training steps in the actual pre-training phase and verify the effectiveness of our method in improving the general reasoning ability of LLMs.We also want to point out that our proposed method is effectively up-sampling the given training set and might amplify unwanted artifacts/biases if exist in the original dataset.</p>
<p>Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</p>
<p>Figure 2 .
2
Figure 2. KL divergence between various reference distributions and LM distribution, with different maximum random walk lengths, averaged over Countries (top) and UMLS (bottom) testing set, respectively.The rows correspond to the LM distribution PLM(e2|e1, r) with maximum pre-training random walk path lengths (Lmax) ranging from 1 to 10. From left to right, the columns correspond to the weighted aggregation distribution Pw(e2|e1, r) with maximum random walk path lengths (Nmax) from 1 to 10, the unweighted aggregation distribution Ps(e2|e1, r) with maximum random walk path lengths (Nmax) from 1 to 10, the reference distribution P * (e2|e1, r), and the uniform distribution Pu(e2), respectively.A darker color represents a smaller KL value, meaning that the two distributions are closer.In general, KL[Pw, PLM] is always smaller than KL[Ps, PLM], which implies that LM is learning the difference in rule importance.KL[P * , PLM] and KL[Pu, PLM] serve as anchor points to show the scale of KL values.KL[P * , PLM] is generally high because the probability mass concentrates on correct answers, thus it can be very different from the LM distribution.Thus KL[P * , PLM] shows how peaky the LM distribution is, and KL[Pu, PLM] shows how flat the LM distribution is.</p>
<p>Figure 3 .
3
Figure 3. Testing accuracy w.r.t.various maximum pre-training random walk lengths (1 ≤ Lmax ≤ 10) on Countries (left) and UMLS (right) datasets, respectively.For Countries, the LM (PLM) performance converges to the weighted aggregation (Pw) performance, while for UMLS, LM consistently outperforms both weighted (Pw) and unweighted (Ps) aggregation performance.This is likely because LM (PLM) can learn a better logical rule weighting scheme than weighted aggregation (Pw) in more complex KGs.</p>
<p>Figure 4 .
4
Figure 4. Testing accuracy of LM trained on different random walk path lengths.Each line corresponds to a different KG dataset and thus is not directly comparable.We want to highlight the common trend here that each line peaks at some optimal path length.</p>
<p>Input:</p>
<p>CoT dataset D, latent graph G, maximum path length Lmax.Randomly initialize current node a = A k .Initialize path p = [] repeat Randomly choose a CoT step r i j ∈ a. Uniformly sample m from [1, L].Append r i j , r i j+1 , ..., r i min{j+m,n i } to path p. Suppose r i min{j+m,n i } ∈ A l .Set a = A l .until len(p) ≥ Lmax.</p>
<p>Figure 5 .
5
Figure 5. Testing accuracy of continue pre-training with our random walk paths of different length Lmax.Each line corresponds to a different MWP dataset and thus is not directly comparable.We want to highlight the common trend here that each line would peak at some optimal path length range, which is similar to Figure4.</p>
<p>show high-frequency node patterns of CoTs in the training set and corresponding CoT examples.We show examples from GSM8K and StrategyQA as they are shorter.With the GSM8K examples, we show our method discovers a 2-step pattern that first computes the baseline quantity and then performs division/multiplication to get the goal quantity based on the question specification.With the StrategyQA examples, we show a 3-step pattern that first</p>
<p>Figure 6 .
6
Figure 6.High-frequency node patterns in the training data of GSM8K and StrategyQA, discovered by our constructed latent reasoning graphs, with example CoT solutions belonging to the node pattern.</p>
<p>with 3.5k training examples in total.</p>
<p>Figure 7 .
7
Figure 7.Additional high-frequency node patterns in the training data of GSM8K and StrategyQA, discovered by our constructed latent reasoning graphs, with example CoT solutions belonging to the node pattern.</p>
<p>Table 1 .
1
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation Testing accuracy of different open source LMs continue pre-trained with our random walk paths and then supervised fine-tuned.The supervised fine-tuning baseline (SFT) is fine-tuned by the same number of total steps.Results are reported on five CoT datasets.
ModelMethod GSM8K AQUA SVAMP StrategyQA LogicalDeduction Avg.Gemma (2B)SFT24.831.456.454.250.743.5Ours26.133.960.356.351.645.6Yi (6B)SFT32.237.065.865.862.252.6Ours33.139.867.070.063.354.6Llama 2 (7B)SFT26.830.053.358.455.344.8Ours28.534.655.863.756.147.7Llama 2 (13B)SFT37.135.066.469.555.752.7Ours41.237.469.071.257.755.3AblationGSM8K AQUA SVAMP Avg.#Steps =026.830.053.336.720027.530.153.637.150028.534.655.839.6100024.932.351.636.3#Nodes =026.830.053.336.71026.830.354.837.35026.629.954.737.110028.534.655.839.620026.631.152.536.7</p>
<p>Table 2 .
2
Ablation on the number of random walk training steps M and the number of clusters/nodes K.</p>
<p>6+ reasoning steps per question.7</p>
<p>We use a randomly initialized GPT-2 model(Radford et al., 2019).
In practice, we take T = 0.01.
More dataset details can be found in Appendix C.1.
More related work on logical reasoning and math reasoning can be found in Appendix B.
AcknowledgementThis work was supported by the National Science Foundation award #2048122.The views expressed are those of the author and do not reflect the official policy or position of the US government.
Z Azerbayev, H Schoelkopf, K Paster, M D Santos, S Mcaleer, A Q Jiang, J Deng, S Biderman, S Welleck, Llemma, arXiv:2310.10631An open language model for mathematics. 2023arXiv preprint</p>
<p>When do program-of-thoughts work for reasoning?. Z Bi, N Zhang, Y Jiang, S Deng, G Zheng, H Chen, arXiv:2308.154522023arXiv preprint</p>
<p>On approximate reasoning capabilities of low-rank vector spaces. G Bouchard, S Singh, T Trouillon, AAAI Spring Symposia. 2015</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, Transactions on Machine Learning Research. 2023</p>
<p>Transformer working memory enables regular language reasoning and natural language length extrapolation. T.-C Chi, T.-H Fan, A I Rudnicky, P J Ramadge, arXiv:2305.037962023arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>R Das, S Dhuliawala, M Zaheer, L Vilnis, I Durugkar, A Krishnamurthy, A Smola, A Mccallum, arXiv:1711.05851answer: Reasoning over paths in knowledge bases using reinforcement learning. 2017arXiv preprint</p>
<p>Artificial intelligence / machine learning research using the australian aboriginal alyawarra kinship dataset: Partial bibliography 2004-2020. W Denham, Mathematical Anthropology and Cultural Theory. 2020</p>
<p>Towards revealing the mystery behind chain of thought: A theoretical perspective. G Feng, B Zhang, Y Gu, H Ye, D He, L Wang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Causal abstractions of neural networks. A Geiger, H Lu, T Icard, C Potts, Advances in Neural Information Processing Systems. 202134</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, 2021Transactions of the Association for Computational Linguistics9</p>
<p>A new model for earning in raph domains. M Gori, G Monfardini, F Scarselli, 10.1109/IJCNN.2005.1555942Proceedings of the International Joint Conference on Neural Networks. the International Joint Conference on Neural Networks01 20052</p>
<p>How does gpt-2 compute greater-than?. M Hanna, O Liu, A Variengien, arXiv:2305.00586Interpreting mathematical abilities in a pre-trained language model. 2023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2020</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. Y Hou, J Li, Y Fei, A Stolfo, W Zhou, G Zeng, A Bosselut, M Sachan, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>S Jain, R Kirk, E S Lubana, R P Dick, H Tanaka, E Grefenstette, T Rocktäschel, D S Krueger, arXiv:2311.12786Mechanistically analyzing the effects of finetuning on procedurally defined tasks. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Advances in neural information processing systems. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 202235</p>
<p>Statistical predicate invention. S Kok, P Domingos, 10.1145/1273496.1273551Proceedings of the 24th International Conference on Machine Learning, ICML '07. the 24th International Conference on Machine Learning, ICML '07New York, NY, USA2007Association for Computing Machinery. ISBN 9781595937933</p>
<p>MAWPS: A math word problem repository. R Koncel-Kedziorski, S Roy, A Amini, N Kushman, H Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. K Knight, A Nenkova, O Rambow, the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJune 2016</p>
<p>Random walk inference and learning in a large scale knowledge base. N Lao, T Mitchell, W W Cohen, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UK.Association for Computational LinguisticsJuly 2011</p>
<p>Dissecting chain-of-thought: A study on compositional in-context learning of mlps. Y Li, K Sreenivasan, A Giannou, D Papailiopoulos, S Oymak, arXiv:2305.188692023arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. W Ling, D Yogatama, C Dyer, P Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Transformers learn shortcuts to automata. B Liu, J T Ash, S Goel, A Krishnamurthy, C Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>I Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Auto-regressive next-token predictors are universal learners. E Malach, arXiv:2309.069792023arXiv preprint</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. S Miao, -Y, C.-C Liang, K.-Y Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Triggering multi-hop reasoning for question answering in language models using soft prompts and random walks. K Misra, C Nogueira Dos Santos, S Shakeri, doi: 10.18653Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>URL. </p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, 2022</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Wang, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeDecember 2023. Association for Computational Linguistics</p>
<p>URL. </p>
<p>Are NLP models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>URL. </p>
<p>Why think step by step? reasoning emerges from the locality of experience. B Prystawski, M Y Li, N Goodman, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>M Qu, J Chen, L.-P Xhonneux, Y Bengio, J Tang, Rnnlogic, arXiv:2010.04029Learning logic rules for reasoning on knowledge graphs. 2020arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>How capable can a transformer become? a study on synthetic. R Ramesh, M Khona, R P Dick, H Tanaka, E S Lubana, arXiv:2311.129972023interpretable tasks. arXiv preprint</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. Y Razeghi, I V Logan, R L Gardner, M Singh, S , arXiv:2202.072062022arXiv preprint</p>
<p>Markov logic networks. M Richardson, P Domingos, 10.1007/s10994-006-5833-1Mach. Learn. 0885- 6125621-2feb 2006</p>
<p>Modeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, Van Den, R Berg, I Titov, M Welling, The Semantic Web: 15th International Conference. Proceedings. Heraklion, Crete, GreeceSpringer2018. June 3-7, 2018. 201815</p>
<p>. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, A Xiang, A Parrish, A Nie, A Hussain, A Askell, A Dsouza, A Slone, A Rahane, A S Iyer, A J Andreassen, A Madotto, A Santilli, A Stuhlmüller, A M Dai, A La, A Lampinen, A Zou, A Jiang, A Chen, A Vuong, A Gupta, A Gottardi, A Norelli, A Venkatesh, A Gholamidavoodi, A Tabassum, A Menezes, A Kirubarajan, A Mullokandov, A Sabharwal, A Herrick, A Efrat, A Erdem, ¸ Karakas, A Roberts, B R Loe, B S Zoph, B Bojanowski, B Özyurt, B Hedayatnia, B Neyshabur, B Inden, B Stein, B Ekmekci, B Lin, B Y Howald, B Orinion, B Diao, C Dour, C Stinson, C Argueta, C Ferri, C Singh, C Rathkopf, C Meng, C Baral, C Wu, C Callison-Burch, C Waites, C Voigt, C Manning, C D Potts, C Ramirez, C Rivera, C E Siro, C Raffel, C Ashcraft, C Garbacea, C Sileo, D Garrette, D Hendrycks, D Kilman, D Roth, D Freeman, C D Khashabi, D Levy, D González, D M Perszyk, D Hernandez, D Chen, D Ippolito, D Gilboa, D Dohan, D Drakard, D Jurgens, D Datta, D Ganguli, D Emelin, D Kleyko, D Yuret, D Chen, D Tam, D Hupkes, D Misra, D Buzan, D Mollo, D C Yang, D Lee, D.-H Schrader, D Shutova, E Cubuk, E D Segal, E Hagerman, E Barnes, E Donoway, E Pavlick, E Rodolà, E Lam, E Chu, E Tang, E Erdem, E Chang, E Chi, E A Dyer, E Jerzak, E Kim, E Manyasi, E E Zheltonozhskii, E Xia, F Siar, F Martínez-Plumed, F Happé, F Chollet, F Rong, F Mishra, G Winata, G I De Melo, G Kruszewski, G Parascandolo, G Mariani, G Wang, G X Jaimovitch-Lopez, G Betz, G Gur-Ari, G Galijasevic, H Kim, H Rashkin, H Hajishirzi, H Mehta, H Bogar, H Shevlin, H F A Schuetze, H Yakura, H Zhang, H Wong, H M Ng, I Noble, I Jumelet, J Geissinger, J Kernion, J Hilton, J Lee, J Fisac, J F Simon, J B Koppel, J Zheng, J Zou, J Kocon, J Thompson, J Wingfield, J Kaplan, J Radom, J Sohl-Dickstein, J Phang, J Wei, J Yosinski, J Novikova, J Bosscher, J Marsh, J Kim, J Taal, J Engel, J Alabi, J Xu, J Song, J Tang, J Waweru, J Burden, J Miller, J Balis, J U Batchelder, J Berant, J Frohberg, J Rozen, J Hernandez-Orallo, J Boudeman, J Guerr, J Jones, J Tenenbaum, J B Rule, J S Chua, J Kanclerz, K Livescu, K Krauth, K Gopalakrishnan, K Ignatyeva, K Markert, K Dhole, K Gimpel, K Omondi, K Mathewson, K W Chiafullo, K Shkaruta, K Shridhar, K Mcdonell, K Richardson, K Reynolds, L Gao, L Zhang, L Dugan, L Qin, L Contreras-Ochando, L Morency, L.-P Moschella, L Lam, L Noble, L Schmidt, L He, L Oliveros-Colón, L Metz, L Senel, L K Bosma, M Sap, M Hoeve, M T Farooqi, M Faruqui, M Mazeika, M Baturan, M Marelli, M Maru, M Ramirez-Quintana, M J Tolkiehn, M Giulianelli, M Lewis, M Potthast, M Leavitt, M L Hagen, M Schubert, M Baitemirova, M O Arnaud, M Mcelrath, M Yee, M A Cohen, M Gu, M Ivanitskiy, M Starritt, M Strube, M Swedrowski, M Bevilacqua, M Yasunaga, M Kale, M Cain, M Xu, M Suzgun, M Walker, M Tiwari, M Bansal, M Aminnaseri, M Geva, M Gheini, M , T , M V Peng, N Chi, N A Lee, N Krakover, N G , .-A Cameron, N Roberts, N Doiron, N Martinez, N Nangia, N Deckers, N Muennighoff, N Keskar, N S Iyer, N S Constant, N Fiedel, N Wen, N Zhang, O Agha, O Elbaghdadi, O Levy, O Evans, O Casares, P A M Doshi, P Fung, P Liang, P P Vicol, P Alipoormolabashi, P Liao, P Liang, P Chang, P W Eckersley, P Htut, P M Hwang, P Miłkowski, P Patil, P Pezeshkpour, P Oli, P Mei, Q Lyu, Q Chen, Q Banjade, R Rudolph, R E Gabriel, R Habacker, R Risco, R Millière, R Garg, R Barnes, R Saurous, R A Arakawa, R Raymaekers, R Frank, R Sikand, R Novak, R Sitelew, R Bras, R L Liu, R Jacobs, R Zhang, R Salakhutdinov, R Chi, R A Lee, S R Stovall, R Teehan, R Yang, R Singh, S Mohammad, S M Anand, S Dillavou, S Shleifer, S Wiseman, S Gruetter, S Bowman, S R Schoenholz, S S Han, S Kwatra, S Rous, S A Ghazarian, S Ghosh, S Casey, S Bischoff, S Gehrmann, S Schuster, S Sadeghi, S Hamdan, S Zhou, S Srivastava, S Shi, S Singh, S Asaadi, S Gu, S S Pachchigar, S Toshniwal, S Upadhyay, S Debnath, S S Shakeri, S Thormeyer, S Melzi, S Reddy, S Makini, S P Lee, S.-H Torene, S Hatwar, S Dehaene, S Divic, S Ermon, S Biderman, S Lin, S Prasad, S Piantadosi, S Shieber, S Misherghi, S Kiritchenko, S Mishra, S Linzen, T Schuster, T Li, T Yu, T Ali, T Hashimoto, T Wu, T.-L Desbordes, T Rothschild, T Phan, T Wang, T Nkinyili, T Schick, T Kornev, T Tunduny, T Gerstenberg, T Chang, T Neeraj, T Khot, T Shultz, T Shaham, U Misra, V Demberg, V Nyamai, V Raunak, V Ramasesh, V V Padmakumar, V Srikumar, V Fedus, W Saunders, W Zhang, W Vossen, W Ren, X Tong, X Zhao, X Wu, X Shen, X Yaghoobzadeh, Y Lakretz, Y Song, Y Bahri, Y Choi, Y Yang, Y Hao, Y Chen, Y Belinkov, Y Hou, Y Hou, Y Bai, Y Seid, Z Zhao, Z Wang, Z Wang, Z J Wang, Z Wu, 2023Z. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Representing text for joint embedding of text and knowledge bases. K Toutanova, D Chen, P Pantel, H Poon, P Choudhury, M Gamon, 10.18653/v1/D15-1174Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbonAssociation for Computational LinguisticsSeptember 2015</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Towards understanding chainof-thought prompting: An empirical study of what matters. B Wang, S Min, X Deng, J Shen, Y Wu, L Zettlemoyer, H Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Interpretability at scale: Identifying causal mechanisms in alpaca. Z Wu, A Geiger, T Icard, C Potts, N Goodman, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Conditions for length generalization in learning reasoning skills. C Xiao, B Liu, arXiv:2311.161732023arXiv preprint</p>
<p>W Xiong, T Hoang, W Y Wang, Deeppath, arXiv:1707.06690A reinforcement learning method for knowledge graph reasoning. 2017arXiv preprint</p>
<p>Differentiable learning of logical rules for knowledge base reasoning. F Yang, Z Yang, W W Cohen, 201730Advances in neural information processing systems</p>
<p>K Yang, A M Swope, A Gu, R Chalamala, P Song, S Yu, S Godil, R Prenger, A Anandkumar, Leandojo, arXiv:2306.15626Theorem proving with retrieval-augmented language models. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Z Yuan, H Yuan, C Li, G Dong, C Tan, C Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>What algorithms can transformers learn? a study in length generalization. H Zhou, A Bradley, E Littwin, N Razin, O Saremi, J Susskind, S Bengio, Nakkiran , arXiv:2310.160282023arXiv preprint</p>
<p>Neural bellman-ford networks: A general graph neural network framework for link prediction. Z Zhu, Z Zhang, L.-P Xhonneux, J Tang, M Ranzato, A Beygelzimer, Y Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems. J W , Curran Associates, Inc202134</p>            </div>
        </div>

    </div>
</body>
</html>
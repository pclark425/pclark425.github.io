<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7373 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7373</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7373</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-232014530</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2102.11570v1.pdf" target="_blank">Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Anomalies or failures in large computer systems, such as the cloud, have an impact on a large number of users that communicate, compute, and store information. Therefore, timely and accurate anomaly detection is necessary for reliability, security, safe operation, and mitigation of losses in these increasingly important systems. Recently, the evolution of the software industry opens up several problems that need to be tackled including (1) addressing the software evolution due software upgrades, and (2) solving the cold-start problem, where data from the system of interest is not available. In this paper, we propose a framework for anomaly detection in log data, as a major troubleshooting source of system information. To that end, we utilize pre-trained general-purpose language models to preserve the semantics of log messages and map them into log vector embeddings. The key idea is that these representations for the logs are robust and less invariant to changes in the logs, and therefore, result in a better generalization of the anomaly detection models. We perform several experiments on a cloud dataset evaluating different language models for obtaining numerical log representations such as BERT, GPT-2, and XL. The robustness is evaluated by gradually altering log messages, to simulate a change in semantics. Our results show that the proposed approach achieves high performance and robustness, which opens up possibilities for future research in this direction.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7373.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7373.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-Log</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (used for log embeddings) with Bi-LSTM anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sentence-level BERT embeddings are used to convert log templates into dense vectors; a Bi-LSTM learns sequences of these embeddings and is used with two objectives (next-template classification and next-embedding regression) to detect anomalies in log streams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional encoder transformer pre-trained for sentence-level representations (used here to obtain sentence/log-template embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Embed log templates with pre-trained sentence-level language model (BERT) → feed sequences of embeddings to a Bi-LSTM; detect anomalies via (A) multi-class next-template prediction with nearest-template matching and top-k rule, or (B) next-embedding regression using MSE and thresholding (q-th percentile).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>CloudLab OpenStack log dataset from Loghub: training on a 'normal' dataset (sequences of parsed log templates). Also used synthetic augmentations (semantic token deletion/swap/imputation and sequential deletion/swap/imputation) and dataset A→B alterations for transfer experiments; few-shot adaptation on altered dataset B for transfer (number of shots not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries as tokenized templates; sequential/time-series of template embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CloudLab OpenStack log dataset (Loghub); additionally synthetically altered versions (semantic and sequential alterations) and constructed dataset A / B for transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 (reported per experiment and per learning objective)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Regression (semantic alteration): Precision 0.43, Recall 1.00, F1 0.56. Regression (sequential alteration): Precision 0.49, Recall 1.00, F1 0.66. Classification (semantic): Precision 0.37, Recall 1.00, F1 0.54. Classification (sequential): Precision 0.50, Recall 1.00, F1 0.67. Model transfer (P=15% altered): Regression B-similar: P 0.58, R 0.70, F1 0.63; Regression B-different: P 0.52, R 1.00, F1 0.68. Classification B-similar: P 0.61, R 1.00, F1 0.75; Classification B-different: P 0.68, R 1.00, F1 0.81.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against other pre-trained sentence-level embeddings (GPT-2 and XL-Transformers) within the same framework; no classical anomaly-detection baselines (e.g., Isolation Forest) reported with quantitative scores in the tables. The paper also contrasts with one-hot/template index encodings in discussion but doesn't provide numeric baselines for them.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Training is sequence-prediction (self-/supervised on normal sequences); model transfer uses few-shot adaptation on dataset B (few-shot count not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires parsed templates (uses Drain). Classification objective assumes a closed set of templates and needs nearest-template matching with a maximal-distance hyperparameter to handle novel templates; sensitivity to choice of q-percentile threshold for regression. Reported behavior: BERT is the most consistent and robust across objectives; however, classification still needs template-matching heuristics to handle novelties.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7373.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7373.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-Log</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (used for log embeddings) with Bi-LSTM anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2 sentence-level embeddings are used to represent log templates; sequences of these embeddings are modeled by a Bi-LSTM and anomalies are detected either by next-template classification (with nearest-template matching/top-k) or next-embedding regression (MSE thresholding).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only autoregressive transformer pre-trained for language modeling; used here to produce sentence/log-template embeddings (sentence-level embedding variant used by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Embed log templates with GPT-2 → Bi-LSTM sequence model; detect anomalies by (A) multi-class classification of next template (with nearest-template matching and top-k rule) or (B) regression to predict next-template embedding with MSE and q-th percentile thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>CloudLab OpenStack log dataset (Loghub) normal sequences; synthetic semantic and sequential log alterations for robustness testing; dataset A/B constructed for transfer experiments; few-shot adaptation on B (not numerically specified).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries (parsed templates), sequential/time-series of template embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CloudLab OpenStack log dataset (Loghub) with synthetic alterations and A/B transfer splits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Regression (semantic alteration): Precision 0.88, Recall 1.00, F1 0.94. Regression (sequential alteration): Precision 0.79, Recall 1.00, F1 0.87. Classification (semantic): Precision 0.24, Recall 0.70, F1 0.36. Classification (sequential): Precision 0.31, Recall 0.70, F1 0.43. Model transfer (P=15% altered): Regression B-similar: P 0.23, R 0.05, F1 0.08; Regression B-different: P 0.94, R 1.00, F1 0.97. Classification B-similar: P 0.27, R 1.00, F1 0.43; Classification B-different: P 0.09, R 1.00, F1 0.17.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against BERT and XL-Transformers embeddings under identical Bi-LSTM setups. No reported classical anomaly detection baselines with numerical comparison in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Sequence-model training on normal data (self-/supervised). Model transfer uses few-shot adaptation on dataset B (number of shots unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance strongly affected by choice of learning objective: GPT-2 embeddings perform very well for regression-based next-embedding prediction but notably worse for the classification objective and in small-change transfer scenarios (e.g., B-similar regression yields poor recall). Shows instability vs. objective and transfer settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7373.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7373.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XL-Log</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XL-Transformers (XLNet-style) with Bi-LSTM anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sentence-level XL-style embeddings are used to map log templates to vectors; a Bi-LSTM models sequences and anomalies are detected via next-template classification (with nearest-template matching/top-k) or next-embedding regression (MSE thresholding).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XL-Transformers (XLNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalized autoregressive (XLNet-style) transformer pretraining (referred to as XL-Transformers in the paper) used to obtain sentence-level embeddings for log templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Embed log templates with XL-style pre-trained model → Bi-LSTM sequence model; anomaly detection via (A) multi-class next-template prediction with nearest-template matching/top-k, or (B) regression to next embedding with MSE and q-th percentile threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>CloudLab OpenStack log dataset (Loghub) normal sequences; synthetic semantic and sequential alterations applied for robustness tests; constructed dataset A/B for transfer evaluation with few-shot adaptation on B.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Parsed log templates; sequential/time-series of template embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CloudLab OpenStack log dataset (Loghub) and synthetically altered variants plus A/B transfer splits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Regression (semantic alteration): Precision 0.21, Recall 0.63, F1 0.31. Regression (sequential alteration): Precision 0.32, Recall 0.61, F1 0.42. Classification (semantic): Precision 0.26, Recall 1.00, F1 0.41. Classification (sequential): Precision 0.36, Recall 1.00, F1 0.53. Model transfer (P=15% altered): Regression B-similar: P 0.45, R 0.70, F1 0.55; Regression B-different: P 0.18, R 0.47, F1 0.26. Classification B-similar: P 0.53, R 1.00, F1 0.69; Classification B-different: P 0.23, R 1.00, F1 0.38.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Directly compared to BERT and GPT-2 embeddings under the same Bi-LSTM pipeline; no classical anomaly-detection baselines reported with numeric results in the paper's main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Training on normal sequences (self-/supervised); model transfer uses few-shot adaptation on target dataset B (exact shot count not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>XL-Transformers performed inconsistently: reasonable for similar-dataset transfer but failed when changes were small in some settings (low scores for B-different regression). Overall less consistent than BERT across objectives and transfer scenarios. Same limitations as classification closed-world assumption and nearest-template matching heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-attentive classification-based anomaly detection in unstructured logs <em>(Rating: 2)</em></li>
                <li>Deeplog: Anomaly detection and diagnosis from system logs through deep learning <em>(Rating: 2)</em></li>
                <li>Robust log-based anomaly detection on unstable log data <em>(Rating: 1)</em></li>
                <li>Multi-source anomaly detection in distributed it systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7373",
    "paper_id": "paper-232014530",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "BERT-Log",
            "name_full": "BERT (used for log embeddings) with Bi-LSTM anomaly detector",
            "brief_description": "Sentence-level BERT embeddings are used to convert log templates into dense vectors; a Bi-LSTM learns sequences of these embeddings and is used with two objectives (next-template classification and next-embedding regression) to detect anomalies in log streams.",
            "citation_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_description": "Bidirectional encoder transformer pre-trained for sentence-level representations (used here to obtain sentence/log-template embeddings).",
            "model_size": null,
            "anomaly_detection_approach": "Embed log templates with pre-trained sentence-level language model (BERT) → feed sequences of embeddings to a Bi-LSTM; detect anomalies via (A) multi-class next-template prediction with nearest-template matching and top-k rule, or (B) next-embedding regression using MSE and thresholding (q-th percentile).",
            "prompt_template": null,
            "training_data": "CloudLab OpenStack log dataset from Loghub: training on a 'normal' dataset (sequences of parsed log templates). Also used synthetic augmentations (semantic token deletion/swap/imputation and sequential deletion/swap/imputation) and dataset A→B alterations for transfer experiments; few-shot adaptation on altered dataset B for transfer (number of shots not specified).",
            "data_type": "Log entries as tokenized templates; sequential/time-series of template embeddings",
            "dataset_name": "CloudLab OpenStack log dataset (Loghub); additionally synthetically altered versions (semantic and sequential alterations) and constructed dataset A / B for transfer experiments",
            "evaluation_metric": "Precision, Recall, F1 (reported per experiment and per learning objective)",
            "performance": "Regression (semantic alteration): Precision 0.43, Recall 1.00, F1 0.56. Regression (sequential alteration): Precision 0.49, Recall 1.00, F1 0.66. Classification (semantic): Precision 0.37, Recall 1.00, F1 0.54. Classification (sequential): Precision 0.50, Recall 1.00, F1 0.67. Model transfer (P=15% altered): Regression B-similar: P 0.58, R 0.70, F1 0.63; Regression B-different: P 0.52, R 1.00, F1 0.68. Classification B-similar: P 0.61, R 1.00, F1 0.75; Classification B-different: P 0.68, R 1.00, F1 0.81.",
            "baseline_comparison": "Compared against other pre-trained sentence-level embeddings (GPT-2 and XL-Transformers) within the same framework; no classical anomaly-detection baselines (e.g., Isolation Forest) reported with quantitative scores in the tables. The paper also contrasts with one-hot/template index encodings in discussion but doesn't provide numeric baselines for them.",
            "zero_shot_or_few_shot": "Training is sequence-prediction (self-/supervised on normal sequences); model transfer uses few-shot adaptation on dataset B (few-shot count not specified).",
            "limitations_or_failure_cases": "Requires parsed templates (uses Drain). Classification objective assumes a closed set of templates and needs nearest-template matching with a maximal-distance hyperparameter to handle novel templates; sensitivity to choice of q-percentile threshold for regression. Reported behavior: BERT is the most consistent and robust across objectives; however, classification still needs template-matching heuristics to handle novelties.",
            "computational_cost": null,
            "uuid": "e7373.0",
            "source_info": {
                "paper_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "GPT2-Log",
            "name_full": "GPT-2 (used for log embeddings) with Bi-LSTM anomaly detector",
            "brief_description": "GPT-2 sentence-level embeddings are used to represent log templates; sequences of these embeddings are modeled by a Bi-LSTM and anomalies are detected either by next-template classification (with nearest-template matching/top-k) or next-embedding regression (MSE thresholding).",
            "citation_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_description": "Decoder-only autoregressive transformer pre-trained for language modeling; used here to produce sentence/log-template embeddings (sentence-level embedding variant used by authors).",
            "model_size": null,
            "anomaly_detection_approach": "Embed log templates with GPT-2 → Bi-LSTM sequence model; detect anomalies by (A) multi-class classification of next template (with nearest-template matching and top-k rule) or (B) regression to predict next-template embedding with MSE and q-th percentile thresholding.",
            "prompt_template": null,
            "training_data": "CloudLab OpenStack log dataset (Loghub) normal sequences; synthetic semantic and sequential log alterations for robustness testing; dataset A/B constructed for transfer experiments; few-shot adaptation on B (not numerically specified).",
            "data_type": "Log entries (parsed templates), sequential/time-series of template embeddings",
            "dataset_name": "CloudLab OpenStack log dataset (Loghub) with synthetic alterations and A/B transfer splits",
            "evaluation_metric": "Precision, Recall, F1",
            "performance": "Regression (semantic alteration): Precision 0.88, Recall 1.00, F1 0.94. Regression (sequential alteration): Precision 0.79, Recall 1.00, F1 0.87. Classification (semantic): Precision 0.24, Recall 0.70, F1 0.36. Classification (sequential): Precision 0.31, Recall 0.70, F1 0.43. Model transfer (P=15% altered): Regression B-similar: P 0.23, R 0.05, F1 0.08; Regression B-different: P 0.94, R 1.00, F1 0.97. Classification B-similar: P 0.27, R 1.00, F1 0.43; Classification B-different: P 0.09, R 1.00, F1 0.17.",
            "baseline_comparison": "Compared against BERT and XL-Transformers embeddings under identical Bi-LSTM setups. No reported classical anomaly detection baselines with numerical comparison in tables.",
            "zero_shot_or_few_shot": "Sequence-model training on normal data (self-/supervised). Model transfer uses few-shot adaptation on dataset B (number of shots unspecified).",
            "limitations_or_failure_cases": "Performance strongly affected by choice of learning objective: GPT-2 embeddings perform very well for regression-based next-embedding prediction but notably worse for the classification objective and in small-change transfer scenarios (e.g., B-similar regression yields poor recall). Shows instability vs. objective and transfer settings.",
            "computational_cost": null,
            "uuid": "e7373.1",
            "source_info": {
                "paper_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "XL-Log",
            "name_full": "XL-Transformers (XLNet-style) with Bi-LSTM anomaly detector",
            "brief_description": "Sentence-level XL-style embeddings are used to map log templates to vectors; a Bi-LSTM models sequences and anomalies are detected via next-template classification (with nearest-template matching/top-k) or next-embedding regression (MSE thresholding).",
            "citation_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
            "mention_or_use": "use",
            "model_name": "XL-Transformers (XLNet)",
            "model_description": "Generalized autoregressive (XLNet-style) transformer pretraining (referred to as XL-Transformers in the paper) used to obtain sentence-level embeddings for log templates.",
            "model_size": null,
            "anomaly_detection_approach": "Embed log templates with XL-style pre-trained model → Bi-LSTM sequence model; anomaly detection via (A) multi-class next-template prediction with nearest-template matching/top-k, or (B) regression to next embedding with MSE and q-th percentile threshold.",
            "prompt_template": null,
            "training_data": "CloudLab OpenStack log dataset (Loghub) normal sequences; synthetic semantic and sequential alterations applied for robustness tests; constructed dataset A/B for transfer evaluation with few-shot adaptation on B.",
            "data_type": "Parsed log templates; sequential/time-series of template embeddings",
            "dataset_name": "CloudLab OpenStack log dataset (Loghub) and synthetically altered variants plus A/B transfer splits",
            "evaluation_metric": "Precision, Recall, F1",
            "performance": "Regression (semantic alteration): Precision 0.21, Recall 0.63, F1 0.31. Regression (sequential alteration): Precision 0.32, Recall 0.61, F1 0.42. Classification (semantic): Precision 0.26, Recall 1.00, F1 0.41. Classification (sequential): Precision 0.36, Recall 1.00, F1 0.53. Model transfer (P=15% altered): Regression B-similar: P 0.45, R 0.70, F1 0.55; Regression B-different: P 0.18, R 0.47, F1 0.26. Classification B-similar: P 0.53, R 1.00, F1 0.69; Classification B-different: P 0.23, R 1.00, F1 0.38.",
            "baseline_comparison": "Directly compared to BERT and GPT-2 embeddings under the same Bi-LSTM pipeline; no classical anomaly-detection baselines reported with numeric results in the paper's main tables.",
            "zero_shot_or_few_shot": "Training on normal sequences (self-/supervised); model transfer uses few-shot adaptation on target dataset B (exact shot count not provided).",
            "limitations_or_failure_cases": "XL-Transformers performed inconsistently: reasonable for similar-dataset transfer but failed when changes were small in some settings (low scores for B-different regression). Overall less consistent than BERT across objectives and transfer scenarios. Same limitations as classification closed-world assumption and nearest-template matching heuristics.",
            "computational_cost": null,
            "uuid": "e7373.2",
            "source_info": {
                "paper_title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-attentive classification-based anomaly detection in unstructured logs",
            "rating": 2,
            "sanitized_title": "selfattentive_classificationbased_anomaly_detection_in_unstructured_logs"
        },
        {
            "paper_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Robust log-based anomaly detection on unstable log data",
            "rating": 1,
            "sanitized_title": "robust_logbased_anomaly_detection_on_unstable_log_data"
        },
        {
            "paper_title": "Multi-source anomaly detection in distributed it systems",
            "rating": 1,
            "sanitized_title": "multisource_anomaly_detection_in_distributed_it_systems"
        }
    ],
    "cost": 0.01168825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models</p>
<p>Harold Ott 
Distributed and Operating Systems
TU Berlin
BerlinGermany</p>
<p>Jasmin Bogatinovski jasmin.bogatinovski@tu-berlin.de 
Distributed and Operating Systems
TU Berlin
BerlinGermany</p>
<p>Alexander Acker alexander.acker@tu-berlin.de 
Distributed and Operating Systems
TU Berlin
BerlinGermany</p>
<p>Sasho Nedelkoski nedelkoski@tu-berlin.de 
Distributed and Operating Systems
TU Berlin
BerlinGermany</p>
<p>Odej Kao odej.kao@tu-berlin.de 
Distributed and Operating Systems
TU Berlin
BerlinGermany</p>
<p>Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models
* Equal contribution.Index Terms-anomaly detectionlog analysisdeep learninglanguage modelstransfer learning
Anomalies or failures in large computer systems, such as the cloud, have an impact on a large number of users that communicate, compute, and store information. Therefore, timely and accurate anomaly detection is necessary for reliability, security, safe operation, and mitigation of losses in these increasingly important systems. Recently, the evolution of the software industry opens up several problems that need to be tackled including (1) addressing the software evolution due software upgrades, and (2) solving the cold-start problem, where data from the system of interest is not available. In this paper, we propose a framework for anomaly detection in log data, as a major troubleshooting source of system information. To that end, we utilize pre-trained general-purpose language models to preserve the semantics of log messages and map them into log vector embeddings. The key idea is that these representations for the logs are robust and less invariant to changes in the logs, and therefore, result in a better generalization of the anomaly detection models. We perform several experiments on a cloud dataset evaluating different language models for obtaining numerical log representations such as BERT, GPT-2, and XL. The robustness is evaluated by gradually altering log messages, to simulate a change in semantics. Our results show that the proposed approach achieves high performance and robustness, which opens up possibilities for future research in this direction.</p>
<p>I. INTRODUCTION</p>
<p>Modern computer systems such as cloud platforms are a combination of complex multi-layered software and hardware. The complexity implies high maintenance overhead for the operators of these systems, making the manual operation cumbersome. In extreme cases, where system anomalies or failures happen, it can lead to SLA violations. Large service providers are aware of such cases and make the automation operation and maintenance tasks a priority.</p>
<p>Recently, a plethora of methods were introduced to automate and provide scalable AI-driven solutions to perform a range of operational tasks including anomaly detection and failure analysis [1]- [3]. In the foundation of these methods are the system data. Although there are various data sources describing system behaviour, system logs are an omnipresent data source [3], [4]. They are one of the most used data sources for troubleshooting.</p>
<p>The evolution of the software industry opens up several problems that need to be tackled. The detection of the abnormal behaviour of the system is one of them. When considering the anomaly detection models from log data, two of the most important challenges are (1) addressing the software evolution due software upgrades, and (2) solving the cold-start problem [5]. In both cases, anomaly detection models have to be dynamically optimized and adapted to the new setting. Exposing the underlying properties of the log messages in a system-agnostic manner (e.g. semantics, length, etc.) arises as an important requirement from the anomaly detection methods utilizing system logs.</p>
<p>On the contrary, many of the existing approaches are based on the invariant assumption, i.e. log templates never change. Furthermore, they rely on the assumption capturing all possible variations of log messages. Approaches, such as matching certain keywords (e.g. "error"), constructing a black-list of log events or anomalous matching regular expressions, are infertile under the circumstances of constant system's evolution. They usually lead to many unnecessary alarms, a problem known as alarm fatigue.</p>
<p>To mitigate the drawbacks of the invariant assumption, we propose an anomaly detection framework capable of preserving the shared properties between the log messages. More specifically, we utilized transfer learning and deep language modeling to learn a robust, context-aware representation of the log messages. Whenever a new log line is introduced, the framework assigns numerical-vector representation to it utilizing prior information from all the previously presented log messages. As such it is effective in reducing the coldstart problem the anomaly detection model is facing after an update. Through time, the framework reuses the accumulated knowledge for the log messages to improve the performance and the underlying representation. In a nutshell, the framework provides a mechanism to transfer knowledge from previous log messages and automatically detect anomalies in logs affected by pre-processing noise and changes of log events by updates of the underlying software.</p>
<p>The contributions of this work are summarized as follows.</p>
<p>1) A general framework for learning context and semanticaware numerical log vector representations suitable for anomaly detection. 2) Comparison of three semantic-level general-purpose language embedding models for anomaly detection. 3) Comparison of two learning objectives for anomaly detection utilizing general language models. 4) Robust model transfer approach for reduction of the false positive rate after software update. 5) We provide a publicly available implementation of the method and the datasets. 1 The remaining of the paper is structured as follows. In section II, we provide the related work for anomaly detection in log data. In sections III, we present the preliminary the proposed framework. Section IV summarizes the evaluation of the different language models, learning objective as well as model transfer. Section V concludes the paper.</p>
<p>II. RELATED WORK</p>
<p>As a major data type for the system behaviour, the literature recognizes sustainable utilization of the log data for anomaly detection in both the industry and academia [3]- [7]. The work on anomaly detection from log data follows two general directions: supervised and unsupervised methods. In this work, our focus is on unsupervised learning approaches.</p>
<p>The unsupervised approaches have greater practical relevance, because labeling of log messages is an expensive procedure. There is a number of approaches that have been developed using the log event count within a certain window to transform log messages to numerical representations. Xu et al. [6] proposed using the Principal Component Analysis (PCA) method on such vectors. It follows a standard machine learning techniques of investigation of the second norm of the lower principle components to decide if the log is normal or an anomaly.</p>
<p>There are several works for log anomaly detection that utilize deep learning approaches. For example, Zhang et al. [8] used LSTM to predict subsequent log events based on a window of preceding events. The ability to correctly predicting the next event is used to determine anomalous events. DeepLog [4] is utilizing a similar method. It is claimed that robustness to novel events is achieved by a synonym/antonym database that is used to generate auxiliary samples. Vijayakumar et al. [9] trained a stacked-LSTM to model the operation log samples of normal and anomalous events. However, the input to the unsupervised methods is a one-hot vector of logs representing the indices of the log templates. Therefore, it cannot cope with newly appearing log events.</p>
<p>Several studies [5] have leveraged NLP techniques to analyze log data based on the idea that log is a natural language sequence. Those works are utilizing word embeddings which are later averaged in order to represent the full log message. Non-learnable aggregation is a heuristic that often does not hold when going from words to sentences [10]. Different from all the above methods, we utilize state-of-the-art language models for obtaining numerical representations for log messages. It enables using end-to-end trainable vector representations that can be used in various recurrent networks e.g. Bi-LSTM [11] for anomaly detection. The log representations are robust to semantic-invariant changes of the log messages, providing good generalization.</p>
<p>III. ROBUST LOG ANOMALY DETECTION</p>
<p>The architecture of the framework is presented in Figure 2. It is composed of two phases, offline training phase and online anomaly detection.</p>
<p>The training phase is composed of the following steps. First, the raw log messages from the system are preprocessed. This includes a transformation of the log into a template and variable part (e.g., VM Creation took 8 seconds; template: "VM Creation took * seconds", variables: [8]). Each of the templates is then transformed into a numerical vector using language models such as BERT, GPT, and XL [12]- [14]. Utilizing the pre-trained embeddings from these general-purpose models aims to capture the semantic properties the log messages, important for generalization over different data [5]. In the second step, we chain the embedding vectors through time in a recurrent neural network (Bi-LSTM [11]) that learns the normal system behaviour. It is then utilized for anomaly detection by detecting deviations from the expected system behaviour. It enables robust detection of sequential anomalies. Important to note is that the neural network is trained on pre-trained numerical representations, therefore, it largely facilitates the transfer of the learned model to new log data that can appear due to software upgrades or due cold-start problems.</p>
<p>In the prediction phase, the log messages are transformed into log vectors via the same preprocessing steps as in training. Then, the sequences of such log vectors are provided as input to the anomaly detection model. The prediction from the sequential model is utilized to decide if the input sequence is normal or not. In the following, we describe each of the parts in detail.</p>
<p>A. Log preprocessing and log vectorization</p>
<p>The raw log messages generated by the systems are noisy with semi-structured form. To structure them and to obtain the information from the logs needed for the anomaly detection model, they require to be parsed [3]. Log parsing provides a mapping function of the raw log messages into log templates e.g log instruction in the source code. In this work, we adopt Drain [15], due to its speed and efficiency.</p>
<p>Next, the log templates are transformed into numerical vectors. Formally, we write a log vector (embedding) as w i ∈ R d , where d is the size of the vector embedding. The goal of the log vectorization is to preserve important properties of log messages and distinct normal against anomaly log messages.  To better illustrate the importance of the log vectors, in Figure 1, we provide a visual comparison between normal and anomalous log messages when standard one-hot encoding is utilized against vector embeddings obtained from pre-trained methods. Improvements in the log vectorization translate to improvements in the robustness and generalization of the anomaly detection models.</p>
<p>To that end, we formalize two properties that a log vector embedding should poses. (1) Distinguishable: the log vectors should represent semantic differences between the log messages. For example, VM Create finished and VM Fatal error are templates with different semantics, even though they share the same words (instance) and synonyms (terminating, deleting). (2) Tolerance: the embeddings should represent the similarity between different templates with the same or very similar semantics. For example, VM Create finished is semantically very similar to VM Create completed.</p>
<p>To preserve both properties, we refer to the natural language models, where these properties are one of the major parts of research. Exploiting general-purpose language models, which are pre-trained on large corpora of texts (e.g., Wikipedia) enables preserving of general textual structures. We focus on utilizing sentence-level embeddings, in contrary to word level embeddings. Sentence level embedding provide direct and efficient mapping from log message to log vector, without any intermediate steps (e.g. averaging of word vectors).</p>
<p>B. Bi-LSTM for Sequential Anomaly Detection</p>
<p>Once obtained, the log vectors are grouped (by timestamp) into sequences of size δ (window size), i.e., the sequences are formed of consecutive log vector embeddings, w i : w i+δ . We define two learning tasks which are utilized to learn the normal sequences of log messages: (1)Prediction of the log template as a class (classification, via minimization of the cross-entropy loss), and (2) prediction of the log vector (regression, via minimization of the mean squared error), of the log message that appears at the next position in the sequence W i+δ+1 , given the w i : w i+δ sequence as input. Figure 3 depicts the overview of the Bi-LSTM model used to optimize the objectives [11]. The input data is passed to the forward and backward layers of the Bi-LSTM. We selected this model for learning the sequences as it offers a two-sided view and improved properties for sequence learning, in comparison to the single LSTM networks.</p>
<p>The output of the Bi-LSTM network is an abstract numerical representation of the input sequence, which is then utilized for optimizing the objective. The subsequent two linear layers are applying a transformation to acquire the desired dimensions, i.e., d for regression and n (number of classes) for classification. Finally, an activation function f is applied to the output of the last linear layer. We use cross-entropy for classification and mean squared error for regression.</p>
<p>Anomaly Detection using Multi-Class Classification. For this learning objective, we used all available log templates as a target class (total of n). The training is performed on the assumption that the data contains an abundance of normal log messages, while in the prediction phase, the input data contains normal and anomalous log templates.</p>
<p>One major issue in this setup is that of the "close-world" classification objective requires apriori knowledge of all log templates. However, during prediction, it is expected that novel log templates will emerge. To address the absence of all templates at the prediction phase, we apply a nearest template matching procedure to mitigate this limitation.</p>
<p>In the template matching procedure, we calculate the dis-  tance between the embedding of the novel template and all of the known target embeddings. The novel template is assigned the class target that has the smallest distances to the known target templates. To prevent matching on arbitrary novelties, a parameter maximal distance is introduced. When the minimal distance to the template to the set of known templates greater then some the maximal distance, the novel template is discarded and anomaly label is directly assigned. The matching process is applied on w i i+δ+1 in order to obtain t i+δ+1 .</p>
<p>After the matching process, the model predicts a probability distribution P r[t i+δ+1 |w i:i+δ ] = (p(t)|∀t ∈ T). It described the probability of a template t ∈ T to occur as a successor of templates w i:i+δ . Due to the noise in the sequential appearing of the templates, we consider the top-k (out of |T |) templates with the highest probabilities to appear as relevant as the next template. If the actual template class t i+δ+1 is within the top− k predictions with the highest probability, we consider is as normal. Otherwise, it is labeled as an anomaly.</p>
<p>Anomaly Detection using Log Vector Regression. For the regression learning objective, the neural network is trained to minimize the mean squared error (MSE). The input of the network is a sequence of vector embeddings for the templates, while the corresponding target value for the sequence is the vector embedding of the next template. Compared to classification, the log vector embeddings for regression are always obtainable.</p>
<p>After the model is trained, the parameters for the anomaly detection models are calculated. The regression anomaly detection module has as a parameter the q − th percentile of the squared error for the training samples. The mean squared error of every target for each training sequence template at position i + s + 1 and the neural network's predicted template vector, is computed. Afterwards, the q-th percentile of the agglomerated loss values of the training dataset is computed. To detect anomaly, when novel sample from test dataset is introduced, the squared error between the predicted template and the vector embedding of the nearest matched template. The system will then mark every log event whose embedding loss value is above the calculated q-th percentile as an anomaly and normal, otherwise.</p>
<p>C. Model Transfer</p>
<p>Utilizing pre-trained general-purpose language models for extracting log representations and training the Bi-LSTM model allows the transfer of the model to new unseen logs. The model transfer is achieved in the following way.</p>
<p>Let dataset A be the training dataset from already known log messages, and dataset B be a dataset from an updated or new service or system. After the preprocessing, the model is trained on the dataset A. Then, the following steps are executed. First, every log event of dataset B is mapped to the nearest neighbour of dataset A, i.e. the embedding with the shortest cosine distance. In the case of classification, it gets assigned the same class target. Second, a few-shot training on dataset B will be executed. Finally, with the adjusted model on training dataset B, the prediction phase on a test dataset B is executed as previously described for the classification and regression learning objectives.</p>
<p>The initial training on dataset A preserves semantic and contextual information from previous log messages. The fewshow training on dataset B allows the model to adapt to the specifics of the dataset B and improve the results on anomaly detection.</p>
<p>IV. EVALUATION</p>
<p>To demonstrate the usefulness of our framework for anomaly detection and transferability of the models from different software deployments we conducted two evaluation experiments. In the first experimental scenario, we investigate how effective are the representations from sentencelevel language models for anomaly detection on 1) ground truth anomalies and 2) synthetic anomalies obtained via log alteration. In the second experimental scenario, we evaluate the transferability of the models during software updates.</p>
<p>A. Log Datasets</p>
<p>For our experiments, we utilize the CloudLab OpenStack log dataset available at the Loghub [4]. It is composed of two sets of experiments. During the first set of experiments, the Openstack instances were created and their runtime was monitored. The second set of experiments is similar to the former, however, occasionally anomalies were injected. The first dataset, we refer to as a normal dataset while the second one as anomalous dataset. Furthermore, to evaluate the framework, we additionally manipulated the normal dataset and created two additional test sets described in the following.</p>
<p>Log alteration. To evaluate the feasibility of sentence-level based embeddings for anomaly detection in log data we augmented our data with a synthetic dataset. We refer to this data as log alteration data. We identified two points of alteration in the log messages; semantic and contextual alteration. The alterations are applied to normal data. Therefore, the overall anomaly detection model should be robust against these alterations. Classifying suchlike altered log messages as anomalies are considered as false alarms. For both, the semantic and structural changes we identified 3 types of alteration, namely: deletion, swap and imputation.</p>
<p>For the semantic changes, we assume a log event to contains n tokens originating from the normal data. Deletion operation involves deleting of l randomly selected words in the log message. Swap operation involves, replacing l tokens with a random token. Imputation operation involves imputing l words at a random position of original log event. The parameter l controls the intensity of the alternation. It is expected that log events with higher alternation intensity have a higher probability to be detected as anomalies compared to events that were altered with lower intensity.</p>
<p>For the structural changes, we assume a log sequence to contains m log templates originating from the normal data. Deletion operation involves deleting of l random log events from the sequence. Swap operation involves, replacing the k templates appearing after randomly selected index i, to a randomly chosen index j. For the indices the inequality j &lt; i holds. Imputation operation for sequences involves selecting index at position i and repeating it l times consecutively. The parameter l controls the number of imputations. It is expected that log events with higher alternation intensity have a higher probability to be detected as anomalies compared to events that were altered with lower intensity. Augmentation to Simulate a Different Dataset. Since the software is often updated and thus changed constantly by developers, log statements are also subject to change. To simulate the evolution of the system, we construed an artificial dataset that simulates changed log messages. We constructed two datasets, we refer to as dataset A and B, in the following manner. We start with the normal data we refer to dataset A. Firstly, we randomly sample p% of the logs in A. Secondly, the sampled log lines are altered using the three semantic alteration techniques with additional word augmentation. The alteration parameters are set to random values in the range 5-100 % of the range of allowed values for altering parameters. This allows simulating different dataset. We refer to this altered dataset as a dataset B. Finally, we create two versions of the dataset B. If the alteration is not severe (e.g. 20% of the log messages is changed) the dataset is referred to as Bsimilar, otherwise, the dataset is referred to as B-different. The datasets A and B are used for transferring the contextual and semantic accumulated knowledge in the following way. The model is trained on this dataset A for e epochs (60 in our study). Then part of the dataset B is used to conduct few-shot training. The final evaluation is done on the task of anomaly detection in the second part of dataset B.</p>
<p>B. Semantic-level language embedding evaluation</p>
<p>This section presents the evaluation of the results. We first evaluate the sentence-level embeddings capability of the different language models for anomaly detection independent on the learning task. Namely, we compare BERT, XL-Transformers and GPT-2 on the regression-based approach and the classification-based approach for anomaly detection. Afterwards, the results of the evaluation using the model transfer learning approach are presented.</p>
<p>1) Regression-based anomaly detection.: TABLE I enlists the results from the comparison of the three language models on the task of anomaly detection. We divided the experiments into two subsets according to the type of alteration. Semantic alteration is related to the semantic changes of the log messages, while the sequential alteration is related to sequential alteration, described previously. For the semantically alerted log messages, GPT-2 yields better results compared to BERT and XL-Transformers with regards to all metrics. For the sequential altering of the log messages, there is a small drop of F1-score and precision for the GPT-2 embeddings. However, the same metrics increase for BERT and XL embedding. The results from both scenarios imply that GPT-2 and BERT embeddings are more robust when either the semantic or sequential changes are considered.</p>
<p>2) Classification : When considering the classification task we conducted the two separate results as in the case of regression. For semantically alerted log messages the scores are reversed. More specifically, BERT is showing the best results, followed by XL and GPT-2. The same pattern appears when considering the sequential learning scenario. General comparison of the scores between the regression and classification tasks shows that GPT-2 embeddings are highly affected by the optimization objective. The definition of the problem as a classification task is favourable when considering structural and sequential changes.</p>
<p>C. Model Transfer Evaluation</p>
<p>For the evaluation of model transfer we conducted two experiments, for both the regression-and classification learning objectives on the task of anomaly detection. The results are listed in TABLE II. For the regression learning objective when considering the large alteration, it can be seen that the both GPT-2 and BERT are performing well. However, when considering the small alteration, although BERT embeddings still retain high score, the GPT-2 tends to produce weaker results. On contrary, while being good performing method on the task of similar log messages, XL-Transformers fails when the changes of the log messages are not drastic.</p>
<p>On the classification learning objective, when considering both large and small alterations, the model utilizing BERT tends to outperform the remaining two. Comparing the XL-Transformers and GPT-2, it can be observed that the former outperforms the latter. Comparing the results alongside the learning objectives, it can be observed that the classification problem definition, slightly outperforms the definition of the problem as a regression task.</p>
<p>D. Discussion</p>
<p>The good results from both the classification and regression learning objectives show that the framework is useful for anomaly detection in setting where the data is evolving through time. When evaluating the different forms of alteration of the log messages and sequences of log messages BERT, as a general-purpose language model on sentence-level embeddings, shows to perform more consistently and robustly across the two learning objectives. It is followed by XL-Transformers and GPT-2 accordingly. GPT-2 shows strong results in experiment type for regression learning objective, but not as competitive for classification learning objective. Similar observations can be made for model transfer in settings where there are both small and large changes in the log messages.</p>
<p>Comparison of the different learning objectives shows that the definition of the learning task as a classification problem can produce better results compared to it defined as a regression problem. This is an interesting result from this study.</p>
<p>The plug-and-play strategy allows for testing different language models. As seen by the results, the different language models can highly influence the quality of the results for anomaly detection, with different word embeddings having strengths and weaknesses in different categories. Improving the NLP language models via increasing the number of parameters e.g. [16] will result in even better performance.</p>
<p>V. CONCLUSION</p>
<p>This paper addresses the problem of log anomaly detection in large computer systems. We addressed the generalization problem for anomaly detection on unseen logs by introducing a plug-and-play framework that utilizes pre-trained language models for obtaining numerical, semantically aware embeddings for log events. Bi-LSTM neural network is used as a method for exploiting contextual properties of log messages in the task of anomaly detection. Empirically, we show that the proposed approach is robust to alteration in the log messages -scenarios frequently occurring in practice due to software updates and deploying new services or systems. The results show that the framework achieves high performance using state-of-the-art sentence-level language models. Furthermore, we show that not every representation is equally useful for anomaly detection. Some of the language models fail to generate log representations that can be separated by a learned decision boundary. The underlying learning objective is also very important to obtain good results in the task of anomaly detection. The proposed approach opens new potential for anomaly detection not just from log data, but from other sources that have the notion of a distributed representation of an event e.g., distributed tracing data. We believe that the method will motivate further research in the direction of development of pre-trained language models on log data. This would enhance the log representations, and thus, improve the performance of the anomaly detection methods.</p>
<p>Fig. 1 .
1Log vector representation using invariant embeddings (left) and semantically-aware embeddings (right).</p>
<p>Fig. 3 .
3Unfolded Bi-LSTM model used for anomaly detection of the embedding sequence.</p>
<p>Fig. 2. Overview of the framework utilizing sentence level pre-trained language models.Training data </p>
<p>Raw log messages 
... 
i. Took 8 seconds to build 
instance 
... </p>
<p>Templates ( ) 
... 
i. Took * seconds to build 
instance 
... </p>
<p>Log parsing </p>
<p>Language 
Model </p>
<p>... 
i. [0.573, -0.623, ..., -0.249] 
... </p>
<p>Log vectorization </p>
<p>Template -&gt; embedding table </p>
<p>New log data (prediction/test) </p>
<p>Nearest template matching for test logs </p>
<p>Training data </p>
<p>0.132 
0.142 
... 
... </p>
<p>-0.297 
-0.262 
Next template prediction 
(classification) </p>
<p>Next embedding 
prediction 
(regression) </p>
<p>Bi-LSTM Model </p>
<p>Prediction for anomaly </p>
<p>Test data </p>
<p>Input: </p>
<p>Output: </p>
<p>TABLE I COMPARISON
IOF THE SENTANCE-LEVEL LANGUAGE EMBEDDING MODELS FOR THE TASK OF ANOMALY DETECTION.learning objective 
type of 
experiment </p>
<p>Precision score 
Recall score 
F1 score 
GPT-2 
XL 
BERT GPT-2 
XL 
BERT 
GPT-2 XL 
BERT </p>
<p>regression </p>
<p>semantic 
0.88 
0.21 
0.43 
1.00 
0.63 1.00 
0.94 
0.31 
0.56 
sequential 
0.79 
0.32 
0.49 
1.00 
0.61 1.00 
0.87 
0.42 
0.66 </p>
<p>classification </p>
<p>semantic 
0.24 
0.26 
0.37 
0.70 
1.00 1.00 
0.36 
0.41 
0.54 
sequantial 
0.31 
0.36 
0.5 
0.70 
1.00 1.00 
0.43 
0.53 
0.67 </p>
<p>TABLE II EVALUATION
IIRESULTS FOR THE MODEL TRANSFER AFTER SOFTWARE UPDATES. PERCENTAGE OF ALTERED LOG MESSAGES IS P=15%.learning objective 
type of 
experiment </p>
<p>Precision score 
Recall score 
F1 score 
GPT-2 
XL 
BERT GPT-2 
XL 
BERT 
GPT-2 XL 
BERT </p>
<p>regression 
B-similar 
0.23 
0.45 
0.58 
0.05 
0.7 
0.7 
0.08 
0.55 
0.63 
B-different 
0.94 
0.18 
0.52 
1.00 
0.47 1.00 
0.97 
0.26 
0.68 </p>
<p>classification 
B-similar 
0.27 
0.53 
0.61 
1.00 
1.00 1.00 
0.43 
0.69 
0.75 
B-different 
0.09 
0.23 
0.68 
1.00 
1.00 1.00 
0.17 
0.38 
0.81 </p>
<p>https://github.com/haraldott/anomaly detection main</p>
<p>Multi-source anomaly detection in distributed it systems. J Bogatinovski, S Nedelkoski, arXiv:2101.04977arXiv preprintJ. Bogatinovski and S. Nedelkoski, "Multi-source anomaly detection in distributed it systems," arXiv preprint arXiv:2101.04977, 2021.</p>
<p>Selfattentive classification-based anomaly detection in unstructured logs. S Nedelkoski, J Bogatinovski, A Acker, J Cardoso, O Kao, S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, "Self- attentive classification-based anomaly detection in unstructured logs," 2020.</p>
<p>Tools and benchmarks for automated log parsing. J Zhu, S He, J Liu, P He, Q Xie, Z Zheng, M R Lyu, 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEEJ. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, "Tools and benchmarks for automated log parsing," in 2019 IEEE/ACM 41st In- ternational Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 2019, pp. 121-130.</p>
<p>Deeplog: Anomaly detection and diagnosis from system logs through deep learning. M Du, F Li, G Zheng, V Srikumar, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityACMM. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning," in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communica- tions Security. ACM, 2017, pp. 1285-1298.</p>
<p>Robust log-based anomaly detection on unstable log data. X Zhang, Y Xu, Q Lin, B Qiao, H Zhang, Y Dang, C Xie, X Yang, Q Cheng, Z Li, Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringX. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li et al., "Robust log-based anomaly detection on unstable log data," in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2019, pp. 807-817.</p>
<p>Detecting large-scale system problems by mining console logs. W Xu, L Huang, A Fox, D Patterson, M I Jordan, Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles. the ACM SIGOPS 22nd symposium on Operating systems principlesW. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, "Detecting large-scale system problems by mining console logs," in Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, 2009, pp. 117-132.</p>
<p>Self-supervised log parsing. S Nedelkoski, J Bogatinovski, A Acker, J Cardoso, O Kao, 2020 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. SpringerS. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, "Self-supervised log parsing," in 2020 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD). Springer, 2020.</p>
<p>Automated it system failure prediction: A deep learning approach. K Zhang, J Xu, M R Min, G Jiang, K Pelechrinis, H Zhang, 2016 IEEE International Conference on Big Data (Big Data). K. Zhang, J. Xu, M. R. Min, G. Jiang, K. Pelechrinis, and H. Zhang, "Automated it system failure prediction: A deep learning approach," 2016 IEEE International Conference on Big Data (Big Data), pp. 1291- 1300, 2016.</p>
<p>Long short-term memory based operation log anomaly detection. R Vinayakumar, K P Soman, P Poornachandran, 2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI). R. Vinayakumar, K. P. Soman, and P. Poornachandran, "Long short-term memory based operation log anomaly detection," 2017 International Conference on Advances in Computing, Communications and Informat- ics (ICACCI), pp. 236-242, 2017.</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their composi- tionality," in Advances in neural information processing systems, 2013, pp. 3111-3119.</p>
<p>Bidirectional lstm-crf models for sequence tagging. Z Huang, W Xu, K Yu, arXiv:1508.01991arXiv preprintZ. Huang, W. Xu, and K. Yu, "Bidirectional lstm-crf models for sequence tagging," arXiv preprint arXiv:1508.01991, 2015.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, "Language models are unsupervised multitask learners."</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in neural information processing systems. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," in Advances in neural information processing systems, 2019, pp. 5753-5763.</p>
<p>Drain: An online log parsing approach with fixed depth tree. P He, J Zhu, Z Zheng, M R Lyu, 2017 IEEE International Conference on Web Services (ICWS). IEEEP. He, J. Zhu, Z. Zheng, and M. R. Lyu, "Drain: An online log parsing approach with fixed depth tree," in 2017 IEEE International Conference on Web Services (ICWS). IEEE, 2017, pp. 33-40.</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A , arXiv:2005.14165arXiv preprintT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
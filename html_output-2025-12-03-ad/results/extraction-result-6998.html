<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6998 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6998</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6998</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-275906943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.14427v2.pdf" target="_blank">GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better</a></p>
                <p><strong>Paper Abstract:</strong> The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text. However, unlike graph data, natural language inherently has sequential order. We observe a counter-intuitive fact that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing. Additionally, due to LLMs' limited input context length, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning. To address these gaps, we introduce GraphSOS (Graph Sampling and Order Selection). This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning. Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning. Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphSOS improves LLMs' performance and generalization ability on graph tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6998.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6998.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature-Edge List</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature List + Edge List natural language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text serialization that converts node attributes into a Feature List (text descriptions per node) and graph topology into an Edge List of node-pair (or pair/triple) tuples; used as the primary input g(G) for LLM-as-predictor experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Feature List + Edge List</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes' textual attributes (for TAG datasets) are listed as a Feature List [Node 0: <text>, Node 1: <text>, ...] and edges are listed as an Edge List of pairs or triples [(0,1), (1,2), ...] (or triples for KG facts). The two lists are concatenated into a single natural-language sequence and fed to the LLM along with the question.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (topology preserved if all edges included)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>element-wise serialization: emit Feature List (node descriptions) then Edge List (pairs/triples); ordering of elements in each list can be permuted (random or selected orders).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Cora, Citeseer, Pubmed, Texas, Wisconsin, Cornell (TAG); MetaQA and other graph-QA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-Attributed Graph (TAG) node classification; graph question-answering (graph QA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3-8B, Qwen 2.5-7B (backbones used with this representation); also evaluated with GraphGPT, GraphWiz as baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM backbones fine-tuned or instruction-tuned in the study: LLaMA 3-8B and Qwen 2.5-7B; also closed-source baselines (GraphGPT, GraphWiz) reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream task accuracy (classification/QA accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraphSOS (using Feature+Edge List with SSM+OSM + GraphCoT) achieved, e.g., node classification: 77.0% (Citeseer), 70.5% (Cora), 76.5% (Texas) with LLaMA-3; graph-QA: 89.6% on MetaQA (LLaMA-3 GraphSOS-2stage-OSM) (values reported in Tables 2-3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a direct, easy-to-provide textual input for LLMs; however, training effectiveness strongly depends on the sequence order and subgraph selection. GraphSOS components (SSM and OSM) that act on this representation improved downstream accuracy and reduced order-induced performance variance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Order-sensitive: different permutations of the same Feature/Edge lists cause large performance fluctuations (from high accuracy to near-random); can produce very long token sequences (context-length limits); random neighbor sampling (without SSM) can mix heterophilic neighbors and hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to random-order serialization, using an Order Selector Module (OSM) to pick better permutations reduces performance variance and improves accuracy. Compared to naive random neighbor sampling, Subgraph Sampling Module (SSM) that scores and biases sampling toward homophilic subgraphs improved node-classification performance, especially on heterophilic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6998.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6998.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Triple List</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Triple List / Edge Triple serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text format that lists knowledge-graph triples as textual tuples (subject, predicate, object) as a sequence of triples; used for graph QA tasks where node attributes may be absent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Knowledge Triple List</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each KG fact is converted to a textual triple list such as [(Lore, release_year, 2012), (ActorA, acted_in, MovieB), ...]. Triples (or edges) are concatenated into a single sequence which is provided to the LLM along with the question; number of triples per question can be large (dozens to ~1,200 in MetaQA retrievals).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (fact-list), lossless for provided triples</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge/triple listing centered on target entity: retrieve 1-hop and 2-hop triples and serialize as a triple list; order of triples can be permuted.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MetaQA (movie knowledge graph), graph reasoning QA datasets (9 tasks used in Chen et al. 2024 dataset referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph question-answering (graph QA), path / reasoning QA</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3-8B and Qwen 2.5-7B evaluated in GraphSOS; closed-source baselines (GPT variants) also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM backbones as above; GraphSOS applies SSM (to reduce irrelevant triples) and OSM (to choose order) before LLM processing; Graph CoT used to distill chain-of-thought style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on QA tasks (percentage of correct answers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraphSOS (LLaMA-3 GraphSOS-2stage-OSM) achieved 89.6% ±1.0 on MetaQA (Table 3); increasing order-candidate m improved accuracy but increased inference time (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>When combined with SSM (to limit and improve the retrieved triple set) and OSM (to select ordering) and with GraphCoT distillation, LLMs produced substantially higher QA accuracy and more stable zero-shot performance compared to naive triple-list inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Large triple lists can exceed model context windows (per-question triples ranged up to ~1,198), requiring sampling or truncation; ordering sensitivity remains a problem without OSM; naive sampling can include many irrelevant or heterophilic triples harming reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to unfiltered triple lists or random-order triple lists, applying Subgraph Sampling (to focus on relevant triples) and Order Selection (to stabilize sequence ordering) improved performance and generalization; GraphSOS outperforms several baselines on MetaQA when using this representation with its modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6998.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6998.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Talk-Like-A-Graph variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TALK-LIKE-A-GRAPH (GPT-Adjacency / GPT-Incident / GPT-Expert variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior graph-to-text encoding strategies (from Fatemi et al.) which verbalize graphs in different formats (adjacency, incidence, or expert/formatted views) for LLMs; used in this paper as baseline graph-to-text encodings evaluated with closed-source GPT-3.5-turbo-16k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency / Incident / Expert textual encodings (TALK-LIKE-A-GRAPH variants)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Reported variants that convert graph structure into text via different textual encodings: adjacency-based descriptions (verbalizing adjacency relationships), incidence-based lists (incident-edge listings per node), and expert-formatted encodings (task-specific handcrafted textual formats). The paper treats these as baseline encoding strategies for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (format-dependent), potentially lossy depending on condensation choices</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Adjacency listing (node -> neighbor lists), incidence listing (edge-centric descriptions), or expert-curated textual formats; exact encoding specifics are in the referenced work (Fatemi et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Evaluated by this paper as baselines on TAG node classification datasets and graph QA datasets (values reported in Tables 2 and 3).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TAG node classification; graph QA (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-16k used in the original TALK-LIKE-A-GRAPH variants (closed-source) and evaluated here as zero-shot baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-3.5-turbo-16k used by the TALK-LIKE-A-GRAPH variants; in this paper they serve as baseline encodings that are not fine-tuned by the authors due to closed-source constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (node classification and QA accuracy) reported in paper tables</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from this paper: on MetaQA, GPT-Adjacency: 86.9% ±1.1, GPT-Incident: 86.8% ±1.1, GPT-Expert: 86.9% ±1.9 (Table 3); on some TAG node-classification datasets these variants produced low or variable accuracy (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>These encodings can be used zero-shot with large closed LLMs; they provide baseline performance without fine-tuning but do not benefit from the GraphSOS training pipeline in this paper (closed-source models were not fine-tuned here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance varies by encoding choice; in some TAG tasks they reported poor accuracy (sensitivity to encoding design); closed-source usage limited experimentation and fine-tuning in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>GraphSOS (with Feature+Edge List plus SSM and OSM and GraphCoT tuning) outperformed these baseline TALK-LIKE-A-GRAPH variants on many supervised and zero-shot tasks in the paper, especially when ordering and subgraph quality were optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6998.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6998.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural-language node/edge serialization (Ren et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language node-and-edge serialization paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonly used paradigm cited by the authors where graphs are converted into natural-language descriptions listing nodes and edges; cited as mainstream prior practice for feeding graphs into LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A survey of large language models for graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language node/edge serialization (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generic conversion of graph elements to plain-language descriptions (e.g., 'Node A: <attributes>'; 'Edge: (A, B) is_friend_of'), producing a single textual prompt for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>List node descriptions and list edges as textual tuples; order usually arbitrary unless canonicalized explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General LLM-as-predictor graph tasks (surveyed prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a simple interface to feed graphs to LLMs, but (as discussed in this paper) it is prone to order-sensitivity and context-length problems unless additional mechanisms (order selection, sampling) are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Order sensitivity (LLMs can perform very differently for different equivalent serializations), context-window limitations, and potential for sampling irrelevant neighbors when subgraphs are truncated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>The paper contrasts this generic paradigm with its GraphSOS approach (which adds learned subgraph sampling and order selection) and shows that these additions reduce variance and improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Graphwiz: An instruction-following language model for graph computational problems <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graphs <em>(Rating: 2)</em></li>
                <li>A survey of large language models for graphs <em>(Rating: 2)</em></li>
                <li>Can large language models understand graph structured data ? an empirical evaluation and benchmarking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6998",
    "paper_id": "paper-275906943",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Feature-Edge List",
            "name_full": "Feature List + Edge List natural language serialization",
            "brief_description": "A simple graph-to-text serialization that converts node attributes into a Feature List (text descriptions per node) and graph topology into an Edge List of node-pair (or pair/triple) tuples; used as the primary input g(G) for LLM-as-predictor experiments in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Feature List + Edge List",
            "representation_description": "Nodes' textual attributes (for TAG datasets) are listed as a Feature List [Node 0: &lt;text&gt;, Node 1: &lt;text&gt;, ...] and edges are listed as an Edge List of pairs or triples [(0,1), (1,2), ...] (or triples for KG facts). The two lists are concatenated into a single natural-language sequence and fed to the LLM along with the question.",
            "representation_type": "sequential, token-based (topology preserved if all edges included)",
            "encoding_method": "element-wise serialization: emit Feature List (node descriptions) then Edge List (pairs/triples); ordering of elements in each list can be permuted (random or selected orders).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Cora, Citeseer, Pubmed, Texas, Wisconsin, Cornell (TAG); MetaQA and other graph-QA tasks",
            "task_name": "Text-Attributed Graph (TAG) node classification; graph question-answering (graph QA)",
            "model_name": "LLaMA 3-8B, Qwen 2.5-7B (backbones used with this representation); also evaluated with GraphGPT, GraphWiz as baselines",
            "model_description": "Open-source LLM backbones fine-tuned or instruction-tuned in the study: LLaMA 3-8B and Qwen 2.5-7B; also closed-source baselines (GraphGPT, GraphWiz) reported for comparison.",
            "performance_metric": "Downstream task accuracy (classification/QA accuracy)",
            "performance_value": "GraphSOS (using Feature+Edge List with SSM+OSM + GraphCoT) achieved, e.g., node classification: 77.0% (Citeseer), 70.5% (Cora), 76.5% (Texas) with LLaMA-3; graph-QA: 89.6% on MetaQA (LLaMA-3 GraphSOS-2stage-OSM) (values reported in Tables 2-3).",
            "impact_on_training": "Provides a direct, easy-to-provide textual input for LLMs; however, training effectiveness strongly depends on the sequence order and subgraph selection. GraphSOS components (SSM and OSM) that act on this representation improved downstream accuracy and reduced order-induced performance variance.",
            "limitations": "Order-sensitive: different permutations of the same Feature/Edge lists cause large performance fluctuations (from high accuracy to near-random); can produce very long token sequences (context-length limits); random neighbor sampling (without SSM) can mix heterophilic neighbors and hurt performance.",
            "comparison_with_other": "Compared to random-order serialization, using an Order Selector Module (OSM) to pick better permutations reduces performance variance and improves accuracy. Compared to naive random neighbor sampling, Subgraph Sampling Module (SSM) that scores and biases sampling toward homophilic subgraphs improved node-classification performance, especially on heterophilic graphs.",
            "uuid": "e6998.0",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Knowledge Triple List",
            "name_full": "Knowledge Triple List / Edge Triple serialization",
            "brief_description": "A graph-to-text format that lists knowledge-graph triples as textual tuples (subject, predicate, object) as a sequence of triples; used for graph QA tasks where node attributes may be absent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Knowledge Triple List",
            "representation_description": "Each KG fact is converted to a textual triple list such as [(Lore, release_year, 2012), (ActorA, acted_in, MovieB), ...]. Triples (or edges) are concatenated into a single sequence which is provided to the LLM along with the question; number of triples per question can be large (dozens to ~1,200 in MetaQA retrievals).",
            "representation_type": "sequential, token-based (fact-list), lossless for provided triples",
            "encoding_method": "edge/triple listing centered on target entity: retrieve 1-hop and 2-hop triples and serialize as a triple list; order of triples can be permuted.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "MetaQA (movie knowledge graph), graph reasoning QA datasets (9 tasks used in Chen et al. 2024 dataset referenced)",
            "task_name": "Graph question-answering (graph QA), path / reasoning QA",
            "model_name": "LLaMA 3-8B and Qwen 2.5-7B evaluated in GraphSOS; closed-source baselines (GPT variants) also evaluated",
            "model_description": "Same LLM backbones as above; GraphSOS applies SSM (to reduce irrelevant triples) and OSM (to choose order) before LLM processing; Graph CoT used to distill chain-of-thought style reasoning.",
            "performance_metric": "Accuracy on QA tasks (percentage of correct answers)",
            "performance_value": "GraphSOS (LLaMA-3 GraphSOS-2stage-OSM) achieved 89.6% ±1.0 on MetaQA (Table 3); increasing order-candidate m improved accuracy but increased inference time (Table 7).",
            "impact_on_training": "When combined with SSM (to limit and improve the retrieved triple set) and OSM (to select ordering) and with GraphCoT distillation, LLMs produced substantially higher QA accuracy and more stable zero-shot performance compared to naive triple-list inputs.",
            "limitations": "Large triple lists can exceed model context windows (per-question triples ranged up to ~1,198), requiring sampling or truncation; ordering sensitivity remains a problem without OSM; naive sampling can include many irrelevant or heterophilic triples harming reasoning.",
            "comparison_with_other": "Compared to unfiltered triple lists or random-order triple lists, applying Subgraph Sampling (to focus on relevant triples) and Order Selection (to stabilize sequence ordering) improved performance and generalization; GraphSOS outperforms several baselines on MetaQA when using this representation with its modules.",
            "uuid": "e6998.1",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Talk-Like-A-Graph variants",
            "name_full": "TALK-LIKE-A-GRAPH (GPT-Adjacency / GPT-Incident / GPT-Expert variants)",
            "brief_description": "Prior graph-to-text encoding strategies (from Fatemi et al.) which verbalize graphs in different formats (adjacency, incidence, or expert/formatted views) for LLMs; used in this paper as baseline graph-to-text encodings evaluated with closed-source GPT-3.5-turbo-16k.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "use",
            "representation_name": "Adjacency / Incident / Expert textual encodings (TALK-LIKE-A-GRAPH variants)",
            "representation_description": "Reported variants that convert graph structure into text via different textual encodings: adjacency-based descriptions (verbalizing adjacency relationships), incidence-based lists (incident-edge listings per node), and expert-formatted encodings (task-specific handcrafted textual formats). The paper treats these as baseline encoding strategies for LLMs.",
            "representation_type": "sequential, token-based (format-dependent), potentially lossy depending on condensation choices",
            "encoding_method": "Adjacency listing (node -&gt; neighbor lists), incidence listing (edge-centric descriptions), or expert-curated textual formats; exact encoding specifics are in the referenced work (Fatemi et al.).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Evaluated by this paper as baselines on TAG node classification datasets and graph QA datasets (values reported in Tables 2 and 3).",
            "task_name": "TAG node classification; graph QA (baseline comparisons)",
            "model_name": "GPT-3.5-turbo-16k used in the original TALK-LIKE-A-GRAPH variants (closed-source) and evaluated here as zero-shot baselines",
            "model_description": "Closed-source GPT-3.5-turbo-16k used by the TALK-LIKE-A-GRAPH variants; in this paper they serve as baseline encodings that are not fine-tuned by the authors due to closed-source constraints.",
            "performance_metric": "Accuracy (node classification and QA accuracy) reported in paper tables",
            "performance_value": "Examples from this paper: on MetaQA, GPT-Adjacency: 86.9% ±1.1, GPT-Incident: 86.8% ±1.1, GPT-Expert: 86.9% ±1.9 (Table 3); on some TAG node-classification datasets these variants produced low or variable accuracy (see Table 2).",
            "impact_on_training": "These encodings can be used zero-shot with large closed LLMs; they provide baseline performance without fine-tuning but do not benefit from the GraphSOS training pipeline in this paper (closed-source models were not fine-tuned here).",
            "limitations": "Performance varies by encoding choice; in some TAG tasks they reported poor accuracy (sensitivity to encoding design); closed-source usage limited experimentation and fine-tuning in this study.",
            "comparison_with_other": "GraphSOS (with Feature+Edge List plus SSM and OSM and GraphCoT tuning) outperformed these baseline TALK-LIKE-A-GRAPH variants on many supervised and zero-shot tasks in the paper, especially when ordering and subgraph quality were optimized.",
            "uuid": "e6998.2",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Natural-language node/edge serialization (Ren et al.)",
            "name_full": "Natural-language node-and-edge serialization paradigm",
            "brief_description": "A commonly used paradigm cited by the authors where graphs are converted into natural-language descriptions listing nodes and edges; cited as mainstream prior practice for feeding graphs into LLMs.",
            "citation_title": "A survey of large language models for graphs",
            "mention_or_use": "mention",
            "representation_name": "Natural-language node/edge serialization (generic)",
            "representation_description": "Generic conversion of graph elements to plain-language descriptions (e.g., 'Node A: &lt;attributes&gt;'; 'Edge: (A, B) is_friend_of'), producing a single textual prompt for LLM input.",
            "representation_type": "sequential, token-based",
            "encoding_method": "List node descriptions and list edges as textual tuples; order usually arbitrary unless canonicalized explicitly.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "",
            "task_name": "General LLM-as-predictor graph tasks (surveyed prior works)",
            "model_name": "",
            "model_description": "",
            "performance_metric": "",
            "performance_value": null,
            "impact_on_training": "Provides a simple interface to feed graphs to LLMs, but (as discussed in this paper) it is prone to order-sensitivity and context-length problems unless additional mechanisms (order selection, sampling) are applied.",
            "limitations": "Order sensitivity (LLMs can perform very differently for different equivalent serializations), context-window limitations, and potential for sampling irrelevant neighbors when subgraphs are truncated.",
            "comparison_with_other": "The paper contrasts this generic paradigm with its GraphSOS approach (which adds learned subgraph sampling and order selection) and shows that these additions reduce variance and improve accuracy.",
            "uuid": "e6998.3",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Graphwiz: An instruction-following language model for graph computational problems",
            "rating": 2,
            "sanitized_title": "graphwiz_an_instructionfollowing_language_model_for_graph_computational_problems"
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "A survey of large language models for graphs",
            "rating": 2,
            "sanitized_title": "a_survey_of_large_language_models_for_graphs"
        },
        {
            "paper_title": "Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "rating": 1,
            "sanitized_title": "can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        }
    ],
    "cost": 0.015064749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better
12 Feb 2025</p>
<p>Xu Chu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#117;&#120;&#117;&#64;&#115;&#116;&#117;&#46;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#99;&#104;&#117;&#120;&#117;&#64;&#115;&#116;&#117;&#46;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Hanlin Xue 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Zhijie Tan 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Bingce Wang 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Tong Mo 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Weiping Li 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Robbin Cindy 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better
12 Feb 202511E69D96EC518F508DE047F0C99986A3arXiv:2501.14427v3[cs.LG]Exp 1: Nodes: [Robin: A boyCindy: A girl]Edges: [(Robinis_friend_ofCindy)(Cindyis_friend_ofRobin)] Attribute: A boy Attribute: A boy
The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text.However, unlike graph data, natural language inherently has sequential order.We observe a counter-intuitive fact that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing.Additionally, due to LLMs' limited input context length, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning.To address these gaps, we introduce Graph-SOS (Graph Sampling and Order Selection).This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning.Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning.Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphSOS improves LLMs' performance and generalization ability on graph tasks.</p>
<p>Introduction</p>
<p>The recent success of Large Language Models (LLMs) (Touvron et al., 2023;Bai et al., 2023) motivates researchers to explore their potential in handling tasks across various modalities, including vision, speech, and tabular data (Li et al., 2023;Fang et al., 2024;Xia et al., 2024), as well as graph data.As a non-Euclidean geometric structure, graphs are indispensable in representing and solving numerous applications, including social network analysis (Kumar et al., 2022;Liu et al., 2024), recommendation systems (Fan et al., 2019;Luo et al., 2024), and spatiotemporal prediction (Pareja et al., 2020;Zhu et al., 2023).Many Graph LLM studiess (Fatemi et al., 2023;Guo et al., 2023;Tang et al., 2024;Chen et al., 2024) focus on converting graph data into natural language text and inputting it along with questions into closed-source LLMs or LLMs fine-tuned with graph tasks.LLMs complete graph tasks based on their inherent knowledge and reasoning capabilities, such as node classification (Tang et al., 2024) and graph questionanswering (Chen et al., 2024).</p>
<p>Despite promising results, we identify two concerning issues and raise questions accordingly.Question I: Do LLMs truly understand and process graph topological structures correctly?In graph learning using LLM as predictor (Chen et al., 2023), the mainstream paradigm serializes graphs using natural language descriptions of nodes and edges (Ren et al., 2024), as shown in Figure 1.Since natural language sequences are one-dimensional, the same graph can have multiple equivalent descriptive orders.However, research on both LLMs and Multimodal Large Language Models (MLLMs) indicates that LLMs/MLLMs have better prompt orders (though no universal optimal order exists for models or tasks) (Lu et al., 2022;Tan et al., 2024).Models perform well when the ordering is correct, while other orders lead to near-random performance, which may affect model performance on graph tasks.We conduct node classification tasks on the text-attributed graph dataset Cora (Chen et al., 2023) and graph question-answering tasks on the movie knowledge graph dataset MetaQA (Zhang et al.,  2018).We test LLMs of different architectures and scales (Qwen 2.5 (Team, 2024), LLaMA 3 (Dubey et al., 2024)) and two advanced Graph LLMs (GraphGPT (Tang et al., 2024), GraphWiz (Chen et al., 2024)) under zero-shot learning settings (Kojima et al., 2022).For each dataset, we randomly permute the order of nodes and edges in the serialized graphs and conduct 10 independent experiments to analyze performance variations across different orderings, leading to some counter-intuitive findings as shown in Figure 2. Furthermore, another question for Graph LLM is Question II: Does context based on randomly sampled partial neighborhoods limit model effectiveness?The current mainstream paradigm of LLM as predictor obtains subgraph representations by randomly sampling n neighbors centered on target nodes and feeding them to LLMs (Ren et al., 2024), to accommodate LLMs' limited context length.Although studies on RAG make efforts on graph data compression (He et al., 2024;Hu et al., 2024), they focus on retrieving external knowledge and enhancing LLM.Different from these studies, to our best knowledge, no work discusses how to determine optimal or relatively better input subgraphs from the provided graph rather than relying on random sampling.Random sampling may sample graph structures that are detrimental to graph learning, such as heterophily (Zhu et al., 2020;Pei et al., 2020).This occurs because sampling nodes of different categories than the target node from its neighbors can lead to incorrect mixing of node features (Zhu et al., 2020), making nodes indistinguishable and resulting in incorrect answers.</p>
<p>To address these issues, we propose a novel framework called GraphSOS (Graph Sampling and Order Selection).</p>
<p>For Question I, GraphSOS introduces an Order Selector Module that selects better sequence order for serialized graphs, which is then fed into the LLM along with the question.Order Selector Module ensures that LLMs receive relatively better-ordered inputs for any graph, thus maintaining performance.For Question II, we introduce a Subgraph Sampling Module before the graph enters the Order Selector Module, which samples subgraphs of target nodes from the graph.We train the random walk process of the Subgraph Sampling Module using concepts from reinforcement learning and preference learning (Brown et al., 2020;Rafailov et al., 2024) to sample better subgraphs.Additionally, to ensure the model follows instructions and derives answers through analysis and logical reasoning of input graphs, we propose Graph Chain of Thought (Graph CoT) obtained through distillation.We use Graph CoT to enhance LLM's reasoning and zero-shot capabilities for graph tasks through instruction tuning.Our contributions can be summarized as:</p>
<p>• We identify current Graph LLMs are sensitive to graph serialization order, and random subgraph sampling can mix node features from different categories incorrectly, affecting model performance.</p>
<p>• We propose GraphSOS, a novel framework that improves LLM graph processing via two key components: a Subgraph Sampling Module for optimal subgraph extraction and an Order Selector Module for better graph serialization order.</p>
<p>• We introduce Graph CoT obtained through distillation and employ instruction tuning to teach models to reason correctly about graph structures.</p>
<p>• We evaluate our model on node classification and graph question-answering tasks and analyze the impact of its components.We demonstrate GraphSOS's superior performance in supervised and zero-shot graph learning settings.</p>
<p>Preliminaries</p>
<p>LLMs for graph data tasks can categorized into "LLM as enhancer" and "LLM as predictor" based on the role of LLM (Chen et al., 2023).This paper focuses on LLM as predictor, which leverages LLM's reasoning capabilities to solve graph tasks, including graph Question-Answering (graph QA) and node classification on Text- Attributed Graph (TAG).A graph can formally represented as G(V, E, X), where V and E represent the sets of nodes and edges respectively.X denotes the feature matrix, where each row vector represents a node's attributes or feature information.For TAG node classification, each node corresponds to a text attribute in X.For graph QA, G may not include meaningful X, such as in shortest path problems.</p>
<p>LLM processes graph inputs in sequence form, thus requiring a graph encoding function to convert the graph into a sequence suitable for language models (Fatemi et al., 2023).The process of LLM obtaining answers can be represented as:
A = f (g(G), Q),(1)
where f (•) formalizes the process of LLM obtaining answers from inputs, Q represents the user's question, A represents LLM's answer, and g(•) denotes the graph encoding function that converts graph G into a sequence, including serialization and possible subgraph sampling processes.</p>
<p>As shown in Table 1, for TAG node classification tasks, we define g(G) as the process of converting the node set V and node feature matrix X into a Feature List and converting the edge set E into an Edge List represented as pairs.For graph QA tasks, we convert the edge set E into pairs or triples based on specific task requirements.Since the graph QA tasks we study do not include node attributes, g(G) for graph QA tasks does not contain a Feature List.</p>
<p>Our objectives are twofold.First, we optimize g(•) to enable LLM to obtain better-sampled subgraphs of target node v in graph G and select relatively better serialization order representations for these subgraphs (in Sections 3.1 and 3.2).Secondly, we train and tune the LLM parameters to enhance the model's graph understanding and reasoning capabilities, to optimize f (•) (in Section 3.3).</p>
<p>Methodology</p>
<p>In this section, we detail the proposed GraphSOS framework, with the overall architecture shown in Figure 3. GraphSOS inputs both graph and question and generates natural language answers as output.The Subgraph Sampling Module takes graph G as input and aims to extract a subgraph of target node v from G. The Order Selector  Module takes the subgraph and question as input, aiming to generate a natural language description of the subgraph and determine the sequence order of elements from both the Feature List and Edge List within the description.Finally, the instruction-tuned LLM generates answers in the Graph CoT answer format.</p>
<p>Subgraph Sampling Module</p>
<p>In the Introduction, we address Question II: Does context based on randomly sampled partial neighborhoods limit model effectiveness?To improve this limitation, we design the Subgraph Sampling Module (SSM) to construct a subgraph G v for target node v from graph G, rather than relying purely on random sampling.As shown in Figure 4, SSM obtains text attributes (i.e., features) of target node v and its k-hop (k = 2) neighbors, using a pre-trained encoder (PTE) to generate encoded features for each node.In our implementation, we use Bert (Devlin et al., 2019) as the PTE, utilizing the [CLS] token embedding from Bert's last layer output as the representation for each node's features.</p>
<p>Next, we aim to use the [CLS] token embedding of target node v as a query to compute its correlation with the [CLS] token embeddings of each neighbor node, guiding random walks to retain highly correlated neighbors in the sampled subgraph G v .We introduce scaled dot-product attention to compute correlation weights between v and its neighbors, which is defined as (Vaswani et al., 2017):
Attention(Q, K, V ) = softmax( QK T √ d k )V,(2)
where d k is the embedding dimension, Q, K, and V represent the embedding matrices for query, key, and value respectively.Here, Q is the [CLS] token embedding vector of target node v, K = V is the matrix composed of [CLS] token embeddings of v's neighbors, and QK T √ d k represents the correlation vector between query vector Q and all vectors in key matrix K, defined as attention weights.We then introduce multi-head attention based on dot-product attention.The [CLS] token embeddings of the target node v and its neighbors are linearly mapped into h (h=4) splits for multi-head cross-attention computation.In the (multihead) cross-attention process, target node v's embedding is mapped to queries, while neighbor nodes' embeddings are mapped to keys.Attention weights are computed based on query and key embeddings.Let µ be the split embeddings, the multi-head cross-attention process can be expressed as:
o attn = 1 h h i=1 head i (µ v i , (µ u1 i , µ u2 i , . . . , µ un i )),(3)
where head i represents attention weights from the i-th attention head, h is the number of attention heads, µ v i is the i-th embedding component of target node v's split embedding, µ un i is the i-th embedding component of neighbor node u n 's split embedding, and n is the total number of v's k-hop neighbors.In our experiments, we use 4 attention heads (h=4) for multi-head cross-attention computation.The attention weights from multi-head attention guide random walks to retain highly correlated neighbors in sampled subgraph G v .Given maximum sample node count n max , the random walk sampling process can be defined as:
P (u j |v) = o attn [j], |V Gv | ≤ n max ,(4)
where o attn [j] represents the j-th value in the attention weight vector, indicating the correlation between neighbor node u j and target node v, P (u j |v) is the probability of transitioning from node v to neighbor node u j in the random walk process, V Gv represents the node set in the sampled subgraph, and |V Gv | represents its node count.The neighbor nodes sampled by random walk together with node v form the subgraph G v .</p>
<p>Scoring Model.To train SSM, we draw inspiration from reinforcement learning and preference learning (Brown et al., 2020;Rafailov et al., 2024).We first construct 500 data instances from the text-attributed graph datasets Citeseer and Cornell (see Section 4.1), where each instance contains a target node and its 2-hop neighbors, and convert them into text descriptions using g(G) as defined in Table 1.Then, we construct positive and negative subgraph examples for each target node data point and train a Scoring Model using crossentropy loss to score subgraphs.Based on task requirements, since we mainly focus on heterophily (Zhu et al., 2020;Pei et al., 2020), positive examples are constructed as subgraphs with strong homophily, while negative examples are constructed as subgraphs with strong heterophily.We use Qwen 2.5-0.5B(Team, 2024) as the Scoring Model, requiring it to output 1 for positive examples and 0 for negative examples.</p>
<p>To obtain continuous scores from the Scoring Model, we compute the softmax values of the probabilities that Qwen 2.5-0.5Bpredicts 0 or 1 for the first token, using the probability of predicting 1 from the softmax values as the model's score.This process can be described as:
P (y = 1|g(G)) = e hsc(g(G)
)1,y=1 e hsc(g(G))1,y=0 + e hsc(g(G))1,y=1 , (5) where h sc represents the output logits from the Scoring Model's last layer, y ∈ {0, 1} indicates the preference label, (•) 1,y=i subscript represents taking the probability value of the first new token corresponding to digit i, and P (y = 1|g(G)) is the score given by the Scoring Model.We use this score to update SSM gradients, defining SSM's loss function as:
L SSM = 1 T (1 − P (y = 1|g(G))) 2 , (6)
where T is the temperature coefficient that adjusts the loss magnitude, and we set T = 5 for smoother loss.It is worth noting that we use a scoring model to train the SSM, rather than directly using an LLM to replace SSM, because LLMs have limited input window sizes, while random walks in SSM have unlimited input windows.Moreover, constructing positive and negative example subgraphs for all target nodes for training would be computationally intensive.Through a Scoring Model trained on limited data, we can score any subgraph processed by SSM, greatly reducing training and data annotation costs.Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>Order Selector Module</p>
<p>In the Introduction, we also address Question I: Do LLMs truly understand and process graph topological structures correctly?We demonstrate that both LLMs and Graph LLMs are sensitive to the order of elements in Feature List and Edge List, which deviates from ideal Graph LLMs, as a graph LLM should maintain high performance regardless of the sequence order in which its nodes and edges are described.To address this gap, we design the Order Selector Module (OSM).This module generates serialized representations of subgraphs produced by SSM and arranges the elements in Feature List and Edge List in optimized orders according to the user's questions.OSM ensures that for any input subgraph, its serialized representation is order-optimal or relatively optimal for model responses.</p>
<p>As shown in Figure 5, OSM receives a subgraph and serializes it into a natural language sequence consisting of Feature List and Edge List.Then, it generates m order representations for this sequence, each being a random permutation of elements in the Feature List and Edge List.When m can enumerate all possible permutations, OSM theoretically selects the optimal order, however, this would result in factorial computational complexity, which is unacceptable in terms of time cost.Therefore, in our experiments, we select a subset of all possible orders, setting m = 10, allowing OSM to produce a fixed number of order permutations and obtain the relatively optimal order among them.</p>
<p>Next, the m sequences and the user's question are input to PTE for encoding.We also use Bert (Devlin et al., 2019) as the encoder, utilizing the [CLS] token embedding from its last layer as the representation for both the question and each ordered sequence.The cross-attention design is almost identical to SSM's, except that query Q becomes the question's embedding vector, and key K becomes the matrix composed of embeddings from m ordered sequences.Cross-attention outputs attention weights, where each element represents the correlation weight between that ordered sequence and the question.We apply Gumbel Softmax (Jang et al., 2016) to the attention weights to obtain a (one-hot) mask of length m, which selects a single optimal order from the m ordered sequences as output.Gumbel Softmax is used to ensure model differentiability.</p>
<p>Training OSM.We train OSM and the LLM in Figure 2 as an end-to-end system, updating OSM parameters using the loss between the language model's output and target answers.Specifically, before training OSM, we first train the LLM through two-stage tuning (in Section 3.3).Then, when training OSM, we freeze the LLM parameters and focus on updating OSM parameters.The loss function is constructed as:
L OSM (π θ ) = − N i=1 log π θ (y i |x i ),(7)
where π θ (y i |x i ) is the conditional probability of LLM generating target output y i given input x i , and N is the number of training samples.During training, LLM parameters are frozen, and only OSM parameters are updated.</p>
<p>Graph Chain-of-Thought Distillation</p>
<p>Traditional graph learning, such as GNN, typically completes graph tasks in two steps (Li et al., 2015;Kipf &amp; Welling, 2016):</p>
<p>Step 1: Aggregate and update node features, Step 2: Make predictions based on aggregated features.However, although many studies input graphs along with questions to LLMs, LLMs often skip Step 1, ignoring graph structure and directly predicting answers, for example in node classification tasks (Tang et al., 2024) and graph question-answering tasks (Chen et al., 2024).This indicates that LLMs often complete graph tasks following incorrect graph reasoning steps.</p>
<p>Inspired by Chain-of-Thought (CoT) research in language models (Wei et al., 2022) (Guo et al., 2023;Fatemi et al., 2023).We aim to distill the capabilities of large closed-source models into our small-parameter model through knowledge distillation.Following general steps for distilling knowledge from LLMs (Tang et al., 2024;Chen et al., 2024), we use GPT-4o to construct answers for the training dataset and construct prompts that make GPT-4o first analyze graph structure, then generate answers in CoT format, with examples shown in Figure 6.</p>
<p>Two-Stage Tuning for LLMs.To enable language models to follow human instructions, instruction tuning is commonly used (Ouyang et al., 2022b).However, to further enable models to think and generate responses using Graph CoT, we define a two-stage tuning approach.Stage 1: Instruction tuning.Stage 2: Direct Preference Optimization (DPO) (Rafailov et al., 2024).In the instruction tuning stage, the LLM is trained directly using the Question and SFT Answer format as shown in Figure 6.We use LoRA (Hu et al., 2022) to improve training efficiency.In the DPO phase, preference data is constructed using Graph CoT answers as winning responses and SFT answers as losing responses, encouraging the LLM to generate responses based on Graph CoT.Loss functions for the two stages are defined as:
L SF T (π θ ) = − N i=1 log π θ (y i |x i ),(8)L DP O (π θ ) = −E (x,yw,y l )∼D log σ[β log( π θ (y w |x) π ref (y w |x) ) −β log( π θ (y l |x) π ref (y l |x) )]
(9) where π θ is LLM parameters, π θ (y i |x i ) is the conditional probability of generating target output y i given input x i , π ref represents the reference policy, which is the LLM parameter state after the first stage instruction tuning and before the second stage DPO training.(x, y w , y l ) is a triplet consisting of input question and its corresponding winning and losing answers, β (β = 1) is the temperature coefficient, σ is the sigmoid function, and D is the training dataset.</p>
<p>Experiments</p>
<p>In this section, we validate the proposed GraphSOS by addressing the following questions: RQ1: How does Graph-SOS perform in supervised and zero-shot learning settings?RQ2: What are the contributions of each key component in GraphSOS to the overall performance?RQ3: How do the hyperparameters affect model performance?The detailed analysis of RQ3 is provided in Appendix C.3.</p>
<p>Experimental Setup</p>
<p>Dataset.We focus on Text-Attributed Graph (TAG) node classification and graph Question-Answering (graph QA) tasks.For TAG node classification, we use three homophily citation TAG datasets provided by Chen et al. (Chen et al., 2023): Cora, Citeseer, and Pubmed, following their training and test set splits.Additionally, following Pei et al. (Pei et al., 2020), we construct three high-heterophily text graph datasets: Texas, Wisconsin, and Cornell, from WebKB.In these datasets, nodes represent webpages, and edges represent hyperlinks between them.Node features are webpage descriptions.Webpages are categorized into four classes: student, course, staff, and faculty, with training, validation, and test sets split in a 1:1:8 ratio.</p>
<p>For graph QA tasks, MetaQA (Zhang et al., 2018) is a movie knowledge graph QA dataset.We select 1,000 samples from the provided 2-hop test set (avoiding training set to prevent data contamination) and split them into training, validation, and test sets in a 1:1:8 ratio.For each question, we retrieve 1-hop and 2-hop triples centered around the target entity from the knowledge graph, with the number of triples per question ranging from 50 to 1,198.Furthermore, we use the graph reasoning QA dataset provided by Chen et al. (Chen et al., 2024), which includes 9 tasks with a total of 18.1k training samples and 400 test samples per task.</p>
<p>Baseline Methods.In our performance comparison, we consider various advanced methods for comprehensive evaluation: (i) The first category includes Graph Neural Networks (GNNs): We use two-layer GNNs, including GCN (Kipf &amp; Welling, 2016), GAT (Veličković et al., 2018), Graph-SAGE (Hamilton et al., 2017), NodeFormer (Wu et al., 2023), etc.As well as GNNs with knowledge distillation: GKD (Yang et al., 2022), GLNN (Zhang et al., 2022), and AdaGMLP (Lu et al., 2024).More baselines and results can be seen in Appendix C.1.For TAG node classification tasks, node text attributes are encoded using BERT (Devlin et al., 2019), and the [CLS] token embedding is used as the node feature vector.For graph QA tasks, we sample one-dimensional features from a normal distribution as node features.(ii) The second category showcases state-of-the-art multi-task capable Graph LLMs, including GraphGPT-7B (Tang et al., 2024), GraphWiz-LLaMa2-7B (Chen et al., 2024), and three variants of TALK-LIKE-A-GRAPH (Fatemi et al., 2023) (GPT-Adjacency, GPT-Incident, and GPT-Expert), which utilize the closed-source GPT-3.5-turbo-16k(Ouyang et al., 2022a) and thus do not participate in any training in our experiments.(iii) The third category consists of simple fine-tuned open-source LLMs, including Qwen 2.5-7B (Team, 2024) and LLaMA 3-8B (Dubey et al., 2024).These models are fine-tuned using the SFT answer format shown in Figure 6 for comparison with our approach.</p>
<p>Implementation Details.We construct GraphSOS using LLaMA 3-8B and Qwen 2.5-7B as backbone models.The experimental configuration details are in Appendix B. We evaluate the models on TAG node classification and graph QA tasks to assess their performance in both supervised and zero-shot settings.The overall performance is shown in Table 2 and Table 3  Graph QA.In Table 3, GNNs are supervised trained except for MetaQA, topology, and hamilton, as GNNs do not support path reasoning.Datasets labeled as SFT indicate supervised fine-tuning using their respective training sets.Specifically, as the most challenging tasks in graph reasoning QA (Chen et al., 2024), hamilton and subgraph use zero-shot settings for all LLM-based methods except Graph-Wiz to test zero-shot learning capabilities, while GraphWiz, designed and trained specifically for graph reasoning QA, is fine-tuned on training sets of all graph QA datasets for comparison.Experimental results show that in SFT settings, GraphSOS leads performance on almost all datasets compared to all baselines while maintaining stable performance.Additionally, "2stage" models typically perform better than "1stage" indicating Graph CoT benefits graph reasoning tasks.In zero-shot learning settings (hamilton and subgraph), despite no supervised training, GraphSOS achieves performance close to or even surpassing supervised-trained GraphWiz, demonstrating GraphSOS's ability to understand and reason about graphs learned from other tasks.</p>
<p>Module Ablation Study (RQ2)</p>
<p>We conduct ablation studies to explore the individual contributions of different components in our proposed GraphSOS.Using LLaMA 3-8B as the base model, results are shown in Table 4.</p>
<p>Effect of Subgraph Sampling.We study the benefits of Subgraph Sampling Module using the "w/o SSM" variant.</p>
<p>In this variant, we directly randomly sample neighbors of target nodes for node classification across three datasets, without using SSM for neighbor sampling.Results in Table 4 show that GraphSOS with SSM outperforms its variant using random sampling.This indicates SSM's ability to select task-relevant subgraphs, especially for heterophily graphs (e.g., Texas), where SSM can reduce the selection of dissimilar neighbors to target nodes.As shown in Figure 9 in Appendix C.2, compared to the "w/o SSM" variant, SSM samples a notably higher proportion of same-class neighbors.Research in graph learning suggests this benefits node classification tasks (McPherson et al., 2001;Battaglia et al., 2018).</p>
<p>Effect of Order Selection.We study the benefits of Order Selector Module in selecting element order in Feature List and Edge List of serialized graphs using the "w/o OSM" variant.In this variant, we randomly arrange elements in Feature List and Edge List and conduct experiments on node classification and graph QA tasks.The results in Effect of Graph CoT.We study the benefits of using DPO to teach models to generate Graph CoT answers during two-stage tuning using the "w/o Graph CoT" variant.In this variant, we use instruction tuning with SFT answers from Figure 6 and conduct experiments on node classification and graph QA tasks.Results in Table 4 show that GraphSOS with two-stage tuning demonstrates consistent performance advantages over single-stage instruction tuning models.Notably, this performance advantage is also evident in zero-shot learning settings on the subgraph dataset, indicating Graph CoT helps models transfer reasoning abilities learned from other tasks to specific tasks, demonstrating zero-shot reasoning and generalization capabilities.</p>
<p>Conclusion</p>
<p>This paper introduces GraphSOS to address design deficiencies in Graph LLMs.We focus on two major issues: order sensitivity to serialized graphs and random subgraph sampling as input.We propose the Order Selector Module and Subgraph Sampling Module as solutions.We also introduce Graph CoT to enhance LLMs' graph reasoning.GraphSOS shows improved performance on graph tasks in both supervised and zero-shot learning settings.</p>
<p>Impact Statement</p>
<p>This work advances graph learning capabilities of Large Language Models through improved sampling, ordering, and reasoning mechanisms.While our proposed Graph Chain of Thought (Graph CoT) approach enhances model interpretability and performance by making reasoning steps explicit, we acknowledge several important considerations regarding its societal and ethical implications.</p>
<p>The primary ethical consideration stems from the inherent limitations of base LLMs that GraphSOS builds upon.Despite the structured reasoning paths, LLMs may still exhibit hallucinations or generate incorrect logical steps, which could propagate through the graph analysis process.This is particularly concerning in high-stakes applications like social network analysis or recommendation systems, where faulty reasoning could lead to biased or harmful decisions.</p>
<p>To mitigate these risks, we recommend:</p>
<ol>
<li>
<p>Using more robust base models with demonstrated reliability.</p>
</li>
<li>
<p>Creating diverse and carefully curated Graph CoT training data that emphasizes accurate reasoning.</p>
</li>
<li>
<p>Implementing validation mechanisms to verify the logical consistency of generated reasoning chains.</p>
</li>
<li>
<p>Maintaining human oversight in critical applications.</p>
</li>
</ol>
<p>Additionally, while Graph CoT improves transparency, it should not be considered a complete solution for model interpretability.The reasoning chains, while human-readable, may still reflect underlying biases present in the training data or model architecture.</p>
<p>We believe the benefits of enhanced graph understanding and explicit reasoning outweigh these risks when proper precautions are taken.Our work makes LLMs for graph tasks more reliable and interpretable, crucial for their responsible deployment in real-world applications.</p>
<p>A. Analyzing Order Sensitivity in LLMs and LLMs for Graph Tasks</p>
<p>Many studies find that LLMs are highly sensitive to prompt order in zero-shot and few-shot settings (Jiang et al., 2019;Lu et al., 2022;Tan et al., 2024).As shown in Figure 7, taking a sorting problem as an example, Qwen2.5-7B-Instruct (Team, 2024) produces different results for the same question when presented in two different orders.Counter to intuition, one order leads to the correct answer while the other leads to an incorrect answer.Experiments by Lu et al. (Lu et al., 2022) further demonstrate that there is no universally optimal order across different LLMs or tasks.</p>
<p>Similar to the order sensitivity studies on LLMs, we find that Graph LLMs are also order-sensitive.As shown in Figure 8, taking node classification on the Cora dataset as an example, when we describe the same citation graph using two different natural language orders (i.e., changing the order of elements in Node features and Edge list), GraphGPT (Tang et al., 2024), which is trained on large-scale Text-Attributed Graph data, produces different results for the two orders.One order leads to the correct answer while the other leads to an incorrect answer.This indicates that there exist relatively better natural language description orders for graphs that enable LLMs or Graph LLMs to perform better on graph-related tasks.The central idea is to apply the rules to a target problem to get a first approximation to the answer; but if the problem is judged to be compellingly similar to a known exception of the rules in any aspect of its behavior, then that aspect is modelled after the exception rather than the rules.The architecture is implemented for the full-scale task of pronouncing surnames.Preliminary results suggest that the system performs almost as well as the best commercial systems.However, of more interest than the absolute performance of the system is the result that this performance was better than what could have been achieved with the rules alone.This illustrates the capacity of the architecture to improve on the rule-based system it starts with.The results also demonstrate a beneficial interaction in the system, in that improving the rules speeds up the case-based component.Paper 454.The title is Data-oriented methods for grapheme-to-phoneme conversion.The abstract of this paper: It is traditionally assumed that various sources of linguistic knowledge and their interaction should be formalised in order to be able to convert words into their phonemic representations with reasonable accuracy.We show that using supervised learning techniques, based on a corpus of transcribed words, the same and even better performance can be achieved, without explicit modeling of linguistic knowledge.In this paper we present two instances of this approach.A first model implements a variant of instance-based learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-tospeech lexicon (table lookup) enriched with default mappings.We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge based approach..A case study in evaluating a case-based system: This paper presents a case study in evaluating a case-based system.It describes the evaluation of Anapron, a system that pronounces names by a combination of rule-based and case-based reasoning.Three sets of experiments were run on Anapron: a set of exploratory measurements to profile the system's operation; a comparison between Anapron and other name-pronunciation systems; and a set of studies that modified various parts of the system to isolate the contribution of each.Lessons learned from these experiments for CBR evaluation methodology and for CBR theory are discussed.This work may not be copied or reproduced in whole or in part for any commercial purpose.Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories of Cambridge, Massachusetts; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice.Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories.All rights reserved.Paper 454.The title is Data-oriented methods for grapheme-to-phoneme conversion.The abstract of this paper: It is traditionally assumed that various sources of linguistic knowledge and their interaction should be formalised in order to be able to convert words into their phonemic representations with reasonable accuracy.We show that using supervised learning techniques, based on a corpus of transcribed words, the same and even better performance can be achieved, without explicit modeling of linguistic knowledge.In this paper we present two instances of this approach.A first model implements a variant of instancebased learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-to-speech lexicon (table lookup) enriched with default mappings.We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge based approach.. Paper 1307.The title is planning in an open-textured domain.Paper 38.The title is Improving rule-based systems through case-based reasoning.The abstract of this paper: A novel architecture is presented for combining rule-based and case-based reasoning.The central idea is to apply the rules to a target problem to get a first approximation to the answer; but if the problem is judged to be compellingly similar to a known exception of the rules in any aspect of its behavior, then that aspect is modelled after the exception rather than the rules.The architecture is implemented for the full-scale task of pronouncing surnames.Preliminary results suggest that the system performs almost as well as the best commercial systems.However, of more interest than the absolute performance of the system is the result that this performance was better than what could have been achieved with the rules alone.This illustrates the capacity of the architecture to improve on the rule-based system it starts with.The results also demonstrate a beneficial interaction in the system, in that improving the rules speeds up the case-based component.</p>
<p>C.2. Module Ablation Results</p>
<p>C.3. Parameter Sensitivity (RQ3)</p>
<p>We analyze the impact of two hyperparameters on GraphSOS: the number of sampled neighbors n max in SSM and the number of order candidates m in OSM. Figure 11 shows the performance of GraphSOS with LLaMA 3-8B as the base model under different settings of sampled neighbors n max .Results indicate that both too high and too low values of n max affect model performance.A low n max leads to limited graph structural information for LLM, restricting model reasoning; a high n max results in overly long context input to LLM, making reasoning difficult (An et al., 2024).Table 7 shows the performance of LLaMA 3-8B-based GraphSOS under different order candidate numbers m.The results indicate that the model performance improves steadily as m increases.This suggests that appropriately increasing m can enhance performance by including more candidate order samples.However, increasing the order leads to growth in inference time overhead.To balance performance and time overhead, we only choose to set m = 10.Nevertheless, we point out that increasing m brings performance improvements.</p>
<p>D. Future Work</p>
<p>In this paper, when discussing graph structures, we focus on homophily and heterophily.However, other structural properties (such as degree, connectivity, symmetry) are worth exploring.We demonstrate that with properly constructed training data, subgraphs with any graph structure that is more beneficial for tasks can be sampled by training GraphSOS's SSM, rather than only homophily and heterophily.Additionally, ensuring accurate Graph CoT steps is crucial for graph reasoning.We encourage introducing more powerful models to generate Graph CoT data or using manually constructed Graph CoT data, and emphasize the effectiveness of constructing larger training datasets to improve model performance.Moreover, the Subgraph Sampling Module constructed in this paper lacks explicit structure-aware components.We encourage new methods to incorporate structure-awareness capabilities to discover, evaluate, and preserve critical paths or paths that play important connecting roles in the topological structure.</p>
<p>Figure 1 .
1
Figure1.Converting a graph into natural language description.Elements in both node and edge lists can be arranged in any order to represent the same graph.</p>
<p>(a) Node classification accuracy of models.(b) Accuracy of models on graph QA tasks.(c) Node classification accuracy of Qwen models with different parameter counts.(d) Accuracy of Qwen models with different parameter counts on graph QA tasks.</p>
<p>Figure 2 .
2
Figure 2. Zero-shot performance of models with different orders of node and edge.</p>
<p>Figure 2(a) and Figure 2(b) show that merely changing the order of nodes and edges in the questions causes performance fluctuations across all these models (both LLMs and Graph LLMs that have undergone embedding alignment and graph task fine-tuning).Additionally, as shown in Figure 2(c) and Figure 2(d), this phenomenon persists across models of the same architecture with different parameter scales, with no significant improvement as model parameters increase.Therefore, regarding Question I, it seems that current LLMs and Graph LLMs do not understand and process graphs well, as an ideal Graph LLM should maintain high performance regardless of the ordering used to represent the same graph.More detailed analysis and examples can be seen in Appendix A.</p>
<p>[Node 2 is …, Node 1 … Node 3 … ] Edge List: [(1, 2), (3, 2), (2, 3) … ] Question: "What is the category of target node 2</p>
<p>Figure 3 .
3
Figure 3.The overall framework of GraphSOS.Trainable components are highlighted in yellow and marked with flame icons.(a) Subgraph Sampling Module: samples and outputs a subgraph of a target node from graph G. (b) Order Selector Module: takes the subgraph and user question, converts the subgraph into a text sequence and selects the sequence order.(c) LLM: generates answers based on question and serialized text representation of the graph.</p>
<p>Figure 4 .
4
Figure 4. Internal details of the Subgraph Sampling Module (SSM).Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>Figure 5 .
5
Figure 5. Internal details of the Order Selector Module (OSM).Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>, representing the proportion of edges connecting nodes of the same class, where Z u,: = Z v,: indicates nodes u and v belong to the same category.Each dataset's H edge (G) is labeled in the Edge Hom.row, with values closer to 0 indicating stronger heterophily in G.All models are supervised trained only on Citeseer and Cornell training sets, with zero-shot learning on the remaining datasets.Results show that in SFT settings, LLaMA 3-GraphSOS-2stage-SSM-OSM outperforms GNNs on Citeseer and Cornell, particularly on heterophily graph Cornell, demonstrating strong heterophily graph learning capabilities.In the zero-shot learning setting, all variants of GraphSOS demonstrate consistent performance advantages, achieving 1-5 times higher accuracy compared to other baselines.</p>
<p>Figure 7 .
7
Figure 7. Examples of Qwen2.5-7B-Instruct'sresponses to the same question with different sequential orderings.</p>
<p>Paper 1307.The title is planning in an open-textured domain.Edge list: 1-hop-edges: ['(38,1802)', '(38,90)', '(38,454)', '(38,1349)', '(38,1307)'].Based on node features and edge list above, which of the following categories does the paper 38 most likely belong to? [Rule Learning, Neural Networks, Case Based, Genetic Algorithms, Theory, Reinforcement Learning, Paper 1349.The title is Pronouncing Names by a Combination of Rule-Based and Case-Based Reasoning.Paper 1802.The title is Case-based Acquisition of User Preferences for Solution Improvement in Ill-Structured Domains,.The abstract of this paper: 1 We have developed an approach to acquire complicated user optimization criteria and use them to guide.Paper 90.The title is The evaluation of Anapron.The abstract of this paper:</p>
<p>Figure 8 .
8
Figure8.Examples of GraphGPT's responses to node classification tasks where the same graph is described in natural language using different sequential orderings.</p>
<p>Figure 9 .
9
Figure 9. Proportion of same-class neighbors in random sampling and SSM sampling.</p>
<p>Figure 10 .
10
Figure 10.Performance fluctuation comparison between random order and OSM-selected order.</p>
<p>Figure 11 .
11
Figure 11.Performance of GraphSOS with different numbers of sampled neighbors nmax.</p>
<p>Table 1 .
1
Construction of serialized text input for the graph.</p>
<p>Task g(G) TAG Node Feature List: [Node 0 ... ], Classification Edge List: [(0, 1) (0, 2) ... ] Graph QA Edge List: [(0, 1) (0, 2) ... ] Knowledge Triple List: [(Lore, release year, Graph QA 2012) ... ]</p>
<p>GraphCoT Answer: This is a node classification task.Paper 0 has a direct connection with paper 1260 and 2-hop connections to papers 3038, 2243, 818, and 2921.Paper 0 discusses "Argument in Multi-Agent Systems" … Both the target paper and its neighbors heavily focus on agent-based systems … which clearly indicates this paper belongs to the "Agents" category.### Agents.This is a node classification task.Paper 0 has a direct connection with paper 1260 and 2-hop connections to papers 3038, 2243, 818, and 2921.Paper 0 discusses "Argument in Multi-Agent Systems" … Both the target paper and its neighbors heavily focus on agent-based systems … which clearly indicates this paper belongs to the "Agents" category.### Agents.[ Paper 0: Argument in Multi-Agent Systems Multi-agent systems research … Paper 1260 … ] Edge List: [1-hop-edges: ['(0,1260)' … ], 2-hop-edges: ['(0,3038)' … ]].Based on the feature and edge list above, which of the following categories does Paper 0 most likely belong to? [Agents, AI, DB, IR, ML, HCI].Answer: Question: Feature List: [ Paper 0: Argument in Multi-Agent Systems Multi-agent systems research … Paper 1260 … ] Edge List: [1-hop-edges: ['(0,1260)' … ], 2-hop-edges: ['(0,3038)' … ]].Based on the feature and edge list above, which of the following categories does Paper 0 most likely belong to? [Agents, AI, DB, IR, ML, HCI].Answer: Figure 6.Constructing answers in Graph CoT format.Yellow highlights indicate analysis steps, while blue highlights show reasoning processes.
Graph CoT Answer: SFT Answer: Agents. SFT Answer: Agents.
Question: Feature List:</p>
<p>Table 2 .
2
Performance comparison (accuracy) on TAG node classification tasks under supervised and zero-shot settings.
DatasetCiteseerCoraPubmedCornellTexasWisconsinEdge Hom.0.780.810.800.260.250.33Training MethodSFT0-shot0-shotSFT0-shot0-shotGCN70.7±0.413.9±3.226.3±2.8 47.4±3.98.9±7.721.2±21.4GAT71.2±0.813.4±5.627.5±3.3 50.5±2.7 37.6±4.9 22.9±19.2GraphSage70.9±0.6 24.2±14.1 25.8±3.0 48.9±3.2 29.5±6.8 23.5±20.1GNNNodeFormer70.5±0.814.5±4.026.1±3.1 75.5±1.4 30.1±6.6 22.8±20.3GKD72.0±0.514.1±4.024.5±3.2 48.2±3.19.7±7.212.3±10.0GLNN73.1±0.313.8±3.721.0±2.8 51.7±3.4 19.4±7.4 21.6±19.772.8±0.414.1±3.511.5±7.9 71.2±1.3 20.1±7.2 22.0±19.5GraphWiz74.9±0.70.1±0.91.5±1.150.0±0.8 48.6±1.260.6±0.6GraphGPT53.2±1.39.1±0.570.1±1.4 49.8±0.7 52.3±0.960.0±1.1Graph LLMGPT-Adjacency17.8±0.564.2±0.720.1±0.6 77.8±0.1 72.9±0.179.1±0.2GPT-Incident18.6±0.465.4±0.320.2±0.7 78.2±0.0 73.1±0.180.2±0.0GPT-Expert18.5±0.265.9±0.320.8±0.8 78.1±0.0 73.2±0.179.9±0.11stage38.4±0.836.8±1.220.2±0.6 71.9±1.5 64.4±0.479.1±0.9Qwen 2.5GraphSOS-2stage GraphSOS-2stage-SSM64.2±1.1 65.3±0.664.2±0.7 65.4±1.470.8±1.3 75.7±0.5 75.8±1.4 72.3±0.9 77.3±1.2 76.9±0.382.1±0.8 83.5±1.0GraphSOS-2stage-SSM-OSM 69.7±0.866.3±0.673.9±1.2 80.1±0.6 78.6±0.984.9±0.51stage74.5±1.29.7±0.87.6±0.576.5±1.3 68.5±0.779.5±1.4LLaMA 3GraphSOS-2stage GraphSOS-2stage-SSM74.9±0.9 75.3±0.367.3±1.5 68.5±1.175.9±0.4 76.5±0.6 72.6±1.0 76.1±1.4 78.9±0.9 74.3±0.581.8±1.2 83.0±0.8GraphSOS-2stage-SSM-OSM 77.0±0.570.5±0.877.6±0.7 79.5±0.9 76.5±0.785.2±0.64.2. Overall Performance of GraphSOS (RQ1)</p>
<p>Table 3 .
3
Performance comparison (accuracy) on graph QA tasks under supervised and zero-shot settings.
DatasetMetaQAcycleconnectbipartitetopologyshortesttriangleflowhamiltonsubgraphTraining MethodSFTSFTSFTSFTSFTSFTSFTSFTSFT/0-shot SFT/0-shotGCN-82.5±1.3 73.0±0.8 81.3±1.1-5.3±0.77.0±1.4 10.3±0.9-62.0±1.2GAT-84.5±0.6 79.8±1.4 83.8±0.5-7.3±1.27.3±0.8 11.8±1.5-64.5±0.4GraphSAGE-83.8±1.1 78.5±1.2 82.9±0.8-7.0±0.97.5±1.1 11.2±1.2-63.8±0.8GNNNodeFormer-83.5±0.9 79.0±1.0 82.7±1.0-6.8±1.08.7±0.9 13.0±1.3-58.2±0.7GKD-84.2±0.8 79.5±1.1 83.1±0.5-7.2±0.87.2±1.2 11.6±1.0-64.2±0.9GLNN-84.7±1.0 80.8±0.7 83.9±0.4-8.2±0.37.1±0.8 11.3±1.4-63.5±1.1AdaGMLP-84.5±0.9 80.5±0.8 83.6±0.5-8.0±0.47.3±0.9 11.5±1.3-63.8±1.0GraphWiz35.3±1.9 70.0±1.1 89.8±0.3 73.3±1.4 16.3±0.7 12.8±1.0 24.0±0.6 28.3±1.339.0±0.870.3±1.5GraphGPT32.9±1.2 72.8±0.4 83.5±1.5 66.8±0.7 0.0±0.09.3±0.5 23.3±1.3 13.3±0.831.8±1.459.8±0.6Graph LLMGPT-Adjacency86.9±1.1 81.2±0.9 89.5±1.0 77.8±0.8 70.1±0.7 24.2±0.7 34.8±1.2 36.2±0.941.5±1.165.1±0.7GPT-Incident86.8±1.1 84.9±0.8 90.3±0.9 79.1±0.7 72.3±0.8 14.5±0.9 25.7±0.8 37.1±1.141.2±0.966.8±1.0GPT-Expert86.9±1.9 81.8±1.0 89.8±1.1 78.2±0.9 70.0±0.7 24.9±0.6 35.1±1.0 36.5±0.841.1±1.266.8±0.81stage40.8±3.5 79.3±1.4 88.0±0.7 73.3±1.2 0.0±0.0 10.3±1.5 24.0±0.9 27.3±0.431.8±1.159.8±0.8Qwen 2.5GraphSOS-2stage80.3±6.8 79.5±0.6 88.0±1.1 74.8±0.9 15.8±1.4 16.8±0.7 21.8±1.2 20.0±1.532.8±0.568.8±1.0GraphSOS-2stage-OSM 86.9±3.5 80.4±1.2 89.3±0.6 77.1±0.9 16.7±0.8 16.9±1.1 24.2±0.9 26.8±1.236.3±1.268.9±0.51stage46.4±3.4 83.5±0.7 90.0±1.2 78.5±0.5 0.0±0.0 14.8±0.8 35.3±1.1 25.8±0.631.8±1.362.5±0.9LLaMA 3GraphSOS-2stage83.3±1.1 89.8±1.3 92.8±0.8 79.3±1.4 17.3±0.4 24.5±1.1 37.5±0.5 38.5±1.241.0±0.769.5±1.5GraphSOS-2stage-OSM 89.6±1.0 92.7±0.7 93.4±1.2 80.2±0.9 18.8±0.8 26.3±0.5 41.2±1.3 40.7±1.042.7±0.972.9±0.8</p>
<p>Table 4 .
4
Ablation study on SSM, OSM, and Graph CoT of Graph-SOS.
CiteseerCoraTexasMetaQAcyclesubgraphw/o SSM74.9±1.2 67.5±0.8 69.6±1.4---w/o OSM75.3±0.3 68.5±1.1 74.3±0.5 83.3±1.1 89.8±1.3 69.5±1.5w/o Graph CoT 71.6±1.4 9.3±0.6 69.3±1.2 47.7±0.8 85.2±1.5 64.9±0.4GraphSOS77.0±0.5 70.5±0.8 76.5±0.7 89.6±1.0 92.7±0.7 72.9±0.8</p>
<p>Table 4
4demonstrate that GraphSOS with OSM outperforms its vari-ants, indicating that the context ordering selected by OSMhelps LLMs understand and reason about graphs, therebyensuring high performance. Additionally, as shown in Fig-ure 10 in Appendix C.2, in statistical results of 10 indepen-dent experiments with randomly permuted element orders inFeature List and Edge List for each dataset, GraphSOS withOSM shows smaller performance fluctuation, indicatingOSM's ability to suppress LLM's order sensitivity.</p>
<p>Table 7 .
7
Accuracy and inference time of baselines and GraphSOS with different numbers of order candidates m on MetaQA.
ModelsAcc(%)time(s)GraphWiz35.3±1.90.43GraphGPT32.9±1.20.51LLaMa 346.4±3.40.65GraphSoS (m =84.5±0.50.81GraphSoS (m = 10) 89.6±1.01.02GraphSoS (m = 15) 89.8±1.21.19GraphSoS (m = 20) 90.2±0.91.37
B. Experimental SetupWe set the learning rate, epoch, batch size, and maximum length for fine-tuning all models to 5e-5, 3, 8, and 1024 respectively.Each experiment is repeated 3 times, with means and standard deviations reported.All experiments use an Intel(R) Xeon(R) Silver 4316 processor as CPU and a single 80G Nvidia A100 GPU.The system memory is 256GB, with Ubuntu 22.03.3 as the operating system, CUDA version 12.4, Python version 3.10.4,and torch version 2.0.1.For Graph CoT distillation in Section 3.3, we use GPT-4o to generate Graph CoT format answers with a temperature of 0.9 and maximum output tokens of 512.We employ LoRA (Low-Rank Adaptation) for fine-tuning.The training dataset is preprocessed using 16 workers with a maximum sequence length of 1024 tokens.The LoRA hyperparameters are set as follows: rank = 8, alpha = 16, and dropout = 0, targeting all model layers.For optimization, we use the AdamW optimizer with a learning rate of 5e-5 and cosine learning rate scheduling.We employ mixed-precision training bfloat16 format.The batch size is set to 2 with a gradient accumulation of 8 steps.Gradient clipping is applied with a maximum norm of 1.0.The model checkpoints are saved every 100 steps, with loss logging occurring every 5 steps.For DPO training, we use identical infrastructure settings but configure preference learning with beta = 0.1 and sigmoid-based preference loss.The preprocessing is handled by 16 workers with a maximum sequence length of 2048 tokens.The LoRA parameters remain the same (rank = 8, targeting all layers), while the learning rate is reduced to 5e-6 with cosine scheduling and 10% warmup ratio.Training uses bfloat16 format over 3 epochs, with batch size = 1 and gradient accumulation steps = 8.Checkpoints are saved every 500 steps, and loss logging occurs every 10 steps.C. Additional ResultsC.1. Complete Experimental ResultsWe also compare with the following baselines, including ChebNet(Defferrard et al., 2016), SGC(Wu et al., 2019), and LW-GNN(Dai et al., 2022).results are shown in Table5and Table6.
Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. S Abu-El-Haija, B Perozzi, A Kapoor, International Conference on Machine Learning. 2019</p>
<p>Make your llm fully utilize the context. S An, Z Ma, Z Lin, N Zheng, J.-G Lou, ArXiv, abs/2404.168112024</p>
<p>. J Bai, S Bai, Y Chu, ArXiv, abs/2309.16609Qwen technical report. 2023</p>
<p>Relational inductive biases, deep learning, and graph networks. P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V F Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, ¸aglar Gülc ¸ehre, H F Song, A J Ballard, J Gilmer, G E Dahl, A Vaswani, K R Allen, C Nash, V Langston, C Dyer, N M O Heess, D Wierstra, P Kohli, M M Botvinick, O Vinyals, Y Li, R Pascanu, ArXiv, abs/1806.01261201846935302</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, ArXiv, abs/2005.141652020</p>
<p>Graphwiz: An instruction-following language model for graph computational problems. N Chen, Y Li, J Tang, J Li, Knowledge Discovery and Data Mining. 2024</p>
<p>Exploring the potential of large language models (llms)in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, J Tang, ACM SIGKDD Explorations Newsletter. 252023</p>
<p>Label-wise graph convolutional network for heterophilic graphs. E Dai, S Zhou, Z Guo, S Wang, 21. PMLR, 09- 12Proceedings of the First Learning on Graphs Conference. B Rieck, R Pascanu, the First Learning on Graphs ConferenceDec 2022198of Proceedings of Machine Learning Research</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems. M Defferrard, X Bresson, P Vandergheynst, 201629</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, North American Chapter. the Association for Computational Linguistics2019</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, ArXiv, abs/2407.217832024</p>
<p>Graph neural networks for social recommendation. W Fan, Y Ma, Q Li, The world wide web conference. 2019</p>
<p>Llama-omni: Seamless speech interaction with large language models. Q Fang, S Guo, Y Zhou, Z Ma, S Zhang, Y Feng, ArXiv, abs/2409.066662024</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J J Halcrow, B Perozzi, ArXiv, abs/2310.045602023</p>
<p>Can large language models understand graph structured data ? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, W Hamilton, Z Ying, J Leskovec, ArXiv, abs/2305.15066Advances in neural information processing systems. 302023. 2017Inductive representation learning on large graphs</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. X He, Y Tian, Y Sun, N V Chawla, T Laurent, Y Lecun, X Bresson, B Hooi, arXiv:2402.076302024arXiv preprint</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, International Conference on Learning Representations. 2022</p>
<p>Grag: Graph retrieval-augmented generation. Y Hu, Z Lei, Z Zhang, B Pan, C Ling, L Zhao, arXiv:2405.165062024arXiv preprint</p>
<p>Categorical reparameterization with gumbel-softmax. E Jang, S S Gu, B Poole, ArXiv, abs/1611.011442016</p>
<p>How can we know what language models know? Transactions of the. Z Jiang, F F Xu, J Araki, G Neubig, 2019Association for Computational Linguistics8</p>
<p>Semi-supervised classification with graph convolutional networks. T Kipf, M Welling, T Kojima, S S Gu, M Reid, ArXiv, abs/1609.02907Advances in neural information processing systems. 2016. 202235Large language models are zero-shot reasoners</p>
<p>Influence maximization in social networks using graph embedding and graph neural network. S Kumar, A Mallik, A Khetarpal, Information Sciences. 6072022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S C H Hoi, International Conference on Machine Learning. 2023</p>
<p>Y Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493Gated graph sequence neural networks. 2015arXiv preprint</p>
<p>Rumor detection with a novel graph neural network approach. T Liu, Q Cai, C Xu, Academic Journal of Science and Technology. 1012024</p>
<p>Adagmlp: Adaboosting gnn-to-mlp knowledge distillation. W Lu, Z Guan, W Zhao, Y Yang, 10.1145/3637528.3671699Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USA2024Association for Computing Machinery. ISBN 9798400704901</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Collaborative sequential recommendations via multi-view gnn-transformers. T Luo, Y Liu, S J Pan, ACM Transactions on Information Systems. 2024</p>
<p>Birds of a feather: Homophily in social networks. M Mcpherson, L Smith-Lovin, J M Cook, Review of Sociology. 272001</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 2022a35</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, ArXiv, abs/2203.021552022b</p>
<p>Evolvegcn: Evolving graph convolutional networks for dynamic graphs. A Pareja, G Domeniconi, J Chen, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Geom-gcn: Geometric graph convolutional networks. H Pei, B Wei, K C Chang, .-C Lei, Y Yang, B , ArXiv, abs/2002.052872020</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, Advances in Neural Information Processing Systems. 362024</p>
<p>A survey of large language models for graphs. X Ren, J Tang, D Yin, N V Chawla, C Huang, Knowledge Discovery and Data Mining. 2024</p>
<p>Order matters: Exploring order sensitivity in multimodal large language models. Z Tan, X Chu, W Li, T Mo, arXiv:2410.169832024arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>5: A party of foundation models. Q Team, Qwen2, September 2024. </p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, ArXiv, abs/2302.139712023</p>
<p>Attention is all you need. A Vaswani, N M Shazeer, N Parmar, Neural Information Processing Systems. 2017</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, International Conference on Learning Representations. 2018accepted as poster</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in neural information processing systems. 202235</p>
<p>Simplifying graph convolutional networks. F Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger, International conference on machine learning. PMLR2019</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, ArXiv, abs/2306.083852023258509408</p>
<p>Chartx &amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. R Xia, B Zhang, H Ye, 2024CoRR</p>
<p>Geometric knowledge distillation: Topology compression for graph neural networks. C Yang, Q Wu, J Yan, Advances in Neural Information Processing Systems. 202235</p>
<p>Graph-less neural networks: Teaching old mlps new tricks via distillation. S Zhang, Y Liu, Y Sun, N Shah, Y Zhang, H Dai, Z Kozareva, A Smola, L Song, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022. 201832International Conference on Learning Representations</p>
<p>Beyond homophily in graph neural networks: Current limitations and effective designs. J Zhu, Y Yan, L Zhao, Advances in neural information processing systems. 202033</p>
<p>Answer from Qwen2.5-7B-Instruct Here's a math problem involving sorting numbers from smallest to largest: Arrange these 20 numbers in ascending order (from smallest to largest): -847. Y Zhu, F Cong, D Zhang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023. -1002, -100, -847.5, -8, -7, -0.451, 0, 0.0036, 0.125, 0.15, 0.25, 1.618, 2.3333, 2.718, 6, 16, 16, 3.1416, 455. .5, 0.0036, -1002, 7/3, π, 455.89, -0.451, 2⁴, -√49, 0, 1.618, 15%, 2.7188999Wingnn: Dynamic graph neural networks with random gradient aggregation window. 4² , 0.125, -10² , 1/4, 9</p>
<p>Answer from Qwen2.5-7B-Instruct Here's a math problem involving sorting numbers from smallest to largest: Arrange these 20 numbers in ascending order. from smallest to largest</p>
<p>. Llm Graph, Graphwiz, 35.3±1.9 70.0±1.1 89.8±0.3 73.3±1.4 16.3±0.7 12.8±1.0 24.0±0.6 28.3±1.3 39.0±0.8 70.3±1.5</p>
<p>. 4±3.4 83.5±0.7 90.0±1.2 78.5±0.5 0.0±0.0 14.8±0.8 35.3±1.1 25.8±0.6 31.8±1.3 62.5±0.9LLaMA. 3</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3869 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3869</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3869</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-bc411487f305e451d7485e53202ec241fcc97d3b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b" target="_blank">CORD-19: The Covid-19 Open Research Dataset</a></p>
                <p><strong>Paper Venue:</strong> NLPCOVID19</p>
                <p><strong>Paper TL;DR:</strong> The mechanics of dataset construction are described, highlighting challenges and key design decisions, an overview of how CORD-19 has been used, and several shared tasks built around the dataset are described.</p>
                <p><strong>Paper Abstract:</strong> The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3869.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3869.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEURAL COVIDEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NEURAL COVIDEX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-and-discovery system for CORD-19 that applies a retrieval-then-rerank pipeline, using BM25 for initial retrieval and a T5-base transformer as an unsupervised reranker to improve ranking of papers for user queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>NEURAL COVIDEX (BM25 + T5-base reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A two-stage retrieval system: (1) lexical retrieval using BM25 to get candidate documents, and (2) an unsupervised T5-base transformer reranker applied to the BM25 results to reorder candidates by relevance to the query.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>CORD-19 paper collection used as the document collection for retrieval (the paper describes CORD-19 as a corpus with 28K initially and growing to >140K papers and tens of thousands of full texts depending on release).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>User queries given as natural language search topics (typical IR queries / TREC-style topics and community-driven questions); system is listed under 'Search and discovery' so queries are free-text search inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-then-rerank using a pretrained T5-base model in an unsupervised reranking role (no detailed prompt or fine-tuning recipe described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Ranked list of relevant papers (document identifiers and metadata; supports discovery UI).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not reported in detail in this paper; the paper situates systems like this within shared tasks (TREC-COVID) where human expert judgments are used for evaluation but NEURAL COVIDEX's specific evaluation metrics/results are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Described as a public tool (covidex.ai) applying T5-based reranking on BM25; no quantitative performance numbers are reported in this CORD-19 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper-level limitations applicable: dependency on quality of PDF/XML parsing and metadata; need for human expert vetting of extraction outputs; licensing/coverage gaps in the corpus; no details on possible reranker failure modes provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No quantitative comparison to baselines or human performance is reported within this paper; placed in the context of IR shared tasks that use human judgments (TREC-COVID).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3869.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3869.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vespa (summarization & recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vespa (T5 summarization; Sentence-BERT & SPECTER recommendations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A publicly-available system that generates abstractive summaries of paper abstracts using T5 and provides 'similar paper' recommendations using sentence/document embeddings (Sentence-BERT and SPECTER).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Vespa (T5 summarizer + Sentence-BERT / SPECTER recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Generates short text summaries of paper abstracts via a T5-based summarization model and finds similar papers using vector similarity over Sentence-BERT and SPECTER document embeddings to support exploration and literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>CORD-19 paper abstracts and metadata (the dataset as distributed by the CORD-19 project; versions vary over time, e.g., April 1st or later releases referenced for shared tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>User-driven: per-paper summarization of abstracts (input = paper abstract); recommendation based on similarity to a chosen paper or query (embedding-based similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Abstractive summarization using a pretrained T5 model; retrieval/recommendation via dense embedding similarity using Sentence-BERT and SPECTER embeddings (no end-to-end LLM reasoning protocol described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Textual abstractive summaries of abstracts; ranked lists of similar/recommended papers (embedding scores, metadata links).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>No quantitative evaluation results reported in this paper for Vespa; system is listed as a public tool rather than as an evaluated experiment within CORD-19.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Vespa is presented as a deployed tool (https://cord19.vespa.ai/) that produces summaries and recommendations using T5 and embedding methods; the CORD-19 paper does not report specific performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Summaries depend on abstract quality and model behavior (no error analysis reported here); broader CORD-19 limitations apply (parsing noise, licensing, incomplete coverage); need for human vetting emphasized in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No direct comparisons to baselines or human summaries provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3869.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3869.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPECTER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPECTER: Document-level representation learning using citation-informed transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based method that produces dense document embeddings (trained using citation information) to represent scientific papers for tasks such as recommendation and similarity; SPECTER embeddings are released alongside CORD-19.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SPECTER (citation-informed transformer embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Learns document-level representations using a transformer encoder and citation-based training signals; embeddings are computed from paper titles and abstracts and used for downstream tasks like similarity and recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Designed to run over the research literature (titles/abstracts); CORD-19 releases SPECTER embeddings with each dataset update for the CORD-19 papers (corpus scale: tens of thousands to >100K papers depending on release).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Similarity and recommendation tasks driven by either a query paper or free-text search; the representation itself is fixed-length embeddings for each document.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Representation learning (transformer-based) using citation-informed objectives (not a generative LLM distillation protocol); facilitates downstream synthesis by clustering/nearest-neighbor search over embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Dense document embeddings (vector files) that can be used to compute similarity, cluster documents, or feed into downstream pipelines for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>The CORD-19 paper notes SPECTER embeddings are released, but does not present the original SPECTER evaluation; evaluation details are available in the SPECTER reference (Cohan et al., 2020) rather than in this dataset paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SPECTER embeddings are provided as a resource for CORD-19 users to support recommendation and discovery; no new quantitative results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embedding-based approaches depend on title/abstract quality and citation signal; may not capture finer-grained full-text evidence; coverage limited to papers present in the corpus and subject to metadata quality issues described elsewhere in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons presented in this paper; original SPECTER paper contains baseline comparisons (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3869.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3869.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT / BioBERT / BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT, BioBERT, and BERT (pretrained transformer language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer language models widely used on CORD-19 for NER, entity linking, sentence extraction, and classification tasks; SciBERT and BioBERT are domain-adapted variants for scientific and biomedical text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciBERT / BioBERT / BERT (pretrained LMs for biomedical/scientific NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Pretrained transformer encoders (BERT family) used as base models that can be fine-tuned for tasks such as named entity recognition, entity linking, sentence classification, and extraction over CORD-19 full texts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>CORD-19 full text and abstracts are used as inputs to downstream tasks (the paper cites use of SciBERT and BioBERT for NER and classification over the CORD-19 corpus and mentions DeepSet releasing a BERT-base model pretrained on CORD-19).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Task-specific inputs: e.g., sentences/passages to be classified, text spans for NER; queries are problem-specific (NER label sets, sentence selection prompts or classifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Fine-tuning of pretrained transformer models on downstream labeled/weakly-supervised tasks (e.g., NER datasets or task-specific classification datasets); also weak supervision and distant supervision methods are referenced for entity linking.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured extractions (entity mentions and normalized identifiers), classified sentences/passages, and feature representations for downstream systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>The CORD-19 paper lists these models as tools used by the community; specific evaluation metrics for particular applications are not given in this paper (evaluation reported in the cited model papers and in downstream task papers).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Community usage is reported: examples include NER and entity linking augmentations of CORD-19 and sentence extraction tasks (e.g., Liang & Xie 2020). No unified quantitative synthesis is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model performance depends on labeled data and domain adaptation; challenges called out by the dataset paper include noisy/full-text parsing, licensing gaps limiting available text, and need for human vetting of extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No direct comparisons in this dataset paper; referenced model papers contain benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3869.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3869.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciFact (RoBERTa-large)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciFact claim verification system (uses RoBERTa-large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A claim verification system adapted to COVID-19 that uses a RoBERTa-large model to retrieve and label supporting or refuting evidence passages from scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciFact (RoBERTa-large entailment + retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Combines retrieval over CORD-19 with an entailment classifier based on RoBERTa-large to decide whether extracted passages support or refute a given scientific claim; presented as a tool for claim verification.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>CORD-19 full texts and abstracts serve as the evidence corpus from which supporting/refuting passages are retrieved.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Input is a target claim (natural-language claim) supplied by a user or task; the system retrieves candidate evidence passages and classifies them relative to the claim.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieve-then-classify pipeline: retrieval of candidate evidence passages followed by entailment/predictive classification with a large pretrained transformer (RoBERTa-large).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Labels per claim (Support / Refute) plus supporting evidence passages (text spans and references to source papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>This paper lists SciFact as an available system; the CORD-19 paper does not provide SciFact's internal evaluation metrics here (original SciFact publications provide benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SciFact is identified as a claim verification tool applied to COVID-19 literature; quantitative results are not reported in this dataset paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on retrieval quality and the reliability of parsed full text; the dataset paper emphasizes the need for human expert vetting of extracted evidence and limitations from parsing/licensing/coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not compared within this paper; SciFact's own publications contain comparisons to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3869.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3869.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSet COVID_BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSet BERTbase model pretrained on CORD-19</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-base model released by DeepSet that has been further pretrained on the CORD-19 corpus to better model COVID-19 related scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORD-19: The Covid-19 Open Research Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DeepSet COVID_BERT (BERTbase pretrained on CORD-19)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A domain-adapted pretrained language model: standard BERT-base architecture further pretrained on text from the CORD-19 corpus to improve representations for COVID-19/biomedical downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Pretraining data: CORD-19 text (abstracts and full text where available); exact size/version not specified in the CORD-19 paper beyond general corpus statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Usable as a backbone for different downstream tasks (NER, classification, retrieval); topic specification determined by downstream fine-tuning tasks (task-specific labels or queries).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Continued pretraining of a masked-language-model objective on the CORD-19 corpus (domain-adaptive pretraining), enabling improved task performance when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Pretrained model weights (BERT-style checkpoint) for use in downstream fine-tuning; not a direct textual synthesis output by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>CORD-19 paper notes availability of the model; no evaluation metrics are reported in this dataset paper for the released weights.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DeepSet's COVID_BERT is reported as a released resource for the community to support COVID-related NLP tasks; quantitative impact on downstream tasks is not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Domain-adaptive pretraining effectiveness depends on corpus coverage and quality; paper-level limitations include parsing noise and incomplete coverage of all relevant literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons are provided in this dataset paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CORD-19: The Covid-19 Open Research Dataset', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>SPECTER: Document-level representation learning using citation-informed transformers <em>(Rating: 2)</em></li>
                <li>SciBERT: A pretrained language model for scientific text <em>(Rating: 2)</em></li>
                <li>BioBERT: a pre-trained biomedical language representation model for biomedical text mining <em>(Rating: 2)</em></li>
                <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach <em>(Rating: 1)</em></li>
                <li>Identifying radiological findings related to covid-19 from medical literature <em>(Rating: 1)</em></li>
                <li>Real-time open-domain question answering with dense-sparse phrase index <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3869",
    "paper_id": "paper-bc411487f305e451d7485e53202ec241fcc97d3b",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "NEURAL COVIDEX",
            "name_full": "NEURAL COVIDEX",
            "brief_description": "A search-and-discovery system for CORD-19 that applies a retrieval-then-rerank pipeline, using BM25 for initial retrieval and a T5-base transformer as an unsupervised reranker to improve ranking of papers for user queries.",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "NEURAL COVIDEX (BM25 + T5-base reranker)",
            "system_or_method_description": "A two-stage retrieval system: (1) lexical retrieval using BM25 to get candidate documents, and (2) an unsupervised T5-base transformer reranker applied to the BM25 results to reorder candidates by relevance to the query.",
            "input_corpus_description": "CORD-19 paper collection used as the document collection for retrieval (the paper describes CORD-19 as a corpus with 28K initially and growing to &gt;140K papers and tens of thousands of full texts depending on release).",
            "topic_or_query_specification": "User queries given as natural language search topics (typical IR queries / TREC-style topics and community-driven questions); system is listed under 'Search and discovery' so queries are free-text search inputs.",
            "distillation_method": "Retrieval-then-rerank using a pretrained T5-base model in an unsupervised reranking role (no detailed prompt or fine-tuning recipe described in this paper).",
            "output_type_and_format": "Ranked list of relevant papers (document identifiers and metadata; supports discovery UI).",
            "evaluation_or_validation_method": "Not reported in detail in this paper; the paper situates systems like this within shared tasks (TREC-COVID) where human expert judgments are used for evaluation but NEURAL COVIDEX's specific evaluation metrics/results are not given here.",
            "results_summary": "Described as a public tool (covidex.ai) applying T5-based reranking on BM25; no quantitative performance numbers are reported in this CORD-19 paper.",
            "limitations_or_challenges": "Paper-level limitations applicable: dependency on quality of PDF/XML parsing and metadata; need for human expert vetting of extraction outputs; licensing/coverage gaps in the corpus; no details on possible reranker failure modes provided here.",
            "comparison_to_baselines_or_humans": "No quantitative comparison to baselines or human performance is reported within this paper; placed in the context of IR shared tasks that use human judgments (TREC-COVID).",
            "uuid": "e3869.0",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Vespa (summarization & recommendation)",
            "name_full": "Vespa (T5 summarization; Sentence-BERT & SPECTER recommendations)",
            "brief_description": "A publicly-available system that generates abstractive summaries of paper abstracts using T5 and provides 'similar paper' recommendations using sentence/document embeddings (Sentence-BERT and SPECTER).",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "Vespa (T5 summarizer + Sentence-BERT / SPECTER recommendation)",
            "system_or_method_description": "Generates short text summaries of paper abstracts via a T5-based summarization model and finds similar papers using vector similarity over Sentence-BERT and SPECTER document embeddings to support exploration and literature synthesis.",
            "input_corpus_description": "CORD-19 paper abstracts and metadata (the dataset as distributed by the CORD-19 project; versions vary over time, e.g., April 1st or later releases referenced for shared tasks).",
            "topic_or_query_specification": "User-driven: per-paper summarization of abstracts (input = paper abstract); recommendation based on similarity to a chosen paper or query (embedding-based similarity).",
            "distillation_method": "Abstractive summarization using a pretrained T5 model; retrieval/recommendation via dense embedding similarity using Sentence-BERT and SPECTER embeddings (no end-to-end LLM reasoning protocol described in this paper).",
            "output_type_and_format": "Textual abstractive summaries of abstracts; ranked lists of similar/recommended papers (embedding scores, metadata links).",
            "evaluation_or_validation_method": "No quantitative evaluation results reported in this paper for Vespa; system is listed as a public tool rather than as an evaluated experiment within CORD-19.",
            "results_summary": "Vespa is presented as a deployed tool (https://cord19.vespa.ai/) that produces summaries and recommendations using T5 and embedding methods; the CORD-19 paper does not report specific performance metrics.",
            "limitations_or_challenges": "Summaries depend on abstract quality and model behavior (no error analysis reported here); broader CORD-19 limitations apply (parsing noise, licensing, incomplete coverage); need for human vetting emphasized in paper.",
            "comparison_to_baselines_or_humans": "No direct comparisons to baselines or human summaries provided in this paper.",
            "uuid": "e3869.1",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SPECTER",
            "name_full": "SPECTER: Document-level representation learning using citation-informed transformers",
            "brief_description": "A transformer-based method that produces dense document embeddings (trained using citation information) to represent scientific papers for tasks such as recommendation and similarity; SPECTER embeddings are released alongside CORD-19.",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "SPECTER (citation-informed transformer embeddings)",
            "system_or_method_description": "Learns document-level representations using a transformer encoder and citation-based training signals; embeddings are computed from paper titles and abstracts and used for downstream tasks like similarity and recommendation.",
            "input_corpus_description": "Designed to run over the research literature (titles/abstracts); CORD-19 releases SPECTER embeddings with each dataset update for the CORD-19 papers (corpus scale: tens of thousands to &gt;100K papers depending on release).",
            "topic_or_query_specification": "Similarity and recommendation tasks driven by either a query paper or free-text search; the representation itself is fixed-length embeddings for each document.",
            "distillation_method": "Representation learning (transformer-based) using citation-informed objectives (not a generative LLM distillation protocol); facilitates downstream synthesis by clustering/nearest-neighbor search over embeddings.",
            "output_type_and_format": "Dense document embeddings (vector files) that can be used to compute similarity, cluster documents, or feed into downstream pipelines for literature synthesis.",
            "evaluation_or_validation_method": "The CORD-19 paper notes SPECTER embeddings are released, but does not present the original SPECTER evaluation; evaluation details are available in the SPECTER reference (Cohan et al., 2020) rather than in this dataset paper.",
            "results_summary": "SPECTER embeddings are provided as a resource for CORD-19 users to support recommendation and discovery; no new quantitative results reported here.",
            "limitations_or_challenges": "Embedding-based approaches depend on title/abstract quality and citation signal; may not capture finer-grained full-text evidence; coverage limited to papers present in the corpus and subject to metadata quality issues described elsewhere in the paper.",
            "comparison_to_baselines_or_humans": "No comparisons presented in this paper; original SPECTER paper contains baseline comparisons (not reproduced here).",
            "uuid": "e3869.2",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SciBERT / BioBERT / BERT",
            "name_full": "SciBERT, BioBERT, and BERT (pretrained transformer language models)",
            "brief_description": "Pretrained transformer language models widely used on CORD-19 for NER, entity linking, sentence extraction, and classification tasks; SciBERT and BioBERT are domain-adapted variants for scientific and biomedical text.",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "SciBERT / BioBERT / BERT (pretrained LMs for biomedical/scientific NLP)",
            "system_or_method_description": "Pretrained transformer encoders (BERT family) used as base models that can be fine-tuned for tasks such as named entity recognition, entity linking, sentence classification, and extraction over CORD-19 full texts.",
            "input_corpus_description": "CORD-19 full text and abstracts are used as inputs to downstream tasks (the paper cites use of SciBERT and BioBERT for NER and classification over the CORD-19 corpus and mentions DeepSet releasing a BERT-base model pretrained on CORD-19).",
            "topic_or_query_specification": "Task-specific inputs: e.g., sentences/passages to be classified, text spans for NER; queries are problem-specific (NER label sets, sentence selection prompts or classifiers).",
            "distillation_method": "Fine-tuning of pretrained transformer models on downstream labeled/weakly-supervised tasks (e.g., NER datasets or task-specific classification datasets); also weak supervision and distant supervision methods are referenced for entity linking.",
            "output_type_and_format": "Structured extractions (entity mentions and normalized identifiers), classified sentences/passages, and feature representations for downstream systems.",
            "evaluation_or_validation_method": "The CORD-19 paper lists these models as tools used by the community; specific evaluation metrics for particular applications are not given in this paper (evaluation reported in the cited model papers and in downstream task papers).",
            "results_summary": "Community usage is reported: examples include NER and entity linking augmentations of CORD-19 and sentence extraction tasks (e.g., Liang & Xie 2020). No unified quantitative synthesis is provided here.",
            "limitations_or_challenges": "Model performance depends on labeled data and domain adaptation; challenges called out by the dataset paper include noisy/full-text parsing, licensing gaps limiting available text, and need for human vetting of extractions.",
            "comparison_to_baselines_or_humans": "No direct comparisons in this dataset paper; referenced model papers contain benchmark comparisons.",
            "uuid": "e3869.3",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SciFact (RoBERTa-large)",
            "name_full": "SciFact claim verification system (uses RoBERTa-large)",
            "brief_description": "A claim verification system adapted to COVID-19 that uses a RoBERTa-large model to retrieve and label supporting or refuting evidence passages from scientific papers.",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "SciFact (RoBERTa-large entailment + retrieval)",
            "system_or_method_description": "Combines retrieval over CORD-19 with an entailment classifier based on RoBERTa-large to decide whether extracted passages support or refute a given scientific claim; presented as a tool for claim verification.",
            "input_corpus_description": "CORD-19 full texts and abstracts serve as the evidence corpus from which supporting/refuting passages are retrieved.",
            "topic_or_query_specification": "Input is a target claim (natural-language claim) supplied by a user or task; the system retrieves candidate evidence passages and classifies them relative to the claim.",
            "distillation_method": "Retrieve-then-classify pipeline: retrieval of candidate evidence passages followed by entailment/predictive classification with a large pretrained transformer (RoBERTa-large).",
            "output_type_and_format": "Labels per claim (Support / Refute) plus supporting evidence passages (text spans and references to source papers).",
            "evaluation_or_validation_method": "This paper lists SciFact as an available system; the CORD-19 paper does not provide SciFact's internal evaluation metrics here (original SciFact publications provide benchmarks).",
            "results_summary": "SciFact is identified as a claim verification tool applied to COVID-19 literature; quantitative results are not reported in this dataset paper.",
            "limitations_or_challenges": "Effectiveness depends on retrieval quality and the reliability of parsed full text; the dataset paper emphasizes the need for human expert vetting of extracted evidence and limitations from parsing/licensing/coverage.",
            "comparison_to_baselines_or_humans": "Not compared within this paper; SciFact's own publications contain comparisons to baselines.",
            "uuid": "e3869.4",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "DeepSet COVID_BERT",
            "name_full": "DeepSet BERTbase model pretrained on CORD-19",
            "brief_description": "A BERT-base model released by DeepSet that has been further pretrained on the CORD-19 corpus to better model COVID-19 related scientific text.",
            "citation_title": "CORD-19: The Covid-19 Open Research Dataset",
            "mention_or_use": "use",
            "system_or_method_name": "DeepSet COVID_BERT (BERTbase pretrained on CORD-19)",
            "system_or_method_description": "A domain-adapted pretrained language model: standard BERT-base architecture further pretrained on text from the CORD-19 corpus to improve representations for COVID-19/biomedical downstream tasks.",
            "input_corpus_description": "Pretraining data: CORD-19 text (abstracts and full text where available); exact size/version not specified in the CORD-19 paper beyond general corpus statistics.",
            "topic_or_query_specification": "Usable as a backbone for different downstream tasks (NER, classification, retrieval); topic specification determined by downstream fine-tuning tasks (task-specific labels or queries).",
            "distillation_method": "Continued pretraining of a masked-language-model objective on the CORD-19 corpus (domain-adaptive pretraining), enabling improved task performance when fine-tuned.",
            "output_type_and_format": "Pretrained model weights (BERT-style checkpoint) for use in downstream fine-tuning; not a direct textual synthesis output by itself.",
            "evaluation_or_validation_method": "CORD-19 paper notes availability of the model; no evaluation metrics are reported in this dataset paper for the released weights.",
            "results_summary": "DeepSet's COVID_BERT is reported as a released resource for the community to support COVID-related NLP tasks; quantitative impact on downstream tasks is not reported here.",
            "limitations_or_challenges": "Domain-adaptive pretraining effectiveness depends on corpus coverage and quality; paper-level limitations include parsing noise and incomplete coverage of all relevant literature.",
            "comparison_to_baselines_or_humans": "No comparisons are provided in this dataset paper.",
            "uuid": "e3869.5",
            "source_info": {
                "paper_title": "CORD-19: The Covid-19 Open Research Dataset",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "SPECTER: Document-level representation learning using citation-informed transformers",
            "rating": 2
        },
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text",
            "rating": 2
        },
        {
            "paper_title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "rating": 2
        },
        {
            "paper_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "rating": 1
        },
        {
            "paper_title": "Identifying radiological findings related to covid-19 from medical literature",
            "rating": 1
        },
        {
            "paper_title": "Real-time open-domain question answering with dense-sparse phrase index",
            "rating": 1
        }
    ],
    "cost": 0.015314999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CORD-19: The Covid-19 Open Research Dataset</h1>
<p>Lucy Lu Wang ${ }^{1, <em>}$ Kyle Lo ${ }^{1, </em>}$ Yoganand Chandrasekhar ${ }^{1}$ Russell Reas ${ }^{1}$<br>Jiangjiang Yang ${ }^{1}$ Douglas Burdick ${ }^{2}$ Darrin Eide ${ }^{3}$ Kathryn Funk ${ }^{4}$<br>Yannis Katsis ${ }^{2}$ Rodney Kinney ${ }^{1}$ Yunyao $\mathrm{Li}^{2}$ Ziyang Liu ${ }^{6}$<br>William Merrill ${ }^{1}$ Paul Mooney ${ }^{5}$ Dewey Murdick ${ }^{7}$ Devvret Rishi ${ }^{5}$<br>Jerry Sheehan ${ }^{4}$ Zhihong Shen ${ }^{3}$ Brandon Stilson ${ }^{1}$ Alex D. Wade ${ }^{6}$<br>Kuansan Wang ${ }^{3}$ Nancy Xin Ru Wang ${ }^{2}$ Chris Wilhelm ${ }^{1}$ Boya Xie ${ }^{3}$<br>Douglas Raymond ${ }^{1}$ Daniel S. Weld ${ }^{1,8}$ Oren Etzioni ${ }^{1}$ Sebastian Kohlmeier ${ }^{1}$<br>${ }^{1}$ Allen Institute for AI ${ }^{2}$ IBM Research ${ }^{3}$ Microsoft Research<br>${ }^{4}$ National Library of Medicine ${ }^{5}$ Kaggle ${ }^{6}$ Chan Zuckerberg Initiative<br>${ }^{7}$ Georgetown University ${ }^{8}$ University of Washington<br>{lucyw, kylel}@allenai.org</p>
<h2>Abstract</h2>
<p>The Covid-19 Open Research Dataset (CORD-19) is a growing ${ }^{1}$ resource of scientific papers on Covid-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded ${ }^{2}$ over 200 K times and has served as the basis of many Covid-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for Covid-19.</p>
<h2>1 Introduction</h2>
<p>On March 16, 2020, the Allen Institute for AI (AI2), in collaboration with our partners at The White House Office of Science and Technology Policy (OSTP), the National Library of Medicine (NLM), the Chan Zuckerburg Initiative (CZI), Microsoft Research, and Kaggle, coordinated by Georgetown University's Center for Security and Emerging Technology (CSET), released the first version</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Papers and preprints are collected from different sources through Semantic Scholar. Released as part of CORD-19 are the harmonized and deduplicated metadata and full text JSON.
of CORD-19. This resource is a large and growing collection of publications and preprints on Covid19 and related historical coronaviruses such as SARS and MERS. The initial release consisted of 28 K papers, and the collection has grown to more than 140 K papers over the subsequent weeks. Papers and preprints from several archives are collected and ingested through the Semantic Scholar literature search engine, ${ }^{3}$ metadata are harmonized and deduplicated, and paper documents are processed through the pipeline established in Lo et al. (2020) to extract full text (more than $50 \%$ of papers in CORD-19 have full text). We commit to providing regular updates to the dataset until an end to the Covid-19 crisis is foreseeable.</p>
<p>CORD-19 aims to connect the machine learning community with biomedical domain experts and policy makers in the race to identify effective treatments and management policies for Covid19. The goal is to harness these diverse and com-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>plementary pools of expertise to discover relevant information more quickly from the literature. Users of the dataset have leveraged AI-based techniques in information retrieval and natural language processing to extract useful information.</p>
<p>Responses to CORD-19 have been overwhelmingly positive, with the dataset being downloaded over 200 K times in the three months since its release. The dataset has been used by clinicians and clinical researchers to conduct systematic reviews, has been leveraged by data scientists and machine learning practitioners to construct search and extraction tools, and is being used as the foundation for several successful shared tasks. We summarize research and shared tasks in Section 4.</p>
<p>In this article, we briefly describe:</p>
<ol>
<li>The content and creation of CORD-19,</li>
<li>Design decisions and challenges around creating the dataset,</li>
<li>Research conducted on the dataset, and how shared tasks have facilitated this research, and</li>
<li>A roadmap for CORD-19 going forward.</li>
</ol>
<h2>2 Dataset</h2>
<p>CORD-19 integrates papers and preprints from several sources (Figure 1), where a paper is defined as the base unit of published knowledge, and a preprint as an unpublished but publicly available counterpart of a paper. Throughout the rest of Section 2, we discuss papers, though the same processing steps are adopted for preprints. First, we ingest into Semantic Scholar paper metadata and documents from each source. Each paper is associated with bibliographic metadata, like title, authors, publication venue, etc, as well as unique identifiers such as a DOI, PubMed Central ID, PubMed ID, the WHO Covidence #, ${ }^{4}$ MAG identifier (Shen et al., 2018), and others. Some papers are associated with documents, the physical artifacts containing paper content; these are the familiar PDFs, XMLs, or physical print-outs we read.</p>
<p>For the CORD-19 effort, we generate harmonized and deduplicated metadata as well as structured full text parses of paper documents as output. We provide full text parses in cases where we have access to the paper documents, and where the documents are available under an open access license</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(e.g. Creative Commons (CC), ${ }^{5}$ publisher-specific COVID-19 licenses, ${ }^{6}$ or identified as open access through DOI lookup in the Unpaywall ${ }^{7}$ database).</p>
<h3>2.1 Sources of papers</h3>
<p>Papers in CORD-19 are sourced from PubMed Central (PMC), PubMed, the World Health Organization's Covid-19 Database, ${ }^{4}$ and preprint servers bioRxiv, medRxiv, and arXiv. The PMC Public Health Emergency Covid-19 Initiative ${ }^{6}$ expanded access to COVID-19 literature by working with publishers to make coronavirus-related papers discoverable and accessible through PMC under open access license terms that allow for reuse and secondary analysis. BioRxiv and medRxiv preprints were initially provided by CZI, and are now ingested through Semantic Scholar along with all other included sources. We also work directly with publishers such as Elsevier ${ }^{8}$ and Springer Nature, ${ }^{9}$ to provide full text coverage of relevant papers available in their back catalog.</p>
<p>All papers are retrieved given the query ${ }^{10}$ :</p>
<div class="codehilite"><pre><span></span><code>&quot;COVID&quot; OR &quot;COVID-19&quot; OR
&quot;Coronavirus&quot; OR &quot;Corona virus&quot;
OR &quot;2019-nCoV&quot; OR &quot;SARS-CoV&quot;
OR &quot;MERS-CoV&quot; OR &quot;Severe Acute
Respiratory Syndrome&quot; OR &quot;Middle
East Respiratory Syndrome&quot;
</code></pre></div>

<p>Papers that match on these keywords in their title, abstract, or body text are included in the dataset. Query expansion is performed by PMC on these search terms, affecting the subset of papers in CORD-19 retrieved from PMC.</p>
<h3>2.2 Processing metadata</h3>
<p>The initial collection of sourced papers suffers from duplication and incomplete or conflicting metadata. We perform the following operations to harmonize and deduplicate all metadata:</p>
<ol>
<li>Cluster papers using paper identifiers</li>
<li>Select canonical metadata for each cluster</li>
<li>Filter clusters to remove unwanted entries
<sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></li>
</ol>
<p>Clustering papers We cluster papers if they overlap on any of the following identifiers: ${$ doi, pmc_id, pubmed_id, arxiv_id, who_covidence_id, mag_id $}$. If two papers from different sources have an identifier in common and no other identifier conflicts between them, we assign them to the same cluster. Each cluster is assigned a unique identifier CORD_UID, which persists between dataset releases. No existing identifier, such as DOI or PMC ID, is sufficient as the primary CORD-19 identifier. Some papers in PMC do not have DOIs; some papers from the WHO, publishers, or preprint servers like arXiv do not have PMC IDs or DOIs.</p>
<p>Occasionally, conflicts occur. For example, a paper $c$ with (doi, pmc_id, pubmed_id) identifiers $\left(x\right.$, null, $\left.z^{\prime}\right)$ might share identifier $x$ with a cluster of papers ${a, b}$ that has identifiers $(x, y, z)$, but has a conflict $z^{\prime} \neq z$. In this case, we choose to create a new cluster ${c}$, containing only paper $c$. ${ }^{11}$</p>
<p>Selecting canonical metadata Among each cluster, the canonical entry is selected to prioritize the availability of document files and the most permissive license. For example, between two papers with PDFs, one available under a CC license and one under a more restrictive COVID-19-specific copyright license, we select the CC-licensed paper entry as canonical. If any metadata in the canonical entry are missing, values from other members of the cluster are promoted to fill in the blanks.</p>
<p>Cluster filtering Some entries harvested from sources are not papers, and instead correspond to materials like tables of contents, indices, or informational documents. These entries are identified in an ad hoc manner and removed from the dataset.</p>
<h3>2.3 Processing full text</h3>
<p>Most papers are associated with one or more PDFs. ${ }^{12}$ To extract full text and bibliographies from each PDF, we use the PDF parsing pipeline created for the S2ORC dataset (Lo et al., 2020). ${ }^{13}$ In (Lo et al., 2020), we introduce the S2ORC JSON format for representing scientific paper full text,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>which is used as the target output for paper full text in CORD-19. The pipeline involves:</p>
<ol>
<li>Parse all PDFs to TEI XML files using GROBID ${ }^{15}$ (Lopez, 2009)</li>
<li>Parse all TEI XML files to S2ORC JSON</li>
<li>Postprocess to clean up links between inline citations and bibliography entries.</li>
</ol>
<p>We additionally parse JATS XML ${ }^{16}$ files available for PMC papers using a custom parser, generating the same target S2ORC JSON format.</p>
<p>This creates two sets of full text JSON parses associated with the papers in the collection, one set originating from PDFs (available from more sources), and one set originating from JATS XML (available only for PMC papers). Each PDF parse has an associated SHA, the 40-digit SHA-1 of the associated PDF file, while each XML parse is named using its associated PMC ID. Around 48\% of CORD-19 papers have an associated PDF parse, and around $37 \%$ have an XML parse, with the latter nearly a subset of the former. Most PDFs ( $&gt;90 \%$ ) are successfully parsed. Around 2.6\% of CORD19 papers are associated with multiple PDF SHA, due to a combination of paper clustering and the existence of supplementary PDF files.</p>
<h3>2.4 Table parsing</h3>
<p>Since the May 12, 2020 release of CORD-19, we also release selected HTML table parses. Tables contain important numeric and descriptive information such as sample sizes and results, which are the targets of many information extraction systems. A separate PDF table processing pipeline is used, consisting of table extraction and table understanding. Table extraction is based on the Smart Document Understanding (SDU) capability included in IBM Watson Discovery. ${ }^{17}$ SDU converts a given PDF document from its native binary representation into a text-based representation like HTML which includes both identified document structures (e.g., tables, section headings, lists) and formatting information (e.g. positions for extracted text). Table understanding (also part of Watson Discovery) then annotates the extracted tables with additional semantic information, such as column and row headers and table captions. We leverage the Global Table Extractor (GTE) (Zheng et al.,</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The distribution of papers per year in CORD-19. A spike in publications occurs in 2020 in response to COVID-19.</p>
<p>2020), which uses a specialized object detection and clustering technique to extract table bounding boxes and structures.</p>
<p>All PDFs are processed through this table extraction and understanding pipeline. If the Jaccard similarity of the table captions from the table parses and CORD-19 parses is above 0.9, we insert the HTML of the matched table into the full text JSON. We extract 188K tables from 54K documents, of which 33K tables are successfully matched to tables in 19K (around 25%) full text documents in CORD-19. Based on preliminary error analysis, we find that match failures are primarily due to caption mismatches between the two parse schemes. Thus, we plan to explore alternate matching functions, potentially leveraging table content and document location as additional features. See Appendix A for example table parses.</p>
<h3>2.5 Dataset contents</h3>
<p>CORD-19 has grown rapidly, now consisting of over 140K papers with over 72K full texts. Over 47K papers and 7K preprints on COVID-19 and coronaviruses have been released since the start of 2020, comprising nearly 40% of papers in the dataset.</p>
<p>Classification of CORD-19 papers to Microsoft Academic Graph (MAG) (Wang et al., 2019, 2020) fields of study (Shen et al., 2018) indicate that the dataset consists predominantly of papers in Medicine (55%), Biology (31%), and Chemistry (3%), which together constitute almost 90% of the corpus. A breakdown of the most common MAG</p>
<table>
<thead>
<tr>
<th>Subfield</th>
<th>Count</th>
<th>% of corpus</th>
</tr>
</thead>
<tbody>
<tr>
<td>Virology</td>
<td>29567</td>
<td>25.5%</td>
</tr>
<tr>
<td>Immunology</td>
<td>15954</td>
<td>13.8%</td>
</tr>
<tr>
<td>Surgery</td>
<td>15667</td>
<td>13.5%</td>
</tr>
<tr>
<td>Internal medicine</td>
<td>12045</td>
<td>10.4%</td>
</tr>
<tr>
<td>Intensive care medicine</td>
<td>10624</td>
<td>9.2%</td>
</tr>
<tr>
<td>Molecular biology</td>
<td>7268</td>
<td>6.3%</td>
</tr>
<tr>
<td>Pathology</td>
<td>6611</td>
<td>5.7%</td>
</tr>
<tr>
<td>Genetics</td>
<td>5231</td>
<td>4.5%</td>
</tr>
<tr>
<td>Other</td>
<td>12997</td>
<td>11.2%</td>
</tr>
</tbody>
</table>
<p>Table 1: MAG subfield of study for CORD-19 papers.</p>
<p>subfields (L1 fields of study) represented in CORD-19 is given in Table 1.</p>
<p>Figure 2 shows the distribution of CORD-19 papers by date of publication. Coronavirus publications increased during and following the SARS and MERS epidemics, but the number of papers published in the early months of 2020 exploded in response to the COVID-19 epidemic. Using author affiliations in MAG, we identify the countries from which the research in CORD-19 is conducted. Large proportions of CORD-19 papers are associated with institutions based in the Americas (around 48K papers), Europe (over 35K papers), and Asia (over 30K papers).</p>
<h3>3 Design decision &amp; challenges</h3>
<p>A number of challenges come into play in the creation of CORD-19. We summarize the primary design requirements of the dataset, along with challenges implicit within each requirement:</p>
<p><strong>Up-to-date</strong> Hundreds of new publications on COVID-19 are released every day, and a dataset like CORD-19 can quickly become irrelevant without regular updates. CORD-19 has been updated daily since May 26. A processing pipeline that produces consistent results day to day is vital to maintaining a changing dataset. That is, the metadata and full text parsing results must be reproducible, identifiers must be persistent between releases, and changes or new features should ideally be compatible with previous versions of the dataset.</p>
<p><strong>Handles data from multiple sources</strong> Papers from different sources must be integrated and harmonized. Each source has its own metadata format, which must be converted to the CORD-19 format, while addressing any missing or extraneous fields. The processing pipeline must also be flexible to adding new sources.</p>
<p>on the CORD-19 landing page.</p>
<p>Clean canonical metadata Because of the diversity of paper sources, duplication is unavoidable. Once paper metadata from each source is cleaned and organized into CORD-19 format, we apply the deduplication logic described in Section 2.2 to identify similar paper entries from different sources. We apply a conservative clustering algorithm, combining papers only when they have shared identifiers but no conflicts between any particular class of identifiers. We justify this because it is less harmful to retain a few duplicate papers than to remove a document that is potentially unique and useful.</p>
<p>Machine readable full text To provide accessible and canonical structured full text, we parse content from PDFs and associated paper documents. The full text is represented in S2ORC JSON format (Lo et al., 2020), a schema designed to preserve most relevant paper structures such as paragraph breaks, section headers, inline references, and citations. S2ORC JSON is simple to use for many NLP tasks, where character-level indices are often employed for annotation of relevant entities or spans. The text and annotation representations in S2ORC share similarities with BioC (Comeau et al., 2019), a JSON schema introduced by the BioCreative community for shareable annotations, with both formats leveraging the flexibility of characterbased span annotations. However, S2ORC JSON also provides a schema for representing other components of a paper, such as its metadata fields, bibliography entries, and reference objects for figures, tables, and equations. We leverage this flexible and somewhat complete representation of S2ORC JSON for CORD-19. We recognize that converting between PDF or XML to JSON is lossy. However, the benefits of a standard structured format, and the ability to reuse and share annotations made on top of that format have been critical to the success of CORD-19.</p>
<p>Observes copyright restrictions Papers in CORD-19 and academic papers more broadly are made available under a variety of copyright licenses. These licenses can restrict or limit the abilities of organizations such as AI2 from redistributing their content freely. Although much of the Covid-19 literature has been made open access by publishers, the provisions on these open access licenses differ greatly across papers. Additionally, many open access licenses grant the ability to read, or "consume" the paper, but may be restrictive in</p>
<p>Given a query:
Does hypertension increase the risks associated with Covid-19?
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example information retrieval and extraction system using CORD-19: Given an input query, the system identifies relevant papers (yellow highlighted rows) and extracts text snippets from the full text JSONs as supporting evidence.
other ways, for example, by not allowing republication of a paper or its redistribution for commercial purposes. The curator of a dataset like CORD19 must pass on best-to-our-knowledge licensing information to the end user.</p>
<h2>4 Research directions</h2>
<p>We provide a survey of various ways researchers have made use of CORD-19. We organize these into four categories: (i) direct usage by clinicians and clinical researchers ( $\S 4.1$ ), (ii) tools and systems to assist clinicians (4.2), (iii) research to support further text mining and NLP research (4.3), and (iv) shared tasks and competitions (4.4).</p>
<h3>4.1 Usage by clinical researchers</h3>
<p>CORD-19 has been used by medical experts as a paper collection for conducting systematic reviews. These reviews address questions about Covid-19 include infection and mortality rates in different demographics (Han et al., 2020), symptoms of the disease (Parasa et al., 2020), identifying suitable drugs for repurposing (Sadegh et al., 2020), management policies (Yaacoub et al., 2020), and interactions with other diseases (Crisan-Dabija et al., 2020; Popa et al., 2020).</p>
<h3>4.2 Tools for clinicians</h3>
<p>Challenges for clinicians and clinical researchers during the current epidemic include (i) keeping up to to date with recent papers about Covid-19, (ii) identifying useful papers from historical coronavirus literature, (iii) extracting useful information from the literature, and (iv) synthesizing knowledge from the literature. To facilitate solutions to</p>
<p>these challenges, dozens of tools and systems over CORD-19 have already been developed. Most combine elements of text-based information retrieval and extraction, as illustrated in Figure 3. We have compiled a list of these efforts on the CORD19 public GitHub repository ${ }^{19}$ and highlight some systems in Table 2. ${ }^{20}$</p>
<h3>4.3 Text mining and NLP research</h3>
<p>The following is a summary of resources released by the NLP community on top of CORD-19 to support other research activities.</p>
<p>Information extraction To support extractive systems, NER and entity linking of biomedical entities can be useful. NER and linking can be performed using NLP toolkits like ScispaCy (Neumann et al., 2019) or language models like BioBERT-base (Lee et al., 2019) and SciBERTbase (Beltagy et al., 2019) finetuned on biomedical NER datasets. Wang et al. (2020) augments CORD-19 full text with entity mentions predicted from several techniques, including weak supervision using the NLM's Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004).</p>
<p>Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain COVID-19-related radiological findings.</p>
<p>Pretrained model weights BioBERT and SciBERT have been popular pretrained LMs for Covid-19-related tasks. DeepSet has released a BERTbase model pretrained on CORD-19. ${ }^{21}$ SPECTER (Cohan et al., 2020) paper embeddings computed using paper titles and abstracts are being released with each CORD-19 update. SeVeN relation embeddings (Espinosa-Anke and Schockaert, 2018) between word pairs have also been made available for CORD-19. ${ }^{22}$</p>
<p>Knowledge graphs The Covid Graph project ${ }^{23}$ releases a Covid-19 knowledge graph built from mining several public data sources, including</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>CORD-19, and is perhaps the largest current initiative in this space. Ahamed and Samad (2020) rely on entity co-occurrences in CORD-19 to construct a graph that enables centrality-based ranking of drugs, pathogens, and biomolecules.</p>
<h3>4.4 Competitions and Shared Tasks</h3>
<p>The adoption of CORD-19 and the proliferation of text mining and NLP systems built on top of the dataset are supported by several Covid-19-related competitions and shared tasks.</p>
<h3>4.4.1 Kaggle</h3>
<p>Kaggle hosts the CORD-19 Research Challenge, ${ }^{24}$ a text-mining challenge that tasks participants with extracting answers to key scientific questions about Covid-19 from the papers in the CORD19 dataset. Round 1 was initiated with a set of open-ended questions, e.g., What is known about transmission, incubation, and environmental stability? and What do we know about Covid-19 risk factors?</p>
<p>More than 500 teams participated in Round 1 of the Kaggle competition. Feedback from medical experts during Round 1 identified that the most useful contributions took the form of article summary tables. Round 2 subsequently focused on this task of table completion, and resulted in 100 additional submissions. A unique tabular schema is defined for each question, and answers are collected from across different automated extractions. For example, extractions for risk factors should include disease severity and fatality metrics, while extractions for incubation should include time ranges. Sufficient knowledge of COVID-19 is necessary to define these schema, to understand which fields are important to include (and exclude), and also to perform error-checking and manual curation.</p>
<h3>4.4.2 TREC</h3>
<p>The TREC-COVID ${ }^{25}$ shared task (Roberts et al., 2020; Voorhees et al., 2020) assesses systems on their ability to rank papers in CORD-19 based on their relevance to Covid-19-related topics. Topics are sourced from MedlinePlus searches, Twitter conversations, library searches at OHSU, as well as from direct conversations with researchers, reflecting actual queries made by the community. To emulate real-world surge in publications and rapidly-</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Project</th>
<th>Link</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Search and discovery</td>
<td>NEURAL COVIDEX</td>
<td>https://covidex.ai/</td>
<td>Uses a T5-base <em>Raffel et al. (2019)</em> unsupervised reranker on BM25 <em>Jones et al. (2000)</em></td>
</tr>
<tr>
<td></td>
<td>COVIDSCHOLAR</td>
<td>https://covidscholar.org/</td>
<td>Adapts Weston et al. <em>2019</em> system for entity-centric queries</td>
</tr>
<tr>
<td></td>
<td>KDCOVID</td>
<td>http://kdcovid.nl/about.html</td>
<td>Uses BioSentVec <em>Chen et al. (2019)</em> similarity to identify relevant sentences</td>
</tr>
<tr>
<td></td>
<td>SPIKE-CORD</td>
<td>https://spike.covid-19.apps.allenai.org</td>
<td>Enables users to define regular expression-like queries to directly search over full text</td>
</tr>
<tr>
<td>Question answering</td>
<td>COVIDASK</td>
<td>https://covidask.korea.ac.kr/</td>
<td>Adapts Seo et al. <em>2019</em> using BioASQ challenge (Task B) dataset <em>Tsatsaronis et al. (2015)</em></td>
</tr>
<tr>
<td></td>
<td>AUEB</td>
<td>http://cslab241.cs.aueb.gr:5000/</td>
<td>Adapts McDonald et al. <em>2018</em> using Tsatsaronis et al. <em>2015</em></td>
</tr>
<tr>
<td>Summarization</td>
<td>Vespa</td>
<td>https://cord19.vespa.ai/</td>
<td>Generates summaries of paper abstracts using T5 <em>Raffel et al. (2019)</em></td>
</tr>
<tr>
<td>Recommendation</td>
<td>Vespa</td>
<td>https://cord19.vespa.ai/</td>
<td>Recommends similar papers using Sentence-BERT <em>Reimers and Gurevych (2019)</em> and SPECTER embeddings <em>Cohan et al. (2020)</em></td>
</tr>
<tr>
<td>Entailment</td>
<td>COVID papers browser</td>
<td>https://github.com/gsarti/covid-papers-browser</td>
<td>Similar to KDCOVID, but uses embeddings from BERT models trained on NLI datasets</td>
</tr>
<tr>
<td>Claim verification</td>
<td>SciFact</td>
<td>https://scifact.apps.allenai.org</td>
<td>Uses RoBERTa-large <em>Liu et al. (2019)</em> to find Support/Refute evidence for COVID-19 claims</td>
</tr>
<tr>
<td>Assistive lit. review</td>
<td>ASReview</td>
<td>https://github.com/asreview/asreview-covid19</td>
<td>Active learning system with a CORD-19 plugin for identifying papers for literature reviews</td>
</tr>
<tr>
<td>Augmented reading</td>
<td>Sinequa</td>
<td>https://covidsearch.sinequa.com/app/covid-search/</td>
<td>In-browser paper reader with entity highlighting on PDFs</td>
</tr>
<tr>
<td>Visualization</td>
<td>SciSight</td>
<td>https://scisight.apps.allenai.org</td>
<td>Network visualizations for browsing research groups working on COVID-19</td>
</tr>
</tbody>
</table>
<p>Table 2: Publicly-available tools and systems for medical experts using CORD-19.</p>
<p>changing information needs, the shared task is organized in multiple rounds. Each round uses a specific version of CORD-19, has newly added topics, and gives participants one week to submit per-topic document rankings for judgment. Round 1 topics included more general questions such as <em>What is the origin of COVID-19?</em> and <em>What are the initial symptoms of COVID-19?</em> while Round 3 topics have become more focused, e.g., <em>What are the observed mutations in the SARS-CoV-2 genome?</em> and <em>What are the longer-term complications of those who recover from COVID-19?</em> Around 60 medical domain experts, including indexers from NLM and medical students from OHSU and UTHealth, are involved in providing gold rankings for evaluation. TREC-COVID opened using the April 1st CORD-19 version and received submissions from over 55 participating teams.</p>
<h2>5 Discussion</h2>
<p>Several hundred new papers on COVID-19 are now being published every day. Automated methods are needed to analyze and synthesize information over this large quantity of content. The computing community has risen to the occasion, but it is clear that there is a critical need for better infrastructure to incorporate human judgments in the loop. Extractions need expert vetting, and search engines and systems must be designed to serve users.</p>
<p>Successful engagement and usage of CORD-19 speaks to our ability to bridge computing and biomedical communities over a common, global cause. From early results of the Kaggle challenge, we have learned which formats are conducive to collaboration, and which questions are the most urgent to answer. However, there is significant work that remains for determining (i) which methods are best to assist textual discovery over the literature, (ii) how best to involve expert curators in the pipeline, and (iii) which extracted results convert to successful COVID-19 treatments and management policies. Shared tasks and challenges, as well as continued analysis and synthesis of feedback will hopefully provide answers to these outstanding questions.</p>
<p>Since the initial release of CORD-19, we have implemented several new features based on com-</p>
<p>munity feedback, such as the inclusion of unique identifiers for papers, table parses, more sources, and daily updates. Most substantial outlying features requests have been implemented or addressed at this time. We will continue to update the dataset with more sources of papers and newly published literature as resources permit.</p>
<h3>5.1 Limitations</h3>
<p>Though we aim to be comprehensive, CORD-19 does not cover many relevant scientific documents on COVID-19. We have restricted ourselves to research papers and preprints, and do not incorporate other types of documents, such as technical reports, white papers, informational publications by governmental bodies, and more. Including these documents is outside the current scope of CORD19 , but we encourage other groups to curate and publish such datasets.</p>
<p>Within the scope of scientific papers, CORD-19 is also incomplete, though we continue to prioritize the addition of new sources. This has motivated the creation of other corpora supporting Covid-19 NLP, such as LitCovid (Chen et al., 2020), which provide complementary materials to CORD-19 derived from PubMed. Though we have since added PubMed as a source of papers in CORD-19, there are other domains such as the social sciences that are not currently represented, and we hope to incorporate these works as part of future work.</p>
<p>We also note the shortage of foreign language papers in CORD-19, especially Chinese language papers produced during the early stages of the epidemic. These papers may be useful to many researchers, and we are working with collaborators to provide them as supplementary data. However, challenges in both sourcing and licensing these papers for re-publication are additional hurdles.</p>
<h3>5.2 Call to action</h3>
<p>Though the full text of many scientific papers are available to researchers through CORD-19, a number of challenges prevent easy application of NLP and text mining techniques to these papers. First, the primary distribution format of scientific papers - PDF - is not amenable to text processing. The PDF file format is designed to share electronic documents rendered faithfully for reading and printing, and mixes visual with semantic information. Significant effort is needed to coerce PDF into a format more amenable to text mining, such as JATS</p>
<p>XML, ${ }^{26}$ BioC (Comeau et al., 2019), or S2ORC JSON (Lo et al., 2020), which is used in CORD-19. Though there is substantial work in this domain, we can still benefit from better PDF parsing tools for scientific documents. As a complement, scientific papers should also be made available in a structured format like JSON, XML, or HTML.</p>
<p>Second, there is a clear need for more scientific content to be made accessible to researchers. Some publishers have made Covid-19 papers openly available during this time, but both the duration and scope of these epidemic-specific licenses are unclear. Papers describing research in related areas (e.g., on other infectious diseases, or relevant biological pathways) have also not been made open access, and are therefore unavailable in CORD-19 or otherwise. Securing release rights for papers not yet in CORD-19 but relevant for Covid-19 research is a significant portion of future work, led by the PMC Covid-19 Initiative. ${ }^{6}$</p>
<p>Lastly, there is no standard format for representing paper metadata. Existing schemas like the JATS XML NISO standard ${ }^{26}$ or library science standards like BibFRAME ${ }^{27}$ or Dublin Core ${ }^{28}$ have been adopted to represent paper metadata. However, these standards can be too coarse-grained to capture all necessary paper metadata elements, or may lack a strict schema, causing representations to vary greatly across publishers who use them. To improve metadata coherence across sources, the community must define and agree upon an appropriate standard of representation.</p>
<h2>Summary</h2>
<p>This project offers a paradigm of how the community can use machine learning to advance scientific research. By allowing computational access to the papers in CORD-19, we increase our ability to perform discovery over these texts. We hope the dataset and projects built on the dataset will serve as a template for future work in this area. We also believe there are substantial improvements that can be made in the ways we publish, share, and work with scientific papers. We offer a few suggestions that could dramatically increase community productivity, reduce redundant effort, and result in better discovery and understanding of the scientific literature.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Through CORD-19, we have learned the importance of bringing together different communities around the same scientific cause. It is clearer than ever that automated text analysis is not the solution, but rather one tool among many that can be directed to combat the Covid-19 epidemic. Crucially, the systems and tools we build must be designed to serve a use case, whether that's improving information retrieval for clinicians and medical professionals, summarizing the conclusions of the latest observational research or clinical trials, or converting these learnings to a format that is easily digestible by healthcare consumers.</p>
<h2>Acknowledgments</h2>
<p>This work was supported in part by NSF Convergence Accelerator award 1936940, ONR grant N00014-18-1-2193, and the University of Washington WRF/Cable Professorship.</p>
<p>We thank The White House Office of Science and Technology Policy, the National Library of Medicine at the National Institutes of Health, Microsoft Research, Chan Zuckerberg Initiative, and Georgetown University's Center for Security and Emerging Technology for co-organizing the CORD-19 initiative. We thank Michael Kratsios, the Chief Technology Officer of the United States, and The White House Office of Science and Technology Policy for providing the initial seed set of questions for the Kaggle CORD-19 research challenge.</p>
<p>We thank Kaggle for coordinating the CORD19 research challenge. In particular, we acknowledge Anthony Goldbloom for providing feedback on CORD-19 and for involving us in discussions around the Kaggle literature review tables project. We thank the National Institute of Standards and Technology (NIST), National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and University of Texas Health Science Center at Houston (UTHealth) for coorganizing the TREC-COVID shared task. In particular, we thank our co-organizers - Steven Bedrick (OHSU), Aaron Cohen (OHSU), Dina Demner-Fushman (NLM), William Hersh (OHSU), Kirk Roberts (UTHealth), Ian Soboroff (NIST), and Ellen Voorhees (NIST) - for feedback on the design of CORD-19.</p>
<p>We acknowledge our partners at Elsevier and Springer Nature for providing additional full text coverage of papers included in the corpus.</p>
<p>We thank Bryan Newbold from the Internet Archive for providing feedback on data quality and helpful comments on early drafts of the manuscript.</p>
<p>We thank Rok Jun Lee, Hrishikesh Sathe, Dhaval Sonawane and Sudarshan Thitte from IBM Watson AI for their help in table parsing.</p>
<p>We also acknowledge and thank our collaborators from AI2: Paul Sayre and Sam Skjonsberg for providing front-end support for CORD-19 and TREC-COVID, Michael Schmitz for setting up the CORD-19 Discourse community forums, Adriana Dunn for creating webpage content and marketing, Linda Wagner for collecting community feedback, Jonathan Borchardt, Doug Downey, Tom Hope, Daniel King, and Gabriel Stanovsky for contributing supplemental data to the CORD-19 effort, Alex Schokking for his work on the Semantic Scholar Covid-19 Research Feed, Darrell Plessas for technical support, and Carissa Schoenick for help with public relations.</p>
<h2>References</h2>
<p>Sabber Ahamed and Manar D. Samad. 2020. Information mining for covid-19 research from a large volume of scientific literature. ArXiv, abs/2004.02085.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36153620, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Olivier Bodenreider. 2004. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32 Database issue:D267-70.
Q. Chen, Y. Peng, and Z. Lu. 2019. Biosentvec: creating sentence embeddings for biomedical texts. In 2019 IEEE International Conference on Healthcare Informatics (ICHI), pages 1-5.</p>
<p>Qingyu Chen, Alexis Allot, and Zhiyong Lu. 2020. Keep up with the latest coronavirus research. $N a$ ture, 579:193 - 193.</p>
<p>Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020. Specter: Document-level representation learning using citation-informed transformers. In $A C L$.</p>
<p>Donald C. Comeau, Chih-Hsuan Wei, Rezarta Islamaj Dogan, and Zhiyong Lu. 2019. Pmc text mining subset in bioc: about three million full-text articles and growing. Bioinformatics.</p>
<p>Radu Crisan-Dabija, Cristina Grigorescu, Cristina Alice Pavel, Bogdan Artene, Iolanda Valentina Popa, Andrei Cernomaz, and Alexandru Burlacu. 2020. Tuberculosis and covid-19 in 2020: lessons from the past viral outbreaks and possible future outcomes. medRxiv.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Luis Espinosa-Anke and Steven Schockaert. 2018. SeVeN: Augmenting word embeddings with unsupervised relation vectors. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2653-2665, Santa Fe, New Mexico, USA. Association for Computational Linguistics.
M. Fathi, Khatoon Vakili, Fatemeh Sayehmiri, Abdolrahman Mohamadkhani, M. Hajiesmaeili, Mostafa Rezaei-Tavirani, and Owrang Eilami. 2020. Prognostic value of comormidity for severity of covid19: A systematic review and meta-analysis study. In medRxiv.</p>
<p>Yang Han, Victor O.K. Li, Jacqueline C.K. Lam, Peiyang Guo, Ruiqiao Bai, and Wilton W.T. Fok. 2020. Who is more susceptible to covid-19 infection and mortality in the states? medRxiv.</p>
<p>Torsten Hothorn, Marie-Charlotte Bopp, H. F. Guenthard, Olivia Keiser, Michel Roelens, Caroline E Weibull, and Michael J Crowther. 2020. Relative coronavirus disease 2019 mortality: A swiss population-based study. In medRxiv.</p>
<p>Karen Sprck Jones, Steve Walker, and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments - part 1. Inf. Process. Manag., 36:779-808.</p>
<p>Shubhi Kaushik, Scott I. Aydin, Kim R. Derespina, Prerna Bansal, Shanna Kowalsky, Rebecca Trachtman, Jennifer K. Gillen, Michelle M. Perez, Sara H. Soshnick, Edward E. Conway, Asher Bercow, Howard S. Seiden, Robert H Pass, Henry Michael Ushay, George Ofori-Amanfo, and Shivanand S Medar. 2020. Multisystem inflammatory syndrome in children (mis-c) associated with sars-cov-2 infection: A multi-institutional study from new york city. The Journal of Pediatrics.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics.</p>
<p>Yuxiao Liang and Pengtao Xie. 2020. Identifying radiological findings related to covid-19 from medical literature. ArXiv, abs/2004.01862.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of ACL.</p>
<p>Patrice Lopez. 2009. Grobid: Combining automatic bibliographic data recognition and term extraction for scholarship publications. In ECDL.</p>
<p>Luis Lpez-Fando, Paulina Bueno, David Snchez Carracedo, Mrcio Augusto Averbeck, David Manuel Castro-Daz, emmanuel chartier-kastler, Francisco Cruz, Roger R Dmochowski, Enrico Finazzi-Agr, Sakineh Hajebrahimi, John Heesakkers, George R Kasyan, Tufan Tarcan, Benot Peyronnet, Mauricio Plata, Brbara Padilla-Fernndez, Frank Van der Aa, Salvador Arlandis, and Hashim Hashim. 2020. Management of female and functional urology patients during the covid pandemic. European Urology Focus.</p>
<p>Ryan McDonald, Georgios-Ioannis Brokos, and Ion Androutsopoulos. 2018. Deep relevance ranking using enhanced document-query interactions. In EMNLP.</p>
<p>Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319-327, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sravanthi Parasa, Madhav Desai, Viveksandeep Thoguluva Chandrasekar, Harsh Patel, Kevin Kennedy, Thomas Rsch, Marco Spadaccini, Matteo Colombo, Roberto Gabbiadini, Everson L. A. Artifon, Alessandro Repici, and Prateek Sharma. 2020. Prevalence of gastrointestinal symptoms and fecal viral shedding in patients with coronavirus disease 2019. JAMA Network Open, 3.</p>
<p>Iolanda Valentina Popa, Mircea Diculescu, Catalina Mihai, Cristina Cijevschi-Prelipcean, and Alexandru Burlacu. 2020. Covid-19 and inflammatory bowel diseases: risk assessment, shared molecular pathways and therapeutic challenges. medRxiv.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen Voorhees, Lucy Lu Wang, and William R Hersh. 2020. TREC-COVID: Rationale and Structure of an Information Retrieval Shared Task for COVID19. Journal of the American Medical Informatics Association. Ocaa091.</p>
<p>Sepideh Sadegh, Julian Matschinske, David B. Blumenthal, Gihanna Galindez, Tim Kacprowski, Markus List, Reza Nasirigerdeh, Mhaned Oubounyt, Andreas Pichlmair, Tim Daniel Rose, Marisol Salgado-Albarrn, Julian Spth, Alexey Stukalov, Nina K. Wenke, Kevin Yuan, Josch K. Pauling, and Jan Baumbach. 2020. Exploring the sars-cov-2 virus-host-drug interactome for drug repurposing.</p>
<p>Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4430-4441, Florence, Italy. Association for Computational Linguistics.</p>
<p>Zhihong Shen, Hao Ma, and Kuansan Wang. 2018. A web-scale system for scientific knowledge exploration. In Proceedings of ACL 2018, System Demonstrations, pages 87-92, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Silvia Stringhini, Ania Wisniak, Giovanni Piumatti, Andrew S. Azman, Stephen A Lauer, Hlne Baysson, David De Ridder, Dusan Petrovic, Stephanie Schrempft, Kailing Marcus, Sabine Yerly, Isabelle Arm Vernez, Olivia Keiser, Samia Hurst, Klara M Posfay-Barbe, Didier Trono, Didier Pittet, Laurent Gtaz, Franois Chappuis, Isabella Eckerle, Nicolas Vuilleumier, Benjamin Meyer, Antoine Flahault, Laurent Kaiser, and Idris Guessous. 2020. Seroprevalence of anti-sars-cov-2 igg antibodies in geneva, switzerland (serocov-pop): a populationbased study. Lancet (London, England).</p>
<p>George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R. Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artires, Axel-Cyrille Ngonga Ngomo, Norman Heino, ric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. In BMC Bioinformatics.</p>
<p>Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2020. TREC-COVID: Constructing a pandemic information retrieval test collection. SIGIR Forum, 54.</p>
<p>Kuansan Wang, Zhihong Shen, Chiyuan Huang, ChiehHan Wu, Yuxiao Dong, and Anshul Kanakia. 2020. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396413.</p>
<p>Kuansan Wang, Zhihong Shen, Chiyuan Huang, ChiehHan Wu, Darrin Eide, Yuxiao Dong, Junjie Qian, Anshul Kanakia, Alvin Chen, and Richard Rogahn. 2019. A review of microsoft academic services for science of science studies. Frontiers in Big Data, 2.</p>
<p>Xuan Wang, Xiangchen Song, Yingjun Guan, Bangzheng Li, and Jiawei Han. 2020. Comprehensive named entity recognition on cord-19 with distant or weak supervision. ArXiv, abs/2003.12218.</p>
<p>Leigh Weston, Vahe Tshitoyan, John Dagdelen, Olga Kononova, Kristin Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature.</p>
<p>Sally Yaacoub, Holger J Schnemann, Joanne Khabsa, Amena El-Harakeh, Assem M Khamis, Fatimah Chamseddine, Rayane El Khoury, Zahra Saad, Layal Hneiny, Carlos Cuello Garcia, Giovanna Elsa Ute Muti-Schnemann, Antonio Bognanni, Chen Chen, Guang Chen, Yuan Zhang, Hong Zhao, Pierre Abi Hanna, Mark Loeb, Thomas Piggott, Marge Reinap, Nesrine Rizk, Rosa Stalteri, Stephanie Duda, Karla Solo, Derek K Chu, and Elie A Akl. 2020. Safe management of bodies of deceased persons with suspected or confirmed covid-19: a rapid systematic review. BMJ Global Health, 5(5).</p>
<p>Xinyi Zheng, Doug Burdick, Lucian Popa, and Xin Ru Nancy Wang. 2020. Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. ArXiv, abs/2005.00589.</p>
<h2>A Table parsing results</h2>
<p>There is high variance in the representation of tables across different paper PDFs. The goal of table parsing is to extract all tables from PDFs and represent them in HTML table format, along with associated titles and headings. In Table 3, we provide several example table parses, showing the high diversity of table representations across documents, the structure of resulting parses, and some common parse errors.</p>
<p>| Effect |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{26}$ https://www.niso.org/publications/z3996-2019-jats
${ }^{27}$ https://www.loc.gov/bibframe/
${ }^{28}$ https://www.dublincore.org/specifications/dublincore/dces/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{24} \mathrm{https}$ ://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge
${ }^{25} \mathrm{https}$ ://ir.nist.gov/covidSubmit/index.html&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8538 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8538</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8538</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-bdabacc164b778c66ddbdee193349d7baf70d438</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bdabacc164b778c66ddbdee193349d7baf70d438" target="_blank">A Configurable Library for Generating and Manipulating Maze Datasets</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks, which supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models.</p>
                <p><strong>Paper Abstract:</strong> Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in research applications.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Maze Transformer Interpretability <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis <em>(Rating: 2)</em></li>
                <li>Datasets for Studying Generalization from Easy to Hard Examples <em>(Rating: 1)</em></li>
                <li>Evaluating ChatGPT and GPT-4 for Visual Programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8538",
    "paper_id": "paper-bdabacc164b778c66ddbdee193349d7baf70d438",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Maze Transformer Interpretability",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
            "rating": 2
        },
        {
            "paper_title": "Datasets for Studying Generalization from Easy to Hard Examples",
            "rating": 1
        },
        {
            "paper_title": "Evaluating ChatGPT and GPT-4 for Visual Programming",
            "rating": 1
        }
    ],
    "cost": 0.005528,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Configurable Library for Generating and Manipulating Maze Datasets</h1>
<p>Michael Igorevich Ivanitskiy ${ }^{<em>, 1}$, Rusheb Shah, Alex F. Spies ${ }^{2}$, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn ${ }^{1}$, and Samy Wu Fung ${ }^{1}$<br></em>Corresponding Author: mivanits@umich.edu<br>${ }^{1}$ Colorado School of Mines, Department of Applied Mathematics and Statistics<br>${ }^{2}$ Imperial College London</p>
<h4>Abstract</h4>
<p>Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present maze-dataset, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in research applications.</p>
<h2>1 Introduction</h2>
<p>Out-of-distribution generalization is a critical challenge in modern machine learning (ML) research. For interpretability and behavioral research in this area, training on algorithmic tasks offers benefits by allowing systematic data generation and task decomposition, as well as simplifying the process of circuit discovery [19]. Although mazes are well suited for these investigations, we have found that existing maze generation packages [1, 7, 4, 16, 21] do not provide support in flexibility of maze generation algorithms with fine-grained control of generation parameters and the ability to easily transform between multiple representations of the mazes (Images, Textual, Tokenized) for training and testing models.</p>
<p>This work aims to facilitate deeper research into generalization and interpretability by addressing these limitations. We introduce maze-dataset, an accessible Python package [10]. This package offers flexible configuration options for maze dataset generation, allowing users to select from a range of algorithms and adjust corresponding parameters (Section 2). Furthermore, it supports various output formats tailored to different ML architectures (Section 3).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example mazes from various algorithms. Left to right: randomized depth-first search (RDFS), RDFS without forks, constrained RDFS, Wilson's [23], RDFS with percolation ( $p=0.1$ ), RDFS with percolation ( $p=0.4$ ), random stack RDFS. Further examples available in the appendix of this work (Section 8).</p>
<h1>2 Maze Generation and Usage</h1>
<p>Our package can be installed from PyPi via pip install maze-dataset, or directly from the git repository [10].
To create a dataset, we first create a MazeDatasetConfig configuration object, which specifies the seed, number, and size of mazes, as well as the generation algorithm and its corresponding parameters. This object is passed to a MazeDataset class to create a dataset. Crucially, this MazeDataset inherits from a PyTorch [17] Dataset, and can thus be easily incorporated into existing data pre-processing and training pipelines, e.g., through the use of a Dataloader class.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">maze_dataset</span><span class="w"> </span><span class="kn">import</span> <span class="n">MazeDataset</span><span class="p">,</span> <span class="n">MazeDatasetConfig</span><span class="p">,</span> <span class="n">LatticeMazeGenerators</span>
<span class="n">cfg</span><span class="p">:</span> <span class="n">MazeDatasetConfig</span> <span class="o">=</span> <span class="n">MazeDatasetConfig</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;example&quot;</span><span class="p">,</span>
    <span class="n">grid_n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">n_mazes</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">maze_ctor</span><span class="o">=</span><span class="n">LatticeMazeGenerators</span><span class="o">.</span><span class="n">gen_dfs</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">dataset</span><span class="p">:</span> <span class="n">MazeDataset</span> <span class="o">=</span> <span class="n">MazeDataset</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</code></pre></div>

<p>When initializing mazes, further configuration options can be specified through the from_config() factory method as necessary. Options include 1) whether to generate the dataset during runtime or load an existing dataset, 2) if and how to parallelize generation, and 3) where to store the generated dataset. Full documentation of this is available in our repository [10]. Available maze generation algorithms are static methods of the LatticeMazeGenerators class and include the following:</p>
<ul>
<li>gen_dfs (randomized depth-first search): Parameters can be passed to constrain the number of accessible cells, the number of forks in the maze, and the maximum tree depth. Creates a spanning tree by default or a partially spanning tree if constrained.</li>
<li>gen_wilson (Wilson's algorithm): Generates a random spanning tree via loop-erased random walk [23].</li>
<li>gen_percolation (percolation): Starting with no connections, every possible lattice connection is set to either true or false with some probability $p$, independently of all other connections. For the kinds of graphs that this process generates, we refer to existing work [3, 5].</li>
<li>gen_dfs_percolation (randomized depth-first search with percolation): A connection exists if it exists in a maze generated via gen_dfs OR gen_percolation. Useful for generating mazes that are not acyclic graphs.</li>
</ul>
<p>Furthermore, a dataset of mazes can be filtered to satisfy certain properties:</p>
<div class="codehilite"><pre><span></span><code><span class="n">dataset_filtered</span><span class="o">:</span><span class="w"> </span><span class="n">MazeDataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dataset</span><span class="o">.</span><span class="na">filter_by</span><span class="o">.</span><span class="na">path_length</span><span class="o">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">3</span><span class="o">)</span>
</code></pre></div>

<p>Custom filters can be specified, and several filters are included:</p>
<ul>
<li>path_length (min_length: int): shortest length from the origin to target should be at least min_length.</li>
<li>start_end_distance (min_distance: int): Manhattan distance between start and end should be at least min_distance, ignoring walls.</li>
<li>remove_duplicates(...): remove mazes which are similar to others in the dataset, measured via Hamming distance.</li>
<li>remove_duplicates_fast(): remove mazes which are exactly identical to others in the dataset.</li>
</ul>
<p>All implemented maze generation algorithms are stochastic by nature. For reproducibility, the seed parameter of MazeDatasetConfig may be set. In practice, we do not find that exact duplicates of mazes are generated with any meaningful frequency, even when generating large datasets.</p>
<h1>3 Output Formats</h1>
<p>Internally, mazes are SolvedMaze objects, which have path information, and a connection list optimized for storing sub-graphs of a lattice. These objects can be converted to and from several formats.</p>
<h2>as_ascii()</h2>
<p>Simple text format for displaying mazes, useful for debugging in a terminal environment.
as_pixels()
numpy array of dtype=uint8 and shape (height, width, 3). The last dimension is RGB color.
<img alt="img-1.jpeg" src="img-1.jpeg" />
as_tokens()
Text-based formats intended for autoregressive transformers. Several deterministic tokenization schemes (Section 3.1) with scalable vocabularies are provided. The order of the adjacency list is, by default, randomized. Also included are utilities for colorizing the tokens according to the part of the task they represent. Colorizing can be done for HTML, LaTeX, or terminal output.</p>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;</span><span class="n">ADJLIST_START</span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="o">&lt;</span><span class="n">ADJLIST_END</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">ORIGIN_START</span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="n">ORIGIN_END</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">TARGET_START</span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="n">TARGET_END</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">PATH_START</span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">4</span><span class="p">)</span>
<span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="n">PATH_END</span><span class="o">&gt;</span>
</code></pre></div>

<p>Figure 2: Various output formats. Top row (left to right): ASCII diagram, rasterized pixel grid, and advanced display. Bottom row: text format for autoregressive networks.</p>
<h3>3.1 Training and Evaluation</h3>
<p>There are examples in the literature for training Recurrent Convolutional Neural Network (RCNN) derived architectures on maze tasks [20]. To this end, we replicate the format of [21] and provide the RasterizedMazeDataset class, which returns rasterized pairs of (input, target) mazes as shown in Figure 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Input is the rasterized maze without the path marked (left), and provide as a target the maze with all but the correct path removed. Configuration options exist to adjust whether endpoints are included and if empty cells should be filled in.</p>
<p>To train autoregressive text models such as transformers, we use the full sequences provided by as_tokens() shown in Figure 2. During deployment we provide only the prompt up to the $&lt;$ PATH_START $&gt;$ token. To map the vocabulary onto indices, we first allocate a portion of the indices for the "special" tokens which these do not represent coordinates. Next, we add each coordinate as a unique token. Coordinates are ordered in the vocabulary such that a maze of size $m$ will be processed the same way as the top $m \times m$ cells of a size- $n$ maze, where $n&gt;m$. This is done so that models can be deployed on mazes smaller than the training size without destroying the structure of the vocabulary. Examples of usage of this dataset to train autoregressive transformers can be found in our maze-transformer library [11]. Other tokenization and vocabulary schemes are also included, such as representing each coordinate as a pair of $i, j$ index tokens.</p>
<p>| <ADJLIST_START> $(0,0) \leftarrow&gt;:(1,0) ;(2,0) \leftarrow&gt;:(3,0) ;(4,1) \leftarrow&gt;:(4,0) ;(2,0) \leftarrow&gt;:(2,1) ;$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  $(1,0) \leftarrow&gt;:(1,1) ;(3,4) \leftarrow&gt;:(2,4) ;(4,2) \leftarrow&gt;:(4,3) ;(0,0) \leftarrow&gt;:(0,1) ;(0,3) \leftarrow&gt;:(0,2) ;$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  $(4,4) \leftarrow&gt;:(3,4) ;(4,3) \leftarrow&gt;:(4,4) ;(4,1) \leftarrow&gt;:(4,2) ;(2,1) \leftarrow&gt;:(2,2) ;(1,4) \leftarrow&gt;:(0,4) ;$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  $(1,2) \leftarrow&gt;:(0,2) ;(2,4) \leftarrow&gt;:(2,3) ;(4,0) \leftarrow&gt;:(3,0) ;(2,2) \leftarrow&gt;:(3,2) ;(1,2) \leftarrow&gt;:(2,2) ;$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  $(1,3) \leftarrow&gt;:(0,3) ;(3,2) \leftarrow&gt;:(3,3) ;(0,2) \leftarrow&gt;:(0,1) ;(3,1) \leftarrow&gt;:(3,2) ;(1,3) \leftarrow&gt;:(1,4) ;$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  <ADJLIST_END> |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |</p>
<p>Figure 4: Left: maze prompt up to $&lt;$ PATH_START $&gt;$. Right: relative ordering of the cells in the vocabulary. Note that the top-left square of size $n \times n$ can be described using only the first $n^{2}$ tokens in the vocabulary.</p>
<h1>4 Benchmarks of Generation Speed</h1>
<p>We provide approximate benchmarks for relative generation time across various algorithms, parameter choices, maze sizes, and dataset sizes.</p>
<p>| Method \&amp; Parameters |  | Average time per maze (ms) |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| Generation algorithm | Generation parameters | all sizes | $\begin{gathered} \text { small } \ (g \leq 10) \end{gathered}$ | $\begin{gathered} \text { medium } \ (10<g \leq 32) \end{gathered}$ | $\begin{gathered} \text { large } \\ (g>32) \end{gathered}$ |
| gen_dfs | accessible_cells $=20$ | 2.4 | 2.4 | 2.6 | 2.4 |
| gen_dfs | do_forks=False | 3.0 | 2.4 | 3.7 | 3.8 |
| gen_dfs | max_tree_depth $=0.5$ | 4.5 | 2.2 | 4.9 | 11.6 |
| gen_dfs |  | 31.1 | 2.8 | 28.0 | 136.5 |
| gen_dfs_percolation | $\mathrm{p}=0.1$ | 53.9 | 3.6 | 42.5 | 252.9 |
| gen_dfs_percolation | $\mathrm{p}=0.4$ | 58.8 | 3.7 | 44.7 | 280.2 |
| gen_percolation | - | 59.1 | 3.3 | 43.6 | 285.2 |
| gen_wilson | - | 767.9 | 10.1 | 212.9 | 4530.4 |
| median (all runs) |  | 10.8 | 6.0 | 44.4 | 367.7 |
| mean (all runs) |  | 490.0 | 11.7 | 187.2 | 2769.6 |</p>
<p>Table 1: Average time to generate a single maze, averaged across multiple runs and dataset size. All benchmarks were run with parallelization disabled on a Intel i9-8950HK CPU.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Plots of maze generation time. Generation time scales exponentially with maze size for all algorithms (left). Generation time does not depend on the number of mazes being generated, and there is minimal overhead to initializing the generation process for a small dataset (right). Wilson's algorithm is notably less efficient than others and has high variance. Note that for both plots, values are averaged across all parameter sets for that algorithm, and parallelization is disabled.</p>
<h2>5 Implementation</h2>
<p>We refer to our GitHub repository [10] for documentation and up-to-date implementation details.</p>
<p>This package utilizes a simple, efficient representation of mazes. Using an adjacency list to represent mazes would lead to a poor lookup time of whether any given connection exists, whilst using a dense adjacency matrix would waste memory by failing to exploit the structure (e.g., only 4 of the diagonals would be filled in). Instead, we describe mazes with the following simple representation: for a d-dimensional lattice with r rows and c columns, we initialize a boolean array A = {0,1}<sup>d×r×c</sup>, which we refer to in the code as a connection_list. The value at A[0, i, j] determines whether a downward connection exists from node [i, j] to [i + 1, j]. Likewise, the value at A[1, i, j] determines whether a rightwards connection to [i, j + 1] exists. Thus, we avoid duplication of data about the existence of connections, at the cost of requiring additional care with indexing when looking for a connection upwards or to the left. Note that this setup allows for a periodic lattice.<sup>1</sup></p>
<p>To produce solutions to mazes, two points are selected uniformly at random without replacement from the connected component of the maze, and the A<sup>∗</sup> algorithm [8] is applied to find the shortest path between them.</p>
<p>Parallelization is implemented via the multiprocessing module in the Python standard library, and parallel generation can be controlled via keyword arguments to the MazeDataset.from_config() function.</p>
<h3>5.1 Relation to Existing Works</h3>
<p>As mentioned in the introduction, a multitude of public and open-source software packages exist for generating mazes [21, 4, 16]. However, our package provides more flexibility and efficiency in the following ways:</p>
<ul>
<li>For rigorous investigations of the response of a model to various distributional shifts, preserving metadata about the generation algorithm with the dataset itself is essential. To this end, our package efficiently stores the dataset along with its metadata in a single human-readable file [9]. This metadata is loaded when the dataset is retrieved from disk and reduces the complexity of discerning the parameters under which a dataset was created.</li>
<li>Prior works provide maze datasets in only a rasterized format, which is not suitable for training autoregressive text-based transformer models. As discussed in Section 3, our package provides these different formats natively.</li>
<li>Our package provides a selection of maze generation algorithms, which all write to a single unified format. All output formats are reversible, and operate to and from this unified format.</li>
</ul>
<p><sup>1</sup>That is, rather than a sub-graph of Z<sup>2</sup>, we are working on the lattice Z/rZ×Z/cZ. This is achieved by using modular arithmetic for indexing. Specifically, when considering connections from a node at position [i, j], the downward connection leads to the node at position [(i + 1)%r, j], and the rightward connection leads to the node at position [i, (j + 1)%c]. However, although our data structure supports this in principle, our algorithms for solving and visualizing the mazes do not. In practice, the last elements of A are always set to 0 to remove the possibility of periodic connections.</p>
<p>As mentioned in Section 3.1, we also include the RasterizedMazeDataset class in our codebase, which can exactly mimic the outputs provided in easy-to-hard-data [21]. Our as_ascii() method provides a format similar to that used in [22]. The text format provided by as_tokens() is similar to that of [14], but provides a custom tokenization scheme.</p>
<h1>5.2 Limitations of maze-dataset</h1>
<p>For simplicity, the package primarily supports mazes that are sub-graphs of a 2-dimensional rectangular lattice. Some support for higher-dimensional lattices is present, but not all output formats are adapted for higher dimensional mazes. As mentioned in Section 5, our codebase does not fully support utilizing the periodic structure allowed by the data structure representing the maze. Since the use of $A^{*}$ described in Section 5 does not have a preference between two paths of equal length, solutions to mazes which are not acyclic may not always be unique.</p>
<h2>6 Conclusion</h2>
<p>The maze-dataset library [10] introduced in this paper provides a flexible and extensible toolkit for generating, processing, and analyzing maze datasets. By supporting various procedural generation algorithms and conversion utilities, it enables the creation of mazes with customizable properties to suit diverse research needs. Planned improvements to the maze-dataset include adding more generation algorithms (such as Prim's algorithm [12, 18, 2] and Kruskal's algorithm [13], among others [6]), adding the ability to augment a maze with an adjacency list to add "shortcuts" to the maze, and resolving certain limitations detailed in Section 5.2. Future work will make extensive use of this library to study interpretability and out-of-distribution generalization in autoregressive transformers [11], recurrent convolutional neural networks [20], and implicit networks [15].</p>
<h2>7 Acknowledgements</h2>
<p>First and foremost, the authors would like to thank each other for the good times had in developing this library and the subsequent research which was carried out. We are also indebted to AI Safety Camp and AI Safety Support for supporting this project and bringing many of the authors together. This work was partially funded by National Science Foundation award DMS-2309810. We thank the Mines Optimization and Deep Learning group (MODL) for fruitful discussions.</p>
<h1>References</h1>
<p>[1] Karl Cobbe et al. "Leveraging Procedural Generation to Benchmark Reinforcement Learning". In: arXiv preprint arXiv:1912.01588 (2019).
[2] Edsger Wybe Dijkstra. "A Note on Two Problems in Connexion with Graphs:(Numerische Mathematik, 1 (1959), p 269-271)". In: (1959).
[3] Hugo Duminil-Copin. Sixty Years of Percolation. Dec. 2017. eprint: 1712 . 04651 (math-ph). URL: http : //arxiv.org/abs/1712.04651 (visited on 09/02/2023).
[4] Emad Ehsan. Maze. 2022. URL: https://github.com/emadehsan/maze.
[5] Michael E. Fisher and John W. Essam. "Some Cluster Size and Percolation Problems". In: Journal of Mathematical Physics 2.4 (Dec. 2004), pp. 609-619. ISSN: 0022-2488. DOI: 10.1063/1.1703745. URL: https : $/ /$ doi.org/10.1063/1.1703745 (visited on 09/02/2023).
[6] Gabrovšek. "Analysis of Maze Generating Algorithms". In: IPSI Transactions on Internet Research. Vol. 15.1. Jan. 2019, pp. 23-30. URL: http://www.ipsitransactions.org/journals/papers/tir/2019jan/p5. pdf.
[7] Luke Harries et al. "MazeExplorer: A Customisable 3D Benchmark for Assessing Generalisation in Reinforcement Learning". In: 2019 IEEE Conf. Games CoG. IEEE Press, pp. 1-4.
[8] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. "A Formal Basis for the Heuristic Determination of Minimum Cost Paths". In: IEEE Transactions on Systems Science and Cybernetics 4.2 (July 1968), pp. 100-107. ISSN: 2168-2887. DOI: 10.1109/TSSC.1968.300136.
[9] Michael Ivanitskiy. ZANJ. URL: https://github.com/mivanit/ZANJ.
[10] Michael I. Ivanitskiy et al. Maze Dataset. 2023. URL: https://github.com/understanding-search/ maze-dataset.
[11] Michael I. Ivanitskiy et al. Maze Transformer Interpretability. 2023. URL: https : / / github . com / understanding-search/maze-transformer.
[12] Vojtech Jarnık. "About a Certain Minimal Problem". In: Práce Moravské Pırodovedecké Spolecnosti 6 (1930), pp. $57-63$.
[13] Joseph B Kruskal. "On the shortest spanning subtree of a graph and the traveling salesman problem". In: Proceedings of the American Mathematical society 7.1 (1956), pp. 48-50. DOI: 10.1090/S0002-9939-1956-0078686-7.
[14] Chang Liu and Bo Wu. "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis". In: arXiv preprint arXiv:2308.11224 (2023).
[15] Daniel McKenzie, Samy Wu Fung, and Howard Heaton. "Faster predict-and-optimize with three-operator splitting". In: arXiv preprint arXiv:2301.13395 (2023).
[16] Ferenc Németh. maze-generation-algorithms. 2019. URL: https://github.com/ferenc-nemeth/maze-generation-algorithms.
[17] Adam Paszke et al. "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: Advances in Neural Information Processing Systems 32. Ed. by H. Wallach et al. Curran Associates, Inc., 2019, pp. 80248035. URL: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
[18] Robert Clay Prim. "Shortest Connection Networks and Some Generalizations". In: The Bell System Technical Journal 36.6 (1957), pp. 1389-1401.
[19] Tilman Räuker et al. "Toward transparent ai: A survey on interpreting the inner structures of deep neural networks". In: 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE. 2023, pp. $464-483$.
[20] Avi Schwarzschild et al. "Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks". In: Advances in Neural Information Processing Systems 34 (2021), pp. 6695-6706.
[21] Avi Schwarzschild et al. Datasets for Studying Generalization from Easy to Hard Examples. Sept. 2021. DOI: 10.48550/arXiv.2108.06011. arXiv: 2108.06011 [cs]. URL: http://arxiv.org/abs/2108.06011 (visited on 09/03/2023).
[22] Adish Singla. "Evaluating ChatGPT and GPT-4 for Visual Programming". In: arXiv preprint arXiv:2308.02522 (2023).
[23] David Bruce Wilson. "Generating Random Spanning Trees More Quickly than the Cover Time". In: Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing - STOC '96. Philadelphia, Pennsylvania, United States: ACM Press, 1996, pp. 296-303. ISBN: 978-0-89791-785-8. DOI: 10.1145/237814. 237880. URL: http://portal.acm.org/citation.cfm?doid=237814.237880.</p>
<h1>8 Appendix: Examples of Generated Mazes</h1>
<p>Figure 6: Results for Wilson's algorithm
demo-g4-rdi-a_wilson-h20657
get_wilsof/grid_n=41
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Results for randomized depth first search, with various parameters
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>            </div>
        </div>

    </div>
</body>
</html>
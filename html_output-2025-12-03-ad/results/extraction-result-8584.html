<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8584 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8584</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8584</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279251771</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.06955v4.pdf" target="_blank">BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8584.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8584.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIS Reasoning 1.0 (Belief-Inconsistent Syllogisms Reasoning Dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 5,000-example Japanese-language benchmark of syllogistic reasoning problems explicitly constructed so that logically valid conclusions contradict common beliefs, designed to probe belief bias in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment test of two-premise syllogisms in Japanese where every example is logically valid (correct answer 'はい'), but conclusions are deliberately belief-inconsistent to reveal belief bias.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Diagnostic binary classification in zero-shot setting using concise Japanese instruction prompts; additional targeted prompt engineering (chain-of-thought, focus_logic, polite, casual) used in error re-evaluation; logit-lens used for internal analysis of open models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used as the primary benchmark in the paper; models reported accuracies include GPT-4o 79.54%, GPT-4-turbo 59.48%, llm-jp-3-13b 59.86%, llm-jp-3-13b-instruct3 40.90%, stockmark-13b 40.34%, Claude-3-sonnet 20.34%, Claude-3-opus 7.18% (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Evaluations performed zero-shot, temperature=0, no fine-tuning; comparisons made across model families (general-purpose vs Japanese-specialized vs instruction-tuned) and with NeuBAROCO complementary samples to show dataset sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Task focuses only on syllogistic reasoning and uses binary scoring; prompt sensitivity affects recoverability; does not cover other reasoning types (causal, probabilistic); selection of topics may still bias results.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Designed to isolate belief bias: demonstrates that even state-of-the-art LLMs can fail when correct logical conclusions conflict with commonsense; highlights need for targeted benchmarks and prompt/architecture/finetuning strategies to mitigate belief bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8584.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's high-performing, multilingual general-purpose LLM used in the paper; shows strong robustness to belief-inconsistent syllogisms relative to other tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose, multilingual OpenAI model optimized for broad reasoning and instruction-following; reportedly trained with large-scale data and instruction tuning/RLHF (paper attributes strong reasoning to advanced training strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0 (syllogistic belief-inconsistent entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary logical entailment judgment (はい/いいえ) on Japanese two-premise syllogisms whose conclusions conflict with common beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation with concise Japanese instruction prompts (temperature=0); targeted prompt-engineering re-evaluation on 100 failure cases including explicit chain-of-thought prompting, focus_logic prompts, polite/ casual variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>79.54% accuracy on BIS Reasoning 1.0; 94.01% accuracy on NeuBAROCO belief-inconsistent subset. On the 100 error samples, CoT prompts recovered performance to 87% and focus_logic to 76% (accuracy on that subset).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed other evaluated models substantially (e.g., GPT-4-turbo ~59.5%, llm-jp-3-13b ~59.9%); explicit CoT and focus_logic prompts improved recovery from initial failures vs basic/casual prompts which gave negligible recovery (3-5%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still fails on ~20% of BIS examples; strongly prompt-sensitive—defaults to commonsense heuristics without explicit logical scaffolding; binary scoring does not reveal partial reasoning successes.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates latent reasoning capability that is unlocked by explicit CoT or logic-focused prompts; suggests training/architectural strategies plus explicit prompting can reduce belief bias but do not fully eliminate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8584.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's high-capability general-purpose model evaluated as part of the multilingual comparison; achieved substantially lower accuracy on BIS than GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose OpenAI model optimized for cost/performance trade-offs and instruction-following; multilingual capabilities but not specialized to Japanese.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment of logically valid yet belief-inconsistent Japanese syllogisms.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese instruction prompts, temperature=0, deterministic decoding; no fine-tuning applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>59.48% accuracy on BIS Reasoning 1.0; 67.66% on NeuBAROCO subset (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperformed GPT-4o by ~20 points on BIS; performed similarly to llm-jp-3-13b in BIS but lower than GPT-4o; NeuBAROCO scores also lower than GPT-4o indicating dataset sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Substantial failures on belief-inconsistent syllogisms; vulnerability to belief bias and prompt design untested in detailed ablations for this model in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Shows that not all OpenAI high-tier models share the same robustness; suggests training/configuration differences matter for mitigating belief bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8584.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude family model evaluated; shows strong performance on NeuBAROCO but very poor performance on the BIS Reasoning 1.0 dataset, indicating sensitivity to dataset formulation and possible suppression from alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's alignment-optimized LLM designed for safety and helpfulness; multilingual capabilities and instruction-following behavior influenced by alignment tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary logical entailment on Japanese belief-inconsistent syllogisms.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese instruction prompts, temperature=0; same evaluation protocol as other proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>20.34% accuracy on BIS Reasoning 1.0; 78.44% on NeuBAROCO (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Large discrepancy between BIS and NeuBAROCO suggests that Claude-3-sonnet's alignment or calibration helped on one benchmark but suppressed acceptance of logically valid yet counterintuitive conclusions on BIS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severely under-recognizes logically valid but belief-conflicting conclusions on BIS; possible alignment-driven suppression or safety calibration interference noted as likely cause.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Alignment-oriented training may reduce generation of implausible content but can harm strict logical validation when truth conflicts with intuitive beliefs; underscores trade-offs between safety/alignment and logical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8584.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3-opus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another Anthropic model variant included in evaluation; performed very poorly on BIS despite reasonable NeuBAROCO accuracy, reinforcing dataset-conditional behavior of aligned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Alignment-optimized Anthropic LLM variant emphasizing safe and helpful responses; multilingual and instruction-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment on Japanese syllogisms with conclusions contradicting common beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese prompts with deterministic decoding; evaluated identically across dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>7.18% accuracy on BIS Reasoning 1.0; 61.07% on NeuBAROCO (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than Claude-3-sonnet on BIS; both Claude variants show sharp drops on BIS relative to NeuBAROCO unlike GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Very low BIS acceptance rate of valid but counterintuitive conclusions; indicates strong susceptibility to alignment-induced suppression for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforces the paper's conclusion that alignment and safety tuning can inadvertently impair strict logical reasoning when truth conflicts with human priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8584.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llm-jp-3-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llm-jp/llm-jp-3-13b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter open-source Japanese-optimized LLM evaluated in the paper, showing moderate performance on BIS and analyzed via logit-lens to probe internal reasoning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llm-jp-3-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open, Japanese-optimized 13B-parameter model produced by the LLM-JP project; trained/fine-tuned with Japanese corpora to improve native performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary judgment of syllogistic entailment in Japanese where valid conclusions contradict common beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese instruction prompts, deterministic decoding; additional internal analysis with the logit-lens across layers to inspect token-level prediction dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>59.86% accuracy on BIS Reasoning 1.0; 67.66% on NeuBAROCO (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparable BIS performance to GPT-4-turbo (~59-60%) but below GPT-4o; logit-lens analysis contrasted its gradual buildup of conclusion signal with stockmark model's premature confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails a substantial fraction of belief-inconsistent cases; binary scoring masks partial reasoning; model shows a particular pattern of slower layerwise buildup of answer signal which may reflect different inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Japanese optimization yields fluency but not guaranteed deductive robustness; internal logit-lens findings suggest architectural or training-induced differences in how reasoning signals accumulate across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8584.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llm-jp-3-13b-instruct3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llm-jp/llm-jp-3-13b-instruct3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned variant of the llm-jp-3-13b model intended to follow instructions more closely, evaluated and found to have lower reasoning robustness on BIS than its base counterpart.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llm-jp-3-13b-instruct3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of the 13B Japanese LLM intended to improve instruction following and helpfulness on Japanese tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary syllogistic entailment judgments in Japanese with belief-inconsistent conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese prompts, same evaluation protocol; compared directly to base llm-jp-3-13b to assess effect of instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>40.90% accuracy on BIS Reasoning 1.0; 38.32% on NeuBAROCO (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms the non-instructed llm-jp-3-13b (59.86%), indicating instruction-tuning did not improve and may have harmed deductive robustness for belief-inconsistent syllogisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large drop relative to base model suggests instruction/fine-tuning can amplify tendencies to favor plausibility or alignment over strict logical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Shows that fine-tuning/instruction-tuning geared toward helpfulness may degrade strict logical fidelity in cases of belief conflict; careful fine-tuning objectives are required to preserve deductive correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8584.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8584.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>stockmark-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>stockmark/stockmark-13b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter Japanese-focused open model evaluated in the study; exhibits premature confidence signals in logit-lens analysis and moderate-to-low accuracy on BIS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>stockmark/stockmark-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 13B Japanese-optimized LLM (Stockmark), trained/fine-tuned to handle Japanese tasks with high fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIS Reasoning 1.0</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment decisions on two-premise syllogisms with belief-inconsistent conclusions, presented in Japanese.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot Japanese prompts under standardized inference settings; internal analysis with logit-lens showing early and strong prediction probability at the Conclusion_End token.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>40.34% accuracy on BIS Reasoning 1.0; 47.90% on NeuBAROCO (accuracy, %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than llm-jp-3-13b (59.86%) despite same parameter count; logit-lens suggests premature confidence which may explain lower accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Premature high-confidence predictions and weak premise-boundary signaling lead to reasoning failures on belief-inconsistent examples; limited recoverability via simple prompting not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Internal layerwise dynamics (premature conclusion signal) may underlie some reasoning failures; highlights the value of interpretability techniques (logit-lens) to diagnose model-specific failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring reasoning biases in large language models through syllogism: Insights from the neubaroco dataset <em>(Rating: 2)</em></li>
                <li>JFLD: A Japanese benchmark for deductive reasoning based on formal logic <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Training a helpful and harmless assistant with reinforcement learning from human feedback <em>(Rating: 1)</em></li>
                <li>Language models show human-like content effects on reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8584",
    "paper_id": "paper-279251771",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "BIS Reasoning 1.0",
            "name_full": "BIS Reasoning 1.0 (Belief-Inconsistent Syllogisms Reasoning Dataset)",
            "brief_description": "A 5,000-example Japanese-language benchmark of syllogistic reasoning problems explicitly constructed so that logically valid conclusions contradict common beliefs, designed to probe belief bias in LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary entailment test of two-premise syllogisms in Japanese where every example is logically valid (correct answer 'はい'), but conclusions are deliberately belief-inconsistent to reveal belief bias.",
            "method_or_approach": "Diagnostic binary classification in zero-shot setting using concise Japanese instruction prompts; additional targeted prompt engineering (chain-of-thought, focus_logic, polite, casual) used in error re-evaluation; logit-lens used for internal analysis of open models.",
            "performance": "Used as the primary benchmark in the paper; models reported accuracies include GPT-4o 79.54%, GPT-4-turbo 59.48%, llm-jp-3-13b 59.86%, llm-jp-3-13b-instruct3 40.90%, stockmark-13b 40.34%, Claude-3-sonnet 20.34%, Claude-3-opus 7.18% (accuracy, %).",
            "baseline_comparison": "Evaluations performed zero-shot, temperature=0, no fine-tuning; comparisons made across model families (general-purpose vs Japanese-specialized vs instruction-tuned) and with NeuBAROCO complementary samples to show dataset sensitivity.",
            "limitations_or_failures": "Task focuses only on syllogistic reasoning and uses binary scoring; prompt sensitivity affects recoverability; does not cover other reasoning types (causal, probabilistic); selection of topics may still bias results.",
            "insights_or_conclusions": "Designed to isolate belief bias: demonstrates that even state-of-the-art LLMs can fail when correct logical conclusions conflict with commonsense; highlights need for targeted benchmarks and prompt/architecture/finetuning strategies to mitigate belief bias.",
            "uuid": "e8584.0",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "OpenAI's high-performing, multilingual general-purpose LLM used in the paper; shows strong robustness to belief-inconsistent syllogisms relative to other tested models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "General-purpose, multilingual OpenAI model optimized for broad reasoning and instruction-following; reportedly trained with large-scale data and instruction tuning/RLHF (paper attributes strong reasoning to advanced training strategies).",
            "model_size": null,
            "reasoning_task_name": "BIS Reasoning 1.0 (syllogistic belief-inconsistent entailment)",
            "reasoning_task_description": "Binary logical entailment judgment (はい/いいえ) on Japanese two-premise syllogisms whose conclusions conflict with common beliefs.",
            "method_or_approach": "Zero-shot evaluation with concise Japanese instruction prompts (temperature=0); targeted prompt-engineering re-evaluation on 100 failure cases including explicit chain-of-thought prompting, focus_logic prompts, polite/ casual variants.",
            "performance": "79.54% accuracy on BIS Reasoning 1.0; 94.01% accuracy on NeuBAROCO belief-inconsistent subset. On the 100 error samples, CoT prompts recovered performance to 87% and focus_logic to 76% (accuracy on that subset).",
            "baseline_comparison": "Outperformed other evaluated models substantially (e.g., GPT-4-turbo ~59.5%, llm-jp-3-13b ~59.9%); explicit CoT and focus_logic prompts improved recovery from initial failures vs basic/casual prompts which gave negligible recovery (3-5%).",
            "limitations_or_failures": "Still fails on ~20% of BIS examples; strongly prompt-sensitive—defaults to commonsense heuristics without explicit logical scaffolding; binary scoring does not reveal partial reasoning successes.",
            "insights_or_conclusions": "Demonstrates latent reasoning capability that is unlocked by explicit CoT or logic-focused prompts; suggests training/architectural strategies plus explicit prompting can reduce belief bias but do not fully eliminate failures.",
            "uuid": "e8584.1",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4-turbo",
            "name_full": "GPT-4-turbo",
            "brief_description": "OpenAI's high-capability general-purpose model evaluated as part of the multilingual comparison; achieved substantially lower accuracy on BIS than GPT-4o.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo",
            "model_description": "General-purpose OpenAI model optimized for cost/performance trade-offs and instruction-following; multilingual capabilities but not specialized to Japanese.",
            "model_size": null,
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary entailment of logically valid yet belief-inconsistent Japanese syllogisms.",
            "method_or_approach": "Zero-shot Japanese instruction prompts, temperature=0, deterministic decoding; no fine-tuning applied.",
            "performance": "59.48% accuracy on BIS Reasoning 1.0; 67.66% on NeuBAROCO subset (accuracy, %).",
            "baseline_comparison": "Underperformed GPT-4o by ~20 points on BIS; performed similarly to llm-jp-3-13b in BIS but lower than GPT-4o; NeuBAROCO scores also lower than GPT-4o indicating dataset sensitivity.",
            "limitations_or_failures": "Substantial failures on belief-inconsistent syllogisms; vulnerability to belief bias and prompt design untested in detailed ablations for this model in paper.",
            "insights_or_conclusions": "Shows that not all OpenAI high-tier models share the same robustness; suggests training/configuration differences matter for mitigating belief bias.",
            "uuid": "e8584.2",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Claude-3-sonnet",
            "name_full": "Claude-3-sonnet",
            "brief_description": "Anthropic's Claude family model evaluated; shows strong performance on NeuBAROCO but very poor performance on the BIS Reasoning 1.0 dataset, indicating sensitivity to dataset formulation and possible suppression from alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3-sonnet",
            "model_description": "Anthropic's alignment-optimized LLM designed for safety and helpfulness; multilingual capabilities and instruction-following behavior influenced by alignment tuning.",
            "model_size": null,
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary logical entailment on Japanese belief-inconsistent syllogisms.",
            "method_or_approach": "Zero-shot Japanese instruction prompts, temperature=0; same evaluation protocol as other proprietary models.",
            "performance": "20.34% accuracy on BIS Reasoning 1.0; 78.44% on NeuBAROCO (accuracy, %).",
            "baseline_comparison": "Large discrepancy between BIS and NeuBAROCO suggests that Claude-3-sonnet's alignment or calibration helped on one benchmark but suppressed acceptance of logically valid yet counterintuitive conclusions on BIS.",
            "limitations_or_failures": "Severely under-recognizes logically valid but belief-conflicting conclusions on BIS; possible alignment-driven suppression or safety calibration interference noted as likely cause.",
            "insights_or_conclusions": "Alignment-oriented training may reduce generation of implausible content but can harm strict logical validation when truth conflicts with intuitive beliefs; underscores trade-offs between safety/alignment and logical fidelity.",
            "uuid": "e8584.3",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Claude-3-opus",
            "name_full": "Claude-3-opus",
            "brief_description": "Another Anthropic model variant included in evaluation; performed very poorly on BIS despite reasonable NeuBAROCO accuracy, reinforcing dataset-conditional behavior of aligned models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3-opus",
            "model_description": "Alignment-optimized Anthropic LLM variant emphasizing safe and helpful responses; multilingual and instruction-tuned.",
            "model_size": null,
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary entailment on Japanese syllogisms with conclusions contradicting common beliefs.",
            "method_or_approach": "Zero-shot Japanese prompts with deterministic decoding; evaluated identically across dataset.",
            "performance": "7.18% accuracy on BIS Reasoning 1.0; 61.07% on NeuBAROCO (accuracy, %).",
            "baseline_comparison": "Performs worse than Claude-3-sonnet on BIS; both Claude variants show sharp drops on BIS relative to NeuBAROCO unlike GPT-4o.",
            "limitations_or_failures": "Very low BIS acceptance rate of valid but counterintuitive conclusions; indicates strong susceptibility to alignment-induced suppression for this task.",
            "insights_or_conclusions": "Reinforces the paper's conclusion that alignment and safety tuning can inadvertently impair strict logical reasoning when truth conflicts with human priors.",
            "uuid": "e8584.4",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "llm-jp-3-13b",
            "name_full": "llm-jp/llm-jp-3-13b",
            "brief_description": "A 13B-parameter open-source Japanese-optimized LLM evaluated in the paper, showing moderate performance on BIS and analyzed via logit-lens to probe internal reasoning dynamics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llm-jp-3-13b",
            "model_description": "Open, Japanese-optimized 13B-parameter model produced by the LLM-JP project; trained/fine-tuned with Japanese corpora to improve native performance.",
            "model_size": "13B",
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary judgment of syllogistic entailment in Japanese where valid conclusions contradict common beliefs.",
            "method_or_approach": "Zero-shot Japanese instruction prompts, deterministic decoding; additional internal analysis with the logit-lens across layers to inspect token-level prediction dynamics.",
            "performance": "59.86% accuracy on BIS Reasoning 1.0; 67.66% on NeuBAROCO (accuracy, %).",
            "baseline_comparison": "Comparable BIS performance to GPT-4-turbo (~59-60%) but below GPT-4o; logit-lens analysis contrasted its gradual buildup of conclusion signal with stockmark model's premature confidence.",
            "limitations_or_failures": "Fails a substantial fraction of belief-inconsistent cases; binary scoring masks partial reasoning; model shows a particular pattern of slower layerwise buildup of answer signal which may reflect different inductive biases.",
            "insights_or_conclusions": "Japanese optimization yields fluency but not guaranteed deductive robustness; internal logit-lens findings suggest architectural or training-induced differences in how reasoning signals accumulate across layers.",
            "uuid": "e8584.5",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "llm-jp-3-13b-instruct3",
            "name_full": "llm-jp/llm-jp-3-13b-instruct3",
            "brief_description": "An instruction-tuned variant of the llm-jp-3-13b model intended to follow instructions more closely, evaluated and found to have lower reasoning robustness on BIS than its base counterpart.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llm-jp-3-13b-instruct3",
            "model_description": "Instruction-tuned variant of the 13B Japanese LLM intended to improve instruction following and helpfulness on Japanese tasks.",
            "model_size": "13B",
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary syllogistic entailment judgments in Japanese with belief-inconsistent conclusions.",
            "method_or_approach": "Zero-shot Japanese prompts, same evaluation protocol; compared directly to base llm-jp-3-13b to assess effect of instruction tuning.",
            "performance": "40.90% accuracy on BIS Reasoning 1.0; 38.32% on NeuBAROCO (accuracy, %).",
            "baseline_comparison": "Underperforms the non-instructed llm-jp-3-13b (59.86%), indicating instruction-tuning did not improve and may have harmed deductive robustness for belief-inconsistent syllogisms.",
            "limitations_or_failures": "Large drop relative to base model suggests instruction/fine-tuning can amplify tendencies to favor plausibility or alignment over strict logical validity.",
            "insights_or_conclusions": "Shows that fine-tuning/instruction-tuning geared toward helpfulness may degrade strict logical fidelity in cases of belief conflict; careful fine-tuning objectives are required to preserve deductive correctness.",
            "uuid": "e8584.6",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "stockmark-13b",
            "name_full": "stockmark/stockmark-13b",
            "brief_description": "A 13B-parameter Japanese-focused open model evaluated in the study; exhibits premature confidence signals in logit-lens analysis and moderate-to-low accuracy on BIS.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "stockmark/stockmark-13b",
            "model_description": "Open-source 13B Japanese-optimized LLM (Stockmark), trained/fine-tuned to handle Japanese tasks with high fluency.",
            "model_size": "13B",
            "reasoning_task_name": "BIS Reasoning 1.0",
            "reasoning_task_description": "Binary entailment decisions on two-premise syllogisms with belief-inconsistent conclusions, presented in Japanese.",
            "method_or_approach": "Zero-shot Japanese prompts under standardized inference settings; internal analysis with logit-lens showing early and strong prediction probability at the Conclusion_End token.",
            "performance": "40.34% accuracy on BIS Reasoning 1.0; 47.90% on NeuBAROCO (accuracy, %).",
            "baseline_comparison": "Performs worse than llm-jp-3-13b (59.86%) despite same parameter count; logit-lens suggests premature confidence which may explain lower accuracy.",
            "limitations_or_failures": "Premature high-confidence predictions and weak premise-boundary signaling lead to reasoning failures on belief-inconsistent examples; limited recoverability via simple prompting not detailed.",
            "insights_or_conclusions": "Internal layerwise dynamics (premature conclusion signal) may underlie some reasoning failures; highlights the value of interpretability techniques (logit-lens) to diagnose model-specific failure modes.",
            "uuid": "e8584.7",
            "source_info": {
                "paper_title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring reasoning biases in large language models through syllogism: Insights from the neubaroco dataset",
            "rating": 2,
            "sanitized_title": "exploring_reasoning_biases_in_large_language_models_through_syllogism_insights_from_the_neubaroco_dataset"
        },
        {
            "paper_title": "JFLD: A Japanese benchmark for deductive reasoning based on formal logic",
            "rating": 2,
            "sanitized_title": "jfld_a_japanese_benchmark_for_deductive_reasoning_based_on_formal_logic"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "rating": 1,
            "sanitized_title": "training_a_helpful_and_harmless_assistant_with_reinforcement_learning_from_human_feedback"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning tasks",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning_tasks"
        }
    ],
    "cost": 0.01351425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning
14 Jul 2025</p>
<p>Ha-Thanh Nguyen nguyenhathanh@nii.ac.jp 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Chaoran Liu 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Qianying Liu 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Hideyuki Tachibana 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Su Myat Noe 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Yusuke Miyao 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Koichi Takeda 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>Sadao Kurohashi 
Research and Development Center for Large Language Models
NII
TokyoJapan</p>
<p>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning
14 Jul 2025EFC03A974893F543839BA3AFCE6C1B52arXiv:2506.06955v4[cs.CL]
We present BIS Reasoning 1.0, the first largescale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs).Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet beliefinconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora.We benchmark state-of-the-art models-including GPT models, Claude models, and leading Japanese LLMs-revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy.Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs.These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable performance on various natural language tasks, yet ensuring reliable logical reasoning remains an open challenge (Morishita et al., 2024;Nguyen et al., 2023).This issue is particularly critical in high-stakes domains such as law, healthcare, and scientific research, where even subtle reasoning errors can lead to severe consequences (Yan et al., 2025).Alarmingly, recent studies show that the persuasive fluency of models like GPT-4 can deceive users into trusting incorrect conclusions (Bajpai et al., 2024).Hence, rigorously evaluating and enhancing LLM reasoning accuracy is crucial before deploying them in domains demanding strict logical rigor.</p>
<p>A major concern in current LLM research is their susceptibility to human-like cognitive biases, notably the belief bias-accepting conclusions aligned with prior beliefs regardless of logical validity.This bias poses significant risks in applications requiring impartial logic.For example, Dasgupta et al. demonstrated that LLMs frequently endorse logically invalid arguments simply because their conclusions appear believable, highlighting a critical vulnerability.</p>
<p>Existing benchmarks for evaluating logical reasoning in LLMs exhibit several limitations.Most influential reasoning benchmarks are predominantly English-based (Li et al., 2023;Qin et al., 2019;Frohberg and Binder, 2022), creating a significant evaluation gap for languages such as Japanese.Although some Japanese datasets exist, like JFLD (Morishita et al., 2024), which tests formal logic isolated from real-world knowledge, and NeuBAROCO (Ozeki et al., 2024), which covers multiple biases but has limited belief-inconsistent content, no dedicated Japanese-language dataset explicitly targets belief-inconsistent reasoning.To address this gap, we introduce BIS Reasoning 1.0-the first Japanese dataset designed explicitly for assessing belief-inconsistent syllogistic reasoning in LLMs.BIS (Belief-Inconsistent Syllogisms) aims to evaluate whether models can uphold logical validity when correct conclusions conflict with typical beliefs or factual knowledge.Figure 1 shows a representative example from the BIS Dataset, illustrating how logically valid conclusions can defy intuitive beliefs.Specifically, our contributions include:</p>
<ol>
<li>BIS Reasoning 1.0 Dataset: We present a carefully curated collection of Japanese syllogistic reasoning problems explicitly constructed to challenge LLMs with logically valid conclusions that contradict common beliefs.This dataset enables the first targeted evaluation of belief-inconsistent reasoning capabilities in Japanese LLMs.</li>
</ol>
<p>Comprehensive Evaluation of Leading</p>
<p>LLMs: We benchmark state-of-the-art models-including GPT-4o, GPT-4-turbo, Claude, and prominent Japanese LLMs-under standardized conditions, providing the first systematic comparison of their performance on belief-inconsistent reasoning in Japanese.</p>
<p>Detailed Analysis of Performance and Bias:</p>
<p>Our analysis identifies significant performance gaps, highlighting that even advanced LLMs struggle disproportionately with beliefinconsistent problems.We quantify these biases, investigate specific syllogistic structures prone to errors, and examine how prompts and chain-of-thought approaches affect reasoning accuracy.</p>
<ol>
<li>Implications for Reliability in Real-World Applications: We discuss critical implications for deploying LLMs in safety-critical domains.BIS Reasoning 1.0 reveals vulnerabilities that standard benchmarks typically overlook, providing crucial insights for improving logical consistency and objectivity in real-world scenarios like law, medicine, and scientific research.</li>
</ol>
<p>Overall, BIS Reasoning 1.0 contributes significantly to understanding and improving logical reasoning in Japanese-language LLMs.By explicitly evaluating belief-inconsistent reasoning, this work advances efforts toward creating reliable, biasresistant models suitable for deployment in highstakes environments.</p>
<p>Related Work</p>
<p>Evaluating the logical reasoning abilities of large language models (LLMs) has become a key research focus.Benchmarks like ReClor (Yu et al., 2025) and LogiQA (Liu et al., 2023) use multiplechoice logic problems derived from standardized exams to test inference beyond surface semantics.Despite advances such as chain-of-thought prompting and neuro-symbolic modeling (Kojima et al., 2022), LLMs still struggle to match human performance on tasks requiring rigorous logic.</p>
<p>LLMs not only make logical errors but also exhibit human-like cognitive biases.One wellstudied bias is belief bias-the tendency to accept conclusions that align with prior beliefs regardless of logical validity (Evans et al., 1983).Studies have shown that LLMs are more accurate on beliefconsistent reasoning tasks and frequently misjudge belief-inconsistent syllogisms (Ando et al., 2023;Ozeki et al., 2024).</p>
<p>Instruction tuning and RLHF can amplify these tendencies.For instance, models like GPT-4 and Claude, while highly fluent, may reinforce beliefaligned reasoning due to human preferences during fine-tuning (Bai et al., 2022).This further underscores the need for benchmarks that reveal latent cognitive biases and test reasoning under beliefconflicting conditions.</p>
<p>In the Japanese language, recent efforts have produced datasets for logical reasoning evaluation, yet limitations remain.JFLD (Morishita et al., 2024) focuses on formal deductive reasoning using artificially constructed propositions to isolate logic from world knowledge.While large in scale and diverse in structure, its use of semantically unnatural sentences prevents assessment of reasoning in realistic settings.JaNLI (Yanaka and Mineshima, 2021) and JAMP (Sugimoto et al., 2023) explore adversarial and temporal inference respectively, but they do not test belief-inconsistent reasoning and lack syllogistic structure.NeuBAROCO (Ozeki et al., 2024) closely aligns with our goals, exploring belief bias in syllogistic reasoning.However, it falls short as a comprehensive benchmark: the Japanese subset contains fewer than 800 examples for the NLI task and under 100 for the multiplechoice format, and it does not exclusively target belief-inconsistent reasoning.In contrast, BIS Reasoning 1.0 offers a focused and large-scale evaluation specifically designed to test logical robustness under belief-conflicting conditions.</p>
<p>These observations point to a significant gap: current Japanese benchmarks either employ unnatural representations, ignore belief-inconsistent logic, or lack scale and coverage of syllogistic forms.While prior work has identified belief bias in LLMs, comprehensive datasets for evaluating such bias in Japanese syllogistic reasoning remain scarce.BIS Reasoning 1.0 directly addresses this gap by providing a focused, large-scale, and naturalistic benchmark specifically designed to test how LLMs handle logically valid conclusions that conflict with intuitive beliefs.</p>
<p>Dataset Construction</p>
<p>We present BIS Reasoning 1.01 , a Japaneselanguage dataset consisting of 5,000 carefully constructed syllogistic reasoning problems.The dataset is specifically designed to test the robustness of logical inference in large language models (LLMs) under conditions of belief inconsistency, where logically valid conclusions explicitly contradict widely held commonsense beliefs.BIS Reasoning 1.0 serves as a diagnostic benchmark for probing whether LLMs can prioritize formal logic over prior-knowledge heuristics in natural language reasoning.</p>
<p>The dataset was developed through a formalized specification process aimed at ensuring both logical rigor and linguistic quality.Each example comprises two premises and one conclusion that is strictly entailed by syllogistic rules, such as classic forms like "All A are B; All C are A; Therefore, All C are B." Crucially, the conclusions are deliberately chosen to conflict with general knowledge, encouraging model errors driven by belief bias.This design isolates the logical reasoning process from superficial semantic plausibility, exposing potential biases embedded in LLM training data.</p>
<p>To ensure linguistic fluency and naturalness, all annotators involved in dataset construction were either native Japanese speakers or individuals with advanced Japanese proficiency.Initially, the dataset covers 46 distinct semantic categories (raw categories), ranging from concrete areas like animals, food, and weather to abstract domains such as law, logic, and emotion.These detailed raw categories were subsequently consolidated into 10 broader final categories to facilitate interpretability, ensure topic balance, and support higher-level reasoning analysis.Figure 2 summarizes the distribution and relationship between these raw and final categories, reflecting a concentration in cognitively rich areas valuable for probing belief-inconsistent reasoning in LLMs.</p>
<p>All examples underwent a two-phase quality assurance (QA) process, initially involving manual review of 10% of examples for iterative feedback, followed by comprehensive review to ensure structural validity, language clarity, and semantic diversity.Issues identified included syntactic violations, duplicated content, ambiguous premises, and category imbalance.</p>
<p>Experiments</p>
<p>General Settings</p>
<p>To evaluate how well LLMs handle logically valid yet belief-inconsistent inferences, we formulate BIS Reasoning 1.0 as a diagnostic test focusing strictly on logical judgment.Specifically, models must determine if a given conclusion logically follows from two premises, even when the conclusion contradicts intuitive beliefs.</p>
<p>We framed the task as a binary classification using concise, instruction-based prompts in Japanese, asking models to judge logical entailment by answering "はい" (Yes) or "いいえ" (No).Each prompt clearly stated two premises and one conclusion, accompanied by a brief system instruction reinforcing the model's logical reasoning role.Models were not required to provide explanations, thereby isolating pure logical inference capability from linguistic fluency or explanatory quality.</p>
<p>Accuracy is measured as the proportion of examples for which the model outputs the correct judgment-always "はい," since all BIS entries are logically valid.This setup ensures that errors stem from reasoning failures, not linguistic ambiguity or semantic bias.</p>
<p>All models in the experiments (see 4.2) were evaluated under identical zero-shot conditions, using Japanese-language prompts with consistent formatting.No fine-tuning or task-specific adaptation was applied.Evaluation was performed on the full dataset to eliminate sampling variance and enable direct comparison of out-of-the-box reasoning robustness.</p>
<p>Model Configuration</p>
<p>We evaluated prominent LLMs spanning both general-purpose and Japanese-specialized cate- gories.The general-purpose group included Ope-nAI's GPT-4o and GPT-4-turbo, as well as Anthropic's Claude-3-sonnet and Claude-3-opus.These models are designed for multilingual tasks and are optimized for general reasoning performance across domains.In contrast, the Japanese-specialized models-llm-jp-3-13b, llm-jp-3-13b-instruct3 (Aizawa et al., 2024), and stockmark/stockmark-13b2 -were trained or fine-tuned specifically on Japanese data, representing dedicated efforts to advance native Japanese LLM capabilities.</p>
<p>Proprietary models were accessed via official APIs, with consistent configurations including temperature set to zero to ensure deterministic outputs.Open-source models were deployed locally using standardized hardware and inference settings.All prompts were in Japanese, and all 5,000 examples in the dataset were evaluated without sampling.This setup guarantees both fairness and reproducibility across model comparisons.</p>
<p>Overall Model Performance</p>
<p>Table 1 summarizes the performance of each model on the BIS Reasoning 1.0 dataset, along with complementary results obtained on over 300 beliefinconsistent syllogistic reasoning samples from the NeuBAROCO benchmark.GPT-4o demonstrated the highest accuracy on both datasets, achieving 79.54% on BIS Reasoning 1.0 and notably excelling with 94.01% on NeuBAROCO.GPT-4-turbo and the Japanese-specialized model llm-jp-3-13b showed comparable results, each reaching around 59-60% accuracy on BIS Reasoning 1.0, but significantly lower than GPT-4o.Models specifically fine-tuned on Japanese instructions, such as llm-jp-3-13b-instruct3 and stockmark-13b, exhibited lower performance, around 40-48%, highlighting potential shortcomings in their reasoning robustness.Surprisingly, Anthropic's Claude models underperformed drastically on the BIS Reasoning 1.0 dataset, with Claude-3-sonnet and Claude-3-opus achieving only 20.34% and 7.18%, respectively, despite relatively strong performances of 78.44% and 61.07% on NeuBAROCO.These results indicate that superior performance in belief-inconsistent reasoning is heavily influenced by a model's underlying training approach and inherent architectural reasoning capabilities rather than mere specialization in Japanese or model size alone.Notably, GPT-4o's ability to consistently achieve high accuracy suggests a strong robustness to belief bias, allowing it to distinguish logical validity effectively despite intuitive contradictions.</p>
<p>The varying performances between datasets also highlight the sensitivity of model evaluations to task formulation and dataset characteristics.The relatively high NeuBAROCO scores of Claude models contrast sharply with their underperformance on BIS Reasoning 1.0, emphasizing the importance of employing diverse benchmarks to comprehensively assess model reasoning capabilities, particularly in challenging belief-inconsistent contexts.</p>
<p>Detailed Error Analysis for GPT-4o</p>
<p>To further investigate GPT-4o's behavior on beliefinconsistent syllogisms, we conducted additional experiments on 100 cases where GPT-4o initially failed.These error samples were reassessed using several carefully designed prompts, each emphasizing different reasoning approaches, linguistic styles, or explicit instructions about the beliefinconsistent nature of the task.Figure 3 summarizes the accuracy results across these varied prompts.</p>
<p>Prompt Impact Analysis The prompt emphasizing an explicit chain-of-thought (CoT) reasoning strategy yielded the highest accuracy improvement (87%) among the previously failed samples.This result clearly demonstrates GPT-4o's latent reasoning capabilities, significantly activated when explicitly guided through structured logical steps.</p>
<p>Similarly, the focus_logic prompt, explicitly mentioning the possibility of belief inconsistency and urging strict logical evaluation, substantially improved GPT-4o's accuracy (76%).This finding suggests that GPT-4o is sensitive to explicit instructional framing and context-setting, effectively reducing its reliance on superficial plausibility heuristics when directed accordingly.</p>
<p>Conversely, informal prompts (casual) and simple instruction (basic) achieved extremely low recovery rates (3% and 5%, respectively).These prompts appear insufficient in addressing GPT-4o's belief bias, indicating that the model defaults to common-sense heuristics without clear guidance.</p>
<p>The polite (敬語, keigo) prompt yielded a moderate accuracy improvement (65%), suggesting linguistic politeness cues may moderately encourage GPT-4o to engage in deeper reflection or careful reasoning, though less effectively than explicitly logical or CoT framing.</p>
<p>Repeating this experiment with Englishlanguage prompts while keeping the Japanese data constant yielded a similar performance pattern, as illustrated in Figure 4.However, the accuracy gaps were less pronounced compared to the Japanese prompts.This narrower gap in performance likely stems from GPT-4o being extensively trained in English, enhancing its robustness to varied prompt styles.Nevertheless, this confirms the key insight: prompt design significantly influences GPT-4o's performance on challenging tasks like belief-inconsistent syllogisms.Table 2 provides detailed descriptions of each prompt type used in the error re-evaluation.</p>
<p>Implications for Model Deployment These findings demonstrate the substantial impact of prompt engineering on GPT-4o's logical reasoning capabilities, particularly in overcoming belief bias.Although GPT-4o already performs strongly relative to other models, explicit prompting-such as instructing it to follow a structured reasoning process or clearly signaling the presence of beliefinconsistent content-markedly enhances its logical consistency.Consequently, when deploying LLMs, especially in contexts demanding precise and unbiased logical inference, strategic prompt design is crucial.Clear, structured instructions, emphasizing logical rigor and explicitly guiding step-by-step reasoning, can significantly mitigate intuitive biases inherent to the model, thus ensuring more reliable and accurate outcomes.</p>
<p>Logit Lens Analysis for Open Models</p>
<p>Based on the results in Table 1, we observe that the open model llm-jp-3-13b achieves competitive performance compared to commercial alternatives.To gain deeper insights into the model's internal state during the belief-inconsistent syllogistic reasoning task, we conduct an analysis using the logit lens technique.</p>
<p>We perform this analysis on two models:</p>
<p>llm-jp/llm-jp-3-13b and stockmark/stockmark-13b.</p>
<p>Both are open, Japanese-optimized models of equal size (13B parameters), making them ideal for controlled comparison.Figure 5 presents a representative example of the logit lens output.</p>
<p>The key observation is the model's behavior at the Conclusion_End token position.For stockmark/stockmark-13b, its prediction probability becomes high early in the network, suggesting a form of premature confidence.In contrast, llm-jp/llm-jp-3-13b shows a gradual increase in this signal across layers, indicating a delayed and layered buildup of reasoning.Additionally, the Premise1_End signal is notably weaker in stockmark, implying possible uncertainty in boundary detection between the premises and the conclusion.</p>
<p>Discussion</p>
<p>The results of our evaluation highlight important distinctions in the reasoning abilities of contemporary language models when confronted with beliefinconsistent syllogisms.GPT-4o's leading performance-nearly 80% accuracy-suggests that advanced training strategies, possibly including largescale instruction tuning and reasoning-augmented supervision, have enabled it to prioritize formal logic over commonsense intuition.Its effectiveness on a Japanese-language task, despite not being explicitly tailored for Japanese, underscores the strength of cross-lingual transfer when reasoning is properly grounded.In contrast, the poor performance of models like Claude-3-opus raises concerns about reasoning suppression in alignment-optimized architectures.While such models may be calibrated to avoid generating implausible outputs, this safety-oriented behavior appears to interfere with their ability to recognize and validate logically correct but counterintuitive conclusions.Similarly, the moderate but consistent performance of Japanese-specialized models suggests that strong language modeling does not necessarily imply strong reasoning.These models handle Japanese fluently yet often fail to maintain deductive consistency, particularly when logic conflicts with expectation.</p>
<p>Another notable finding is the absence of a positive correlation between model size and reasoning performance.Larger models in our evaluation, such as llm-jp-3-172b, underperformed their smaller counterparts, indicating that scale alone is not sufficient.Rather, performance on beliefinconsistent reasoning tasks appears to depend more on the nature of pretraining data, task-aligned fine-tuning, and perhaps architectural inductive biases that support structured inference.</p>
<p>Overall, our results reinforce the view that log-ical reasoning remains a distinct capability in language models-one that must be explicitly tested and optimized for, particularly in high-stakes domains where trust in correctness cannot rest solely on fluency or apparent plausibility.</p>
<p>Limitations</p>
<p>While BIS Reasoning 1.0 reveals important insights into the reasoning capabilities of LLMs, this study has several limitations that should be considered.First, the evaluation focuses exclusively on syllogistic reasoning.While syllogisms offer a controlled and interpretable format, they represent only one class of logical reasoning.The results may not generalize to other reasoning forms such as causal inference, probabilistic reasoning, or multi-hop deductive chains.Second, our evaluation relies on a single prompt design in a zero-shot setting.While this approach offers a consistent testbed, it may not fully capture the capabilities of models that perform better under more advanced prompting strategies or tailored task formulations.Prompt sensitivity remains an open variable, and performance may vary under alternative instructions or reasoning scaffolds.Third, our scoring metric is binary and does not account for partially correct reasoning or nearmisses.Models that correctly identify the logical structure but misword the conclusion, or those that reason correctly but fail to flag belief conflict, are treated the same as entirely incorrect responses.</p>
<p>In terms of model coverage, while we include both general-purpose and Japanese-specialized LLMs, our selection remains limited.Notably, open-source models outside of the LLM-JP and Stockmark ecosystems, as well as mid-sized multilingual models, are not represented in this evaluation.</p>
<p>Lastly, model capabilities are evolving rapidly.The results presented reflect the state of model behavior at a specific point in time (early 2025), and future model updates could yield different performance profiles.</p>
<p>Conclusion</p>
<p>We introduced BIS Reasoning 1.0, the first large-scale Japanese-language dataset explicitly focused on belief-inconsistent syllogistic reasoning.Through a comprehensive evaluation of state-ofthe-art language models, we demonstrated that reasoning under belief conflict remains a fundamental weakness-even for models with native-level Japanese capabilities.Among the models tested, GPT-4o achieved the highest accuracy, showing that architectural and training sophistication can help mitigate (though not eliminate) this form of reasoning bias.</p>
<p>Our results underscore a critical insight: language fluency does not guarantee logical soundness.The performance gap between model families challenges the assumption that scale or localization alone ensures robustness, and reinforces the importance of domain-targeted benchmarks for high-stakes applications.BIS Reasoning 1.0 fills this gap by offering a linguistically natural, cognitively demanding benchmark designed to expose belief-driven reasoning failures.</p>
<p>This work lays the groundwork for future research on belief bias mitigation, logic-aware model design, and dataset extensions across new reasoning paradigms and specialized domains.As LLMs become more deeply embedded in fields such as law, healthcare, and policy, BIS Reasoning 1.0 provides a practical tool for stress-testing their logical fidelity in scenarios where correctness must outweigh intuition.</p>
<p>Figure 1 :
1
Figure 1: A translated example from the BIS Dataset illustrating a belief-inconsistent syllogism: although the conclusion is logically valid, it contradicts common realworld beliefs.</p>
<p>Figure 2 :
2
Figure 2: Combined category analysis of top raw and final semantic categories in the BIS dataset.The figure illustrates the top 5 raw categories (left), top 5 final categories (right), and the heatmap shows the relationship between these raw and final categories, highlighting category intersections and distribution patterns.</p>
<p>Figure 3 :
3
Figure 3: Error Sample Accuracy by Prompt Type for GPT-4o</p>
<p>Figure 4 :
4
Figure 4: Prompt Type Accuracy on Error Samples (%) -Retest with English prompts</p>
<p>Figure 5: Logit lens visualization on the beliefinconsistent syllogistic reasoning task.</p>
<p>Table 1 :
1
Accuracy of models on the BIS Reasoning 1.0 dataset and NeuBAROCO belief-inconsistent syllogisms.
ModelBIS Reasoning 1.0 NeuBAROCOOpenAI/GPT-4o79.5494.01llm-jp/llm-jp-3-13b59.8667.66OpenAI/GPT-4-turbo59.4867.66llm-jp/llm-jp-3-13b-instruct340.9038.32stockmark/stockmark-13b40.3447.90Claude-3-sonnet-2024022920.3478.44Claude-3-opus-202402297.1861.07</p>
<p>Table 2 :
2
Descriptions of prompt types used for re-evaluating GPT-4o errors.</p>
<p>https://hf.co/datasets/nguyenthanhasia/BIS_ Reasoning_v1.0
https://hf.co/stockmark/stockmark-13b
AcknowledgementsWe would like to thank Yusuke Oda, Kiyomaru Hirokazu, Maki Matsuda, and Kouta Nakayama, among others, for their valuable input-ranging from ideas for evaluating large language models, suggestions that improved the literature review, to support in drafting annotation guidelines.
Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hiroyuki Deguchi, Rintaro Enomoto, Kazuki Fujii, Kensuke Fukumoto, Takuya Fukushima, arXiv:2407.03963Namgi Han, and 1 others. 2024. Llm-jp: A crossorganizational project for the research and development of fully open japanese llms. arXiv preprint</p>
<p>Evaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like biases. Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada, Proceedings of the 4th Natural Logic Meets Machine Learning Workshop. the 4th Natural Logic Meets Machine Learning WorkshopNancy, FranceAssociation for Computational Linguistics2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, arXiv:2204.058622022arXiv preprintDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, and 1 others</p>
<p>Can llms replace neil degrasse tyson? evaluating the reliability of llms as science communicators. Prasoon Bajpai, Niladri Chatterjee, Subhabrata Dutta, Tanmoy Chakraborty, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Chan, Antonia Hannah R Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.070512022arXiv preprint</p>
<p>On the conflict between logic and belief in syllogistic reasoning. B T St, Julie L Evans, Paul Barston, Pollard, Memory &amp; cognition. 1131983</p>
<p>CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models. Jörg Frohberg, Frank Binder, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. Jiaxuan Li, Lang Yu, Allyson Ettinger, 10.18653/v1/2023.acl-short.70Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, IEEE ACM Trans. Audio Speech Lang. Process. 2023</p>
<p>JFLD: A Japanese benchmark for deductive reasoning based on formal logic. Terufumi Morishita, Atsuki Yamaguchi, Gaku Morio, Hikaru Tomonari, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024Osamu Imaichi, and Yasuhiro Sogawa</p>
<p>Thanh Ha, Randy Nguyen, Francesca Goebel, Kostas Toni, Ken Stathis, Satoh, arXiv:2306.16638A negation detection assessment of gpts: analysis with the xnot360 dataset. 2023arXiv preprint</p>
<p>Exploring reasoning biases in large language models through syllogism: Insights from the neubaroco dataset. Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Counterfactual story reasoning and generation. Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, Yejin Choi, 10.18653/v1/D19-1509Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Jamp: Controlled Japanese temporal inference dataset for evaluating generalization capacity of language models. Tomoki Sugimoto, Yasumasa Onoe, Hitomi Yanaka, 10.18653/v1/2023.acl-srw.8Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20234Student Research Workshop)</p>
<p>LLM sensitivity evaluation framework for clinical diagnosis. Chenwei Yan, Xiangling Fu, Yuxuan Xiong, Tianyi Wang, Siu Cheung Hui, Ji Wu, Xien Liu, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAEAssociation for Computational Linguistics2025</p>
<p>Assessing the generalization capacity of pre-trained language models through japanese adversarial natural language inference. Hitomi Yanaka, Koji Mineshima, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2021</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
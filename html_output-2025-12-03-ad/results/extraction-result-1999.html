<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1999 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1999</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1999</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-276662518</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.07693v1.pdf" target="_blank">Fully Autonomous Programming Using Iterative Multi-Agent Debugging with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Program synthesis with Large Language Models (LLMs) suffers from a “near-miss syndrome”: The generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these tradeoffs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1999.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1999.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEIDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthesize, Execute, Instruct, Debug, and Repair (SEIDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid multi-agent program-synthesis framework that uses instruction‑tuned LLMs to synthesize and debug code in an iterative tree-search (beam/search-tree) loop, combined with ranking (parent selection) analogous to genetic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SEIDR</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>hybrid (LLM-based operators inside an evolutionary/search framework)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>SEIDR uses LLMs as the synthesis and mutation (repair) operators: a synth model to generate initial draft programs, an explain model to produce bug summaries, and a debug model to produce repaired program variants. The loop forms a tree search controlled by hyperparameters (population size W and per-parent branching N*), and ranking/parent-selection (tournament or lexicase) chooses programs for further repair. LLM calls are token-autoregressive completions (chat/instruction models) with temperature-based sampling; no online fine-tuning of the LLMs is performed during runs (adaptation is via different prompts, multiple sampled outputs, and branching settings).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program synthesis on PSB2 (Program Synthesis Benchmark 2) and HumanEval-X (HumanEval translated to multiple languages), experiments in Python and C++</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PushGP (traditional genetic programming baseline on PSB2), LLM-only generation baselines (Codex/GPT models without iterative repair), and variants of SEIDR (different tree arities, tournament vs lexicase selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>SEIDR with Codex (LLM used for synth+debug) achieved 19 problems solved out of 25 on PSB2-Python (pass@1000 reported) and 17/25 on PSB2-C++; with GPT-3.5 best SEIDR achieved 16/25 on PSB2-Python and 11/25 on PSB2-C++; on HumanEval-C++ SEIDR averaged pass@100 = 78.5% with GPT-3.5 and 84.2% with Llama 3-8B (average pass@100 reported over runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>PushGP (traditional genetic programming) solves 17/25 problems on PSB2 (as reported in the comparison baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>SEIDR itself is the hybrid operator; best SEIDR configuration (Codex, tree arity 10, max 1000 candidates) outperforms PushGP on PSB2-Python and is on par in PSB2-C++ (SEIDR: 19/25 Python vs PushGP:17/25; SEIDR:17/25 C++ vs PushGP:17/25).</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>Diversity is implicitly measured via multiple sampled repairs and beam/tree branching; lexicase selection is used to explicitly increase diversity of parents, but no numeric novelty/diversity metric (e.g., uniqueness counts) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Generalizability experiments across models and datasets show that SEIDR with code-specialized Codex performed best on PSB2, while GPT-3.5 and Llama 3 show mixed results; SEIDR with GPT-3.5 and Llama3 solved nearly all HumanEval-C++ problems across runs (163/164 and 162/164 respectively when taking the union over runs), but did not outperform state-of-the-art on HumanEval-Python. This demonstrates model- and benchmark-dependent generalization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Authors report evidence consistent with training-data effects: Codex (code-specialized) outperformed GPT-3.5 (more general) on PSB2; results on Python are more stable across runs possibly due to pretraining/training-distribution bias toward Python; the authors discuss 'data contamination' and benchmark popularity as partial explanations but cannot fully confirm due to lack of training-set visibility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>SEIDR: typical runs limited to generating up to 100 (generalizability experiments) or 1000 (initial Codex experiments) program candidates; reported wall-clock: ~42 hours for a full PSB2 run and ~156 hours for HumanEval-X per multi-factor experiment; monetary costs: ~550 USD for Codex+GPT-3 experiments, ~266 USD for GPT-3.5 experiments. PushGP/traditional GP: authors state PushGP requires orders-of-magnitude more program executions (they cite 'billions' of executions), e.g., PushGP considers 60 million program evaluations per run and 100 runs in baseline evaluation — giving a stark cost contrast to SEIDR's under-1000 executions for solved problems.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Compared code-specialized pretraining (Codex) vs more general models (GPT-3.5) and a smaller open Llama 3-8B; Codex (code-specialized) produced better results on PSB2, while GPT-3.5/Llama 3 showed strong aggregate coverage on HumanEval-C++ (union across runs); authors attribute differences to specialization vs generalization trade-offs and to dataset composition (PSB2 more demanding).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations on the repair-vs-replace trade-off (tree arity N* and population W) show corner cases: repair-only (N*=1) underperforms due to local minima/under-exploration; replace-only (N*=∞) also underperforms; moderate branching (e.g., N*=10 in initial Codex experiments) gave best results on PSB2 (balanced repair+replace). Authors ran grid of N* ∈ {1,2,4,10,16,100} (generalizability) and observed that moderate N* (≤16) tended to be best though no single best N* generalizes across models/languages.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Authors map SEIDR to beam/local-beam search and a mutation-only GP variant; they reason about search coverage via tree arity and beam width analogy but do not provide formal quantitative coverage/hypothesis-space metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>No online fine-tuning of LLM weights is performed; adaptation is via prompt engineering (INSTRUCT static vs INSTRUCT LLM auto-generated bug summaries), batching multiple sampled explanations/repairs, and changing temperature/sampling; these prompt/adaptive choices materially affect performance but no continual learning/fine-tuning is used.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Near-miss syndrome (LLMs often output near-correct programs that fail tests), repair-only loops prone to local minima and under-exploration, LLM outputs sometimes embed input values rather than reading input (making programs valid only for specific I/O pairs), stochastic nondeterminism causes run variability, and model specialization/generalization mismatch (Codex vs GPT-3.5) limits transfer across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-based operators embedded in an evolutionary/tree search framework can outperform or match traditional GP when (1) the repair-replace trade-off is balanced (moderate branching), (2) code-specialized LLMs are available for demanding code benchmarks, and (3) iterative debug prompts effectively focus repair steps; computational costs (in terms of program executions) are dramatically lower for SEIDR than for PushGP, suggesting that learned operators concentrate probability mass near correct solutions but require careful search to avoid near-miss local minima. Ranking/parent-selection is less influential when debugging prompts directly produce full fixes (score jumps from 0 to 1), but selection (e.g., lexicase) can help for some C++ cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1999.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1999.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PushGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PushGP (genetic programming system used on PSB2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional genetic programming system used as a baseline on PSB2 (lexicase selection variant reported by Helmuth and Kelly); characterized by large-scale program-evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PushGP</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP (tree-based genetic programming with lexicase selection)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Standard genetic programming with population-based evolution, variation operators (crossover, mutation) and lexicase or tournament selection variants; in PSB2 baseline runs, solved-if-any configuration used 100 runs with per-run limits of ~60 million program evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program synthesis on PSB2 (Program Synthesis Benchmark 2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against SEIDR in this paper; SEIDR is evaluated relative to PushGP performance on PSB2</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>PushGP: 17/25 problems solved on PSB2 (as reported by Helmuth & Kelly and used as the baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>PushGP baseline uses very large program-evaluation budgets; paper cites typical PushGP experimental budgets such as 60 million programs per run and 100 runs for evaluation — effectively 'billions' of program executions — contrasted with SEIDR's typical <1000 generated programs for solved instances.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Traditional GP requires massive evaluation budgets and can be prohibitively expensive when program execution/testing is costly; not specifically adaptive to language-model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Traditional GP provides a strong baseline for program synthesis on PSB2 but operates with orders-of-magnitude higher evaluation costs than LLM-guided hybrid approaches; this raises questions about efficiency trade-offs between exhaustive evolutionary search and learned operator-guided search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1999.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1999.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolution-through-LMs (ELM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolution through Large Models (ELM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that replaces genetic-programming mutation operators with calls to a language model (diff model) trained on edits (e.g., git commit diffs), used to encourage creative program variation rather than task-directed bug fixing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolution through Large Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evolution through Large Models (ELM)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (LM used as mutation operator inside GP)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>ELM uses an instruction fine-tuned 'diff' language model (trained on git commit messages and code diffs) to propose edits that act as mutation operators in a GP loop; the LM is given generic instructions like 'Change function f' and is intended to promote creative variation rather than targeted repair.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Reported in referenced work as a 'diff model' trained on git commit messages / code diffs (authors of this paper do not provide exact dataset size/content), but the paper notes ELM's diff model is not directed to the specific task or bug.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>General evolutionary search / creative program variation (as described in the referenced ELM paper); not evaluated on PSB2 within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work as an example of using LMs as GP operators; no direct experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Authors note ELM's model is not directed to solve specific program-synthesis problems, so performance and bias depend on diff-model training corpus (git commits) — tendency toward creativity rather than concrete requirement satisfaction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>ELM's diff model is specialized on commit diffs (domain-specific) rather than general-language pretraining; paper highlights conceptual difference versus task-directed code-specialized LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>ELM uses LM proposals as variation, but does not fine-tune the LM online (no online adaptation reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Because ELM's diff model is provided with general edit instructions rather than task-specific bug descriptions, it may not effectively produce task-correct fixes and may favor creative but irrelevant edits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Using LLMs as mutation operators is a promising direction to inject learned priors into GP, but the quality of the diff/edit model and whether it is task-directed (bug-repair prompts) vs generic (commit-style edits) critically affects whether it aids satisfying concrete correctness constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1999.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1999.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (code-davinci-edit-001)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (edit-capable variant code-davinci-edit-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized autoregressive language model used in SEIDR experiments for synthesis and debugging; produced the best SEIDR results on PSB2 in initial exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Codex (code-davinci-edit-001)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (code-specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Codex-edit is used both to produce initial drafts and to generate repairs (edits) given debug instructions; sampled with temperature schedule (spring sampling) to create diverse candidate updates. It is a code-specialized pre-trained transformer fine-tuned for code generation/editing (exact pretraining details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Used on PSB2 and HumanEval-X inside SEIDR experiments (initial exploration used Codex+GPT-3 for bug summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared experimentally in SEIDR against PushGP and against SEIDR runs with other LLMs (GPT-3.5, Llama 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Within SEIDR, Codex produced best initial-exploration results: SEIDR with Codex solved 19/25 problems on PSB2-Python (pass@1000) and 17/25 on PSB2-C++ (pass@1000); solved at least once across experiments: 20 problems in Python and 18 in C++.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Compared to PushGP baseline (17/25 on PSB2), Codex-enabled SEIDR outperformed PushGP on PSB2-Python and matched or slightly outperformed in C++.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Codex tends to perform better on PSB2 (a demanding benchmark) than more general LLMs in the authors' experiments; limited information on true OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Authors hypothesize Codex's superiority on PSB2 arises from code-specialized pretraining and optimization for code tasks (vs GPT-3.5's general focus).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Codex experiments allowed a larger max program budget (1000) than later GPT-3.5 runs (100); total monetary cost for Codex+GPT-3 experiments reported ~550 USD for the set of runs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Direct empirical contrast in paper: Codex (code-specialized) > GPT-3.5 (general) on PSB2; authors attribute performance differences to specialization vs generality in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>No online fine-tuning; adaptation through prompt and sampling strategies only.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Even with Codex, near-miss syndrome occurs; some programs generated embed concrete input values or have logic errors specific to test templates, and stochastic sampling is needed to explore alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Code-specialized LLMs can concentrate probability mass closer to correct programs in program-synthesis search spaces, enabling more efficient search (fewer executions) than traditional GP, but require iterative debugging and careful search/branching strategies to overcome near-miss failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1999.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1999.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-repair + traditional repair tools (Fan/Gupta/Joshi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based program repair combined with traditional repair tools (examples: Fan et al., Gupta et al., Joshi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related methods that integrate LLM-based repair with specialized program-repair tools (e.g., TBar, Recoder) or use LLMs for a repair step within a synthesis pipeline; discussed as related work comparing learned repair operators to more classical program-repair approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Repair of Programs from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based repair combined with traditional repair tools</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>hybrid (LLM-based repair combined with classical program-repair tools)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Prior work uses LLMs (e.g., Codex) for generating repairs and compares/combines them with specialized automated repair tools such as TBar and Recoder; some work fine-tunes smaller code models for repair. These approaches typically perform a single draft + one debug step (non-iterative) in contrast to SEIDR's iterative tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program repair / program synthesis tasks (prior works cited that examined repair effectiveness, not directly re-run in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional program-repair tools (TBar, Recoder) and LLM-only repair approaches; compared conceptually in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Most cited works used non-iterative (one repair) pipelines; SEIDR contrasts by using iterative multi-step repair.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Prior works that mix LLM repair and specialized tools reveal trade-offs between targeted bug fixes and generalization; SEIDR is presented as extending these ideas to iterative search to address near-miss syndrome.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Integrating LLM-based repair with traditional repair tooling can be useful, but iterative application and search strategies (as in SEIDR) can further mitigate near-miss failures; the directionality and task focus of the repair operator (general edit vs task-directed bug summary) matter greatly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1999.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1999.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lexicase selection (as used in SEIDR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexicase selection (parent selection strategy emphasizing per-test performance diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parent-selection/ranking algorithm from genetic programming that selects candidates by filtering on randomly shuffled test cases to preserve diverse partial solutions, used in SEIDR as an alternative to tournament selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lexicase selection (as RANK agent)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP selection operator</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Lexicase selection picks parents by iteratively filtering the population based on performance on randomly ordered tests, preserving candidates that are best for unique tests; implemented in SEIDR as an alternative to tournament selection to promote diversity of 'skills' across candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Used within SEIDR experiments on PSB2 and HumanEval-X for parent selection in the evolutionary loop.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to tournament selection within SEIDR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>When used inside SEIDR, lexicase selection yielded mean test-pass-rate results similar to tournament selection for most settings; for GPT-3.5 on C++ (both PSB2 and HumanEval) lexicase provided improvements in number of solved problems.</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>Lexicase is explicitly intended to increase diversity across test cases; authors observe lexicase sometimes increased the number of runs solving certain problems but did not consistently outperform tournament selection on average.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>SEIDR experiments swapped tournament for lexicase on best tree-arities: results were mixed — lexicase improved some C++ results but no uniform benefit across datasets/models.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lexicase may select diverse but lower-average-quality parents; when debugging prompts directly fix errors (score jumps 0→1), selection strategy has limited effect.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Within LLM-guided repair loops, parent-selection that preserves test-specific skills (lexicase) can help in settings where partial solutions cover complementary tests, but when LLM debug prompts often produce direct full fixes, the choice of selection strategy becomes less decisive.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evolution through Large Models <em>(Rating: 2)</em></li>
                <li>Repair Is Nearly Generation: Multilingual Program Repair with LLMs <em>(Rating: 2)</em></li>
                <li>Automated Repair of Programs from Large Language Models <em>(Rating: 2)</em></li>
                <li>Applying Genetic Programming to PSB2: The next Generation Program Synthesis Benchmark Suite <em>(Rating: 2)</em></li>
                <li>SelfEvolve: A Code Evolution Framework via Large Language Models <em>(Rating: 1)</em></li>
                <li>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation <em>(Rating: 1)</em></li>
                <li>Comparing Large Language Models and Grammatical Evolution for Code Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1999",
    "paper_id": "paper-276662518",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "SEIDR",
            "name_full": "Synthesize, Execute, Instruct, Debug, and Repair (SEIDR)",
            "brief_description": "A hybrid multi-agent program-synthesis framework that uses instruction‑tuned LLMs to synthesize and debug code in an iterative tree-search (beam/search-tree) loop, combined with ranking (parent selection) analogous to genetic programming.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SEIDR",
            "operator_type": "hybrid (LLM-based operators inside an evolutionary/search framework)",
            "operator_description": "SEIDR uses LLMs as the synthesis and mutation (repair) operators: a synth model to generate initial draft programs, an explain model to produce bug summaries, and a debug model to produce repaired program variants. The loop forms a tree search controlled by hyperparameters (population size W and per-parent branching N*), and ranking/parent-selection (tournament or lexicase) chooses programs for further repair. LLM calls are token-autoregressive completions (chat/instruction models) with temperature-based sampling; no online fine-tuning of the LLMs is performed during runs (adaptation is via different prompts, multiple sampled outputs, and branching settings).",
            "training_data_description": null,
            "domain_or_benchmark": "Program synthesis on PSB2 (Program Synthesis Benchmark 2) and HumanEval-X (HumanEval translated to multiple languages), experiments in Python and C++",
            "comparison_baseline": "PushGP (traditional genetic programming baseline on PSB2), LLM-only generation baselines (Codex/GPT models without iterative repair), and variants of SEIDR (different tree arities, tournament vs lexicase selection)",
            "performance_learned_operator": "SEIDR with Codex (LLM used for synth+debug) achieved 19 problems solved out of 25 on PSB2-Python (pass@1000 reported) and 17/25 on PSB2-C++; with GPT-3.5 best SEIDR achieved 16/25 on PSB2-Python and 11/25 on PSB2-C++; on HumanEval-C++ SEIDR averaged pass@100 = 78.5% with GPT-3.5 and 84.2% with Llama 3-8B (average pass@100 reported over runs).",
            "performance_traditional_operator": "PushGP (traditional genetic programming) solves 17/25 problems on PSB2 (as reported in the comparison baseline).",
            "performance_hybrid_operator": "SEIDR itself is the hybrid operator; best SEIDR configuration (Codex, tree arity 10, max 1000 candidates) outperforms PushGP on PSB2-Python and is on par in PSB2-C++ (SEIDR: 19/25 Python vs PushGP:17/25; SEIDR:17/25 C++ vs PushGP:17/25).",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": "Diversity is implicitly measured via multiple sampled repairs and beam/tree branching; lexicase selection is used to explicitly increase diversity of parents, but no numeric novelty/diversity metric (e.g., uniqueness counts) is reported.",
            "out_of_distribution_performance": "Generalizability experiments across models and datasets show that SEIDR with code-specialized Codex performed best on PSB2, while GPT-3.5 and Llama 3 show mixed results; SEIDR with GPT-3.5 and Llama3 solved nearly all HumanEval-C++ problems across runs (163/164 and 162/164 respectively when taking the union over runs), but did not outperform state-of-the-art on HumanEval-Python. This demonstrates model- and benchmark-dependent generalization behavior.",
            "training_bias_evidence": "Authors report evidence consistent with training-data effects: Codex (code-specialized) outperformed GPT-3.5 (more general) on PSB2; results on Python are more stable across runs possibly due to pretraining/training-distribution bias toward Python; the authors discuss 'data contamination' and benchmark popularity as partial explanations but cannot fully confirm due to lack of training-set visibility.",
            "computational_cost_comparison": "SEIDR: typical runs limited to generating up to 100 (generalizability experiments) or 1000 (initial Codex experiments) program candidates; reported wall-clock: ~42 hours for a full PSB2 run and ~156 hours for HumanEval-X per multi-factor experiment; monetary costs: ~550 USD for Codex+GPT-3 experiments, ~266 USD for GPT-3.5 experiments. PushGP/traditional GP: authors state PushGP requires orders-of-magnitude more program executions (they cite 'billions' of executions), e.g., PushGP considers 60 million program evaluations per run and 100 runs in baseline evaluation — giving a stark cost contrast to SEIDR's under-1000 executions for solved problems.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Compared code-specialized pretraining (Codex) vs more general models (GPT-3.5) and a smaller open Llama 3-8B; Codex (code-specialized) produced better results on PSB2, while GPT-3.5/Llama 3 showed strong aggregate coverage on HumanEval-C++ (union across runs); authors attribute differences to specialization vs generalization trade-offs and to dataset composition (PSB2 more demanding).",
            "ablation_study_results": "Ablations on the repair-vs-replace trade-off (tree arity N* and population W) show corner cases: repair-only (N*=1) underperforms due to local minima/under-exploration; replace-only (N*=∞) also underperforms; moderate branching (e.g., N*=10 in initial Codex experiments) gave best results on PSB2 (balanced repair+replace). Authors ran grid of N* ∈ {1,2,4,10,16,100} (generalizability) and observed that moderate N* (≤16) tended to be best though no single best N* generalizes across models/languages.",
            "hypothesis_space_characterization": "Authors map SEIDR to beam/local-beam search and a mutation-only GP variant; they reason about search coverage via tree arity and beam width analogy but do not provide formal quantitative coverage/hypothesis-space metrics.",
            "adaptation_during_evolution": "No online fine-tuning of LLM weights is performed; adaptation is via prompt engineering (INSTRUCT static vs INSTRUCT LLM auto-generated bug summaries), batching multiple sampled explanations/repairs, and changing temperature/sampling; these prompt/adaptive choices materially affect performance but no continual learning/fine-tuning is used.",
            "failure_modes": "Near-miss syndrome (LLMs often output near-correct programs that fail tests), repair-only loops prone to local minima and under-exploration, LLM outputs sometimes embed input values rather than reading input (making programs valid only for specific I/O pairs), stochastic nondeterminism causes run variability, and model specialization/generalization mismatch (Codex vs GPT-3.5) limits transfer across benchmarks.",
            "key_findings_for_theory": "LLM-based operators embedded in an evolutionary/tree search framework can outperform or match traditional GP when (1) the repair-replace trade-off is balanced (moderate branching), (2) code-specialized LLMs are available for demanding code benchmarks, and (3) iterative debug prompts effectively focus repair steps; computational costs (in terms of program executions) are dramatically lower for SEIDR than for PushGP, suggesting that learned operators concentrate probability mass near correct solutions but require careful search to avoid near-miss local minima. Ranking/parent-selection is less influential when debugging prompts directly produce full fixes (score jumps from 0 to 1), but selection (e.g., lexicase) can help for some C++ cases.",
            "uuid": "e1999.0"
        },
        {
            "name_short": "PushGP",
            "name_full": "PushGP (genetic programming system used on PSB2)",
            "brief_description": "A traditional genetic programming system used as a baseline on PSB2 (lexicase selection variant reported by Helmuth and Kelly); characterized by large-scale program-evaluation budgets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PushGP",
            "operator_type": "traditional GP (tree-based genetic programming with lexicase selection)",
            "operator_description": "Standard genetic programming with population-based evolution, variation operators (crossover, mutation) and lexicase or tournament selection variants; in PSB2 baseline runs, solved-if-any configuration used 100 runs with per-run limits of ~60 million program evaluations.",
            "training_data_description": null,
            "domain_or_benchmark": "Program synthesis on PSB2 (Program Synthesis Benchmark 2)",
            "comparison_baseline": "Compared against SEIDR in this paper; SEIDR is evaluated relative to PushGP performance on PSB2",
            "performance_learned_operator": null,
            "performance_traditional_operator": "PushGP: 17/25 problems solved on PSB2 (as reported by Helmuth & Kelly and used as the baseline in this paper).",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "PushGP baseline uses very large program-evaluation budgets; paper cites typical PushGP experimental budgets such as 60 million programs per run and 100 runs for evaluation — effectively 'billions' of program executions — contrasted with SEIDR's typical &lt;1000 generated programs for solved instances.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": "Traditional GP requires massive evaluation budgets and can be prohibitively expensive when program execution/testing is costly; not specifically adaptive to language-model priors.",
            "key_findings_for_theory": "Traditional GP provides a strong baseline for program synthesis on PSB2 but operates with orders-of-magnitude higher evaluation costs than LLM-guided hybrid approaches; this raises questions about efficiency trade-offs between exhaustive evolutionary search and learned operator-guided search.",
            "uuid": "e1999.1"
        },
        {
            "name_short": "Evolution-through-LMs (ELM)",
            "name_full": "Evolution through Large Models (ELM)",
            "brief_description": "A method that replaces genetic-programming mutation operators with calls to a language model (diff model) trained on edits (e.g., git commit diffs), used to encourage creative program variation rather than task-directed bug fixing.",
            "citation_title": "Evolution through Large Models",
            "mention_or_use": "mention",
            "system_name": "Evolution through Large Models (ELM)",
            "operator_type": "LLM-based (LM used as mutation operator inside GP)",
            "operator_description": "ELM uses an instruction fine-tuned 'diff' language model (trained on git commit messages and code diffs) to propose edits that act as mutation operators in a GP loop; the LM is given generic instructions like 'Change function f' and is intended to promote creative variation rather than targeted repair.",
            "training_data_description": "Reported in referenced work as a 'diff model' trained on git commit messages / code diffs (authors of this paper do not provide exact dataset size/content), but the paper notes ELM's diff model is not directed to the specific task or bug.",
            "domain_or_benchmark": "General evolutionary search / creative program variation (as described in the referenced ELM paper); not evaluated on PSB2 within this paper.",
            "comparison_baseline": "Mentioned in related work as an example of using LMs as GP operators; no direct experimental comparison in this paper.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": "Authors note ELM's model is not directed to solve specific program-synthesis problems, so performance and bias depend on diff-model training corpus (git commits) — tendency toward creativity rather than concrete requirement satisfaction.",
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "ELM's diff model is specialized on commit diffs (domain-specific) rather than general-language pretraining; paper highlights conceptual difference versus task-directed code-specialized LLMs.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "ELM uses LM proposals as variation, but does not fine-tune the LM online (no online adaptation reported in this paper).",
            "failure_modes": "Because ELM's diff model is provided with general edit instructions rather than task-specific bug descriptions, it may not effectively produce task-correct fixes and may favor creative but irrelevant edits.",
            "key_findings_for_theory": "Using LLMs as mutation operators is a promising direction to inject learned priors into GP, but the quality of the diff/edit model and whether it is task-directed (bug-repair prompts) vs generic (commit-style edits) critically affects whether it aids satisfying concrete correctness constraints.",
            "uuid": "e1999.2"
        },
        {
            "name_short": "Codex (code-davinci-edit-001)",
            "name_full": "OpenAI Codex (edit-capable variant code-davinci-edit-001)",
            "brief_description": "A code-specialized autoregressive language model used in SEIDR experiments for synthesis and debugging; produced the best SEIDR results on PSB2 in initial exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Codex (code-davinci-edit-001)",
            "operator_type": "LLM-based (code-specialized)",
            "operator_description": "Codex-edit is used both to produce initial drafts and to generate repairs (edits) given debug instructions; sampled with temperature schedule (spring sampling) to create diverse candidate updates. It is a code-specialized pre-trained transformer fine-tuned for code generation/editing (exact pretraining details not provided in this paper).",
            "training_data_description": null,
            "domain_or_benchmark": "Used on PSB2 and HumanEval-X inside SEIDR experiments (initial exploration used Codex+GPT-3 for bug summarization).",
            "comparison_baseline": "Compared experimentally in SEIDR against PushGP and against SEIDR runs with other LLMs (GPT-3.5, Llama 3).",
            "performance_learned_operator": "Within SEIDR, Codex produced best initial-exploration results: SEIDR with Codex solved 19/25 problems on PSB2-Python (pass@1000) and 17/25 on PSB2-C++ (pass@1000); solved at least once across experiments: 20 problems in Python and 18 in C++.",
            "performance_traditional_operator": "Compared to PushGP baseline (17/25 on PSB2), Codex-enabled SEIDR outperformed PushGP on PSB2-Python and matched or slightly outperformed in C++.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Codex tends to perform better on PSB2 (a demanding benchmark) than more general LLMs in the authors' experiments; limited information on true OOD performance.",
            "training_bias_evidence": "Authors hypothesize Codex's superiority on PSB2 arises from code-specialized pretraining and optimization for code tasks (vs GPT-3.5's general focus).",
            "computational_cost_comparison": "Codex experiments allowed a larger max program budget (1000) than later GPT-3.5 runs (100); total monetary cost for Codex+GPT-3 experiments reported ~550 USD for the set of runs.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Direct empirical contrast in paper: Codex (code-specialized) &gt; GPT-3.5 (general) on PSB2; authors attribute performance differences to specialization vs generality in pretraining.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "No online fine-tuning; adaptation through prompt and sampling strategies only.",
            "failure_modes": "Even with Codex, near-miss syndrome occurs; some programs generated embed concrete input values or have logic errors specific to test templates, and stochastic sampling is needed to explore alternatives.",
            "key_findings_for_theory": "Code-specialized LLMs can concentrate probability mass closer to correct programs in program-synthesis search spaces, enabling more efficient search (fewer executions) than traditional GP, but require iterative debugging and careful search/branching strategies to overcome near-miss failures.",
            "uuid": "e1999.3"
        },
        {
            "name_short": "LLM-repair + traditional repair tools (Fan/Gupta/Joshi et al.)",
            "name_full": "LLM-based program repair combined with traditional repair tools (examples: Fan et al., Gupta et al., Joshi et al.)",
            "brief_description": "Related methods that integrate LLM-based repair with specialized program-repair tools (e.g., TBar, Recoder) or use LLMs for a repair step within a synthesis pipeline; discussed as related work comparing learned repair operators to more classical program-repair approaches.",
            "citation_title": "Automated Repair of Programs from Large Language Models",
            "mention_or_use": "mention",
            "system_name": "LLM-based repair combined with traditional repair tools",
            "operator_type": "hybrid (LLM-based repair combined with classical program-repair tools)",
            "operator_description": "Prior work uses LLMs (e.g., Codex) for generating repairs and compares/combines them with specialized automated repair tools such as TBar and Recoder; some work fine-tunes smaller code models for repair. These approaches typically perform a single draft + one debug step (non-iterative) in contrast to SEIDR's iterative tree search.",
            "training_data_description": null,
            "domain_or_benchmark": "Program repair / program synthesis tasks (prior works cited that examined repair effectiveness, not directly re-run in this paper).",
            "comparison_baseline": "Traditional program-repair tools (TBar, Recoder) and LLM-only repair approaches; compared conceptually in related work.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Most cited works used non-iterative (one repair) pipelines; SEIDR contrasts by using iterative multi-step repair.",
            "failure_modes": "Prior works that mix LLM repair and specialized tools reveal trade-offs between targeted bug fixes and generalization; SEIDR is presented as extending these ideas to iterative search to address near-miss syndrome.",
            "key_findings_for_theory": "Integrating LLM-based repair with traditional repair tooling can be useful, but iterative application and search strategies (as in SEIDR) can further mitigate near-miss failures; the directionality and task focus of the repair operator (general edit vs task-directed bug summary) matter greatly.",
            "uuid": "e1999.4"
        },
        {
            "name_short": "Lexicase selection (as used in SEIDR)",
            "name_full": "Lexicase selection (parent selection strategy emphasizing per-test performance diversity)",
            "brief_description": "A parent-selection/ranking algorithm from genetic programming that selects candidates by filtering on randomly shuffled test cases to preserve diverse partial solutions, used in SEIDR as an alternative to tournament selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Lexicase selection (as RANK agent)",
            "operator_type": "traditional GP selection operator",
            "operator_description": "Lexicase selection picks parents by iteratively filtering the population based on performance on randomly ordered tests, preserving candidates that are best for unique tests; implemented in SEIDR as an alternative to tournament selection to promote diversity of 'skills' across candidates.",
            "training_data_description": null,
            "domain_or_benchmark": "Used within SEIDR experiments on PSB2 and HumanEval-X for parent selection in the evolutionary loop.",
            "comparison_baseline": "Compared directly to tournament selection within SEIDR experiments.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": "When used inside SEIDR, lexicase selection yielded mean test-pass-rate results similar to tournament selection for most settings; for GPT-3.5 on C++ (both PSB2 and HumanEval) lexicase provided improvements in number of solved problems.",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": "Lexicase is explicitly intended to increase diversity across test cases; authors observe lexicase sometimes increased the number of runs solving certain problems but did not consistently outperform tournament selection on average.",
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": null,
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": "SEIDR experiments swapped tournament for lexicase on best tree-arities: results were mixed — lexicase improved some C++ results but no uniform benefit across datasets/models.",
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": "Lexicase may select diverse but lower-average-quality parents; when debugging prompts directly fix errors (score jumps 0→1), selection strategy has limited effect.",
            "key_findings_for_theory": "Within LLM-guided repair loops, parent-selection that preserves test-specific skills (lexicase) can help in settings where partial solutions cover complementary tests, but when LLM debug prompts often produce direct full fixes, the choice of selection strategy becomes less decisive.",
            "uuid": "e1999.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evolution through Large Models",
            "rating": 2
        },
        {
            "paper_title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
            "rating": 2
        },
        {
            "paper_title": "Automated Repair of Programs from Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Applying Genetic Programming to PSB2: The next Generation Program Synthesis Benchmark Suite",
            "rating": 2
        },
        {
            "paper_title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation",
            "rating": 1
        },
        {
            "paper_title": "Comparing Large Language Models and Grammatical Evolution for Code Generation",
            "rating": 1
        }
    ],
    "cost": 0.023752749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fully Autonomous Programming using Iterative Multi-Agent Debugging with Large Language Models
10 Mar 2025</p>
<p>Anastasiia Grishina anastasiia@simula.no 
Simula 
Leon Moonen 
Norway Simula </p>
<p>Norway and University of Oslo
Norway</p>
<p>VADIM LIVENTSEV * , TU
EindhovenThe Netherlands</p>
<p>Philips Research
The Netherlands</p>
<p>AKI HÄRMÄ
Philips Research
The Netherlands</p>
<p>University of Oslo
Simula, Oslo, Oslo, Vadim LiventsevNorway, Norway</p>
<p>TU Eindhoven
EindhovenThe Netherlands</p>
<p>Philips Research
Eindhoven, Aki HärmäThe Netherlands</p>
<p>Philips Research
Eindhoven, Leon MoonenThe Netherlands</p>
<p>Simula, OsloNorway</p>
<p>Fully Autonomous Programming using Iterative Multi-Agent Debugging with Large Language Models
10 Mar 20258407D6718972AA8F2091393051F409EE10.1145/3719351arXiv:2503.07693v1[cs.AI]Accepted for publication in ACM Trans Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.automatic programminglarge language modelsprogram repair
Program synthesis with Large Language Models (LLMs) suffers from a "near-miss syndrome": the generated code closely resembles a correct solution but fails unit tests due to minor errors.We address this with a multiagent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR).Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones.We empirically explore these trade-offs by comparing replace-focused, repair-focused, and hybrid debug strategies.We also evaluate lexicase and tournament selection to rank candidates in each generation.On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches.SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments.To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks.Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising.SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%.Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B.We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs.CCS Concepts: • Software and its engineering → Software design engineering; • Computing methodologies → Neural networks; Model development and analysis; Search methodologies.</p>
<p>Introduction</p>
<p>Automatic programming has been an important goal in the field of artificial intelligence almost since its inception, promising to reduce the workload of software developers by automatically solving some of the tasks they face [Manna and Waldinger 1971].More recently, program synthesis the bug summarization model and Codex (or GPT-3 trained on code) as the program generation and debugging model.In addition, by modifying the tree arity parameter (see Section 2.2), we investigate the trade-off between generating and repairing only one program versus regenerating any program that does not pass all the tests, as well as intermediate configurations, where we build a tree of programs and update the best ones.</p>
<p>While Codex is an early code generation model, the emergence of new models that score better in programming and natural languages motivates further research into the use of SEIDR with newer models.To study the generalizability of the initial SEIDR results, we use two other LLMs, Llama 35 and GPT-3.5, 6 and an additional dataset, HumanEval-X, with different tree arity parameters [Brown et al. 2020;Chen et al. 2021b;Zheng et al. 2023].Moreover, we build up on the initial experiments with Codex and zoom in on the area with the best-performing tree arities in a hyperparameter search for a better repair-replace trade-off resolution.</p>
<p>To reflect on the parent selection strategies used in the Rank agent of SEIDR, we also explore whether the programs should be chosen based on the average performance across all tests or whether SEIDR can benefit from keeping such programs in the loop that fully cover individual tests, but do not perform well on average.Therefore, as an alternative to the tournament selection, we test the best tree arity setups with lexicase selection-based ranking [Helmuth et al. 2015].Moreover, since language models bring in stochasticity, we run the experiments several times to measure the variability of results obtained with fixed hyperparameters and reflect on the results repeatability.</p>
<p>Overall, the current paper contributes to the field of Large Language Models for Software Engineering (LLM4SE) with a framework for code generation and its iterative repair.Compared to the earlier GECCO-2023 paper, this article refines and extends the preliminary results by providing a more in-depth explanation of SEIDR as a multi-agent framework for autonomous programming, extending the original experiments and analysis to include HumanEval-X as an additional benchmark, evaluating GPT-3.5 and Llama 3 as additional LLMs, investigating lexicase selection as an alternative to tournament selection, and conducting a repeatability analysis of the framework's performance over multiple independent runs.Overall, this extension adds three research questions and doubles the number of configurations considered in these questions compared to our preliminary work.</p>
<p>Section 2 presents a framework that adapts Synthesize, Execute, Debug to instruction-tuned Large Language Models in agents that can solve programming tasks in an autonomous fashion.We discuss related work in Section 3, introduce experiments to explore effective search strategies for this framework in Section 4. Finally, in Section 5, we demonstrate that our framework outperforms conventional automatic programming techniques, such as genetic programming and naive application of large language models that generate one solution per problem without updating it iteratively.</p>
<p>Methodology</p>
<p>The proposed five-agent SEIDR framework is summarized in Figure 1, which we discuss in detail in Section 2.1.In essence, to solve a programming task defined as a text description and a collection of I/O examples, we split I/O examples into prompt and validation sets and use the prompt set in a large language model to SYNTHESIZE a population of candidate solutions.We EXECUTE the solutions, test them against the validation set, generate a text description of the identified problems used to INSTRUCT a large language model to produce repaired candidate solutions similar to the way a human developer DEBUGs a program.We RANK the candidates by correctness, measured as matching I/O pairs, discard the worst candidates, and repeat until a fully correct solution is found.</p>
<p>Ingredients</p>
<p>SEIDR makes use of instruction fine-tuned large language models: a synthesis model  synth (code, descr), a debugging model  debug (code, descr), as well as a model  explain (code, descr) that can be used for writing textual instructions, which are forwarded to the code generation model  debug for code updates.Therefore, the design can be described as two agents communicating with each other, whereby one generates code and another provides critical or supervising comments on what should be changed in the generated code.</p>
<p>The models  synth ,  debug , and  explain can be either separate or the same model.The prerequisites are that  synth and  debug models are able to "understand" natural language (descr) and partial or full programs (code) and generate code based on them.The model  explain should be able to "understand" code and natural language and either autocomplete or generate the debugging instruction from scratch.Note that  explain is optional, since alternatively the debugging instructions can be generated from failing tests using static pre-defined templates.In general, SEIDR requires sequence-to-sequence generative models for these agents.In our experiments, we select models for  synth ,  debug , and  explain based on the state-of-the-art transformer architectures [Vaswani et al. 2017] (see Section 4.2).</p>
<p>Each LLM is a highly parameterised probability distribution over the space of (code, description)tuples with parameters estimated on a large diverse (i.e., non-task-specific) corpus.This stochastic nature of language models is an important prerequisite for SEIDR, since it allows us to sample batches of diverse candidate solutions from  synth ,  debug , and  explain .We denote the number of outputs generated with  synth ,  debug , and  explain , correspondingly.Moreover, each model generates the most probable and less probable outputs in each batch, which helps diversify problem solving attempts.In the following implementation-related subsections, we explain how we vary Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<h1>include <vector></h1>
<h1>include <iostream> #include <string> ... /<em> Given an integer x, return "Fizz" if x is divisible by 3, "Buzz" if x is divisible by 5, "FizzBuzz" if x is divisible by 3 and 5, and a string version of x if none of the above hold.For example, input: 3 output: Fizz </em>/ int main() { import os import sys import numpy as np ... """ Given an integer x, return "Fizz" if x is divisible by 3, "Buzz" if x is divisible by 5, "FizzBuzz" if x is divisible by 3 and 5, and a string version of x if none of the above hold.the number of candidate solutions, debug instructions, and repairs generated in a batch by each LLM in SEIDR.</h1>
<p>While SEIDR is described here as a multi-agent system, it can equally be seen as a form of evolutionary algorithm or genetic programming, where the initialization and mutation steps of the system are performed by LLMs,  synth and  debug , correspondingly.Throughout this work, we use agent-oriented programming terminology [Shoham 1993] and evolutionary optimization terminology interchangeably to try to bridge the gaps between these domains.</p>
<p>2.1.1Synthesize.The framework starts with the SYNTHESIZE agent, which is responsible for generating initial draft solutions.We start with a basic template for a chosen programming language that contains a number of standard library imports, as shown in Figure 2.</p>
<p>We populate this template with a comment indicating a textual task description and several I/O examples from the training set.We design the templates with prompt engineering guidelines7 and prior work [de Bruin et al. 2021] in mind.We then sample  synth programs from  synth , setting code to the populated template and description to the natural language description of what the model should generate.We use spring sampling:8 a temperature-based sampling with a monotonically increasing temperature schedule where the -th program is sampled with temperature   ≈  −1  synth (we use approximate equality to enable efficient implementation by means of batching).Thus, the sampling procedure for the first programs approximates a deterministic maximum-likelihood estimation.In combination with the naturalness principle of source code [Allamanis et al. 2018;Jiang et al. 2022], this approach ensures that the samples are diverse, but always contain the most likely programs for the given task.</p>
<p>2.1.2Execute.The EXECUTE agent compiles the programs (if necessary) and launches them using the standard tools for the programming language.The program is run once for every I/O pair in the validation set.Its stdin stream receives all input lines in a given input pair, and its stdout and stderr streams are captured and saved.We then measure the score of the program defined as the  In order to represent this heterogeneous input as text that can be further processed by an LLM, we use template engines that replace placeholders in files or strings with input values and return a formatted string.In recent chat-and instruction-based LLMs, the terms template engine and prompt template are used interchangeably.</p>
<p>We consider two different designs of the INSTRUCT agent: INSTRUCT static and INSTRUCT LLM shown in Figure 3.In both cases, if stderr is not empty, i.e., execution exits with code 0 before getting any output to compare it with the expected output, the stderr-based template engine generates the instruction to fix the error.However, the designs differ in the way they transform failing I/O pairs to generate instructions in case stderr is empty.</p>
<p>INSTRUCT static uses a fixed template and substitutes placeholders for input and output with the corresponding strings of the first failing test case in its template engine.For example, we show the resulting instruction for an exemplar template in Figure 3.In contrast, INSTRUCT LLM uses the failing I/O pair in the LLM for text completion, thereby prompting the text LLM to produce the bug explanation and a summary for debugging.In addition to providing a failing test case or stderr, one may choose to give the model  explain more context, such as the problem name, task description, and the code generated so far.Each call to  explain can result in  explain ≥ 1 instructions, a batch output of an LLM.The prompt templates used for the experiments are detailed in Section 4.3.</p>
<p>2.1.4Debug.The main component of SEIDR that addresses the "near-miss syndrome" is the DEBUG agent.This agent iterates over all programs in the population to repair candidate programs and pass more tests.It uses the instructions written by INSTRUCT to sample from the  debug model  debug times to repair each candidate and create a new population of N candidates.For  debug , the parameter code is set to the current version of the candidate solution and descr to the output of INSTRUCT and any additional context chosen for a specific implementation.The current generation of candidates is then replaced by N outputs of DEBUG.</p>
<p>2.1.5Rank.The RANK agent implements what is known in genetic programming as parent selection [Koza 1994]: it selects the best  programs to be further improved by the DEBUG agent.We consider two different parent selection algorithms: tournament selection and lexicase selection.See Section 5.2.2 for their empirical comparison.</p>
<p>Tournament selection variant used in this work sorts the programs according to their average test score and selects top  candidates.The programs are selected based on the intuition that repair of the best so far (yet imperfect) programs begets good programs.The simple ranking is also referred to as tournament selection, where the best-performing candidates are chosen to participate in the next round.In the classical tournament selection, the first step is to draw  random candidates from the population.In our implementation, the first step is to generate exactly  candidates that are needed for the next round.This approach prioritizes candidates that perform the best on average over all tests.</p>
<p>Lexicase selection [Helmuth et al. 2015] is a ranking approach that maximizes the diversity of selected candidates in addition to their metric-based score.Lexicase selection ensures diversity by keeping the program candidates that perform the best on unique tests as opposed to the program candidates that perform best on average over all tests.The algorithm is as follows:</p>
<p>(1) randomly shuffle the set of tests;</p>
<p>(2) select a program with the best score on test 1;</p>
<p>(3) if several programs are tied, resolve the tie by selecting the best program on test 2; (4) repeat for tests 3, 4, . . ., until only one program is left;</p>
<p>(5) mark this program as "selected"; (6) if less than  programs are selected, go back to step 1.This ensures that even if the average quality of the selected candidates is lower, the batch of  programs collectively contains a higher number of required "skills", as measured by tests.</p>
<p>Meaning of Hyperparameters</p>
<p>After evaluating a given candidate solution in EXECUTE, SEIDR supports two approaches to address the candidate's flaws:</p>
<p>• Replace the candidate with another sample from the current population.</p>
<p>• Use INSTRUCT and DEBUG to repair the candidate.</p>
<p>We refer to this problem as the repair-replace trade-off, by analogy with production economics [Jack and Van der Duyn Schouten 2000].</p>
<p>How does the choice of hyperparameters  , the total number of candidate programs in each generation, and  , the number of selected repairs to be preserved in a generation, influence the flow of SEIDR? and  act as upper bounds on the replace option by limiting the size of the population.In the edge cases,  =  = 1 corresponds to a repair-only process, while  =  = ∞ corresponds to replace-only, as illustrated in Figure 4. Here, the repair-only scenario can also be seen as an LLM-guided random walk [Xia et al. 2020] and replace-only as random sampling from the LLM.Strategies with tree arities between 1 and ∞ are similar to population-based evolutionary algorithms.Note that  is defined by  synth for the initial draft solutions in the first generation and  explain •  debug for later generations.</p>
<p>Observe that a mutation-only genetic algorithm with tournament selection with fixed population size  , such as SEIDR, is equivalent to local beam search with beam width  on an  -ary tree [Russell 2010, Section 4.1.4].This corresponds to a known property of local beam search: it degenerates into a depth-first search at  = 1, whereas setting  = ∞ yields a breadth-first search.</p>
<p>Related Work</p>
<p>Until recently, the tasks of neural program synthesis [Gulwani et al. 2017] and program repair [Le Goues et al. 2012, 2019;Petke et al. 2018;Zhang et al. 2024b] have been considered separately.However, results from genetic programming [Sobania et al. 2021] suggest that evolution is a crucial step in synthesis.A number of important studies bridging this gap in the application of large language models have been carried out concurrently with this work, discussed below.</p>
<p>Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<p>The use of large language models for a program repair step within a program synthesis pipeline has been studied by Joshi et al. [2022] and Gupta et al. [2020], while the specific case of instructiondriven LLMs has been explored by Fan et al. [2023], where the initial synthesis is done by the Codex model [Chen et al. 2021b] and for program repair both Codex and state-of-the-art specialized repair tools such as TBar and Recoder [Just et al. 2014] are considered and compared.Zhang et al. [2023b] do the same, but fine-tune PyCodeGPT-110M [Zan et al. 2022] to use it as a repair model.The resulting framework is a two-step process (1 draft step and 1 debug step), while iterative evolution and search are not explored.</p>
<p>Evolution through Large Models (ELM) [Lehman et al. 2022] proposes to use a language model in place of a mutation operator within a traditional genetic programming framework [Koza 1994].They use a type of instruction fine-tuned model trained on git commit messages known as a diff model.9However, the model is directed neither to solve the programming problem at hand nor to fix bugs.Instead, it is provided with generic instructions such as "Change function f. " This approach is meant for the cases where creative freedom [Stanley and Lehman 2015] is encouraged rather than satisfying concrete requirements.Liu et al. [2023a] demonstrate the advantages of applying ELM to modular components of a solution, rather than the entire solution to the given problem.Some related work is explicitly guided by the metaphor of conversation between agents.Zhang et al. [2023a] implements a repair-only program synthesis loop as a conversation between an analyst agent, a coder, and a tester, while [Dong et al. 2024] does the same for tester, developer, and reviewer.The coder and the developer are roughly equivalent to SYNTHESIZE, the tester to EXECUTE, and the reviewer to INSTRUCT, while the analyst is an agent that prepares a generation prompt for the coder.Both testers are language models that predict the output of a program without actual compilation, execution, or testing, which makes Dong et al. [2024] and Zhang et al. [2023a] specific cases of chain-of-thought prompting [Yu et al. 2023] for program synthesis.</p>
<p>Several studies explore an iterative approach to program synthesis in a manner similar to SEIDR [Chen et al. 2023;Shinn et al. 2023;Xia and Zhang 2023].However, they do not explore the repair-replace trade-off and exclusively implement the repair-only approach that is prone to local minima.SelfEvolve [Jiang et al. 2023] is a repair-only version of a framework similar to SEIDR.SeflEvolve demonstrates the benefits of LLMs evolving not just the source code but an additional natural language text file that acts as the system's knowledge base and is included in the model prompt when generating code.Finally, Self-Taught Optimizer (STOP) [Zelikman et al. 2023] takes the concept of self-improvement to the meta level and uses a large language model to edit the evolutionary algorithm itself (i.e., blocks or contents of Figure 1).Their reflections on the safety implications of such automated algorithm changes are of particular interest when considering this trajectory [Zelikman et al. 2023, Section 8].These concerns do not hold in the context of SEIDR because the algorithm is fixed.In other words, SEIDR does not self-evolve but creates solutions that it improves, and the solutions that are synthesized are closely constrained by the validation set used by the EXECUTE agent.</p>
<p>These efforts to combine large language models and evolutionary techniques fall within the broader context of the quest for effective inductive bias in genetic programming [Whigham 1996]: a crucial property for any learning algorithm [Haussler 1988].Reuter et al. [2023] suggest using graph neural networks for this purpose, while grammatical evolution methods, such as WHGE [Bartoli et al. 2020] and -GE [O'Neill et al. 2004] use the known grammar of the programming language.Large language models are a novel and promising [Custode et al. 2024] alternative to grammatical evolution that incorporates semantics and idiom [Allamanis and Sutton 2014;Orlov 2020] of a programming language in addition to its grammar.</p>
<p>Experimental Design</p>
<p>To explore the capabilities of SEIDR and its generalizability, we test the framework on two benchmarks (PSB2 and HumanEval-X), a total of two different ranking strategies (tournament selection and lexicase selection), three models in the coding part of SEIDR (Codex, GPT-3.5, and Llama 3), two programming languages (Python and C++), and various branching factors.We use three models over the two parts of our experiments: one model in the initial exploration (also reported by [Liventsev et al. 2023]) and two models in the generalizability part.The problems in the benchmarks originate from coding competitions and human-written programming assignments.</p>
<p>During our empirical evaluation of SEIDR, we address the following research questions: RQ1.Repair-replace trade-off exploration: What is the impact of using different tree search strategies in the autonomous programming setting?We experiment with six different tree arities but fix the tournament selection in the ranking part and one prompt.Here, we study the impact of tree arity on the number of resolved problems as well as the speed of obtaining solutions.RQ2.Generalizability of the approach to different LLMs and an additional dataset: How does the choice of an LLM affect the performance of SEIDR?We vary the tree arity and experiment with two additional LLMs and one additional dataset.By default, we use tournament selection as the ranking strategy.RQ3.Repeatability of SEIDR in multiple runs with the same hyperparameters: How does the non-deterministic nature of LLMs affect SEIDR performance when the method is restarted several times with the same hyperparameters?We study how SEIDR results vary in different restarts of the same experiments with unchanged hyperparameters as a result of the LLM nondeterminism.The motivation for RQ3 is that LLMs exhibit stochastic behavior: the same prompt can yield different responses.Essentially, LLMs generate answers token-by-token and predict the next tokens based on the probability distribution over a vocabulary of tokens, which is sensitive to the precision of floating-point operations.If two tokens are predicted to be the next ones, with a very similar probability, it is likely that either of them will be chosen at each individual run.Further tokens are generated auto-regressively and depend on the previous tokens, so once one token diverges, the whole sequence is likely to diverge, too.RQ4.Effect of changing the parent selection strategy in the RANK agent: How does the lexicase selection-based ranking strategy impact performance in comparison to tournament selection in the RANK agent?We use the best-performing tree arities from RQ3 to run the experiments with lexicase selection as the ranking strategy instead of tournament selection, to explore whether a different parent selection algorithm can further improve the results.</p>
<p>Data</p>
<p>Our experiments use the Program Synthesis Benchmark 2 (PSB2) [Helmuth and Kelly 2022] and HumanEval-X [Zheng et al. 2023] in C++ and Python.The key criteria for this choice are the availability of task descriptions in English and unit tests in Python and C++ or language-agnostic unit tests as well as the wide acceptance of these benchmarks in the areas of generative LLMs (HumanEval) and genetic programming (PSB2).</p>
<p>4.1.1PSB2.The first dataset is a benchmark suite of 25 problems for program synthesis that resemble small real-world tasks.PSB2 was developed as a more realistic and challenging version of PSB1 [Helmuth and Spector 2015], the latter consisting of textbook problems and is widely used in genetic programming [Sobania et al. 2022].The problems require different data structures and control flows to be used for effective solutions and are taken from sources, such as competitive programming platforms and educational courses.The problems have descriptions in English, as well as 1 million (M) tests for training and 1M testing-stage tests, including edge or corner cases Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<p>that test the resulting program on complicated inputs.The tests are provided as I/O pairs and are distributed together with the problem descriptions as a PyPI package. 10 In PSB1, the training set consists of the edge test cases and is augmented by random test cases if the number of edge tests is not enough.The test set is formed by random test cases.This terminology is preserved in PSB2.We use the PSB2 training set for ranking and selection of programs (validation) within an experiment and the test set for reporting the result thereof (testing).Thus, we will refer to the PSB2 training set as the validation set, to be more consistent with how it is used in SEIDR.</p>
<p>4.1.2HumanEval-X.The second dataset that we use is a development from the original set of human-written programming tasks in HumanEval [Chen et al. 2021b], which is a standard code generation benchmark for LLMs.HumanEval consists of 164 problems with a docstring representing problem description, a function signature, a correct solution, and unit tests in Python.HumanEval-X is the result of translating correct HumanEval programs and unit tests into five programming languages [Zheng et al. 2023].We use HumanEval-Python for experiments in Python to ensure a comparison with other models in the setup without SEIDR.In addition, we test SEIDR on the HumanEval-C++ part of HumanEval-X.</p>
<p>The test functions of HumanEval-X contain all tests in one function.We split the aggregated test functions into separate tests so that the RANK agent can evaluate the score.On average, the number of tests in HumanEval-Python is 7.25 and 6.95 in HumanEval-C++, which is appointed to a repeated additional test present in some HumanEval-Python examples of the following type: assert True, "This prints if this assert fails 1 (good for debugging!)".This test type is not present in HumanEval-C++.</p>
<p>To reiterate, we keep the original HumanEval-Python setup for direct comparison with the models tested on this benchmark without SEIDR.Because of the limited number of tests, we pass up to five tests to the draft prompt and make all tests visible to SEIDR for the debugging loop.In other words, we do not have a held-out test split for HumanEval-X in the same manner as we do for PSB2.</p>
<p>Models</p>
<p>SEIDR uses up to three LLMs - synth ,  explain , and  debug -in SYNTHESIZE, INSTRUCT LLM , and DEBUG, respectively.The main prerequisite is that  synth and  debug are a text-to-code models which take both a textual description and a draft code as input.Therefore,  synth and  debug can be a chat model, a code completion model, an instruction fine-tuned, or a foundation generative language model pre-trained on code in addition to text.By analogy, the text-to-text  explain can be a chat, instruction fine-tuned, a text completion, or a text generation model pre-trained on text and code.</p>
<p>In our experiments, we use Open AI Generative Pre-trained Transformer (GPT) models by Open AI and Llama 3 by Meta [Rozière et al. 2023].GPT models are auto-regressive transformer models that have the decoder-only architecture as opposed to the original full encoder-decoder transformer.They are pre-trained on both text and code and excel at sequence-to-sequence generative tasks, including code-to-code, text-to-code, and code-to-text.</p>
<p>In our initial experiments, we use Codex-edit 11 as the LLM for writing and debugging programs and GPT-3 12 for bug summarization via text completion [Brown et al. 2020] -both being 175Bparameter models.In our generalizability experiments, we use the GPT-3.5 model 13 for program synthesis, bug summarization, and debugging.The GPT-3.5 model is an improvement over GPT-3 that is optimized for chat and available through an API.This switch is mainly motivated by rapid model updates, an OpenAI announcement that GPT-3 was due to become obsolete, i.e., not actively supported by the company, along with the company's recommendation to switch to GPT-3.5.</p>
<p>To further evaluate the generalization capabilities of SEIDR, we have chosen Llama 3-8B [Rozière et al. 2023], an open-source alternative to GPT models with the same standard decoder-only transformer architecture.Compared to Llama 2 [Touvron et al. 2023], Llama 3 introduces improvements to the architecture and the training process, such as grouped query attention.</p>
<p>Prompts</p>
<p>Prompts in the Initial Exploration of SEIDR.</p>
<p>The prompt for the LLM model is static and consists of the input for editing -candidate program generated so far -and a debug instruction to repair the candidate.The debug instructions are formulated as templates.The instructions describe the violated requirements in terms of the wrong output in a failing I/O test or summarize the bug to capture issues in code logic.We present debug instructions using the template engine format: the brackets { } denote that the placeholder in the brackets will be replaced with the value generated during execution, {I val } and {O val } stand for values of the validation set I/O pair.As shown in Figure 3, the instruction to fix execution errors that abort the program before the resulting output is obtained with stderr lines: Fix {stderr}.Debug instruction that uses the output of candidate program execution is static, formulated as follows:</p>
<p>Make sure that I val -&gt; O val .</p>
<p>(S0)</p>
<p>4.3.2Prompts for Instruction Fine-tuned and Chat Models.The prompts presented in this section are used in the experiments with GPT-3.5 and Llama 3.This implementation is optimized for instruction and chat models, which use prompts as inputs represented as text, partially with code fragments.The models use a system message that describes the "role" of the LLM and a regular message that works as an instruction or a chat message from a user.To provide more context, we always use a problem description, problem name, and programming language to the model as textual input (descr).As code, we also add an initial template depicted in Figure 2 to  synth and the current program candidate to  explain and  debug .The resulting prompts are as follows:</p>
<p>The system message is:</p>
<p>You are an experienced software developer.You write concise code in {language}.</p>
<p>The code must read input from user and return output corresponding to the task description.</p>
<p>The input to  synth looks as follows:</p>
<p>Solve the following code contest problem: {problem_name}.Problem description: {problem_description}.{program_template} Only complete the code, do not add triple quotes, do not give explanations.</p>
<p>Bug explanations are generated with  explain using the following instructions:</p>
<p>Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<p>And the debugging model 𝑝 debug operates on the following instruction:</p>
<p>Solve the following code contest problem: {problem_name}.Problem description: {problem_description}.Currently, the code is <code>{ program_candidate}</code>M odify the code as {bug_summary}.You must only return correct code.Remove any triple quotes, language name or explanations.</p>
<p>Repair-replace Trade-off Settings</p>
<p>The settings for tree arity will also be divided into two experiment sets: the ones for GPT-3 and Codex, and the ones for the GPT-3.5 and Llama 3 experiments.</p>
<p>Tree</p>
<p>Arity for the Initial Exploration of SEIDR.As described in Section 2.2, the population size (number of parents to choose from in the tournament selection or beam width from the beam search perspective)  and tree arity  define the repair-replace trade-off, where higher  and  correspond to repair over replace.We evaluate four options for these hyperparameters as shown in Table 1.We only run the experiments once, due to the experimental timeline and the discontinuation of model support by the model provider.</p>
<p>Because we aim to compare tree search parameters, we fix one default debugging instruction S0 and use the INSTRUCT static agent.Moreover, we set the upper limit for the total number of generated program candidates to 1000 to limit the experimentation time.Although some solutions may not be found within the hard limit, we assume14 that 1000 program candidates form a sufficiently large search space for our experiments. =  = ∞ is achieved in implementation by setting equal  and  equal to the upper limit of the program count of 1000.This ensures that a second generation of programs does not exist.</p>
<p>Tree Arity for SEIDR Generalizability</p>
<p>Experiments.With the shift to chat and instruction models in the generalizability part of our study, we move from generating one bug explanation and one code draft or update to a batch of those.Specifically, each of the three LLMs in SYNTHESIZE, INSTRUCT, and DEBUG can generate sequences in batches.We generate  synth programs in the first generation with  synth model,  explain bug explanations with  explain for each program in a generation, and  debug candidate repairs for each of the debugging instructions using  debug .A new generation of  explain •  debug •  programs created from each of  parents in a previous generation is ranked and filtered to keep the best-performing  candidates for generating the next candidates.</p>
<p>To balance between a reasonable number of experiments and diverse sets of hyperparameters, we fix  explain = 2 to moderately vary the bug descriptions and set  synth =  debug =  .As a reference, in the experiments with GPT-3 and Codex, we generated only one bug explanation ( explain = 1) and used  synth =  debug =  setting, too.We evaluate six options of  as shown in Table 2 and use tournament selection as the ranking strategy.</p>
<p>The choice of these tree branching hyperparameters and the maximum number of generated programs is motivated by the experiments with GPT-3 and Codex, where the best results were obtained for  = 10.Therefore, we explore the area around this value more closely in the generalizability experiments.In the same experiments, the majority of problems in PSB2 were solved within the first 100 generated programs.Therefore, the upper limit for the total number of generated program candidates is set here to 100 to limit the experimentation time.</p>
<p>To account for the stochasticity of language models and the fact that OpenAI's models do not support setting the effective sampling temperature to zero to force deterministic behavior, 15 we ran the experiments six times with each set of hyperparameters.This number of runs was selected to hit a sweet spot between the overall running time and costs of the experiments, while at the same time achieving confidence in the stability of the results in the presence of non-determinism.Note that reporting results over six runs is considerably better than the common practice of having only one run for every selection of hyperparameters, and it is in line with the best-of-class practice in the field of LLMs for code generation [Ouyang et al. 2023].</p>
<p>The total cost of running the experiments with GPT-3 and Codex were around 550 USD, and the experiments with GPT-3.5 amounted to 266 USD. 16Moreover, the time to finish one run with several branching factors and all the tests amounts to ca. 42h for PSB217 and ca.156h for HumanEval-X.Overall, we restart experiments with GPT-3.5 six times for each of the tree arities  synth =  debug =  * ∈ {1, 2, 4, 10, 16, 100} and the same for Llama 3, with a total of 6 × 6 × 2 = 72 experiments.For the lexicase selection experiments, we have 6 runs per model but one bestperforming tree arity, which adds 12 experiments to the total count.We define pass@k as the number of problems that have   = 1 if SEIDR is stopped after generating  programs, following Kulal et al. [2019] "success rate at budget of  programs." Note that Jiang et al. [2023] and Chen et al. [2023] define  as the number of restarts of the iterative method, the budget in terms of trees of programs.We choose against this approach, since it threatens the validity of the comparison between iterative tree-based program synthesis and repair-only baseline by giving the iterative approach additional budget in terms of the number of programs it can generate.</p>
<p>Codex [Chen et al. 2021b] calculates pass@n&gt;k and constructs an unbiased estimator of pass@k with lower variance, ensuring statistically robust results.We cannot apply this adjustment for SEIDR, since the adjustment assumes that the programs are independent and identically distributed, while SEIDR is a Markov chain with dependencies between iterations.</p>
<p>EPG reflects the number of programs generated before the first occurrence of the program that passes all validation test cases.EPG is indicative of the computational cost of solving a problem distributed in terms of LLM inferences and program compilations and executions.For a single execution of SEIDR, EPG is equivalent to the smallest  at which pass@k=1.</p>
<p>Implementation Details</p>
<p>To summarize the setup, in this study, we have two groups of experiments.The first group of experiments is dedicated to the initial exploration of RQ1 with Codex-edit (code-davinci-edit-001) as the LLM for writing and debugging programs.We have referred to these experiments as Initial Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<p>Exploration of SEIDR with Codex and GPT-3, and test the hyperparameter choices only on PSB2 as detailed in Table 1.Here, we use wide steps between tree arity values (see Section 4.4.1).</p>
<p>The second set of experiments mainly focuses on the generalizability (RQ2) of SEIDR and its robustness to restarting experiments with the same hyperparameters (RQ3).The motivation here is to potentially improve on GPT-3 with a newer, generally more powerful version, GPT-3.5, and its smaller open-source competitor, Llama 3. GPT-3.5 and Llama 3 are used in more fine-grained repair-replace trade-off exploration and ranking experiments (RQ1).We have referred to these experiments as SEIDR Generalizability Experiments and test SEIDR both on PSB2 and HumanEval.Building on the findings of the initial exploration, we use more fine-grained tree arity values (see Section 4.4.2,Table 2) and use the prompts from Section 4.3.2.Thus, INSTRUCT is represented by the INSTRUCT LLM agent and creates  debug bug summaries.Each program update creates  child programs from one parent with the SYNTHESIZE and DEBUG agents.We also compare the performance of SEIDR with the current state-of-the-art without SEIDR (see Section 5.2.1).</p>
<p>The second set of experiments is further updated with an alternative ranking strategy, lexicase selection (RQ4).For each model, dataset, and language, we choose the best-performing tree arity from RQ2 and exchange the tournament selection algorithm with the lexicase selection.This selection step chooses parents for debugging updates in each generation.</p>
<p>In all experiments, we set the limit to generate a maximum of  program candidates during the search for the candidate that passes all validation tests.If we reach  candidates and none of them pass all validation tests, we store the test pass rate for the last generated candidate and the best test pass rate achieved throughout the search.For the first set of experiments, we set  = 1000, and for the generalizability ones, we limit  to 100, after finding out that for the majority of problems, a solution is found among the first 100 programs or not found at all.</p>
<p>Following Helmuth and Kelly [2021], we use 2000 I/O pairs ((, )  in Figure 1) from the test split of PSB2 to evaluate the candidate program that has passed all validation test cases ((, )  in Figure 1) during debugging.Due to repetitive calls to EXECUTE, we have to resolve the speed of testing versus precision trade-off while choosing the number of validation test pairs.We resolve the trade-off by fixing the validation set size at 100 for the initial experiments and 50 for the generalizability exploration, which has more runs with the same hyperparameters.We have run a preliminary experiment to confirm that we do not lose the final test pass rate points on 2000 tests when we decreased the validation test set size from 100 (which was used in the GECCO-2023 paper) to 50 for the generalizability exploration.Due to a small number of tests in HumanEval-X, all tests are made visible to the debugging LLM and during the validation step.To operate with the chosen LLMs in SEIDR, we use ollama 18 and LangChain. 19To ensure that the program candidates generated from the same parent program are different from each other, we change the temperature parameter of the LLMs.</p>
<p>Results and Discussion</p>
<p>In this section, we present the results of the initial exploration, where we investigate the repairreplace trade-off in SEIDR with Codex and GPT-3 (RQ1) using the PSB2 benchmark.We then continue with generalizability (RQ2) and repeatability (RQ3) experiments with GPT-3.5 and Llama 3 on PSB2 and HumanEval.Finally, we test lexicase selection as the ranking strategy (RQ4).The results highlight the benefit of compromise strategies with tree arity of 10 and 100 over repair-only ( = 1) and replace-only ( = ∞) strategies.The results show that the repair-only scheme is outperformed by other strategies.We explain the poor performance of the repair-only strategy by the fact that the search space is under-explored.Specifically, the replace scenario ensures that the LLM for the debugging component represented by Codex-edit in our experiments generates different updates of program candidates using variable temperatures.The probability of finding a better fix is higher when more alternatives are generated to update the draft program at  &gt; 1 compared to  = 1.The search strategy with  = 10 yields the best results: it performs on par with PushGP for C++ and outperforms the baseline during Python program synthesis by +2 problems, resulting in a total of 19 programs that pass all test cases.The results imply that generating a moderate number of programs in parallel during the DEBUG step works better than the policies in which more updates are generated for each program (100 or 1000), or those in which only one program is updated iteratively.</p>
<p>We present the analogy of the solution speed for all four arities and the fixed default debug instruction in Figure 6.In detail, we show the distribution of EPG values in all experiments to explore how many candidate updates are generated before the solution is found.We zoom in to the cases with solutions found with up to the first 10 program candidates in Figure 6a and show the EPG distribution with the step of 100 candidates in Figure 6b.In addition, we break down the results into each tree arity in Figure 7, showing the EPG on a heatmap scale and the TPR as a number between 0 and 1, or the signs "+" if a problem is solved and "-" if the final TPR is 0.</p>
<p>Out of 100 experiments for each language, in 21-24% of runs in Python and C++, the draft program is already the solution (EPG=0).For 31-33% of the experiments, the solution is found after discarding 5 candidates.Around half of the experiments do not generate more than 100 programs.However, 5 problems are solved with more than 500 generated programs in Python and 1 problem in C++ (with  = 10).The results imply that the first steps in updating the draft program are crucial for solving the problem.The chances of solving the problem in later stages of the search, such as after 100 programs have been generated, are low.This confirms our initial assumption in Section 4.4 that 1000 programs are sufficient.</p>
<p>To briefly analyze Figure 7, we observe that some problems are solved in both languages, whereas some others -only in Python.In addition, only five problems are not solved in any SEIDR 0 1 2 3 4 5 6 7 8 9  configuration in Python (bouncing-balls, bowling, cut-vector, dice-game and leaders) and seven in C++ (bouncing-balls, cut-vector, dice-game, leaders, shopping-list, snow-day, and vector-distance).</p>
<p>Upon closer inspection of generated programs, we have noticed that in bouncing-balls, the programs have logical errors and differ considerably between programming languages, as well as in the majority of unsolved problems.Test cases and debug instructions in bowling frequently skewed the resulting programs to return answers to individual bowling score strings instead of writing an algorithm to calculate the score based on each next character.The latter mistake happened in other unresolved problems, such as cut-vector.Qualitative analysis has also shown that some programs failed to read input from the user and instead defined input strings within the code, which limited the program to testing only one I/O pair, although the algorithm was correct.</p>
<p>Repair-replace trade-off in the initial exploration (RQ1): SEIDR with Codex as the coding LLM outperforms the PushGP baseline on PSB2 in Python and performs on par with it in C++ experiments with tree arity of 10.Search strategies with tree arity larger than one benefit from the replace possibility of the SEIDR framework as a consequence of using variable temperature for Codex-edit.The repair component is also crucial for the framework because the replace-only search policy (with tree arity of ∞) performs worse than the policies that alternate between replace and repair during the program update (with tree arity of 10 or 100).</p>
<p>Generalizability Experiments</p>
<p>In this section, we present and discuss replace-repair trade-off results obtained with GPT-3.5 and Llama 3 and the effect of switching from tournament selection to lexicase selection with the best hyperparameter settings found for the trade-off.We count the number of fully solved problems Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.(i.e., reached   = 1) in experiments with the hyperparameter settings described in Table 2 in six runs and present the language-specific results for PSB2 and HumanEval.We also explore the total number of programs that need to be generated before a solution is obtained, as well as in how many of the six runs each problem is fully solved.The figures are described in detail in the dedicated sections.</p>
<p>Repair-replace</p>
<p>Trade-off and Robustness to Restarts in the Generalizability Experiments.We compare the number of solved problems in the experiments with  synth =  debug values of 1, 2, 4, 10, 16, ∞ (100) and  explain = 2 in Python and C++ and tournament selection ranking strategy in Figure 8.We will refer to the equally set values of  synth and  debug as  * hereafter.As before, experiments with  * = 1 and  * = ∞ correspond to ablation studies, where the replace option or the repair option is turned off.The results of SEIDR are compared to the baseline performance of PushGP on the PSB2 benchmark, which solves 17 out of 25 problems.The boxplots show the inter-quartile range between the first and third quartiles observed in six runs and the median values as horizontal lines within the boxes.The trend present for all the datasets and models is that the results for Python are more condensed over six runs than for C++.Since access to statistics about the training data for Llama 3 and GPT-3.5 is not provided, reasons for more stable performance in Python across runs can possibly lie in the training data distribution but cannot be confirmed.However, the fact that HumanEval-Python is a popular code generation benchmark against which models are compared may affect the results on this and other Python benchmarks.In the same line of comparison of results between two programming languages, SEIDR with GPT-3.5 performs better in Python than in C++ on PSB2.</p>
<p>Llama 3 performs worse than GPT-3.5 on PSB2 in both languages.This dataset has more test cases in stock than HumanEval and can be considered a more thorough test of the coding and debugging capabilities of LLMs.Following a recent trend, where larger models outperform smaller ones, the results of SEIDR on PSB2 confirm that the smaller model (Llama 3) performs worse than the larger one (GPT-3.5).</p>
<p>Looking back at the initial experiments with Codex and PSB2, we notice the degradation of performance from Codex to GPT-3.5:SEIDR with Codex solved 19 problems in Python and 17 in C++ with tree arity 10, while the best-performing result of SEIDR GPT-3.5 is 16 (tree arity of 10, too) in Python and 11 in C++ (several tree arities, but not 10).This result can be explained by the focus of LLM builders on the generalization of knowledge and performance on a variety of tasks, while Codex specializes in code generation.Moreover, due to increased costs from Codex to GPT-3.5, we decrease the maximum number of generated program candidates from 1000 for Codex to 100 for GPT-3.5.However, only two problems are solved with Codex with  &gt; 100: indices of substring (at program candidate #510) and substitution cipher (at candidate #210) in Python, bowling (candidate #912) and indices of substring (#410) in C++.Meanwhile, some problems are solved by Codex earlier than at the debugging attempt #100 and not solved by GPT-3.5 and vice versa.Therefore, the reduction of the maximum generated programs has only a partial effect on the difference between the results of the two models.</p>
<p>In addition to reporting the number of fully solved problems, Figure 9 also reports the mean Test Pass Rate measured in %.For HumanEval-C++, SEIDR with GPT-3.5 has better results with smaller tree arity values than with larger ones.In Python, experiments with 1 &lt;  * &lt; 100 (i.e., non-corner case values of  * ) yield better mean TPR for SEIDR with GPT-3.5 on PSB2 and with Llama 3 -for HumanEval-Python.SEIDR with Llama 3 performs with slightly better mean TPR towards larger  * on PSB2-C++, and at the same time, it performs well on HumanEval-C++, so the difference between results with different  * is small.The maximum of mean TPR for both datasets and models and max number of solved problems are obtained with  * = 16 or less, except for Llama 3 in PSB2-C++.From this part of the experiments, we notice that moderate or small tree arities ( * ≤ 16) are preferred, but there is no one leading tree arity.</p>
<p>The number of runs in which a problem is solved and the average speed of finding those solutions are shown in Figure 10 for PSB2, Figure 11 for HumanEval-Python and Figure 12 for HumanEval-C++.These figures show which problems were not solved in any run of any experiment (a row with zeros colored white) and what problems are easier to solve than others (e.g., solved in all runs, or at least one with each tree arity  * , or earlier in the search tree and shown in brighter rather than darker color but not white).We also show the results of lexicase selection runs marked with "lex." in these three figures, but will discuss them in a separate section.</p>
<p>The majority of solved PSB2 problems are solved in more than one run per setting with GPT-3.5 and faster (with fewer attempts) in Python than in C++ as illustrated in Figure 10.For Llama 3, most of the solutions are obtained in 1-3 runs.The trend for both datasets is that, for problems where a solution is found, SEIDR with GPT-3.5 makes fewer attempts in Python (brighter colors prevail in the corresponding parts of Figures 10 and 11) than SEIDR with GPT-3.5 in C++ or SEIDR with Llama 3 in both languages (darker shades in the GPT-3.5 on C++ and Llama 3 parts of Figures 10  and 12).6 5 6 6 6 6   5 5 5 6 6 6   0 0 0 0 0 0   0 0 0 0 0 0   6 5 6 6 6 6   5 5 6 6 4 6   1 3 1 2 3 2   6 4 6 6 4 2 2 3 0 0 2   1 0 1 1 1 5   0 0 0 0 0 0   0 0 0 0 0 0   6 5 5 6 6 5   0 0 0 0 0 3 1 2 0 4 2   2 1 3 2 4 0   0 0 0 0 0 0   0 0 0 0 0 0   4 1 3 2 5 2   4 3 3 3 5 0   2 2 2 3 5 0 0 0 0 0 0   0 1 2 1 3 5   0 0 0 0 0 0   0 0 0 0 0 0   1 2 1 6 3 0   2 2 2 4 5 6   1 4 2 5 3  To explore the capabilities of SEIDR with the studied models, we can take a union over experiments and count the number of problems solved at least once across 3620 restarts with a fixed dataset, language and model.In this way, in addition to reporting the number of problems solved in each run in the boxplot, we can derive the number of problems solved in any run with any set of hyperparameter using information in Figures 10-12.For example, SEIDR with GPT-3.5 does not solve only 7 problems out of 25 PSB2-Python tasks (see Figure 10).Namely, bouncing-balls, cutvector, dice-game, indices-of-substring, middle-character, solve-boolean, and substitution-cipher are solved in 0 runs with all the variations of  * .In other words, 18 unique problems are solved in the collection of all the experiments using SEIDR with GPT-3.5 on PSB2-Python.SEIDR with GPT-3.5 does not solve 12 out of 25 PSB2-C++ tasks with any hyperparameter settings.The number of solved problems in any run with SEIDR and Llama 3 are 10 for PSB2 in both languages.To support the finding about degradation of performance happening in a non-specialized model GPT-3.5 compared to the code-specialized Codex, we calculate the number of solved problems in any run with Codex and obtain 20 problems solved at least once in Python and 18 in C++.</p>
<p>Similarly, SEIDR with GPT-3.5 solves 141 out of 164 HumanEval-Python problems at least once in all runs, collectively, and 128 problems with Llama 3 (see Figure 11).In HumanEval-C++, SEIDR with GPT-3.5 solves 163 out of 164 problems (except CPP/137) in the union of all runs and 162 with Llama 3 (except CPP/137 and CPP/151) as follows from Figure 12.</p>
<p>In Figure 13, we present the analogy of the speed of obtaining solutions, EPG.We zoom in to the cases with solutions found with up to the first 10 program candidates in Figures 13a and 13c            PSB2 and HumanEval-X, respectively.The coarser-grained EPG distribution with the step of 10 candidates is shown in Figures 13b and 13d.</p>
<p>In detail, we show the distribution of EPG values in all experiments to explore what proportion of candidate updates is made before a solution is found.For example, on average, over 70% of Python solutions by SEIDR with GPT-3.5 are solved from the first attempt, i.e., have EPG= 0 (see Figure 13a).Python solutions are more frequently found from the first attempt by SEIDR with Llama 3 than later in the tree, although less frequently than with GPT-3.5.Most of solutions found by SEIDR benefit from the iterative repair and generate up to 10 extra programs with both models (see Figures 13b, 13d).However, some solutions are also found later in the tree search.</p>
<p>The EPG distribution results for correctly solved problems with   = 1 imply that the first steps in the update of the draft program are crucial for solving the problem.The chances of solving the problem in the later stages of the search are low.This confirms our assumption in Section 4.4 that 100 programs are sufficient in the generalizability experiments.Repair-replace trade-off for SEIDR with GPT-3.5 and Llama 3 in the generalizability experiments (RQ1, RQ2, RQ3): Unlike for SEIDR with Codex and a fixed debugging instruction (see Section 5.1.1),SEIDR with GPT-3.5 and Llama 3 does not show any distinct trend for all the languages and datasets in terms of the preferred tree arity value.Results over a number of runs with the same hyperparameter settings are more condensed for Python than for C++, which can be a result of optimizing LLMs for high performance on popular coding benchmarks in Python.If solutions to problems are found by SEIDR, it is done at an earlier tree search step for Python than for C++ (i.e., with a smaller EPG).The majority of solutions are found within the first 10 updates of program candidates.SEIDR solves 163 out of 164 HumanEval-C++ problems with GPT-3.5 at least once over all runs with all restarts and hyperparameter sets, and 162 -with Llama 3.</p>
<p>Parent Selection</p>
<p>Strategies within the Ranking Agent in the Generalizability Experiments.</p>
<p>Based on the mean TPR over all runs reported in Figure 9, we fix the best-performing  * for each dataset, language and model and run the experiments with lexicase selection instead of tournament selection as a parent selection strategy.The hyperparameters are shown in Table 3.We compare the number of problems solved with these settings and two types of parent selection algorithms in Figure 14 and mean TPR in Figure 15.Mean TPR with lexicase selection and the best tournament selection configuration are similar for two selection strategies for most of the experiments.Similar results are obtained for the number of fully solved problems, with the exception of C++ results of SEIDR with GPT-3.5 on both datasets, where lexicase selection improved the results.</p>
<p>To zoom in on the improvement details, we refer back to Figures 10-12, where separate columns are dedicated to lexicase selection experiments (marked with "lex.").We do not observe different programs solved with lexicase selection than with tournament selection.Some problems are solved in all six runs with lexicase selection and in fewer runs with tournament selection, such as find-pair and fizz-buzz in the C++ experiments with GPT-3.5.</p>
<p>To confirm the effect of lexicase selection on the program candidate search, we have explored the test pass rate dynamics.An example run of SEIDR with GPT-3.5 on HumanEval is shown in Figure 16.We observe that in the vast majority of cases, the test score jumps from 0 to 1 directly, not as a result of reordering candidates, but as a result of SEIDR bug summarization and candidate Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.update.Therefore, we appoint differences in results between lexicase and tournament parent selection primarily to the LLMs themselves and the debugging loop rather than to the parent selection strategy.Specifically, when an LLM gets the prompt that fixes exactly the error in a program candidate, the score jumps to 1 because of the correct prompt provided to the model more frequently than because a better candidate is chosen on a previous step.</p>
<p>Parent selection strategies for SEIDR with GPT-3.5 and Llama 3 in the generalizability experiments (RQ2, RQ3, RQ4): No leading ranking strategy is found for SEIDR experiments with GPT-3.5 and Llama 3 as measured by the average test pass rate and the average number of solved problems over six runs.In the vast majority of experiments where a solution is found and at least one debugging attempt is made the score jumps from 0 to 1 as opposed to climbing up incrementally and makes the ranking strategy less impactful than effective prompting.</p>
<p>5.2.3</p>
<p>Overall Generalizability of SEIDR with parent selection strategies and repair-replace tradeoff.We combine all results in Table 4 for PSB2 and Table 5 for HumanEval-X, where we show the number of solved problems at different cutoffs ().An average is taken over all runs for the experiments with multiple restarts, and the results for one run are shown for SEIDR with Codex.Note that at cutoff  = 1, iterations of SEIDR are not taken into account, because the first program candidate is the one LLMs generate from a template, and the trees at cutoff  = 1 have only one program in each of them, regardless of the parent selection strategy or tree arity.Here, the difference between pass@1 results with different parent selection and tree arity is explained by the stochastic behavior of the underlying LLMs.We keep pass@1 results to inform the reader about the stochasticity of the models and for the ease of visual comparison of values in each row.</p>
<p>We compare the performance of PSB2 solutions synthesized with SEIDR to the PushGP genetic programming system with down-sampled lexicase selection [Helmuth and Spector 2022].To compare the performance of SEIDR with other LLMs without SEIDR, we report the average percentage of solved problems for HumanEval-X.For HumanEval-X, we cite the performance of the state-of-the-art models as reported by their authors without SEIDR, such as CodeGeex [Zheng et al. 2023], GPT-3.5 and GPT-4 [OpenAI 2023], for reference, and compare SEIDR with GPT-3.5 and Code Llama [Rozière et al. 2023] results at different cutoffs ( for @).Furthermore, we report the number of solved problems in the union of all experiments for a given dataset, model, and language, because different problems are solved in different restarts of the method.We refer to these results as "solved at least once" hereafter.</p>
<p>In PSB2-Python experiments, SEIDR with Codex has a higher pass@1000 than the maximum pass@100 measured for SEIDR with GPT-3.5.SEIDR with Codex also solves 20 problems at least once, whereas SEIDR with GPT-3.5 solves 18. SEIDR with Codex also outperforms other LLMs on PSB2 in C++.No parent selection strategy or tree arity  * is leading across all PSB2 experiments.</p>
<p>In the HumanEval-C++ experiments, SEIDR performs well at larger , i.e., when debugging steps are made.Remarkably, the union of SEIDR experiments with both GPT-3.5 and a much smaller Llama 3 solve 163 problems (or 99.4% in Table 5) and 162 problems (or 98.8%) in HumanEval-C++, correspondingly.SEIDR does not outperform other LLMs on HumanEval-Python, which, as mentioned earlier, could be the effect of HumanEval-Python being a popular benchmark for testing LLMs and indirectly optimizing the models.</p>
<p>Direct comparison of SEIDR results with other iterative program synthesis frameworks is challenging because, in addition to differences in benchmarks used, Jiang et al. [2023] and [Chen et al. 2023] do not always report the number of generated programs, which is the primary metric that we use to compare iteration against a replace-only baseline.Summary of SEIDR results: SEIDR with Codex benefits from the repair-replace trade-off when building a search tree of PSB2 solutions.The best results on PSB2 are achieved with Codex, tree arity of 10 and a maximum of 1000 programs generated.A trend of having the best tree arity set to 10 does not fully hold for other tested models, GPT-3.5 and Llama 3, where different tree arities performed better in different settings, e.g., depending on the parent selection algorithm, dataset, and programming language.Due to increasing costs of newer models time required for testing, we have run GPT-3.5 and Llama 3 six times with each set of hyperparameters and stopped building the search tree at 100 programs.In HumanEval-C++, the union of these runs has solved 163 problems with GPT-3.5, 162 problems with a much smaller Llama 3-8B model.SEIDR runs have also solved 18 PSB2 problems in C++ and 20 in Python with Codex at least once in all the experiments.The numbers for SEIDR with GPT-3.5 are lower than for Codex, possibly due to the focus of GPT-3.5 on general reasoning and of Codex -on coding.A smaller Llama 3 model performs poorly on PSB2, which we appoint to the difficulty of the benchmark and the popularity of HumanEval and to the fact that the models can be indirectly optimized for higher performance on HumanEval than on other programming benchmarks.</p>
<p>Threats to Validity</p>
<p>External threats to validity concern SEIDR performance with benchmarks and language models different from those tested.Specifically, PSB2 and HumanEval-X contain programming tasks that require smaller functions to be generated than production-scale software.Although some canonical solutions in HumanEval-Python have been criticised [Liu et al. 2023b], we primarily use unit tests to evaluate the output of SEIDR and do not compute the exact match.Therefore, these weaknesses do not impact our results.</p>
<p>Internal threats relate to the implementation.We use PSB2, which has both corner cases and regular tests available.To ensure a fair comparison with other studies on PSB2, we evaluate and report results on the provided test set of PSB2, of which we randomly pick 2000 tests.A risk that a program does not pass tests other than the ones picked persists but is assumed to be low given the large enough number of 2000 tests.</p>
<p>The large language models for code editing and text completion used in this study are nondeterministic, which can affect the results.Due to prohibitive model inference costs and discontinuation of support for earlier models, each experiment with Codex and GPT-3 is run only once.Experiments with Llama 3 and GPT-3.5 are restarted six times.We acknowledge that more restarts can provide a more complete picture of the approach performance, although this is not the standard practice in the LLM domain [Ouyang et al. 2023] and can come at considerable financial and environmental costs.Our temperature-based sampling procedure described in Section2.1.1 reduces this stochasticity significantly, especially for low-EPG results, because earlier solutions in the search tree are obtained with lower temperatures, and lower temperature limits the stochasticity.Codex, GPT-3, and GPT-3.5 are black-box models and can generate malicious code [Pearce et al. 2022].Therefore, when coding in real life, the LLM output should be carefully reviewed before running it.</p>
<p>The results can be skewed towards high performance in the programming languages prevailing in the pre-training dataset used by the authors of the tested LLMs -an issue also known as data contamination.However, if the models were able to directly reproduce the solutions seen in the training data, these problems would have been solved from the first attempt.Since we observe  &gt; 1 values, data contamination is likely to have only a partial effect on the results.Moreover, the results can be affected by popular evaluation benchmarks: even though the benchmarks themselves are usually not parts of the pre-training dataset, the published LLMs are likely to be optimized for high performance on these benchmarks.</p>
<p>Conclusion</p>
<p>In this study, we propose SEIDR, a multi-agent framework to solve the challenge of fully autonomous programming.In SEIDR, the program synthesis procedure is augmented from the direct generation of code with large language models instructions to iterative calls to a DEBUG agent followed by the RANK agent.The DEBUG agent performs a tree search across program candidates generated by a large language model for code.The LLM used for code repair takes imperfect program candidates and instructions for their improvement as prompts.The instructions are obtained from both static templates with failing test case descriptions and templates with auto-generated bug summaries by a text completion language model.</p>
<p>In addition to the initial exploration of hyperparameters that influence the population size at each generation and the number of children for each parent in SEIDR iterations, we extend the framework to test two different ranking strategies (tournament selection and lexicase selection), three models in the coding part of SEIDR (Codex, GPT-3.5 and Llama 3), two datasets (PSB2 and HumanEval-X), two programming languages (Python and C++), and various branching factors.With the update to newer models, the prompts and parameters of the tree search are updated accordingly.</p>
<p>Contributions: We run one set of initial exploration experiments and two sets of generalizability experiments.In the initial exploration, we test SEIDR with Codex-edit as the model for draft program synthesis and debugging in Python and C++ on the PSB2 benchmark.In our experiments, SEIDR outperforms the PushGP baseline and achieves the state-of-the-art result with 19 solved problems out of 25.It requires under 1000 program executions to solve them, in stark contrast to billions22 of executions in PushGP, making it feasible in the areas with costly testing, such as robotics.Investigation of the repair-replace trade-off shows that SEIDR with tree arity of 10 outperforms both the replace-only strategy and the repair-only approach.</p>
<p>To study the generalizability of SEIDR, we experiment with GPT-3.5 and Llama 3-8B as the models for draft program synthesis, explaining errors in synthesized programs and their debugging in Python and C++ on the PSB2 and HumanEval-X benchmarks.Our experiments with lexicase and tournament parent selection in the RANK agent do not show consistent improvement with either policy or any tree arity.One observation is that program candidates in generations prior to the final solution do not pass any test in the test suite in the vast majority of cases.The test pass rate for the final solution abruptly increases in one generation from 0 to all tests passed.</p>
<p>In our generalizability experiments, SEIDR shows better performance than using LLMs without SEIDR with the same prompts and the same budget in terms of programs to generate.The method achieves high results of 78.5% average pass@100 on HumanEval-C++ with GPT-3.5 and 84.2% with Llama 3. Remarkably, in HumanEval-C++, the union of SEIDR restarts with different hyperparameters has solved 163 problems with GPT-3.5, 162 problems with a much smaller Llama 3-8B model.The union of SEIDR runs has solved 18 PSB2 problems in C++ and 20 in Python with Codex.Future work: Further investigation of SEIDR generalizability and ranking strategies are some of the areas for future work.Benchmarks with more tests than in HumanEval-X may shed more light on the most effective choice of the number of programs generated from each bug explanation, as well as the framework's comparison on large-context projects.As an agent-based framework, SEIDR shows how LLM-based agents and other non-LLM agents or components can collaboratively interact and solve software engineering tasks.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of SEIDR, a multi-agent iterative framework that uses LLMs to implement the Synthesize, Execute, Instruct, Debug, and Rank feedback loop.</p>
<p>Fig. 2 .
2
Fig. 2. Anatomy of SYNTHESIZE templates</p>
<p>Fig. 3 .
3
Fig. 3. Overview of the two designs for the INSTRUCT agent.</p>
<p>Fig. 4. Repair-replace trade-off as a tree search problem.</p>
<p>Fig. 5 .
5
Fig. 5. Number of solved PSB2 problems depending on tree arity in beam search for prompt type S0.</p>
<p>Fig. 6.Distribution of the number of generated programs during each problem-solving attempt in the experiments with different tree arities where a problem solution is found.</p>
<p>Fig. 7 .
7
Fig. 7. Number of excess programs generated (in color) and test pass rate (as numbers) depending on tree arity.Higher EPG values are shown in darker shades.We denote solved problems with "+" (test pass rate = 1), unsolved problems with "-" (test pass rate = 0), and show the test pass rate for partially solved problems.</p>
<p>Fig.8.Repair-replace trade-off as a tree search problem in SEIDR: the total number of solved problems as measured by   = 1 using SEIDR with GPT-3.5 and Llama 3 depending on tree arity * .</p>
<p>Fig.9.Repair-replace trade-off as a tree search problem in SEIDR: mean   measured in % obtained using SEIDR with GPT-3.5 and Llama 3 depending on tree arity  * .</p>
<p>Fig. 10.PSB2: mean Excess Programs Generated (in color) and the number of runs in which a task is solved.The experiments in which a specific problem is not solved in any run are shown in white.</p>
<p>Fig. 11.HumanEval-Python: mean Excess Programs Generated (in color) and the number of runs in which a task is solved.The problems that are not solved in any run are colored white.</p>
<p>Fig.12.HumanEval-C++: mean Excess Programs Generated (in color) and the number of runs in which a task is solved.The problems that are not solved in any run are colored white.</p>
<p>Fig. 13.Distribution (in %) of the number of generated programs with GPT-3.5 and Llama 3 during each problem-solving attempt on average over 6 runs with different tree arities  synth ,  debug .</p>
<p>Fig. 14 .
14
Fig. 14.Number of solved problems in lexicase and tournament selection experiments for SEIDR with GPT-3.5 and Llama 3 and fixed tree arity  * .</p>
<p>Fig. 15 .
15
Fig. 15.Mean   (measured in %) in lexicase and tournament selection experiments for SEIDR with GPT-3.5 and Llama 3 and fixed tree arity  * .</p>
<p>Fig. 16 .
16
Fig. 16.The best score on validation tests (y-axis) obtained up to an indicated logging step (x-axis) for problems solved from the second or later attempts with lexicase selection using SEIDR with GPT-3.5.Results are obtained for HumanEval."Step" on the x-axis corresponds to logging settings and roughly indicates the speed of finding solutions to different problems.Each line stands for a unique solution.</p>
<p>Table 1 .
1
Initial exploration of SEIDR: hyperparameters in the tree arity experiments.
experiment #1234population size (beam width), 𝑊110100∞ (1000)tree arity, 𝑁110100∞ (1000)max programs generated1000promptS0modelsCodex as 𝑝 synth and 𝑝 debug# restarts (or runs) with the same hyperparameters1datasetsPSB2languagesPython, C++I'm trying to solve the following code contest problem: {problem_name}.Problem description: {problem_description}.Currently, the code is<code>{program_candidate}</code>The issue is{stderr} or "it must return {expected_output} for input {input},but it returns {output}".
Describe how I should fix the code in a very concise manner.</p>
<p>Table 2 .
2
SEIDR generalizability experiments: hyperparameters in the tree arity grid search.
experiment #161718192021population size (beam width), 𝑊1481016∞ (100)# programs in the 1st generation, 𝑁 synth1481016∞ (100)# bug explanations for candidate, 𝑁 explain22222-# repairs for each explanation, 𝑁 debug1481016-max programs generated100promptssee Section 4.3.2models(a) GPT-3.5 as 𝑝 synth, 𝑝 debug, 𝑝 explain, (b) Llama 3 as 𝑝 synth, 𝑝 debug, 𝑝 explain# runs per experiment6datasetsPSB2, HumanEval-XlanguagesPython, C++
4.5 Performance IndicatorsIn our experiments, we compare the number of fully solved programs obtained with SEIDR with different values of hyperparameters.For a more detailed analysis of results, we use test pass rate (TPR) and Excess Programs Generated (EPG).TPR reflects the percentage of fully passed test cases based on the exact match of program output and test output.The TPR metric is used for the final evaluation of generated programs and does not reflect partial passing of the I/O test as opposed to the score as calculated by the RANK agent (see Section 2.1.2).</p>
<p>for
Python | GPT-3.5 tournament selectionlex.Python | Llama 3 tournament selectionlex.Python | GPT-3.5 tournament selectionlex.Python | Llama 3 tournament selectionlex.0111000234353044154410130100241120100003000000425445565666666622445664000023043666666666666663 46 06 06 06 06 06 06 16 15 06 26 16 16 06 044 456 66 56 66 66 66 66 66 66 66 66 66 66 66 69056666666666666646666666666666666436110643553044766666666666666766666666666666486666666666666686666666666666649666666600000009111330401100015000000000000000106666666666666651666666656666661166666663546466526666666666666612566666656666665300000000000000701321222020234302540000101000000014535110644365055566666666666666156666606220210356666666656656661666666666666666576666666666666617664466666666665866666666666666Problem18 19 20 21 225 3 6 5 63 4 5 6 61 4 6 5 61 4 0 0 00 1 3 0 00 0 4 0 02 4 4 4 61 0 6 1 40 0 6 2 42 0 6 2 50 0 6 1 50 0 6 3 40 0 6 0 00 0 6 3 5Problem59 60 61 62 636 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 66 2 6 6 16 4 6 6 66 5 6 6 36 6 6 6 46 6 6 6 36 6 6 6 36 6 6 6 450EPG232100002135540664644446402124232466666666666666650000000000000025666666666666666666666660556666260000000000000067666666624436662701235630100020686666666664566528666666666666666966565660103120302966666666666666706666666544536430035666513455667166666665556666316666666003426172666666665466663200000000000000736666666041323133112346366565657465556653524334342314305334310575012210000000113566666666311101760000201000000036 370 03 11 10 00 00 04 00 11 30 11 20 10 02 177 780 60 60 60 60 60 60 60 51 50 40 12 20 00 2103855600063565605796666666666666639021000034444028066666660000000402311101200200381012010000212031820000001836566666846666666856655664866666666876666666883466665896666666900000000910225224926123443930000000946666666951011212966666666970100250986666666996666666Problem100 101 102 103 1046 0 6 6 16 0 5 6 04 0 3 6 04 0 2 6 05 0 0 6 06 0 0 6 45 0 3 6 110524255031060001100107666666610866666661096666666110666656611100000001126554664113666666611400000001156566666116114666311766666661185333354119666666612001001001210445313122666666612410 16∞ (100)412410 16∞ (100)Ndebug = Nsynth = N *</p>
<p>Table 3 .
3
SEIDR generalizability experiments: hyperparameters in the lexicase selection experiments.
experiment # 2223242526272829datasetsPSB2HumanEvalmodelsGPT-3.5Llama 3GPT-3.5Llama 3language C++ Python C++ Python C++ Python C++ Pythonpopulation size (beam width), 𝑊2161616241010# programs in the 1st generation, 𝑁 synth2161616241010# bug explanations for candidate, 𝑁 explain22222222# repairs for each explanation, 𝑁 debug2161616241010max programs generated100promptssee Section 4.3.2# runs per experiment6</p>
<p>21</p>
<p>Table 4 .
4
Average number of solved PSB2 problems using SEIDR with different LLMs and PushGP.The best results among SEIDR-only experiments at each  are highlighted in bold.If pass@100 is not available, we cite the best available results in brackets. * stands for the number of debug candidates and debug instructions to generate (tree arity).
Language Model in SEIDR𝑁  *  Parent selection pass@1 pass@10pass@100PythonGPT-3.51tournament10.512.012.02tournament9.311.512.04tournament9.812.213.310tournament10.712.714.516tournament10.011.313.316lexicase10.212.314.2100 tournament10.812.213.7solved at least once by SEIDR with GPT-3.5131718Codex10tournament510 14 (pass@1000=19)solved at least once by SEIDR with Codex813 17 (pass@1000=20)PushGP (no SEIDR) ----(17)Llama 31tournament1.21.32.32tournament1.01.51.54tournament1.01.52.310tournament1.52.02.216tournament1.72.82.816lexicase1.01.72.0100 tournament1.01.03.8solved at least once by SEIDR with Llama 34511C++GPT-3.51tournament0.05.65.52tournament1.05.06.02lexicase1.39.010.04tournament1.04.66.810tournament1.01.35.616tournament1.01.28.3100 tournament1.01.22.3solved at least once by SEIDR with GPT-3.521313Codex10tournament312 14 (pass@1000=17)solved at least once by SEIDR with Codex1012 15 (pass@1000=18)PushGP (no SEIDR) ----(17)Llama 31tournament0.01.02.02tournament0.02.02.84tournament1.02.02.610tournament1.01.44.016tournament0.01.83.816lexicase1.01.43.5100 tournament0.00.03.7solved at least once by SEIDR with Llama 34511</p>
<p>Table 5 .
5
Percentage of solved tasks in HumanEval-X using SEIDR with different LLMs.The best results among SEIDR-only experiments at each  are highlighted in bold. * stands for the number of debug candidates and debug instructions to generate (tree arity).
Language Model in SEIDR𝑁  *  Rankingpass@1 pass@10 pass@100PythonGPT-3.51tournament53.460.463.32tournament51.358.263.74tournament52.158.464.84lexicase52.559.666.110tournament51.158.361.716tournament53.059.663.5100 tournament54.056.762.5solved at least once by SEIDR with GPT-3.570.784.187.8Llama 31tournament22.044.047.12tournament24.048.551.84tournament22.543.151.810tournament24.042.953.310lexicase26.244.154.116tournament23.541.651.5100 tournament21.426.348.0solved at least once by SEIDR with Llama 356.773.879.3LLM results without SEIDRGPT-3.5 (ChatGPT)--48.1--GPT-4--67.0--Code Llama 34B--48.876.893.0Unnatural Code Llama 34B --62.285.295.4CodeGeeX--22.8939.5760.92C++GPT-3.51tournament3.455.978.52tournament4.736.669.62lexicase4.335.973.64tournament4.527.762.910tournament3.613.749.716tournament4.513.251.1100 tournament3.67.131.3solved at least once by SEIDR with GPT-3.520.189.099.4Llama 31tournament29.659.579.52tournament18.651.671.44tournament20.050.673.910tournament25.656.480.010lexicase25.960.984.216tournament20.953.075.5100 tournament27.139.779.2solved at least once by SEIDR with Llama 372.097.698.8LLM results without SEIDRCode Llama 34B--47.8--CodeGeeX--17.0632.2151.00
Accepted for publication in ACM Trans.Evol.Learn.Optim., February 2025.</p>
<p>https://ai.meta.com/blog/meta-llama-3/
https://openai.com/form/researcher-access-program/ Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results
https://vadim.me/publications/spring/ Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
https://carper.ai/diff-models-a-new-way-to-edit-code/ Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
This assumption is later confirmed in Section 5.1.1.
https://community.openai.com/t/observing-discrepancy-in-completions-with-temperature-0/73380
For comparison, one run with GPT-4o cost us 315 USD (early July 2024), so further use of this model was discarded.
Due to the local setup for testing, API call limits, and the number of tests. Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
https://ollama.ai/
https://www.langchain.com/ Accepted for publication in ACM Trans. Evol. Learn. Optim., February
.
We have 6 runs and 6 different values of 𝑁 * . Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
[Chen et al. 2023] does have a sentence "Note that typically one debugging turn is sufficient" Accepted for publication in ACM Trans. Evol. Learn. Optim., February 2025.
A problem is considered "solved" by PushGP if at least 1 of 100 runs, each with a limit of 60 million programs, was successful.
AcknowledgmentsThe work presented in this paper was supported by the European Commission through Horizon 2020 grant 812882, and by the Research Council of Norway through the secureIT project (#288787).The empirical evaluation made use of the Experimental Infrastructure for Exploration of Exascale Computing (eX3), supported by the Research Council of Norway through grant #270053.Data AvailabilityTo support open science and allow for replication and verification of our work, a replication package containing the code and results is publicly available in Zenodo.23
A Survey of Genetic Programming and Its Applications. Milad Taleby Ahvanooey, Qianmu Li, Ming Wu, Shuo Wang, KSII Transactions on Internet and Information Systems (TIIS). 132019. 2019</p>
<p>A Survey of Machine Learning for Big Code and Naturalness. Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, Charles Sutton, 10.1145/3212695Comput. Surveys. 514372018. July 2018</p>
<p>Mining Idioms from Source Code. Miltiadis Allamanis, Charles Sutton, 10.1145/2635868.2635901arXiv:1404.0417ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM2014</p>
<p>Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Rajeev Alur, Rastislav Bodik, Eric Dallal, Dana Fisman, Pranav Garg, Garvit Juniwal, Hadas Kress-Gazit, P Madhusudan, M K Milo, Mukund Martin, Shamwaditya Raghothaman, Sanjit A Saha, Rishabh Seshia, Singh, 10.3233/978-1-61499-495-4-1Syntax-Guided Synthesis. Dependable Software Systems Engineering. 2015. 2015</p>
<p>Weighted Hierarchical Grammatical Evolution. Alberto Bartoli, Mauro Castelli, Eric Medvet, 10.1109/TCYB.2018.2876563IEEE Transactions on Cybernetics. 502020. Feb. 2020</p>
<p>Interpretable, Verifiable, and Robust Reinforcement Learning via Program Synthesis. Osbert Bastani, Jeevana Priya Inala, Armando Solar-Lezama, doi:10.j5znxxAI -Beyond Explainable AI: International Workshop in Conjunction with ICML -Revised and Extended Papers. Andreas Holzinger, Randy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert Müller, Wojciech Samek, Springer2022</p>
<p>Neurosymbolic Repair for Low-Code Formula Languages. Rohan Bavishi, Harshit Joshi, José Cambronero, Anna Fariha, Sumit Gulwani, Vu Le, Ivan Radicek, Ashish Tiwari, OOPSLA. ACM. 2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Jenny Blain, ; Routledge, B Tom, Benjamin Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Gretchen Herbert-Voss, Tom Krueger, Rewon Henighan, Aditya Child, Daniel M Ramesh, Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Litwin, International Conference on Neural Information Processing Systems (NeurIPS) (NIPS'20). Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; Red Hook, NY, USACurran Associates, Inc2002. 2020Nine Worlds of Seid-magic: Ecstasy and Neo-shamanism in North European Paganism</p>
<p>Deep Learning Based Vulnerability Detection: Are We There Yet?. Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, Baishakhi Ray, 10.1109/tse.2021.3087402IEEE Transactions on Software Engineering. 482022. Sept. 2022</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, 10.48550/arXiv.2107.03374arXiv:2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large Language Models Trained on Code. Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlishJan Leike</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, 10.48550/arXiv.2304.05128arXiv:2304.051282023</p>
<p>Accepted for publication in ACM. 10.5281/zenodo.13754705Trans. Evol. Learn. Optim. February 2025</p>
<p>Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages. Xinyun Chen, Dawn Song, Yuandong Tian, Advances in Neural Information Processing Systems. Curran Associates, Inc2021a34</p>
<p>Systematic Literature Review: XAI and Clinical Decision Support. In Diverse Perspectives and State-of-the-Art Approaches to the Utilization of Data-Driven Clinical Decision Support Systems. Thomas M Connolly, Mario Soflano, Petros Papadopoulos, 2023IGI Global</p>
<p>Comparing Large Language Models and Grammatical Evolution for Code Generation. Leonardo Lucio Custode, Chiara Camilla Migliore, Marco Rambaldi, Giovanni Roveri, Iacca, 10.1145/3638530.3664162Genetic and Evolutionary Computation Conference (GECCO) -Companion (GECCO '24 Companion). New York, NY, USAACM2024</p>
<p>Vadim Sander De Bruin, Milan Liventsev, Petković, 10.48550/arXiv.2108.07129arXiv:2108.07129Autoencoders as Tools for Program Synthesis. 2021</p>
<p>A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective. Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah, 10.1145/3450494ACM Transactions on Internet of Things. 22021. 2021</p>
<p>Code Search: A Survey of Techniques for Finding Code. Luca Di, Grazia , Michael Pradel, 10.1145/3565971ACM Comput. Surv. 55312023. Feb. 2023</p>
<p>Self-Collaboration Code Generation via ChatGPT. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, 10.48550/arXiv.2304.07590arXiv:2304.075902024</p>
<p>Automated Repair of Programs from Large Language Models. Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, Shin Hwei, Tan , 10.48550/arXiv.2205.10583arXiv:2205.105832023</p>
<p>A Review-Based Comparative Study of Bad Smell Detection Tools. Eduardo Fernandes, Johnatan Oliveira, Gustavo Vale, Thanis Paiva, Eduardo Figueiredo, 10.1145/2915970.2915984International Conference on Evaluation and Assessment in Software Engineering (EASE '16). New York, NY, USAACM2016</p>
<p>A Survey of Deep Learning Techniques for Autonomous Driving. Sorin Grigorescu, Bogdan Trasnea, 10.1002/rob.21918Journal of Field Robotics. 372020. 2020Tiberiu Cocias, and Gigel Macesanu</p>
<p>Programming by Examples (and Its Applications in Data Wrangling). Sumit Gulwani, 2016Microsoft Corportation22Redmond, WA, USATechnical Report</p>
<p>. Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, 10.1561/2500000010Program Synthesis. Foundations and Trends® in Programming Languages. 42017. 2017</p>
<p>Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis. Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, Dawn Song, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Quantifying Inductive Bias: AI Learning Algorithms and Valiant's Learning Framework. Daniel Conrad, Halbert , 10.1016/0004-3702(88)90002-1Artificial Intelligence. 3621984. 1988. Sept. 1988Ph. D. Dissertation. University of California, Berkeley. David HausslerProgramming by Example</p>
<p>PSB2: The Second Program Synthesis Benchmark Suite. Thomas Helmuth, Peter Kelly, 10.48550/arXiv.2106.06086arXiv:2106.060862021</p>
<p>Applying Genetic Programming to PSB2: The next Generation Program Synthesis Benchmark Suite. Genetic Programming and Evolvable Machines. Thomas Helmuth, Peter Kelly, 10.1145/2739480.2754769doi:10.1145/2739480.2754769Genetic and Evolutionary Computation Conference (GECCO). Madrid SpainACM2022. Sept. 2022. 201523General Program Synthesis Benchmark Suite</p>
<p>Problem-Solving Benefits of Down-Sampled Lexicase Selection. Thomas Helmuth, Lee Spector, 10.1162/artl_a_00341Artificial Life. 272022. March 2022</p>
<p>Solving Uncompromising Problems With Lexicase Selection. Thomas Helmuth, Lee Spector, James Matheson, 10.1109/tevc.2014.2362729IEEE Transactions on Evolutionary Computation. 192015. Oct. 2015</p>
<p>Mapping Language to Code in Programmatic Context. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, 10.48550/arXiv.1808.09588arXiv:1808.095882018</p>
<p>Optimal Repair-Replace Strategies for a Warranted Product. Nat Jack, Frank Van Der Duyn Schouten, 10.1016/s0925-5273(00)00012-8International Journal of Production Economics. 6712000. Aug. 2000</p>
<p>The Role of Explainability in Assuring Safety of Machine Learning in Healthcare. Yan Jia, John Mcdermid, Tom Lawton, Ibrahim Habli, 10.1109/tetc.2022.3171314IEEE Transactions on Emerging Topics in Computing. 1042022. Oct. 2022</p>
<p>SelfEvolve: A Code Evolution Framework via Large Language Models. Shuyang Jiang, Yuhao Wang, Yu Wang, 10.48550/arXiv.2306.02907arXiv:2306.029072023</p>
<p>Do Bugs Lead to Unnaturalness of Source Code?. Yanjie Jiang, Hui Liu, Yuxia Zhang, Weixing Ji, Hao Zhong, Lu Zhang, 10.1145/3540250.3549149ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. New York, NY, USAACM2022ESEC/FSE 2022)</p>
<p>. Accepted for publication in ACM Trans. Evol. Learn. Optim. February 2025</p>
<p>Repair Is Nearly Generation: Multilingual Program Repair with LLMs. Harshit Joshi, José Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, Gust Verbruggen, 10.48550/arXiv.2208.11640arXiv:2208.116402022</p>
<p>Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs. René Just, Darioush Jalali, Michael D Ernst, 10.1145/2610384.2628055International Symposium on Software Testing and Analysis (ISSTA). San Jose, CA, USAACM2014</p>
<p>Big Code Search: A Bibliography. Kisub Kim, Sankalp Ghatpande, Dongsun Kim, Xin Zhou, Kui Liu, Tegawendé F Bissyandé, Jacques Klein, Yves Le Traon, 10.1145/3604905ACM Comput. Surv. 56492023. Aug. 2023</p>
<p>Genetic Programming II. John R Koza, 1994MIT17</p>
<p>SPoC: Search-based Pseudocode to Code. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy S Liang, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>A Systematic Study of Automated Program Repair: Fixing 55 out of 105 Bugs for $8 Each. Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, Westley Weimer, 10.1109/icse.2012.6227211International Conference on Software Engineering (ICSE). ZurichIEEE2012</p>
<p>Automated Program Repair. Claire Le Goues, Michael Pradel, Abhik Roychoudhury, 10.1145/3318162Commun. ACM. 622019. Nov. 2019</p>
<p>Evolution through Large Models. Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O Stanley, 10.48550/arXiv.2206.08896arXiv:2206.088962022</p>
<p>Competition-Level Code Generation with AlphaCode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, 10.1126/science.abq1158Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Dec. 2022378</p>
<p>Algorithm Evolution Using Large Language Model. Fei Liu, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang, 10.48550/arXiv.2311.15249arXiv:2311.152492023a</p>
<p>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Advances in Neural Information Processing Systems. Curran Associates, Inc2023b36</p>
<p>Fully Autonomous Programming with Large Language Models. Vadim Liventsev, Anastasiia Grishina, Aki Härmä, Leon Moonen, 10.1145/3583131.3590481Genetic and Evolutionary Computation Conference (GECCO). ACM2023</p>
<p>BF++: A Language for General-Purpose Program Synthesis. Vadim Liventsev, Aki Härmä, Milan Petković, 10.48550/arXiv.2101.09571arXiv:2101.095712021</p>
<p>CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shengyu Shao Kun Deng, Shujie Fu, Liu, arXiv:2102.04664Neural Information Processing Systems Track on Datasets and Benchmarks. Curran Associates, Inc2021</p>
<p>Fundamentals of Deductive Program Synthesis. Z Manna, R Waldinger, 10.1109/32.153379IEEE Transactions on Software Engineering. 181992. 1992</p>
<p>Toward Automatic Program Synthesis. Zohar Manna, Richard J Waldinger, 10.1145/362566.362568Commun. ACM. 141971. 1971</p>
<p>A Review of Shared Control for Automated Vehicles: Theory and Applications. Mauricio Marcano, Sergio Díaz, Joshué Pérez, Eloy Irigoyen, 10.1109/thms.2020.3017748IEEE Transactions on Human-Machine Systems. 502020. 2020</p>
<p>CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models. Changan Niu, Chuanyi Li, Vincent Ng, Bin Luo, 10.48550/arXiv.2302.04030arXiv:2302.040302023</p>
<p>𝜋 Grammatical Evolution. O' Michael, Anthony Neill, Miguel Brabazon, Sean Mc Nicolau, Peter Garraghy, Keenan, 10.1007/978-3-540-24855-2_70Genetic and Evolutionary Computation Conference (GECCO), Kalyanmoy Deb. Berlin, HeidelbergSpringer2004</p>
<p>. 10.48550/arXiv.2303.08774arXiv:2303.08774Technical Report</p>
<p>Finding Idioms in Source Code Using Subtree Counting Techniques. Dmitry Orlov, 10.1007/978-3-030-61470-6_4Leveraging Applications of Formal Methods, Verification and Validation: Engineering Principles. Tiziana Margaria, Bernhard Steffen, ChamSpringer2020</p>
<p>LLM Is Like a Box of Chocolates: The Non-determinism of ChatGPT in Code Generation. Shuyin Ouyang, Jie M Zhang, Mark Harman, Meng Wang, 10.48550/arXiv.2308.02828arXiv:2308.02828Trans. Evol. Learn. Optim. 2023. February 2025</p>
<p>Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions. Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, 10.1109/sp46214.2022.9833571IEEE Symposium on Security and Privacy (SP). IEEE2022</p>
<p>Genetic Improvement of Software: A Comprehensive Survey. Justyna Petke, O Saemundur, Mark Haraldsson, William B Harman, David R Langdon, John R White, Woodward, 10.1109/tevc.2017.2693219IEEE Transactions on Evolutionary Computation. 2232018. June 2018</p>
<p>FlashMeta: A Framework for Inductive Program Synthesis. Oleksandr Polozov, Sumit Gulwani, 10.1145/2814270.2814310OOPSLA. ACM. 2015</p>
<p>CodeBLEU: A Method for Automatic Evaluation of Code Synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, 10.48550/arXiv.2009.10297arXiv:2009.102972020</p>
<p>Graph Networks as Inductive Bias for Genetic Programming: Symbolic Models for Particle-Laden Flows. Julia Reuter, Hani Elmestikawy, Fabien Evrard, Sanaz Mostaghim, Berend Van Wachem, 10.1007/978-3-031-29573-7_3Genetic Programming, Gisele Pappa. Mario Giacobini, Zdenek Vasicek, ChamSpringer2023</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, 10.48550/arXiv.2308.12950arXiv:2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. </p>
<p>Artificial Intelligence a Modern Approach. Russell Stuart, 2010Pearson Education, Inc</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362023. Dec. 2023</p>
<p>Exploring the Robustness of Large Language Models for Solving Programming Problems. Atsushi Shirafuji, Yutaka Watanobe, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki, 10.48550/arXiv.2306.14583arXiv:2306.145832023</p>
<p>Agent-Oriented Programming. Yoav Shoham, 10.1016/0004-3702(93)90034-9Artificial Intelligence. 60931993. March 1993</p>
<p>Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of Github Copilot and Genetic Programming. Dominik Sobania, Martin Briesch, Franz Rothlauf, 10.1145/3512290.3528700Genetic and Evolutionary Computation Conference (GECCO). Boston MassachusettsACM2022</p>
<p>Recent Developments in Program Synthesis with Evolutionary Algorithms. Dominik Sobania, Dirk Schweim, Franz Rothlauf, 10.48550/arXiv.2108.12227arXiv:2108.122272021</p>
<p>Why Greatness Cannot Be Planned: The Myth of the Objective. Kenneth O Stanley, Joel Lehman, 2015SpringerCham Heidelberg New York Dordrecht London</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.13971arXiv:2302.139712023</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, ; Neurips, ) , I Guyon, U V Luxburg, S Bengio, H Wallach, arXiv:1706.03762International Conference on Neural Information Processing Systems. R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc2017</p>
<p>Search Bias, Language Bias, and Genetic Programming. Genetic Programming. A Peter, Whigham, 1996. 1996. 1996</p>
<p>Conversational Automated Program Repair. Chunqiu Steven, Xia , Lingming Zhang, 10.48550/arXiv.2301.13246arXiv:2301.132462023</p>
<p>Random Walks: A Review of Algorithms and Applications. Feng Xia, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, Xiangjie Kong, 10.1109/TETCI.2019.2952908IEEE Transactions on Emerging Topics in Computational Intelligence. 422020. April 2020</p>
<p>A Systematic Evaluation of Large Language Models of Code. F Frank, Uri Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, 10.1145/3520312.3534862International Symposium on Machine Programming (MAPS 2022). New York, NY, USAACM2022</p>
<p>CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation. Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang, 10.18653/v1/2023.findings-emnlp.337Findings of the Association for Computational Linguistics (EMNLP). Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen, 10.48550/arXiv.2310.04959arXiv:2310.04959Towards Better Chain-of-Thought Prompting Strategies: A Survey. 2023</p>
<p>. Accepted for publication in ACM Trans. Evol. Learn. Optim. February 2025</p>
<p>CERT: Continual Pre-training on Sketches for Library-oriented Code Generation. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou, 10.24963/ijcai.2022/329International Joint Conference on Artificial Intelligence. 20223</p>
<p>Large Language Models Meet NL2Code: A Survey. Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou, 10.18653/v1/2023.acl-long.411Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational Linguistics20231</p>
<p>NAPS: Natural Program Synthesis Dataset. Maksym Zavershynskyi, Alex Skidanov, Illia Polosukhin, 10.48550/arXiv.1807.0316arXiv:1807.03162018</p>
<p>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman, Kalai , Annual Workshop on Optimization for Machine Learning (OPT). openreview.net. 2023</p>
<p>Self-Edit: Fault-Aware Code Editor for Code Generation. Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin, 10.18653/v1/2023.acl-long.45Annual Meeting of the Association for Computational Linguistics. Long Papers. Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers; Toronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>A Systematic Literature Review on Large Language Models for Automated Program Repair. Quanjun Zhang, Chunrong Fang, Yang Xie, Yuxiang Ma, Weisong Sun, Yun Yang, Zhenyu Chen, 10.48550/arXiv.2405.01466arXiv:2405.014662024b</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, arXiv:2308.10792Instruction Tuning for Large Language Models: A Survey. 2024a</p>
<p>STEAM: Simulating the InTeractive BEhavior of ProgrAMmers for Automatic Bug Fixing. Yuwei Zhang, Zhi Jin, Ying Xing, Ge Li, 10.48550/arXiv.2308.14460arXiv:2308.144602023a</p>
<p>CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang, 10.1145/3580305.3599790ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). Long Beach CA USAACM2023. 19 February 2025. February 2025</p>            </div>
        </div>

    </div>
</body>
</html>